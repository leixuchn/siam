INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "150"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-07 10:48:23.970144: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:48:23.970288: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:48:23.970294: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:48:23.970299: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:48:23.970303: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-07 10:48:27.623130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 6.45GiB
2017-12-07 10:48:27.623170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-07 10:48:27.623177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-07 10:48:27.623191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-07 10:48:48.635259: step 0, loss = 2.03, batch loss = 1.97 (0.6 examples/sec; 12.641 sec/batch; 1167h:30m:09s remains)
2017-12-07 10:48:49.457065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3180313 -4.3191881 -4.3232436 -4.3217621 -4.3085246 -4.2852445 -4.2563391 -4.23052 -4.2208557 -4.2270727 -4.2428937 -4.2658453 -4.2912335 -4.3112655 -4.3240423][-4.3136516 -4.3154674 -4.3176484 -4.3071985 -4.2782106 -4.2373028 -4.1922321 -4.1597037 -4.1555343 -4.174819 -4.2043858 -4.2382641 -4.2714362 -4.2982855 -4.3168025][-4.304791 -4.3072782 -4.3033419 -4.2752261 -4.2235723 -4.1575408 -4.088737 -4.0452452 -4.0510864 -4.0927134 -4.1443009 -4.1980357 -4.2461696 -4.2842164 -4.3098993][-4.290792 -4.2948456 -4.2838159 -4.2389421 -4.1670694 -4.07612 -3.976882 -3.9179282 -3.9418833 -4.0115457 -4.0887218 -4.1649847 -4.2294617 -4.2766891 -4.3074555][-4.2752228 -4.280427 -4.2608738 -4.1994109 -4.1045022 -3.9816825 -3.8443274 -3.7709596 -3.827589 -3.9343061 -4.0393481 -4.1376486 -4.215766 -4.2717161 -4.306797][-4.2577028 -4.25997 -4.2308378 -4.1551523 -4.0373836 -3.8813951 -3.7045064 -3.6304557 -3.7400846 -3.8874867 -4.0144696 -4.1250372 -4.2090797 -4.2702146 -4.3069406][-4.2398624 -4.23528 -4.1976933 -4.1147056 -3.9861321 -3.8156865 -3.6266685 -3.5881269 -3.7489316 -3.913676 -4.0388784 -4.1420064 -4.2211528 -4.2779641 -4.31076][-4.225656 -4.2132683 -4.1745124 -4.102982 -3.9976282 -3.8616974 -3.7260559 -3.7371788 -3.8878212 -4.0196438 -4.1070871 -4.1823072 -4.244976 -4.2906823 -4.3169608][-4.2126775 -4.1961207 -4.1632838 -4.1153812 -4.0475011 -3.9633629 -3.8909032 -3.9208426 -4.028039 -4.1141381 -4.164423 -4.2152572 -4.2644591 -4.3024888 -4.3229246][-4.2043719 -4.1877069 -4.16355 -4.1382818 -4.1029806 -4.0592828 -4.026257 -4.0551295 -4.1242938 -4.1749082 -4.1978955 -4.2312117 -4.2734518 -4.308435 -4.3263011][-4.2019486 -4.18714 -4.1722889 -4.1605248 -4.1470742 -4.1287189 -4.1149836 -4.1346574 -4.1755996 -4.2031837 -4.2145023 -4.2430387 -4.2831483 -4.3151546 -4.329957][-4.2109127 -4.2022538 -4.1969285 -4.19454 -4.194437 -4.1891732 -4.18325 -4.1914706 -4.2073631 -4.2199759 -4.2275014 -4.2555294 -4.2942405 -4.3217273 -4.3328743][-4.233819 -4.234488 -4.2358556 -4.2373133 -4.2423191 -4.2380571 -4.2288189 -4.2246027 -4.2226858 -4.2250805 -4.23032 -4.2586 -4.2971311 -4.3230643 -4.3333273][-4.2498116 -4.2580633 -4.2645588 -4.2672248 -4.2706285 -4.2603726 -4.2422733 -4.2272205 -4.2166615 -4.2159882 -4.2233348 -4.2537975 -4.2935829 -4.3200207 -4.3314505][-4.244575 -4.2558403 -4.2655025 -4.2704821 -4.2724371 -4.256669 -4.2282763 -4.2035708 -4.1914592 -4.1949835 -4.2072845 -4.2422185 -4.2844877 -4.3132515 -4.3278465]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 10:48:55.354491: step 10, loss = 2.05, batch loss = 2.00 (16.4 examples/sec; 0.487 sec/batch; 44h:59m:24s remains)
INFO - root - 2017-12-07 10:48:59.942343: step 20, loss = 2.05, batch loss = 2.00 (16.9 examples/sec; 0.473 sec/batch; 43h:43m:02s remains)
INFO - root - 2017-12-07 10:49:04.562341: step 30, loss = 2.05, batch loss = 1.99 (17.6 examples/sec; 0.455 sec/batch; 42h:02m:22s remains)
INFO - root - 2017-12-07 10:49:09.101771: step 40, loss = 2.08, batch loss = 2.02 (18.4 examples/sec; 0.435 sec/batch; 40h:07m:36s remains)
INFO - root - 2017-12-07 10:49:13.642075: step 50, loss = 2.07, batch loss = 2.02 (18.1 examples/sec; 0.442 sec/batch; 40h:49m:26s remains)
INFO - root - 2017-12-07 10:49:18.235463: step 60, loss = 2.10, batch loss = 2.04 (17.8 examples/sec; 0.450 sec/batch; 41h:33m:50s remains)
INFO - root - 2017-12-07 10:49:22.799757: step 70, loss = 2.06, batch loss = 2.00 (16.8 examples/sec; 0.477 sec/batch; 44h:04m:48s remains)
INFO - root - 2017-12-07 10:49:27.373918: step 80, loss = 2.06, batch loss = 2.00 (16.7 examples/sec; 0.479 sec/batch; 44h:12m:43s remains)
INFO - root - 2017-12-07 10:49:31.974539: step 90, loss = 2.09, batch loss = 2.03 (17.4 examples/sec; 0.459 sec/batch; 42h:24m:47s remains)
INFO - root - 2017-12-07 10:49:36.466658: step 100, loss = 2.07, batch loss = 2.01 (18.1 examples/sec; 0.443 sec/batch; 40h:55m:10s remains)
2017-12-07 10:49:37.069433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2460532 -4.2385912 -4.2305923 -4.2157383 -4.2109222 -4.2255411 -4.2487903 -4.2662678 -4.2653975 -4.2458458 -4.2024636 -4.1403427 -4.0785828 -4.0319104 -4.0178676][-4.2529931 -4.2296672 -4.2088332 -4.1885986 -4.187355 -4.2092352 -4.2380924 -4.2531681 -4.2427058 -4.2104621 -4.15493 -4.0820632 -4.014185 -3.9712844 -3.9735606][-4.2625561 -4.2303953 -4.2030683 -4.18139 -4.1829977 -4.2054005 -4.2302547 -4.2365 -4.2165041 -4.1769466 -4.1185336 -4.0528812 -3.9997885 -3.9786727 -4.0025005][-4.2656579 -4.2377133 -4.2114787 -4.1893277 -4.1895971 -4.2016764 -4.2099342 -4.2032628 -4.1804748 -4.144423 -4.1028404 -4.0685339 -4.0554972 -4.0659266 -4.0961595][-4.2729568 -4.2550497 -4.2290521 -4.2024808 -4.1907282 -4.1820464 -4.1663451 -4.1462584 -4.1275644 -4.11057 -4.1043353 -4.11303 -4.1375847 -4.1659713 -4.1897182][-4.275526 -4.268054 -4.2453885 -4.2081285 -4.1686783 -4.1272278 -4.0881095 -4.0624223 -4.0562038 -4.06983 -4.10307 -4.1480885 -4.1950712 -4.2296743 -4.247685][-4.2698393 -4.2644463 -4.2391806 -4.1901326 -4.1244464 -4.0491295 -3.9872212 -3.9630706 -3.9839292 -4.036303 -4.1001344 -4.1669855 -4.2249007 -4.2609749 -4.2779303][-4.2481718 -4.2392945 -4.2152338 -4.1673141 -4.0909562 -3.9966378 -3.9195523 -3.9037194 -3.959321 -4.0480847 -4.1260462 -4.1956458 -4.2513452 -4.2851658 -4.300005][-4.2107115 -4.1966763 -4.1840997 -4.1578889 -4.1027861 -4.02835 -3.9671266 -3.9635885 -4.0242414 -4.1116219 -4.1801481 -4.2339578 -4.2744818 -4.2973137 -4.3026848][-4.1555586 -4.14178 -4.149354 -4.1603622 -4.148211 -4.1194005 -4.0937071 -4.09998 -4.1368828 -4.1906552 -4.2331896 -4.2625256 -4.2801991 -4.2857285 -4.2814984][-4.088809 -4.0865765 -4.1229358 -4.1717906 -4.1985235 -4.2055902 -4.2060943 -4.2174692 -4.2361174 -4.260066 -4.2742124 -4.2773123 -4.2720246 -4.2612963 -4.249795][-4.0270839 -4.0453129 -4.1079173 -4.1820955 -4.2312274 -4.2547483 -4.2654338 -4.2796946 -4.291451 -4.2995963 -4.292953 -4.2738008 -4.2519121 -4.2298217 -4.2117114][-3.9895923 -4.0294409 -4.1058273 -4.1883693 -4.2443509 -4.2701616 -4.2832122 -4.2993007 -4.3104758 -4.3117518 -4.2912869 -4.254961 -4.2202892 -4.1895394 -4.16607][-4.0123425 -4.0579128 -4.1297731 -4.2040691 -4.2526264 -4.2714334 -4.2816367 -4.2961049 -4.3041549 -4.2991786 -4.2731862 -4.2317939 -4.1940756 -4.1630611 -4.13997][-4.081419 -4.1185021 -4.1705737 -4.2248487 -4.2590027 -4.2698154 -4.2744513 -4.2825909 -4.2857885 -4.2772737 -4.2548237 -4.2220049 -4.1946597 -4.1743622 -4.1563787]]...]
INFO - root - 2017-12-07 10:49:41.664655: step 110, loss = 2.08, batch loss = 2.02 (16.7 examples/sec; 0.478 sec/batch; 44h:10m:06s remains)
INFO - root - 2017-12-07 10:49:46.248251: step 120, loss = 2.05, batch loss = 2.00 (17.7 examples/sec; 0.451 sec/batch; 41h:39m:54s remains)
INFO - root - 2017-12-07 10:49:50.928237: step 130, loss = 2.06, batch loss = 2.01 (16.9 examples/sec; 0.474 sec/batch; 43h:48m:06s remains)
INFO - root - 2017-12-07 10:49:55.519799: step 140, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 39h:41m:04s remains)
INFO - root - 2017-12-07 10:50:00.111039: step 150, loss = 2.11, batch loss = 2.05 (17.0 examples/sec; 0.471 sec/batch; 43h:31m:06s remains)
INFO - root - 2017-12-07 10:50:04.704318: step 160, loss = 2.05, batch loss = 1.99 (16.5 examples/sec; 0.485 sec/batch; 44h:44m:07s remains)
INFO - root - 2017-12-07 10:50:09.363362: step 170, loss = 2.08, batch loss = 2.03 (16.2 examples/sec; 0.493 sec/batch; 45h:30m:09s remains)
INFO - root - 2017-12-07 10:50:13.932106: step 180, loss = 2.06, batch loss = 2.00 (17.3 examples/sec; 0.464 sec/batch; 42h:48m:22s remains)
INFO - root - 2017-12-07 10:50:18.477424: step 190, loss = 2.08, batch loss = 2.02 (19.3 examples/sec; 0.414 sec/batch; 38h:14m:21s remains)
INFO - root - 2017-12-07 10:50:23.059195: step 200, loss = 2.06, batch loss = 2.00 (18.1 examples/sec; 0.442 sec/batch; 40h:49m:24s remains)
2017-12-07 10:50:23.682149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2261539 -4.2173662 -4.2158957 -4.2212548 -4.230206 -4.2365189 -4.2369514 -4.2327142 -4.2248392 -4.2122936 -4.2032871 -4.2013745 -4.2028413 -4.2036414 -4.2114372][-4.185338 -4.168664 -4.1635418 -4.1706052 -4.1842084 -4.1953411 -4.1981349 -4.1949215 -4.1911736 -4.1803823 -4.1703658 -4.170639 -4.1758132 -4.1806793 -4.1941204][-4.1594963 -4.1354856 -4.1260743 -4.1350665 -4.1533895 -4.1688757 -4.17331 -4.1720805 -4.175025 -4.169364 -4.1580229 -4.1565647 -4.1624885 -4.1700864 -4.1864781][-4.1499748 -4.1222377 -4.1082034 -4.1168823 -4.1370831 -4.1521387 -4.1557622 -4.1580892 -4.1704197 -4.1724415 -4.1591182 -4.1493492 -4.1478839 -4.1530428 -4.1672492][-4.1496935 -4.1186743 -4.0967922 -4.0978322 -4.1133304 -4.1205106 -4.1153965 -4.1196394 -4.14158 -4.1533952 -4.1444097 -4.1294293 -4.1181421 -4.1172886 -4.12855][-4.1527691 -4.1211367 -4.0912433 -4.0808315 -4.0838151 -4.073741 -4.052588 -4.0502062 -4.0742249 -4.09903 -4.1081233 -4.1018372 -4.0852818 -4.0766611 -4.0841107][-4.1438785 -4.1152706 -4.0827851 -4.0645657 -4.0574913 -4.0365667 -4.0058608 -3.9915335 -4.0040684 -4.0331521 -4.062151 -4.0708342 -4.0579162 -4.0462751 -4.051949][-4.1255422 -4.104331 -4.0773206 -4.0591927 -4.0533996 -4.0394454 -4.01421 -3.9925969 -3.9854987 -4.0028372 -4.0374861 -4.0581555 -4.0548263 -4.0493226 -4.059042][-4.1092834 -4.0976906 -4.0787797 -4.0654306 -4.0667558 -4.0657697 -4.0538568 -4.03569 -4.0178509 -4.0167074 -4.0402722 -4.06492 -4.0717249 -4.0732093 -4.0855417][-4.1119075 -4.1092873 -4.0970192 -4.0866971 -4.0885396 -4.0917025 -4.0881858 -4.0772018 -4.0584927 -4.0478396 -4.0573134 -4.074636 -4.0834026 -4.0880508 -4.1008878][-4.122117 -4.1233888 -4.113337 -4.1035709 -4.1016655 -4.1034012 -4.1045785 -4.1005883 -4.0868564 -4.076056 -4.0782127 -4.0868683 -4.0938311 -4.10067 -4.1122642][-4.1260967 -4.1295071 -4.1209955 -4.1133046 -4.1121182 -4.1156583 -4.1200113 -4.119092 -4.1095734 -4.1014156 -4.1024947 -4.1082764 -4.1156206 -4.1231632 -4.1306863][-4.1284881 -4.1335278 -4.1278176 -4.1236806 -4.1254883 -4.1330624 -4.1389785 -4.1384692 -4.1320405 -4.1293054 -4.1340528 -4.1420174 -4.1509743 -4.1565633 -4.1593566][-4.1390734 -4.1452913 -4.1415014 -4.1382546 -4.1424608 -4.1533947 -4.1592503 -4.1568394 -4.1518612 -4.15473 -4.1645327 -4.1752229 -4.1834707 -4.18648 -4.1871333][-4.1588068 -4.1646037 -4.1615176 -4.1579361 -4.1627755 -4.1743584 -4.1796513 -4.1763167 -4.1722689 -4.1774545 -4.1885152 -4.1989236 -4.2056375 -4.2079086 -4.2085633]]...]
INFO - root - 2017-12-07 10:50:28.235861: step 210, loss = 2.04, batch loss = 1.98 (18.2 examples/sec; 0.440 sec/batch; 40h:34m:56s remains)
INFO - root - 2017-12-07 10:50:32.828719: step 220, loss = 2.08, batch loss = 2.02 (18.2 examples/sec; 0.439 sec/batch; 40h:31m:04s remains)
INFO - root - 2017-12-07 10:50:37.430772: step 230, loss = 2.06, batch loss = 2.00 (18.6 examples/sec; 0.430 sec/batch; 39h:41m:36s remains)
INFO - root - 2017-12-07 10:50:41.632836: step 240, loss = 2.04, batch loss = 1.98 (26.4 examples/sec; 0.303 sec/batch; 27h:56m:07s remains)
INFO - root - 2017-12-07 10:50:46.194932: step 250, loss = 2.06, batch loss = 2.00 (17.1 examples/sec; 0.469 sec/batch; 43h:17m:31s remains)
INFO - root - 2017-12-07 10:50:50.700170: step 260, loss = 2.06, batch loss = 2.00 (16.1 examples/sec; 0.497 sec/batch; 45h:50m:09s remains)
INFO - root - 2017-12-07 10:50:55.297636: step 270, loss = 2.09, batch loss = 2.03 (17.3 examples/sec; 0.462 sec/batch; 42h:40m:51s remains)
INFO - root - 2017-12-07 10:50:59.845917: step 280, loss = 2.05, batch loss = 2.00 (17.2 examples/sec; 0.465 sec/batch; 42h:53m:55s remains)
INFO - root - 2017-12-07 10:51:04.433299: step 290, loss = 2.07, batch loss = 2.01 (18.0 examples/sec; 0.445 sec/batch; 41h:04m:30s remains)
INFO - root - 2017-12-07 10:51:09.218718: step 300, loss = 2.09, batch loss = 2.03 (18.1 examples/sec; 0.442 sec/batch; 40h:48m:52s remains)
2017-12-07 10:51:09.815548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2566266 -4.2389107 -4.2225094 -4.2049909 -4.1759992 -4.1486239 -4.1578836 -4.211731 -4.2639403 -4.297729 -4.3153028 -4.3234191 -4.3261614 -4.3239951 -4.3138103][-4.2333722 -4.2275543 -4.2281284 -4.2286906 -4.217823 -4.2023883 -4.2081985 -4.2429843 -4.2779779 -4.3020692 -4.3176537 -4.3236461 -4.3255191 -4.3238964 -4.3136659][-4.218533 -4.2220697 -4.2325835 -4.2421126 -4.2389522 -4.2255726 -4.2277913 -4.2490582 -4.272584 -4.2911215 -4.3064046 -4.3151708 -4.3194456 -4.3178897 -4.3086829][-4.21621 -4.2254519 -4.2390056 -4.2461042 -4.2395368 -4.220571 -4.214191 -4.2267756 -4.2473011 -4.2660418 -4.2819824 -4.29538 -4.3062973 -4.306932 -4.3020782][-4.2235818 -4.2346249 -4.2428508 -4.2385035 -4.2179141 -4.1892233 -4.169219 -4.1681318 -4.1924868 -4.222033 -4.2449837 -4.2664833 -4.2833657 -4.2878623 -4.2886724][-4.2367425 -4.2379818 -4.2322769 -4.2084055 -4.1687708 -4.1258945 -4.0895038 -4.0751147 -4.1130753 -4.1646223 -4.2030058 -4.2337074 -4.2531419 -4.2600594 -4.2667794][-4.2360005 -4.2245922 -4.2054086 -4.1642942 -4.1065383 -4.0465627 -3.9895539 -3.9633861 -4.0204868 -4.0973134 -4.1507173 -4.1897779 -4.2122507 -4.2263908 -4.2440314][-4.2387447 -4.2192569 -4.191936 -4.1427374 -4.0749464 -4.000185 -3.9262495 -3.8976951 -3.9635792 -4.0497131 -4.1117 -4.1545653 -4.1795273 -4.2019129 -4.2278976][-4.2478738 -4.2305408 -4.2057509 -4.1608958 -4.0981693 -4.0283632 -3.9637117 -3.9430547 -3.9941218 -4.0593 -4.1107121 -4.1470547 -4.16537 -4.1875381 -4.2134056][-4.2566471 -4.2502985 -4.236649 -4.2049026 -4.1573491 -4.1039848 -4.0605607 -4.0444312 -4.0657539 -4.0990515 -4.1325855 -4.1540976 -4.1601329 -4.1747975 -4.1969395][-4.270906 -4.2740197 -4.271153 -4.2533975 -4.2193651 -4.1822295 -4.1551118 -4.1401081 -4.1413198 -4.1534176 -4.1696997 -4.1742864 -4.1649251 -4.168766 -4.184792][-4.2800279 -4.2869353 -4.2890882 -4.2811637 -4.2616606 -4.240427 -4.2233338 -4.2110963 -4.20577 -4.2067766 -4.2121558 -4.2053976 -4.18327 -4.1742177 -4.1818085][-4.2707305 -4.2773714 -4.2814789 -4.2809258 -4.275454 -4.2689614 -4.262372 -4.2550087 -4.2467189 -4.2412877 -4.242075 -4.2364731 -4.2133303 -4.1962485 -4.1933503][-4.2424264 -4.2486143 -4.2562871 -4.2634754 -4.2700224 -4.2765408 -4.2792077 -4.2779374 -4.2745385 -4.2703462 -4.2718592 -4.2730322 -4.2533169 -4.2256708 -4.206368][-4.2126536 -4.2180314 -4.229609 -4.2429614 -4.2581692 -4.274127 -4.2841554 -4.2866058 -4.2856679 -4.2809753 -4.2819662 -4.2880831 -4.2718945 -4.2397151 -4.2112632]]...]
INFO - root - 2017-12-07 10:51:15.833954: step 310, loss = 2.08, batch loss = 2.03 (14.5 examples/sec; 0.552 sec/batch; 50h:58m:34s remains)
INFO - root - 2017-12-07 10:51:21.536305: step 320, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 61h:05m:57s remains)
INFO - root - 2017-12-07 10:51:28.309124: step 330, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 59h:49m:05s remains)
INFO - root - 2017-12-07 10:51:35.221071: step 340, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 67h:31m:30s remains)
INFO - root - 2017-12-07 10:51:42.100306: step 350, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 63h:49m:49s remains)
INFO - root - 2017-12-07 10:51:48.928096: step 360, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 58h:47m:59s remains)
INFO - root - 2017-12-07 10:51:55.652935: step 370, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 61h:16m:48s remains)
INFO - root - 2017-12-07 10:52:02.462012: step 380, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 62h:41m:55s remains)
INFO - root - 2017-12-07 10:52:09.235639: step 390, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 67h:37m:59s remains)
INFO - root - 2017-12-07 10:52:16.144587: step 400, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 62h:23m:49s remains)
2017-12-07 10:52:16.880037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872982 -4.2800636 -4.2732196 -4.2652655 -4.2567644 -4.2524905 -4.2546868 -4.262918 -4.2708974 -4.2703838 -4.2637305 -4.2534981 -4.2400932 -4.2275686 -4.2193556][-4.28929 -4.2865624 -4.2810836 -4.2716293 -4.2620893 -4.259253 -4.2682204 -4.2863607 -4.3029242 -4.310061 -4.3103786 -4.3018775 -4.2807875 -4.2524414 -4.2246804][-4.27478 -4.2733955 -4.2645774 -4.2500038 -4.2406116 -4.23645 -4.2440004 -4.2662663 -4.2932143 -4.3133683 -4.3279982 -4.3293366 -4.3110428 -4.2753348 -4.23038][-4.2620063 -4.2545452 -4.2352009 -4.2125134 -4.2013316 -4.1937041 -4.1948824 -4.2115622 -4.2423491 -4.274735 -4.3089347 -4.3266625 -4.3202558 -4.2878828 -4.2335644][-4.2584152 -4.238152 -4.2040153 -4.1694403 -4.1503057 -4.1348076 -4.1277237 -4.1380005 -4.1703744 -4.2126489 -4.2628284 -4.2978959 -4.306509 -4.282033 -4.2239542][-4.2568312 -4.2254806 -4.1754084 -4.1246271 -4.0921383 -4.0653782 -4.0461693 -4.0478764 -4.0837235 -4.140172 -4.2045255 -4.251534 -4.2734661 -4.2634068 -4.2140403][-4.2549014 -4.21938 -4.1600208 -4.0940762 -4.0406804 -3.989475 -3.9446602 -3.927578 -3.9675708 -4.0480938 -4.1311579 -4.193224 -4.2299538 -4.24051 -4.2161183][-4.258708 -4.2256632 -4.1686983 -4.0961576 -4.0240316 -3.9424593 -3.8563695 -3.8015227 -3.8357253 -3.9402084 -4.0463119 -4.1279187 -4.1847458 -4.2189012 -4.2258911][-4.267282 -4.242393 -4.2009864 -4.1415305 -4.0720019 -3.9858408 -3.8809733 -3.7918148 -3.7961826 -3.8831825 -3.9824221 -4.067811 -4.1394658 -4.1945186 -4.2297182][-4.2807851 -4.2655725 -4.2446294 -4.2104182 -4.1638246 -4.1039557 -4.0222974 -3.9399426 -3.9144096 -3.9463217 -3.9995513 -4.0581231 -4.122251 -4.1814241 -4.2305441][-4.2986016 -4.2891655 -4.2808008 -4.2679405 -4.2473607 -4.2197523 -4.1761427 -4.1231737 -4.0928564 -4.0892348 -4.101634 -4.1272173 -4.1694722 -4.2133746 -4.254458][-4.3148866 -4.3097944 -4.3042645 -4.2999306 -4.2954092 -4.2908349 -4.2783031 -4.2559276 -4.2367845 -4.2252803 -4.2217064 -4.2289176 -4.2489433 -4.2714958 -4.2938495][-4.3283596 -4.3278441 -4.3224893 -4.3187184 -4.3188028 -4.3227468 -4.3258724 -4.3205581 -4.3119283 -4.3049817 -4.3002429 -4.300446 -4.3073812 -4.3161511 -4.3253269][-4.3388958 -4.3409157 -4.33651 -4.3313303 -4.3304482 -4.3338594 -4.338407 -4.3381066 -4.3360481 -4.3344431 -4.3323956 -4.331605 -4.3334255 -4.3371005 -4.3403239][-4.3444948 -4.3464956 -4.3432713 -4.338686 -4.3370681 -4.3395696 -4.3425608 -4.3431749 -4.3426852 -4.3420181 -4.3414478 -4.3418584 -4.3432159 -4.3453946 -4.3460469]]...]
INFO - root - 2017-12-07 10:52:23.627221: step 410, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 63h:10m:29s remains)
INFO - root - 2017-12-07 10:52:30.310986: step 420, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 65h:16m:03s remains)
INFO - root - 2017-12-07 10:52:37.193545: step 430, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 63h:09m:35s remains)
INFO - root - 2017-12-07 10:52:43.947869: step 440, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:52m:58s remains)
INFO - root - 2017-12-07 10:52:50.674201: step 450, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 61h:05m:46s remains)
INFO - root - 2017-12-07 10:52:57.460516: step 460, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 65h:39m:50s remains)
INFO - root - 2017-12-07 10:53:04.309230: step 470, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 66h:15m:50s remains)
INFO - root - 2017-12-07 10:53:11.195411: step 480, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 62h:09m:22s remains)
INFO - root - 2017-12-07 10:53:18.026860: step 490, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:41m:45s remains)
INFO - root - 2017-12-07 10:53:24.874046: step 500, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 64h:06m:46s remains)
2017-12-07 10:53:25.619315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.250936 -4.2347054 -4.2250385 -4.2195153 -4.205792 -4.2007918 -4.205461 -4.2102017 -4.214602 -4.211504 -4.1986566 -4.1778274 -4.14948 -4.1206918 -4.0947223][-4.2226181 -4.2039175 -4.1923151 -4.1834092 -4.1648521 -4.1566386 -4.15979 -4.1710029 -4.1914668 -4.1998625 -4.1900287 -4.164773 -4.1269574 -4.0949883 -4.0705118][-4.193954 -4.1738014 -4.1557827 -4.1355128 -4.1097975 -4.1014504 -4.112318 -4.1405849 -4.1797457 -4.1918278 -4.1713839 -4.1350145 -4.0932174 -4.0624352 -4.0441904][-4.1751671 -4.1517534 -4.1230359 -4.0853252 -4.0519571 -4.0453181 -4.0676785 -4.1135054 -4.1655045 -4.1772652 -4.147862 -4.1062078 -4.0669065 -4.0390635 -4.0269828][-4.1547589 -4.1267262 -4.0877938 -4.0408278 -4.0066252 -4.0019875 -4.0270791 -4.0755019 -4.1317768 -4.1435094 -4.1128225 -4.0732923 -4.0433197 -4.027247 -4.0283847][-4.1333642 -4.1079397 -4.0683684 -4.0198507 -3.9809377 -3.963089 -3.9691505 -4.003706 -4.0557046 -4.0704823 -4.0443511 -4.0184 -4.0104227 -4.015214 -4.03298][-4.1186347 -4.1031513 -4.0737824 -4.0309095 -3.9811308 -3.9307411 -3.8902092 -3.8864398 -3.9302049 -3.9668512 -3.9628115 -3.9633911 -3.9822166 -4.0034151 -4.031054][-4.1015615 -4.0921974 -4.0764318 -4.045619 -3.9915314 -3.9151938 -3.822361 -3.7658935 -3.8068056 -3.88096 -3.9176831 -3.9511423 -3.9879982 -4.0120254 -4.0358829][-4.0894356 -4.0741749 -4.0591249 -4.0357289 -3.9917688 -3.9277115 -3.8335254 -3.7646766 -3.7995381 -3.87935 -3.9320161 -3.9845955 -4.0305476 -4.0504022 -4.0621986][-4.0819964 -4.0614877 -4.0473075 -4.0343843 -4.0118666 -3.9780135 -3.91723 -3.8733487 -3.8993134 -3.9489453 -3.9864178 -4.0385017 -4.0851216 -4.0996532 -4.10018][-4.0735683 -4.065383 -4.0664454 -4.0674281 -4.0603142 -4.0484767 -4.0175071 -3.9940989 -4.0078487 -4.0227652 -4.032167 -4.0666318 -4.1063113 -4.1204934 -4.116694][-4.0942135 -4.1017017 -4.1148338 -4.1170096 -4.1094937 -4.1020241 -4.0836086 -4.0668721 -4.070261 -4.0662713 -4.0570245 -4.07424 -4.104455 -4.1211891 -4.1218381][-4.1264882 -4.1369066 -4.1502509 -4.1488013 -4.1347723 -4.124958 -4.108726 -4.0921049 -4.0883274 -4.0803084 -4.07227 -4.0837908 -4.1052904 -4.123086 -4.1331143][-4.1513329 -4.1587963 -4.1680989 -4.1641655 -4.1486897 -4.1379395 -4.1250291 -4.1110587 -4.1092577 -4.1084819 -4.108459 -4.1146841 -4.1220107 -4.1340809 -4.148685][-4.1636806 -4.167686 -4.1784492 -4.1771541 -4.1657152 -4.156805 -4.1448727 -4.1361766 -4.1421661 -4.1502461 -4.1546421 -4.1512513 -4.1440272 -4.1481676 -4.1597795]]...]
INFO - root - 2017-12-07 10:53:32.350847: step 510, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:57m:59s remains)
INFO - root - 2017-12-07 10:53:39.025402: step 520, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 61h:53m:06s remains)
INFO - root - 2017-12-07 10:53:45.805085: step 530, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 65h:26m:30s remains)
INFO - root - 2017-12-07 10:53:52.609405: step 540, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 65h:24m:31s remains)
INFO - root - 2017-12-07 10:53:59.314803: step 550, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 59h:25m:44s remains)
INFO - root - 2017-12-07 10:54:05.945748: step 560, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 59h:08m:41s remains)
INFO - root - 2017-12-07 10:54:12.798097: step 570, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 64h:32m:04s remains)
INFO - root - 2017-12-07 10:54:19.690589: step 580, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 66h:15m:01s remains)
INFO - root - 2017-12-07 10:54:26.476707: step 590, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 61h:30m:45s remains)
INFO - root - 2017-12-07 10:54:33.213850: step 600, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 61h:22m:51s remains)
2017-12-07 10:54:33.953818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2006545 -4.1900406 -4.1818709 -4.16867 -4.1596241 -4.1572609 -4.1671262 -4.1976848 -4.2358913 -4.2620196 -4.2777729 -4.2836146 -4.2678556 -4.2414503 -4.2259903][-4.1909285 -4.1835446 -4.1818748 -4.1746173 -4.1667938 -4.166677 -4.1763763 -4.2052717 -4.2405849 -4.2621217 -4.2711873 -4.2712121 -4.2594252 -4.2417679 -4.2338533][-4.1748829 -4.1652288 -4.1715403 -4.1780825 -4.1799941 -4.1834865 -4.1919727 -4.2181726 -4.244628 -4.2557673 -4.2553487 -4.2501006 -4.2407947 -4.2289972 -4.2289805][-4.1622944 -4.1498656 -4.1591935 -4.1727219 -4.1745667 -4.1700754 -4.170742 -4.1922846 -4.2093754 -4.2149043 -4.2155714 -4.2169409 -4.2133245 -4.204577 -4.2079835][-4.1449142 -4.1397157 -4.1485982 -4.1629643 -4.1584644 -4.1381245 -4.1271138 -4.1415563 -4.1557684 -4.1675038 -4.1818838 -4.1929193 -4.1918368 -4.1805911 -4.1784015][-4.1234179 -4.1241679 -4.1274409 -4.1320086 -4.1163197 -4.0807409 -4.0588522 -4.0727754 -4.1013408 -4.1324854 -4.1614132 -4.1763706 -4.1705232 -4.150012 -4.1356378][-4.1203866 -4.1244683 -4.1151767 -4.0946145 -4.0539646 -3.9980462 -3.9660597 -3.9896157 -4.043406 -4.0951524 -4.1271887 -4.1357441 -4.1210318 -4.0899515 -4.0671391][-4.1235547 -4.1305408 -4.1156096 -4.0751143 -4.0127187 -3.9436119 -3.908535 -3.9372849 -4.0027323 -4.0521832 -4.0713787 -4.0691986 -4.0492277 -4.012888 -3.9919262][-4.127182 -4.1307583 -4.1197915 -4.0809312 -4.0168424 -3.9539731 -3.9235387 -3.9421463 -3.9897161 -4.0157661 -4.0170779 -4.0069342 -3.9899228 -3.96192 -3.9600086][-4.1546593 -4.1607585 -4.157145 -4.1293406 -4.0802507 -4.0339279 -4.0101967 -4.0133295 -4.0341821 -4.0352311 -4.0219092 -4.0057988 -3.9881198 -3.9661291 -3.979197][-4.1858473 -4.19375 -4.1959505 -4.1814604 -4.15121 -4.1263075 -4.1146479 -4.1108541 -4.1154003 -4.106349 -4.0864863 -4.0690937 -4.0489249 -4.0283418 -4.0409102][-4.215817 -4.2152262 -4.2137518 -4.2059984 -4.1920738 -4.1858506 -4.1887565 -4.18943 -4.1899066 -4.1803317 -4.1643581 -4.1483541 -4.1303968 -4.113327 -4.1198912][-4.22609 -4.2175355 -4.213222 -4.211916 -4.2113714 -4.2203326 -4.2354608 -4.2439709 -4.2461114 -4.2402172 -4.2293787 -4.2174535 -4.2049432 -4.1924429 -4.1938214][-4.2164855 -4.2061567 -4.198638 -4.2011051 -4.2114778 -4.2320304 -4.2541065 -4.2679691 -4.273746 -4.2712917 -4.2634983 -4.2544556 -4.2468348 -4.2377419 -4.2372661][-4.2104721 -4.2009974 -4.1891685 -4.1922231 -4.2090464 -4.2342072 -4.2574248 -4.2718649 -4.2774854 -4.27618 -4.2706614 -4.2641439 -4.2593923 -4.2519722 -4.2509542]]...]
INFO - root - 2017-12-07 10:54:40.748226: step 610, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.726 sec/batch; 66h:57m:56s remains)
INFO - root - 2017-12-07 10:54:47.390859: step 620, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 63h:51m:52s remains)
INFO - root - 2017-12-07 10:54:54.236321: step 630, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 59h:24m:53s remains)
INFO - root - 2017-12-07 10:55:01.112402: step 640, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 63h:46m:41s remains)
INFO - root - 2017-12-07 10:55:08.002296: step 650, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 65h:45m:03s remains)
INFO - root - 2017-12-07 10:55:14.835010: step 660, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 66h:07m:25s remains)
INFO - root - 2017-12-07 10:55:21.694941: step 670, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.687 sec/batch; 63h:18m:18s remains)
INFO - root - 2017-12-07 10:55:28.536799: step 680, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.624 sec/batch; 57h:32m:43s remains)
INFO - root - 2017-12-07 10:55:35.339300: step 690, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 63h:26m:04s remains)
INFO - root - 2017-12-07 10:55:42.127934: step 700, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 64h:36m:19s remains)
2017-12-07 10:55:42.858659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20241 -4.1967759 -4.1929445 -4.190228 -4.1887307 -4.1857867 -4.1813536 -4.1779561 -4.1776819 -4.179533 -4.18041 -4.1751428 -4.1622014 -4.1466179 -4.1374617][-4.2263074 -4.2193494 -4.2131844 -4.2075849 -4.2029791 -4.1964478 -4.1875525 -4.17945 -4.1754732 -4.1749768 -4.175 -4.1696377 -4.159513 -4.1461167 -4.1392064][-4.2510366 -4.2431 -4.2364082 -4.230279 -4.2258649 -4.2186284 -4.2081504 -4.1976085 -4.1913066 -4.1884422 -4.1864653 -4.1810956 -4.1743307 -4.1647315 -4.16044][-4.2461195 -4.237555 -4.2324524 -4.229178 -4.2283149 -4.2236195 -4.2146468 -4.2062521 -4.2034674 -4.2039914 -4.2029948 -4.1977162 -4.1914682 -4.1821423 -4.177979][-4.2145376 -4.201139 -4.1968789 -4.1996231 -4.204411 -4.2035871 -4.19698 -4.1933494 -4.2000384 -4.20991 -4.2155476 -4.214726 -4.2096791 -4.1989989 -4.192277][-4.17165 -4.151855 -4.1471715 -4.155365 -4.1644917 -4.163722 -4.1554346 -4.15516 -4.1723218 -4.1953235 -4.2134738 -4.2224851 -4.2222905 -4.2128 -4.2050276][-4.1427684 -4.1183615 -4.1129003 -4.1225562 -4.1290674 -4.1205831 -4.1044683 -4.1034489 -4.127809 -4.1629405 -4.1942477 -4.2154813 -4.2236276 -4.2194343 -4.2133441][-4.1371231 -4.1171584 -4.1141343 -4.1214662 -4.1204114 -4.1026254 -4.0762711 -4.069262 -4.0916576 -4.1291838 -4.16602 -4.1945329 -4.2093611 -4.2109179 -4.2095][-4.1483774 -4.1382942 -4.1405354 -4.1473155 -4.1431427 -4.1250052 -4.1000853 -4.0910592 -4.10495 -4.1339979 -4.1645269 -4.1876483 -4.2001057 -4.2016115 -4.2011724][-4.1709085 -4.1675382 -4.1706119 -4.1750641 -4.1699753 -4.1554861 -4.1390381 -4.1356087 -4.1466742 -4.1685762 -4.1909451 -4.2052288 -4.2090626 -4.20414 -4.1986141][-4.1975365 -4.1924767 -4.189702 -4.1865849 -4.1760716 -4.1611691 -4.1518321 -4.1555557 -4.1684604 -4.1888733 -4.2094984 -4.2205424 -4.2198911 -4.2095423 -4.1985993][-4.2215204 -4.2123027 -4.2049632 -4.1974573 -4.183898 -4.1672053 -4.1591291 -4.1650872 -4.1785502 -4.1967072 -4.2143078 -4.2237468 -4.2216611 -4.2097459 -4.1976271][-4.2339387 -4.2238545 -4.2168522 -4.2119122 -4.2015104 -4.1867595 -4.1779113 -4.1805611 -4.1890879 -4.20132 -4.2136259 -4.2198319 -4.2173381 -4.2064795 -4.197145][-4.2331181 -4.2242675 -4.2202287 -4.2191057 -4.2149553 -4.2073312 -4.2017083 -4.2015519 -4.2045164 -4.2097359 -4.2154341 -4.2176981 -4.2131467 -4.2031379 -4.1963429][-4.225244 -4.2174683 -4.214407 -4.21441 -4.2135372 -4.2119694 -4.211113 -4.2113295 -4.2116008 -4.2122965 -4.2134638 -4.2140374 -4.2095776 -4.2011671 -4.1962643]]...]
INFO - root - 2017-12-07 10:55:49.574465: step 710, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:25m:26s remains)
INFO - root - 2017-12-07 10:55:56.081768: step 720, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 62h:13m:17s remains)
INFO - root - 2017-12-07 10:56:02.970970: step 730, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 66h:24m:29s remains)
INFO - root - 2017-12-07 10:56:09.769303: step 740, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 62h:47m:52s remains)
INFO - root - 2017-12-07 10:56:16.576798: step 750, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 61h:46m:50s remains)
INFO - root - 2017-12-07 10:56:23.452261: step 760, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 61h:58m:04s remains)
INFO - root - 2017-12-07 10:56:30.401217: step 770, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 64h:35m:41s remains)
INFO - root - 2017-12-07 10:56:37.342061: step 780, loss = 2.09, batch loss = 2.04 (11.1 examples/sec; 0.719 sec/batch; 66h:15m:25s remains)
INFO - root - 2017-12-07 10:56:44.102332: step 790, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 67h:19m:51s remains)
INFO - root - 2017-12-07 10:56:50.837910: step 800, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 59h:08m:35s remains)
2017-12-07 10:56:51.542746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1272 -4.1451769 -4.1811686 -4.1912422 -4.1832647 -4.1615424 -4.1409597 -4.1405392 -4.1587353 -4.1765079 -4.1836362 -4.1790113 -4.1763434 -4.1795754 -4.1851749][-4.1609373 -4.1850204 -4.2244663 -4.2292361 -4.2166648 -4.1929665 -4.1618271 -4.145112 -4.1560087 -4.1741943 -4.1792507 -4.1730304 -4.1708369 -4.1732292 -4.1777787][-4.1919951 -4.2166028 -4.2544527 -4.25919 -4.2521315 -4.2363033 -4.2057109 -4.1818876 -4.1830621 -4.1996365 -4.2047458 -4.1947169 -4.1858377 -4.1818619 -4.188169][-4.2206345 -4.2411513 -4.272872 -4.2806988 -4.2778478 -4.2649159 -4.2407413 -4.2127552 -4.2031884 -4.216011 -4.2216721 -4.2093182 -4.1955609 -4.1880817 -4.1995678][-4.247241 -4.25638 -4.2724466 -4.2769289 -4.2749143 -4.2626567 -4.2414207 -4.2076492 -4.189136 -4.1973143 -4.2023878 -4.1878862 -4.1765742 -4.1701646 -4.1858029][-4.2575693 -4.2495956 -4.2421618 -4.235702 -4.2306404 -4.2190685 -4.1998386 -4.1568565 -4.1261382 -4.1397 -4.1515737 -4.1417074 -4.1346235 -4.1329422 -4.1508222][-4.2416506 -4.218194 -4.1866388 -4.1633811 -4.1521468 -4.139122 -4.1185751 -4.0564675 -4.0119934 -4.0449462 -4.0785379 -4.0814719 -4.08171 -4.0905585 -4.1166048][-4.2041483 -4.1737061 -4.1314039 -4.1052704 -4.09881 -4.0909882 -4.0667772 -3.9888942 -3.9369349 -3.9909816 -4.0423861 -4.060122 -4.0720329 -4.0861845 -4.1140318][-4.172801 -4.1555839 -4.1265397 -4.1179986 -4.1277304 -4.1296949 -4.1093225 -4.0478272 -4.0129843 -4.0500717 -4.0864964 -4.1013765 -4.1131186 -4.118959 -4.1352797][-4.1685753 -4.175 -4.1720943 -4.1852055 -4.2044864 -4.2115121 -4.1974664 -4.1575661 -4.1351461 -4.1449857 -4.1596 -4.1610694 -4.1620646 -4.1571083 -4.1624651][-4.1897807 -4.214097 -4.2287512 -4.2508082 -4.27145 -4.2803736 -4.2706833 -4.2407184 -4.221004 -4.2187285 -4.21793 -4.2062912 -4.1929321 -4.1788135 -4.1785355][-4.209547 -4.2442122 -4.2648578 -4.2837481 -4.3005824 -4.3110156 -4.3020639 -4.2765894 -4.2593994 -4.2549734 -4.245657 -4.223752 -4.1970491 -4.1797094 -4.1835089][-4.2093859 -4.2478571 -4.264163 -4.2717543 -4.2839489 -4.2973862 -4.2908444 -4.269784 -4.2561326 -4.2531862 -4.2452784 -4.2229786 -4.1954036 -4.182353 -4.1925788][-4.1979842 -4.2317734 -4.2383223 -4.2343836 -4.2444687 -4.262619 -4.2625937 -4.2455077 -4.2346153 -4.23382 -4.2304153 -4.21674 -4.2016463 -4.1977568 -4.2070456][-4.2036328 -4.2288213 -4.2284913 -4.21665 -4.22214 -4.2393694 -4.2425442 -4.2296481 -4.2217 -4.2215466 -4.2196112 -4.2137933 -4.2125969 -4.2164187 -4.2232718]]...]
INFO - root - 2017-12-07 10:56:58.276312: step 810, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 66h:16m:33s remains)
INFO - root - 2017-12-07 10:57:05.013269: step 820, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 66h:52m:10s remains)
INFO - root - 2017-12-07 10:57:11.833843: step 830, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 59h:15m:26s remains)
INFO - root - 2017-12-07 10:57:18.637916: step 840, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 64h:09m:10s remains)
INFO - root - 2017-12-07 10:57:25.531283: step 850, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 66h:50m:32s remains)
INFO - root - 2017-12-07 10:57:32.281228: step 860, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 58h:51m:07s remains)
INFO - root - 2017-12-07 10:57:38.921827: step 870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 62h:32m:23s remains)
INFO - root - 2017-12-07 10:57:45.682925: step 880, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 63h:09m:48s remains)
INFO - root - 2017-12-07 10:57:52.414521: step 890, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 61h:57m:18s remains)
INFO - root - 2017-12-07 10:57:59.205471: step 900, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 63h:30m:55s remains)
2017-12-07 10:57:59.853670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29938 -4.2888203 -4.2819519 -4.2818942 -4.2863755 -4.293149 -4.300005 -4.303124 -4.3037395 -4.3033915 -4.302947 -4.3043709 -4.3081789 -4.312366 -4.3158646][-4.2901845 -4.2704244 -4.2552562 -4.2522836 -4.2559619 -4.2663827 -4.2822042 -4.2952175 -4.3029823 -4.306076 -4.3056283 -4.3054466 -4.307405 -4.3100567 -4.3128614][-4.2816448 -4.2538419 -4.2286873 -4.2171783 -4.2157192 -4.2282181 -4.2513895 -4.2727332 -4.2889934 -4.30049 -4.3049426 -4.307374 -4.3107219 -4.3112588 -4.3110371][-4.2715759 -4.2378464 -4.2021093 -4.17979 -4.1715584 -4.1831713 -4.2117229 -4.2389846 -4.2617807 -4.2825732 -4.2979555 -4.3077292 -4.3162489 -4.3171806 -4.3128057][-4.2591081 -4.2209744 -4.172411 -4.1347966 -4.1173792 -4.1262751 -4.1599889 -4.1963482 -4.2290392 -4.2565722 -4.277544 -4.2975106 -4.3161159 -4.3224239 -4.3168039][-4.2465563 -4.2030363 -4.14158 -4.087081 -4.0577817 -4.0559039 -4.0861244 -4.130918 -4.18082 -4.2209892 -4.2476583 -4.2733 -4.2997308 -4.3156533 -4.3160286][-4.2337561 -4.1846428 -4.1115856 -4.0410395 -3.9940193 -3.9678643 -3.9780612 -4.0224137 -4.0958228 -4.163568 -4.2047119 -4.2372088 -4.2687325 -4.2928352 -4.3036723][-4.2287273 -4.1779351 -4.1038918 -4.0269704 -3.9632211 -3.9021933 -3.8646808 -3.8833704 -3.9765551 -4.0793276 -4.1440897 -4.1898994 -4.2307644 -4.2614555 -4.2820539][-4.23556 -4.1922827 -4.1303544 -4.061924 -3.9936306 -3.9081578 -3.8235788 -3.7845771 -3.8603857 -3.9833176 -4.0728922 -4.1367283 -4.1961169 -4.2374072 -4.2657156][-4.246923 -4.2162371 -4.1730061 -4.1241584 -4.0674319 -3.987056 -3.8937418 -3.8123827 -3.8296924 -3.9255896 -4.0186648 -4.0942268 -4.1697793 -4.2247887 -4.26097][-4.2585034 -4.2406039 -4.2175593 -4.1882253 -4.1473637 -4.086997 -4.0161772 -3.9426036 -3.922158 -3.9619949 -4.024128 -4.0873165 -4.1635451 -4.22628 -4.2657719][-4.2712297 -4.2622037 -4.2537866 -4.2404356 -4.2142649 -4.1750875 -4.1360445 -4.0948215 -4.0718379 -4.0799403 -4.1041079 -4.1362782 -4.1927218 -4.2477121 -4.2816963][-4.284184 -4.2779632 -4.27486 -4.2707272 -4.256814 -4.2356277 -4.2208714 -4.2063394 -4.1962886 -4.199614 -4.2056589 -4.2134614 -4.2430587 -4.280539 -4.3015842][-4.2991447 -4.2943444 -4.292129 -4.2910967 -4.2853255 -4.2769403 -4.2732363 -4.2700281 -4.2672029 -4.2710795 -4.273747 -4.2748227 -4.2870712 -4.30677 -4.3169904][-4.3119226 -4.309123 -4.3077164 -4.3063602 -4.3027735 -4.2986054 -4.2978873 -4.2980838 -4.297708 -4.3011827 -4.30377 -4.3053861 -4.3103323 -4.3186221 -4.3229451]]...]
INFO - root - 2017-12-07 10:58:06.515766: step 910, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 59h:11m:10s remains)
INFO - root - 2017-12-07 10:58:13.181373: step 920, loss = 2.09, batch loss = 2.04 (11.3 examples/sec; 0.706 sec/batch; 65h:03m:01s remains)
INFO - root - 2017-12-07 10:58:19.951139: step 930, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 63h:50m:29s remains)
INFO - root - 2017-12-07 10:58:26.771772: step 940, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.699 sec/batch; 64h:20m:10s remains)
INFO - root - 2017-12-07 10:58:33.520507: step 950, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.630 sec/batch; 58h:03m:45s remains)
INFO - root - 2017-12-07 10:58:40.334476: step 960, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 60h:55m:11s remains)
INFO - root - 2017-12-07 10:58:47.119547: step 970, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 64h:40m:52s remains)
INFO - root - 2017-12-07 10:58:53.882118: step 980, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 62h:57m:24s remains)
INFO - root - 2017-12-07 10:59:00.663035: step 990, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 64h:05m:12s remains)
INFO - root - 2017-12-07 10:59:07.367192: step 1000, loss = 2.03, batch loss = 1.97 (12.7 examples/sec; 0.627 sec/batch; 57h:46m:42s remains)
2017-12-07 10:59:08.140998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3071527 -4.3007379 -4.2922468 -4.2881021 -4.2928786 -4.3030477 -4.3162537 -4.3255296 -4.3260059 -4.320241 -4.3083782 -4.2978792 -4.294806 -4.3005476 -4.3120146][-4.27697 -4.267148 -4.2552929 -4.247582 -4.2527194 -4.2689347 -4.2867808 -4.2960157 -4.2910542 -4.2784381 -4.2582688 -4.2430606 -4.2421174 -4.2537708 -4.2740526][-4.2477207 -4.235086 -4.2230887 -4.2132521 -4.2165484 -4.2342744 -4.25177 -4.259234 -4.250535 -4.2354717 -4.2098007 -4.1923852 -4.1934485 -4.2108359 -4.2360229][-4.2344141 -4.2194319 -4.2057033 -4.1915751 -4.1885505 -4.1991553 -4.2085795 -4.2111745 -4.205965 -4.20116 -4.1805882 -4.1648083 -4.1678076 -4.1870646 -4.2110257][-4.2297721 -4.2131996 -4.1966138 -4.1796045 -4.1676383 -4.1607 -4.1483431 -4.1392632 -4.1446409 -4.1620626 -4.1580563 -4.1492453 -4.1556273 -4.1739879 -4.1917796][-4.2146344 -4.1955004 -4.1748252 -4.1579018 -4.1386533 -4.1072431 -4.06318 -4.0392823 -4.0610337 -4.1077185 -4.1253519 -4.1284175 -4.1456833 -4.1712 -4.18633][-4.2006726 -4.1801357 -4.15624 -4.1349611 -4.1056528 -4.0511632 -3.9708147 -3.923347 -3.9646363 -4.0435281 -4.0840917 -4.1010714 -4.1305165 -4.1671605 -4.1875105][-4.1931362 -4.171423 -4.1481042 -4.127512 -4.0896153 -4.0202041 -3.919126 -3.8525188 -3.901824 -3.9940805 -4.0418973 -4.0613976 -4.0969133 -4.1401205 -4.1736746][-4.1878819 -4.16596 -4.1399345 -4.1177039 -4.0819607 -4.0220828 -3.9433737 -3.8922453 -3.9290376 -3.9901857 -4.0183883 -4.027638 -4.0589123 -4.100637 -4.1430006][-4.1761041 -4.1563406 -4.1314621 -4.1097856 -4.0843492 -4.0475054 -4.0020289 -3.9702389 -3.9919856 -4.0178466 -4.0219288 -4.0162082 -4.0408926 -4.0787411 -4.12178][-4.1803646 -4.1613889 -4.142395 -4.1275072 -4.1127563 -4.097271 -4.0800538 -4.0638995 -4.0749254 -4.0773611 -4.0587864 -4.0357957 -4.0517974 -4.0849023 -4.1286907][-4.2175255 -4.2004113 -4.1864557 -4.178504 -4.1725779 -4.1709514 -4.1710048 -4.1668458 -4.1755013 -4.1664529 -4.133677 -4.1010532 -4.1119103 -4.13858 -4.1759982][-4.2736807 -4.2589917 -4.2460585 -4.2386074 -4.2336192 -4.2379675 -4.2474151 -4.251792 -4.2591772 -4.2482982 -4.2148814 -4.1832347 -4.1894116 -4.2105384 -4.2385931][-4.3183846 -4.3059907 -4.2944818 -4.2892394 -4.2872071 -4.2940454 -4.3035479 -4.3077073 -4.3105488 -4.3009038 -4.2764034 -4.2511411 -4.2524085 -4.2676358 -4.2857761][-4.34051 -4.3319473 -4.3229032 -4.3200922 -4.3209114 -4.3258972 -4.3311057 -4.3333344 -4.3343673 -4.3292818 -4.3156381 -4.2995987 -4.2992406 -4.3097849 -4.321167]]...]
INFO - root - 2017-12-07 10:59:14.889922: step 1010, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 64h:25m:52s remains)
INFO - root - 2017-12-07 10:59:21.504689: step 1020, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 64h:17m:07s remains)
INFO - root - 2017-12-07 10:59:28.245134: step 1030, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 60h:40m:32s remains)
INFO - root - 2017-12-07 10:59:35.079549: step 1040, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.653 sec/batch; 60h:05m:04s remains)
INFO - root - 2017-12-07 10:59:41.884096: step 1050, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 64h:36m:04s remains)
INFO - root - 2017-12-07 10:59:48.626869: step 1060, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 60h:19m:48s remains)
INFO - root - 2017-12-07 10:59:55.432364: step 1070, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 61h:08m:17s remains)
INFO - root - 2017-12-07 11:00:02.242449: step 1080, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 59h:25m:34s remains)
INFO - root - 2017-12-07 11:00:09.225025: step 1090, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 65h:06m:35s remains)
INFO - root - 2017-12-07 11:00:16.012004: step 1100, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 63h:20m:47s remains)
2017-12-07 11:00:16.698353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2054591 -4.1616206 -4.1119723 -4.0786624 -4.0650349 -4.0874839 -4.1456256 -4.1969514 -4.2281132 -4.2443285 -4.2539673 -4.257297 -4.2714067 -4.2843118 -4.290309][-4.2082353 -4.1647973 -4.1020665 -4.0377398 -3.9911647 -4.0055246 -4.0819407 -4.154273 -4.1996765 -4.2255807 -4.2422485 -4.2514143 -4.2676663 -4.282639 -4.2929087][-4.2253704 -4.190928 -4.136066 -4.0667124 -4.0031652 -3.9997613 -4.0631728 -4.133585 -4.1801291 -4.2075758 -4.2270293 -4.2436953 -4.265141 -4.2831154 -4.294745][-4.2342911 -4.2083411 -4.1705437 -4.1187029 -4.0620966 -4.0472479 -4.0820408 -4.1322751 -4.1709671 -4.1958656 -4.2191458 -4.2397065 -4.2627783 -4.2836418 -4.2964067][-4.2198067 -4.1953077 -4.1626091 -4.1233816 -4.0780911 -4.05595 -4.0654731 -4.094358 -4.1332169 -4.1665158 -4.2005763 -4.2268319 -4.252634 -4.2792649 -4.2955384][-4.1927857 -4.1704082 -4.1414862 -4.10749 -4.0658197 -4.0327988 -4.0196977 -4.0272651 -4.0666528 -4.1147223 -4.1625519 -4.1988473 -4.2311587 -4.2647481 -4.2885919][-4.17783 -4.1572032 -4.1335845 -4.1066914 -4.0677342 -4.02618 -3.9955132 -3.982512 -4.0144987 -4.0737634 -4.1282229 -4.1698914 -4.2068567 -4.2461972 -4.2787786][-4.1964636 -4.1744676 -4.1522865 -4.1287131 -4.0955749 -4.0572386 -4.0276132 -4.0084734 -4.0247788 -4.0739193 -4.1217928 -4.1621389 -4.2011676 -4.2429123 -4.2786818][-4.2209954 -4.2021976 -4.1797428 -4.1539226 -4.1242337 -4.0957046 -4.0761619 -4.0602717 -4.0630355 -4.0936384 -4.1326795 -4.1694627 -4.2087955 -4.2515883 -4.2874169][-4.225492 -4.2164321 -4.1989903 -4.1728926 -4.1421232 -4.1133733 -4.0972018 -4.0843778 -4.0795846 -4.096292 -4.132411 -4.17129 -4.211091 -4.2557049 -4.2921319][-4.2134194 -4.21593 -4.209137 -4.1913323 -4.1643939 -4.1344256 -4.1135435 -4.0975442 -4.0853643 -4.0925627 -4.126308 -4.1674223 -4.2079492 -4.2525024 -4.2888255][-4.2127872 -4.2220984 -4.2252135 -4.2183557 -4.2003903 -4.1735706 -4.148406 -4.1242151 -4.1024194 -4.0996737 -4.1270571 -4.1664114 -4.2061048 -4.246356 -4.2794938][-4.2295837 -4.2361503 -4.24015 -4.2404532 -4.2352085 -4.2198362 -4.1983695 -4.1690426 -4.1404934 -4.128962 -4.1457105 -4.1773329 -4.2118378 -4.2462363 -4.2742152][-4.2397556 -4.2407141 -4.2423134 -4.2466049 -4.25068 -4.24739 -4.2355232 -4.2066131 -4.1747389 -4.1564369 -4.1634936 -4.1875124 -4.2162318 -4.2447534 -4.2690954][-4.23477 -4.2314267 -4.2317004 -4.2367234 -4.2437143 -4.2458873 -4.2396126 -4.2131615 -4.1831336 -4.1643515 -4.1672335 -4.1851087 -4.2118406 -4.2374291 -4.2596064]]...]
INFO - root - 2017-12-07 11:00:23.387933: step 1110, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 62h:32m:24s remains)
INFO - root - 2017-12-07 11:00:29.966368: step 1120, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 60h:10m:43s remains)
INFO - root - 2017-12-07 11:00:36.904664: step 1130, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 63h:15m:31s remains)
INFO - root - 2017-12-07 11:00:43.691487: step 1140, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 63h:41m:48s remains)
INFO - root - 2017-12-07 11:00:50.472403: step 1150, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.681 sec/batch; 62h:38m:55s remains)
INFO - root - 2017-12-07 11:00:57.219825: step 1160, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:34m:17s remains)
INFO - root - 2017-12-07 11:01:03.961986: step 1170, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 61h:42m:16s remains)
INFO - root - 2017-12-07 11:01:10.645418: step 1180, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 64h:47m:48s remains)
INFO - root - 2017-12-07 11:01:17.440183: step 1190, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 58h:06m:41s remains)
INFO - root - 2017-12-07 11:01:24.126141: step 1200, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 58h:20m:51s remains)
2017-12-07 11:01:24.804933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26148 -4.257956 -4.2682171 -4.2815332 -4.2842841 -4.2811112 -4.2822905 -4.2906246 -4.3019557 -4.3055387 -4.2986736 -4.2886419 -4.2818065 -4.2745337 -4.26937][-4.22947 -4.2185531 -4.2351689 -4.2636952 -4.27922 -4.2833834 -4.289423 -4.3029342 -4.3195515 -4.3261805 -4.3158026 -4.295197 -4.2776585 -4.2633452 -4.2538757][-4.178688 -4.167625 -4.1908145 -4.2346478 -4.2599316 -4.2671318 -4.2717729 -4.2907124 -4.3133121 -4.3258672 -4.3208685 -4.3001328 -4.2781444 -4.25697 -4.238688][-4.1562552 -4.1531239 -4.1812739 -4.2283583 -4.2475252 -4.242352 -4.2389665 -4.2582974 -4.2833753 -4.3022513 -4.308567 -4.299747 -4.2840595 -4.2607775 -4.2368298][-4.1604552 -4.172297 -4.2036076 -4.2402134 -4.2401943 -4.2087927 -4.1795797 -4.1911225 -4.2241569 -4.2547016 -4.2792025 -4.2898746 -4.2879 -4.2694912 -4.2456555][-4.1589808 -4.1777 -4.2062078 -4.2247272 -4.1995783 -4.1363468 -4.0668111 -4.0542197 -4.1128345 -4.1796074 -4.2299895 -4.2627873 -4.2812052 -4.2753015 -4.2542443][-4.1614647 -4.1803226 -4.1998663 -4.1946239 -4.1427755 -4.0482497 -3.9239058 -3.8699906 -3.967865 -4.091526 -4.1798406 -4.2363119 -4.2692065 -4.2743192 -4.257957][-4.1654768 -4.1778765 -4.1908484 -4.1718483 -4.1086707 -4.0047569 -3.8641834 -3.7890613 -3.9107296 -4.0625043 -4.1647053 -4.2295203 -4.267271 -4.2796364 -4.2681308][-4.1701193 -4.1793094 -4.1905169 -4.1737089 -4.1243 -4.0509477 -3.9628098 -3.9209816 -4.0023546 -4.1115417 -4.1882081 -4.2416091 -4.2748642 -4.2899337 -4.285501][-4.195085 -4.2008343 -4.2109542 -4.2069292 -4.1836066 -4.1451859 -4.1081777 -4.0935359 -4.1326528 -4.1882043 -4.2282066 -4.25772 -4.2793632 -4.2926149 -4.2962112][-4.233964 -4.2334824 -4.2404022 -4.2429495 -4.2321844 -4.2149444 -4.209209 -4.2153831 -4.2320061 -4.2513571 -4.2624993 -4.2696352 -4.2760525 -4.2815437 -4.29292][-4.2761006 -4.2732458 -4.27273 -4.2693529 -4.2601528 -4.2549543 -4.2648745 -4.2751961 -4.27602 -4.273797 -4.26928 -4.2663727 -4.2643218 -4.2640166 -4.274375][-4.2982721 -4.2907434 -4.28501 -4.2775149 -4.2672863 -4.2652068 -4.2784505 -4.2834716 -4.2715993 -4.256484 -4.2460976 -4.2429476 -4.2430272 -4.245739 -4.2535291][-4.2945037 -4.2866917 -4.28198 -4.2757936 -4.2683392 -4.2674141 -4.2754579 -4.2763038 -4.2590423 -4.2401805 -4.2304091 -4.2271175 -4.2293386 -4.239748 -4.2504354][-4.2835197 -4.2775807 -4.2762728 -4.2721643 -4.2703643 -4.2737103 -4.2772779 -4.2743692 -4.25832 -4.241889 -4.2337022 -4.2310357 -4.2354741 -4.2519126 -4.2691312]]...]
INFO - root - 2017-12-07 11:01:31.513779: step 1210, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 61h:32m:30s remains)
INFO - root - 2017-12-07 11:01:38.195323: step 1220, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 61h:13m:00s remains)
INFO - root - 2017-12-07 11:01:45.024558: step 1230, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 61h:12m:50s remains)
INFO - root - 2017-12-07 11:01:51.840824: step 1240, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 65h:52m:48s remains)
INFO - root - 2017-12-07 11:01:58.594182: step 1250, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 65h:56m:51s remains)
INFO - root - 2017-12-07 11:02:05.319174: step 1260, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 62h:10m:28s remains)
INFO - root - 2017-12-07 11:02:12.110840: step 1270, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 59h:42m:21s remains)
INFO - root - 2017-12-07 11:02:18.858733: step 1280, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 59h:01m:41s remains)
INFO - root - 2017-12-07 11:02:25.644537: step 1290, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 64h:41m:23s remains)
INFO - root - 2017-12-07 11:02:32.360376: step 1300, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 63h:27m:35s remains)
2017-12-07 11:02:33.093926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1891332 -4.1949873 -4.1830111 -4.1758327 -4.1666212 -4.1465755 -4.1511965 -4.1715231 -4.1848454 -4.2021651 -4.215909 -4.209403 -4.1983318 -4.17099 -4.1322913][-4.1921206 -4.2106485 -4.2064552 -4.1989536 -4.1877694 -4.1586137 -4.14252 -4.1560659 -4.1769853 -4.2001104 -4.2209678 -4.2190208 -4.2095966 -4.1885428 -4.1515493][-4.1907525 -4.2139382 -4.2157288 -4.2085195 -4.200346 -4.1752739 -4.1469173 -4.1522408 -4.1784244 -4.2034154 -4.2290335 -4.2337079 -4.2284617 -4.216258 -4.1852274][-4.1935344 -4.2155 -4.2165837 -4.2020078 -4.1914587 -4.1712909 -4.1364341 -4.1340628 -4.1651473 -4.1957097 -4.2255125 -4.2418303 -4.2425508 -4.23397 -4.2063684][-4.1865344 -4.2001953 -4.1943421 -4.1691856 -4.1521597 -4.1358256 -4.1009989 -4.0878735 -4.1192818 -4.1594825 -4.1974211 -4.2242913 -4.2293534 -4.2183051 -4.1951122][-4.1393743 -4.1325545 -4.114532 -4.0811663 -4.0584607 -4.0445757 -4.0114913 -3.9846935 -4.0115223 -4.0671077 -4.1182637 -4.161478 -4.1778975 -4.1702971 -4.1535649][-4.0800772 -4.04568 -4.0144629 -3.9796677 -3.9532881 -3.9385695 -3.8997221 -3.8455791 -3.8640509 -3.9428535 -4.0101509 -4.0659819 -4.0975471 -4.1001215 -4.0905924][-4.0449352 -3.9949152 -3.9567747 -3.9216897 -3.8884354 -3.8615723 -3.8108215 -3.7322891 -3.7366319 -3.8332677 -3.9125295 -3.9638102 -3.9958415 -4.0075493 -4.0101709][-4.0512776 -4.0057421 -3.9709032 -3.9420264 -3.9071088 -3.8736832 -3.8270855 -3.7612574 -3.754282 -3.8292837 -3.8933792 -3.9265084 -3.9487424 -3.9590554 -3.962877][-4.0745893 -4.041481 -4.0175071 -3.9982138 -3.9718592 -3.9449534 -3.9121912 -3.8711236 -3.862062 -3.9015446 -3.9387178 -3.95199 -3.960578 -3.9622037 -3.9561663][-4.1100912 -4.0899744 -4.07463 -4.0627832 -4.0486336 -4.035243 -4.0170126 -3.9927504 -3.9797907 -3.9902322 -4.00067 -3.9984756 -3.9996493 -3.9996173 -3.9865146][-4.1399779 -4.1243229 -4.1123791 -4.10526 -4.1005969 -4.0997329 -4.0928745 -4.0782027 -4.0639973 -4.0585337 -4.0543585 -4.0471816 -4.0475631 -4.0498857 -4.0370855][-4.1604805 -4.1435304 -4.1279006 -4.1186624 -4.1177516 -4.1240654 -4.1260152 -4.1176953 -4.1059041 -4.0969038 -4.0940719 -4.0913706 -4.09394 -4.0998178 -4.0932717][-4.1807775 -4.1612988 -4.1397457 -4.1251369 -4.12163 -4.1294813 -4.1370239 -4.1328368 -4.1238475 -4.1164312 -4.1170282 -4.1191854 -4.123323 -4.1310897 -4.131475][-4.2032065 -4.1846266 -4.1609259 -4.14085 -4.1319957 -4.1377344 -4.1459265 -4.1429763 -4.1357012 -4.1297836 -4.1290259 -4.1295857 -4.1333976 -4.1426935 -4.1480913]]...]
INFO - root - 2017-12-07 11:02:39.847646: step 1310, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 60h:48m:50s remains)
INFO - root - 2017-12-07 11:02:46.435030: step 1320, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 63h:22m:32s remains)
INFO - root - 2017-12-07 11:02:53.139486: step 1330, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.701 sec/batch; 64h:28m:57s remains)
INFO - root - 2017-12-07 11:02:59.925478: step 1340, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 63h:19m:40s remains)
INFO - root - 2017-12-07 11:03:06.699054: step 1350, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 62h:49m:01s remains)
INFO - root - 2017-12-07 11:03:13.491856: step 1360, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 60h:38m:01s remains)
INFO - root - 2017-12-07 11:03:20.271591: step 1370, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 64h:33m:29s remains)
INFO - root - 2017-12-07 11:03:27.081949: step 1380, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 63h:52m:56s remains)
INFO - root - 2017-12-07 11:03:33.866115: step 1390, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 63h:54m:33s remains)
INFO - root - 2017-12-07 11:03:40.683066: step 1400, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 57h:48m:28s remains)
2017-12-07 11:03:41.401283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2112556 -4.2157574 -4.2139993 -4.206892 -4.2023005 -4.2008429 -4.1951342 -4.1823153 -4.1707654 -4.1677489 -4.1719375 -4.173665 -4.1760783 -4.18212 -4.1945229][-4.2047229 -4.2112832 -4.2129607 -4.2064309 -4.2029953 -4.2040091 -4.1983685 -4.1866131 -4.1788645 -4.178688 -4.1845121 -4.1846838 -4.184618 -4.188128 -4.1956487][-4.197001 -4.2027073 -4.2062964 -4.2006531 -4.1954489 -4.1966338 -4.1926374 -4.1838865 -4.1782417 -4.1812682 -4.1920018 -4.1950312 -4.1950841 -4.1967683 -4.2008657][-4.16956 -4.1742897 -4.1800408 -4.1764488 -4.1703634 -4.1708589 -4.1701131 -4.1660976 -4.1637673 -4.1689525 -4.1834946 -4.1913319 -4.1913085 -4.1927972 -4.1961946][-4.1315203 -4.1328068 -4.1397777 -4.1361456 -4.1259294 -4.1207194 -4.1185603 -4.1198506 -4.1249623 -4.134294 -4.1546412 -4.1700354 -4.1743979 -4.175528 -4.1785569][-4.1002936 -4.0917397 -4.0906196 -4.079514 -4.0617652 -4.0503387 -4.0510464 -4.0624723 -4.0833993 -4.1032505 -4.1283236 -4.1485944 -4.1565866 -4.1572585 -4.1575389][-4.1086588 -4.0821133 -4.0627394 -4.03428 -3.9980242 -3.9722567 -3.9734724 -3.9970233 -4.0414963 -4.0807571 -4.1160078 -4.1442947 -4.1591396 -4.1610088 -4.1577287][-4.1450944 -4.1056151 -4.0731716 -4.0326047 -3.9799502 -3.9395356 -3.9375944 -3.9661279 -4.0269289 -4.084806 -4.1327295 -4.17064 -4.1925578 -4.1980815 -4.1938539][-4.1891046 -4.1513624 -4.1211481 -4.0846105 -4.035111 -3.9951358 -3.989279 -4.012279 -4.0661407 -4.118288 -4.16304 -4.1966662 -4.2200656 -4.2314291 -4.2316623][-4.2268558 -4.1946778 -4.1701694 -4.1422338 -4.1059985 -4.0797262 -4.0796762 -4.0991392 -4.1357322 -4.1667085 -4.1927567 -4.2071819 -4.21789 -4.226831 -4.231461][-4.252955 -4.225358 -4.2029428 -4.1792374 -4.151813 -4.1336055 -4.133265 -4.146718 -4.1681461 -4.18248 -4.1950531 -4.1959124 -4.1947742 -4.1996264 -4.2064366][-4.2670965 -4.24683 -4.2282572 -4.2081633 -4.1853509 -4.1638074 -4.1510434 -4.1486292 -4.1567855 -4.1617203 -4.1680651 -4.1693139 -4.1692271 -4.1795626 -4.1928506][-4.2706933 -4.2610383 -4.2475004 -4.23254 -4.2125874 -4.1868691 -4.1608586 -4.1462684 -4.1439319 -4.1435981 -4.1506753 -4.1612267 -4.1722631 -4.1930819 -4.2132397][-4.261869 -4.2644167 -4.258337 -4.2493935 -4.2338023 -4.2090297 -4.1805892 -4.1610451 -4.151998 -4.1478243 -4.1561232 -4.173378 -4.1945758 -4.224761 -4.2501221][-4.230444 -4.2406244 -4.244945 -4.2468009 -4.2432408 -4.2286849 -4.2073436 -4.1885452 -4.1756215 -4.1684093 -4.1764331 -4.1940742 -4.2172074 -4.2463503 -4.269382]]...]
INFO - root - 2017-12-07 11:03:48.067989: step 1410, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 65h:02m:22s remains)
INFO - root - 2017-12-07 11:03:54.717454: step 1420, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 63h:40m:10s remains)
INFO - root - 2017-12-07 11:04:01.623195: step 1430, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 59h:39m:58s remains)
INFO - root - 2017-12-07 11:04:08.534946: step 1440, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 59h:16m:54s remains)
INFO - root - 2017-12-07 11:04:15.358389: step 1450, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 63h:42m:54s remains)
INFO - root - 2017-12-07 11:04:22.201303: step 1460, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.723 sec/batch; 66h:30m:05s remains)
INFO - root - 2017-12-07 11:04:28.943195: step 1470, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 61h:15m:07s remains)
INFO - root - 2017-12-07 11:04:35.752250: step 1480, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 61h:12m:53s remains)
INFO - root - 2017-12-07 11:04:42.257257: step 1490, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 65h:16m:11s remains)
INFO - root - 2017-12-07 11:04:48.969055: step 1500, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 66h:29m:09s remains)
2017-12-07 11:04:49.662287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.303133 -4.3010073 -4.30313 -4.3082991 -4.3124237 -4.3039885 -4.283803 -4.2620454 -4.2484407 -4.2453671 -4.2515039 -4.2464547 -4.22784 -4.2040968 -4.1978769][-4.29777 -4.2916446 -4.2866106 -4.2863569 -4.2915874 -4.2886505 -4.2762952 -4.2602553 -4.2481947 -4.2474117 -4.2545896 -4.2462397 -4.2180486 -4.1815319 -4.1698494][-4.28333 -4.2748103 -4.26497 -4.2631483 -4.2703772 -4.2709084 -4.2593341 -4.2435384 -4.2331276 -4.2372541 -4.2502956 -4.2402139 -4.1998563 -4.1523347 -4.1350703][-4.2739172 -4.2652245 -4.2527671 -4.247818 -4.2545304 -4.2583494 -4.2495041 -4.2374 -4.2318721 -4.2387772 -4.2513318 -4.2331333 -4.1836782 -4.1337662 -4.1173024][-4.2706337 -4.2587872 -4.2428122 -4.2347116 -4.2390966 -4.2435384 -4.2393575 -4.2360125 -4.2394552 -4.2497988 -4.2530155 -4.2227125 -4.1685877 -4.1270251 -4.1228547][-4.2664008 -4.2481256 -4.2277751 -4.2167597 -4.2161136 -4.213264 -4.2097216 -4.2204347 -4.2378926 -4.2509766 -4.2441878 -4.2080941 -4.1633935 -4.1385989 -4.1485939][-4.2645173 -4.2429872 -4.2221584 -4.2075429 -4.192843 -4.1752687 -4.170711 -4.1937118 -4.2212677 -4.2344685 -4.2222848 -4.1930861 -4.1659212 -4.1563306 -4.1749778][-4.2701268 -4.2494397 -4.2309027 -4.2107158 -4.1821423 -4.1528053 -4.1503844 -4.1779146 -4.2034 -4.2114992 -4.203197 -4.1878262 -4.1751132 -4.175384 -4.1966667][-4.2804646 -4.264853 -4.2518682 -4.2299714 -4.1933036 -4.1579361 -4.1526384 -4.1710773 -4.1858811 -4.1934385 -4.1981583 -4.196682 -4.1929541 -4.1993423 -4.21335][-4.2807903 -4.2727923 -4.2651286 -4.2411695 -4.2018151 -4.1677146 -4.15342 -4.1578636 -4.1665416 -4.1792531 -4.1964726 -4.2048311 -4.2057915 -4.209722 -4.2146268][-4.267683 -4.2651019 -4.259697 -4.2320318 -4.1923242 -4.1593442 -4.1425819 -4.141685 -4.1505785 -4.1690631 -4.1934729 -4.2064638 -4.2068 -4.2057652 -4.202805][-4.2521925 -4.2507305 -4.2428 -4.2148576 -4.178123 -4.1500106 -4.1316929 -4.1297874 -4.1427884 -4.1651168 -4.1920152 -4.2041345 -4.202311 -4.1985826 -4.1936955][-4.2400603 -4.23584 -4.2236834 -4.1981006 -4.17132 -4.15236 -4.1358805 -4.1335278 -4.1467476 -4.1681795 -4.1946578 -4.2047124 -4.2028351 -4.2005582 -4.1995163][-4.2312303 -4.2243371 -4.2074127 -4.1873717 -4.1726689 -4.1619883 -4.1510787 -4.1473165 -4.1537776 -4.1722784 -4.1975946 -4.2051973 -4.2033892 -4.2079706 -4.212676][-4.2126703 -4.2063475 -4.188962 -4.1769023 -4.1714711 -4.1701207 -4.1676912 -4.1641264 -4.16236 -4.1737113 -4.1925 -4.1973515 -4.1989784 -4.207397 -4.2112775]]...]
INFO - root - 2017-12-07 11:04:56.368972: step 1510, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:20m:20s remains)
INFO - root - 2017-12-07 11:05:02.968160: step 1520, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 64h:28m:45s remains)
INFO - root - 2017-12-07 11:05:09.857824: step 1530, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 66h:13m:19s remains)
INFO - root - 2017-12-07 11:05:16.679339: step 1540, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:54m:45s remains)
INFO - root - 2017-12-07 11:05:23.395860: step 1550, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 60h:04m:09s remains)
INFO - root - 2017-12-07 11:05:30.155485: step 1560, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 59h:14m:05s remains)
INFO - root - 2017-12-07 11:05:37.000471: step 1570, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 64h:08m:08s remains)
INFO - root - 2017-12-07 11:05:43.796306: step 1580, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 65h:49m:09s remains)
INFO - root - 2017-12-07 11:05:50.621138: step 1590, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 63h:12m:25s remains)
INFO - root - 2017-12-07 11:05:57.343688: step 1600, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 59h:59m:03s remains)
2017-12-07 11:05:58.048532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2131906 -4.2137961 -4.2102747 -4.2034769 -4.193625 -4.1859145 -4.175 -4.1652832 -4.1660118 -4.1803937 -4.1899219 -4.1904135 -4.196425 -4.2003989 -4.1925688][-4.2013187 -4.2053671 -4.20129 -4.1914868 -4.1776609 -4.1674361 -4.1573663 -4.1483026 -4.1502352 -4.167551 -4.178215 -4.1801038 -4.1880875 -4.1901531 -4.1716762][-4.1835246 -4.1960135 -4.1983123 -4.1939487 -4.1810131 -4.1712384 -4.1598978 -4.1511059 -4.15109 -4.1598954 -4.1662683 -4.1727767 -4.1889114 -4.1933031 -4.17101][-4.1362643 -4.1576791 -4.1713352 -4.1781778 -4.1719804 -4.1649346 -4.1551261 -4.1461983 -4.1392989 -4.1356158 -4.1355758 -4.1521835 -4.1790967 -4.1870427 -4.166688][-4.0888615 -4.1123681 -4.1349792 -4.1515269 -4.1497436 -4.1430497 -4.1369472 -4.1326137 -4.1229024 -4.1088147 -4.1040673 -4.1294169 -4.1627908 -4.1697969 -4.148674][-4.0620937 -4.0854959 -4.1105857 -4.1252012 -4.1195765 -4.1079764 -4.1011949 -4.0978441 -4.0866642 -4.0706139 -4.07082 -4.106142 -4.1435385 -4.1473088 -4.1234512][-4.0610533 -4.0861444 -4.1097355 -4.1165338 -4.1008191 -4.0781932 -4.0576239 -4.0341034 -4.0015645 -3.9817319 -4.0031905 -4.0606222 -4.1098351 -4.1190281 -4.1013331][-4.0868649 -4.1162567 -4.1358666 -4.1312947 -4.108017 -4.0869493 -4.0609708 -4.0162411 -3.9540138 -3.9205632 -3.9619212 -4.0411735 -4.0982471 -4.1144409 -4.1110148][-4.1127586 -4.144588 -4.1628847 -4.1537318 -4.1320877 -4.1256609 -4.116744 -4.0793624 -4.022748 -3.9960032 -4.0348907 -4.0990014 -4.1404486 -4.1547976 -4.1570754][-4.1422572 -4.1704979 -4.1861634 -4.1801715 -4.1653447 -4.1708603 -4.17738 -4.1589022 -4.1261435 -4.1167684 -4.1458807 -4.18189 -4.2015834 -4.2134252 -4.2150011][-4.1867924 -4.2149258 -4.2304449 -4.2297888 -4.2191586 -4.22232 -4.2261295 -4.2118778 -4.1934924 -4.1980782 -4.2249603 -4.2463098 -4.2530303 -4.2609181 -4.2634058][-4.2343497 -4.2560415 -4.2700033 -4.2728643 -4.2653956 -4.2664351 -4.2692375 -4.2583117 -4.2464242 -4.2557015 -4.2792158 -4.2957716 -4.2997127 -4.3014588 -4.299808][-4.2686386 -4.278543 -4.2853231 -4.2889533 -4.2880316 -4.2899413 -4.2920036 -4.28509 -4.2771459 -4.2844262 -4.2998238 -4.3116889 -4.3146272 -4.3138113 -4.3104796][-4.2872324 -4.2878647 -4.2892179 -4.2943149 -4.299655 -4.3034353 -4.3020191 -4.2964253 -4.29091 -4.2899308 -4.2901134 -4.2920213 -4.29546 -4.2970796 -4.2959404][-4.2827382 -4.2792778 -4.2790513 -4.285553 -4.2928505 -4.2961178 -4.2953176 -4.2934747 -4.2866879 -4.2762203 -4.2644815 -4.2582245 -4.260901 -4.2659025 -4.2679381]]...]
INFO - root - 2017-12-07 11:06:04.837440: step 1610, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 62h:03m:03s remains)
INFO - root - 2017-12-07 11:06:11.423761: step 1620, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 60h:09m:12s remains)
INFO - root - 2017-12-07 11:06:18.288463: step 1630, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 58h:57m:56s remains)
INFO - root - 2017-12-07 11:06:25.083930: step 1640, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 65h:53m:52s remains)
INFO - root - 2017-12-07 11:06:32.024935: step 1650, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.744 sec/batch; 68h:21m:10s remains)
INFO - root - 2017-12-07 11:06:38.887624: step 1660, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 64h:49m:47s remains)
INFO - root - 2017-12-07 11:06:45.716845: step 1670, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:17m:47s remains)
INFO - root - 2017-12-07 11:06:52.488444: step 1680, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 61h:39m:14s remains)
INFO - root - 2017-12-07 11:06:59.307610: step 1690, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 63h:36m:41s remains)
INFO - root - 2017-12-07 11:07:06.090094: step 1700, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:51m:27s remains)
2017-12-07 11:07:06.731328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1657519 -4.0999966 -4.0608044 -4.0896087 -4.1596813 -4.2299218 -4.2882719 -4.3103456 -4.3032169 -4.2939582 -4.2855606 -4.2880006 -4.3055272 -4.3214083 -4.3115187][-4.1389031 -4.048583 -3.9989953 -4.0406861 -4.1346641 -4.2211008 -4.2791815 -4.2961526 -4.2835174 -4.2721548 -4.2680378 -4.2788267 -4.3053532 -4.3267016 -4.3134189][-4.1369286 -4.0359664 -3.9852877 -4.0339613 -4.1348658 -4.2230887 -4.2686133 -4.2725077 -4.2520723 -4.2396007 -4.2435474 -4.2660913 -4.3044891 -4.327908 -4.3097138][-4.1618729 -4.0785704 -4.0496712 -4.0977335 -4.1834016 -4.2521086 -4.27416 -4.2525935 -4.2154088 -4.2027884 -4.2162919 -4.2523007 -4.3003736 -4.32251 -4.3005557][-4.1948214 -4.1507874 -4.1515055 -4.1919122 -4.2434869 -4.2763991 -4.2665577 -4.2119565 -4.1515317 -4.1400409 -4.1740375 -4.2302403 -4.2881713 -4.3086414 -4.2858753][-4.2238274 -4.2103567 -4.2285943 -4.2535415 -4.2685351 -4.2606153 -4.207603 -4.1091776 -4.0160165 -4.0152545 -4.0912795 -4.1831541 -4.2554703 -4.2787466 -4.2585711][-4.2389808 -4.246079 -4.2687116 -4.2798681 -4.2659311 -4.2182708 -4.111907 -3.9522252 -3.8203089 -3.8498793 -3.9842591 -4.1146641 -4.2027564 -4.2349725 -4.2226768][-4.2544093 -4.2736712 -4.2941542 -4.292264 -4.257875 -4.1828189 -4.0498095 -3.8633912 -3.7333636 -3.7966611 -3.955173 -4.0917468 -4.1791849 -4.2122245 -4.2022858][-4.2785449 -4.2935081 -4.3015456 -4.287292 -4.2501721 -4.1809478 -4.0680103 -3.9298797 -3.8613281 -3.9252708 -4.0447788 -4.1454077 -4.2069082 -4.2208843 -4.2002139][-4.296298 -4.302217 -4.3009734 -4.2833085 -4.252923 -4.2017403 -4.1249943 -4.0485168 -4.0299311 -4.0844393 -4.1646929 -4.2279921 -4.2569604 -4.2457724 -4.2124829][-4.3004842 -4.3066282 -4.3058043 -4.2919011 -4.2703161 -4.236515 -4.189496 -4.1500745 -4.1523209 -4.1973062 -4.2499671 -4.2858772 -4.2923141 -4.2689886 -4.2353568][-4.3019428 -4.3076673 -4.3092108 -4.3000298 -4.2880464 -4.2706308 -4.2422047 -4.2175565 -4.2233553 -4.2566676 -4.2910943 -4.3105712 -4.3083615 -4.2889094 -4.2646275][-4.3078518 -4.3094473 -4.307055 -4.2996764 -4.2952833 -4.2896385 -4.2758808 -4.262712 -4.2650223 -4.2821584 -4.2997303 -4.3078256 -4.3048997 -4.2958136 -4.2824903][-4.3183351 -4.3159795 -4.3094606 -4.3028817 -4.3022614 -4.3033524 -4.3016982 -4.2986927 -4.2992916 -4.3040633 -4.30852 -4.3085866 -4.3066645 -4.3032055 -4.2955604][-4.3286657 -4.3247676 -4.3178315 -4.3125844 -4.3123231 -4.3153195 -4.3200865 -4.323276 -4.3245158 -4.3224916 -4.31885 -4.3148465 -4.3123565 -4.3107185 -4.3065662]]...]
INFO - root - 2017-12-07 11:07:13.515031: step 1710, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 59h:32m:44s remains)
INFO - root - 2017-12-07 11:07:20.152501: step 1720, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 66h:23m:17s remains)
INFO - root - 2017-12-07 11:07:26.971456: step 1730, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 60h:54m:34s remains)
INFO - root - 2017-12-07 11:07:33.776588: step 1740, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:46m:57s remains)
INFO - root - 2017-12-07 11:07:40.508745: step 1750, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:52m:27s remains)
INFO - root - 2017-12-07 11:07:47.364121: step 1760, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 62h:55m:59s remains)
INFO - root - 2017-12-07 11:07:54.296923: step 1770, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 65h:51m:35s remains)
INFO - root - 2017-12-07 11:08:01.100440: step 1780, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 65h:18m:56s remains)
INFO - root - 2017-12-07 11:08:08.005540: step 1790, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 59h:44m:16s remains)
INFO - root - 2017-12-07 11:08:14.689351: step 1800, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 65h:54m:45s remains)
2017-12-07 11:08:15.450914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2792916 -4.2989721 -4.3028493 -4.2911177 -4.2721605 -4.2367034 -4.1867905 -4.1475425 -4.1481829 -4.1787071 -4.2177234 -4.258841 -4.2989631 -4.3242016 -4.3291211][-4.2977362 -4.3190374 -4.3239427 -4.3069954 -4.281096 -4.242033 -4.188261 -4.15195 -4.1567941 -4.1870489 -4.2230611 -4.2620683 -4.2982378 -4.31972 -4.3245497][-4.2965508 -4.3160605 -4.322298 -4.303524 -4.2756257 -4.2334437 -4.1731238 -4.1381726 -4.1522961 -4.1845016 -4.2207408 -4.2610927 -4.2960539 -4.3148293 -4.32051][-4.2845826 -4.2979059 -4.3003187 -4.2800407 -4.2515278 -4.209321 -4.140605 -4.100379 -4.122375 -4.1629086 -4.2055235 -4.2514734 -4.2903256 -4.3087096 -4.3158665][-4.2713294 -4.273746 -4.2666187 -4.2468429 -4.2240233 -4.1810532 -4.1001244 -4.0537143 -4.0840583 -4.1422091 -4.1944456 -4.2438498 -4.2830009 -4.3018475 -4.3113189][-4.2611995 -4.2516608 -4.2304068 -4.2074938 -4.187757 -4.1375074 -4.0370388 -3.9839222 -4.0360665 -4.1189942 -4.1865182 -4.2391324 -4.27695 -4.2963018 -4.3081179][-4.2558851 -4.2363524 -4.2029748 -4.1715183 -4.1424303 -4.0695071 -3.9320161 -3.8611369 -3.9431272 -4.0628419 -4.1558304 -4.2229824 -4.2673759 -4.2925262 -4.3067942][-4.2574372 -4.2315845 -4.1928234 -4.1522017 -4.1080585 -4.01009 -3.8347039 -3.740258 -3.857245 -4.0144148 -4.1317577 -4.2122827 -4.26161 -4.2900281 -4.3069773][-4.2541151 -4.2310119 -4.2055097 -4.1714773 -4.1308594 -4.0441904 -3.891875 -3.7999825 -3.8947129 -4.0370007 -4.1440887 -4.2172647 -4.2652693 -4.2934155 -4.3094783][-4.2351313 -4.2263718 -4.2201414 -4.202548 -4.1759772 -4.1162305 -4.0084929 -3.9382882 -4.0030589 -4.1077929 -4.1863117 -4.2430873 -4.2834578 -4.3048859 -4.3151293][-4.2013955 -4.2105322 -4.2257309 -4.2250476 -4.2122416 -4.1727753 -4.0966496 -4.045928 -4.09038 -4.166532 -4.2218738 -4.26703 -4.30268 -4.3183084 -4.3226175][-4.1839356 -4.2040915 -4.2291656 -4.2388997 -4.2378416 -4.2137523 -4.161716 -4.1243505 -4.1552086 -4.2091117 -4.2494311 -4.2854061 -4.3141503 -4.3249822 -4.3267264][-4.19538 -4.2163491 -4.24167 -4.2528906 -4.2554221 -4.2440238 -4.2090015 -4.1821885 -4.2002392 -4.2380943 -4.2692094 -4.296968 -4.319139 -4.3266649 -4.3283663][-4.2117023 -4.2310529 -4.2533126 -4.263432 -4.2670484 -4.2636514 -4.239831 -4.2224073 -4.2320356 -4.2568784 -4.2775741 -4.297678 -4.3173337 -4.3245549 -4.3282323][-4.2232676 -4.2384238 -4.2562828 -4.2638507 -4.2663145 -4.2662573 -4.2504997 -4.2392898 -4.2441144 -4.2566929 -4.2714496 -4.2881746 -4.3077083 -4.3169608 -4.3245177]]...]
INFO - root - 2017-12-07 11:08:22.129219: step 1810, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 56h:42m:27s remains)
INFO - root - 2017-12-07 11:08:28.716159: step 1820, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 60h:27m:07s remains)
INFO - root - 2017-12-07 11:08:35.612106: step 1830, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.708 sec/batch; 65h:01m:46s remains)
INFO - root - 2017-12-07 11:08:42.471690: step 1840, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 67h:00m:54s remains)
INFO - root - 2017-12-07 11:08:49.350836: step 1850, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 63h:07m:13s remains)
INFO - root - 2017-12-07 11:08:56.196454: step 1860, loss = 2.02, batch loss = 1.97 (12.7 examples/sec; 0.632 sec/batch; 58h:03m:52s remains)
INFO - root - 2017-12-07 11:09:03.015344: step 1870, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 65h:47m:23s remains)
INFO - root - 2017-12-07 11:09:09.761919: step 1880, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 67h:24m:19s remains)
INFO - root - 2017-12-07 11:09:16.554124: step 1890, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 63h:45m:23s remains)
INFO - root - 2017-12-07 11:09:23.288053: step 1900, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 60h:34m:09s remains)
2017-12-07 11:09:23.988606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2067451 -4.2006607 -4.2015052 -4.2136641 -4.2316375 -4.2516308 -4.267118 -4.2755709 -4.2779765 -4.2734246 -4.2640195 -4.2607675 -4.2592921 -4.2465549 -4.2246051][-4.2275534 -4.226439 -4.234457 -4.2518015 -4.2720766 -4.291842 -4.3056288 -4.3116655 -4.31352 -4.3111358 -4.306221 -4.30751 -4.3090773 -4.2989607 -4.2841067][-4.2572894 -4.2538152 -4.2594934 -4.27292 -4.2890029 -4.3077893 -4.3230958 -4.3327975 -4.3379393 -4.338397 -4.3382759 -4.34243 -4.3429832 -4.3341813 -4.3221793][-4.2712851 -4.262063 -4.2583017 -4.26121 -4.2688069 -4.2817364 -4.291677 -4.3025551 -4.31265 -4.3188491 -4.3218312 -4.3261156 -4.3261533 -4.3221154 -4.3176279][-4.270071 -4.2517686 -4.2330809 -4.2182035 -4.2068644 -4.2006741 -4.198451 -4.2138863 -4.2371011 -4.2527351 -4.2618523 -4.2692184 -4.2717752 -4.2736459 -4.2772465][-4.2383022 -4.2065272 -4.1649809 -4.121501 -4.0806942 -4.0462523 -4.0314403 -4.05882 -4.1020813 -4.1317883 -4.1506572 -4.1672783 -4.1797185 -4.1927252 -4.2071257][-4.1833072 -4.1372662 -4.0727477 -4.0008345 -3.9323406 -3.8743277 -3.851449 -3.8955948 -3.966764 -4.0223832 -4.0634055 -4.09694 -4.1214485 -4.1416512 -4.1608467][-4.163116 -4.112915 -4.0477338 -3.9818721 -3.9292896 -3.8929269 -3.8909645 -3.9493723 -4.0293231 -4.090445 -4.1366472 -4.1646504 -4.1748514 -4.1781034 -4.1798339][-4.1870189 -4.1504588 -4.10872 -4.0754929 -4.0602694 -4.0564661 -4.0684681 -4.1142244 -4.169 -4.2070956 -4.2359538 -4.2469492 -4.2411456 -4.231956 -4.2219744][-4.205709 -4.1853533 -4.1691222 -4.1648088 -4.1737838 -4.1811976 -4.1910338 -4.2190094 -4.2512436 -4.2697673 -4.281806 -4.2821403 -4.2710862 -4.2562604 -4.2394848][-4.2236362 -4.2193027 -4.2247195 -4.2374325 -4.2493582 -4.2517395 -4.2546539 -4.269362 -4.2866745 -4.29067 -4.2877049 -4.2800145 -4.26911 -4.25518 -4.2403049][-4.246594 -4.2507229 -4.26583 -4.281858 -4.286139 -4.2811861 -4.2796683 -4.2859173 -4.2924128 -4.2885637 -4.2775803 -4.2690082 -4.2646565 -4.2556629 -4.2435207][-4.2589483 -4.2666526 -4.2812791 -4.2911315 -4.2892714 -4.2805676 -4.2759705 -4.2775044 -4.2798839 -4.272635 -4.2606883 -4.2554765 -4.2540693 -4.2427268 -4.2318234][-4.2584219 -4.2623129 -4.268012 -4.2687516 -4.2634745 -4.2575579 -4.2579517 -4.2636333 -4.2672114 -4.2615371 -4.2515631 -4.2508755 -4.2508831 -4.2398076 -4.2290006][-4.2331343 -4.2316122 -4.2285905 -4.2201018 -4.213141 -4.216166 -4.2278705 -4.2427893 -4.2487822 -4.2467675 -4.2433133 -4.2495131 -4.254117 -4.2493553 -4.2416587]]...]
INFO - root - 2017-12-07 11:09:30.783237: step 1910, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 65h:28m:50s remains)
INFO - root - 2017-12-07 11:09:37.328439: step 1920, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:26m:33s remains)
INFO - root - 2017-12-07 11:09:44.050519: step 1930, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 57h:38m:07s remains)
INFO - root - 2017-12-07 11:09:50.910737: step 1940, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 63h:02m:57s remains)
INFO - root - 2017-12-07 11:09:57.771702: step 1950, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 65h:19m:54s remains)
INFO - root - 2017-12-07 11:10:04.519613: step 1960, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:48m:41s remains)
INFO - root - 2017-12-07 11:10:11.356830: step 1970, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 59h:53m:38s remains)
INFO - root - 2017-12-07 11:10:18.199394: step 1980, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 61h:07m:12s remains)
INFO - root - 2017-12-07 11:10:25.061116: step 1990, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 64h:29m:43s remains)
INFO - root - 2017-12-07 11:10:31.867579: step 2000, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 64h:53m:52s remains)
2017-12-07 11:10:32.556617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2490759 -4.2423015 -4.240881 -4.2480383 -4.2532096 -4.2513471 -4.2519221 -4.2541785 -4.2473407 -4.2382445 -4.2335496 -4.235105 -4.2411895 -4.2482858 -4.2513][-4.2492309 -4.2403145 -4.2372446 -4.2461715 -4.2533526 -4.2485514 -4.2449465 -4.2475462 -4.2418804 -4.2301111 -4.2165279 -4.2072444 -4.205152 -4.2104616 -4.2174358][-4.2466965 -4.2426186 -4.2410765 -4.24953 -4.2555714 -4.2439814 -4.2321644 -4.2329021 -4.235147 -4.230525 -4.2143412 -4.1945477 -4.1774888 -4.1697578 -4.1710057][-4.2370396 -4.23496 -4.2339048 -4.2403407 -4.2428069 -4.2262707 -4.2064142 -4.2014089 -4.213232 -4.2233362 -4.2117634 -4.1856313 -4.1578722 -4.1364951 -4.1299067][-4.2187614 -4.2082238 -4.2023807 -4.2068229 -4.209475 -4.1925168 -4.1604195 -4.1425867 -4.1605563 -4.1891985 -4.1930895 -4.1739235 -4.1465631 -4.1206942 -4.1106873][-4.2167954 -4.194057 -4.1768541 -4.1718941 -4.1731229 -4.1550612 -4.1017761 -4.0551305 -4.0707173 -4.1285453 -4.1687975 -4.1721559 -4.15543 -4.1303434 -4.1181145][-4.2336493 -4.20023 -4.1678524 -4.1472864 -4.1434793 -4.121192 -4.043591 -3.9552269 -3.9558802 -4.0467954 -4.1315861 -4.1659055 -4.1675797 -4.1494417 -4.1397276][-4.2420397 -4.2058477 -4.1654248 -4.1348653 -4.129909 -4.1105423 -4.0321736 -3.9285243 -3.9091778 -4.0000749 -4.1005955 -4.1537623 -4.1681857 -4.1545711 -4.146277][-4.2454333 -4.2145138 -4.1793213 -4.1545658 -4.15912 -4.1565952 -4.1071978 -4.0245476 -3.98862 -4.0358081 -4.1071186 -4.1523046 -4.1660867 -4.1536651 -4.1448894][-4.229816 -4.2100043 -4.1922231 -4.1870165 -4.2056932 -4.2212844 -4.2016163 -4.1455717 -4.1021872 -4.1073995 -4.1390266 -4.1657081 -4.1736312 -4.1628928 -4.1534219][-4.2063084 -4.1914124 -4.1872416 -4.2002444 -4.2306352 -4.2574625 -4.2567029 -4.2233343 -4.1846285 -4.170886 -4.1793284 -4.1938715 -4.1990609 -4.190486 -4.1772437][-4.2001324 -4.183835 -4.1817217 -4.2000766 -4.2309113 -4.2612123 -4.2718754 -4.2557321 -4.2274885 -4.21283 -4.213295 -4.2216673 -4.2267971 -4.220459 -4.2064629][-4.20879 -4.1887951 -4.1829658 -4.1966195 -4.2195725 -4.2482405 -4.2666235 -4.2629681 -4.2442336 -4.2337971 -4.2325931 -4.2362275 -4.2416763 -4.2394209 -4.2301836][-4.2258863 -4.2015967 -4.1895952 -4.194325 -4.2071018 -4.2288175 -4.2495351 -4.2544494 -4.2430754 -4.2361779 -4.2352128 -4.2372365 -4.2435541 -4.245749 -4.2426324][-4.2462764 -4.2247314 -4.2116289 -4.2092004 -4.212831 -4.2259307 -4.2435765 -4.2530947 -4.24952 -4.2463923 -4.2455707 -4.2472558 -4.2544675 -4.2583666 -4.2562642]]...]
INFO - root - 2017-12-07 11:10:39.267518: step 2010, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 61h:35m:38s remains)
INFO - root - 2017-12-07 11:10:45.916376: step 2020, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 66h:01m:44s remains)
INFO - root - 2017-12-07 11:10:52.909359: step 2030, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 65h:46m:03s remains)
INFO - root - 2017-12-07 11:10:59.774286: step 2040, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 63h:03m:39s remains)
INFO - root - 2017-12-07 11:11:06.470984: step 2050, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 59h:16m:32s remains)
INFO - root - 2017-12-07 11:11:13.202437: step 2060, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 63h:11m:10s remains)
INFO - root - 2017-12-07 11:11:20.051987: step 2070, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 66h:41m:10s remains)
INFO - root - 2017-12-07 11:11:26.828220: step 2080, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 63h:18m:34s remains)
INFO - root - 2017-12-07 11:11:33.612059: step 2090, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 62h:02m:16s remains)
INFO - root - 2017-12-07 11:11:40.436872: step 2100, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 62h:33m:39s remains)
2017-12-07 11:11:41.184013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15231 -4.1531162 -4.1636472 -4.1878557 -4.1993294 -4.1960931 -4.2008862 -4.2102323 -4.2260566 -4.2558837 -4.2878265 -4.3139777 -4.3269944 -4.3246031 -4.3048964][-4.1570764 -4.1662264 -4.1814427 -4.2050686 -4.2157354 -4.2170086 -4.2228856 -4.2387667 -4.2638187 -4.297802 -4.3257957 -4.3435726 -4.352767 -4.345233 -4.3220305][-4.1692991 -4.1754651 -4.1877017 -4.2053833 -4.211812 -4.2132082 -4.218821 -4.2370839 -4.2693739 -4.309154 -4.3372941 -4.3534741 -4.363493 -4.3527336 -4.3250885][-4.1844616 -4.1812057 -4.1934867 -4.2096548 -4.2053347 -4.1947403 -4.1924372 -4.2058558 -4.2427878 -4.2909861 -4.3254757 -4.3451314 -4.3581338 -4.3470993 -4.3199644][-4.1978588 -4.1883307 -4.19961 -4.2102089 -4.1886635 -4.1499052 -4.1291409 -4.1386361 -4.1865339 -4.2474446 -4.2952886 -4.3235288 -4.3428903 -4.3363609 -4.3146868][-4.2157879 -4.2058945 -4.2098446 -4.2078414 -4.1633549 -4.0863171 -4.0352931 -4.0426683 -4.1086779 -4.1889434 -4.254209 -4.2978096 -4.3275294 -4.3304038 -4.3201494][-4.2327905 -4.2324052 -4.2338958 -4.219027 -4.1586351 -4.0556588 -3.9706762 -3.9639087 -4.0430012 -4.1409082 -4.21901 -4.2765651 -4.3150082 -4.3270912 -4.323657][-4.24614 -4.2588758 -4.2590103 -4.2389402 -4.1766253 -4.0741997 -3.9783094 -3.9575937 -4.0264192 -4.1223068 -4.200635 -4.266223 -4.3066835 -4.3186903 -4.3146014][-4.2470646 -4.2681494 -4.2669005 -4.2469754 -4.1924143 -4.1042533 -4.0125685 -3.987051 -4.0376825 -4.1229005 -4.19692 -4.2638645 -4.3027372 -4.3078284 -4.2989655][-4.2357144 -4.258873 -4.2584257 -4.2419128 -4.1998382 -4.1289148 -4.0505857 -4.0280352 -4.0631537 -4.1329665 -4.1978645 -4.2604771 -4.2979326 -4.2999668 -4.2870374][-4.2322292 -4.2496285 -4.2482338 -4.2366834 -4.2055144 -4.150805 -4.087121 -4.0715432 -4.1036439 -4.1596384 -4.2100353 -4.2575574 -4.2880168 -4.289813 -4.2801266][-4.2506509 -4.2592192 -4.25082 -4.2392187 -4.2148519 -4.176846 -4.1314621 -4.123229 -4.1526456 -4.1970048 -4.2320147 -4.2591243 -4.2729087 -4.2717481 -4.2707458][-4.2813549 -4.2871041 -4.2742481 -4.2556443 -4.2312927 -4.2034187 -4.1754937 -4.1751332 -4.2019887 -4.2371454 -4.256484 -4.2590461 -4.2453079 -4.2316351 -4.2364306][-4.3016176 -4.3096571 -4.3002014 -4.2811756 -4.2609768 -4.2411389 -4.2245612 -4.2289839 -4.2510953 -4.2751541 -4.2814832 -4.2621832 -4.2168946 -4.1824183 -4.1860175][-4.2955551 -4.3044391 -4.3069406 -4.3009763 -4.2908916 -4.2797031 -4.2719636 -4.279757 -4.2962475 -4.3077536 -4.30299 -4.270247 -4.2096467 -4.1586013 -4.1542268]]...]
INFO - root - 2017-12-07 11:11:47.726868: step 2110, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 57h:41m:16s remains)
INFO - root - 2017-12-07 11:11:54.332448: step 2120, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 59h:58m:12s remains)
INFO - root - 2017-12-07 11:12:01.193039: step 2130, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 64h:00m:22s remains)
INFO - root - 2017-12-07 11:12:08.193842: step 2140, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.759 sec/batch; 69h:36m:20s remains)
INFO - root - 2017-12-07 11:12:15.038793: step 2150, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 62h:53m:49s remains)
INFO - root - 2017-12-07 11:12:21.777714: step 2160, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 58h:17m:32s remains)
INFO - root - 2017-12-07 11:12:28.637265: step 2170, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 60h:16m:20s remains)
INFO - root - 2017-12-07 11:12:35.461083: step 2180, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 63h:58m:04s remains)
INFO - root - 2017-12-07 11:12:42.331896: step 2190, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 65h:54m:17s remains)
INFO - root - 2017-12-07 11:12:49.051237: step 2200, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 61h:56m:26s remains)
2017-12-07 11:12:49.783706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.265305 -4.2702169 -4.2754087 -4.2610993 -4.2358141 -4.194159 -4.1428385 -4.1167297 -4.1369634 -4.1667433 -4.1814127 -4.2132168 -4.2494459 -4.2712331 -4.26897][-4.2537556 -4.2671623 -4.2779675 -4.2645521 -4.2342958 -4.1859989 -4.1330752 -4.1124654 -4.1389012 -4.1755357 -4.1978412 -4.2322707 -4.2741194 -4.2970624 -4.29373][-4.2479167 -4.2712831 -4.28488 -4.2661591 -4.22148 -4.1579833 -4.0977015 -4.0851455 -4.1251607 -4.17475 -4.210814 -4.2502489 -4.2953649 -4.320682 -4.3126597][-4.2492309 -4.28197 -4.295723 -4.26714 -4.2041712 -4.1200933 -4.0442295 -4.0360351 -4.093668 -4.1643333 -4.2202225 -4.2701406 -4.3181086 -4.3408995 -4.323967][-4.2556119 -4.291883 -4.3032417 -4.2647352 -4.1839933 -4.0794039 -3.9866295 -3.9801581 -4.0563908 -4.1520572 -4.2296972 -4.2916408 -4.3391237 -4.3522992 -4.3214421][-4.2629461 -4.298255 -4.30501 -4.258635 -4.160193 -4.0302191 -3.9155691 -3.907568 -4.0059404 -4.1300116 -4.2335472 -4.3078089 -4.3508048 -4.3509812 -4.309382][-4.2762017 -4.306242 -4.30619 -4.2547336 -4.14369 -3.9897194 -3.8481894 -3.8296282 -3.9443035 -4.0929723 -4.2202086 -4.3067141 -4.3485141 -4.3420773 -4.29449][-4.2966933 -4.3168058 -4.3087935 -4.2542286 -4.1418934 -3.9794788 -3.8241231 -3.7894249 -3.9005618 -4.0544333 -4.1946945 -4.2925086 -4.3389778 -4.3309517 -4.2774744][-4.3225794 -4.3324533 -4.3161459 -4.2590103 -4.1548047 -4.0049939 -3.8532352 -3.8061192 -3.8971586 -4.0372391 -4.1753182 -4.2755442 -4.3252544 -4.3200555 -4.2650123][-4.3457561 -4.3487964 -4.3276 -4.2718987 -4.1818433 -4.0563049 -3.9215131 -3.86991 -3.9356384 -4.0517507 -4.1752558 -4.2661943 -4.3113222 -4.3085413 -4.2598166][-4.3570151 -4.3564868 -4.3359017 -4.2878327 -4.2161064 -4.1205583 -4.0114036 -3.9610245 -4.0017471 -4.08873 -4.1883621 -4.2629185 -4.2999721 -4.29803 -4.2591577][-4.3531256 -4.3496103 -4.3318958 -4.2943487 -4.242939 -4.1758485 -4.0971971 -4.0578551 -4.0820966 -4.1439724 -4.2180915 -4.2726688 -4.30053 -4.2972136 -4.2639627][-4.3430181 -4.335753 -4.319602 -4.2919011 -4.2551284 -4.2080073 -4.1563511 -4.1336021 -4.1519003 -4.1985292 -4.2532353 -4.2914014 -4.3101 -4.3051987 -4.2759986][-4.3314013 -4.3193502 -4.3029175 -4.2834234 -4.25686 -4.2225075 -4.1898642 -4.1778722 -4.195435 -4.2334461 -4.273078 -4.3000703 -4.3133059 -4.3104324 -4.2878227][-4.3223834 -4.3073564 -4.2919154 -4.2777948 -4.2582855 -4.2332616 -4.2115874 -4.203321 -4.2164078 -4.2440882 -4.2722139 -4.2928271 -4.304832 -4.3061986 -4.2933149]]...]
INFO - root - 2017-12-07 11:12:56.564983: step 2210, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 62h:55m:09s remains)
INFO - root - 2017-12-07 11:13:03.199014: step 2220, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 67h:49m:36s remains)
INFO - root - 2017-12-07 11:13:09.934359: step 2230, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 63h:45m:01s remains)
INFO - root - 2017-12-07 11:13:16.704595: step 2240, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 59h:15m:48s remains)
INFO - root - 2017-12-07 11:13:23.591974: step 2250, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 62h:36m:51s remains)
INFO - root - 2017-12-07 11:13:30.549427: step 2260, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 67h:51m:31s remains)
INFO - root - 2017-12-07 11:13:37.321804: step 2270, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 62h:54m:11s remains)
INFO - root - 2017-12-07 11:13:44.046669: step 2280, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 60h:14m:08s remains)
INFO - root - 2017-12-07 11:13:50.748805: step 2290, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 58h:38m:54s remains)
INFO - root - 2017-12-07 11:13:57.682475: step 2300, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:43m:04s remains)
2017-12-07 11:13:58.527152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.278018 -4.2755561 -4.2659445 -4.2439904 -4.2210274 -4.1998243 -4.1799169 -4.1591573 -4.1403856 -4.1232405 -4.0963826 -4.0827575 -4.0979056 -4.1348848 -4.179656][-4.2757397 -4.2635355 -4.2457256 -4.2176194 -4.1879773 -4.1578922 -4.1306558 -4.0996556 -4.0706711 -4.0617142 -4.0630569 -4.0820746 -4.1183357 -4.1668448 -4.2154293][-4.2595949 -4.23094 -4.2007437 -4.1720648 -4.1491842 -4.1202736 -4.0926003 -4.0536366 -4.0186715 -4.0228624 -4.0535197 -4.0955729 -4.148993 -4.2060943 -4.25463][-4.2424965 -4.2123423 -4.1804576 -4.1540995 -4.135694 -4.1048684 -4.0710664 -4.0307188 -4.0102725 -4.027451 -4.0647397 -4.113678 -4.1728072 -4.22965 -4.2746539][-4.2429409 -4.21677 -4.18333 -4.1486874 -4.1176548 -4.0702243 -4.0202107 -3.9921885 -4.01459 -4.0578365 -4.1032839 -4.15544 -4.20928 -4.2533774 -4.2855291][-4.2385545 -4.2235622 -4.1913834 -4.1382337 -4.0785313 -4.0008397 -3.9286921 -3.9199066 -3.9989 -4.0754967 -4.1365547 -4.1953893 -4.2442575 -4.275609 -4.2928457][-4.2142324 -4.2089267 -4.1757917 -4.1114068 -4.0325766 -3.9347587 -3.8516097 -3.8681083 -3.9888048 -4.0860553 -4.1558819 -4.218648 -4.2680259 -4.2914004 -4.298481][-4.1731319 -4.1662292 -4.1449738 -4.1010513 -4.0430975 -3.9730246 -3.9223642 -3.9500992 -4.0474782 -4.1262341 -4.1850944 -4.2385631 -4.2784767 -4.2950387 -4.2978988][-4.123632 -4.1349053 -4.1438251 -4.1341915 -4.1091213 -4.0787692 -4.0646834 -4.0901065 -4.1476555 -4.1960816 -4.233829 -4.2659626 -4.2869806 -4.296742 -4.2996869][-4.11417 -4.1410184 -4.1613164 -4.162921 -4.1529689 -4.1468253 -4.154603 -4.1811886 -4.2200041 -4.2537031 -4.2803407 -4.2965946 -4.3018322 -4.3040137 -4.3072324][-4.1361532 -4.1597557 -4.1744022 -4.1759529 -4.1729264 -4.1780567 -4.1940055 -4.2205076 -4.2494063 -4.2763677 -4.2998691 -4.3112864 -4.3134594 -4.31472 -4.3183551][-4.1680775 -4.1828475 -4.1904273 -4.1906214 -4.1901369 -4.1929464 -4.2068181 -4.2325058 -4.2555308 -4.27978 -4.3053584 -4.3184357 -4.3236842 -4.3270912 -4.329937][-4.1894512 -4.19967 -4.2043138 -4.2054434 -4.2060452 -4.2062755 -4.2157421 -4.2379251 -4.2614574 -4.2858996 -4.3106012 -4.3252106 -4.3299026 -4.3317876 -4.3336434][-4.189723 -4.2015572 -4.2093797 -4.2112937 -4.2150025 -4.2206488 -4.23394 -4.2572942 -4.28184 -4.3032351 -4.322495 -4.3327875 -4.3337727 -4.33418 -4.3354506][-4.1994309 -4.2133651 -4.2208147 -4.22389 -4.2338686 -4.2505755 -4.2691712 -4.289546 -4.3075914 -4.3198743 -4.329236 -4.3334675 -4.333602 -4.3340492 -4.3345723]]...]
INFO - root - 2017-12-07 11:14:05.317997: step 2310, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 61h:37m:09s remains)
INFO - root - 2017-12-07 11:14:11.846116: step 2320, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 58h:38m:51s remains)
INFO - root - 2017-12-07 11:14:18.606434: step 2330, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 60h:26m:00s remains)
INFO - root - 2017-12-07 11:14:25.375985: step 2340, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 65h:47m:02s remains)
INFO - root - 2017-12-07 11:14:32.163763: step 2350, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 67h:10m:34s remains)
INFO - root - 2017-12-07 11:14:39.042297: step 2360, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:42m:30s remains)
INFO - root - 2017-12-07 11:14:45.793204: step 2370, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 61h:00m:44s remains)
INFO - root - 2017-12-07 11:14:52.544720: step 2380, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 62h:38m:29s remains)
INFO - root - 2017-12-07 11:14:59.433461: step 2390, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 64h:51m:02s remains)
INFO - root - 2017-12-07 11:15:06.254854: step 2400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 64h:22m:34s remains)
2017-12-07 11:15:06.922152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1922016 -4.2180519 -4.2383189 -4.2510891 -4.2668242 -4.2737694 -4.2553725 -4.2267923 -4.2033281 -4.2013316 -4.2119637 -4.2309 -4.2519579 -4.2573161 -4.2628732][-4.1705351 -4.209147 -4.2416062 -4.2575912 -4.2646489 -4.2591467 -4.2348695 -4.2070713 -4.1868992 -4.1886449 -4.2021403 -4.224813 -4.245707 -4.2493448 -4.2546458][-4.1661568 -4.2089787 -4.2455988 -4.253643 -4.2455206 -4.2320704 -4.2103438 -4.1898756 -4.1805296 -4.1876159 -4.2034597 -4.2223082 -4.2385621 -4.2418518 -4.2502174][-4.1904893 -4.2222223 -4.2430081 -4.2322497 -4.2076168 -4.1866646 -4.1726975 -4.1628609 -4.1641517 -4.1803565 -4.2021942 -4.2194729 -4.2327127 -4.2383356 -4.2503414][-4.2023377 -4.2136087 -4.2121024 -4.1818066 -4.1415668 -4.112608 -4.1033211 -4.1049781 -4.1232471 -4.1559625 -4.18779 -4.2085333 -4.22429 -4.2361121 -4.2519593][-4.185061 -4.1782374 -4.1576071 -4.1164579 -4.067935 -4.0278521 -4.00595 -4.0034361 -4.0327053 -4.0857124 -4.1330094 -4.166018 -4.1956911 -4.2205162 -4.2443271][-4.1246071 -4.10979 -4.0859728 -4.0490508 -4.0065017 -3.9604118 -3.9168053 -3.8930943 -3.9193482 -3.9824426 -4.0429273 -4.0952692 -4.1498876 -4.1944146 -4.23051][-4.0749655 -4.0600262 -4.0425477 -4.0214562 -3.997699 -3.9590774 -3.9061923 -3.863086 -3.8727903 -3.9247823 -3.9860265 -4.0514951 -4.1213646 -4.1759806 -4.2199149][-4.0788321 -4.07022 -4.0677872 -4.0636206 -4.0573215 -4.0369577 -3.9976728 -3.9579282 -3.9529831 -3.9769244 -4.0163231 -4.0694327 -4.1304 -4.1792374 -4.2221003][-4.1405811 -4.1362581 -4.1420112 -4.14518 -4.1438417 -4.1351514 -4.1129737 -4.088 -4.0817909 -4.0881162 -4.105588 -4.136869 -4.1769819 -4.2106185 -4.2425556][-4.2203245 -4.2173738 -4.2223616 -4.2249885 -4.22339 -4.2196546 -4.2098579 -4.1981959 -4.1950817 -4.195066 -4.2000642 -4.2137341 -4.2341661 -4.2522068 -4.2708168][-4.2758465 -4.2724862 -4.2752857 -4.2764931 -4.2772584 -4.278626 -4.277144 -4.2738872 -4.2727785 -4.2703319 -4.2706351 -4.2760515 -4.2827425 -4.2876992 -4.2930312][-4.3046532 -4.3026958 -4.3038583 -4.3041654 -4.3083134 -4.3136458 -4.3169022 -4.3176007 -4.3168836 -4.3133235 -4.3114424 -4.3135386 -4.3150315 -4.3137422 -4.3121524][-4.3230081 -4.3216491 -4.3218837 -4.321847 -4.3252 -4.3299313 -4.3330736 -4.3346181 -4.3351984 -4.3340716 -4.3334785 -4.3343325 -4.3345108 -4.3321562 -4.3286853][-4.3337779 -4.33242 -4.3316841 -4.3310266 -4.3316097 -4.3330231 -4.33387 -4.3347993 -4.33634 -4.3372369 -4.3386216 -4.3404303 -4.3410487 -4.3397336 -4.3379207]]...]
INFO - root - 2017-12-07 11:15:13.652746: step 2410, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 62h:49m:08s remains)
INFO - root - 2017-12-07 11:15:20.364586: step 2420, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 61h:47m:41s remains)
INFO - root - 2017-12-07 11:15:27.212519: step 2430, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 58h:46m:19s remains)
INFO - root - 2017-12-07 11:15:34.003290: step 2440, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 59h:43m:02s remains)
INFO - root - 2017-12-07 11:15:40.784593: step 2450, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 63h:57m:41s remains)
INFO - root - 2017-12-07 11:15:47.595737: step 2460, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 67h:25m:44s remains)
INFO - root - 2017-12-07 11:15:54.417120: step 2470, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.669 sec/batch; 61h:22m:27s remains)
INFO - root - 2017-12-07 11:16:01.229274: step 2480, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 57h:39m:05s remains)
INFO - root - 2017-12-07 11:16:08.104752: step 2490, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 60h:52m:03s remains)
INFO - root - 2017-12-07 11:16:14.946790: step 2500, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 63h:22m:17s remains)
2017-12-07 11:16:15.681043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3571749 -4.3527813 -4.3493648 -4.3513193 -4.3549175 -4.35651 -4.3582053 -4.36016 -4.3619642 -4.3631849 -4.3628592 -4.3631997 -4.3624907 -4.3624687 -4.3632846][-4.3427029 -4.3317232 -4.324955 -4.3292708 -4.3376813 -4.3423142 -4.3470788 -4.3520255 -4.3549547 -4.3554235 -4.3543396 -4.3555593 -4.3568025 -4.3596272 -4.362041][-4.3218737 -4.3039227 -4.2935691 -4.2984319 -4.3073535 -4.3089533 -4.3124824 -4.3212185 -4.3259668 -4.3259368 -4.32531 -4.3314023 -4.3405533 -4.3504124 -4.3565421][-4.2967181 -4.2713375 -4.2545314 -4.2539816 -4.258749 -4.253 -4.2517576 -4.2666206 -4.2774057 -4.2780862 -4.2775259 -4.2885885 -4.3086295 -4.3295054 -4.3423586][-4.269938 -4.236886 -4.20792 -4.1950393 -4.190187 -4.1750913 -4.164597 -4.1825857 -4.2004886 -4.2049026 -4.2065086 -4.2266741 -4.2615538 -4.296628 -4.3183532][-4.240396 -4.1977386 -4.1520166 -4.1191797 -4.0965438 -4.0660472 -4.0444937 -4.0657849 -4.0926762 -4.1074872 -4.1205225 -4.1583748 -4.2109485 -4.2614479 -4.293735][-4.2085452 -4.1549249 -4.0950403 -4.0457196 -4.0085425 -3.9654012 -3.9342873 -3.9606922 -3.9999797 -4.025579 -4.0529971 -4.107214 -4.1731668 -4.2318411 -4.2722673][-4.1804543 -4.1216841 -4.0556955 -4.0084391 -3.9848895 -3.9548626 -3.9259915 -3.9451246 -3.9810405 -4.0049334 -4.0312 -4.0824757 -4.1483855 -4.2095 -4.2548137][-4.1591063 -4.1015811 -4.0435042 -4.0162482 -4.0291247 -4.0339332 -4.0169978 -4.0169458 -4.0303922 -4.0386705 -4.0498328 -4.0849462 -4.141326 -4.202157 -4.249999][-4.1613455 -4.1124763 -4.0677266 -4.0614104 -4.1000557 -4.1269088 -4.122036 -4.1084418 -4.0990357 -4.0923629 -4.0901566 -4.1110387 -4.1553731 -4.2114344 -4.2574782][-4.1570921 -4.1177721 -4.0858936 -4.0961428 -4.1450253 -4.1817856 -4.1865287 -4.1745019 -4.1579132 -4.1467166 -4.1407104 -4.1560674 -4.190486 -4.2348452 -4.2731562][-4.1516967 -4.1178217 -4.0957494 -4.1169586 -4.1677604 -4.2049532 -4.2159572 -4.2125773 -4.2055464 -4.2006431 -4.1986952 -4.2122808 -4.23876 -4.2692027 -4.2960281][-4.1784806 -4.1542983 -4.1421618 -4.1631312 -4.2029309 -4.22991 -4.2366633 -4.2352519 -4.2356615 -4.2349429 -4.2341337 -4.2450166 -4.2667847 -4.29028 -4.3110337][-4.2111497 -4.1984529 -4.1928697 -4.207252 -4.2331691 -4.2507277 -4.2558851 -4.255372 -4.2563267 -4.2557974 -4.2543149 -4.2623672 -4.2803841 -4.3011656 -4.3201365][-4.2419806 -4.2349944 -4.2307444 -4.2370324 -4.2525721 -4.2657733 -4.2730293 -4.2770839 -4.2814507 -4.2844462 -4.2874541 -4.2960081 -4.3106575 -4.326211 -4.339251]]...]
INFO - root - 2017-12-07 11:16:22.402603: step 2510, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.643 sec/batch; 58h:56m:44s remains)
INFO - root - 2017-12-07 11:16:29.095257: step 2520, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 62h:17m:14s remains)
INFO - root - 2017-12-07 11:16:35.937409: step 2530, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 66h:14m:39s remains)
INFO - root - 2017-12-07 11:16:42.689230: step 2540, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 63h:03m:41s remains)
INFO - root - 2017-12-07 11:16:49.500375: step 2550, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:57m:50s remains)
INFO - root - 2017-12-07 11:16:56.291027: step 2560, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 59h:45m:42s remains)
INFO - root - 2017-12-07 11:17:03.143941: step 2570, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 64h:17m:29s remains)
INFO - root - 2017-12-07 11:17:10.105745: step 2580, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 65h:04m:32s remains)
INFO - root - 2017-12-07 11:17:17.049902: step 2590, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 62h:27m:11s remains)
INFO - root - 2017-12-07 11:17:23.699725: step 2600, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 59h:16m:08s remains)
2017-12-07 11:17:24.392342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1876974 -4.1828704 -4.1855946 -4.1905441 -4.1988926 -4.201149 -4.1993771 -4.1960268 -4.1828666 -4.1638741 -4.1449876 -4.1273675 -4.1171169 -4.1241865 -4.1452627][-4.1691885 -4.1574812 -4.1623392 -4.1764655 -4.19321 -4.20311 -4.20809 -4.2081728 -4.1930652 -4.1602345 -4.1254568 -4.0988011 -4.0882125 -4.0980911 -4.1198][-4.1603103 -4.1514692 -4.1613917 -4.1830392 -4.2032046 -4.213479 -4.2139454 -4.2083611 -4.1903119 -4.1511474 -4.1097245 -4.0791821 -4.0659761 -4.0682297 -4.0818172][-4.1644707 -4.1637721 -4.1778884 -4.1982694 -4.2141442 -4.2166405 -4.20897 -4.1978583 -4.1794538 -4.1466074 -4.1096125 -4.0770841 -4.0541258 -4.0414248 -4.0430136][-4.1772518 -4.1814957 -4.194418 -4.2092667 -4.2168245 -4.2087474 -4.1927896 -4.1732516 -4.1536789 -4.1328936 -4.1103334 -4.082819 -4.053556 -4.0297356 -4.0199375][-4.1846771 -4.1865449 -4.1960826 -4.2083125 -4.2116547 -4.1953292 -4.1703668 -4.138422 -4.1139159 -4.1025796 -4.0968714 -4.0807271 -4.053741 -4.0299959 -4.02128][-4.1723423 -4.1749811 -4.1834335 -4.19751 -4.1995068 -4.1779613 -4.1449733 -4.1024947 -4.074687 -4.0725379 -4.0795555 -4.0779982 -4.062448 -4.0462394 -4.0440888][-4.1589556 -4.1748018 -4.1910558 -4.2037787 -4.2022066 -4.1767926 -4.1376424 -4.0930924 -4.0690532 -4.0698066 -4.0806937 -4.0887227 -4.0827923 -4.0714483 -4.0695548][-4.1508422 -4.1858 -4.2102494 -4.2211223 -4.2173662 -4.1929884 -4.1530585 -4.11241 -4.0936027 -4.0944786 -4.102747 -4.1147633 -4.1186614 -4.112 -4.1098628][-4.147594 -4.1930985 -4.2231483 -4.2341065 -4.2325497 -4.21318 -4.1780829 -4.1418743 -4.1281 -4.1340637 -4.1444559 -4.1540518 -4.1564107 -4.1497374 -4.1468062][-4.1567378 -4.2021646 -4.234405 -4.2480369 -4.247839 -4.2322445 -4.2020025 -4.1737704 -4.1680059 -4.1819172 -4.1936908 -4.1952715 -4.1872163 -4.175045 -4.1732745][-4.1722836 -4.2118835 -4.2421846 -4.2587872 -4.2641726 -4.2538977 -4.2285185 -4.2071342 -4.2058554 -4.2211266 -4.2303958 -4.2270265 -4.2124524 -4.2001581 -4.201231][-4.1912117 -4.2205486 -4.2481689 -4.2664552 -4.2761269 -4.2719564 -4.2552347 -4.2414565 -4.2420721 -4.2545118 -4.2601018 -4.2528615 -4.2363 -4.2255411 -4.2263002][-4.2222733 -4.2376451 -4.25702 -4.273509 -4.2847581 -4.2861357 -4.27832 -4.2718196 -4.2737551 -4.28153 -4.2828565 -4.2755647 -4.2632484 -4.2562079 -4.2576609][-4.2614636 -4.2653708 -4.2752261 -4.2855172 -4.2938209 -4.297349 -4.2955117 -4.2935891 -4.2958903 -4.3012404 -4.3027706 -4.29921 -4.2924805 -4.2888803 -4.2896767]]...]
INFO - root - 2017-12-07 11:17:31.130333: step 2610, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 63h:38m:47s remains)
INFO - root - 2017-12-07 11:17:37.828557: step 2620, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 64h:06m:17s remains)
INFO - root - 2017-12-07 11:17:44.684225: step 2630, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 62h:42m:42s remains)
INFO - root - 2017-12-07 11:17:51.591009: step 2640, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 61h:18m:22s remains)
INFO - root - 2017-12-07 11:17:58.433637: step 2650, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 65h:10m:56s remains)
INFO - root - 2017-12-07 11:18:05.218659: step 2660, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 64h:49m:39s remains)
INFO - root - 2017-12-07 11:18:12.045822: step 2670, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 58h:59m:22s remains)
INFO - root - 2017-12-07 11:18:18.882516: step 2680, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 60h:37m:35s remains)
INFO - root - 2017-12-07 11:18:25.611675: step 2690, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:40m:37s remains)
INFO - root - 2017-12-07 11:18:32.409603: step 2700, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 65h:00m:16s remains)
2017-12-07 11:18:33.094258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.206552 -4.2259274 -4.2546825 -4.283062 -4.2977576 -4.3003082 -4.301734 -4.3064857 -4.3082733 -4.3049574 -4.3011289 -4.2975087 -4.2958274 -4.2955403 -4.2954631][-4.2181191 -4.2344618 -4.2562647 -4.2795148 -4.2914424 -4.2929411 -4.2957067 -4.3005886 -4.3026428 -4.2996273 -4.2959981 -4.2926283 -4.2915735 -4.2914243 -4.2913976][-4.2563939 -4.265203 -4.2740045 -4.2827668 -4.28448 -4.2796659 -4.2789736 -4.2832165 -4.2867875 -4.2879047 -4.2890825 -4.2906208 -4.2936158 -4.2953272 -4.2946234][-4.2777977 -4.27878 -4.2758436 -4.27273 -4.2663393 -4.2561297 -4.2513628 -4.256197 -4.2637844 -4.2701287 -4.2774768 -4.2849865 -4.2922244 -4.29567 -4.2932386][-4.2593217 -4.2547855 -4.24564 -4.2337279 -4.2233448 -4.2125716 -4.2065783 -4.2141852 -4.2274718 -4.2384639 -4.2492867 -4.2598376 -4.2707515 -4.2752681 -4.2690945][-4.2125468 -4.2065797 -4.1950502 -4.1804366 -4.1708932 -4.1627913 -4.1575828 -4.1677213 -4.1883273 -4.2079425 -4.2253246 -4.2403946 -4.2542429 -4.2584186 -4.2464004][-4.15956 -4.1488042 -4.1296387 -4.1114645 -4.1007409 -4.0930223 -4.0871325 -4.0955873 -4.1220045 -4.157033 -4.187686 -4.2104621 -4.2280636 -4.2324963 -4.2166944][-4.0988646 -4.0815077 -4.0512757 -4.0266948 -4.01263 -4.0024719 -3.9915323 -3.9952195 -4.02678 -4.0791464 -4.1249456 -4.15449 -4.1736026 -4.1766672 -4.1607332][-4.0552783 -4.0327663 -4.0016246 -3.981617 -3.9690986 -3.9535389 -3.9328904 -3.9266362 -3.9558825 -4.0167656 -4.0725369 -4.1085658 -4.1291881 -4.1311531 -4.1157136][-4.0807462 -4.0641041 -4.0469036 -4.0424142 -4.0403695 -4.03249 -4.0176554 -4.0126157 -4.0341573 -4.0818372 -4.1267529 -4.1534915 -4.1643004 -4.159091 -4.1399903][-4.1559148 -4.1436028 -4.1317968 -4.130589 -4.1325612 -4.1318822 -4.1274233 -4.1304903 -4.14634 -4.176754 -4.2042656 -4.219686 -4.2216086 -4.2154331 -4.2017097][-4.2169361 -4.2104573 -4.1983528 -4.1900511 -4.1883006 -4.1886253 -4.1908035 -4.197154 -4.206778 -4.2228389 -4.2348928 -4.2373352 -4.2332125 -4.2281485 -4.2212319][-4.22251 -4.2191019 -4.2039847 -4.1863375 -4.178278 -4.1789041 -4.1834087 -4.1882238 -4.1915507 -4.2008319 -4.2051163 -4.199914 -4.1950388 -4.1960807 -4.1968789][-4.2028217 -4.1958942 -4.1741405 -4.1480036 -4.1339951 -4.1353507 -4.1443172 -4.1495385 -4.1496215 -4.155014 -4.1566806 -4.1539588 -4.1563873 -4.167623 -4.1810775][-4.198884 -4.1825018 -4.1556835 -4.129405 -4.117527 -4.1220279 -4.1345 -4.1397667 -4.1398644 -4.1450915 -4.1486731 -4.1508589 -4.1581817 -4.1758485 -4.195302]]...]
INFO - root - 2017-12-07 11:18:39.880781: step 2710, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.680 sec/batch; 62h:15m:20s remains)
INFO - root - 2017-12-07 11:18:46.515453: step 2720, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 65h:37m:00s remains)
INFO - root - 2017-12-07 11:18:53.197605: step 2730, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 63h:42m:12s remains)
INFO - root - 2017-12-07 11:19:00.053560: step 2740, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 62h:47m:38s remains)
INFO - root - 2017-12-07 11:19:06.867612: step 2750, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 59h:05m:50s remains)
INFO - root - 2017-12-07 11:19:13.720532: step 2760, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 66h:57m:03s remains)
INFO - root - 2017-12-07 11:19:20.585413: step 2770, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 66h:14m:19s remains)
INFO - root - 2017-12-07 11:19:27.382982: step 2780, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 63h:24m:29s remains)
INFO - root - 2017-12-07 11:19:34.231655: step 2790, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 57h:38m:33s remains)
INFO - root - 2017-12-07 11:19:41.005703: step 2800, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:51m:59s remains)
2017-12-07 11:19:41.703978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.137733 -4.1367149 -4.13431 -4.1370544 -4.1431675 -4.1400223 -4.1034212 -4.0519528 -4.031158 -4.0719485 -4.1439257 -4.2119131 -4.2632551 -4.3040056 -4.3251433][-4.1393733 -4.1217422 -4.1054683 -4.099556 -4.1065526 -4.1094618 -4.0844307 -4.03984 -4.0215588 -4.0655165 -4.1400976 -4.2071829 -4.2554846 -4.2955713 -4.3160973][-4.1760426 -4.1456046 -4.11125 -4.0868783 -4.0816488 -4.0873156 -4.078752 -4.048912 -4.0346837 -4.0820184 -4.156724 -4.2151327 -4.2585845 -4.2957668 -4.3146887][-4.2095647 -4.1739635 -4.1323733 -4.0943131 -4.0717683 -4.0622911 -4.0576334 -4.0371971 -4.028604 -4.08686 -4.17049 -4.2264829 -4.2646084 -4.3013072 -4.3195329][-4.2322373 -4.1965 -4.1475492 -4.0958519 -4.0502996 -4.01583 -4.0008287 -3.9841354 -3.9838793 -4.0576086 -4.1541662 -4.2166052 -4.2565737 -4.296351 -4.3171086][-4.2366095 -4.2016354 -4.1490335 -4.0784616 -4.0008969 -3.9477873 -3.9305573 -3.9170303 -3.9231954 -4.0155659 -4.1261578 -4.1988983 -4.2424369 -4.287323 -4.3136511][-4.2373514 -4.2001662 -4.1419272 -4.0558672 -3.9557834 -3.8938353 -3.8798485 -3.8658504 -3.880197 -3.9850581 -4.1057839 -4.1891718 -4.23623 -4.2824879 -4.3133831][-4.2318459 -4.1924434 -4.1312385 -4.0505996 -3.9532034 -3.8974926 -3.8837373 -3.8667326 -3.88341 -3.9847848 -4.1015806 -4.188395 -4.2400331 -4.28619 -4.316741][-4.2235155 -4.1815343 -4.1284385 -4.0776591 -4.0266218 -3.9883661 -3.9677088 -3.9359407 -3.9377432 -4.0147476 -4.1146431 -4.1951361 -4.2437553 -4.2870021 -4.3165417][-4.2007918 -4.1565852 -4.114418 -4.0933275 -4.0868659 -4.0735588 -4.0552249 -4.0173368 -4.0045972 -4.0584755 -4.1391144 -4.2073927 -4.2493987 -4.2869081 -4.3149333][-4.1643262 -4.1158934 -4.0840249 -4.0771279 -4.0913696 -4.0935097 -4.082233 -4.0532393 -4.0484595 -4.094727 -4.16224 -4.2200851 -4.25626 -4.2913895 -4.3174233][-4.1492147 -4.1000309 -4.0757842 -4.0792994 -4.0975542 -4.0953774 -4.0811496 -4.063364 -4.0745111 -4.1189628 -4.1759968 -4.2253356 -4.2590923 -4.2962904 -4.3222475][-4.1576037 -4.1130543 -4.0960412 -4.1073551 -4.1213641 -4.1077352 -4.0871897 -4.075767 -4.0976992 -4.1408548 -4.1904845 -4.2307572 -4.2599096 -4.2991643 -4.3256941][-4.1707907 -4.1246328 -4.1082158 -4.1217632 -4.1325955 -4.1227288 -4.1016221 -4.0939417 -4.118309 -4.1571622 -4.2024117 -4.2353072 -4.2599 -4.2987404 -4.32568][-4.15786 -4.1208062 -4.107749 -4.1195593 -4.1354318 -4.140738 -4.1292305 -4.1225142 -4.1439767 -4.1761742 -4.2145061 -4.2440453 -4.2635407 -4.2987442 -4.32513]]...]
INFO - root - 2017-12-07 11:19:48.348159: step 2810, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 61h:07m:32s remains)
INFO - root - 2017-12-07 11:19:55.034270: step 2820, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 62h:13m:47s remains)
INFO - root - 2017-12-07 11:20:01.794395: step 2830, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:32m:56s remains)
INFO - root - 2017-12-07 11:20:08.663707: step 2840, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 65h:28m:21s remains)
INFO - root - 2017-12-07 11:20:15.534300: step 2850, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 67h:34m:57s remains)
INFO - root - 2017-12-07 11:20:22.424280: step 2860, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 63h:53m:30s remains)
INFO - root - 2017-12-07 11:20:29.253486: step 2870, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 58h:09m:28s remains)
INFO - root - 2017-12-07 11:20:36.115624: step 2880, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:53m:17s remains)
INFO - root - 2017-12-07 11:20:42.944169: step 2890, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 64h:14m:54s remains)
INFO - root - 2017-12-07 11:20:49.785075: step 2900, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 64h:12m:12s remains)
2017-12-07 11:20:50.540651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2812271 -4.2878852 -4.2893391 -4.2840724 -4.272934 -4.2495761 -4.2113419 -4.1803 -4.1722531 -4.1913342 -4.2306886 -4.2671251 -4.2783084 -4.2635274 -4.2399249][-4.2819738 -4.280767 -4.2704277 -4.2596817 -4.2539592 -4.2421646 -4.2143974 -4.1896296 -4.1848059 -4.2083449 -4.2510133 -4.287066 -4.2964158 -4.2780881 -4.2486749][-4.2879319 -4.277966 -4.2578449 -4.2466769 -4.2502313 -4.2516565 -4.2355247 -4.213429 -4.2015944 -4.2177839 -4.2570133 -4.2894468 -4.296195 -4.2771559 -4.2475314][-4.2948728 -4.2794728 -4.2568984 -4.2495956 -4.2590017 -4.2670569 -4.2596354 -4.2395997 -4.2201447 -4.2242966 -4.2538767 -4.2784328 -4.2807217 -4.2628336 -4.2352591][-4.29608 -4.2823033 -4.2621355 -4.2590737 -4.2681432 -4.2754979 -4.2715058 -4.255003 -4.2324657 -4.2263956 -4.2437429 -4.2604656 -4.2580557 -4.2400279 -4.2148919][-4.293612 -4.2873197 -4.2744961 -4.2745013 -4.2802348 -4.284317 -4.281436 -4.26873 -4.2481751 -4.2365251 -4.2431922 -4.2524691 -4.2445211 -4.2228141 -4.1990309][-4.2908449 -4.2918868 -4.2857852 -4.2877259 -4.2903748 -4.2904081 -4.2877154 -4.2791286 -4.2650695 -4.2578712 -4.2624702 -4.2668953 -4.2536573 -4.22673 -4.2007418][-4.2845364 -4.2933645 -4.291173 -4.2876978 -4.2811842 -4.275671 -4.2745237 -4.269486 -4.2626839 -4.2649431 -4.2731819 -4.2771182 -4.2624054 -4.2350588 -4.2100639][-4.2680917 -4.2896142 -4.2912478 -4.2780623 -4.2590065 -4.2475448 -4.2471066 -4.2438188 -4.2396431 -4.2447748 -4.2533636 -4.2570472 -4.2450519 -4.2262907 -4.2125154][-4.2436719 -4.2798767 -4.2859697 -4.2645769 -4.2349319 -4.2186337 -4.2195678 -4.2193151 -4.2127724 -4.2135606 -4.2166629 -4.2185946 -4.2128525 -4.20539 -4.2070961][-4.2312217 -4.2755384 -4.2836523 -4.2581873 -4.2235589 -4.2040768 -4.2057757 -4.2055092 -4.1918454 -4.1821051 -4.1761932 -4.1745009 -4.1718364 -4.1761856 -4.1932182][-4.2473211 -4.2867618 -4.2902331 -4.2633014 -4.2304287 -4.2117052 -4.2123122 -4.2109179 -4.1924663 -4.1726427 -4.1547418 -4.1422343 -4.1381917 -4.1516962 -4.1769009][-4.27209 -4.29469 -4.2889867 -4.2621155 -4.2371879 -4.2265186 -4.2305231 -4.2304773 -4.213789 -4.1915526 -4.1639419 -4.139452 -4.1315041 -4.1482358 -4.1693478][-4.2789946 -4.2833433 -4.272234 -4.2522645 -4.2411113 -4.2418551 -4.249835 -4.2512112 -4.2401018 -4.2208967 -4.1912379 -4.1607876 -4.1536446 -4.1679831 -4.1752882][-4.2633581 -4.2558165 -4.246798 -4.239758 -4.24297 -4.2508326 -4.2594266 -4.263123 -4.2593293 -4.2459583 -4.2187657 -4.1885757 -4.1839237 -4.1942534 -4.1895027]]...]
INFO - root - 2017-12-07 11:20:57.261805: step 2910, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:59m:40s remains)
INFO - root - 2017-12-07 11:21:03.750290: step 2920, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 62h:41m:55s remains)
INFO - root - 2017-12-07 11:21:10.706402: step 2930, loss = 2.10, batch loss = 2.05 (11.2 examples/sec; 0.716 sec/batch; 65h:34m:10s remains)
INFO - root - 2017-12-07 11:21:17.449552: step 2940, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 62h:31m:20s remains)
INFO - root - 2017-12-07 11:21:24.248175: step 2950, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 59h:43m:36s remains)
INFO - root - 2017-12-07 11:21:31.177416: step 2960, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.682 sec/batch; 62h:24m:13s remains)
INFO - root - 2017-12-07 11:21:37.942528: step 2970, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 62h:43m:25s remains)
INFO - root - 2017-12-07 11:21:44.695935: step 2980, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 64h:39m:47s remains)
INFO - root - 2017-12-07 11:21:51.535572: step 2990, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 59h:15m:57s remains)
INFO - root - 2017-12-07 11:21:58.436960: step 3000, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 58h:22m:27s remains)
2017-12-07 11:21:59.249931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1921763 -4.1850424 -4.1879954 -4.1989918 -4.2020469 -4.19897 -4.1926379 -4.1802988 -4.166501 -4.1552958 -4.1484823 -4.1542087 -4.17334 -4.1927958 -4.212697][-4.17058 -4.1694059 -4.1811442 -4.1975942 -4.1991653 -4.1909943 -4.1795497 -4.1647758 -4.1508718 -4.1327677 -4.1157207 -4.1160612 -4.1349277 -4.1563268 -4.1802521][-4.1568494 -4.1611166 -4.1777844 -4.1950464 -4.1933208 -4.1761432 -4.1554837 -4.1399927 -4.1249809 -4.0978618 -4.0689144 -4.068861 -4.0915275 -4.1198215 -4.1508474][-4.1330452 -4.1373816 -4.1535988 -4.1656995 -4.154665 -4.1283092 -4.1045513 -4.0931368 -4.0784936 -4.0423727 -4.0106931 -4.01877 -4.0530772 -4.0943503 -4.1370487][-4.0914145 -4.089057 -4.096468 -4.0970469 -4.0769253 -4.04673 -4.0283618 -4.0309367 -4.026073 -3.9910402 -3.9633904 -3.9821427 -4.0278053 -4.0779047 -4.1281652][-4.0431085 -4.0304236 -4.0262556 -4.0115228 -3.9836459 -3.9567382 -3.9540443 -3.9750473 -3.9849913 -3.96306 -3.9424064 -3.9605265 -4.007009 -4.0579305 -4.1042476][-4.0013828 -3.9799943 -3.9647102 -3.9404418 -3.9063659 -3.8828506 -3.8919742 -3.9263051 -3.9514923 -3.950424 -3.9332991 -3.9390001 -3.9756284 -4.0197887 -4.060288][-3.9698913 -3.9460051 -3.9255567 -3.9003463 -3.8681364 -3.8528004 -3.8692265 -3.9054906 -3.9362521 -3.9470692 -3.930733 -3.9266391 -3.9508951 -3.9870346 -4.0250187][-3.9748135 -3.9540255 -3.9355831 -3.9143314 -3.8940046 -3.888278 -3.9026523 -3.9280324 -3.9481895 -3.9572349 -3.9436648 -3.9384062 -3.9594152 -3.9935946 -4.0334196][-4.0336962 -4.0183225 -4.0037942 -3.9893582 -3.9789824 -3.9769597 -3.9832189 -3.9943736 -4.0004921 -4.003808 -3.9961457 -3.9955814 -4.0158563 -4.0492697 -4.088665][-4.1209388 -4.1142488 -4.1051068 -4.0940833 -4.0858831 -4.0814719 -4.0820122 -4.0875359 -4.0899429 -4.0924721 -4.0889683 -4.091146 -4.1076465 -4.1350956 -4.1668549][-4.2037334 -4.2040987 -4.2008319 -4.1938152 -4.1869473 -4.1810441 -4.1795826 -4.1836462 -4.1878114 -4.1914082 -4.190896 -4.1921406 -4.2008967 -4.2180958 -4.2389827][-4.2690864 -4.272368 -4.2721353 -4.2690768 -4.2650638 -4.2611065 -4.2587271 -4.2595649 -4.2624044 -4.2643738 -4.2643733 -4.2648487 -4.2688932 -4.2774711 -4.2896085][-4.3071465 -4.3097649 -4.3103895 -4.3095083 -4.3081818 -4.3065004 -4.3043485 -4.3033333 -4.30419 -4.305356 -4.3062119 -4.3072362 -4.3103 -4.31574 -4.3219781][-4.3267026 -4.3279204 -4.328001 -4.3280978 -4.3281045 -4.3277512 -4.3269868 -4.326756 -4.3271456 -4.3281274 -4.32946 -4.330987 -4.3333197 -4.3362713 -4.338881]]...]
INFO - root - 2017-12-07 11:22:05.995958: step 3010, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 66h:08m:20s remains)
INFO - root - 2017-12-07 11:22:12.564484: step 3020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 59h:19m:46s remains)
INFO - root - 2017-12-07 11:22:19.426791: step 3030, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 63h:47m:57s remains)
INFO - root - 2017-12-07 11:22:26.219376: step 3040, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 63h:40m:04s remains)
INFO - root - 2017-12-07 11:22:32.997527: step 3050, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 57h:31m:10s remains)
INFO - root - 2017-12-07 11:22:39.875103: step 3060, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 59h:48m:24s remains)
INFO - root - 2017-12-07 11:22:46.633563: step 3070, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:39m:22s remains)
INFO - root - 2017-12-07 11:22:53.530140: step 3080, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 64h:07m:13s remains)
INFO - root - 2017-12-07 11:23:00.326053: step 3090, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 61h:50m:59s remains)
INFO - root - 2017-12-07 11:23:07.156837: step 3100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 61h:17m:13s remains)
2017-12-07 11:23:07.856832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2368298 -4.2300477 -4.2255645 -4.2232795 -4.2177525 -4.2078452 -4.1920085 -4.1741815 -4.1600566 -4.1542416 -4.1555753 -4.149951 -4.1309376 -4.1110215 -4.0959978][-4.2395406 -4.2426291 -4.244894 -4.2463813 -4.2417493 -4.2296419 -4.2060132 -4.179028 -4.1572971 -4.1473851 -4.1470714 -4.1390839 -4.1201348 -4.1019273 -4.090992][-4.2406588 -4.24848 -4.2538404 -4.2571568 -4.2527809 -4.2399526 -4.214314 -4.1802335 -4.148706 -4.1322579 -4.126893 -4.1180563 -4.1023006 -4.0882406 -4.0831432][-4.2479939 -4.2561555 -4.2629218 -4.265626 -4.2571449 -4.2377448 -4.2083812 -4.1690702 -4.1300893 -4.1102276 -4.1044641 -4.098237 -4.0843468 -4.0703321 -4.067523][-4.2650547 -4.2675462 -4.2686806 -4.2639594 -4.2465634 -4.2195497 -4.1834059 -4.1381426 -4.0977974 -4.0826664 -4.0849152 -4.0861349 -4.0761852 -4.0652361 -4.0647869][-4.2819018 -4.2762346 -4.2671356 -4.249054 -4.2212472 -4.1855326 -4.1385622 -4.0857406 -4.0494804 -4.0465307 -4.0626516 -4.0744896 -4.0708003 -4.0643711 -4.0674987][-4.2889485 -4.2772312 -4.2575088 -4.2258749 -4.1867418 -4.138814 -4.0770164 -4.0148878 -3.9854486 -4.0014129 -4.0372567 -4.062068 -4.0665874 -4.0646114 -4.0701337][-4.2877927 -4.2694087 -4.2395849 -4.198679 -4.1540928 -4.1016388 -4.0366964 -3.9790761 -3.9653041 -3.996393 -4.0397921 -4.0679684 -4.0762115 -4.0783381 -4.0869079][-4.28522 -4.2589927 -4.2209153 -4.1767116 -4.1360693 -4.0940447 -4.0463443 -4.01585 -4.0219555 -4.0513153 -4.0806642 -4.0946865 -4.0967669 -4.0992241 -4.10924][-4.2817516 -4.2491355 -4.2071128 -4.1630907 -4.1296444 -4.0992579 -4.0693879 -4.0589089 -4.0726533 -4.0956035 -4.1111979 -4.1136389 -4.1095238 -4.1107783 -4.1208773][-4.2766051 -4.2406359 -4.1986866 -4.1588292 -4.1306005 -4.1056638 -4.0856194 -4.0812125 -4.0942864 -4.1147814 -4.1271715 -4.1269474 -4.12243 -4.1237216 -4.1317906][-4.2710652 -4.2344337 -4.1950865 -4.1609344 -4.1364532 -4.1154475 -4.100265 -4.0960431 -4.10626 -4.1255808 -4.1384354 -4.1406269 -4.138411 -4.1405163 -4.1449704][-4.2653584 -4.2295494 -4.1955137 -4.1674676 -4.1460013 -4.1293278 -4.1154418 -4.1089964 -4.1169462 -4.1337552 -4.1472349 -4.1530714 -4.1576695 -4.1638441 -4.1667252][-4.2538214 -4.2191353 -4.1913095 -4.1681032 -4.149971 -4.1370764 -4.1256585 -4.1228533 -4.131916 -4.1445346 -4.1556239 -4.1623182 -4.1698871 -4.1785583 -4.1799545][-4.2290726 -4.196599 -4.1750889 -4.1582475 -4.1460471 -4.1371145 -4.1290803 -4.1334248 -4.1465712 -4.1569629 -4.1635594 -4.1654077 -4.1709504 -4.1780138 -4.1789041]]...]
INFO - root - 2017-12-07 11:23:14.591858: step 3110, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 63h:43m:02s remains)
INFO - root - 2017-12-07 11:23:21.261993: step 3120, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 61h:34m:27s remains)
INFO - root - 2017-12-07 11:23:28.110445: step 3130, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 59h:31m:49s remains)
INFO - root - 2017-12-07 11:23:35.048621: step 3140, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 65h:15m:45s remains)
INFO - root - 2017-12-07 11:23:41.870865: step 3150, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.724 sec/batch; 66h:12m:43s remains)
INFO - root - 2017-12-07 11:23:48.601126: step 3160, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 60h:02m:12s remains)
INFO - root - 2017-12-07 11:23:55.478860: step 3170, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 59h:19m:49s remains)
INFO - root - 2017-12-07 11:24:02.403856: step 3180, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 65h:29m:38s remains)
INFO - root - 2017-12-07 11:24:09.232745: step 3190, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 62h:47m:48s remains)
INFO - root - 2017-12-07 11:24:16.051892: step 3200, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 62h:40m:11s remains)
2017-12-07 11:24:16.771380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3181238 -4.3153453 -4.3147864 -4.3082819 -4.2977161 -4.2915096 -4.2890806 -4.2863941 -4.2816839 -4.2778072 -4.2789907 -4.2869043 -4.2988863 -4.30746 -4.3086133][-4.3006434 -4.2989206 -4.2992368 -4.288846 -4.2745781 -4.2685432 -4.2666306 -4.2644053 -4.2606807 -4.254611 -4.2528973 -4.2626405 -4.2773952 -4.2849393 -4.2784266][-4.2803745 -4.2769341 -4.2769394 -4.265141 -4.2512441 -4.2442122 -4.2397494 -4.235826 -4.2346516 -4.2281671 -4.2214885 -4.228992 -4.2426667 -4.2467909 -4.2303472][-4.2590451 -4.2495656 -4.2443876 -4.2305961 -4.2170329 -4.2075386 -4.2028828 -4.2023139 -4.2079639 -4.2026992 -4.1922693 -4.1958742 -4.2084107 -4.2110314 -4.1873059][-4.2399139 -4.2235217 -4.2117639 -4.195044 -4.1772504 -4.1548114 -4.1407824 -4.1480746 -4.1689553 -4.1707811 -4.1604023 -4.1657743 -4.1851249 -4.1985431 -4.181581][-4.2266579 -4.2032647 -4.1842146 -4.1645346 -4.1360908 -4.0884504 -4.0496144 -4.0538216 -4.090035 -4.1100674 -4.1142979 -4.1313825 -4.166429 -4.1977086 -4.1981049][-4.2213459 -4.1934266 -4.165978 -4.14044 -4.0957088 -4.017282 -3.9399481 -3.9260194 -3.9719074 -4.015048 -4.0480795 -4.0964222 -4.1551585 -4.2028837 -4.2209368][-4.2190862 -4.1922464 -4.1621132 -4.1383262 -4.0891671 -3.9858596 -3.875495 -3.8433468 -3.8895204 -3.947912 -4.0054417 -4.07469 -4.1397295 -4.1847 -4.2082214][-4.2226729 -4.1969914 -4.1671815 -4.15094 -4.1118116 -4.0153656 -3.9074576 -3.874383 -3.9128406 -3.9701107 -4.02202 -4.0789752 -4.1277013 -4.1574988 -4.1743779][-4.2279735 -4.2050352 -4.1798964 -4.1752458 -4.152873 -4.0799437 -3.9917734 -3.9576125 -3.97679 -4.0139723 -4.0491104 -4.0932417 -4.1274323 -4.1419353 -4.1465316][-4.2276754 -4.2086105 -4.1950507 -4.2043142 -4.1957016 -4.1462994 -4.0767331 -4.0369396 -4.0388031 -4.0544462 -4.07019 -4.1031365 -4.1290059 -4.1343122 -4.1311178][-4.2236886 -4.2051287 -4.2025466 -4.2226453 -4.2257018 -4.1936049 -4.1402583 -4.098536 -4.0879769 -4.0887113 -4.0902925 -4.1142311 -4.1351628 -4.1353211 -4.1287637][-4.227273 -4.2074246 -4.2102847 -4.235498 -4.2450275 -4.22606 -4.1904135 -4.1581588 -4.1448231 -4.1391053 -4.1334152 -4.1463127 -4.16016 -4.1588955 -4.1505313][-4.2469349 -4.2267189 -4.2285795 -4.2518139 -4.2650948 -4.2587204 -4.2376213 -4.21512 -4.2029381 -4.1966186 -4.1917148 -4.1996484 -4.2091546 -4.2096786 -4.2044868][-4.2738628 -4.2559271 -4.2547536 -4.2710085 -4.2832403 -4.2824759 -4.2708178 -4.2538648 -4.2430263 -4.2387204 -4.237792 -4.2499728 -4.2619276 -4.2673335 -4.2693052]]...]
INFO - root - 2017-12-07 11:24:23.524412: step 3210, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 63h:19m:06s remains)
INFO - root - 2017-12-07 11:24:30.279582: step 3220, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 67h:08m:01s remains)
INFO - root - 2017-12-07 11:24:37.076339: step 3230, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 59h:59m:05s remains)
INFO - root - 2017-12-07 11:24:43.902806: step 3240, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 59h:07m:43s remains)
INFO - root - 2017-12-07 11:24:50.720627: step 3250, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 62h:58m:08s remains)
INFO - root - 2017-12-07 11:24:57.610986: step 3260, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 66h:34m:25s remains)
INFO - root - 2017-12-07 11:25:04.458000: step 3270, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 62h:46m:33s remains)
INFO - root - 2017-12-07 11:25:11.271624: step 3280, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 56h:55m:13s remains)
INFO - root - 2017-12-07 11:25:17.991427: step 3290, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 58h:19m:34s remains)
INFO - root - 2017-12-07 11:25:24.853660: step 3300, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 65h:56m:45s remains)
2017-12-07 11:25:25.588447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3199086 -4.3055053 -4.2970991 -4.2966957 -4.2983751 -4.299365 -4.2974362 -4.2951584 -4.2880282 -4.2794614 -4.276732 -4.2856555 -4.301218 -4.3049 -4.2904148][-4.3166394 -4.3029757 -4.2958846 -4.2932754 -4.2907591 -4.2882957 -4.2842507 -4.2843127 -4.28605 -4.27825 -4.2682667 -4.2716274 -4.2878895 -4.2915196 -4.2685189][-4.310133 -4.2975793 -4.2922258 -4.2839851 -4.27272 -4.2638021 -4.2554636 -4.256628 -4.2653213 -4.2606659 -4.2486205 -4.2489052 -4.2599449 -4.2563448 -4.2283754][-4.3062248 -4.2932386 -4.2876673 -4.2711825 -4.2504234 -4.2289906 -4.205308 -4.1971655 -4.2079782 -4.2152777 -4.2171926 -4.2230659 -4.2285218 -4.2150927 -4.1866322][-4.30389 -4.2836976 -4.2686324 -4.2438178 -4.2130055 -4.1736116 -4.1197987 -4.0919018 -4.1142831 -4.1522717 -4.1852427 -4.20572 -4.2049417 -4.1799011 -4.1517487][-4.2950125 -4.2662487 -4.24132 -4.2095532 -4.1653619 -4.096755 -3.9964824 -3.9294066 -3.9763143 -4.067812 -4.1419463 -4.1842275 -4.1873012 -4.1593685 -4.1334267][-4.2832317 -4.2480178 -4.2134414 -4.1723852 -4.1142974 -4.0168395 -3.8629336 -3.7379885 -3.8138437 -3.9711819 -4.0875878 -4.1500077 -4.1644511 -4.1442447 -4.1230121][-4.2762928 -4.2387781 -4.1974759 -4.1500797 -4.0866375 -3.9924614 -3.8552845 -3.7427945 -3.813308 -3.9662561 -4.0825176 -4.1489582 -4.1675262 -4.1492176 -4.1292915][-4.2831244 -4.2504706 -4.2061205 -4.1564655 -4.0962348 -4.0333719 -3.9739053 -3.9386473 -3.9840133 -4.0703125 -4.1416926 -4.184844 -4.1963744 -4.175889 -4.1512733][-4.2893143 -4.2613869 -4.2197142 -4.1735053 -4.1232629 -4.0913219 -4.0829177 -4.0931807 -4.1282592 -4.1709371 -4.2040629 -4.2248707 -4.22326 -4.19683 -4.1687145][-4.2946095 -4.2734804 -4.2400784 -4.2028427 -4.16722 -4.1573982 -4.1686463 -4.1929975 -4.2216716 -4.2423458 -4.2517362 -4.2533617 -4.2414923 -4.2142324 -4.1887751][-4.304821 -4.2910061 -4.2704926 -4.2461672 -4.22723 -4.2284155 -4.242312 -4.26153 -4.2806511 -4.2908177 -4.2891154 -4.2814107 -4.2664485 -4.244154 -4.2251649][-4.3209004 -4.3102913 -4.2994933 -4.287796 -4.2827153 -4.2886305 -4.2986474 -4.3094964 -4.3202014 -4.3235731 -4.3169255 -4.308558 -4.29693 -4.2821984 -4.2723722][-4.3314366 -4.3217974 -4.3155861 -4.3123932 -4.3136563 -4.3187165 -4.3229847 -4.3268209 -4.3302979 -4.3303766 -4.3263192 -4.3230476 -4.3172398 -4.3090177 -4.3045764][-4.3370328 -4.3254957 -4.3192353 -4.3187141 -4.321281 -4.3238745 -4.3252292 -4.3261466 -4.3253927 -4.3234692 -4.322474 -4.3228555 -4.3220587 -4.320653 -4.3210392]]...]
INFO - root - 2017-12-07 11:25:32.364168: step 3310, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 62h:29m:12s remains)
INFO - root - 2017-12-07 11:25:39.021580: step 3320, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 63h:08m:48s remains)
INFO - root - 2017-12-07 11:25:45.845630: step 3330, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 66h:11m:49s remains)
INFO - root - 2017-12-07 11:25:52.692266: step 3340, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 66h:05m:47s remains)
INFO - root - 2017-12-07 11:25:59.292674: step 3350, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 61h:28m:43s remains)
INFO - root - 2017-12-07 11:26:06.084097: step 3360, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 60h:09m:44s remains)
INFO - root - 2017-12-07 11:26:12.858983: step 3370, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 63h:40m:44s remains)
INFO - root - 2017-12-07 11:26:19.820747: step 3380, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 66h:45m:27s remains)
INFO - root - 2017-12-07 11:26:26.621535: step 3390, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 61h:42m:12s remains)
INFO - root - 2017-12-07 11:26:33.416927: step 3400, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 57h:41m:54s remains)
2017-12-07 11:26:34.097126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1152854 -4.1230474 -4.1513367 -4.1871667 -4.2085047 -4.2072949 -4.1972456 -4.1931572 -4.2024837 -4.2197237 -4.2392931 -4.2528987 -4.2538667 -4.251174 -4.2533617][-4.0689397 -4.0949335 -4.1363831 -4.1832438 -4.2099047 -4.2123 -4.2080441 -4.205689 -4.2101994 -4.2232151 -4.2394414 -4.2493453 -4.2481251 -4.2404089 -4.2330174][-4.0612793 -4.0988846 -4.1514778 -4.2033734 -4.2331095 -4.2368546 -4.236557 -4.2317886 -4.2263193 -4.2304096 -4.23775 -4.2422194 -4.2395463 -4.226912 -4.2052813][-4.0912681 -4.1344991 -4.1889811 -4.23357 -4.2539005 -4.2538781 -4.2499919 -4.2409453 -4.2281804 -4.2277508 -4.2333817 -4.2385383 -4.2403364 -4.2267318 -4.1948638][-4.1415863 -4.1788788 -4.2220545 -4.2527709 -4.2634468 -4.2552238 -4.2422314 -4.2278256 -4.2125936 -4.2161918 -4.22758 -4.2357841 -4.2395568 -4.2262473 -4.1966157][-4.1780653 -4.2043715 -4.2334628 -4.2517252 -4.2521639 -4.2356997 -4.2098022 -4.1804643 -4.1579051 -4.1676435 -4.1899781 -4.2067871 -4.2170219 -4.21292 -4.1980815][-4.18282 -4.1996961 -4.2148433 -4.2211385 -4.2136121 -4.1856213 -4.1424603 -4.0908184 -4.0570593 -4.0760121 -4.124126 -4.1656165 -4.1956511 -4.2067871 -4.208323][-4.171803 -4.1753263 -4.17378 -4.167069 -4.1517019 -4.1165509 -4.0616832 -3.9863374 -3.9362872 -3.9823866 -4.0776129 -4.1542187 -4.2030325 -4.2251287 -4.2364326][-4.1802478 -4.1725645 -4.1586447 -4.145062 -4.127986 -4.0955544 -4.0405655 -3.9705062 -3.9373004 -4.0076914 -4.1148014 -4.1918607 -4.2369356 -4.2546535 -4.2609763][-4.1913495 -4.1790094 -4.1625848 -4.1531663 -4.1454668 -4.1284304 -4.1020045 -4.0768971 -4.0716777 -4.1192293 -4.1884665 -4.2342634 -4.2541046 -4.257 -4.2545724][-4.1961031 -4.1815 -4.169241 -4.1730804 -4.1800375 -4.1764669 -4.1727691 -4.1736975 -4.175725 -4.1989913 -4.2328606 -4.248127 -4.2450409 -4.2354927 -4.228651][-4.1992121 -4.17795 -4.1678863 -4.1792364 -4.1945438 -4.1971307 -4.200017 -4.2102418 -4.2182975 -4.2331095 -4.2496624 -4.2461796 -4.223639 -4.2046585 -4.1948147][-4.2065282 -4.1801629 -4.1650596 -4.17166 -4.189043 -4.1946416 -4.2012239 -4.2141843 -4.2258 -4.2369041 -4.2458186 -4.2308583 -4.1975727 -4.1707444 -4.1604962][-4.200532 -4.172967 -4.1538115 -4.1585846 -4.1731615 -4.1774073 -4.1835022 -4.1981306 -4.2097921 -4.219624 -4.2264352 -4.2129459 -4.1803775 -4.1495886 -4.1373982][-4.1983929 -4.1751757 -4.1633258 -4.1674566 -4.171577 -4.1669507 -4.1675115 -4.1756387 -4.1822562 -4.1949811 -4.2074385 -4.2004304 -4.1758647 -4.1483927 -4.1381984]]...]
INFO - root - 2017-12-07 11:26:40.880163: step 3410, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 64h:00m:12s remains)
INFO - root - 2017-12-07 11:26:47.570805: step 3420, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 60h:29m:49s remains)
INFO - root - 2017-12-07 11:26:54.322479: step 3430, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 60h:19m:10s remains)
INFO - root - 2017-12-07 11:27:01.121968: step 3440, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 59h:04m:41s remains)
INFO - root - 2017-12-07 11:27:07.818005: step 3450, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 63h:41m:31s remains)
INFO - root - 2017-12-07 11:27:14.590870: step 3460, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 62h:35m:50s remains)
INFO - root - 2017-12-07 11:27:21.357645: step 3470, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 62h:21m:51s remains)
INFO - root - 2017-12-07 11:27:28.062444: step 3480, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 59h:20m:26s remains)
INFO - root - 2017-12-07 11:27:34.862914: step 3490, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 63h:09m:41s remains)
INFO - root - 2017-12-07 11:27:41.635256: step 3500, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.677 sec/batch; 61h:50m:33s remains)
2017-12-07 11:27:42.307972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3417611 -4.3448296 -4.3466163 -4.3463416 -4.3440757 -4.3421869 -4.3425155 -4.34288 -4.3449831 -4.3488364 -4.3525076 -4.3562131 -4.3568578 -4.3543377 -4.351716][-4.3377771 -4.3376422 -4.3377328 -4.3367586 -4.3334579 -4.3296866 -4.3287292 -4.3294873 -4.334414 -4.3399954 -4.3446903 -4.3492951 -4.349834 -4.3456378 -4.3429985][-4.3280764 -4.3249454 -4.3227148 -4.3208156 -4.3168292 -4.3115759 -4.308073 -4.3076715 -4.313108 -4.3174529 -4.319633 -4.3238859 -4.32639 -4.325 -4.3274608][-4.3209085 -4.314858 -4.3079271 -4.3021464 -4.2948318 -4.2854648 -4.2783065 -4.2741904 -4.27629 -4.277163 -4.27718 -4.2810545 -4.2868705 -4.2916441 -4.3027763][-4.3139863 -4.3041511 -4.2910032 -4.2768755 -4.2637553 -4.2497125 -4.2385378 -4.23172 -4.2299175 -4.2299995 -4.2317762 -4.2395215 -4.2489591 -4.2576523 -4.2746983][-4.3047943 -4.2921915 -4.2727079 -4.2506566 -4.232388 -4.2149816 -4.202858 -4.2002106 -4.195538 -4.1920943 -4.1962547 -4.2122793 -4.2266369 -4.2365651 -4.2540007][-4.2945509 -4.2801991 -4.2576542 -4.2322507 -4.2078605 -4.181354 -4.1680617 -4.1725311 -4.1706057 -4.1649814 -4.1705351 -4.1931272 -4.2116961 -4.2222395 -4.2399645][-4.2914305 -4.2768559 -4.2532306 -4.2261825 -4.1923103 -4.1501713 -4.1320491 -4.1413455 -4.1504164 -4.1559339 -4.1678257 -4.1955123 -4.2159314 -4.2246342 -4.2381353][-4.28954 -4.2776055 -4.2553468 -4.2217774 -4.173552 -4.1204224 -4.1029935 -4.1184912 -4.1425934 -4.1666517 -4.1901903 -4.215838 -4.232336 -4.2352667 -4.24102][-4.2900853 -4.2854533 -4.2628894 -4.2205076 -4.1612887 -4.1083522 -4.1004333 -4.1255045 -4.1604648 -4.1924205 -4.2188983 -4.2407365 -4.2516336 -4.2466121 -4.2459054][-4.296587 -4.3014445 -4.2840719 -4.2434483 -4.187397 -4.1420331 -4.1398277 -4.1682229 -4.1995196 -4.2248483 -4.2450242 -4.2607317 -4.2676411 -4.2590952 -4.2539158][-4.3096323 -4.3231568 -4.3165383 -4.2897983 -4.2479558 -4.2122717 -4.2085247 -4.2278495 -4.247942 -4.2650218 -4.275948 -4.2811551 -4.2810454 -4.2681365 -4.2585959][-4.310926 -4.3279533 -4.3303118 -4.319468 -4.2946649 -4.2702971 -4.2622061 -4.2718282 -4.2833548 -4.2935066 -4.2976604 -4.2941427 -4.2873683 -4.2702365 -4.2553015][-4.297987 -4.3128662 -4.3179116 -4.3157578 -4.3029542 -4.285821 -4.2757549 -4.2793107 -4.288857 -4.2963552 -4.297565 -4.2903223 -4.27931 -4.2595344 -4.2441444][-4.2740703 -4.2830086 -4.2858295 -4.286263 -4.2795253 -4.2678623 -4.2575483 -4.2562685 -4.2653766 -4.2720842 -4.2725992 -4.2668452 -4.2566504 -4.2405863 -4.2306023]]...]
INFO - root - 2017-12-07 11:27:48.998488: step 3510, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 58h:36m:00s remains)
INFO - root - 2017-12-07 11:27:55.636838: step 3520, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 60h:27m:57s remains)
INFO - root - 2017-12-07 11:28:02.385993: step 3530, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 64h:40m:38s remains)
INFO - root - 2017-12-07 11:28:09.271458: step 3540, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 64h:56m:04s remains)
INFO - root - 2017-12-07 11:28:16.032909: step 3550, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 62h:36m:55s remains)
INFO - root - 2017-12-07 11:28:22.800957: step 3560, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:51m:29s remains)
INFO - root - 2017-12-07 11:28:29.687674: step 3570, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 62h:12m:36s remains)
INFO - root - 2017-12-07 11:28:36.672674: step 3580, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 61h:47m:31s remains)
INFO - root - 2017-12-07 11:28:43.420212: step 3590, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 61h:39m:56s remains)
INFO - root - 2017-12-07 11:28:50.148794: step 3600, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 63h:39m:18s remains)
2017-12-07 11:28:50.984239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2433271 -4.2341847 -4.2199531 -4.1860938 -4.1496239 -4.1347513 -4.1438227 -4.167088 -4.1864448 -4.206893 -4.2137265 -4.1994877 -4.1671371 -4.130197 -4.1034355][-4.2273912 -4.228539 -4.2288427 -4.2039862 -4.1632075 -4.1386261 -4.1450043 -4.1702404 -4.18873 -4.2095175 -4.2193108 -4.2100263 -4.183321 -4.148387 -4.1165648][-4.2161446 -4.2289791 -4.2430425 -4.2322507 -4.1938853 -4.16013 -4.1609864 -4.1860461 -4.2069507 -4.2263064 -4.2383504 -4.2352676 -4.2181644 -4.190443 -4.1580839][-4.1986928 -4.2183642 -4.2379575 -4.2409739 -4.2117004 -4.1756 -4.172894 -4.1962023 -4.2189941 -4.2374344 -4.2515068 -4.255033 -4.2477059 -4.2279181 -4.1986933][-4.17077 -4.1942282 -4.2140365 -4.2252617 -4.208571 -4.1787119 -4.1758642 -4.1970477 -4.2167964 -4.2331667 -4.2498322 -4.2615085 -4.2616515 -4.2467728 -4.2238092][-4.137692 -4.1607752 -4.1799746 -4.1967788 -4.1925292 -4.1746697 -4.1743188 -4.1926732 -4.2058744 -4.2154737 -4.2301407 -4.2470908 -4.25262 -4.2426133 -4.2265615][-4.1057425 -4.122756 -4.1432519 -4.1649427 -4.16966 -4.1647968 -4.1689348 -4.1849484 -4.1906157 -4.189043 -4.1945796 -4.211472 -4.2206993 -4.2176847 -4.2112236][-4.1020961 -4.1082296 -4.1287746 -4.1571174 -4.1690183 -4.1711111 -4.1776147 -4.1879635 -4.1818352 -4.1658654 -4.1595078 -4.1748986 -4.1882396 -4.195756 -4.2008104][-4.1321797 -4.1263928 -4.1392879 -4.1672764 -4.1824164 -4.185617 -4.192275 -4.1960459 -4.1786013 -4.1539292 -4.143661 -4.1566391 -4.1700931 -4.1837182 -4.1977162][-4.1715112 -4.1607189 -4.1659193 -4.1862559 -4.1982112 -4.1999717 -4.2054825 -4.20239 -4.1782846 -4.153729 -4.1488266 -4.1598639 -4.1683908 -4.1817269 -4.20034][-4.2042489 -4.1936746 -4.1938748 -4.20445 -4.2108512 -4.2105308 -4.2116156 -4.2012296 -4.1769218 -4.1586633 -4.1616483 -4.17424 -4.1817546 -4.19129 -4.20658][-4.2172809 -4.2085233 -4.2077594 -4.2135191 -4.2160039 -4.2130709 -4.2098746 -4.1942759 -4.1714005 -4.1583333 -4.1673827 -4.1830745 -4.191421 -4.1977391 -4.2085967][-4.2222018 -4.2159581 -4.2151995 -4.2205219 -4.222981 -4.2185049 -4.2104015 -4.1923056 -4.1706166 -4.1601338 -4.1713371 -4.1884 -4.1975842 -4.2020006 -4.2112589][-4.238143 -4.2356472 -4.2373052 -4.2444277 -4.2472515 -4.2422328 -4.2321224 -4.2138467 -4.1938677 -4.1838737 -4.193285 -4.2084661 -4.2165909 -4.2200751 -4.2269154][-4.2592711 -4.2598438 -4.2629108 -4.27075 -4.2752566 -4.2721891 -4.264895 -4.251236 -4.2355409 -4.2261019 -4.2312865 -4.2422833 -4.2480969 -4.2485409 -4.2504234]]...]
INFO - root - 2017-12-07 11:28:57.803419: step 3610, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 63h:56m:55s remains)
INFO - root - 2017-12-07 11:29:04.376068: step 3620, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 65h:00m:30s remains)
INFO - root - 2017-12-07 11:29:11.097756: step 3630, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 59h:14m:10s remains)
INFO - root - 2017-12-07 11:29:17.946250: step 3640, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 62h:36m:52s remains)
INFO - root - 2017-12-07 11:29:24.777963: step 3650, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 61h:11m:39s remains)
INFO - root - 2017-12-07 11:29:31.383068: step 3660, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.676 sec/batch; 61h:45m:55s remains)
INFO - root - 2017-12-07 11:29:38.202611: step 3670, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:29m:46s remains)
INFO - root - 2017-12-07 11:29:45.055942: step 3680, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 63h:42m:33s remains)
INFO - root - 2017-12-07 11:29:51.842128: step 3690, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 66h:33m:36s remains)
INFO - root - 2017-12-07 11:29:58.671989: step 3700, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 62h:49m:10s remains)
2017-12-07 11:29:59.450591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2324696 -4.2160826 -4.2212491 -4.2404666 -4.2617259 -4.2848783 -4.2886205 -4.2724442 -4.2392383 -4.2183638 -4.2215466 -4.2517247 -4.2879505 -4.3152523 -4.3314795][-4.1816387 -4.1690636 -4.1849465 -4.2163472 -4.2414179 -4.2652888 -4.2664685 -4.2419629 -4.1930423 -4.1610413 -4.166348 -4.2095137 -4.2591348 -4.2967553 -4.3214536][-4.1576176 -4.1556697 -4.1817675 -4.2179656 -4.2411356 -4.2599788 -4.2517672 -4.2177734 -4.1607676 -4.1258459 -4.1358261 -4.1824141 -4.2355709 -4.2791696 -4.3087692][-4.1719575 -4.1773009 -4.204906 -4.2384076 -4.2581944 -4.2613697 -4.23446 -4.1890144 -4.133007 -4.1137404 -4.1368337 -4.186769 -4.2382765 -4.2778435 -4.3030314][-4.1847973 -4.1962724 -4.2248549 -4.253715 -4.2639523 -4.2458596 -4.1915312 -4.1277976 -4.0865574 -4.1032343 -4.1490335 -4.2073555 -4.2586784 -4.2912893 -4.3073978][-4.1896086 -4.2063203 -4.2358527 -4.253335 -4.242033 -4.19248 -4.1027632 -4.0209675 -4.0147057 -4.081964 -4.1554551 -4.2229948 -4.2758574 -4.3034935 -4.3138037][-4.2074471 -4.2172918 -4.2328553 -4.2300606 -4.1869016 -4.0903187 -3.9446127 -3.8353612 -3.8848729 -4.0225725 -4.1365051 -4.2208567 -4.2801037 -4.3068304 -4.3179049][-4.232573 -4.2346964 -4.2312412 -4.1993313 -4.1179295 -3.9659553 -3.7386551 -3.5885422 -3.7177596 -3.9380634 -4.0962343 -4.2028341 -4.2739568 -4.3060675 -4.3194265][-4.2586107 -4.2526379 -4.2347922 -4.183054 -4.0796332 -3.9012485 -3.6328902 -3.4796915 -3.6597564 -3.9087214 -4.0747142 -4.1859207 -4.2569509 -4.2938595 -4.3128633][-4.27259 -4.2596054 -4.2388129 -4.185801 -4.0958724 -3.9567978 -3.762099 -3.6837695 -3.82533 -4.002296 -4.1163259 -4.1979017 -4.2483935 -4.2787681 -4.3013849][-4.2804532 -4.2664108 -4.2459836 -4.2035151 -4.1410804 -4.0549693 -3.9468679 -3.9233196 -4.017662 -4.1188021 -4.1814256 -4.2297597 -4.2616558 -4.2800708 -4.297905][-4.2876391 -4.2749538 -4.2565241 -4.2251563 -4.186039 -4.1373463 -4.0834484 -4.0868707 -4.148881 -4.20251 -4.2355618 -4.2644248 -4.2837539 -4.2930565 -4.3028908][-4.2910237 -4.2825465 -4.2702579 -4.2524066 -4.2328691 -4.2091675 -4.1817703 -4.1933651 -4.2293172 -4.2539697 -4.2671008 -4.2846313 -4.2983608 -4.3049717 -4.3104062][-4.2928457 -4.2890396 -4.2845993 -4.2778735 -4.2697983 -4.2566032 -4.2344 -4.2388783 -4.2598906 -4.2761688 -4.2849021 -4.3000369 -4.3115029 -4.3160844 -4.3187566][-4.2877111 -4.2883606 -4.289896 -4.2888122 -4.282084 -4.266715 -4.2430482 -4.2410626 -4.2585325 -4.2777276 -4.2889638 -4.3054619 -4.3187861 -4.3233643 -4.3253446]]...]
INFO - root - 2017-12-07 11:30:06.183492: step 3710, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 63h:42m:41s remains)
INFO - root - 2017-12-07 11:30:12.847221: step 3720, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 65h:05m:11s remains)
INFO - root - 2017-12-07 11:30:19.684688: step 3730, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 60h:53m:47s remains)
INFO - root - 2017-12-07 11:30:26.500700: step 3740, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:21m:28s remains)
INFO - root - 2017-12-07 11:30:33.417091: step 3750, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 64h:06m:45s remains)
INFO - root - 2017-12-07 11:30:40.355405: step 3760, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 67h:06m:58s remains)
INFO - root - 2017-12-07 11:30:47.101203: step 3770, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 61h:33m:18s remains)
INFO - root - 2017-12-07 11:30:53.940552: step 3780, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 63h:50m:31s remains)
INFO - root - 2017-12-07 11:31:00.763798: step 3790, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 58h:18m:23s remains)
INFO - root - 2017-12-07 11:31:07.615264: step 3800, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 63h:19m:17s remains)
2017-12-07 11:31:08.350528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3450451 -4.3389711 -4.3292208 -4.3230348 -4.3209338 -4.3216634 -4.3252583 -4.3271203 -4.3246255 -4.3203273 -4.31617 -4.3126593 -4.3094945 -4.3094664 -4.3120918][-4.3449478 -4.3378115 -4.3251877 -4.3153439 -4.3123279 -4.3135643 -4.3192725 -4.3222213 -4.3172917 -4.3110538 -4.3090749 -4.3085184 -4.3049159 -4.303339 -4.30587][-4.3333154 -4.3189917 -4.2984118 -4.2825603 -4.2791033 -4.2847962 -4.2941251 -4.2995992 -4.2949114 -4.2903309 -4.2892056 -4.2890835 -4.2854848 -4.2847457 -4.2884455][-4.3099437 -4.2828646 -4.2487793 -4.2225394 -4.2182236 -4.23328 -4.2521639 -4.2657409 -4.2657976 -4.2676063 -4.2695289 -4.2679043 -4.2633533 -4.2621918 -4.2663531][-4.2832584 -4.2438426 -4.1928697 -4.150001 -4.129796 -4.1381183 -4.1596131 -4.1813941 -4.1937346 -4.2163744 -4.233355 -4.2365046 -4.2352829 -4.2382731 -4.2426009][-4.2718921 -4.224339 -4.161603 -4.1002889 -4.0517812 -4.030365 -4.0284748 -4.0354123 -4.0609374 -4.1223865 -4.1710453 -4.1935577 -4.2070589 -4.2224236 -4.2316704][-4.271193 -4.2260208 -4.1631174 -4.0925431 -4.0206094 -3.9592035 -3.9034071 -3.8547547 -3.8750389 -3.9812012 -4.0700474 -4.11993 -4.1544747 -4.1882496 -4.2132463][-4.2810636 -4.247808 -4.1952271 -4.1313696 -4.0583076 -3.978997 -3.8766141 -3.7587409 -3.7461915 -3.8704245 -3.9835408 -4.0600653 -4.1168189 -4.1660557 -4.2045121][-4.2956076 -4.2805915 -4.2504883 -4.2086711 -4.157083 -4.0954008 -4.0059547 -3.8968596 -3.8615625 -3.9334893 -4.0129566 -4.0775151 -4.1296659 -4.1766152 -4.2155938][-4.3092203 -4.305903 -4.2949886 -4.2789226 -4.2501578 -4.2079988 -4.1438637 -4.0752149 -4.04797 -4.0756106 -4.1115465 -4.1428728 -4.1711569 -4.2057123 -4.2371979][-4.305078 -4.3051424 -4.3053551 -4.3074183 -4.2977872 -4.271687 -4.231616 -4.1998858 -4.1894722 -4.1987367 -4.2086887 -4.2163644 -4.2234612 -4.2426081 -4.2650447][-4.2934685 -4.2915235 -4.2929907 -4.3020821 -4.3078732 -4.2988882 -4.280365 -4.2738957 -4.2748647 -4.2790627 -4.2815266 -4.2777896 -4.2750649 -4.2830095 -4.2971444][-4.2881846 -4.2851582 -4.2853584 -4.2957549 -4.3096471 -4.3151155 -4.3144445 -4.3211551 -4.3254418 -4.3258467 -4.3254504 -4.3200865 -4.3153629 -4.3176107 -4.3251958][-4.2958302 -4.29108 -4.29111 -4.2998352 -4.3126993 -4.3228335 -4.3295054 -4.3378682 -4.3409629 -4.3393908 -4.338429 -4.3355303 -4.33255 -4.3327503 -4.3370328][-4.3088784 -4.3042955 -4.303401 -4.3075309 -4.314014 -4.3207 -4.3260145 -4.3305736 -4.3315358 -4.330914 -4.3322587 -4.333456 -4.3339953 -4.3361573 -4.3399444]]...]
INFO - root - 2017-12-07 11:31:15.095131: step 3810, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 59h:26m:00s remains)
INFO - root - 2017-12-07 11:31:21.717178: step 3820, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 58h:15m:47s remains)
INFO - root - 2017-12-07 11:31:28.493676: step 3830, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 61h:16m:50s remains)
INFO - root - 2017-12-07 11:31:35.358488: step 3840, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 65h:17m:32s remains)
INFO - root - 2017-12-07 11:31:42.247557: step 3850, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 61h:37m:04s remains)
INFO - root - 2017-12-07 11:31:49.037986: step 3860, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 57h:57m:01s remains)
INFO - root - 2017-12-07 11:31:55.906237: step 3870, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 61h:36m:03s remains)
INFO - root - 2017-12-07 11:32:02.679286: step 3880, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 64h:06m:32s remains)
INFO - root - 2017-12-07 11:32:09.475384: step 3890, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 59h:56m:28s remains)
INFO - root - 2017-12-07 11:32:16.239821: step 3900, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:17m:23s remains)
2017-12-07 11:32:16.979570: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2257156 -4.1488295 -4.0772915 -4.054234 -4.0857229 -4.1376357 -4.1756172 -4.1955371 -4.1990919 -4.1849532 -4.1690526 -4.1528268 -4.1366057 -4.1332273 -4.1500821][-4.2176728 -4.1253071 -4.0388412 -4.0192614 -4.0696993 -4.1350784 -4.1741238 -4.194355 -4.1969652 -4.1802664 -4.1674948 -4.1561103 -4.1439877 -4.1384616 -4.1516466][-4.2138424 -4.1071815 -4.0107474 -3.994597 -4.0646119 -4.1435771 -4.1830325 -4.2019548 -4.2034044 -4.1871347 -4.1758671 -4.1633234 -4.1476989 -4.1400771 -4.1489143][-4.2135158 -4.0986176 -3.9939625 -3.9794714 -4.0645933 -4.1541839 -4.1947832 -4.2083931 -4.2048 -4.1896081 -4.1836705 -4.17194 -4.156949 -4.1512632 -4.1540847][-4.2202544 -4.1031952 -3.9924388 -3.9729722 -4.0650034 -4.1622124 -4.2021079 -4.2083979 -4.203402 -4.1913214 -4.1877728 -4.1800585 -4.17122 -4.1723633 -4.1745477][-4.2344074 -4.1219611 -4.0116596 -3.9814563 -4.0667477 -4.1639476 -4.20116 -4.2033286 -4.2013035 -4.1919508 -4.1892457 -4.1860442 -4.1869125 -4.1933541 -4.1931376][-4.247426 -4.1451178 -4.0436821 -4.0084076 -4.079987 -4.1711478 -4.2050905 -4.2043972 -4.2016082 -4.1903086 -4.1821537 -4.1787872 -4.1861825 -4.1980824 -4.2002306][-4.2538829 -4.162672 -4.0735474 -4.0403466 -4.0983624 -4.1829805 -4.2195706 -4.2192659 -4.211669 -4.1949468 -4.1763859 -4.1662445 -4.1740532 -4.1890988 -4.197298][-4.2575612 -4.1763334 -4.101233 -4.0735717 -4.1231756 -4.2023168 -4.2418628 -4.2415123 -4.2326803 -4.2154078 -4.1915641 -4.1733837 -4.1724544 -4.1814337 -4.1916294][-4.2590332 -4.1862741 -4.1228814 -4.10108 -4.1447563 -4.2166305 -4.2543383 -4.2547803 -4.249094 -4.2388196 -4.2162924 -4.19523 -4.1854482 -4.1844206 -4.1914773][-4.2558861 -4.1903114 -4.1390476 -4.1261315 -4.1648097 -4.2264857 -4.2606668 -4.2617283 -4.2566614 -4.2493868 -4.2309203 -4.212122 -4.2012463 -4.1988454 -4.2045][-4.25483 -4.1987171 -4.1591034 -4.1538844 -4.187098 -4.2358608 -4.2646875 -4.2653475 -4.2618279 -4.2563214 -4.2395926 -4.2248559 -4.2183552 -4.2208562 -4.2291684][-4.2556653 -4.21134 -4.1821394 -4.1817117 -4.2094078 -4.2450213 -4.2667 -4.2672443 -4.2645888 -4.258594 -4.2437778 -4.2320795 -4.2281623 -4.235816 -4.2498627][-4.261776 -4.2294559 -4.2079458 -4.2082009 -4.2307835 -4.257688 -4.2740569 -4.2741561 -4.2705441 -4.2636771 -4.2515154 -4.2427621 -4.2408943 -4.2492433 -4.2643819][-4.2758493 -4.2536974 -4.2383251 -4.2378287 -4.254981 -4.2753787 -4.2874331 -4.2868013 -4.2805371 -4.2724705 -4.2621279 -4.2568531 -4.25858 -4.2674446 -4.2809305]]...]
INFO - root - 2017-12-07 11:32:23.659398: step 3910, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 62h:09m:19s remains)
INFO - root - 2017-12-07 11:32:30.294786: step 3920, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 64h:02m:32s remains)
INFO - root - 2017-12-07 11:32:37.118620: step 3930, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 58h:09m:45s remains)
INFO - root - 2017-12-07 11:32:43.916421: step 3940, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:44m:10s remains)
INFO - root - 2017-12-07 11:32:50.761145: step 3950, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.724 sec/batch; 66h:02m:42s remains)
INFO - root - 2017-12-07 11:32:57.531710: step 3960, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 61h:18m:38s remains)
INFO - root - 2017-12-07 11:33:04.238398: step 3970, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 60h:56m:19s remains)
INFO - root - 2017-12-07 11:33:11.094784: step 3980, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.669 sec/batch; 61h:03m:37s remains)
INFO - root - 2017-12-07 11:33:17.913643: step 3990, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 64h:57m:34s remains)
INFO - root - 2017-12-07 11:33:24.829021: step 4000, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 67h:44m:46s remains)
2017-12-07 11:33:25.509485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3206983 -4.2872634 -4.2448936 -4.1994252 -4.170979 -4.1648445 -4.1643982 -4.1924548 -4.2421679 -4.2702255 -4.2828746 -4.2906141 -4.296731 -4.3001871 -4.3042026][-4.3314562 -4.3008375 -4.2644978 -4.2237492 -4.1929784 -4.1747723 -4.157618 -4.1770139 -4.2293348 -4.2577395 -4.2690148 -4.2739077 -4.275631 -4.2753873 -4.2759242][-4.3418832 -4.3166938 -4.2891126 -4.2576671 -4.2312803 -4.2082644 -4.1821165 -4.1917853 -4.2368507 -4.2590017 -4.265244 -4.2649894 -4.2614212 -4.2549343 -4.2484274][-4.3445458 -4.324039 -4.3043423 -4.2829151 -4.2643871 -4.2440252 -4.2166266 -4.2170129 -4.2487926 -4.2625108 -4.2636538 -4.2598166 -4.2530913 -4.2430363 -4.2324452][-4.3382297 -4.3170404 -4.2979012 -4.2802935 -4.2663255 -4.2501411 -4.224164 -4.2186251 -4.2421503 -4.2523685 -4.2528687 -4.2513151 -4.2486706 -4.2438684 -4.2377911][-4.3305836 -4.30476 -4.2813368 -4.2600374 -4.2415676 -4.2214642 -4.19023 -4.1817074 -4.204577 -4.216558 -4.2227097 -4.2308874 -4.2411909 -4.2480807 -4.252389][-4.3269448 -4.2964025 -4.267333 -4.2383852 -4.2084122 -4.1754546 -4.1325035 -4.1198387 -4.1433377 -4.1576338 -4.1686435 -4.1877465 -4.2127032 -4.231739 -4.2460408][-4.3237524 -4.2884827 -4.2538171 -4.2180929 -4.1795788 -4.138155 -4.0848556 -4.0633264 -4.0814905 -4.0923643 -4.1012688 -4.1232395 -4.156713 -4.1840396 -4.2058039][-4.3144622 -4.2742167 -4.23595 -4.1991572 -4.1651287 -4.1277456 -4.0749083 -4.0418935 -4.0467505 -4.0479279 -4.0514908 -4.0742893 -4.1111159 -4.1397004 -4.1616507][-4.3053489 -4.2616839 -4.2203064 -4.1859741 -4.1568723 -4.126792 -4.0844455 -4.0533347 -4.0546703 -4.0504971 -4.0514097 -4.0737958 -4.1069407 -4.1276622 -4.1417389][-4.2997289 -4.2573829 -4.2167549 -4.1840549 -4.1565795 -4.128336 -4.0965505 -4.0780377 -4.0893369 -4.0895462 -4.0875072 -4.10452 -4.1302452 -4.142426 -4.1491923][-4.2969189 -4.2600851 -4.224792 -4.1958694 -4.1678681 -4.1388669 -4.1137843 -4.1079612 -4.1289196 -4.1340146 -4.1291327 -4.1396585 -4.1593437 -4.1678886 -4.17246][-4.3111138 -4.2845507 -4.2584596 -4.2355647 -4.2109809 -4.1849394 -4.1649966 -4.1679077 -4.1935272 -4.2011037 -4.1933022 -4.1962934 -4.2085075 -4.214663 -4.22107][-4.3387265 -4.3242126 -4.3073463 -4.293139 -4.2757354 -4.2560029 -4.2416143 -4.2481847 -4.2707243 -4.2756038 -4.26608 -4.2629638 -4.2672396 -4.27104 -4.2784052][-4.3589406 -4.3524094 -4.3437285 -4.3370862 -4.3283353 -4.3162861 -4.3077464 -4.3143888 -4.3285246 -4.3283992 -4.31848 -4.3123894 -4.3109045 -4.3126187 -4.3194361]]...]
INFO - root - 2017-12-07 11:33:32.252698: step 4010, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 61h:29m:01s remains)
INFO - root - 2017-12-07 11:33:38.879161: step 4020, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 64h:01m:01s remains)
INFO - root - 2017-12-07 11:33:45.713769: step 4030, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 65h:36m:39s remains)
INFO - root - 2017-12-07 11:33:52.561669: step 4040, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 59h:19m:12s remains)
INFO - root - 2017-12-07 11:33:59.409517: step 4050, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 58h:58m:02s remains)
INFO - root - 2017-12-07 11:34:06.325254: step 4060, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 63h:03m:37s remains)
INFO - root - 2017-12-07 11:34:13.156853: step 4070, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 65h:39m:37s remains)
INFO - root - 2017-12-07 11:34:19.944249: step 4080, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 63h:03m:20s remains)
INFO - root - 2017-12-07 11:34:26.719318: step 4090, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 60h:12m:32s remains)
INFO - root - 2017-12-07 11:34:33.482489: step 4100, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 59h:38m:25s remains)
2017-12-07 11:34:34.343740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3119817 -4.312952 -4.315022 -4.3151822 -4.3147335 -4.3148623 -4.3166885 -4.3192177 -4.3202906 -4.3206582 -4.32068 -4.3206434 -4.3205261 -4.3209658 -4.32159][-4.2769785 -4.278245 -4.2811756 -4.279882 -4.2784529 -4.2795286 -4.2855964 -4.2934871 -4.2971439 -4.2982774 -4.2989168 -4.2990808 -4.2986674 -4.2988577 -4.2993631][-4.2313256 -4.2332439 -4.2354283 -4.2310491 -4.2283697 -4.2328358 -4.2463651 -4.2637281 -4.2743549 -4.2778678 -4.279428 -4.2793989 -4.2778993 -4.2760863 -4.2737174][-4.1944065 -4.1924748 -4.1882429 -4.1766577 -4.1698704 -4.1764765 -4.1981163 -4.2286234 -4.2543259 -4.2682042 -4.2749338 -4.2763486 -4.2732396 -4.2669606 -4.2581577][-4.1809139 -4.1690154 -4.1530466 -4.1297717 -4.1128335 -4.1130066 -4.1332307 -4.1727533 -4.2193718 -4.2550268 -4.2759609 -4.2848086 -4.2833672 -4.2744164 -4.2606349][-4.1733618 -4.1531286 -4.1258864 -4.0903773 -4.0565314 -4.0327187 -4.0285459 -4.0624437 -4.130846 -4.1936445 -4.2341518 -4.2561932 -4.2644949 -4.2605205 -4.2503839][-4.168179 -4.1432204 -4.1092668 -4.0627337 -4.0075107 -3.9451303 -3.8896465 -3.8955925 -3.9816685 -4.0726051 -4.1332889 -4.1722703 -4.1990547 -4.2132807 -4.2184944][-4.1767507 -4.152266 -4.1194105 -4.0722523 -4.0046616 -3.9131055 -3.8131347 -3.7786393 -3.8612208 -3.9608984 -4.0257874 -4.0724583 -4.1154656 -4.153 -4.1779523][-4.2072506 -4.1906691 -4.1675406 -4.1332078 -4.0767908 -3.997957 -3.9116011 -3.8712187 -3.9124324 -3.9740024 -4.0132537 -4.0450525 -4.0836129 -4.1223063 -4.14948][-4.2487049 -4.2419896 -4.2282066 -4.2071276 -4.1709442 -4.1211567 -4.0696936 -4.0431776 -4.0573196 -4.0795388 -4.0879908 -4.0936394 -4.1090655 -4.1289883 -4.1445346][-4.2878323 -4.292429 -4.2886748 -4.2775846 -4.2550488 -4.2276549 -4.2012587 -4.1835818 -4.1828508 -4.1831851 -4.1734328 -4.1592317 -4.1523232 -4.1534238 -4.15847][-4.3103204 -4.323946 -4.3301153 -4.3284187 -4.3183351 -4.30433 -4.2896261 -4.2751575 -4.2661872 -4.2569232 -4.2368875 -4.2112246 -4.1916003 -4.1817484 -4.1793575][-4.3108206 -4.3308892 -4.3454328 -4.3514204 -4.3491049 -4.3403735 -4.3290963 -4.3179321 -4.3072486 -4.2938161 -4.2690663 -4.2392039 -4.216598 -4.2033157 -4.1956096][-4.2986717 -4.3214135 -4.3400679 -4.3499742 -4.3511868 -4.3440223 -4.3324013 -4.3212614 -4.3098865 -4.2937484 -4.2677703 -4.2404675 -4.2214355 -4.2112546 -4.2009139][-4.2831678 -4.3039923 -4.3206329 -4.3276377 -4.3265157 -4.3163648 -4.3008914 -4.2867684 -4.2743015 -4.2574415 -4.2353091 -4.2171249 -4.2065787 -4.1995835 -4.1893668]]...]
INFO - root - 2017-12-07 11:34:41.093122: step 4110, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 58h:43m:35s remains)
INFO - root - 2017-12-07 11:34:47.854966: step 4120, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 62h:30m:57s remains)
INFO - root - 2017-12-07 11:34:54.869691: step 4130, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 65h:22m:02s remains)
INFO - root - 2017-12-07 11:35:01.644302: step 4140, loss = 2.03, batch loss = 1.98 (12.9 examples/sec; 0.621 sec/batch; 56h:35m:48s remains)
INFO - root - 2017-12-07 11:35:08.529006: step 4150, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 61h:37m:16s remains)
INFO - root - 2017-12-07 11:35:15.371664: step 4160, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 60h:04m:48s remains)
INFO - root - 2017-12-07 11:35:22.142532: step 4170, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 64h:02m:39s remains)
INFO - root - 2017-12-07 11:35:28.916974: step 4180, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 65h:28m:21s remains)
INFO - root - 2017-12-07 11:35:35.714699: step 4190, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 62h:15m:01s remains)
INFO - root - 2017-12-07 11:35:42.543481: step 4200, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:14m:48s remains)
2017-12-07 11:35:43.249799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2606325 -4.252738 -4.2524166 -4.2537971 -4.2510624 -4.24139 -4.2297277 -4.2113934 -4.1930046 -4.1773562 -4.1646519 -4.1590109 -4.1697946 -4.2004161 -4.2410388][-4.2426357 -4.2380757 -4.2366271 -4.2279582 -4.2089157 -4.1805716 -4.1527953 -4.1228619 -4.0974088 -4.0782447 -4.0633316 -4.0586905 -4.0733843 -4.1137319 -4.1683211][-4.2394171 -4.2372041 -4.2309957 -4.2075233 -4.17013 -4.1231585 -4.081408 -4.0464206 -4.0231686 -4.00619 -3.9936512 -3.9936149 -4.0149426 -4.0621762 -4.1232443][-4.2243934 -4.2218847 -4.2096043 -4.175704 -4.1287379 -4.0753918 -4.0318975 -4.0046406 -3.9954007 -3.9940522 -3.9965131 -4.0086021 -4.037168 -4.0816941 -4.1334496][-4.1795797 -4.1797915 -4.1667094 -4.1303391 -4.0842233 -4.0373325 -4.00697 -4.000824 -4.0127411 -4.0305576 -4.0513005 -4.0739622 -4.1038971 -4.13726 -4.1704912][-4.1143508 -4.1225104 -4.1125593 -4.0773931 -4.0370193 -4.0026016 -3.9897561 -4.0039515 -4.0339904 -4.0676103 -4.1000929 -4.1290951 -4.157444 -4.1816673 -4.2003775][-4.0487175 -4.0657063 -4.0622211 -4.0348315 -4.0049887 -3.9835835 -3.9824514 -4.00869 -4.0499096 -4.0898042 -4.1245289 -4.1543722 -4.1816578 -4.2050209 -4.2201896][-4.0130405 -4.0362134 -4.0401063 -4.0268 -4.0183158 -4.0174909 -4.02615 -4.053206 -4.0885706 -4.1170435 -4.14233 -4.168951 -4.1981997 -4.2283521 -4.250464][-4.0329127 -4.0604038 -4.07414 -4.0787892 -4.0897574 -4.1018553 -4.1118126 -4.1270676 -4.1399183 -4.1459417 -4.1564174 -4.1760583 -4.206357 -4.2438612 -4.2762189][-4.0838385 -4.1152611 -4.1384544 -4.1551523 -4.1719284 -4.1839223 -4.1895537 -4.1909585 -4.1818533 -4.1664948 -4.1596279 -4.1670427 -4.1934772 -4.2348289 -4.2761607][-4.1281919 -4.1572018 -4.1831884 -4.2053094 -4.224504 -4.2386842 -4.2458291 -4.2401257 -4.2151222 -4.1791644 -4.1516485 -4.1416836 -4.1593966 -4.2020726 -4.2523422][-4.1636238 -4.1878262 -4.2132754 -4.2384386 -4.259141 -4.2758102 -4.2852268 -4.2771492 -4.2450242 -4.1976147 -4.1549869 -4.1314735 -4.1414146 -4.1819863 -4.2341185][-4.2146645 -4.2325196 -4.252564 -4.2740512 -4.2912855 -4.3043771 -4.3117237 -4.3045969 -4.27872 -4.239 -4.2005529 -4.1781425 -4.1841826 -4.2127819 -4.2467203][-4.2756205 -4.2874069 -4.2998195 -4.3130493 -4.3230076 -4.330389 -4.3346515 -4.3303623 -4.3152704 -4.2925224 -4.2681251 -4.2501521 -4.2471242 -4.252667 -4.2575259][-4.3306303 -4.3363686 -4.3410516 -4.3454366 -4.3471212 -4.3473382 -4.3477798 -4.3446336 -4.3362417 -4.3244987 -4.3094234 -4.2916083 -4.2752695 -4.2581763 -4.2364035]]...]
INFO - root - 2017-12-07 11:35:49.903869: step 4210, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 63h:29m:59s remains)
INFO - root - 2017-12-07 11:35:56.518523: step 4220, loss = 2.10, batch loss = 2.05 (11.2 examples/sec; 0.713 sec/batch; 65h:01m:16s remains)
INFO - root - 2017-12-07 11:36:03.359450: step 4230, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 60h:44m:48s remains)
INFO - root - 2017-12-07 11:36:10.340885: step 4240, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 66h:09m:25s remains)
INFO - root - 2017-12-07 11:36:17.262855: step 4250, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.708 sec/batch; 64h:32m:34s remains)
INFO - root - 2017-12-07 11:36:24.019862: step 4260, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 65h:05m:17s remains)
INFO - root - 2017-12-07 11:36:30.650349: step 4270, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 0.538 sec/batch; 49h:05m:04s remains)
INFO - root - 2017-12-07 11:36:37.521571: step 4280, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 63h:44m:34s remains)
INFO - root - 2017-12-07 11:36:44.399723: step 4290, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 68h:05m:53s remains)
INFO - root - 2017-12-07 11:36:51.258507: step 4300, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 59h:48m:30s remains)
2017-12-07 11:36:51.989902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1776295 -4.1748905 -4.171371 -4.1612415 -4.1449118 -4.1235867 -4.1087618 -4.1129627 -4.1323156 -4.1524329 -4.1683764 -4.1769156 -4.175878 -4.1685162 -4.1588979][-4.1662235 -4.1668663 -4.1630378 -4.1470571 -4.1194577 -4.085814 -4.0665307 -4.0765471 -4.1052589 -4.1324415 -4.1556807 -4.1726532 -4.1757188 -4.1673779 -4.1545687][-4.1554589 -4.1577768 -4.1500063 -4.123651 -4.0818391 -4.0375252 -4.0193434 -4.0398817 -4.0789413 -4.114285 -4.1447434 -4.1677322 -4.1730514 -4.1632667 -4.1460462][-4.1507444 -4.1518312 -4.1369677 -4.0987945 -4.04301 -3.9915664 -3.9764857 -4.008625 -4.0596189 -4.1044559 -4.1424055 -4.1669126 -4.1713462 -4.158464 -4.1337762][-4.1524067 -4.1483068 -4.1243567 -4.0753675 -4.0085921 -3.9543455 -3.9429133 -3.987438 -4.0520315 -4.1059408 -4.1497808 -4.1748004 -4.1773238 -4.1579847 -4.1198916][-4.1585078 -4.144289 -4.1112065 -4.0530214 -3.9782393 -3.9235995 -3.9155192 -3.9735463 -4.0530143 -4.1167922 -4.1657629 -4.1912589 -4.1903753 -4.1598716 -4.1034513][-4.1639695 -4.1396141 -4.0990396 -4.0332937 -3.9531345 -3.8962543 -3.891248 -3.9629707 -4.0549774 -4.1271939 -4.1801138 -4.2070947 -4.2034965 -4.1625676 -4.0900016][-4.1622577 -4.1320314 -4.0899358 -4.0217671 -3.9379003 -3.8759749 -3.872628 -3.9553707 -4.0574265 -4.1370058 -4.1924205 -4.2214804 -4.2174215 -4.1721425 -4.0921092][-4.1477919 -4.118104 -4.0856948 -4.0310025 -3.9563196 -3.89403 -3.88929 -3.9704263 -4.0730968 -4.1526246 -4.205193 -4.2345905 -4.2330637 -4.1916637 -4.11429][-4.1264729 -4.0995684 -4.0827703 -4.0513792 -3.9999468 -3.9503696 -3.9484594 -4.0179887 -4.1085215 -4.1768293 -4.2179141 -4.242157 -4.2424212 -4.2105284 -4.1457691][-4.1073751 -4.08689 -4.0856962 -4.0792046 -4.0541868 -4.019752 -4.0210218 -4.0770535 -4.1499782 -4.203136 -4.2301021 -4.2463393 -4.2464333 -4.2244334 -4.1720772][-4.0922656 -4.084044 -4.1001406 -4.1149077 -4.1097603 -4.087079 -4.0881872 -4.129529 -4.1844649 -4.2239466 -4.2401266 -4.2496495 -4.2487783 -4.2308006 -4.1829076][-4.0825009 -4.0892582 -4.1208696 -4.1493497 -4.1573887 -4.1451588 -4.1456938 -4.1715531 -4.2095351 -4.2400494 -4.2524052 -4.2571659 -4.2515445 -4.2292047 -4.1781116][-4.0801229 -4.096375 -4.1348314 -4.1682682 -4.1823072 -4.1780148 -4.1798224 -4.1962538 -4.2248836 -4.2504649 -4.2626858 -4.2644434 -4.2533989 -4.22578 -4.1741552][-4.0856543 -4.1081982 -4.1458349 -4.1762328 -4.1875763 -4.1858239 -4.1854205 -4.1927457 -4.2160063 -4.2411175 -4.2572026 -4.2602205 -4.246 -4.2163911 -4.1709771]]...]
INFO - root - 2017-12-07 11:36:58.801080: step 4310, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 65h:10m:59s remains)
INFO - root - 2017-12-07 11:37:05.399033: step 4320, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 63h:19m:39s remains)
INFO - root - 2017-12-07 11:37:12.143401: step 4330, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:20m:25s remains)
INFO - root - 2017-12-07 11:37:18.837860: step 4340, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 58h:49m:04s remains)
INFO - root - 2017-12-07 11:37:25.754752: step 4350, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 64h:10m:32s remains)
INFO - root - 2017-12-07 11:37:32.611886: step 4360, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 64h:29m:45s remains)
INFO - root - 2017-12-07 11:37:39.398387: step 4370, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 63h:47m:13s remains)
INFO - root - 2017-12-07 11:37:46.324734: step 4380, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 60h:01m:46s remains)
INFO - root - 2017-12-07 11:37:53.097235: step 4390, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:36m:56s remains)
INFO - root - 2017-12-07 11:37:59.863913: step 4400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 63h:59m:10s remains)
2017-12-07 11:38:00.496016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1852431 -4.1987853 -4.2046466 -4.1989889 -4.1933403 -4.1992183 -4.2140732 -4.2195282 -4.1947017 -4.1706667 -4.16529 -4.1786456 -4.2119303 -4.2288222 -4.2177796][-4.1865249 -4.198863 -4.2008448 -4.1900725 -4.1895251 -4.2015462 -4.2162309 -4.2217193 -4.2042084 -4.1857324 -4.1792336 -4.1865554 -4.2028723 -4.2033176 -4.1938381][-4.1573362 -4.1662092 -4.1656289 -4.1536851 -4.1525912 -4.16062 -4.1756577 -4.1880684 -4.1874762 -4.1801929 -4.1745472 -4.1753645 -4.1733379 -4.1653652 -4.1627145][-4.0912919 -4.0990419 -4.1019559 -4.090764 -4.0875473 -4.0927811 -4.1078429 -4.1264114 -4.14089 -4.1410017 -4.1368814 -4.1315923 -4.1175628 -4.104177 -4.1009][-4.013535 -4.0236707 -4.0334187 -4.0293174 -4.0303917 -4.0378938 -4.049921 -4.0653725 -4.0794268 -4.0798092 -4.0769558 -4.0724468 -4.0583382 -4.0483713 -4.0446229][-3.9847846 -3.9951174 -4.0031867 -3.996347 -3.9938521 -3.9964406 -3.9908502 -3.9910021 -4.0101671 -4.0254622 -4.0248809 -4.02102 -4.0102859 -4.0062361 -4.0045457][-3.982903 -3.9861956 -3.9809978 -3.9624772 -3.9475958 -3.9246233 -3.8784883 -3.8499863 -3.887548 -3.9352915 -3.9464135 -3.9534984 -3.955544 -3.9587288 -3.9732726][-4.0055342 -3.9904366 -3.9673824 -3.9365897 -3.9054403 -3.8534546 -3.7736938 -3.7254035 -3.7900248 -3.8694677 -3.8934026 -3.9159508 -3.9412632 -3.9629521 -4.00147][-4.06592 -4.036582 -4.0058618 -3.9742041 -3.9415982 -3.8877926 -3.8189385 -3.7871077 -3.8501678 -3.9194012 -3.9403446 -3.967339 -4.0051618 -4.0360227 -4.080667][-4.1327982 -4.1030979 -4.0747404 -4.0510583 -4.0289874 -3.9959803 -3.9591041 -3.9471695 -3.988884 -4.0307436 -4.0434523 -4.062736 -4.0936084 -4.1193032 -4.1576543][-4.1880221 -4.1673741 -4.1486983 -4.1341925 -4.1204042 -4.1042709 -4.0888877 -4.0861912 -4.1068811 -4.1284552 -4.135541 -4.1471133 -4.1657691 -4.1814041 -4.2063251][-4.2297716 -4.21685 -4.2068906 -4.1986928 -4.1917295 -4.1874552 -4.1833897 -4.1815038 -4.1861153 -4.1948128 -4.1990938 -4.2038975 -4.2101779 -4.2136173 -4.2236586][-4.2656136 -4.2551007 -4.2475805 -4.2415128 -4.2387319 -4.2408571 -4.2405605 -4.2360878 -4.2325406 -4.2327576 -4.230701 -4.2271953 -4.2216864 -4.2121353 -4.2086892][-4.2956362 -4.2876129 -4.2810578 -4.2769976 -4.2754092 -4.2767787 -4.2757511 -4.2711077 -4.2661109 -4.2624421 -4.2553678 -4.2441649 -4.2272067 -4.2051563 -4.1864033][-4.3217039 -4.3176718 -4.3121729 -4.3084054 -4.3054833 -4.3033872 -4.3013391 -4.29892 -4.2962303 -4.2927513 -4.2856979 -4.2726803 -4.2519045 -4.2207561 -4.1878505]]...]
INFO - root - 2017-12-07 11:38:07.253350: step 4410, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 60h:56m:41s remains)
INFO - root - 2017-12-07 11:38:13.949390: step 4420, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 60h:08m:26s remains)
INFO - root - 2017-12-07 11:38:20.882785: step 4430, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 65h:08m:01s remains)
INFO - root - 2017-12-07 11:38:27.737123: step 4440, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.743 sec/batch; 67h:39m:54s remains)
INFO - root - 2017-12-07 11:38:34.489771: step 4450, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 62h:20m:01s remains)
INFO - root - 2017-12-07 11:38:41.287762: step 4460, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:23m:01s remains)
INFO - root - 2017-12-07 11:38:48.142449: step 4470, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:35m:24s remains)
INFO - root - 2017-12-07 11:38:54.998154: step 4480, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 62h:15m:28s remains)
INFO - root - 2017-12-07 11:39:01.806371: step 4490, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 60h:32m:53s remains)
INFO - root - 2017-12-07 11:39:08.600061: step 4500, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.683 sec/batch; 62h:15m:17s remains)
2017-12-07 11:39:09.276727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3108168 -4.3206096 -4.3259706 -4.3256145 -4.3174562 -4.3052382 -4.2882342 -4.2663484 -4.2499905 -4.2511797 -4.2552991 -4.2622595 -4.2791734 -4.2957063 -4.3074265][-4.3068027 -4.3217049 -4.3272915 -4.322752 -4.3066115 -4.2840328 -4.2569623 -4.2194352 -4.1924167 -4.2014422 -4.2145033 -4.2302966 -4.2605195 -4.286931 -4.3047504][-4.3029914 -4.3183351 -4.3215303 -4.3104868 -4.2842937 -4.2495227 -4.2082114 -4.148047 -4.1166048 -4.147459 -4.1818953 -4.2085133 -4.2472358 -4.2827563 -4.3075862][-4.2957549 -4.3100252 -4.3109293 -4.2971268 -4.2633734 -4.2144618 -4.15048 -4.0678048 -4.0410371 -4.1014404 -4.15607 -4.1889334 -4.2325273 -4.2748117 -4.3059077][-4.2874966 -4.297152 -4.2946653 -4.2734795 -4.2282081 -4.1627321 -4.0676675 -3.9577038 -3.9465775 -4.0435538 -4.1234231 -4.1665339 -4.2184176 -4.2674603 -4.3031187][-4.2781534 -4.28311 -4.2806239 -4.2551389 -4.1987419 -4.1152954 -3.9768758 -3.8231211 -3.8295186 -3.9763858 -4.0882406 -4.14659 -4.2067418 -4.263133 -4.30156][-4.26907 -4.2759218 -4.2805052 -4.2560563 -4.1989088 -4.1098828 -3.9374461 -3.7396042 -3.7566669 -3.9432573 -4.0752158 -4.1441407 -4.2082973 -4.2692981 -4.3082075][-4.25906 -4.275465 -4.2918568 -4.2754221 -4.2284865 -4.1551733 -4.0026817 -3.82415 -3.8328977 -3.9960678 -4.1108565 -4.1718807 -4.2296815 -4.28531 -4.3165631][-4.2484794 -4.2734489 -4.2969394 -4.287148 -4.2548842 -4.2038064 -4.0974636 -3.9798949 -3.9905238 -4.0988793 -4.1699338 -4.2120743 -4.2551641 -4.2967572 -4.3172374][-4.23518 -4.262239 -4.279963 -4.2734165 -4.25719 -4.2238045 -4.1543779 -4.0856972 -4.1047173 -4.1805921 -4.22552 -4.2492971 -4.2751713 -4.3021431 -4.3119807][-4.2214575 -4.2464485 -4.2551441 -4.2522922 -4.246613 -4.221036 -4.1749129 -4.1379824 -4.1618624 -4.2199821 -4.2562633 -4.2700491 -4.2788897 -4.2899084 -4.291719][-4.2087564 -4.2302642 -4.2391844 -4.2408047 -4.2371459 -4.2181525 -4.1906958 -4.1743445 -4.1956215 -4.2336893 -4.2601061 -4.2661886 -4.2620583 -4.2621837 -4.2604375][-4.2037358 -4.2207379 -4.2324152 -4.2353272 -4.2300854 -4.2176752 -4.2049947 -4.2044296 -4.2221637 -4.2409444 -4.2525139 -4.2496004 -4.2394147 -4.2346568 -4.2311454][-4.2128415 -4.2302046 -4.2446327 -4.2472668 -4.2413917 -4.2307038 -4.2224116 -4.2273445 -4.2398143 -4.24785 -4.2494731 -4.2399564 -4.2270155 -4.217154 -4.2088556][-4.2317204 -4.252532 -4.2714148 -4.2747211 -4.2661347 -4.2543726 -4.2425704 -4.2397285 -4.2428746 -4.2444429 -4.23889 -4.2294607 -4.218883 -4.2098684 -4.2008643]]...]
INFO - root - 2017-12-07 11:39:16.165405: step 4510, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 64h:46m:12s remains)
INFO - root - 2017-12-07 11:39:22.853100: step 4520, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 62h:19m:36s remains)
INFO - root - 2017-12-07 11:39:29.755804: step 4530, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 63h:37m:20s remains)
INFO - root - 2017-12-07 11:39:36.664182: step 4540, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 59h:18m:41s remains)
INFO - root - 2017-12-07 11:39:43.469844: step 4550, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 62h:10m:24s remains)
INFO - root - 2017-12-07 11:39:50.406772: step 4560, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 66h:13m:38s remains)
INFO - root - 2017-12-07 11:39:57.255894: step 4570, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 64h:43m:12s remains)
INFO - root - 2017-12-07 11:40:03.847906: step 4580, loss = 2.07, batch loss = 2.02 (16.3 examples/sec; 0.492 sec/batch; 44h:48m:23s remains)
INFO - root - 2017-12-07 11:40:10.651933: step 4590, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 63h:13m:44s remains)
INFO - root - 2017-12-07 11:40:17.503143: step 4600, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 64h:23m:16s remains)
2017-12-07 11:40:18.144910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2867508 -4.2863035 -4.2880559 -4.2905087 -4.2926702 -4.2923 -4.2909517 -4.29172 -4.2892795 -4.2785335 -4.2683158 -4.2626872 -4.2587566 -4.25398 -4.2486][-4.2874684 -4.2845054 -4.2837334 -4.28484 -4.2870593 -4.2884135 -4.2900314 -4.2935953 -4.2924533 -4.28302 -4.2726536 -4.2627821 -4.2560883 -4.2507739 -4.2464662][-4.2667418 -4.258965 -4.2549982 -4.2539716 -4.2560749 -4.2591777 -4.2632213 -4.2709236 -4.2757912 -4.2755904 -4.2723804 -4.265471 -4.2616305 -4.2602773 -4.2590041][-4.23981 -4.2260451 -4.2171211 -4.2130361 -4.2140417 -4.219192 -4.2260003 -4.2379203 -4.2499666 -4.2596831 -4.265851 -4.2651358 -4.26574 -4.267787 -4.2688131][-4.2131057 -4.1931925 -4.1778007 -4.1696529 -4.1653938 -4.1668735 -4.1725454 -4.1876326 -4.2075844 -4.2271233 -4.2431507 -4.2527676 -4.2604542 -4.26494 -4.2659087][-4.1606832 -4.1351013 -4.1137786 -4.100986 -4.0884471 -4.0797615 -4.0795474 -4.0942464 -4.1193495 -4.1488104 -4.176733 -4.1977477 -4.2155857 -4.2288589 -4.236763][-4.1152892 -4.0849056 -4.058207 -4.0391297 -4.0139351 -3.9868197 -3.9708831 -3.9765115 -4.0005245 -4.0368819 -4.0789046 -4.115819 -4.1472044 -4.1725745 -4.191987][-4.1172857 -4.0910788 -4.06712 -4.047461 -4.0178771 -3.9811325 -3.9515524 -3.9381638 -3.9444888 -3.9713249 -4.0143847 -4.0601969 -4.1019354 -4.1366415 -4.16483][-4.1646929 -4.1466284 -4.1302557 -4.1187196 -4.0999589 -4.0742617 -4.0485973 -4.0274186 -4.019434 -4.0292907 -4.0572877 -4.0909715 -4.1240196 -4.1549249 -4.1806211][-4.2218885 -4.213769 -4.2053404 -4.1997814 -4.1904249 -4.1754894 -4.1585693 -4.141098 -4.1331415 -4.1377335 -4.1551018 -4.1758657 -4.1965055 -4.2151256 -4.2281466][-4.2546625 -4.2536092 -4.2512107 -4.2506266 -4.2500668 -4.2448916 -4.2369576 -4.2279377 -4.2256637 -4.2279196 -4.2356706 -4.2445965 -4.2551575 -4.2633953 -4.2669344][-4.2664118 -4.2693939 -4.2731686 -4.2787738 -4.285501 -4.2872014 -4.2843161 -4.2785211 -4.2767644 -4.2734337 -4.2707782 -4.2691054 -4.2717505 -4.274075 -4.2728481][-4.2578077 -4.2630816 -4.270617 -4.2792511 -4.2882757 -4.2924619 -4.2930679 -4.2908096 -4.2881851 -4.276958 -4.2649636 -4.25552 -4.2532239 -4.2545319 -4.2548652][-4.23628 -4.2423682 -4.2503228 -4.2578812 -4.2645025 -4.2682114 -4.2700186 -4.2708492 -4.2654681 -4.2479978 -4.2300072 -4.2154708 -4.2118397 -4.2163377 -4.2218003][-4.2137671 -4.2185826 -4.2251883 -4.2294273 -4.2311473 -4.2331338 -4.2348418 -4.2368655 -4.2287779 -4.2087808 -4.1883264 -4.1725388 -4.1717 -4.183723 -4.1975865]]...]
INFO - root - 2017-12-07 11:40:24.908775: step 4610, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 61h:10m:35s remains)
INFO - root - 2017-12-07 11:40:31.490436: step 4620, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 63h:25m:38s remains)
INFO - root - 2017-12-07 11:40:38.419558: step 4630, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.748 sec/batch; 68h:07m:54s remains)
INFO - root - 2017-12-07 11:40:45.216720: step 4640, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 61h:12m:06s remains)
INFO - root - 2017-12-07 11:40:51.958338: step 4650, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.626 sec/batch; 57h:02m:59s remains)
INFO - root - 2017-12-07 11:40:58.860163: step 4660, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.684 sec/batch; 62h:16m:05s remains)
INFO - root - 2017-12-07 11:41:05.577388: step 4670, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.682 sec/batch; 62h:04m:36s remains)
INFO - root - 2017-12-07 11:41:12.401084: step 4680, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 64h:16m:01s remains)
INFO - root - 2017-12-07 11:41:19.142089: step 4690, loss = 2.03, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 61h:19m:27s remains)
INFO - root - 2017-12-07 11:41:25.925899: step 4700, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 59h:09m:00s remains)
2017-12-07 11:41:26.683724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2350869 -4.2407966 -4.2605686 -4.2822485 -4.287446 -4.2713528 -4.2537432 -4.247117 -4.251678 -4.2617288 -4.263979 -4.2449641 -4.2254944 -4.2325778 -4.2528038][-4.206779 -4.2270317 -4.2569518 -4.285181 -4.2898426 -4.2644658 -4.2412744 -4.234014 -4.2416353 -4.2539248 -4.2569804 -4.2400966 -4.2253056 -4.237514 -4.2635164][-4.1691203 -4.1968279 -4.2389684 -4.2777019 -4.2842865 -4.2515855 -4.2177782 -4.2123933 -4.2301702 -4.2465363 -4.2446337 -4.2273254 -4.2193494 -4.2347 -4.2629962][-4.1608639 -4.1828461 -4.227221 -4.2669315 -4.2630968 -4.2094316 -4.1552005 -4.1560397 -4.194901 -4.2247548 -4.2253075 -4.2106895 -4.207489 -4.2271605 -4.2606654][-4.1692028 -4.1840744 -4.2238746 -4.2501831 -4.2229266 -4.1334 -4.0526729 -4.071928 -4.1432614 -4.1905942 -4.1952567 -4.1809692 -4.1797419 -4.2083158 -4.2516804][-4.1849427 -4.1957493 -4.226613 -4.2348413 -4.1790142 -4.056818 -3.9552848 -3.9926224 -4.0948272 -4.1553245 -4.1581645 -4.1386161 -4.1387534 -4.176734 -4.2329679][-4.2076859 -4.2209883 -4.2448106 -4.2408772 -4.1730275 -4.0493531 -3.9501965 -3.9877851 -4.093606 -4.1530542 -4.1460233 -4.1174722 -4.1211252 -4.1660905 -4.2291455][-4.2465591 -4.2579565 -4.2743659 -4.2641735 -4.203732 -4.1025796 -4.0161343 -4.0393023 -4.1274476 -4.1783609 -4.164598 -4.1311378 -4.1369171 -4.1837263 -4.2452917][-4.2871618 -4.2964606 -4.3034167 -4.2874031 -4.2390571 -4.1610341 -4.087708 -4.0983667 -4.1651168 -4.2062197 -4.190382 -4.1528645 -4.1586108 -4.2084842 -4.266664][-4.3077822 -4.3135266 -4.3140368 -4.2978559 -4.2618313 -4.2054372 -4.1494207 -4.1515083 -4.2010813 -4.2343149 -4.2218189 -4.1842947 -4.18359 -4.229147 -4.2803946][-4.3087139 -4.311564 -4.31268 -4.3027997 -4.277277 -4.2393827 -4.1998878 -4.1989717 -4.2335043 -4.2616043 -4.2566357 -4.2236948 -4.2158008 -4.251688 -4.29353][-4.301095 -4.3069897 -4.3123512 -4.3093934 -4.2931809 -4.2693996 -4.2440815 -4.2443538 -4.2686834 -4.2887287 -4.2850585 -4.2607989 -4.2538123 -4.2812181 -4.3113918][-4.3025994 -4.3118963 -4.3190079 -4.3201332 -4.3118834 -4.2966609 -4.2820215 -4.2851033 -4.3013415 -4.3110476 -4.3049955 -4.2905083 -4.288465 -4.3072805 -4.326437][-4.3108072 -4.3213906 -4.3298345 -4.3346004 -4.3321862 -4.3215556 -4.3111644 -4.3129663 -4.3231606 -4.3268943 -4.3197474 -4.3106418 -4.3106132 -4.3189025 -4.3290339][-4.3147211 -4.3233776 -4.3310838 -4.3373094 -4.3371615 -4.329865 -4.3235397 -4.324122 -4.3302603 -4.3324833 -4.3278055 -4.3215628 -4.3206229 -4.3214803 -4.3238573]]...]
INFO - root - 2017-12-07 11:41:33.441138: step 4710, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 62h:50m:18s remains)
INFO - root - 2017-12-07 11:41:40.052633: step 4720, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 59h:19m:46s remains)
INFO - root - 2017-12-07 11:41:46.939568: step 4730, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 63h:29m:29s remains)
INFO - root - 2017-12-07 11:41:53.864823: step 4740, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 66h:55m:25s remains)
INFO - root - 2017-12-07 11:42:00.751858: step 4750, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.698 sec/batch; 63h:35m:18s remains)
INFO - root - 2017-12-07 11:42:07.541601: step 4760, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.643 sec/batch; 58h:29m:45s remains)
INFO - root - 2017-12-07 11:42:14.427172: step 4770, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.687 sec/batch; 62h:29m:49s remains)
INFO - root - 2017-12-07 11:42:21.277852: step 4780, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 65h:38m:37s remains)
INFO - root - 2017-12-07 11:42:28.113408: step 4790, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:34m:28s remains)
INFO - root - 2017-12-07 11:42:34.859772: step 4800, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 61h:30m:42s remains)
2017-12-07 11:42:35.555363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2363296 -4.2375875 -4.2408366 -4.2409716 -4.2443819 -4.2508216 -4.2561989 -4.2663631 -4.2820272 -4.2944779 -4.300561 -4.2935162 -4.2814603 -4.2771654 -4.2850142][-4.1844716 -4.1896019 -4.1966686 -4.1989956 -4.202239 -4.2069936 -4.2115417 -4.224474 -4.24555 -4.2674417 -4.2852054 -4.2899981 -4.2861881 -4.2848253 -4.2919683][-4.1422725 -4.1545315 -4.1637692 -4.166471 -4.1670761 -4.1671829 -4.1680303 -4.1797028 -4.2024751 -4.2310095 -4.2568555 -4.2712836 -4.2775025 -4.2830343 -4.2936139][-4.1120973 -4.1315551 -4.14176 -4.1449509 -4.1412592 -4.1348228 -4.1314192 -4.1393614 -4.1606722 -4.1919174 -4.2219887 -4.2425637 -4.2565303 -4.2689371 -4.2834539][-4.1069388 -4.1210089 -4.1244707 -4.1219087 -4.109571 -4.0936861 -4.0831294 -4.0883021 -4.1130009 -4.150115 -4.1832995 -4.207047 -4.22545 -4.2417235 -4.2570996][-4.1313996 -4.1315989 -4.1185408 -4.0965376 -4.0640044 -4.0301986 -4.0033092 -4.0006146 -4.0387893 -4.0962391 -4.1400876 -4.168046 -4.1888723 -4.2055712 -4.2198443][-4.1724381 -4.1645441 -4.1358471 -4.0906076 -4.0317736 -3.97128 -3.9155617 -3.8937426 -3.9469285 -4.0318346 -4.0944433 -4.1305671 -4.1524692 -4.167551 -4.1815085][-4.2111034 -4.206574 -4.1711593 -4.1111078 -4.0331006 -3.9528577 -3.8755474 -3.8339069 -3.8843982 -3.9803672 -4.0531759 -4.0958624 -4.1191692 -4.1349435 -4.1507068][-4.2356973 -4.2389746 -4.2078428 -4.1493 -4.0717316 -3.9923465 -3.9171441 -3.8704748 -3.8998635 -3.9745836 -4.0337057 -4.0719066 -4.0946474 -4.1139975 -4.13238][-4.2474146 -4.2596378 -4.24288 -4.1995077 -4.1372528 -4.071116 -4.0111666 -3.970396 -3.977793 -4.0206561 -4.0555472 -4.0809336 -4.0969329 -4.1128807 -4.1276126][-4.2472591 -4.2633333 -4.2645931 -4.2420478 -4.1994729 -4.15209 -4.11176 -4.0824513 -4.0809231 -4.1010566 -4.1169829 -4.1298356 -4.1370111 -4.14198 -4.14273][-4.2413316 -4.256072 -4.2702503 -4.2630639 -4.237309 -4.2082858 -4.18637 -4.1716442 -4.1725163 -4.1811686 -4.1860065 -4.1893516 -4.1905775 -4.1875815 -4.175323][-4.2294135 -4.239337 -4.2573218 -4.2590189 -4.2475324 -4.2332067 -4.2245283 -4.224556 -4.2311277 -4.2370787 -4.2368875 -4.2341447 -4.2331142 -4.2308912 -4.2184105][-4.2139788 -4.2170587 -4.2328382 -4.2386346 -4.2379646 -4.2344632 -4.2352133 -4.2438884 -4.2546754 -4.2634583 -4.2648907 -4.2609563 -4.2614546 -4.2641845 -4.25951][-4.1966033 -4.1943769 -4.2075682 -4.2149429 -4.2201662 -4.2231627 -4.2294726 -4.2403393 -4.2521267 -4.2625585 -4.2663097 -4.2635927 -4.266655 -4.27385 -4.2780938]]...]
INFO - root - 2017-12-07 11:42:42.310642: step 4810, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 60h:59m:38s remains)
INFO - root - 2017-12-07 11:42:48.822921: step 4820, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 62h:05m:38s remains)
INFO - root - 2017-12-07 11:42:55.739520: step 4830, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 63h:38m:27s remains)
INFO - root - 2017-12-07 11:43:02.676496: step 4840, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 57h:53m:27s remains)
INFO - root - 2017-12-07 11:43:09.608452: step 4850, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 64h:41m:45s remains)
INFO - root - 2017-12-07 11:43:16.477482: step 4860, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 63h:26m:02s remains)
INFO - root - 2017-12-07 11:43:23.276603: step 4870, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 59h:09m:29s remains)
INFO - root - 2017-12-07 11:43:30.165479: step 4880, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 60h:12m:33s remains)
INFO - root - 2017-12-07 11:43:36.909139: step 4890, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 62h:48m:48s remains)
INFO - root - 2017-12-07 11:43:43.762990: step 4900, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 64h:09m:29s remains)
2017-12-07 11:43:44.484655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1463871 -4.1815143 -4.2150178 -4.2211409 -4.1966019 -4.1704783 -4.1745253 -4.1866384 -4.2016678 -4.2341495 -4.269433 -4.2934337 -4.2992616 -4.2815938 -4.2590857][-4.1511722 -4.1923938 -4.230021 -4.2385526 -4.2151866 -4.186214 -4.1846647 -4.1952724 -4.2162933 -4.2526689 -4.2830763 -4.3002071 -4.3069587 -4.29096 -4.2643142][-4.147438 -4.1899767 -4.2307081 -4.2422857 -4.2208872 -4.189291 -4.180728 -4.1900945 -4.2191362 -4.2600846 -4.2875829 -4.2968731 -4.302031 -4.2900085 -4.2619486][-4.1340203 -4.1738911 -4.2141547 -4.2271481 -4.207303 -4.1727676 -4.154531 -4.1598296 -4.1987791 -4.2491484 -4.2790279 -4.2865191 -4.293087 -4.2830372 -4.253304][-4.1213789 -4.1551528 -4.1920571 -4.2067223 -4.1864619 -4.1451926 -4.1121283 -4.1085272 -4.1572762 -4.221128 -4.2580237 -4.271688 -4.2784739 -4.2636218 -4.2303438][-4.1293688 -4.1562338 -4.1859603 -4.199276 -4.179131 -4.1319394 -4.0783954 -4.0556545 -4.1106405 -4.1893234 -4.2374358 -4.2579346 -4.2622352 -4.2387581 -4.2011189][-4.1605158 -4.1787009 -4.1959996 -4.202548 -4.1845741 -4.1391072 -4.0803452 -4.0490088 -4.1011381 -4.1829462 -4.2374434 -4.2604704 -4.2613869 -4.233789 -4.1936255][-4.1934781 -4.200491 -4.2023659 -4.1965203 -4.1764779 -4.1398306 -4.1011939 -4.0882215 -4.1327815 -4.2012997 -4.2514009 -4.2720885 -4.2719226 -4.2469954 -4.21059][-4.2127419 -4.2055902 -4.1925273 -4.1747775 -4.15143 -4.1284075 -4.1201363 -4.1334271 -4.1740279 -4.22658 -4.2683153 -4.2854166 -4.2847991 -4.2651 -4.2388096][-4.2066627 -4.1893358 -4.1711431 -4.1496897 -4.1289229 -4.12018 -4.1378536 -4.1672254 -4.202126 -4.2426939 -4.2772694 -4.2918553 -4.292388 -4.2803721 -4.2657647][-4.1962385 -4.1775985 -4.1629477 -4.1450377 -4.1288033 -4.1291909 -4.1586514 -4.1936455 -4.2240644 -4.2568274 -4.283546 -4.2948761 -4.2950678 -4.2882576 -4.2827716][-4.2057943 -4.1898141 -4.17972 -4.1653762 -4.1513391 -4.1550236 -4.1879869 -4.2216005 -4.248611 -4.2736692 -4.2908373 -4.2971792 -4.2962804 -4.2929692 -4.2936368][-4.2289839 -4.21587 -4.2098355 -4.1996055 -4.1862197 -4.189177 -4.2202477 -4.2483072 -4.2675085 -4.2852178 -4.2941208 -4.2946978 -4.2937684 -4.2932782 -4.2975688][-4.2506704 -4.2429719 -4.2402024 -4.2352543 -4.2255197 -4.2272487 -4.2512193 -4.2692838 -4.2783604 -4.2879114 -4.2905927 -4.288887 -4.2888136 -4.2915883 -4.2983608][-4.2626848 -4.261189 -4.2632232 -4.2639389 -4.2587028 -4.2577777 -4.2698555 -4.275712 -4.2754517 -4.27837 -4.2768803 -4.2740922 -4.2736692 -4.2773533 -4.2850385]]...]
INFO - root - 2017-12-07 11:43:51.235889: step 4910, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 60h:51m:07s remains)
INFO - root - 2017-12-07 11:43:57.790640: step 4920, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 64h:34m:52s remains)
INFO - root - 2017-12-07 11:44:04.653866: step 4930, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 63h:19m:49s remains)
INFO - root - 2017-12-07 11:44:11.373406: step 4940, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 57h:52m:11s remains)
INFO - root - 2017-12-07 11:44:18.236075: step 4950, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.735 sec/batch; 66h:50m:03s remains)
INFO - root - 2017-12-07 11:44:25.141098: step 4960, loss = 2.09, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 65h:20m:05s remains)
INFO - root - 2017-12-07 11:44:31.891527: step 4970, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 62h:41m:12s remains)
INFO - root - 2017-12-07 11:44:38.690952: step 4980, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 62h:51m:49s remains)
INFO - root - 2017-12-07 11:44:45.444535: step 4990, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 56h:16m:00s remains)
INFO - root - 2017-12-07 11:44:52.221276: step 5000, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 62h:17m:51s remains)
2017-12-07 11:44:53.072933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1773934 -4.1777968 -4.181354 -4.177084 -4.1407857 -4.1002131 -4.0845156 -4.08495 -4.1086316 -4.1466641 -4.1868396 -4.2175074 -4.2398257 -4.2583232 -4.26255][-4.1645622 -4.1738396 -4.1839108 -4.1854358 -4.1553855 -4.1141586 -4.0970917 -4.0953183 -4.1075745 -4.1344109 -4.1681695 -4.2010727 -4.2298341 -4.2476029 -4.2532854][-4.1440787 -4.1686358 -4.1870151 -4.1848936 -4.153192 -4.1121531 -4.0983286 -4.0963769 -4.1039176 -4.1260595 -4.1566143 -4.1902962 -4.216259 -4.22794 -4.236207][-4.1082573 -4.1534138 -4.18381 -4.184401 -4.1551752 -4.1205621 -4.1041617 -4.0899467 -4.0899796 -4.1088271 -4.1347518 -4.1690879 -4.1950388 -4.2046809 -4.2168393][-4.0680852 -4.13843 -4.1868458 -4.1974812 -4.1747642 -4.1463389 -4.1166282 -4.0729933 -4.0682349 -4.0981336 -4.1234097 -4.1549206 -4.1798162 -4.1922565 -4.2112012][-4.0571961 -4.1431904 -4.2027631 -4.2196746 -4.191761 -4.1535554 -4.0870252 -3.9951034 -3.9922073 -4.0562592 -4.1021361 -4.136198 -4.1607366 -4.1812296 -4.2093134][-4.0822663 -4.157341 -4.20887 -4.2212534 -4.1824217 -4.12207 -4.0050249 -3.8536248 -3.8613691 -3.9816732 -4.0682788 -4.1039963 -4.1223097 -4.1506782 -4.19022][-4.1176624 -4.1740603 -4.2110672 -4.2143407 -4.1639009 -4.0824456 -3.9298353 -3.732224 -3.7589462 -3.9343853 -4.0510888 -4.0813689 -4.0837584 -4.109416 -4.1563134][-4.1644421 -4.2010484 -4.2260385 -4.2159262 -4.1594434 -4.086328 -3.9584084 -3.7937047 -3.8392131 -4.0039682 -4.0989151 -4.1011405 -4.0723472 -4.0828404 -4.1265726][-4.2194061 -4.2372365 -4.250752 -4.2326713 -4.1823573 -4.1316586 -4.056251 -3.9613771 -4.0047007 -4.1109581 -4.1570072 -4.1319752 -4.0872436 -4.0970621 -4.134944][-4.2568235 -4.264677 -4.2688608 -4.2551551 -4.220562 -4.18767 -4.147728 -4.0991268 -4.1249676 -4.1790075 -4.1938639 -4.1551914 -4.1077204 -4.1205049 -4.1579547][-4.2764444 -4.2779655 -4.2793183 -4.2730823 -4.2523494 -4.2327719 -4.2120142 -4.1868644 -4.197588 -4.2166243 -4.2196774 -4.1869249 -4.1470079 -4.1485543 -4.1765375][-4.2907977 -4.2882214 -4.288692 -4.2887468 -4.2804565 -4.2665744 -4.2553167 -4.2426872 -4.2440572 -4.2463512 -4.2455921 -4.2266626 -4.1920328 -4.1779537 -4.188385][-4.3058786 -4.3037624 -4.3033366 -4.3033047 -4.297524 -4.2855749 -4.2776561 -4.270256 -4.2675171 -4.2676439 -4.2659826 -4.2566342 -4.2336926 -4.2150297 -4.2134881][-4.3170853 -4.3169875 -4.3153157 -4.3139462 -4.3080974 -4.2967086 -4.2895088 -4.2839031 -4.27984 -4.2807512 -4.2823157 -4.2801986 -4.2673607 -4.2508669 -4.244247]]...]
INFO - root - 2017-12-07 11:44:59.834492: step 5010, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 59h:23m:16s remains)
INFO - root - 2017-12-07 11:45:06.413093: step 5020, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:59m:44s remains)
INFO - root - 2017-12-07 11:45:13.118226: step 5030, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 60h:11m:16s remains)
INFO - root - 2017-12-07 11:45:19.863248: step 5040, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 63h:20m:17s remains)
INFO - root - 2017-12-07 11:45:26.606269: step 5050, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:37m:53s remains)
INFO - root - 2017-12-07 11:45:33.520938: step 5060, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.676 sec/batch; 61h:27m:39s remains)
INFO - root - 2017-12-07 11:45:40.354157: step 5070, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 59h:46m:23s remains)
INFO - root - 2017-12-07 11:45:47.158949: step 5080, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 65h:13m:28s remains)
INFO - root - 2017-12-07 11:45:54.035523: step 5090, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 66h:17m:23s remains)
INFO - root - 2017-12-07 11:46:00.824487: step 5100, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 61h:42m:57s remains)
2017-12-07 11:46:01.492331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1982517 -4.2234006 -4.23517 -4.2254796 -4.2245431 -4.2430067 -4.2535253 -4.2491221 -4.240253 -4.2415867 -4.2579942 -4.2704763 -4.2566648 -4.2175064 -4.1824579][-4.1594715 -4.1864095 -4.206429 -4.2063112 -4.2074995 -4.2209616 -4.23124 -4.2275543 -4.217329 -4.2163224 -4.2300105 -4.2450433 -4.2345357 -4.19443 -4.1568422][-4.1884584 -4.2054443 -4.2207437 -4.21903 -4.2137375 -4.2124615 -4.2094169 -4.2035036 -4.200202 -4.2074714 -4.2252932 -4.2453632 -4.2402143 -4.2057924 -4.1707153][-4.2473812 -4.2517986 -4.2533617 -4.24454 -4.2285337 -4.21406 -4.196497 -4.1852446 -4.1906295 -4.2069135 -4.2262449 -4.2461252 -4.2464671 -4.2234154 -4.2015777][-4.2719541 -4.2678738 -4.2595739 -4.2448878 -4.2200084 -4.1972456 -4.1702557 -4.1533966 -4.1637483 -4.1857805 -4.2053838 -4.2255135 -4.2304931 -4.2169261 -4.2117515][-4.2627282 -4.2547445 -4.2374206 -4.2128906 -4.1757274 -4.1398144 -4.1010504 -4.0799417 -4.1002183 -4.136281 -4.1656346 -4.189002 -4.1989279 -4.1996922 -4.20917][-4.2383919 -4.2285666 -4.2007642 -4.1621265 -4.1083517 -4.0493836 -3.98707 -3.9542165 -3.9905314 -4.0556087 -4.104516 -4.1379089 -4.1576486 -4.1774693 -4.2014685][-4.2081614 -4.1974349 -4.1642914 -4.1151319 -4.0507941 -3.9760323 -3.8977132 -3.8604023 -3.911932 -4.0037637 -4.074657 -4.1214023 -4.1476645 -4.1721506 -4.1998124][-4.2130237 -4.2022767 -4.1681628 -4.1200066 -4.0673018 -4.0068865 -3.9448752 -3.9231672 -3.9687986 -4.045074 -4.1092138 -4.1520796 -4.1696262 -4.1739516 -4.1819625][-4.2375503 -4.2276845 -4.2004242 -4.1648383 -4.13255 -4.092823 -4.052494 -4.0410604 -4.0704737 -4.1201935 -4.1637187 -4.1885419 -4.1871014 -4.170022 -4.16258][-4.2470784 -4.2459984 -4.2317419 -4.2044377 -4.1799359 -4.1506004 -4.130127 -4.1307549 -4.15006 -4.1796303 -4.2049789 -4.2161441 -4.2036152 -4.1773314 -4.1615996][-4.2446952 -4.2522545 -4.2491932 -4.2304468 -4.2124596 -4.1947885 -4.1893559 -4.1979132 -4.2096157 -4.2232494 -4.2315545 -4.2305322 -4.2139211 -4.18812 -4.1709023][-4.2551794 -4.2657318 -4.2690568 -4.2627988 -4.2536125 -4.2440076 -4.2413278 -4.2450585 -4.2466674 -4.2498422 -4.2491679 -4.2415485 -4.2253757 -4.200407 -4.1795244][-4.28892 -4.2988524 -4.3011441 -4.2969027 -4.2897754 -4.2826705 -4.278059 -4.2774076 -4.2759857 -4.2749047 -4.2717276 -4.2628789 -4.2493105 -4.2280011 -4.2053022][-4.3092222 -4.3158946 -4.3178849 -4.3164911 -4.3123264 -4.3079252 -4.3048358 -4.3049493 -4.3050447 -4.3040233 -4.3009257 -4.2947483 -4.287498 -4.275516 -4.2590985]]...]
INFO - root - 2017-12-07 11:46:08.246573: step 5110, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 59h:27m:03s remains)
INFO - root - 2017-12-07 11:46:15.019443: step 5120, loss = 2.06, batch loss = 2.01 (10.6 examples/sec; 0.757 sec/batch; 68h:48m:30s remains)
INFO - root - 2017-12-07 11:46:21.856050: step 5130, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 64h:20m:17s remains)
INFO - root - 2017-12-07 11:46:28.615828: step 5140, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:38m:40s remains)
INFO - root - 2017-12-07 11:46:35.498281: step 5150, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 60h:44m:07s remains)
INFO - root - 2017-12-07 11:46:42.389332: step 5160, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 63h:26m:38s remains)
INFO - root - 2017-12-07 11:46:49.229588: step 5170, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 64h:25m:20s remains)
INFO - root - 2017-12-07 11:46:56.052061: step 5180, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 64h:58m:23s remains)
INFO - root - 2017-12-07 11:47:02.834226: step 5190, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 59h:31m:26s remains)
INFO - root - 2017-12-07 11:47:09.654523: step 5200, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 63h:59m:08s remains)
2017-12-07 11:47:10.343573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.339066 -4.3396435 -4.3390722 -4.3389473 -4.3389087 -4.3388515 -4.3377147 -4.3364735 -4.3364811 -4.3372707 -4.33815 -4.3385878 -4.3392377 -4.3395567 -4.3408976][-4.328959 -4.3283229 -4.3277855 -4.3265824 -4.3258061 -4.3255281 -4.3227348 -4.3216925 -4.3244724 -4.3260255 -4.3270144 -4.3283172 -4.3305888 -4.33166 -4.3334117][-4.3085551 -4.3066597 -4.3055482 -4.3034658 -4.30148 -4.3011184 -4.2947607 -4.2908244 -4.2959032 -4.2987552 -4.2984557 -4.2983894 -4.3019466 -4.30487 -4.3095345][-4.2816 -4.2775235 -4.2744942 -4.2710743 -4.2664938 -4.2652831 -4.2501125 -4.2380381 -4.24703 -4.2529345 -4.2528353 -4.2517705 -4.2566814 -4.2631369 -4.2726026][-4.2499137 -4.2394338 -4.2331891 -4.228446 -4.2191305 -4.2125235 -4.178926 -4.1519928 -4.1685467 -4.1845517 -4.1899061 -4.1900315 -4.1952224 -4.206562 -4.2250195][-4.2124834 -4.1932354 -4.1827106 -4.1747665 -4.1591511 -4.1431022 -4.0803208 -4.0320954 -4.066895 -4.1059432 -4.1243043 -4.131669 -4.1391959 -4.1557403 -4.1863408][-4.1852365 -4.1581955 -4.1381059 -4.1187286 -4.0915461 -4.0556116 -3.9557562 -3.8829746 -3.951508 -4.0287528 -4.074111 -4.0996618 -4.1132531 -4.1343927 -4.1738539][-4.173223 -4.1424017 -4.1155024 -4.0877581 -4.0574379 -4.0188189 -3.9211435 -3.8584726 -3.9474828 -4.0398355 -4.0965738 -4.1287913 -4.1434121 -4.1631365 -4.1988492][-4.1874337 -4.1628456 -4.1430912 -4.1254535 -4.112586 -4.0937591 -4.0327411 -3.9936371 -4.0562906 -4.1168485 -4.1546307 -4.1770325 -4.1886845 -4.2054825 -4.2320194][-4.214304 -4.200953 -4.1945257 -4.1898618 -4.1903729 -4.1850615 -4.1510124 -4.1284504 -4.163156 -4.1930685 -4.2113495 -4.2237482 -4.2328987 -4.2440414 -4.2628174][-4.2446747 -4.2408895 -4.2448831 -4.2469721 -4.2498155 -4.2486467 -4.22868 -4.2129588 -4.2303925 -4.242732 -4.2491941 -4.2582359 -4.2686286 -4.2761917 -4.287827][-4.2713451 -4.2714105 -4.2781787 -4.2816257 -4.2826047 -4.2821131 -4.2665005 -4.2517285 -4.2601776 -4.2650323 -4.267168 -4.2754431 -4.2871451 -4.2942286 -4.3023906][-4.291234 -4.29252 -4.2980146 -4.3008304 -4.3010216 -4.2990274 -4.2855759 -4.273447 -4.2769647 -4.27816 -4.2795067 -4.2874722 -4.2983165 -4.3047466 -4.3100309][-4.3090396 -4.3095727 -4.3123302 -4.3145146 -4.3148608 -4.3129969 -4.3040929 -4.2976408 -4.3002648 -4.3008308 -4.3017778 -4.3058457 -4.3117824 -4.3154125 -4.3180504][-4.3244796 -4.3244634 -4.3265734 -4.3287406 -4.3291278 -4.3281174 -4.3242345 -4.322176 -4.3230205 -4.3235307 -4.3241267 -4.32422 -4.3258014 -4.3265166 -4.3279405]]...]
INFO - root - 2017-12-07 11:47:17.072360: step 5210, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 58h:18m:43s remains)
INFO - root - 2017-12-07 11:47:23.719990: step 5220, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 61h:28m:45s remains)
INFO - root - 2017-12-07 11:47:30.548060: step 5230, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 64h:27m:41s remains)
INFO - root - 2017-12-07 11:47:37.409441: step 5240, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 64h:23m:38s remains)
INFO - root - 2017-12-07 11:47:44.210807: step 5250, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 61h:37m:10s remains)
INFO - root - 2017-12-07 11:47:51.195587: step 5260, loss = 2.09, batch loss = 2.04 (11.9 examples/sec; 0.674 sec/batch; 61h:17m:44s remains)
INFO - root - 2017-12-07 11:47:58.113101: step 5270, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 61h:07m:34s remains)
INFO - root - 2017-12-07 11:48:04.931073: step 5280, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 63h:28m:49s remains)
INFO - root - 2017-12-07 11:48:11.795257: step 5290, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 60h:00m:34s remains)
INFO - root - 2017-12-07 11:48:18.591719: step 5300, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:53m:56s remains)
2017-12-07 11:48:19.296989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2785258 -4.2855368 -4.2913747 -4.2847619 -4.2654796 -4.2502508 -4.2325988 -4.222158 -4.2213535 -4.21996 -4.2251039 -4.2359066 -4.2488527 -4.2701745 -4.2899017][-4.2532573 -4.2601371 -4.2659407 -4.2580256 -4.2351193 -4.2166457 -4.1979446 -4.189671 -4.19128 -4.1874633 -4.1879206 -4.1968455 -4.2118092 -4.2381368 -4.2598691][-4.2117004 -4.2202544 -4.2280383 -4.2182064 -4.1922441 -4.1693025 -4.1502771 -4.1471038 -4.1556206 -4.153357 -4.1508026 -4.157589 -4.1717172 -4.2001324 -4.2215285][-4.1692977 -4.1814346 -4.1937938 -4.184021 -4.1542792 -4.1234293 -4.10209 -4.1034751 -4.1197462 -4.1225381 -4.1201158 -4.126554 -4.1409378 -4.1704321 -4.1889877][-4.134428 -4.147347 -4.164381 -4.1578007 -4.1262407 -4.0869184 -4.0612812 -4.06544 -4.0884004 -4.1000109 -4.103272 -4.1104364 -4.1268406 -4.1558414 -4.1698627][-4.1093478 -4.1223416 -4.1437917 -4.1397977 -4.1008854 -4.05012 -4.0201497 -4.028059 -4.0584407 -4.0830374 -4.0961123 -4.1059966 -4.1238461 -4.1491156 -4.15862][-4.1110539 -4.1276927 -4.148375 -4.1382828 -4.0862355 -4.0201769 -3.9839079 -3.9943259 -4.0320959 -4.0713429 -4.0948043 -4.1093411 -4.129354 -4.1501322 -4.1578][-4.1226606 -4.1444287 -4.1630044 -4.1462927 -4.0852604 -4.0118585 -3.975033 -3.9882846 -4.0281315 -4.0731411 -4.0996461 -4.1139426 -4.1320477 -4.1504683 -4.1628585][-4.1268682 -4.14996 -4.1662679 -4.1484766 -4.0918522 -4.0266776 -3.999444 -4.0166388 -4.0515909 -4.0874972 -4.10486 -4.1116829 -4.1247797 -4.1436186 -4.1626539][-4.1276107 -4.1507006 -4.167398 -4.1538429 -4.1086488 -4.0561013 -4.0341458 -4.046762 -4.0729475 -4.0968366 -4.1041369 -4.1053381 -4.1162262 -4.1373506 -4.1602478][-4.1278186 -4.146347 -4.1631274 -4.1553049 -4.1220784 -4.08181 -4.0636292 -4.0703468 -4.089201 -4.1046877 -4.1078691 -4.1094637 -4.1226659 -4.14678 -4.17083][-4.1236343 -4.1368046 -4.1530566 -4.1514854 -4.1292238 -4.1019406 -4.0911517 -4.0990281 -4.1144037 -4.1240349 -4.1249857 -4.1274886 -4.1434355 -4.1690459 -4.1920042][-4.1239262 -4.1324744 -4.1467733 -4.1482635 -4.1326036 -4.1140509 -4.1106448 -4.1229091 -4.1389117 -4.1458764 -4.1460176 -4.1488662 -4.165205 -4.1908927 -4.2097931][-4.1316547 -4.1352878 -4.1454639 -4.149281 -4.1396828 -4.1279716 -4.1295204 -4.1449566 -4.1621189 -4.1679115 -4.1666613 -4.1685467 -4.1822553 -4.205297 -4.2216787][-4.1362958 -4.1365662 -4.1454363 -4.1538773 -4.1495571 -4.1438456 -4.1477222 -4.1635761 -4.1807671 -4.185966 -4.1854086 -4.1875129 -4.1983323 -4.2183056 -4.2317171]]...]
INFO - root - 2017-12-07 11:48:26.038625: step 5310, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.667 sec/batch; 60h:37m:58s remains)
INFO - root - 2017-12-07 11:48:32.592815: step 5320, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 63h:01m:01s remains)
INFO - root - 2017-12-07 11:48:39.407768: step 5330, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.648 sec/batch; 58h:51m:04s remains)
INFO - root - 2017-12-07 11:48:46.257684: step 5340, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 63h:34m:55s remains)
INFO - root - 2017-12-07 11:48:53.197803: step 5350, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 65h:41m:40s remains)
INFO - root - 2017-12-07 11:49:00.038055: step 5360, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 62h:47m:19s remains)
INFO - root - 2017-12-07 11:49:06.843058: step 5370, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 59h:23m:12s remains)
INFO - root - 2017-12-07 11:49:13.607918: step 5380, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:33m:23s remains)
INFO - root - 2017-12-07 11:49:20.401355: step 5390, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 62h:17m:33s remains)
INFO - root - 2017-12-07 11:49:27.239252: step 5400, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 64h:40m:26s remains)
2017-12-07 11:49:27.891196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3089123 -4.3119287 -4.3146629 -4.3162556 -4.3159127 -4.3122787 -4.3064728 -4.3001518 -4.2967091 -4.2988 -4.3051414 -4.3131404 -4.3207626 -4.3234267 -4.3210163][-4.3053269 -4.3102188 -4.312016 -4.3091979 -4.3022294 -4.2918844 -4.2805452 -4.2712 -4.2653594 -4.2694049 -4.2804527 -4.2956843 -4.3111048 -4.3188734 -4.3187094][-4.3056908 -4.3086085 -4.30731 -4.2973957 -4.2808838 -4.261559 -4.2446485 -4.2318044 -4.222877 -4.2267938 -4.243094 -4.2675719 -4.2929354 -4.3081379 -4.3122396][-4.3021064 -4.3020892 -4.2974072 -4.2810955 -4.2554507 -4.2253094 -4.1992726 -4.1819568 -4.1701012 -4.1737151 -4.1970186 -4.2312832 -4.2651811 -4.2882214 -4.2976432][-4.2931852 -4.2895613 -4.2823987 -4.261982 -4.2261567 -4.1815228 -4.1410265 -4.1154437 -4.0962276 -4.0960865 -4.1286311 -4.1783166 -4.2246537 -4.259973 -4.2790675][-4.2777638 -4.2722311 -4.265255 -4.2408934 -4.19063 -4.1254997 -4.0662766 -4.0254993 -3.9923053 -3.9863925 -4.0341439 -4.1093988 -4.1734772 -4.2232032 -4.2542405][-4.2608628 -4.2507997 -4.2439432 -4.2213168 -4.1679497 -4.0924711 -4.0169997 -3.9553761 -3.8984785 -3.8851132 -3.9526491 -4.0575876 -4.1424818 -4.2042108 -4.2426906][-4.2466054 -4.2327008 -4.22966 -4.2184896 -4.1816583 -4.1179452 -4.0416141 -3.9668128 -3.8916943 -3.8704154 -3.9438596 -4.0580444 -4.14777 -4.2114844 -4.2523603][-4.231925 -4.2149305 -4.2171803 -4.2234807 -4.2149758 -4.1832886 -4.1297336 -4.0617132 -3.9837065 -3.950268 -4.0046358 -4.1015134 -4.1783462 -4.2281494 -4.2606626][-4.1964068 -4.1776652 -4.191566 -4.2213511 -4.2425961 -4.2436776 -4.2201214 -4.1718078 -4.1052566 -4.0633383 -4.0861306 -4.1504226 -4.205162 -4.2378469 -4.2574034][-4.1542892 -4.1372209 -4.158926 -4.2063332 -4.2483172 -4.2679887 -4.2684317 -4.2453747 -4.2023273 -4.1652904 -4.1633353 -4.1963305 -4.22953 -4.2450109 -4.2511625][-4.0947843 -4.0704126 -4.0944233 -4.1598682 -4.2223759 -4.2587209 -4.2808228 -4.2792468 -4.2583261 -4.2366414 -4.227427 -4.23922 -4.255794 -4.2590704 -4.2531505][-4.0137825 -3.9681804 -3.990391 -4.0793266 -4.170413 -4.2278929 -4.2672229 -4.2841759 -4.2852182 -4.2856064 -4.2831244 -4.2829208 -4.2841883 -4.273828 -4.258153][-3.9555008 -3.8870754 -3.9073203 -4.0184808 -4.1373696 -4.2123303 -4.2585154 -4.28305 -4.2958994 -4.3111629 -4.3163815 -4.3131723 -4.306706 -4.2884479 -4.2677913][-3.9684408 -3.8982019 -3.9174044 -4.0292063 -4.1475582 -4.2182178 -4.2597761 -4.2827711 -4.2961936 -4.3139319 -4.3249593 -4.3262835 -4.3225765 -4.3074865 -4.2887783]]...]
INFO - root - 2017-12-07 11:49:34.637922: step 5410, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:51m:58s remains)
INFO - root - 2017-12-07 11:49:41.333308: step 5420, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 65h:09m:46s remains)
INFO - root - 2017-12-07 11:49:48.220681: step 5430, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 64h:03m:44s remains)
INFO - root - 2017-12-07 11:49:54.987049: step 5440, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:34m:28s remains)
INFO - root - 2017-12-07 11:50:01.810955: step 5450, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 60h:14m:53s remains)
INFO - root - 2017-12-07 11:50:08.659967: step 5460, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.733 sec/batch; 66h:34m:29s remains)
INFO - root - 2017-12-07 11:50:15.560306: step 5470, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.738 sec/batch; 67h:04m:58s remains)
INFO - root - 2017-12-07 11:50:22.304278: step 5480, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.672 sec/batch; 61h:03m:25s remains)
INFO - root - 2017-12-07 11:50:29.145702: step 5490, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 61h:14m:45s remains)
INFO - root - 2017-12-07 11:50:35.930049: step 5500, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 60h:47m:16s remains)
2017-12-07 11:50:36.669624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2008085 -4.2031093 -4.1974945 -4.1873684 -4.1913371 -4.1973581 -4.2031479 -4.208632 -4.2144566 -4.2204032 -4.2152877 -4.2160583 -4.2295523 -4.2356081 -4.2344117][-4.1834216 -4.1897473 -4.1846852 -4.1759729 -4.1867805 -4.1947956 -4.1979246 -4.1990767 -4.2017684 -4.2076883 -4.2022548 -4.1987967 -4.2108603 -4.2231193 -4.2276754][-4.1620126 -4.1694231 -4.1656647 -4.16464 -4.178431 -4.1872225 -4.194447 -4.1991692 -4.20263 -4.2087946 -4.2008829 -4.187254 -4.1946707 -4.2126093 -4.2231975][-4.1525459 -4.1569347 -4.1540546 -4.1607757 -4.1684589 -4.1734037 -4.1853094 -4.1939845 -4.2006965 -4.2030392 -4.1922269 -4.1685448 -4.1697488 -4.1944046 -4.2145147][-4.141922 -4.1406951 -4.1390319 -4.1495147 -4.15363 -4.1510129 -4.1586504 -4.1689081 -4.1799822 -4.1825342 -4.1734385 -4.14365 -4.1347165 -4.1574059 -4.183857][-4.1387138 -4.122581 -4.1080151 -4.1115217 -4.1053972 -4.0805597 -4.06902 -4.09008 -4.1267824 -4.1452541 -4.1455083 -4.1177549 -4.1017637 -4.118711 -4.142334][-4.1211691 -4.0955076 -4.0655808 -4.0535512 -4.02732 -3.9603641 -3.8893054 -3.9239075 -4.0183983 -4.0801535 -4.1078591 -4.0928645 -4.0828481 -4.1040983 -4.1255379][-4.0920286 -4.0593228 -4.0156093 -3.9887993 -3.9528959 -3.8508048 -3.7015281 -3.7167227 -3.8757439 -3.9882553 -4.0380135 -4.0399308 -4.0573187 -4.0977159 -4.1228657][-4.0829821 -4.0561986 -4.02568 -4.0048251 -3.9870608 -3.9137368 -3.7882946 -3.7675519 -3.89664 -4.0062361 -4.0523672 -4.0640373 -4.0904107 -4.1360831 -4.1610761][-4.0821843 -4.0701079 -4.0668626 -4.0671811 -4.0736108 -4.0469427 -3.98831 -3.9636986 -4.0275526 -4.09971 -4.1264729 -4.1389303 -4.1587415 -4.18827 -4.2026849][-4.0951471 -4.0998507 -4.1149883 -4.1295705 -4.1499949 -4.1540117 -4.1363845 -4.1210666 -4.1404414 -4.1734133 -4.181263 -4.1842575 -4.1983929 -4.2183485 -4.22613][-4.1279984 -4.1444225 -4.1728959 -4.1912918 -4.2063732 -4.21701 -4.2167482 -4.2071657 -4.2010036 -4.2071252 -4.193696 -4.175209 -4.1806312 -4.2014103 -4.2153082][-4.15851 -4.1799116 -4.2102251 -4.2222319 -4.2305164 -4.2375183 -4.2390862 -4.2293549 -4.2110362 -4.2006145 -4.1731482 -4.1333222 -4.1316004 -4.1648731 -4.1950269][-4.1652417 -4.1880083 -4.2136774 -4.2223868 -4.2285347 -4.23565 -4.2395577 -4.2322435 -4.2159591 -4.2004647 -4.1747909 -4.1403332 -4.1426005 -4.185112 -4.219214][-4.1812592 -4.192482 -4.208303 -4.2149191 -4.2229047 -4.2318115 -4.2379651 -4.2347131 -4.2255588 -4.217948 -4.2060833 -4.191391 -4.2011337 -4.2402263 -4.2679505]]...]
INFO - root - 2017-12-07 11:50:43.081886: step 5510, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 57h:23m:20s remains)
INFO - root - 2017-12-07 11:50:49.751157: step 5520, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 59h:24m:02s remains)
INFO - root - 2017-12-07 11:50:56.529780: step 5530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:21m:12s remains)
INFO - root - 2017-12-07 11:51:03.361083: step 5540, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 64h:23m:03s remains)
INFO - root - 2017-12-07 11:51:10.089394: step 5550, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 59h:12m:02s remains)
INFO - root - 2017-12-07 11:51:16.881236: step 5560, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 59h:31m:48s remains)
INFO - root - 2017-12-07 11:51:23.759654: step 5570, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 60h:12m:43s remains)
INFO - root - 2017-12-07 11:51:30.527784: step 5580, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 62h:50m:17s remains)
INFO - root - 2017-12-07 11:51:37.333847: step 5590, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 65h:36m:03s remains)
INFO - root - 2017-12-07 11:51:43.988134: step 5600, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 64h:01m:30s remains)
2017-12-07 11:51:44.724370: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2807302 -4.2716684 -4.2607245 -4.2491074 -4.2383718 -4.2338161 -4.2345176 -4.23966 -4.2510333 -4.2650418 -4.2773089 -4.2824121 -4.2846813 -4.2910094 -4.3009257][-4.2654724 -4.2513528 -4.2380033 -4.2234163 -4.2078314 -4.1967163 -4.1890416 -4.1883221 -4.2003217 -4.2227182 -4.2457581 -4.2577014 -4.263267 -4.2715411 -4.2838635][-4.2545071 -4.2344956 -4.219821 -4.204567 -4.1813006 -4.1549816 -4.1297522 -4.1183081 -4.1330037 -4.1708608 -4.2097931 -4.2321162 -4.2423425 -4.2496634 -4.2614307][-4.2466135 -4.2227678 -4.2104235 -4.1958051 -4.160315 -4.1104321 -4.0613461 -4.0368109 -4.0527639 -4.1079831 -4.1668878 -4.2025595 -4.2197413 -4.2282805 -4.239161][-4.2360215 -4.2123642 -4.20461 -4.1904435 -4.1456218 -4.0753021 -4.0008478 -3.956857 -3.9697301 -4.0395026 -4.1191015 -4.1697655 -4.1977043 -4.2109628 -4.2238007][-4.2154202 -4.1954904 -4.1959243 -4.1887188 -4.143609 -4.0602932 -3.9619133 -3.8850212 -3.8755927 -3.9555187 -4.062068 -4.1324229 -4.1716371 -4.19191 -4.2096171][-4.1911044 -4.1734023 -4.180932 -4.1854019 -4.1514983 -4.0672979 -3.9454246 -3.822093 -3.7737586 -3.8663919 -4.0050321 -4.0908408 -4.1377306 -4.1661634 -4.1907287][-4.1797705 -4.1609712 -4.1714406 -4.1854525 -4.1695824 -4.1004353 -3.9737864 -3.8271105 -3.7590208 -3.8571093 -3.9989572 -4.0779696 -4.1172881 -4.1443172 -4.1716189][-4.1886415 -4.1677818 -4.1804233 -4.2021813 -4.2052884 -4.1613474 -4.0550752 -3.9218307 -3.8557718 -3.9264472 -4.0361643 -4.0891647 -4.1110144 -4.1295142 -4.1549144][-4.2182841 -4.1954713 -4.20306 -4.2244935 -4.2379165 -4.2167029 -4.1394863 -4.0305023 -3.9705539 -4.0092859 -4.0853949 -4.1172128 -4.1229272 -4.1332922 -4.1571732][-4.2654123 -4.2453718 -4.2453856 -4.2597857 -4.2753844 -4.2718592 -4.2240667 -4.14425 -4.0908537 -4.1026344 -4.15195 -4.1703434 -4.1670632 -4.1729417 -4.1914883][-4.306283 -4.2912197 -4.286921 -4.2924094 -4.3052216 -4.3094673 -4.2800894 -4.2219896 -4.1780167 -4.1777945 -4.2083688 -4.2228794 -4.221168 -4.2243228 -4.2356539][-4.322082 -4.3119988 -4.3079143 -4.3106494 -4.3202047 -4.3255224 -4.3073754 -4.267767 -4.2361226 -4.232718 -4.2480168 -4.2598605 -4.2620692 -4.26409 -4.2715006][-4.3229489 -4.3183537 -4.317009 -4.3182268 -4.3227034 -4.3242431 -4.3128786 -4.2886128 -4.2692394 -4.2642956 -4.2707973 -4.2794371 -4.2857823 -4.2900987 -4.2959809][-4.321805 -4.3197293 -4.3199792 -4.3197732 -4.31781 -4.313951 -4.3074961 -4.2964573 -4.286387 -4.2826691 -4.2847652 -4.2921891 -4.3015442 -4.3089013 -4.3133521]]...]
INFO - root - 2017-12-07 11:51:51.484463: step 5610, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 66h:19m:17s remains)
INFO - root - 2017-12-07 11:51:58.122236: step 5620, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 60h:44m:58s remains)
INFO - root - 2017-12-07 11:52:04.907838: step 5630, loss = 2.09, batch loss = 2.04 (11.0 examples/sec; 0.728 sec/batch; 66h:05m:13s remains)
INFO - root - 2017-12-07 11:52:11.657857: step 5640, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 58h:07m:44s remains)
INFO - root - 2017-12-07 11:52:18.413590: step 5650, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 57h:28m:07s remains)
INFO - root - 2017-12-07 11:52:25.225200: step 5660, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 63h:39m:41s remains)
INFO - root - 2017-12-07 11:52:32.000217: step 5670, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 63h:12m:41s remains)
INFO - root - 2017-12-07 11:52:38.820567: step 5680, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 65h:45m:37s remains)
INFO - root - 2017-12-07 11:52:45.677497: step 5690, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 60h:03m:50s remains)
INFO - root - 2017-12-07 11:52:52.520849: step 5700, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:23m:42s remains)
2017-12-07 11:52:53.311258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2506723 -4.229187 -4.1984787 -4.1694603 -4.151154 -4.1487789 -4.1464453 -4.1352677 -4.1310959 -4.13031 -4.124054 -4.1144595 -4.1007595 -4.0799046 -4.073884][-4.2496767 -4.2243857 -4.1859188 -4.1438875 -4.1108947 -4.0986319 -4.090405 -4.07549 -4.0649214 -4.0592637 -4.0524445 -4.0451937 -4.0319829 -4.0096612 -4.0062795][-4.2532816 -4.2250357 -4.1791477 -4.1265354 -4.0835423 -4.0650983 -4.05619 -4.0419455 -4.0275507 -4.01693 -4.0082536 -4.0020719 -3.9915736 -3.9731016 -3.9731236][-4.2569108 -4.2289743 -4.1829896 -4.1315002 -4.0894446 -4.07114 -4.0647311 -4.0535069 -4.0392632 -4.0278544 -4.018465 -4.0096374 -3.9961917 -3.9802237 -3.9793384][-4.2583141 -4.2357507 -4.1995687 -4.1620421 -4.1315064 -4.119504 -4.1189222 -4.1152864 -4.1054935 -4.0969491 -4.0871711 -4.0748796 -4.0568476 -4.0394764 -4.0340662][-4.25623 -4.243289 -4.2210684 -4.1978779 -4.1786571 -4.1746893 -4.1802468 -4.184515 -4.1810703 -4.1754937 -4.1658354 -4.1535473 -4.1362205 -4.118876 -4.1121993][-4.2501659 -4.2435422 -4.2289615 -4.2108831 -4.1952691 -4.1931829 -4.2019773 -4.2112136 -4.2150717 -4.2138934 -4.2079687 -4.2028689 -4.1947165 -4.18589 -4.1838174][-4.2416797 -4.2353158 -4.2201767 -4.2002759 -4.1838613 -4.1809206 -4.1893878 -4.2001305 -4.2064638 -4.2074037 -4.2074032 -4.2135963 -4.21897 -4.2223988 -4.2279096][-4.2333655 -4.2231436 -4.203145 -4.1800747 -4.1612043 -4.1569276 -4.1645837 -4.1715651 -4.1748991 -4.1754565 -4.1800337 -4.1961594 -4.2147317 -4.2294989 -4.2416835][-4.2267442 -4.2133265 -4.18918 -4.1646175 -4.1436672 -4.1374164 -4.1424518 -4.1445894 -4.1444945 -4.1455016 -4.1539283 -4.1759691 -4.2025776 -4.2244034 -4.2408605][-4.2230244 -4.2086673 -4.1848221 -4.1616473 -4.1392846 -4.131103 -4.1349273 -4.1362863 -4.1354618 -4.1383662 -4.1484175 -4.1717963 -4.2008009 -4.2244134 -4.2404103][-4.2214828 -4.2096696 -4.1898341 -4.1709247 -4.1502905 -4.1426878 -4.1478825 -4.1526127 -4.1529245 -4.1569376 -4.165659 -4.1860142 -4.2125864 -4.2327518 -4.2445126][-4.2256775 -4.217073 -4.20097 -4.1870513 -4.1713977 -4.1667509 -4.1746292 -4.1847849 -4.1877685 -4.1926842 -4.1989632 -4.2140231 -4.2336698 -4.2468939 -4.2525263][-4.2350755 -4.2294474 -4.2168512 -4.2065144 -4.1966748 -4.1959343 -4.2066731 -4.2213607 -4.2274756 -4.2326365 -4.2359085 -4.2448912 -4.2565784 -4.2630424 -4.263473][-4.24737 -4.244494 -4.2343149 -4.2254968 -4.2204285 -4.2227292 -4.2346263 -4.2507243 -4.2580905 -4.2621446 -4.2631464 -4.268055 -4.2741704 -4.2760472 -4.2736559]]...]
INFO - root - 2017-12-07 11:53:00.048906: step 5710, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 61h:54m:44s remains)
INFO - root - 2017-12-07 11:53:06.684516: step 5720, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.677 sec/batch; 61h:27m:34s remains)
INFO - root - 2017-12-07 11:53:13.499438: step 5730, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 60h:33m:22s remains)
INFO - root - 2017-12-07 11:53:20.345180: step 5740, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 63h:46m:15s remains)
INFO - root - 2017-12-07 11:53:27.214917: step 5750, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 64h:01m:46s remains)
INFO - root - 2017-12-07 11:53:33.970046: step 5760, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.708 sec/batch; 64h:18m:02s remains)
INFO - root - 2017-12-07 11:53:40.812775: step 5770, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:30m:30s remains)
INFO - root - 2017-12-07 11:53:47.550840: step 5780, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 58h:37m:11s remains)
INFO - root - 2017-12-07 11:53:54.347527: step 5790, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 66h:49m:36s remains)
INFO - root - 2017-12-07 11:54:01.121746: step 5800, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 65h:57m:47s remains)
2017-12-07 11:54:01.838583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1399517 -4.1511197 -4.16474 -4.1799545 -4.2030196 -4.2254853 -4.2422013 -4.2491884 -4.23875 -4.2102709 -4.1742015 -4.151052 -4.1538692 -4.1713181 -4.1838264][-4.1090155 -4.1377831 -4.1738558 -4.1992917 -4.2219262 -4.2370152 -4.2417626 -4.23819 -4.2242913 -4.1998358 -4.1756821 -4.1587319 -4.1584015 -4.1731038 -4.1853213][-4.1105375 -4.1441402 -4.19165 -4.21856 -4.2302251 -4.2283859 -4.22394 -4.2175012 -4.2063193 -4.1927614 -4.1828012 -4.1731067 -4.1679688 -4.1762733 -4.183919][-4.1264515 -4.1532426 -4.1900325 -4.2009716 -4.1908345 -4.170342 -4.1628475 -4.1654806 -4.1655931 -4.1626253 -4.1638136 -4.167 -4.1621113 -4.1634059 -4.1619973][-4.11812 -4.1324949 -4.148612 -4.1389723 -4.1011834 -4.0579042 -4.0492039 -4.0722108 -4.0908732 -4.100461 -4.1146822 -4.1348925 -4.1407909 -4.1367087 -4.1217766][-4.1174488 -4.1115394 -4.100112 -4.0630865 -3.9972391 -3.9214385 -3.8976755 -3.9434848 -3.993897 -4.02756 -4.0631309 -4.10693 -4.1339293 -4.1375637 -4.1173763][-4.125762 -4.0985322 -4.0578065 -3.9907985 -3.8992376 -3.7899933 -3.7265151 -3.7886376 -3.8874335 -3.9555373 -4.01396 -4.0793357 -4.1290603 -4.1466885 -4.1344361][-4.1447377 -4.1067214 -4.0568371 -3.9864566 -3.9070916 -3.814254 -3.737119 -3.7750406 -3.8746166 -3.9503939 -4.0067372 -4.0703588 -4.1227818 -4.149333 -4.1507483][-4.1784339 -4.1469274 -4.1084914 -4.0562162 -4.0067911 -3.9578073 -3.9105334 -3.9199507 -3.9801834 -4.03584 -4.0728812 -4.1127377 -4.1455564 -4.1650276 -4.1713314][-4.2099676 -4.1869507 -4.1645141 -4.1351647 -4.1054721 -4.0854168 -4.0709305 -4.075561 -4.1114974 -4.148725 -4.1701312 -4.1912394 -4.2029262 -4.2051139 -4.2003894][-4.23855 -4.2222071 -4.211617 -4.19857 -4.1812816 -4.175199 -4.1774163 -4.1819377 -4.2036462 -4.2306881 -4.2448478 -4.2572837 -4.2612147 -4.2555361 -4.2404947][-4.2648211 -4.2509389 -4.2451558 -4.2428966 -4.2357178 -4.2379656 -4.2469444 -4.2500658 -4.2592082 -4.2733097 -4.283483 -4.2893682 -4.2910237 -4.2873383 -4.271934][-4.2797313 -4.2701445 -4.2666636 -4.26779 -4.26752 -4.2718897 -4.2802668 -4.2850246 -4.2891932 -4.293983 -4.2983313 -4.3004513 -4.3003273 -4.3013749 -4.29392][-4.2846642 -4.2784224 -4.2755771 -4.2774444 -4.2797537 -4.2844419 -4.2920861 -4.2981491 -4.3000875 -4.3001842 -4.3014159 -4.2998691 -4.2983174 -4.3008008 -4.3005342][-4.2882838 -4.2832761 -4.2804322 -4.2803693 -4.2818456 -4.2847052 -4.2898674 -4.2955642 -4.2974377 -4.2966938 -4.2961154 -4.2952471 -4.2951431 -4.2972393 -4.2989469]]...]
INFO - root - 2017-12-07 11:54:08.679060: step 5810, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 58h:44m:17s remains)
INFO - root - 2017-12-07 11:54:15.209017: step 5820, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 59h:19m:02s remains)
INFO - root - 2017-12-07 11:54:21.969275: step 5830, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 61h:54m:56s remains)
INFO - root - 2017-12-07 11:54:28.767060: step 5840, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 59h:41m:03s remains)
INFO - root - 2017-12-07 11:54:35.596780: step 5850, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 62h:31m:55s remains)
INFO - root - 2017-12-07 11:54:42.478873: step 5860, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 63h:00m:59s remains)
INFO - root - 2017-12-07 11:54:49.277733: step 5870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 61h:36m:20s remains)
INFO - root - 2017-12-07 11:54:56.052225: step 5880, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 58h:21m:46s remains)
INFO - root - 2017-12-07 11:55:02.847812: step 5890, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:58m:08s remains)
INFO - root - 2017-12-07 11:55:09.658262: step 5900, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 63h:06m:03s remains)
2017-12-07 11:55:10.452531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462282 -4.256062 -4.2722821 -4.282115 -4.2864304 -4.281919 -4.2696176 -4.2583981 -4.2587743 -4.2649207 -4.2700548 -4.270165 -4.2623792 -4.2493048 -4.2347589][-4.2178564 -4.2328386 -4.2567329 -4.2684488 -4.2727056 -4.2640805 -4.24454 -4.2300496 -4.2317829 -4.2422962 -4.2515297 -4.2525811 -4.2416062 -4.2184982 -4.1931553][-4.1899724 -4.2051129 -4.2333965 -4.2483687 -4.2511177 -4.2368922 -4.2090511 -4.1919203 -4.1930342 -4.2090349 -4.224421 -4.233233 -4.2260017 -4.1938953 -4.1567416][-4.1645179 -4.1774383 -4.2052131 -4.2186327 -4.2159424 -4.1940475 -4.1562972 -4.1369281 -4.14336 -4.1651449 -4.1853409 -4.2012072 -4.2021532 -4.1701179 -4.1232677][-4.1432915 -4.1591849 -4.1826859 -4.1855173 -4.1680961 -4.1339612 -4.0823212 -4.05796 -4.076808 -4.1117892 -4.1448445 -4.1695237 -4.1802535 -4.15281 -4.0993524][-4.1325731 -4.1525578 -4.1702542 -4.158433 -4.1227694 -4.0668521 -3.9954967 -3.9666414 -4.0081878 -4.0659451 -4.118145 -4.15578 -4.1761508 -4.1525636 -4.0944414][-4.149375 -4.1697922 -4.1759386 -4.1452894 -4.0839791 -4.0003147 -3.9062533 -3.875515 -3.9341993 -4.015842 -4.0936666 -4.1514425 -4.1828022 -4.1679344 -4.1121893][-4.1856256 -4.2022343 -4.1992788 -4.1603642 -4.08881 -3.9929004 -3.8926897 -3.865952 -3.9199667 -4.0056529 -4.0933266 -4.1641774 -4.1996441 -4.1876993 -4.1403823][-4.2136354 -4.2286482 -4.2260442 -4.1958275 -4.1414227 -4.0606179 -3.9765399 -3.9543529 -3.9931459 -4.0552197 -4.1243925 -4.192626 -4.2241483 -4.2057858 -4.1609674][-4.2125459 -4.2256684 -4.2288451 -4.2184439 -4.1911254 -4.1399579 -4.07843 -4.0569205 -4.080008 -4.1167822 -4.162128 -4.2169061 -4.2417512 -4.2180376 -4.1694932][-4.2117667 -4.2234521 -4.2339544 -4.2401829 -4.2359791 -4.2134433 -4.1708145 -4.1471944 -4.1559529 -4.1772118 -4.2055807 -4.2433453 -4.2592726 -4.23241 -4.1807656][-4.2294869 -4.2403502 -4.2534671 -4.2675548 -4.2772284 -4.2737522 -4.2466559 -4.2247467 -4.2269063 -4.2408023 -4.2541018 -4.2764473 -4.2818213 -4.2571163 -4.207057][-4.2597485 -4.274477 -4.2886238 -4.30052 -4.3075371 -4.3093534 -4.2926559 -4.2779045 -4.2817006 -4.2956858 -4.3015089 -4.312387 -4.30972 -4.2846203 -4.2410126][-4.2924018 -4.313417 -4.3290381 -4.3337841 -4.331131 -4.3248863 -4.3127456 -4.303515 -4.3109145 -4.3281889 -4.3340206 -4.3377023 -4.32848 -4.3036261 -4.2710953][-4.3188472 -4.3400884 -4.3521709 -4.3502712 -4.3393097 -4.3272638 -4.3151593 -4.3085828 -4.3156481 -4.3335552 -4.3417192 -4.3423357 -4.3332109 -4.3163981 -4.2952404]]...]
INFO - root - 2017-12-07 11:55:17.218457: step 5910, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 58h:45m:14s remains)
INFO - root - 2017-12-07 11:55:23.874424: step 5920, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 59h:57m:33s remains)
INFO - root - 2017-12-07 11:55:30.689386: step 5930, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 65h:08m:44s remains)
INFO - root - 2017-12-07 11:55:37.586917: step 5940, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 65h:17m:16s remains)
INFO - root - 2017-12-07 11:55:44.416439: step 5950, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 62h:39m:37s remains)
INFO - root - 2017-12-07 11:55:51.155831: step 5960, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:37m:35s remains)
INFO - root - 2017-12-07 11:55:57.980931: step 5970, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 59h:46m:35s remains)
INFO - root - 2017-12-07 11:56:04.813410: step 5980, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 64h:06m:17s remains)
INFO - root - 2017-12-07 11:56:11.616863: step 5990, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 61h:27m:43s remains)
INFO - root - 2017-12-07 11:56:18.420778: step 6000, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 59h:12m:51s remains)
2017-12-07 11:56:19.190618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1188416 -4.1345587 -4.1822891 -4.2286596 -4.2432156 -4.24604 -4.2563052 -4.2654562 -4.2692761 -4.264349 -4.2496319 -4.2213635 -4.1774168 -4.136415 -4.1211586][-4.10597 -4.1214762 -4.1687231 -4.2152128 -4.2318945 -4.2358546 -4.2448778 -4.2539024 -4.2607131 -4.2587557 -4.2444534 -4.2129169 -4.1615758 -4.1192908 -4.1115565][-4.1172833 -4.1311889 -4.1680012 -4.2025933 -4.2084184 -4.2095513 -4.2252192 -4.2397289 -4.250546 -4.2515879 -4.2350936 -4.2011805 -4.1517158 -4.1155515 -4.1159697][-4.1436305 -4.151669 -4.1718664 -4.19091 -4.181253 -4.1789289 -4.2017207 -4.2239509 -4.2373462 -4.2418108 -4.2283635 -4.1981621 -4.1540122 -4.1220775 -4.1262016][-4.1843491 -4.1863632 -4.1906772 -4.1911173 -4.1630831 -4.1475816 -4.1680107 -4.1926823 -4.2064734 -4.21699 -4.2171936 -4.2045608 -4.1739755 -4.1425858 -4.1421814][-4.22016 -4.215951 -4.2076621 -4.1920652 -4.147356 -4.1199412 -4.1331577 -4.1502438 -4.1542764 -4.1620235 -4.1818829 -4.1992092 -4.1950049 -4.1738877 -4.1730757][-4.22157 -4.2174792 -4.2104263 -4.1962905 -4.1505165 -4.1173506 -4.1198106 -4.1172662 -4.0939083 -4.0838313 -4.1154594 -4.1618547 -4.1897216 -4.1947808 -4.2070208][-4.1897578 -4.1958323 -4.2061596 -4.20848 -4.1745453 -4.1399364 -4.1261482 -4.092762 -4.0331063 -3.9966853 -4.0307946 -4.1018333 -4.1629858 -4.1988287 -4.2274432][-4.1464405 -4.1728582 -4.2093148 -4.2357965 -4.2186303 -4.1838202 -4.1541333 -4.0878019 -3.9897103 -3.924228 -3.9564829 -4.0445743 -4.1295757 -4.1895452 -4.2323451][-4.1114511 -4.1638589 -4.2275658 -4.27654 -4.2775583 -4.2430296 -4.193831 -4.1027174 -3.9836812 -3.9081509 -3.9368172 -4.0290251 -4.1171045 -4.1814013 -4.2270141][-4.0894079 -4.1680541 -4.2477841 -4.3069196 -4.3179922 -4.2825165 -4.2235775 -4.1302958 -4.0186253 -3.9476523 -3.9720473 -4.05299 -4.1278687 -4.1828003 -4.220715][-4.0706415 -4.1622167 -4.239285 -4.300385 -4.3186831 -4.2872624 -4.2324853 -4.1556711 -4.0724483 -4.019341 -4.0353842 -4.0937014 -4.1462874 -4.1858444 -4.213243][-4.0501366 -4.1292753 -4.192872 -4.2572379 -4.2859573 -4.2661228 -4.2240782 -4.16858 -4.115097 -4.0844817 -4.0954113 -4.1268687 -4.152916 -4.1767411 -4.1955833][-4.0399885 -4.0943646 -4.1441016 -4.2094069 -4.2448688 -4.2378058 -4.2117858 -4.1735935 -4.13921 -4.1239519 -4.1324229 -4.1444225 -4.1513767 -4.1646109 -4.1790347][-4.0553508 -4.0891657 -4.1291924 -4.1863079 -4.2199 -4.2189093 -4.2068343 -4.183907 -4.1623154 -4.1573029 -4.1652575 -4.1682057 -4.1692853 -4.1781888 -4.1917186]]...]
INFO - root - 2017-12-07 11:56:25.993961: step 6010, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 63h:51m:43s remains)
INFO - root - 2017-12-07 11:56:32.617489: step 6020, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.681 sec/batch; 61h:44m:15s remains)
INFO - root - 2017-12-07 11:56:39.336810: step 6030, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 58h:01m:07s remains)
INFO - root - 2017-12-07 11:56:46.105565: step 6040, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 60h:59m:02s remains)
INFO - root - 2017-12-07 11:56:52.851246: step 6050, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 64h:07m:18s remains)
INFO - root - 2017-12-07 11:56:59.653294: step 6060, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 63h:13m:55s remains)
INFO - root - 2017-12-07 11:57:06.459815: step 6070, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 63h:37m:14s remains)
INFO - root - 2017-12-07 11:57:13.185302: step 6080, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 58h:49m:32s remains)
INFO - root - 2017-12-07 11:57:20.096375: step 6090, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 59h:16m:47s remains)
INFO - root - 2017-12-07 11:57:26.916286: step 6100, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 62h:50m:31s remains)
2017-12-07 11:57:27.609249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3254023 -4.3315735 -4.3326807 -4.3259 -4.31411 -4.3035574 -4.2980056 -4.29598 -4.2983704 -4.3108993 -4.3249526 -4.3313828 -4.3270125 -4.3192115 -4.3162303][-4.3094177 -4.31304 -4.3124304 -4.3027277 -4.2860985 -4.2701731 -4.2619162 -4.25958 -4.2621627 -4.2807384 -4.3023434 -4.3124032 -4.3067241 -4.297121 -4.2967129][-4.2993975 -4.3002992 -4.2980285 -4.2860613 -4.2643828 -4.2422557 -4.2320557 -4.2331777 -4.235589 -4.2552376 -4.2819824 -4.2970939 -4.2891455 -4.2762132 -4.2764978][-4.2937012 -4.2900796 -4.2860026 -4.27455 -4.250566 -4.2231641 -4.2075047 -4.2116275 -4.216042 -4.2339854 -4.2585087 -4.2785673 -4.2715797 -4.2566485 -4.2583137][-4.2858496 -4.2783184 -4.2724657 -4.2628813 -4.2394686 -4.2078605 -4.18746 -4.1952524 -4.2068973 -4.223731 -4.2383881 -4.2549248 -4.2472153 -4.2300496 -4.2319098][-4.2798414 -4.2699213 -4.262536 -4.2551122 -4.2330279 -4.1987543 -4.1778712 -4.1931491 -4.2145967 -4.23228 -4.2397094 -4.2488542 -4.2330861 -4.2069125 -4.204145][-4.27547 -4.2635779 -4.2536893 -4.2453904 -4.224577 -4.1900663 -4.1704483 -4.1878161 -4.2099867 -4.231864 -4.2444153 -4.24987 -4.2282944 -4.193953 -4.1854458][-4.2743998 -4.2603106 -4.248096 -4.2357664 -4.2126679 -4.1766539 -4.1552596 -4.1649551 -4.1830006 -4.2125621 -4.2368917 -4.2447815 -4.2252803 -4.1965909 -4.1909814][-4.2820177 -4.2695136 -4.2552943 -4.2359924 -4.2066207 -4.1688552 -4.1439443 -4.1444545 -4.1603589 -4.1975288 -4.2311392 -4.2417426 -4.2299228 -4.2146125 -4.2147837][-4.2925239 -4.2834988 -4.2691331 -4.2451057 -4.2092605 -4.1686468 -4.1433349 -4.14197 -4.1615543 -4.1981959 -4.2317214 -4.2439876 -4.2401114 -4.2351723 -4.2366052][-4.2964048 -4.2905765 -4.2763262 -4.2497196 -4.2114396 -4.1699333 -4.1438508 -4.1458464 -4.1706657 -4.2067595 -4.235199 -4.2472963 -4.2489295 -4.2486882 -4.2493448][-4.2920961 -4.2862244 -4.271194 -4.2462196 -4.2146311 -4.1784048 -4.1579847 -4.1635723 -4.1893888 -4.2199745 -4.24068 -4.25103 -4.2547746 -4.2570667 -4.2596703][-4.2878819 -4.2812095 -4.2673712 -4.250783 -4.2334709 -4.2092152 -4.1944852 -4.1988497 -4.2189183 -4.239768 -4.2525687 -4.2579346 -4.2594647 -4.2616105 -4.2641821][-4.2857113 -4.2818532 -4.2744803 -4.2686605 -4.2647681 -4.2543349 -4.2464848 -4.2488241 -4.2608981 -4.2716551 -4.27603 -4.2755675 -4.2738919 -4.27427 -4.2768369][-4.2877221 -4.286613 -4.28471 -4.2848063 -4.2880573 -4.2868333 -4.2851605 -4.2870479 -4.2918582 -4.2946849 -4.2945867 -4.2923217 -4.2907124 -4.2910895 -4.2927632]]...]
INFO - root - 2017-12-07 11:57:34.414974: step 6110, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 60h:50m:48s remains)
INFO - root - 2017-12-07 11:57:41.050713: step 6120, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.666 sec/batch; 60h:22m:26s remains)
INFO - root - 2017-12-07 11:57:47.728337: step 6130, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 59h:20m:21s remains)
INFO - root - 2017-12-07 11:57:54.543891: step 6140, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 61h:49m:45s remains)
INFO - root - 2017-12-07 11:58:01.304551: step 6150, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 59h:00m:09s remains)
INFO - root - 2017-12-07 11:58:08.213743: step 6160, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 64h:13m:05s remains)
INFO - root - 2017-12-07 11:58:14.975230: step 6170, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 64h:01m:54s remains)
INFO - root - 2017-12-07 11:58:21.823196: step 6180, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 61h:06m:42s remains)
INFO - root - 2017-12-07 11:58:28.646170: step 6190, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 57h:28m:28s remains)
INFO - root - 2017-12-07 11:58:35.462956: step 6200, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 57h:07m:53s remains)
2017-12-07 11:58:36.184351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2134805 -4.2031813 -4.2164702 -4.2264514 -4.2221928 -4.1963038 -4.1572194 -4.1129689 -4.0811329 -4.0876193 -4.1331034 -4.1749582 -4.2062507 -4.2385521 -4.2798066][-4.2226863 -4.2221031 -4.2395182 -4.2469096 -4.2355556 -4.1998072 -4.15225 -4.1082091 -4.0829892 -4.0888638 -4.1314445 -4.1792502 -4.2215071 -4.266151 -4.3092723][-4.2394443 -4.2482109 -4.268373 -4.2732692 -4.2522888 -4.2044973 -4.1460056 -4.09668 -4.0746183 -4.082952 -4.1285739 -4.1855683 -4.2369423 -4.2859778 -4.323452][-4.2580366 -4.2740703 -4.293891 -4.2921643 -4.2563939 -4.1962423 -4.1223989 -4.0630078 -4.0461106 -4.0691757 -4.1250954 -4.1950603 -4.2569871 -4.3039126 -4.3293328][-4.2806969 -4.2972708 -4.3091831 -4.2942634 -4.2464137 -4.1747165 -4.0830278 -4.0101528 -4.0067534 -4.0559516 -4.1309114 -4.2124481 -4.2812047 -4.3198295 -4.3256][-4.3021278 -4.3137684 -4.3128204 -4.2833004 -4.2212882 -4.133038 -4.0267954 -3.9528766 -3.981777 -4.0669065 -4.1581707 -4.241128 -4.3024006 -4.3257194 -4.3095584][-4.2998366 -4.305182 -4.2940593 -4.2506504 -4.1738 -4.069622 -3.9637148 -3.9129221 -3.9819634 -4.0910306 -4.1864023 -4.2610931 -4.3072729 -4.3146758 -4.2805829][-4.2743554 -4.2774367 -4.260726 -4.2073913 -4.11941 -4.0158668 -3.9406853 -3.935055 -4.02308 -4.1271796 -4.2105627 -4.2734351 -4.30365 -4.2943544 -4.2486663][-4.2454505 -4.25375 -4.2361336 -4.1772084 -4.0860238 -3.9983814 -3.966197 -3.9968574 -4.0796809 -4.1666665 -4.236856 -4.2850628 -4.296906 -4.2706318 -4.2185588][-4.2187815 -4.2343612 -4.2170711 -4.15211 -4.0643458 -4.0022492 -4.0052681 -4.0523777 -4.1257963 -4.2001071 -4.2610841 -4.2959251 -4.2915373 -4.2531843 -4.1943545][-4.1951485 -4.2108641 -4.1854796 -4.1186752 -4.0457306 -4.0150104 -4.0450277 -4.1043715 -4.1703796 -4.2290425 -4.2783341 -4.3017159 -4.2884336 -4.2435279 -4.1792521][-4.1831169 -4.1915326 -4.1572914 -4.0986748 -4.0505838 -4.0415449 -4.0825167 -4.1458836 -4.2016068 -4.2461886 -4.2875533 -4.3035221 -4.2826705 -4.2314444 -4.1634545][-4.189394 -4.1862869 -4.1465483 -4.1030831 -4.07886 -4.0823236 -4.1205196 -4.175818 -4.2222152 -4.2602463 -4.2949424 -4.3033404 -4.2742724 -4.2158508 -4.1454945][-4.1985164 -4.1880631 -4.1518183 -4.1252384 -4.1219969 -4.1311216 -4.1614842 -4.2024279 -4.239409 -4.2705345 -4.2936578 -4.2937179 -4.2605667 -4.2000613 -4.12885][-4.2081065 -4.2049785 -4.1795549 -4.1650081 -4.1686306 -4.1747894 -4.1931643 -4.2205253 -4.24671 -4.2691588 -4.2825933 -4.2758479 -4.2415452 -4.1829896 -4.1179485]]...]
INFO - root - 2017-12-07 11:58:42.837889: step 6210, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 60h:18m:15s remains)
INFO - root - 2017-12-07 11:58:49.387625: step 6220, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 59h:13m:45s remains)
INFO - root - 2017-12-07 11:58:56.214997: step 6230, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 58h:15m:37s remains)
INFO - root - 2017-12-07 11:59:02.989906: step 6240, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 65h:50m:34s remains)
INFO - root - 2017-12-07 11:59:09.872863: step 6250, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 65h:31m:22s remains)
INFO - root - 2017-12-07 11:59:16.706723: step 6260, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.664 sec/batch; 60h:11m:15s remains)
INFO - root - 2017-12-07 11:59:23.436581: step 6270, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:49m:42s remains)
INFO - root - 2017-12-07 11:59:30.296183: step 6280, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 62h:33m:32s remains)
INFO - root - 2017-12-07 11:59:37.113249: step 6290, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 62h:20m:56s remains)
INFO - root - 2017-12-07 11:59:43.931358: step 6300, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:56m:02s remains)
2017-12-07 11:59:44.697197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3148365 -4.2766929 -4.2212167 -4.1536689 -4.0841227 -4.0478325 -4.0688157 -4.1124997 -4.1362271 -4.1431847 -4.1492929 -4.1564775 -4.1658726 -4.1815305 -4.2005548][-4.3187037 -4.278573 -4.2175031 -4.144598 -4.064959 -4.0109534 -4.02322 -4.0842476 -4.1279387 -4.1410294 -4.144465 -4.1494417 -4.1618233 -4.1836891 -4.2096519][-4.3251748 -4.2877679 -4.2255955 -4.1500096 -4.0602736 -3.984175 -3.9754105 -4.044817 -4.1087618 -4.1314325 -4.1334953 -4.136085 -4.1502924 -4.179018 -4.2124848][-4.3292475 -4.2981834 -4.2398534 -4.1655684 -4.0725327 -3.9796467 -3.9446378 -4.0098295 -4.0874095 -4.1187491 -4.1180859 -4.1153932 -4.1284513 -4.1625381 -4.2022562][-4.3296466 -4.3061738 -4.253871 -4.1857662 -4.0975637 -3.9969854 -3.9391572 -3.9900687 -4.0740204 -4.1133256 -4.1147509 -4.1081643 -4.1161432 -4.147944 -4.1847725][-4.3270669 -4.3113527 -4.2683654 -4.2101378 -4.1308308 -4.0277467 -3.951859 -3.9791155 -4.0586696 -4.1053734 -4.1155396 -4.1105266 -4.1117964 -4.1371946 -4.1634212][-4.3239331 -4.3130641 -4.2793136 -4.2304335 -4.1601405 -4.0600758 -3.9747412 -3.9776874 -4.0439239 -4.0989819 -4.1231575 -4.1250281 -4.1192369 -4.1296568 -4.1398163][-4.3218837 -4.3136911 -4.288084 -4.249784 -4.1927004 -4.1049294 -4.0213556 -4.0042448 -4.0491729 -4.1066685 -4.1444244 -4.1565108 -4.1486287 -4.1439848 -4.1355329][-4.32159 -4.3142462 -4.2946992 -4.2657 -4.2234621 -4.1549993 -4.0820327 -4.0556316 -4.0766959 -4.1236653 -4.1691914 -4.1941481 -4.1911945 -4.1786637 -4.1570539][-4.3233838 -4.3168855 -4.3025002 -4.2817726 -4.2522821 -4.2059336 -4.1480136 -4.1206565 -4.1225519 -4.1535096 -4.1990476 -4.2357969 -4.2431035 -4.2289357 -4.20139][-4.3261905 -4.3198504 -4.3092394 -4.2942495 -4.2748137 -4.2490273 -4.206109 -4.1765623 -4.1625681 -4.1819148 -4.2264061 -4.2736983 -4.2925253 -4.2811389 -4.2531815][-4.3304033 -4.3235841 -4.315032 -4.3041859 -4.2933812 -4.2848396 -4.25626 -4.224998 -4.2020564 -4.2149172 -4.2563128 -4.3090267 -4.3348246 -4.327652 -4.3029857][-4.3367057 -4.3299994 -4.3239069 -4.3162885 -4.3108168 -4.311 -4.2938046 -4.2630424 -4.2387743 -4.2470722 -4.282073 -4.3322144 -4.3611178 -4.35853 -4.339221][-4.34001 -4.33298 -4.3269758 -4.3230753 -4.3223271 -4.3274026 -4.3203197 -4.294507 -4.2711453 -4.2727857 -4.2970014 -4.3383965 -4.3650279 -4.3664179 -4.3544211][-4.3388214 -4.3319736 -4.3252335 -4.3233218 -4.3270488 -4.3370528 -4.337214 -4.3183823 -4.296855 -4.2897539 -4.3008184 -4.3298717 -4.3511662 -4.3564987 -4.3522229]]...]
INFO - root - 2017-12-07 11:59:51.514063: step 6310, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 65h:16m:08s remains)
INFO - root - 2017-12-07 11:59:58.189802: step 6320, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 62h:11m:33s remains)
INFO - root - 2017-12-07 12:00:04.917925: step 6330, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 60h:28m:25s remains)
INFO - root - 2017-12-07 12:00:11.579717: step 6340, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.630 sec/batch; 57h:04m:32s remains)
INFO - root - 2017-12-07 12:00:18.372173: step 6350, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 61h:43m:08s remains)
INFO - root - 2017-12-07 12:00:25.167617: step 6360, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 61h:31m:27s remains)
INFO - root - 2017-12-07 12:00:31.943406: step 6370, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 63h:03m:31s remains)
INFO - root - 2017-12-07 12:00:38.716094: step 6380, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 56h:47m:43s remains)
INFO - root - 2017-12-07 12:00:45.480345: step 6390, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 58h:53m:15s remains)
INFO - root - 2017-12-07 12:00:52.262282: step 6400, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 64h:21m:33s remains)
2017-12-07 12:00:52.976279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.222219 -4.2236781 -4.2237844 -4.2170649 -4.2089567 -4.2028222 -4.1947751 -4.1831026 -4.1771927 -4.1790953 -4.1967478 -4.2135925 -4.2137151 -4.1976137 -4.1754861][-4.2287889 -4.2201395 -4.2112355 -4.1988854 -4.1902122 -4.1882854 -4.1873555 -4.1800327 -4.1760483 -4.1792727 -4.1972675 -4.2176495 -4.2201591 -4.2057672 -4.1828651][-4.2233171 -4.2119923 -4.1999993 -4.1828809 -4.1701293 -4.1642447 -4.161694 -4.1575928 -4.1595011 -4.1684923 -4.1866217 -4.2084389 -4.2130437 -4.202158 -4.1817851][-4.2172723 -4.2104583 -4.1995316 -4.1787453 -4.1568861 -4.1366911 -4.126235 -4.128232 -4.1439285 -4.1624446 -4.1790185 -4.1935096 -4.1983342 -4.1920905 -4.1760988][-4.2127748 -4.2096052 -4.2014303 -4.1794586 -4.1466932 -4.1097364 -4.092803 -4.1032791 -4.1334362 -4.1615553 -4.17313 -4.1755977 -4.1733627 -4.1676927 -4.1553888][-4.2032828 -4.2037125 -4.1965384 -4.1732488 -4.1328387 -4.0878377 -4.0690784 -4.0886922 -4.13053 -4.1634893 -4.1671562 -4.1560864 -4.1430516 -4.1339808 -4.1279163][-4.178288 -4.1780272 -4.1709213 -4.1503057 -4.1113205 -4.0666394 -4.0471492 -4.0713272 -4.1211414 -4.1569705 -4.1594129 -4.1437163 -4.1279912 -4.1230903 -4.1314206][-4.147645 -4.1403413 -4.1343589 -4.1211705 -4.0921936 -4.0519357 -4.0301342 -4.0518446 -4.1007404 -4.1362462 -4.1437778 -4.13941 -4.1401353 -4.1503906 -4.1683879][-4.1224194 -4.108705 -4.1044111 -4.1000772 -4.0828691 -4.0476246 -4.0230846 -4.0364747 -4.0784779 -4.1110663 -4.1239328 -4.13465 -4.1592054 -4.1879687 -4.2105961][-4.126647 -4.1056252 -4.0990796 -4.1043229 -4.0999565 -4.0721951 -4.0500317 -4.0553293 -4.0853567 -4.1065855 -4.1214552 -4.1423306 -4.1809721 -4.2168527 -4.2363067][-4.144269 -4.1192579 -4.1104403 -4.1200733 -4.1284056 -4.1170363 -4.10398 -4.1048303 -4.1200581 -4.1294522 -4.1422167 -4.1659088 -4.1993909 -4.2277532 -4.2380676][-4.1684318 -4.1447711 -4.1333108 -4.1401353 -4.1534152 -4.154542 -4.1490297 -4.1484389 -4.1560431 -4.1584811 -4.1678348 -4.1827984 -4.1990795 -4.2131281 -4.2141991][-4.1964736 -4.177465 -4.1620655 -4.1617413 -4.1722932 -4.1787925 -4.1798434 -4.1819792 -4.1897683 -4.1904016 -4.1919022 -4.1917462 -4.1899877 -4.1917529 -4.1886239][-4.2192078 -4.20299 -4.1878343 -4.1824732 -4.18776 -4.1962266 -4.2024193 -4.208426 -4.2170739 -4.2163563 -4.2085047 -4.1932945 -4.1781535 -4.1713343 -4.1691794][-4.2413807 -4.2250614 -4.2098088 -4.2014279 -4.2010393 -4.207221 -4.2156725 -4.2246242 -4.234796 -4.2338262 -4.2202353 -4.1973133 -4.1792874 -4.1734567 -4.175005]]...]
INFO - root - 2017-12-07 12:00:59.740204: step 6410, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 57h:06m:28s remains)
INFO - root - 2017-12-07 12:01:06.420684: step 6420, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 61h:11m:51s remains)
INFO - root - 2017-12-07 12:01:13.219976: step 6430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 63h:03m:36s remains)
INFO - root - 2017-12-07 12:01:19.912736: step 6440, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 61h:16m:15s remains)
INFO - root - 2017-12-07 12:01:26.766804: step 6450, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 58h:15m:42s remains)
INFO - root - 2017-12-07 12:01:33.659368: step 6460, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 63h:12m:14s remains)
INFO - root - 2017-12-07 12:01:40.503010: step 6470, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 62h:30m:47s remains)
INFO - root - 2017-12-07 12:01:47.212549: step 6480, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.674 sec/batch; 61h:04m:10s remains)
INFO - root - 2017-12-07 12:01:53.959723: step 6490, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 60h:00m:42s remains)
INFO - root - 2017-12-07 12:02:00.772960: step 6500, loss = 2.09, batch loss = 2.04 (12.6 examples/sec; 0.634 sec/batch; 57h:22m:15s remains)
2017-12-07 12:02:01.476816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3469734 -4.3571773 -4.3590832 -4.3440909 -4.3076544 -4.2616806 -4.2166095 -4.1809897 -4.1557331 -4.139225 -4.1473212 -4.1846576 -4.2321248 -4.2674861 -4.289547][-4.3554726 -4.3645978 -4.3628912 -4.3384824 -4.2917047 -4.2377038 -4.1884789 -4.1535211 -4.1402683 -4.1418538 -4.1538577 -4.1836133 -4.2295241 -4.2667937 -4.2877889][-4.3580956 -4.3641281 -4.3565674 -4.3235917 -4.2654271 -4.2042809 -4.1512609 -4.118669 -4.115273 -4.1266737 -4.1452065 -4.1717381 -4.2211976 -4.2638774 -4.2862144][-4.3590131 -4.3636627 -4.3477859 -4.3040133 -4.2339363 -4.1639829 -4.1097879 -4.0793257 -4.0780993 -4.0962024 -4.1194005 -4.1520061 -4.2125778 -4.2617478 -4.2894516][-4.3620481 -4.3661251 -4.343399 -4.2843971 -4.1993747 -4.1188126 -4.0595589 -4.0271225 -4.034122 -4.0644317 -4.0995731 -4.1468172 -4.2158017 -4.2671347 -4.29687][-4.3677435 -4.3692446 -4.3392439 -4.2621222 -4.1574144 -4.0606189 -3.9890614 -3.9624298 -3.9950423 -4.0446672 -4.0985632 -4.1607342 -4.2318969 -4.2823968 -4.3097558][-4.3688803 -4.3671021 -4.3287106 -4.2338176 -4.1036758 -3.9806814 -3.8956547 -3.8959031 -3.9642439 -4.0363593 -4.1092911 -4.1799779 -4.24966 -4.2973633 -4.3171344][-4.3638492 -4.3555341 -4.3079219 -4.1980753 -4.041903 -3.890656 -3.8053432 -3.8413825 -3.938261 -4.0309191 -4.1228952 -4.1998143 -4.2661572 -4.3062482 -4.3140125][-4.3547659 -4.3378654 -4.2809863 -4.1662474 -4.0070295 -3.850013 -3.77138 -3.8259804 -3.9286067 -4.0305681 -4.1318574 -4.2158546 -4.2781143 -4.3059692 -4.2983117][-4.3430595 -4.3150668 -4.2511277 -4.1478643 -4.0157375 -3.8891859 -3.8289318 -3.877321 -3.961381 -4.0550985 -4.1502476 -4.232224 -4.2849755 -4.3047895 -4.2889562][-4.3316116 -4.2933955 -4.2254758 -4.14185 -4.0520725 -3.972436 -3.9371142 -3.9694915 -4.0307631 -4.1054473 -4.1826048 -4.2493505 -4.2888522 -4.2993526 -4.28142][-4.3253946 -4.2831712 -4.2190652 -4.1572304 -4.0998945 -4.0539765 -4.0412769 -4.0698075 -4.1161051 -4.1701479 -4.2241154 -4.2689004 -4.2926803 -4.2963777 -4.2818751][-4.3244991 -4.2868252 -4.2333217 -4.1872239 -4.1467385 -4.11524 -4.1153817 -4.144165 -4.182641 -4.2239838 -4.2637429 -4.2919488 -4.303865 -4.3034534 -4.29241][-4.3329782 -4.308404 -4.2680726 -4.2312737 -4.1995754 -4.1756105 -4.1786256 -4.2025437 -4.2329078 -4.2633581 -4.2924328 -4.3081474 -4.313344 -4.3130755 -4.3055844][-4.3418832 -4.3283777 -4.3006983 -4.2737675 -4.2520924 -4.2367463 -4.2393451 -4.2548623 -4.2768579 -4.2956743 -4.312345 -4.3194594 -4.3208156 -4.320961 -4.3164878]]...]
INFO - root - 2017-12-07 12:02:08.224474: step 6510, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.704 sec/batch; 63h:44m:34s remains)
INFO - root - 2017-12-07 12:02:14.866890: step 6520, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 65h:21m:51s remains)
INFO - root - 2017-12-07 12:02:21.637831: step 6530, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 58h:07m:33s remains)
INFO - root - 2017-12-07 12:02:28.521439: step 6540, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 60h:01m:19s remains)
INFO - root - 2017-12-07 12:02:35.441856: step 6550, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 63h:30m:12s remains)
INFO - root - 2017-12-07 12:02:42.299285: step 6560, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 65h:36m:50s remains)
INFO - root - 2017-12-07 12:02:49.157780: step 6570, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 60h:49m:12s remains)
INFO - root - 2017-12-07 12:02:55.860327: step 6580, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:09m:23s remains)
INFO - root - 2017-12-07 12:03:02.804774: step 6590, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 64h:40m:08s remains)
INFO - root - 2017-12-07 12:03:09.684917: step 6600, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 65h:00m:39s remains)
2017-12-07 12:03:10.340715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3148632 -4.2830896 -4.2410655 -4.2046432 -4.1761174 -4.1461129 -4.1142693 -4.0856833 -4.0676246 -4.0731664 -4.0930467 -4.1097503 -4.1382666 -4.1537585 -4.1434331][-4.3252425 -4.2937026 -4.25141 -4.2146525 -4.18233 -4.1534204 -4.1257515 -4.0973325 -4.0799437 -4.082233 -4.1024833 -4.1286416 -4.1601405 -4.1757631 -4.1614175][-4.3267694 -4.296279 -4.2553358 -4.2179837 -4.1863613 -4.1570764 -4.1327744 -4.113658 -4.1044703 -4.1018944 -4.1159067 -4.141366 -4.1686983 -4.1800046 -4.1663208][-4.3213992 -4.2886477 -4.246057 -4.2063012 -4.1769867 -4.1468043 -4.1230507 -4.1146955 -4.1140041 -4.1124868 -4.1197171 -4.1351509 -4.1561933 -4.1685958 -4.1621809][-4.3123097 -4.2720275 -4.2249541 -4.1872396 -4.1618867 -4.1276588 -4.0942593 -4.0861335 -4.09705 -4.105402 -4.1106706 -4.1175261 -4.1388974 -4.156991 -4.1603384][-4.2936373 -4.242188 -4.187377 -4.1482711 -4.1201291 -4.0747623 -4.0290771 -4.0270042 -4.0560417 -4.0819635 -4.0959873 -4.1028805 -4.1242437 -4.1462975 -4.1562033][-4.2746167 -4.2138271 -4.1501632 -4.1009846 -4.0637221 -4.0083828 -3.9556806 -3.9612024 -4.0081282 -4.0492778 -4.0731874 -4.0833645 -4.1083865 -4.1337109 -4.1459327][-4.2634234 -4.2031546 -4.1418943 -4.0922036 -4.0510421 -3.9918363 -3.9342546 -3.9355693 -3.9828653 -4.0308304 -4.0604429 -4.068202 -4.08386 -4.102262 -4.1145625][-4.2630253 -4.2138271 -4.1669927 -4.12939 -4.0957365 -4.0412321 -3.9798234 -3.9699929 -4.0052967 -4.0512686 -4.0798354 -4.0810204 -4.0767565 -4.0750275 -4.077117][-4.2694311 -4.22951 -4.1904964 -4.1638312 -4.1403623 -4.09881 -4.04795 -4.0351439 -4.061264 -4.099196 -4.1201391 -4.10939 -4.0869761 -4.0709677 -4.069119][-4.2706146 -4.2354846 -4.2019563 -4.1836553 -4.170536 -4.14329 -4.1091495 -4.0979347 -4.11221 -4.1361532 -4.1457844 -4.1280823 -4.0972881 -4.0784616 -4.0854263][-4.2733092 -4.2427092 -4.2142797 -4.2023015 -4.1967015 -4.1811976 -4.162313 -4.1533608 -4.155417 -4.1606054 -4.1560922 -4.1322069 -4.096148 -4.0793304 -4.0992661][-4.2855468 -4.2549386 -4.2264209 -4.2158747 -4.2142439 -4.2048807 -4.19236 -4.1878252 -4.1870985 -4.1838636 -4.1754427 -4.1558003 -4.1249948 -4.1094813 -4.1299272][-4.3048143 -4.2789917 -4.2550035 -4.2474265 -4.2497106 -4.2470984 -4.23835 -4.2335787 -4.2298365 -4.22328 -4.2135773 -4.1979718 -4.1742153 -4.1581674 -4.1710458][-4.3232141 -4.304462 -4.2886953 -4.2870421 -4.2934985 -4.294157 -4.28527 -4.2784719 -4.2722239 -4.2641883 -4.2561235 -4.2478757 -4.2334023 -4.2207084 -4.2278023]]...]
INFO - root - 2017-12-07 12:03:17.050492: step 6610, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 58h:22m:24s remains)
INFO - root - 2017-12-07 12:03:23.773636: step 6620, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 63h:39m:08s remains)
INFO - root - 2017-12-07 12:03:30.583471: step 6630, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 64h:58m:54s remains)
INFO - root - 2017-12-07 12:03:37.366774: step 6640, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:40m:24s remains)
INFO - root - 2017-12-07 12:03:44.096112: step 6650, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:39m:44s remains)
INFO - root - 2017-12-07 12:03:50.948923: step 6660, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 61h:26m:31s remains)
INFO - root - 2017-12-07 12:03:57.793598: step 6670, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 61h:42m:16s remains)
INFO - root - 2017-12-07 12:04:04.586945: step 6680, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 62h:33m:15s remains)
INFO - root - 2017-12-07 12:04:11.331125: step 6690, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 59h:29m:57s remains)
INFO - root - 2017-12-07 12:04:18.252218: step 6700, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 63h:22m:01s remains)
2017-12-07 12:04:18.954133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1590161 -4.1756964 -4.1836958 -4.161551 -4.1180835 -4.0684853 -4.0305152 -4.0220275 -4.0638065 -4.1249623 -4.1825414 -4.2165461 -4.2328129 -4.2295303 -4.2007523][-4.1439104 -4.1635094 -4.183598 -4.1778717 -4.140655 -4.0856743 -4.0319242 -4.0065751 -4.0375905 -4.0946884 -4.1548681 -4.1974506 -4.2211723 -4.2285714 -4.2128797][-4.1477313 -4.1624656 -4.1852927 -4.1913171 -4.1617351 -4.1074986 -4.0449204 -4.0073743 -4.0207958 -4.061893 -4.1122341 -4.1548872 -4.1810451 -4.1988316 -4.2003589][-4.1487966 -4.1690645 -4.1962934 -4.206811 -4.1808763 -4.130487 -4.0709205 -4.0281978 -4.0254345 -4.0465045 -4.0777316 -4.1103959 -4.1341972 -4.1618857 -4.1791663][-4.1334243 -4.1641335 -4.2020092 -4.2184296 -4.1936979 -4.1454911 -4.0921297 -4.0546093 -4.0478907 -4.0594492 -4.0752645 -4.0994048 -4.1198344 -4.14572 -4.1678367][-4.0992336 -4.1340728 -4.1812897 -4.2052126 -4.1786475 -4.1277466 -4.0709414 -4.0370164 -4.0424466 -4.0682755 -4.0912423 -4.1160617 -4.1330628 -4.1503024 -4.1692953][-4.0575142 -4.0866141 -4.1434827 -4.1739974 -4.1500058 -4.0931778 -4.0181637 -3.971925 -3.9843466 -4.0373678 -4.0896282 -4.1319971 -4.152163 -4.1650085 -4.1828132][-4.02485 -4.0468059 -4.1015439 -4.1325784 -4.1126647 -4.0545793 -3.9750311 -3.9216778 -3.9360397 -4.0008092 -4.073782 -4.1334076 -4.160264 -4.1718144 -4.1882191][-4.0069976 -4.0219822 -4.0652323 -4.0917115 -4.0763054 -4.0235 -3.9602039 -3.921061 -3.9364812 -3.9900885 -4.0577459 -4.1186242 -4.1461067 -4.1539268 -4.1679039][-4.0102038 -4.0225625 -4.0569043 -4.0777721 -4.0697374 -4.0334234 -3.9900966 -3.9658811 -3.9813676 -4.0209565 -4.0733256 -4.1221251 -4.1412487 -4.1398778 -4.1485763][-4.026515 -4.0464268 -4.0815964 -4.0992322 -4.0921121 -4.0681553 -4.041575 -4.0285349 -4.0429521 -4.0740871 -4.1167097 -4.1534872 -4.1621976 -4.1513753 -4.1529236][-4.0525045 -4.0831065 -4.1221266 -4.1385636 -4.1305647 -4.1137681 -4.0976052 -4.0920315 -4.1092687 -4.1373286 -4.1747923 -4.2033257 -4.20416 -4.1865683 -4.1784048][-4.1146131 -4.1440969 -4.1765003 -4.1891294 -4.1833425 -4.1731195 -4.1632714 -4.1625795 -4.1794033 -4.2036772 -4.2351623 -4.2556939 -4.25212 -4.232152 -4.2166171][-4.1928592 -4.2116947 -4.2310424 -4.2408309 -4.2392912 -4.2351475 -4.2293534 -4.2279587 -4.2392764 -4.2581754 -4.2821617 -4.2963691 -4.2938457 -4.278254 -4.2617249][-4.2565618 -4.2667689 -4.2787724 -4.2883344 -4.289824 -4.2862029 -4.2801967 -4.2785454 -4.2852011 -4.2982478 -4.3147249 -4.3247371 -4.3249478 -4.3151531 -4.3026052]]...]
INFO - root - 2017-12-07 12:04:25.674475: step 6710, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 60h:11m:03s remains)
INFO - root - 2017-12-07 12:04:32.433597: step 6720, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 62h:53m:31s remains)
INFO - root - 2017-12-07 12:04:39.295664: step 6730, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 57h:18m:24s remains)
INFO - root - 2017-12-07 12:04:46.142774: step 6740, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 64h:05m:55s remains)
INFO - root - 2017-12-07 12:04:52.775934: step 6750, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 61h:06m:25s remains)
INFO - root - 2017-12-07 12:04:59.550240: step 6760, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 57h:15m:53s remains)
INFO - root - 2017-12-07 12:05:06.448969: step 6770, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 61h:26m:31s remains)
INFO - root - 2017-12-07 12:05:13.185035: step 6780, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 63h:23m:18s remains)
INFO - root - 2017-12-07 12:05:20.052525: step 6790, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 63h:14m:14s remains)
INFO - root - 2017-12-07 12:05:26.868655: step 6800, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 63h:31m:50s remains)
2017-12-07 12:05:27.524928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2928581 -4.2631712 -4.2259631 -4.1947217 -4.1840343 -4.2118793 -4.2347612 -4.2409925 -4.23436 -4.2317567 -4.2239676 -4.2089944 -4.199079 -4.1914959 -4.1888323][-4.2931204 -4.2601504 -4.2158346 -4.1760783 -4.1556044 -4.1787605 -4.201345 -4.2065797 -4.2001681 -4.2038379 -4.202055 -4.1886005 -4.1744452 -4.1664648 -4.1687][-4.2925158 -4.2602315 -4.2127032 -4.1683455 -4.1388526 -4.1486177 -4.1621771 -4.1611123 -4.1519012 -4.1604548 -4.167635 -4.1572208 -4.139533 -4.1290779 -4.1285367][-4.2897592 -4.2572145 -4.2103105 -4.1656313 -4.1262407 -4.1215611 -4.1262078 -4.1212072 -4.1121306 -4.1235046 -4.1372838 -4.1273351 -4.1030755 -4.0894761 -4.0829053][-4.2887125 -4.2567306 -4.2118874 -4.1722 -4.1329536 -4.1199207 -4.1217341 -4.1201215 -4.112277 -4.1252437 -4.14421 -4.1359539 -4.1168165 -4.1053376 -4.0942712][-4.2823367 -4.2459707 -4.1998076 -4.1643581 -4.1347265 -4.1239223 -4.1247611 -4.124033 -4.1214204 -4.138659 -4.1616588 -4.16399 -4.1570797 -4.154582 -4.1465979][-4.2696433 -4.2248054 -4.17267 -4.1354413 -4.1160345 -4.109622 -4.108777 -4.1084933 -4.1110053 -4.1326947 -4.15846 -4.1719632 -4.1773057 -4.1803632 -4.1785774][-4.2548442 -4.201992 -4.1435251 -4.104681 -4.0917754 -4.0861149 -4.0752087 -4.0681648 -4.0649796 -4.0830417 -4.1097183 -4.1318393 -4.1514964 -4.1638136 -4.16994][-4.2370834 -4.1717839 -4.1040163 -4.05777 -4.0443263 -4.0394578 -4.0180078 -3.9984174 -3.9848471 -3.9978559 -4.0268497 -4.0604792 -4.0957756 -4.12189 -4.1381803][-4.223279 -4.1484385 -4.069952 -4.0151153 -3.9990454 -3.9978495 -3.9793839 -3.9556394 -3.938107 -3.948905 -3.9774432 -4.0170674 -4.0621462 -4.0994296 -4.1282787][-4.2295041 -4.160244 -4.08524 -4.0313411 -4.015717 -4.0221944 -4.015851 -4.0004644 -3.9872494 -3.9939613 -4.0159512 -4.0539141 -4.0975628 -4.1386566 -4.1732545][-4.2597189 -4.2099686 -4.155457 -4.1183157 -4.1102924 -4.1228261 -4.1256356 -4.1200366 -4.1117821 -4.1137624 -4.1267 -4.1513681 -4.1805506 -4.2107153 -4.23706][-4.3003225 -4.27122 -4.2381563 -4.2183681 -4.2180972 -4.2295079 -4.2334437 -4.2320237 -4.2249956 -4.2221842 -4.2285995 -4.243156 -4.2594566 -4.2769141 -4.291872][-4.3300366 -4.31619 -4.2964911 -4.2838469 -4.28438 -4.2923403 -4.2969666 -4.29727 -4.2922926 -4.2879968 -4.2900667 -4.2973137 -4.306087 -4.3151517 -4.3232427][-4.3402705 -4.3332343 -4.3219595 -4.3145185 -4.3163552 -4.3221869 -4.3259239 -4.3265491 -4.32314 -4.3194156 -4.3194437 -4.3220282 -4.3262138 -4.33153 -4.336875]]...]
INFO - root - 2017-12-07 12:05:34.268534: step 6810, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 65h:29m:26s remains)
INFO - root - 2017-12-07 12:05:40.692679: step 6820, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 63h:43m:34s remains)
INFO - root - 2017-12-07 12:05:47.458254: step 6830, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 62h:00m:46s remains)
INFO - root - 2017-12-07 12:05:54.204046: step 6840, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:39m:58s remains)
INFO - root - 2017-12-07 12:06:00.992336: step 6850, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 57h:31m:44s remains)
INFO - root - 2017-12-07 12:06:07.854340: step 6860, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 63h:07m:55s remains)
INFO - root - 2017-12-07 12:06:14.624877: step 6870, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 61h:38m:23s remains)
INFO - root - 2017-12-07 12:06:21.405574: step 6880, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 62h:30m:05s remains)
INFO - root - 2017-12-07 12:06:28.138683: step 6890, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 63h:27m:38s remains)
INFO - root - 2017-12-07 12:06:34.855877: step 6900, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:52m:28s remains)
2017-12-07 12:06:35.629416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.188231 -4.1653037 -4.1695952 -4.1835337 -4.1957011 -4.1923103 -4.1809006 -4.16809 -4.1627645 -4.1728082 -4.1923065 -4.2136288 -4.223134 -4.2313204 -4.2357554][-4.2079496 -4.194303 -4.1988363 -4.2125506 -4.2261491 -4.2230735 -4.210012 -4.19484 -4.1812177 -4.1865597 -4.2076197 -4.2358942 -4.2505813 -4.2556329 -4.2535725][-4.2117214 -4.2094889 -4.21607 -4.231607 -4.2459521 -4.2429657 -4.2289438 -4.2137823 -4.1989121 -4.2028484 -4.2263718 -4.2557058 -4.2705927 -4.2732534 -4.2656989][-4.1984863 -4.2064381 -4.2202492 -4.2396936 -4.2524686 -4.2399063 -4.2148805 -4.1958547 -4.1842675 -4.1929021 -4.223639 -4.2572188 -4.2757063 -4.2789369 -4.2639723][-4.1802464 -4.2020516 -4.2252245 -4.2422247 -4.2378092 -4.1991057 -4.1482077 -4.1212726 -4.1262097 -4.1547475 -4.2015324 -4.2456064 -4.2687845 -4.2677903 -4.2441297][-4.1721091 -4.1992426 -4.2208433 -4.2241197 -4.193666 -4.1170979 -4.0218267 -3.9889674 -4.0296912 -4.097826 -4.1710315 -4.2281375 -4.2539177 -4.2481375 -4.2160454][-4.1697645 -4.1888905 -4.1939344 -4.1751356 -4.1103787 -3.9836197 -3.8363557 -3.8128579 -3.92397 -4.047338 -4.1438942 -4.2103648 -4.2344995 -4.2222733 -4.1855111][-4.1771364 -4.1742945 -4.1580224 -4.1160312 -4.0174012 -3.8443522 -3.6584468 -3.6748686 -3.8637938 -4.0284395 -4.1414108 -4.2084293 -4.2290373 -4.2121477 -4.170754][-4.186831 -4.1673341 -4.133812 -4.0763931 -3.9703794 -3.8113914 -3.6771049 -3.7379518 -3.9180055 -4.0670948 -4.1652513 -4.2227821 -4.2389183 -4.218123 -4.175386][-4.1899948 -4.1675372 -4.12562 -4.0693526 -3.9898703 -3.9048667 -3.864259 -3.9246366 -4.0400972 -4.1335192 -4.1959524 -4.2335629 -4.2435875 -4.2256136 -4.1877475][-4.1825337 -4.1577129 -4.1143308 -4.0731182 -4.0351562 -4.0088043 -4.0140295 -4.0568819 -4.1172304 -4.1682029 -4.2053266 -4.22951 -4.2385392 -4.2329574 -4.2055354][-4.1608248 -4.1367173 -4.1007609 -4.0841212 -4.086051 -4.0951014 -4.1103988 -4.1310029 -4.1542797 -4.1757631 -4.1965175 -4.2172589 -4.2313261 -4.2397389 -4.2282276][-4.1404538 -4.1224527 -4.0968747 -4.0990191 -4.1307693 -4.1600041 -4.1794086 -4.1816893 -4.1790562 -4.1761847 -4.1810346 -4.1981025 -4.2198052 -4.241971 -4.2412152][-4.1429987 -4.1275539 -4.1001205 -4.1066384 -4.1508179 -4.1922874 -4.2175183 -4.2146721 -4.1976271 -4.1760058 -4.1653686 -4.1808114 -4.2092614 -4.2364669 -4.2406521][-4.161921 -4.1394682 -4.102325 -4.1016603 -4.1468315 -4.1933117 -4.2179642 -4.2165527 -4.1988873 -4.1774096 -4.1642218 -4.1769161 -4.2057667 -4.2321177 -4.24036]]...]
INFO - root - 2017-12-07 12:06:42.446416: step 6910, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:54m:09s remains)
INFO - root - 2017-12-07 12:06:49.107038: step 6920, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 57h:37m:06s remains)
INFO - root - 2017-12-07 12:06:55.807909: step 6930, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:22m:43s remains)
INFO - root - 2017-12-07 12:07:02.720678: step 6940, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 62h:56m:29s remains)
INFO - root - 2017-12-07 12:07:09.627867: step 6950, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 66h:35m:17s remains)
INFO - root - 2017-12-07 12:07:16.463085: step 6960, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 61h:24m:29s remains)
INFO - root - 2017-12-07 12:07:23.161504: step 6970, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 58h:56m:56s remains)
INFO - root - 2017-12-07 12:07:30.161882: step 6980, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 66h:06m:25s remains)
INFO - root - 2017-12-07 12:07:36.974823: step 6990, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 61h:41m:52s remains)
INFO - root - 2017-12-07 12:07:43.760055: step 7000, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 61h:47m:13s remains)
2017-12-07 12:07:44.445752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2082639 -4.2039351 -4.2059011 -4.2128077 -4.2176251 -4.229424 -4.2420049 -4.2429223 -4.2369261 -4.2305789 -4.229528 -4.2271414 -4.221427 -4.2191029 -4.2233415][-4.2195272 -4.2095966 -4.2041411 -4.202168 -4.1990318 -4.2078085 -4.217216 -4.2150679 -4.2050743 -4.1990528 -4.2001953 -4.1999569 -4.1938705 -4.1948547 -4.2043538][-4.2276454 -4.2131429 -4.2037215 -4.1939082 -4.1803975 -4.181263 -4.18552 -4.1767316 -4.1661639 -4.1693282 -4.1791692 -4.1803417 -4.1715555 -4.1748519 -4.1904063][-4.2244396 -4.208252 -4.1972041 -4.1851864 -4.1633348 -4.1516485 -4.1455884 -4.1312237 -4.1293206 -4.1493878 -4.1693726 -4.1722875 -4.1609783 -4.1645584 -4.1850452][-4.2192988 -4.2027416 -4.1918983 -4.1759977 -4.143486 -4.1127305 -4.0847197 -4.0617962 -4.0790095 -4.1221266 -4.1550536 -4.1637864 -4.1519966 -4.1546535 -4.1763706][-4.1965389 -4.172946 -4.1563349 -4.1329203 -4.0888357 -4.0363083 -3.9825196 -3.9562652 -3.997467 -4.0657988 -4.1148944 -4.1298776 -4.1187882 -4.1210361 -4.1496005][-4.1760621 -4.1466522 -4.1286917 -4.1115689 -4.0704365 -4.0126133 -3.9570215 -3.9403067 -3.9888322 -4.0581145 -4.1093106 -4.1231952 -4.111105 -4.1098313 -4.1367817][-4.1716986 -4.1448545 -4.1327872 -4.1273007 -4.0968404 -4.0521431 -4.0132957 -4.0104079 -4.047967 -4.0951967 -4.1298041 -4.1308961 -4.1133752 -4.1053491 -4.12145][-4.1683941 -4.1486578 -4.1429248 -4.1421685 -4.1224923 -4.0972571 -4.0793729 -4.0856838 -4.1115179 -4.13546 -4.1507597 -4.1430268 -4.1232052 -4.1122465 -4.1169882][-4.1628118 -4.1540556 -4.1573138 -4.16093 -4.1489682 -4.1367984 -4.1384144 -4.1510181 -4.1697845 -4.18114 -4.1816196 -4.1665678 -4.1455345 -4.1319127 -4.1269045][-4.1644931 -4.1620822 -4.1705136 -4.1804724 -4.1736665 -4.1704574 -4.1799984 -4.1940804 -4.2071595 -4.2119842 -4.2054272 -4.1874957 -4.1686549 -4.1561241 -4.1514][-4.1731534 -4.1697936 -4.1779976 -4.188066 -4.1860757 -4.1881943 -4.2010345 -4.2141008 -4.2218432 -4.2222071 -4.21248 -4.19944 -4.1860876 -4.1792207 -4.1789289][-4.1938133 -4.1910362 -4.1998181 -4.2109647 -4.2141457 -4.2184796 -4.225595 -4.2315087 -4.2331409 -4.2309594 -4.2248254 -4.2186646 -4.2136469 -4.2128248 -4.2176642][-4.2190638 -4.2176185 -4.2274017 -4.23937 -4.2445097 -4.2489886 -4.25138 -4.2524562 -4.2523742 -4.2516465 -4.2502871 -4.2518029 -4.2557807 -4.2620625 -4.2708421][-4.2374482 -4.23874 -4.25073 -4.2626967 -4.2679119 -4.2702827 -4.2707257 -4.2699952 -4.26942 -4.2702122 -4.2718067 -4.2756109 -4.2813697 -4.2886457 -4.2987957]]...]
INFO - root - 2017-12-07 12:07:51.268077: step 7010, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 63h:40m:20s remains)
INFO - root - 2017-12-07 12:07:57.896123: step 7020, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:51m:30s remains)
INFO - root - 2017-12-07 12:08:04.709873: step 7030, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 64h:35m:14s remains)
INFO - root - 2017-12-07 12:08:11.551004: step 7040, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:31m:57s remains)
INFO - root - 2017-12-07 12:08:18.430362: step 7050, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 60h:36m:34s remains)
INFO - root - 2017-12-07 12:08:25.125893: step 7060, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 64h:47m:33s remains)
INFO - root - 2017-12-07 12:08:31.948232: step 7070, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:52m:04s remains)
INFO - root - 2017-12-07 12:08:38.850080: step 7080, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:15m:33s remains)
INFO - root - 2017-12-07 12:08:45.704548: step 7090, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 59h:20m:08s remains)
INFO - root - 2017-12-07 12:08:52.507697: step 7100, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.700 sec/batch; 63h:18m:36s remains)
2017-12-07 12:08:53.185918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2055473 -4.1951113 -4.1741505 -4.1397629 -4.12151 -4.1174483 -4.1258473 -4.1312203 -4.1216059 -4.1246262 -4.1498966 -4.1709991 -4.1758432 -4.1621928 -4.15191][-4.1915622 -4.185503 -4.1670232 -4.1335764 -4.1098666 -4.1029963 -4.1211562 -4.1366587 -4.1290817 -4.1287279 -4.1511078 -4.1703529 -4.1713586 -4.1541672 -4.1428618][-4.17431 -4.1738358 -4.160789 -4.1332545 -4.1112375 -4.1072578 -4.1339726 -4.1611471 -4.1636491 -4.1621933 -4.173697 -4.1823349 -4.1764889 -4.1562314 -4.1465507][-4.1564283 -4.1647849 -4.1579165 -4.1334181 -4.1127677 -4.1116862 -4.1444941 -4.1789341 -4.1922364 -4.19718 -4.20352 -4.203311 -4.1914759 -4.1709962 -4.1619577][-4.1431255 -4.1586637 -4.1575122 -4.1379366 -4.1174049 -4.1122041 -4.1385112 -4.1673861 -4.1844273 -4.2019916 -4.2174072 -4.2158933 -4.2011242 -4.18065 -4.1701822][-4.1395392 -4.1582756 -4.1644311 -4.1522679 -4.130969 -4.1147656 -4.1157546 -4.1150131 -4.1230054 -4.159729 -4.2013273 -4.2123981 -4.1988969 -4.1788249 -4.1665077][-4.159348 -4.1763315 -4.1841831 -4.1743517 -4.1469064 -4.1130857 -4.0739045 -4.0233855 -4.010181 -4.0681419 -4.14199 -4.1757174 -4.1741533 -4.1626482 -4.1584229][-4.1970773 -4.207242 -4.211699 -4.2000585 -4.1650953 -4.1132383 -4.0338387 -3.9305577 -3.883153 -3.9524775 -4.0605149 -4.122251 -4.1392069 -4.1425552 -4.1568446][-4.2294049 -4.2346625 -4.233582 -4.2253208 -4.1921353 -4.134582 -4.0402269 -3.9161491 -3.8487751 -3.9002533 -4.0082293 -4.0826631 -4.1146612 -4.1327248 -4.1629233][-4.2355175 -4.2438006 -4.2410188 -4.2365694 -4.2181768 -4.1764383 -4.1031885 -4.0101409 -3.9525523 -3.9670672 -4.0234351 -4.0694103 -4.0941277 -4.1184478 -4.1572232][-4.2092218 -4.2215276 -4.2221479 -4.2213411 -4.219511 -4.2028465 -4.1627316 -4.1123776 -4.0772281 -4.0683565 -4.0741005 -4.0776677 -4.0804572 -4.0947933 -4.1294451][-4.1649847 -4.175344 -4.181273 -4.1857495 -4.1942129 -4.1935925 -4.1805849 -4.1653342 -4.1565232 -4.1515861 -4.1384768 -4.11663 -4.0985627 -4.0909595 -4.1079187][-4.119576 -4.1267557 -4.1437807 -4.1560593 -4.166678 -4.1694565 -4.1710963 -4.1773334 -4.1934981 -4.2051363 -4.1947002 -4.1670208 -4.1374025 -4.1126275 -4.1111364][-4.0952926 -4.1042418 -4.1359067 -4.1575112 -4.1645079 -4.1595335 -4.1584883 -4.1655116 -4.1909008 -4.2194519 -4.2228317 -4.2015967 -4.1723466 -4.1461983 -4.1379571][-4.1141033 -4.1289253 -4.1701875 -4.1951637 -4.1939983 -4.1776056 -4.1648235 -4.1603236 -4.1788149 -4.2070127 -4.2175784 -4.2047763 -4.1836486 -4.16891 -4.1627755]]...]
INFO - root - 2017-12-07 12:08:59.969245: step 7110, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.624 sec/batch; 56h:23m:01s remains)
INFO - root - 2017-12-07 12:09:06.584816: step 7120, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 60h:33m:55s remains)
INFO - root - 2017-12-07 12:09:13.492208: step 7130, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 66h:07m:37s remains)
INFO - root - 2017-12-07 12:09:20.396849: step 7140, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 63h:21m:03s remains)
INFO - root - 2017-12-07 12:09:27.326881: step 7150, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 64h:49m:38s remains)
INFO - root - 2017-12-07 12:09:33.983623: step 7160, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:27m:23s remains)
INFO - root - 2017-12-07 12:09:40.695484: step 7170, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 58h:15m:56s remains)
INFO - root - 2017-12-07 12:09:47.464430: step 7180, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 64h:40m:30s remains)
INFO - root - 2017-12-07 12:09:54.379917: step 7190, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 60h:00m:58s remains)
INFO - root - 2017-12-07 12:10:01.160744: step 7200, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 64h:05m:09s remains)
2017-12-07 12:10:01.905939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3228617 -4.3130078 -4.3016911 -4.2925372 -4.2869592 -4.2810421 -4.2753744 -4.2716084 -4.273325 -4.2809424 -4.2848496 -4.2886815 -4.2979145 -4.3068552 -4.3139644][-4.3066611 -4.2886534 -4.2697396 -4.2549791 -4.2469888 -4.2402506 -4.2309642 -4.2227321 -4.2244625 -4.2352934 -4.2419848 -4.2469521 -4.2625003 -4.2791076 -4.2925019][-4.2779503 -4.2466 -4.2172031 -4.1965685 -4.1871724 -4.1766372 -4.1621051 -4.1502905 -4.1529951 -4.1675205 -4.1805334 -4.1893816 -4.2119803 -4.2370505 -4.260457][-4.2500944 -4.2075105 -4.17066 -4.1445279 -4.1342125 -4.1196008 -4.0982227 -4.08833 -4.0967393 -4.1139727 -4.12832 -4.1396618 -4.1660447 -4.1952243 -4.22689][-4.23156 -4.1798215 -4.1351104 -4.1014042 -4.0825882 -4.0554304 -4.024446 -4.0229688 -4.0429049 -4.0599618 -4.0718908 -4.08954 -4.1209245 -4.1580596 -4.1991668][-4.2125397 -4.1492176 -4.091857 -4.0469685 -4.0175014 -3.9772894 -3.9391246 -3.94882 -3.9861276 -4.0069828 -4.0174651 -4.0406213 -4.0789618 -4.1279707 -4.1784668][-4.1954451 -4.1239672 -4.0551009 -4.0026608 -3.97012 -3.9261661 -3.8871331 -3.8991506 -3.9411993 -3.9692836 -3.9849133 -4.0110159 -4.0542693 -4.1081061 -4.1629][-4.1859865 -4.1171627 -4.0485854 -3.9983735 -3.9738429 -3.9437654 -3.917686 -3.9203708 -3.9469445 -3.972965 -3.9939961 -4.0177188 -4.0552192 -4.1039896 -4.15618][-4.18721 -4.122 -4.0608983 -4.0208716 -4.0076227 -3.9924302 -3.9791722 -3.972086 -3.9769585 -3.9955094 -4.0184617 -4.0386186 -4.0697389 -4.1152816 -4.1657767][-4.1981931 -4.1387787 -4.0885539 -4.0588207 -4.0465331 -4.0378537 -4.0394487 -4.0359783 -4.0255179 -4.0327582 -4.0563345 -4.078711 -4.1056209 -4.1471243 -4.1915269][-4.2247243 -4.1762934 -4.1381478 -4.1191683 -4.1071939 -4.100934 -4.1146603 -4.1233077 -4.1095867 -4.1081634 -4.1277232 -4.1548381 -4.1785393 -4.209549 -4.240232][-4.2707348 -4.2426772 -4.220911 -4.2131748 -4.2065725 -4.2006478 -4.2178659 -4.2348633 -4.2258396 -4.2176189 -4.2278361 -4.2521873 -4.2710018 -4.2894864 -4.3038049][-4.3110881 -4.3020992 -4.2970185 -4.2958107 -4.2938261 -4.29021 -4.3045626 -4.3192263 -4.3150663 -4.3056111 -4.30787 -4.326046 -4.3390675 -4.3459949 -4.3482957][-4.3308611 -4.3326836 -4.3356309 -4.3369985 -4.3373461 -4.3367763 -4.3465862 -4.3535814 -4.349369 -4.3411126 -4.3400712 -4.3527393 -4.3600841 -4.3620949 -4.3611541][-4.3379378 -4.3431554 -4.3484511 -4.350543 -4.3498821 -4.3490105 -4.3527851 -4.3545027 -4.3511782 -4.3443451 -4.342577 -4.3505278 -4.3567119 -4.3599119 -4.3596725]]...]
INFO - root - 2017-12-07 12:10:08.700713: step 7210, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 63h:37m:29s remains)
INFO - root - 2017-12-07 12:10:15.471696: step 7220, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.736 sec/batch; 66h:30m:16s remains)
INFO - root - 2017-12-07 12:10:22.293939: step 7230, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.669 sec/batch; 60h:26m:12s remains)
INFO - root - 2017-12-07 12:10:29.031043: step 7240, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.651 sec/batch; 58h:51m:04s remains)
INFO - root - 2017-12-07 12:10:35.790557: step 7250, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 62h:23m:24s remains)
INFO - root - 2017-12-07 12:10:42.544037: step 7260, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:48m:11s remains)
INFO - root - 2017-12-07 12:10:49.344885: step 7270, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 63h:27m:48s remains)
INFO - root - 2017-12-07 12:10:56.185311: step 7280, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:46m:35s remains)
INFO - root - 2017-12-07 12:11:02.971636: step 7290, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 59h:56m:45s remains)
INFO - root - 2017-12-07 12:11:09.995374: step 7300, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.784 sec/batch; 70h:50m:26s remains)
2017-12-07 12:11:10.750801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.252315 -4.2391582 -4.241859 -4.252192 -4.2618051 -4.2632647 -4.2568259 -4.2528591 -4.2537646 -4.2526298 -4.2468357 -4.2400231 -4.2437973 -4.2536039 -4.2673817][-4.2045355 -4.1846967 -4.1874914 -4.2033377 -4.2172136 -4.2201529 -4.210959 -4.205976 -4.2043996 -4.1998868 -4.19283 -4.1912994 -4.2045708 -4.2231922 -4.2387371][-4.18013 -4.1632957 -4.1672497 -4.1884384 -4.2024512 -4.2017231 -4.1912856 -4.1871872 -4.1852536 -4.1798744 -4.1771259 -4.1836233 -4.2019043 -4.2211671 -4.2335138][-4.1791625 -4.1732326 -4.1812682 -4.1962118 -4.1996202 -4.1891379 -4.1705728 -4.16084 -4.1627803 -4.1722 -4.1851482 -4.2008204 -4.2182846 -4.2262421 -4.2274566][-4.1899161 -4.1940255 -4.2052026 -4.2108955 -4.1976938 -4.168057 -4.1229906 -4.0911555 -4.103179 -4.1486979 -4.1909137 -4.2151403 -4.2261276 -4.2231255 -4.2119689][-4.2111187 -4.2231727 -4.234169 -4.2311 -4.2040386 -4.1505671 -4.0602231 -3.9794762 -4.0088615 -4.1113062 -4.1848259 -4.216073 -4.2290154 -4.226171 -4.2112694][-4.2268667 -4.23827 -4.2453256 -4.2406197 -4.2095561 -4.1445003 -4.0244379 -3.9049416 -3.9460821 -4.0849814 -4.1762152 -4.2102089 -4.2290034 -4.2343831 -4.2226834][-4.2323527 -4.2451277 -4.2532916 -4.2541833 -4.2317929 -4.1771355 -4.07973 -3.9904592 -4.0202489 -4.1242261 -4.1944914 -4.2195425 -4.2365761 -4.2463694 -4.2372851][-4.2297931 -4.24542 -4.2578125 -4.2630625 -4.2553787 -4.2246556 -4.1690331 -4.1267791 -4.1427689 -4.1980925 -4.2299967 -4.2358923 -4.2412758 -4.2438855 -4.23329][-4.2195139 -4.2330103 -4.248879 -4.2582474 -4.2599235 -4.2485752 -4.2252159 -4.2147889 -4.2251258 -4.2495017 -4.256073 -4.2524657 -4.2492809 -4.2410398 -4.2250953][-4.2162142 -4.2238789 -4.2403688 -4.2536244 -4.2599411 -4.2573466 -4.2494531 -4.25191 -4.2591066 -4.2713938 -4.2738657 -4.2741718 -4.2755132 -4.2654581 -4.2431674][-4.2405834 -4.2481394 -4.2634978 -4.2748847 -4.2758307 -4.273201 -4.2698393 -4.2727308 -4.2806792 -4.292882 -4.2986488 -4.3073492 -4.3149171 -4.3052764 -4.2781477][-4.2715521 -4.2794924 -4.2931509 -4.3025312 -4.2998405 -4.2952666 -4.291203 -4.2937222 -4.3020082 -4.3149447 -4.3232288 -4.3316178 -4.3365474 -4.326108 -4.2982926][-4.2898712 -4.2949553 -4.3041868 -4.3123484 -4.3132915 -4.3113594 -4.3058591 -4.3078966 -4.3173079 -4.3293152 -4.3371453 -4.3414311 -4.3430195 -4.3351622 -4.3103538][-4.3124156 -4.3159761 -4.3192239 -4.3223162 -4.3267097 -4.328764 -4.32308 -4.3233395 -4.3323808 -4.3453503 -4.3515019 -4.3528676 -4.3516388 -4.3463511 -4.32727]]...]
INFO - root - 2017-12-07 12:11:17.468448: step 7310, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 57h:07m:27s remains)
INFO - root - 2017-12-07 12:11:24.184064: step 7320, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 63h:32m:02s remains)
INFO - root - 2017-12-07 12:11:30.932813: step 7330, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 64h:56m:28s remains)
INFO - root - 2017-12-07 12:11:37.708536: step 7340, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 58h:57m:29s remains)
INFO - root - 2017-12-07 12:11:44.604449: step 7350, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 59h:34m:05s remains)
INFO - root - 2017-12-07 12:11:51.500840: step 7360, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 64h:30m:19s remains)
INFO - root - 2017-12-07 12:11:58.159758: step 7370, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 61h:52m:12s remains)
INFO - root - 2017-12-07 12:12:05.004518: step 7380, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 57h:28m:14s remains)
INFO - root - 2017-12-07 12:12:11.872049: step 7390, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:38m:43s remains)
INFO - root - 2017-12-07 12:12:18.609040: step 7400, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 64h:21m:09s remains)
2017-12-07 12:12:19.333574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2806787 -4.2948618 -4.3068357 -4.2962627 -4.2705841 -4.2315793 -4.1769609 -4.1283913 -4.0992951 -4.1159482 -4.1601357 -4.2086291 -4.2423058 -4.2593727 -4.2663145][-4.3021684 -4.31065 -4.3119712 -4.2980709 -4.2740626 -4.2320151 -4.1763458 -4.1258488 -4.1020026 -4.1230597 -4.1654186 -4.205709 -4.2361126 -4.2543273 -4.2639923][-4.3043041 -4.3116541 -4.3080034 -4.2971897 -4.2823095 -4.2490559 -4.2018309 -4.1523509 -4.1248021 -4.14086 -4.1787553 -4.2110758 -4.2380562 -4.2607532 -4.2733917][-4.303514 -4.3069353 -4.2983155 -4.2893109 -4.283164 -4.2621064 -4.224576 -4.1716013 -4.1339955 -4.1420803 -4.1774583 -4.2097197 -4.2389941 -4.2682233 -4.2850142][-4.3140116 -4.3122778 -4.3005962 -4.2879486 -4.276155 -4.2483025 -4.1999731 -4.1336446 -4.0924439 -4.1084304 -4.1569052 -4.2008958 -4.2384191 -4.271728 -4.2936635][-4.3295226 -4.3249836 -4.3125181 -4.2942743 -4.2623854 -4.20569 -4.1217895 -4.0244341 -3.993259 -4.0461454 -4.1262574 -4.1857991 -4.231514 -4.2705793 -4.2971668][-4.335505 -4.3297563 -4.3200092 -4.2967939 -4.2422919 -4.1443291 -4.0033693 -3.858644 -3.8487659 -3.9668818 -4.0908079 -4.1699724 -4.225668 -4.2715912 -4.3018794][-4.3314824 -4.3269925 -4.320303 -4.2951269 -4.2275295 -4.1011276 -3.9198194 -3.7387705 -3.7470176 -3.9120679 -4.0628428 -4.1569223 -4.2231412 -4.275383 -4.307693][-4.3242455 -4.3210196 -4.3158278 -4.2938781 -4.2323217 -4.1216593 -3.9669843 -3.8137047 -3.8059127 -3.9339461 -4.0649261 -4.1562648 -4.225522 -4.27753 -4.308322][-4.3214803 -4.3185043 -4.3135366 -4.2973485 -4.252532 -4.1773162 -4.0811758 -3.9827414 -3.9552987 -4.0144525 -4.09595 -4.1700969 -4.2353287 -4.2850213 -4.3136382][-4.3226376 -4.320951 -4.3155746 -4.3021321 -4.2716389 -4.2255569 -4.1708269 -4.1107516 -4.0806174 -4.1007996 -4.14631 -4.2041693 -4.2606006 -4.3022046 -4.3251839][-4.3248639 -4.3252244 -4.3184805 -4.3052149 -4.282443 -4.2532549 -4.2189546 -4.1798716 -4.1574259 -4.1668196 -4.1996994 -4.2472959 -4.2917328 -4.3212705 -4.3346024][-4.3238907 -4.3269143 -4.3210273 -4.308208 -4.288424 -4.2650685 -4.2395577 -4.2121739 -4.1979194 -4.2068448 -4.2383347 -4.2813005 -4.3143754 -4.3325505 -4.3372703][-4.3160863 -4.3219647 -4.3205013 -4.3129792 -4.2973714 -4.2766757 -4.25316 -4.2326665 -4.2239656 -4.2332683 -4.2620678 -4.2983184 -4.322237 -4.3321309 -4.3324175][-4.3047471 -4.314271 -4.3174062 -4.3149061 -4.3035326 -4.2875504 -4.2672877 -4.2506328 -4.2458425 -4.2552004 -4.2780809 -4.3040051 -4.3197117 -4.32476 -4.3240552]]...]
INFO - root - 2017-12-07 12:12:26.040235: step 7410, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 56h:48m:47s remains)
INFO - root - 2017-12-07 12:12:32.593824: step 7420, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 56h:56m:01s remains)
INFO - root - 2017-12-07 12:12:39.504898: step 7430, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 64h:33m:44s remains)
INFO - root - 2017-12-07 12:12:46.369203: step 7440, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 64h:06m:20s remains)
INFO - root - 2017-12-07 12:12:53.097393: step 7450, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.685 sec/batch; 61h:48m:44s remains)
INFO - root - 2017-12-07 12:12:59.905301: step 7460, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:29m:58s remains)
INFO - root - 2017-12-07 12:13:06.714469: step 7470, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 59h:26m:50s remains)
INFO - root - 2017-12-07 12:13:13.485464: step 7480, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 61h:24m:06s remains)
INFO - root - 2017-12-07 12:13:20.207805: step 7490, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 64h:26m:31s remains)
INFO - root - 2017-12-07 12:13:27.017859: step 7500, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 61h:43m:56s remains)
2017-12-07 12:13:27.832488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3469481 -4.343009 -4.34341 -4.34399 -4.3325558 -4.3132572 -4.3004808 -4.3038116 -4.3068566 -4.3059139 -4.3046489 -4.3035603 -4.3001709 -4.2887807 -4.2804179][-4.3512039 -4.3451033 -4.3419542 -4.3343859 -4.30855 -4.2747765 -4.2541494 -4.26163 -4.270565 -4.2756429 -4.2789845 -4.279036 -4.2748995 -4.2599521 -4.2459559][-4.3486586 -4.3434844 -4.3388004 -4.3193703 -4.27596 -4.2225962 -4.1900625 -4.2049336 -4.2321887 -4.2543983 -4.2669339 -4.2694979 -4.2601275 -4.2336383 -4.2086358][-4.3492651 -4.3463488 -4.3378329 -4.3062954 -4.2468138 -4.1723814 -4.124362 -4.1498485 -4.202919 -4.244236 -4.2669587 -4.2723484 -4.2557979 -4.2163153 -4.1802583][-4.347878 -4.3461947 -4.3308949 -4.2879782 -4.215169 -4.1232052 -4.058 -4.0875335 -4.1634541 -4.2226663 -4.2556572 -4.2612405 -4.2369943 -4.1913023 -4.1526642][-4.3403296 -4.3345032 -4.30958 -4.2566571 -4.1731749 -4.0653305 -3.9807444 -4.0107355 -4.10705 -4.1893358 -4.2367253 -4.2476449 -4.2223077 -4.1737528 -4.1412492][-4.3244963 -4.3090863 -4.2764521 -4.2179952 -4.1277585 -4.0055327 -3.8936048 -3.9176171 -4.0438609 -4.1542792 -4.2139492 -4.2310681 -4.20558 -4.1615615 -4.1448088][-4.3004117 -4.2724295 -4.2306943 -4.1749883 -4.0941463 -3.9699125 -3.8324795 -3.8389854 -3.9852338 -4.1114192 -4.1765375 -4.1941986 -4.1655765 -4.1291323 -4.1360159][-4.265811 -4.2308989 -4.1887221 -4.1514969 -4.1051464 -4.0228233 -3.9215937 -3.9153934 -4.018528 -4.111382 -4.1548657 -4.1634088 -4.1316509 -4.1002331 -4.1216197][-4.2317109 -4.2017627 -4.1691871 -4.1556392 -4.1439395 -4.1028671 -4.0448689 -4.0398631 -4.099575 -4.1494346 -4.1725116 -4.1738539 -4.141068 -4.1089664 -4.1273451][-4.2123857 -4.1897874 -4.168191 -4.1685219 -4.1743336 -4.16037 -4.1306686 -4.125556 -4.1571417 -4.1839824 -4.1949439 -4.1933341 -4.1630569 -4.1287174 -4.1410823][-4.2040296 -4.1877503 -4.17642 -4.182858 -4.1964917 -4.202219 -4.191576 -4.1866722 -4.2029624 -4.2157812 -4.2192416 -4.2182922 -4.1951051 -4.168427 -4.1762161][-4.2099624 -4.1996727 -4.193687 -4.2009974 -4.2187662 -4.2323256 -4.2315779 -4.2294092 -4.2388287 -4.2472434 -4.2493072 -4.2527213 -4.2411036 -4.2247634 -4.2266803][-4.2348757 -4.23133 -4.2287207 -4.2349639 -4.249773 -4.2641144 -4.2677088 -4.2681456 -4.275208 -4.2806706 -4.2846975 -4.2906442 -4.287292 -4.2793322 -4.2796469][-4.2688808 -4.2700996 -4.2725067 -4.276885 -4.287612 -4.3008046 -4.3070655 -4.309195 -4.3125243 -4.315537 -4.32024 -4.3250751 -4.3250766 -4.321527 -4.3205271]]...]
INFO - root - 2017-12-07 12:13:34.533428: step 7510, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 64h:12m:15s remains)
INFO - root - 2017-12-07 12:13:41.150890: step 7520, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 65h:00m:09s remains)
INFO - root - 2017-12-07 12:13:48.007228: step 7530, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 61h:42m:16s remains)
INFO - root - 2017-12-07 12:13:54.723958: step 7540, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 56h:49m:13s remains)
INFO - root - 2017-12-07 12:14:01.680952: step 7550, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 64h:17m:13s remains)
INFO - root - 2017-12-07 12:14:08.488666: step 7560, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 65h:15m:31s remains)
INFO - root - 2017-12-07 12:14:15.402209: step 7570, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 64h:01m:00s remains)
INFO - root - 2017-12-07 12:14:22.122540: step 7580, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 57h:46m:24s remains)
INFO - root - 2017-12-07 12:14:28.912810: step 7590, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 59h:04m:43s remains)
INFO - root - 2017-12-07 12:14:35.757185: step 7600, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.719 sec/batch; 64h:51m:58s remains)
2017-12-07 12:14:36.516799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3289676 -4.2939372 -4.2499995 -4.2130504 -4.1748023 -4.1544147 -4.1817031 -4.2237668 -4.2489209 -4.2658086 -4.2850552 -4.2959614 -4.2881532 -4.2654862 -4.2231379][-4.340384 -4.3027225 -4.25255 -4.2076211 -4.161902 -4.1349912 -4.1609488 -4.1986265 -4.2172379 -4.2349758 -4.2671785 -4.2903347 -4.2843456 -4.2564983 -4.2066255][-4.3507361 -4.3116117 -4.2591743 -4.2085066 -4.1543283 -4.114048 -4.1269574 -4.1548762 -4.16595 -4.1852331 -4.2314134 -4.2720876 -4.2781339 -4.2539129 -4.2019382][-4.3607292 -4.3269334 -4.2795048 -4.2285595 -4.1673484 -4.1116219 -4.1061563 -4.1248045 -4.1273408 -4.1470017 -4.2029419 -4.25631 -4.2760596 -4.2579832 -4.211556][-4.3641396 -4.3358321 -4.2960992 -4.2535663 -4.1921625 -4.1265206 -4.1107578 -4.1283317 -4.1239619 -4.13146 -4.1808043 -4.2346716 -4.2647338 -4.2624025 -4.2313256][-4.3563809 -4.329042 -4.2970572 -4.2637444 -4.1984234 -4.1253343 -4.1078787 -4.1307616 -4.1238885 -4.1111426 -4.1399875 -4.1907868 -4.2363739 -4.2600074 -4.2553339][-4.3427873 -4.3131981 -4.285471 -4.2527122 -4.1749249 -4.0891943 -4.0721774 -4.1001592 -4.0864544 -4.0484657 -4.0572782 -4.1165614 -4.1886482 -4.2421622 -4.262361][-4.3320417 -4.3011827 -4.2718921 -4.23279 -4.1396751 -4.0380626 -4.019022 -4.050354 -4.0301471 -3.9731255 -3.9766941 -4.053102 -4.1486745 -4.2222319 -4.2606668][-4.3323913 -4.3042703 -4.275116 -4.2347159 -4.1435661 -4.041914 -4.0235038 -4.0584378 -4.0411263 -3.9814403 -3.9771149 -4.048213 -4.1404476 -4.2122793 -4.254281][-4.3400254 -4.3202209 -4.2955732 -4.2608824 -4.1867704 -4.1022859 -4.0895023 -4.1265235 -4.1198292 -4.073514 -4.0578833 -4.09758 -4.16527 -4.2223864 -4.2547555][-4.3502374 -4.3374534 -4.3171587 -4.2921853 -4.2441683 -4.1885934 -4.1848407 -4.2184348 -4.2205348 -4.1913476 -4.17089 -4.182116 -4.2184567 -4.2502861 -4.2670088][-4.3555512 -4.3480196 -4.3300719 -4.309103 -4.2766523 -4.2426596 -4.247685 -4.2806511 -4.2933993 -4.2823744 -4.2680287 -4.2661915 -4.2791328 -4.2909675 -4.2943516][-4.3532958 -4.3497996 -4.3348737 -4.3131032 -4.28278 -4.2581587 -4.2691298 -4.3031282 -4.3218737 -4.3214564 -4.3143754 -4.311821 -4.3162856 -4.3204942 -4.31966][-4.3442497 -4.3409014 -4.3283205 -4.306355 -4.2749805 -4.2536564 -4.26692 -4.3004742 -4.3184505 -4.3201327 -4.3167691 -4.3177118 -4.3228397 -4.3271036 -4.3277082][-4.342689 -4.3382406 -4.3237782 -4.2997656 -4.2693944 -4.252368 -4.2672887 -4.2980404 -4.3144364 -4.3173933 -4.3178368 -4.3200722 -4.3242092 -4.3268561 -4.32581]]...]
INFO - root - 2017-12-07 12:14:43.225460: step 7610, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 58h:28m:03s remains)
INFO - root - 2017-12-07 12:14:49.832612: step 7620, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 58h:52m:06s remains)
INFO - root - 2017-12-07 12:14:56.633423: step 7630, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 64h:26m:02s remains)
INFO - root - 2017-12-07 12:15:03.458388: step 7640, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 62h:32m:28s remains)
INFO - root - 2017-12-07 12:15:10.281125: step 7650, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 62h:51m:43s remains)
INFO - root - 2017-12-07 12:15:17.037812: step 7660, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 58h:31m:39s remains)
INFO - root - 2017-12-07 12:15:23.937432: step 7670, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 61h:51m:46s remains)
INFO - root - 2017-12-07 12:15:30.643483: step 7680, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 61h:40m:43s remains)
INFO - root - 2017-12-07 12:15:37.518823: step 7690, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 60h:30m:50s remains)
INFO - root - 2017-12-07 12:15:44.338288: step 7700, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 62h:31m:22s remains)
2017-12-07 12:15:45.037016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22717 -4.2425466 -4.2341256 -4.2164879 -4.2005816 -4.2026305 -4.2255049 -4.2594266 -4.285007 -4.301095 -4.3082676 -4.3026443 -4.281136 -4.2583394 -4.2479014][-4.1726255 -4.198616 -4.1888075 -4.1734986 -4.1633072 -4.16794 -4.19401 -4.2295713 -4.2515888 -4.260519 -4.2622867 -4.2559934 -4.2369342 -4.2138619 -4.206358][-4.1301842 -4.1601429 -4.1522465 -4.1424789 -4.1415749 -4.1518884 -4.1742997 -4.2020841 -4.2170773 -4.2192478 -4.2169671 -4.2146082 -4.204349 -4.1896906 -4.1908612][-4.1409454 -4.163538 -4.1596141 -4.1546426 -4.157732 -4.1628881 -4.1718297 -4.1838589 -4.1919889 -4.1918559 -4.1906891 -4.1956434 -4.1989942 -4.2001357 -4.2105775][-4.2009382 -4.2060089 -4.1963143 -4.1877561 -4.1787791 -4.1634583 -4.1541319 -4.1562266 -4.1648006 -4.1647763 -4.1621785 -4.1701827 -4.1818533 -4.195035 -4.2179828][-4.2391839 -4.22245 -4.1964455 -4.1778541 -4.1496549 -4.1130147 -4.094213 -4.097105 -4.1081982 -4.1099663 -4.1131473 -4.1287179 -4.1469908 -4.1698337 -4.2058277][-4.2418804 -4.2104387 -4.16695 -4.12884 -4.0752416 -4.02725 -4.0113883 -4.025733 -4.0455284 -4.0533547 -4.0679007 -4.0927162 -4.1180592 -4.1511741 -4.1962504][-4.24281 -4.20641 -4.151381 -4.0931163 -4.0222812 -3.9770322 -3.9718671 -3.9949224 -4.024332 -4.0458264 -4.0675263 -4.09237 -4.1215377 -4.16461 -4.2116156][-4.2478185 -4.2155719 -4.1617775 -4.1038327 -4.0440683 -4.010818 -4.00687 -4.0236559 -4.0601587 -4.0956488 -4.1215682 -4.1444397 -4.1731682 -4.2148581 -4.25132][-4.2529478 -4.2284408 -4.1894865 -4.1531506 -4.1209049 -4.1057215 -4.1064939 -4.1213727 -4.1573472 -4.1921744 -4.2124262 -4.2290306 -4.2507238 -4.276103 -4.29526][-4.2466679 -4.2318397 -4.2142868 -4.2063208 -4.2024097 -4.2051964 -4.2149935 -4.2318659 -4.2601905 -4.2787905 -4.2867742 -4.2930193 -4.3036327 -4.3163314 -4.3240366][-4.2245321 -4.2280526 -4.2375546 -4.2566857 -4.2722349 -4.2839189 -4.2940564 -4.3078527 -4.3237195 -4.3285155 -4.327889 -4.3283134 -4.3337126 -4.3394809 -4.3414335][-4.1865797 -4.2096477 -4.2443709 -4.2830529 -4.3099127 -4.3231726 -4.3269281 -4.3324075 -4.3394156 -4.3395329 -4.3378344 -4.3383031 -4.3418646 -4.3460875 -4.3474369][-4.1464968 -4.17994 -4.2299104 -4.2805305 -4.3118553 -4.3235631 -4.3242092 -4.3257256 -4.3295236 -4.3298473 -4.3302178 -4.3320584 -4.3353028 -4.338058 -4.3396115][-4.13037 -4.1620584 -4.2106171 -4.26045 -4.2929287 -4.3061485 -4.3109636 -4.3144011 -4.3186073 -4.32082 -4.3217826 -4.3229694 -4.3240213 -4.3251052 -4.3258696]]...]
INFO - root - 2017-12-07 12:15:51.808681: step 7710, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 64h:30m:22s remains)
INFO - root - 2017-12-07 12:15:58.531058: step 7720, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 62h:22m:54s remains)
INFO - root - 2017-12-07 12:16:05.465743: step 7730, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 59h:18m:52s remains)
INFO - root - 2017-12-07 12:16:12.409968: step 7740, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 59h:41m:19s remains)
INFO - root - 2017-12-07 12:16:19.328832: step 7750, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 64h:11m:38s remains)
INFO - root - 2017-12-07 12:16:26.034124: step 7760, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 59h:23m:09s remains)
INFO - root - 2017-12-07 12:16:32.831616: step 7770, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 59h:30m:03s remains)
INFO - root - 2017-12-07 12:16:39.651494: step 7780, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:34m:10s remains)
INFO - root - 2017-12-07 12:16:46.564220: step 7790, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.724 sec/batch; 65h:19m:28s remains)
INFO - root - 2017-12-07 12:16:53.350427: step 7800, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 65h:58m:18s remains)
2017-12-07 12:16:54.116939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2508783 -4.2387156 -4.234211 -4.2260146 -4.2181315 -4.2207255 -4.223846 -4.2237153 -4.223393 -4.221837 -4.2261343 -4.2424741 -4.2626648 -4.2760921 -4.2773652][-4.215373 -4.2046123 -4.209064 -4.2072015 -4.2025628 -4.20752 -4.2112784 -4.2095656 -4.20918 -4.2077456 -4.2126365 -4.2279296 -4.2495332 -4.2647991 -4.2678633][-4.18174 -4.1722636 -4.18761 -4.1984105 -4.1976409 -4.1950059 -4.1904902 -4.1851687 -4.1900244 -4.1967144 -4.2069726 -4.2221274 -4.239109 -4.2511511 -4.2572608][-4.1736851 -4.1645131 -4.1797485 -4.1882706 -4.1781263 -4.1583624 -4.1377072 -4.13487 -4.1569142 -4.1774654 -4.19838 -4.2173815 -4.2292233 -4.2391987 -4.2480578][-4.1809759 -4.1740885 -4.1830907 -4.1836939 -4.163353 -4.1277795 -4.0907888 -4.0918407 -4.1369648 -4.1703863 -4.2002311 -4.2229538 -4.2323332 -4.2397881 -4.2496858][-4.1755304 -4.1702962 -4.1769481 -4.1687632 -4.1333423 -4.0789452 -4.0153756 -4.0123868 -4.0874863 -4.1462722 -4.1902404 -4.2239556 -4.2358832 -4.2449603 -4.2567096][-4.1699615 -4.1657763 -4.1659861 -4.1426558 -4.0798864 -3.9849515 -3.869751 -3.8481097 -3.9694989 -4.0757847 -4.1428366 -4.1877985 -4.2079 -4.2214003 -4.2387381][-4.187242 -4.1881838 -4.1787519 -4.1367564 -4.0496292 -3.9201078 -3.7542312 -3.704447 -3.865777 -4.0112219 -4.0986042 -4.150383 -4.1745229 -4.1897697 -4.2058225][-4.2159991 -4.2247071 -4.2175517 -4.1804566 -4.1070991 -4.0083804 -3.8881736 -3.85005 -3.9629719 -4.0819011 -4.1489358 -4.1844416 -4.1997046 -4.2047091 -4.2097406][-4.245111 -4.2595472 -4.2572351 -4.2309122 -4.1798458 -4.1174407 -4.0478125 -4.0207281 -4.0867434 -4.1713715 -4.2208824 -4.240365 -4.2469225 -4.2484431 -4.2494059][-4.2601004 -4.2768464 -4.2778444 -4.2606153 -4.2245603 -4.181488 -4.137897 -4.1171441 -4.1524563 -4.2100048 -4.2476587 -4.2646179 -4.2719269 -4.2762675 -4.2803583][-4.2752013 -4.2963481 -4.3008404 -4.2871776 -4.2569923 -4.2208333 -4.1906309 -4.1795406 -4.2016182 -4.2373 -4.2606678 -4.2718325 -4.2773976 -4.2811494 -4.2826653][-4.2807279 -4.3031669 -4.3109746 -4.2992468 -4.2749681 -4.2511978 -4.2394772 -4.240716 -4.2548141 -4.274848 -4.286881 -4.2898612 -4.2898421 -4.2878084 -4.282701][-4.2801971 -4.29677 -4.3057108 -4.2975764 -4.280622 -4.2684417 -4.2685652 -4.2763553 -4.2847247 -4.2938 -4.2997432 -4.3014784 -4.3008337 -4.29857 -4.2932253][-4.2809381 -4.294014 -4.3021832 -4.2989397 -4.2880468 -4.2823133 -4.2867551 -4.2953 -4.3011656 -4.3046684 -4.307447 -4.3096037 -4.3112044 -4.3120441 -4.3106189]]...]
INFO - root - 2017-12-07 12:17:00.818016: step 7810, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:47m:28s remains)
INFO - root - 2017-12-07 12:17:07.539843: step 7820, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 65h:14m:25s remains)
INFO - root - 2017-12-07 12:17:14.434024: step 7830, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.711 sec/batch; 64h:08m:49s remains)
INFO - root - 2017-12-07 12:17:21.313365: step 7840, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.671 sec/batch; 60h:28m:47s remains)
INFO - root - 2017-12-07 12:17:28.111579: step 7850, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 58h:04m:29s remains)
INFO - root - 2017-12-07 12:17:34.952414: step 7860, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 64h:01m:03s remains)
INFO - root - 2017-12-07 12:17:41.766079: step 7870, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 62h:41m:40s remains)
INFO - root - 2017-12-07 12:17:48.477674: step 7880, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 58h:23m:55s remains)
INFO - root - 2017-12-07 12:17:55.287641: step 7890, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 61h:04m:46s remains)
INFO - root - 2017-12-07 12:18:02.134317: step 7900, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 58h:15m:11s remains)
2017-12-07 12:18:02.949308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21139 -4.1997938 -4.2013488 -4.2122927 -4.218708 -4.2198415 -4.2186971 -4.2183514 -4.2226257 -4.2359667 -4.2512836 -4.2617011 -4.2665062 -4.2593775 -4.2498255][-4.1713943 -4.1597219 -4.1680169 -4.1842365 -4.18947 -4.1851335 -4.1805334 -4.1817341 -4.1896386 -4.2081347 -4.2286539 -4.2429571 -4.2493086 -4.2397833 -4.2258973][-4.162807 -4.1547832 -4.1671915 -4.1850014 -4.187686 -4.1766744 -4.1654983 -4.1640754 -4.173604 -4.1948624 -4.2204595 -4.2393808 -4.2468963 -4.2350087 -4.2184119][-4.1762648 -4.1732621 -4.1831703 -4.1940336 -4.1882257 -4.1675959 -4.1506281 -4.1458163 -4.1523881 -4.1725736 -4.2024016 -4.2266345 -4.23543 -4.2247868 -4.2117424][-4.1863952 -4.1896381 -4.1945491 -4.1958351 -4.1805234 -4.1554341 -4.1378694 -4.1322308 -4.1366973 -4.153728 -4.1819019 -4.2047515 -4.2113881 -4.2059412 -4.2035251][-4.1982212 -4.2046871 -4.2036519 -4.1943307 -4.1729412 -4.1499977 -4.1378145 -4.1354642 -4.1383476 -4.1505246 -4.1723318 -4.18883 -4.1923413 -4.190824 -4.1987319][-4.2013183 -4.2037582 -4.1981845 -4.18366 -4.1632566 -4.1476846 -4.1434751 -4.145606 -4.1465 -4.1515946 -4.1646786 -4.17407 -4.1743031 -4.1745629 -4.1885552][-4.1940203 -4.192605 -4.1868911 -4.1755552 -4.1644936 -4.1587811 -4.1612077 -4.1664839 -4.1630206 -4.1579833 -4.1616254 -4.164567 -4.1597509 -4.1580877 -4.1718512][-4.1798987 -4.1760035 -4.175703 -4.1741529 -4.1763082 -4.1791668 -4.1859908 -4.1884527 -4.1742492 -4.1584129 -4.152658 -4.1497993 -4.1417542 -4.1380854 -4.15051][-4.1643662 -4.1626024 -4.1726704 -4.1824384 -4.1947083 -4.2004867 -4.2055526 -4.2010579 -4.1763687 -4.1525884 -4.138989 -4.1338835 -4.1232581 -4.1148648 -4.125586][-4.155313 -4.1571622 -4.1749454 -4.1917562 -4.2079477 -4.2114887 -4.2099552 -4.1992745 -4.1693425 -4.1435795 -4.1284266 -4.1246548 -4.1164088 -4.1090674 -4.1191096][-4.1506267 -4.1538391 -4.1748724 -4.19411 -4.2101388 -4.2110653 -4.2042179 -4.18859 -4.1571107 -4.1344628 -4.12207 -4.1230755 -4.1218076 -4.119967 -4.1288538][-4.1518455 -4.1559262 -4.1768808 -4.1955023 -4.20928 -4.21044 -4.2015657 -4.183094 -4.1543665 -4.1364608 -4.1299815 -4.1354175 -4.14157 -4.1457763 -4.1527162][-4.1719556 -4.1769738 -4.1930666 -4.2076945 -4.2166314 -4.2187262 -4.2084069 -4.1878319 -4.1633205 -4.1507654 -4.152082 -4.1615105 -4.1703625 -4.1761394 -4.18278][-4.2098556 -4.2138715 -4.2231736 -4.2306004 -4.2346315 -4.23614 -4.2275729 -4.2095509 -4.1909294 -4.1840868 -4.189918 -4.1976643 -4.2035575 -4.2069354 -4.2126389]]...]
INFO - root - 2017-12-07 12:18:09.589342: step 7910, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 61h:26m:35s remains)
INFO - root - 2017-12-07 12:18:16.109626: step 7920, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 57h:35m:01s remains)
INFO - root - 2017-12-07 12:18:22.843856: step 7930, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 58h:38m:32s remains)
INFO - root - 2017-12-07 12:18:29.584703: step 7940, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 63h:01m:26s remains)
INFO - root - 2017-12-07 12:18:36.468096: step 7950, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 62h:01m:59s remains)
INFO - root - 2017-12-07 12:18:43.214291: step 7960, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.681 sec/batch; 61h:22m:22s remains)
INFO - root - 2017-12-07 12:18:49.949340: step 7970, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 58h:34m:21s remains)
INFO - root - 2017-12-07 12:18:56.763146: step 7980, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.643 sec/batch; 57h:58m:09s remains)
INFO - root - 2017-12-07 12:19:03.444224: step 7990, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 62h:00m:14s remains)
INFO - root - 2017-12-07 12:19:10.300073: step 8000, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 62h:07m:05s remains)
2017-12-07 12:19:11.005843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2196627 -4.2117572 -4.2146974 -4.2208676 -4.2162066 -4.2116952 -4.2168555 -4.2246966 -4.2320509 -4.2443433 -4.2627897 -4.2731838 -4.2707882 -4.2623553 -4.2568655][-4.2101407 -4.197535 -4.1987219 -4.1994047 -4.1897573 -4.1829624 -4.1883287 -4.2001152 -4.2129889 -4.2315621 -4.2580757 -4.2744675 -4.2737632 -4.2638569 -4.2549543][-4.2001696 -4.1842828 -4.1803393 -4.174046 -4.1571178 -4.1420369 -4.1391487 -4.1523972 -4.177711 -4.2111211 -4.248642 -4.2722507 -4.2763019 -4.2699862 -4.2602196][-4.1890221 -4.1721258 -4.1635375 -4.1500978 -4.1246057 -4.0963244 -4.0794821 -4.0899353 -4.1278377 -4.1789966 -4.2261562 -4.2565269 -4.2689781 -4.2702694 -4.2648411][-4.180335 -4.1657448 -4.1538887 -4.1345978 -4.1040668 -4.0641856 -4.0317349 -4.034306 -4.0803013 -4.1450849 -4.1983747 -4.2339115 -4.2548213 -4.2635665 -4.2630959][-4.1752043 -4.1617689 -4.1457238 -4.1233349 -4.090301 -4.0401015 -3.9896274 -3.9799664 -4.031621 -4.1073451 -4.1659503 -4.2045512 -4.2320976 -4.2471776 -4.251411][-4.1764336 -4.1600428 -4.1353893 -4.1037469 -4.0614238 -3.9964058 -3.9235442 -3.8985391 -3.9582796 -4.0502691 -4.1196308 -4.1663065 -4.2039351 -4.2287927 -4.2413712][-4.1782994 -4.1583161 -4.1267977 -4.0844088 -4.0336423 -3.9607463 -3.8762248 -3.8414316 -3.904887 -4.0010428 -4.075501 -4.12855 -4.1757197 -4.2130041 -4.2373271][-4.177866 -4.1586237 -4.1285157 -4.08584 -4.0411649 -3.9852049 -3.9233549 -3.8966331 -3.9358492 -4.0026441 -4.0589309 -4.1014581 -4.1439452 -4.1835823 -4.2166181][-4.1725793 -4.1529889 -4.1261344 -4.0889492 -4.0555115 -4.0265813 -3.9959798 -3.9800642 -3.9969025 -4.0320234 -4.0644174 -4.0892663 -4.1172595 -4.1512365 -4.1874833][-4.16726 -4.1420574 -4.1137094 -4.0801797 -4.0583448 -4.0535097 -4.0504141 -4.0466118 -4.0537128 -4.0722051 -4.0914321 -4.1026196 -4.1179876 -4.1468925 -4.1784592][-4.1668754 -4.1362419 -4.1035032 -4.0704455 -4.0594211 -4.0753098 -4.0918345 -4.0949893 -4.096983 -4.1079335 -4.120369 -4.1239028 -4.1329103 -4.1550674 -4.1760015][-4.1776443 -4.1464987 -4.113131 -4.0842409 -4.0840492 -4.1147413 -4.1361275 -4.1354527 -4.1284475 -4.1307449 -4.1392088 -4.1402283 -4.1456609 -4.1601238 -4.1706986][-4.2064266 -4.1785145 -4.1473989 -4.1236267 -4.1318631 -4.1652436 -4.1794634 -4.1703281 -4.1524162 -4.1462011 -4.1524453 -4.1576285 -4.1661611 -4.1756897 -4.1750755][-4.2385521 -4.218328 -4.1931014 -4.1766319 -4.1883726 -4.2156329 -4.2186961 -4.1985011 -4.1684837 -4.1554093 -4.1629896 -4.1748714 -4.1901569 -4.2001624 -4.1924682]]...]
INFO - root - 2017-12-07 12:19:17.796866: step 8010, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:41m:21s remains)
INFO - root - 2017-12-07 12:19:24.494549: step 8020, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 66h:16m:13s remains)
INFO - root - 2017-12-07 12:19:31.243480: step 8030, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 62h:56m:53s remains)
INFO - root - 2017-12-07 12:19:37.993527: step 8040, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 60h:45m:10s remains)
INFO - root - 2017-12-07 12:19:44.837634: step 8050, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 60h:58m:42s remains)
INFO - root - 2017-12-07 12:19:51.558130: step 8060, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 63h:29m:14s remains)
INFO - root - 2017-12-07 12:19:58.418458: step 8070, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 64h:10m:28s remains)
INFO - root - 2017-12-07 12:20:05.295694: step 8080, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:18m:06s remains)
INFO - root - 2017-12-07 12:20:12.011491: step 8090, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 56h:56m:11s remains)
INFO - root - 2017-12-07 12:20:18.831644: step 8100, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 61h:54m:14s remains)
2017-12-07 12:20:19.600766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18326 -4.1501331 -4.1372347 -4.1429625 -4.1536 -4.1507792 -4.1396289 -4.1465883 -4.1526766 -4.1484213 -4.1547937 -4.1724925 -4.1966677 -4.2223377 -4.2407165][-4.1774864 -4.146771 -4.1271706 -4.1250615 -4.1291876 -4.1182594 -4.1050591 -4.1221852 -4.1406455 -4.1488481 -4.1620917 -4.1745133 -4.1917853 -4.216979 -4.2378473][-4.1729259 -4.1495934 -4.1242604 -4.111331 -4.1057744 -4.0889907 -4.0773654 -4.1054926 -4.1338773 -4.1530952 -4.1750793 -4.1840949 -4.1927147 -4.2152815 -4.2363439][-4.1731625 -4.1588407 -4.1301 -4.1021481 -4.0788536 -4.0511456 -4.0406194 -4.0829167 -4.1228971 -4.15296 -4.1840024 -4.1935277 -4.1959753 -4.2149696 -4.234787][-4.1696897 -4.1633949 -4.1369519 -4.0964546 -4.051075 -4.00094 -3.9820845 -4.0399065 -4.0993004 -4.1428156 -4.1818571 -4.1955214 -4.1967068 -4.215302 -4.2350411][-4.1595454 -4.1569204 -4.134594 -4.0860977 -4.0211315 -3.9414783 -3.9024282 -3.9718454 -4.0574932 -4.1189685 -4.1659193 -4.186574 -4.1944284 -4.2176423 -4.2380877][-4.1543775 -4.1488905 -4.1274505 -4.0779152 -4.005568 -3.906183 -3.84469 -3.9191642 -4.0240526 -4.0970578 -4.146853 -4.1727667 -4.1904407 -4.2229877 -4.2459397][-4.1657376 -4.148272 -4.12226 -4.078301 -4.0196819 -3.9305348 -3.8679481 -3.9305983 -4.0290813 -4.0957279 -4.1386528 -4.1666837 -4.1936369 -4.2343607 -4.2582088][-4.1860437 -4.1552687 -4.1238813 -4.0906596 -4.0579758 -4.0021229 -3.9610534 -4.0070939 -4.0777316 -4.1229057 -4.1533265 -4.1788397 -4.210361 -4.252521 -4.2733755][-4.2048492 -4.168632 -4.1351519 -4.1125031 -4.1040282 -4.0810065 -4.0651331 -4.0997796 -4.1431184 -4.1658049 -4.182857 -4.2044234 -4.2350211 -4.272903 -4.2878413][-4.2093606 -4.1731467 -4.1401486 -4.1280837 -4.1407623 -4.14414 -4.1458178 -4.1701651 -4.1920972 -4.1990123 -4.2057843 -4.2215819 -4.2481413 -4.2815466 -4.2924557][-4.2055693 -4.1731215 -4.1438441 -4.139411 -4.164422 -4.1827569 -4.1920519 -4.2065978 -4.2158566 -4.2142687 -4.2148089 -4.2255468 -4.2487965 -4.2791495 -4.286901][-4.2015467 -4.1775393 -4.1571636 -4.1567159 -4.1836295 -4.2059932 -4.2145028 -4.2204804 -4.2237105 -4.2198029 -4.2178774 -4.2261586 -4.2466063 -4.2713675 -4.2748132][-4.2085614 -4.1961904 -4.18622 -4.187737 -4.2113948 -4.2319489 -4.2366147 -4.2352428 -4.2317629 -4.2243195 -4.2228022 -4.2317867 -4.2486687 -4.2640495 -4.2608843][-4.2251596 -4.2233558 -4.2203722 -4.219161 -4.2358451 -4.2517424 -4.2532516 -4.2477088 -4.2394919 -4.2310519 -4.2318134 -4.2415228 -4.2520585 -4.2578 -4.2485132]]...]
INFO - root - 2017-12-07 12:20:26.323900: step 8110, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.664 sec/batch; 59h:52m:23s remains)
INFO - root - 2017-12-07 12:20:32.997102: step 8120, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:57m:39s remains)
INFO - root - 2017-12-07 12:20:39.875286: step 8130, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:51m:38s remains)
INFO - root - 2017-12-07 12:20:46.685167: step 8140, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.723 sec/batch; 65h:06m:00s remains)
INFO - root - 2017-12-07 12:20:53.413649: step 8150, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:17m:17s remains)
INFO - root - 2017-12-07 12:21:00.269923: step 8160, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 61h:02m:53s remains)
INFO - root - 2017-12-07 12:21:06.965048: step 8170, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 56h:48m:48s remains)
INFO - root - 2017-12-07 12:21:13.900056: step 8180, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 60h:41m:33s remains)
INFO - root - 2017-12-07 12:21:20.820110: step 8190, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 65h:45m:09s remains)
INFO - root - 2017-12-07 12:21:27.551660: step 8200, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 62h:36m:12s remains)
2017-12-07 12:21:28.229083: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25372 -4.2529697 -4.2571898 -4.2617044 -4.2597284 -4.2513332 -4.2515178 -4.2606578 -4.2696042 -4.2726574 -4.270781 -4.2679362 -4.2682691 -4.2691126 -4.2673583][-4.2233081 -4.2197428 -4.2281966 -4.2361159 -4.23185 -4.217165 -4.2094665 -4.2122335 -4.224381 -4.2307076 -4.2273746 -4.2243476 -4.229094 -4.2344246 -4.2318783][-4.2075839 -4.198144 -4.2026472 -4.2083888 -4.2013345 -4.1778927 -4.1582112 -4.1552715 -4.1735268 -4.18519 -4.1841269 -4.1848741 -4.190578 -4.19552 -4.191102][-4.2106543 -4.1991396 -4.1944113 -4.191412 -4.1795511 -4.152884 -4.1246557 -4.1223116 -4.1488013 -4.1664815 -4.1651525 -4.167316 -4.169497 -4.1686106 -4.1630287][-4.1972551 -4.1849751 -4.1780348 -4.1655741 -4.14901 -4.1226063 -4.0950084 -4.0948019 -4.1241589 -4.1442475 -4.150054 -4.1584144 -4.161098 -4.156404 -4.1538391][-4.1631379 -4.1457038 -4.1386328 -4.1252747 -4.1021924 -4.0631928 -4.0311251 -4.0417776 -4.0744925 -4.0983582 -4.120851 -4.1449976 -4.1553087 -4.1529984 -4.1549449][-4.1146483 -4.0981975 -4.0981469 -4.0897503 -4.0530729 -3.9847314 -3.9349992 -3.9517586 -3.9947596 -4.0335827 -4.0799913 -4.1280551 -4.1530876 -4.1556668 -4.1607447][-4.0655694 -4.0567374 -4.0769777 -4.0833168 -4.0461183 -3.9683645 -3.9083891 -3.9213107 -3.9744306 -4.0297704 -4.0890675 -4.1413836 -4.1658268 -4.1646667 -4.1675444][-4.0453291 -4.0491571 -4.087872 -4.1116824 -4.0927105 -4.0342116 -3.9817309 -3.9795282 -4.0256829 -4.0844364 -4.1374855 -4.1723104 -4.1833506 -4.1732969 -4.1703024][-4.0778823 -4.0873613 -4.1306944 -4.1583557 -4.151536 -4.1112833 -4.0668659 -4.0543947 -4.0851221 -4.1333404 -4.1733265 -4.1977015 -4.2044868 -4.1932554 -4.1879649][-4.1570234 -4.1645455 -4.1966367 -4.21685 -4.2113461 -4.1792674 -4.1435823 -4.1253972 -4.138299 -4.1709528 -4.2022181 -4.2224393 -4.232769 -4.226747 -4.2228312][-4.2141538 -4.2172346 -4.2384133 -4.25183 -4.2485909 -4.2280555 -4.2085056 -4.2025595 -4.2110677 -4.2242284 -4.2368164 -4.2466955 -4.253089 -4.2510533 -4.2507377][-4.24018 -4.2470427 -4.2611189 -4.2678404 -4.2639446 -4.251956 -4.2439632 -4.2477374 -4.2535644 -4.2559376 -4.2593751 -4.2598376 -4.2619381 -4.2623124 -4.2640734][-4.2652097 -4.2754483 -4.2844877 -4.287025 -4.2782345 -4.2699065 -4.2669463 -4.2703862 -4.2724957 -4.2728758 -4.2754645 -4.2769566 -4.2809248 -4.2838745 -4.28601][-4.2959952 -4.3019948 -4.3039389 -4.3012457 -4.2921591 -4.2856903 -4.2852473 -4.2903237 -4.2958527 -4.2981949 -4.2986693 -4.3034773 -4.3090968 -4.3099222 -4.3087935]]...]
INFO - root - 2017-12-07 12:21:35.032626: step 8210, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 61h:20m:40s remains)
INFO - root - 2017-12-07 12:21:41.642834: step 8220, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 62h:38m:30s remains)
INFO - root - 2017-12-07 12:21:48.464072: step 8230, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 62h:42m:35s remains)
INFO - root - 2017-12-07 12:21:55.206797: step 8240, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 56h:49m:31s remains)
INFO - root - 2017-12-07 12:22:02.030432: step 8250, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 58h:09m:15s remains)
INFO - root - 2017-12-07 12:22:08.865293: step 8260, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 63h:55m:37s remains)
INFO - root - 2017-12-07 12:22:15.649293: step 8270, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 62h:56m:08s remains)
INFO - root - 2017-12-07 12:22:22.524118: step 8280, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 62h:37m:01s remains)
INFO - root - 2017-12-07 12:22:29.321170: step 8290, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:17m:20s remains)
INFO - root - 2017-12-07 12:22:36.003311: step 8300, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:20m:44s remains)
2017-12-07 12:22:36.707381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3634815 -4.3613534 -4.3575015 -4.3533816 -4.3501534 -4.3484278 -4.3475938 -4.3456345 -4.3424311 -4.3417363 -4.3438091 -4.3457284 -4.3473454 -4.349874 -4.3544841][-4.3574443 -4.3552079 -4.3509932 -4.345685 -4.3406034 -4.3371243 -4.3355951 -4.3330231 -4.32931 -4.3294144 -4.3324056 -4.334271 -4.3346162 -4.3358746 -4.3398361][-4.3505449 -4.3469071 -4.3407874 -4.3333693 -4.3264456 -4.3215194 -4.3180294 -4.312212 -4.3063436 -4.3061628 -4.3099365 -4.311574 -4.3105903 -4.3104267 -4.3133311][-4.3435593 -4.336875 -4.3266048 -4.3142409 -4.3027854 -4.2931085 -4.2825465 -4.2679276 -4.2568603 -4.25758 -4.2648311 -4.2667713 -4.2647128 -4.265172 -4.2702446][-4.328001 -4.3146639 -4.2966971 -4.2781496 -4.2607012 -4.2432652 -4.222178 -4.1925473 -4.1715741 -4.174932 -4.191452 -4.1982088 -4.1977863 -4.202826 -4.2150912][-4.3000841 -4.2770996 -4.2493587 -4.2248917 -4.2014136 -4.1741629 -4.1401224 -4.0924649 -4.0571661 -4.0685086 -4.1056972 -4.1255546 -4.1299748 -4.141118 -4.16195][-4.2681861 -4.2332077 -4.197094 -4.1697054 -4.143362 -4.1062069 -4.0568967 -3.9865985 -3.9302964 -3.95961 -4.0341682 -4.0774126 -4.0880308 -4.102098 -4.1276178][-4.2449174 -4.2042384 -4.16609 -4.1394863 -4.10695 -4.0536251 -3.9867523 -3.8962855 -3.82718 -3.8830311 -3.9961333 -4.06164 -4.0756526 -4.087709 -4.1121273][-4.2462206 -4.2114935 -4.17874 -4.154047 -4.1146789 -4.0501361 -3.9786005 -3.8962088 -3.8410976 -3.9065762 -4.0238485 -4.0921974 -4.103755 -4.1114526 -4.128581][-4.2644215 -4.2387018 -4.2103057 -4.183917 -4.1430969 -4.0855508 -4.0296135 -3.9754224 -3.9436555 -3.9968848 -4.0892811 -4.1465216 -4.1567097 -4.1629481 -4.1713929][-4.2755384 -4.2545085 -4.2309909 -4.2076278 -4.1755381 -4.1354051 -4.0986714 -4.067934 -4.0495892 -4.08072 -4.1407666 -4.1857157 -4.2013354 -4.2108512 -4.2153893][-4.2923779 -4.2767682 -4.2589388 -4.2428026 -4.223362 -4.1998038 -4.1793504 -4.1643472 -4.151206 -4.1591611 -4.1910491 -4.2241955 -4.2440796 -4.2577324 -4.2603388][-4.314333 -4.3059669 -4.2950997 -4.2879047 -4.2802129 -4.269135 -4.2599945 -4.2544012 -4.2441306 -4.2389855 -4.2508 -4.2700438 -4.2876511 -4.3012609 -4.3042121][-4.334774 -4.333178 -4.3293986 -4.3299909 -4.3303318 -4.3283467 -4.3276424 -4.3279643 -4.3196349 -4.3082228 -4.3047032 -4.3097529 -4.3203077 -4.3329415 -4.3378835][-4.3522243 -4.3530407 -4.3507051 -4.3515325 -4.3538394 -4.355957 -4.3590059 -4.3626103 -4.3578291 -4.34795 -4.3398962 -4.3367462 -4.3407965 -4.3498335 -4.3558364]]...]
INFO - root - 2017-12-07 12:22:43.465964: step 8310, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 57h:54m:07s remains)
INFO - root - 2017-12-07 12:22:50.041956: step 8320, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:09m:04s remains)
INFO - root - 2017-12-07 12:22:56.796444: step 8330, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 61h:42m:47s remains)
INFO - root - 2017-12-07 12:23:03.585132: step 8340, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 62h:48m:32s remains)
INFO - root - 2017-12-07 12:23:10.384599: step 8350, loss = 2.11, batch loss = 2.05 (11.6 examples/sec; 0.690 sec/batch; 62h:06m:36s remains)
INFO - root - 2017-12-07 12:23:17.111361: step 8360, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:34m:44s remains)
INFO - root - 2017-12-07 12:23:23.984458: step 8370, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 60h:10m:49s remains)
INFO - root - 2017-12-07 12:23:30.824727: step 8380, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 64h:41m:08s remains)
INFO - root - 2017-12-07 12:23:37.560169: step 8390, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 63h:46m:20s remains)
INFO - root - 2017-12-07 12:23:44.371490: step 8400, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 60h:56m:16s remains)
2017-12-07 12:23:45.030888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3627481 -4.3775268 -4.3742886 -4.3334017 -4.2583971 -4.181262 -4.1363626 -4.1337667 -4.1686058 -4.2134943 -4.2575154 -4.2857633 -4.2999873 -4.3000708 -4.2956877][-4.37413 -4.3914862 -4.3948231 -4.3658361 -4.3044424 -4.2314377 -4.1752887 -4.1496534 -4.1597252 -4.1889162 -4.2293777 -4.2659254 -4.2935109 -4.305541 -4.3083][-4.3851023 -4.403687 -4.408267 -4.3845606 -4.3293543 -4.2571211 -4.1901889 -4.14679 -4.1372147 -4.1560755 -4.198092 -4.2433062 -4.285902 -4.3130889 -4.3260803][-4.3915458 -4.4097652 -4.4146194 -4.3928046 -4.3385763 -4.2633748 -4.1875877 -4.1291933 -4.1030612 -4.1169319 -4.1624718 -4.2152023 -4.2731724 -4.3172193 -4.3426862][-4.3947268 -4.4102974 -4.4119587 -4.3894758 -4.3333206 -4.2539706 -4.1694379 -4.0960684 -4.0527573 -4.0632787 -4.1164169 -4.1775346 -4.2473335 -4.3057957 -4.3435497][-4.3960686 -4.40741 -4.4013247 -4.37014 -4.3046956 -4.2166381 -4.1194916 -4.0265532 -3.9654729 -3.98364 -4.0547471 -4.1304731 -4.2119646 -4.2837038 -4.3325443][-4.3926044 -4.3980021 -4.380466 -4.3330245 -4.2527928 -4.15413 -4.0445824 -3.9326773 -3.859453 -3.8953762 -3.9897163 -4.08059 -4.1705089 -4.2523532 -4.3132443][-4.3844805 -4.381628 -4.3518634 -4.2883272 -4.195962 -4.0956039 -3.9870667 -3.8738146 -3.806006 -3.8574424 -3.9596555 -4.053854 -4.144125 -4.2296019 -4.2963347][-4.3731947 -4.3618288 -4.3240209 -4.2515492 -4.1570473 -4.0663238 -3.9758289 -3.8872514 -3.8472509 -3.9001179 -3.9856315 -4.0671558 -4.14976 -4.2312026 -4.2953782][-4.3620882 -4.3458395 -4.3058724 -4.2349377 -4.148046 -4.0734086 -4.0100522 -3.9589076 -3.949384 -3.9962597 -4.0594106 -4.1239867 -4.1944814 -4.263938 -4.3142042][-4.3557162 -4.340446 -4.3066549 -4.24723 -4.1758213 -4.1191454 -4.0808735 -4.0627112 -4.072648 -4.1079993 -4.1511884 -4.1998806 -4.2552977 -4.3079395 -4.338912][-4.3545666 -4.3446908 -4.3228683 -4.2819548 -4.2308259 -4.191771 -4.1734958 -4.1751285 -4.191082 -4.2145185 -4.2413769 -4.2740335 -4.3110476 -4.3428993 -4.3521652][-4.3547435 -4.3511915 -4.3411818 -4.3192129 -4.2891746 -4.2668128 -4.2609024 -4.2689524 -4.2841682 -4.30003 -4.3168736 -4.3355036 -4.3523498 -4.3618608 -4.3528776][-4.3545156 -4.354948 -4.3528886 -4.345068 -4.3326564 -4.3235269 -4.324039 -4.3322787 -4.3435688 -4.3537397 -4.363133 -4.3696847 -4.3693266 -4.3618512 -4.3411164][-4.3538127 -4.3556623 -4.3564329 -4.3560472 -4.3543634 -4.3538122 -4.3572407 -4.3630409 -4.3690262 -4.3733087 -4.375011 -4.3702745 -4.3585224 -4.3407631 -4.3137083]]...]
INFO - root - 2017-12-07 12:23:51.833212: step 8410, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 64h:13m:26s remains)
INFO - root - 2017-12-07 12:23:58.561858: step 8420, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 64h:01m:30s remains)
INFO - root - 2017-12-07 12:24:05.378543: step 8430, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 59h:01m:54s remains)
INFO - root - 2017-12-07 12:24:12.300185: step 8440, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 60h:15m:29s remains)
INFO - root - 2017-12-07 12:24:19.119473: step 8450, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.699 sec/batch; 62h:53m:27s remains)
INFO - root - 2017-12-07 12:24:25.995615: step 8460, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 65h:00m:20s remains)
INFO - root - 2017-12-07 12:24:32.784870: step 8470, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 61h:47m:17s remains)
INFO - root - 2017-12-07 12:24:39.607671: step 8480, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 56h:57m:19s remains)
INFO - root - 2017-12-07 12:24:46.293849: step 8490, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 58h:23m:19s remains)
INFO - root - 2017-12-07 12:24:53.082263: step 8500, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 62h:46m:28s remains)
2017-12-07 12:24:53.839427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281393 -4.3285351 -4.3246884 -4.3221931 -4.3220782 -4.3232059 -4.3236141 -4.3227148 -4.3211188 -4.320744 -4.3218803 -4.3234425 -4.325191 -4.3274212 -4.3277497][-4.3273993 -4.3271742 -4.3227663 -4.3202453 -4.3205848 -4.3216615 -4.3215828 -4.3207259 -4.3194408 -4.3187947 -4.3197551 -4.3217196 -4.3246613 -4.32811 -4.329186][-4.3144426 -4.3106918 -4.3028383 -4.2991161 -4.2996612 -4.3011489 -4.30144 -4.3018813 -4.302043 -4.3029623 -4.3060808 -4.3108954 -4.31697 -4.3227715 -4.3256226][-4.2979064 -4.2871361 -4.271791 -4.2619014 -4.2589273 -4.2580204 -4.2563057 -4.2558656 -4.2567029 -4.2604232 -4.2689843 -4.2805586 -4.291698 -4.3002658 -4.3043218][-4.2738786 -4.254837 -4.2293305 -4.20902 -4.1976209 -4.190856 -4.1845517 -4.1805315 -4.1801882 -4.186904 -4.2042642 -4.2262135 -4.2437124 -4.2541375 -4.2578421][-4.2513704 -4.2274556 -4.1927891 -4.1595359 -4.1355453 -4.1179876 -4.1013265 -4.0883174 -4.0834045 -4.0909424 -4.1165 -4.1493726 -4.1732664 -4.18505 -4.1887488][-4.2443347 -4.2255316 -4.1893334 -4.148798 -4.1163197 -4.0895157 -4.0608807 -4.0357747 -4.0220885 -4.024405 -4.0494509 -4.0849915 -4.1088052 -4.1182327 -4.1208138][-4.2240996 -4.2132349 -4.1837339 -4.1496954 -4.1249461 -4.1083417 -4.0901108 -4.0738239 -4.0644841 -4.0639157 -4.0794749 -4.104629 -4.1190362 -4.1217794 -4.1214285][-4.212707 -4.2063794 -4.1840463 -4.1606741 -4.14846 -4.1475143 -4.147994 -4.1485353 -4.1500931 -4.152658 -4.1615424 -4.1760926 -4.1831126 -4.1826029 -4.181993][-4.2408791 -4.2396736 -4.2267632 -4.2129741 -4.2071996 -4.210218 -4.2170229 -4.2250533 -4.2325311 -4.2379932 -4.2428551 -4.24927 -4.2519083 -4.2507343 -4.2501631][-4.2800536 -4.2815776 -4.2754989 -4.2690878 -4.2668724 -4.2701349 -4.2771645 -4.2855425 -4.2938261 -4.3003774 -4.3040109 -4.3066907 -4.30787 -4.3075862 -4.3077435][-4.3150353 -4.3160987 -4.3131065 -4.3098598 -4.3074818 -4.3078251 -4.310689 -4.3149638 -4.3202276 -4.3249793 -4.32731 -4.3287759 -4.3300614 -4.3302646 -4.3303261][-4.3299909 -4.3298292 -4.3273325 -4.324307 -4.3213544 -4.3197865 -4.3193684 -4.3202224 -4.323009 -4.3253479 -4.3255634 -4.32576 -4.3262992 -4.3263578 -4.3265929][-4.3253164 -4.3243523 -4.3222566 -4.3215885 -4.320744 -4.3192129 -4.3174367 -4.3165059 -4.3169355 -4.316484 -4.3143888 -4.3129516 -4.3118382 -4.3107238 -4.3110862][-4.3094387 -4.3087287 -4.3074841 -4.309535 -4.3115788 -4.3117948 -4.3111157 -4.3109517 -4.3109431 -4.3096547 -4.3068109 -4.3042822 -4.3016157 -4.29969 -4.3000908]]...]
INFO - root - 2017-12-07 12:25:00.661227: step 8510, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.669 sec/batch; 60h:12m:22s remains)
INFO - root - 2017-12-07 12:25:07.230787: step 8520, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 56h:31m:27s remains)
INFO - root - 2017-12-07 12:25:14.016171: step 8530, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:30m:06s remains)
INFO - root - 2017-12-07 12:25:20.880630: step 8540, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 61h:24m:58s remains)
INFO - root - 2017-12-07 12:25:27.667413: step 8550, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:48m:58s remains)
INFO - root - 2017-12-07 12:25:34.409514: step 8560, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 57h:36m:28s remains)
INFO - root - 2017-12-07 12:25:41.232469: step 8570, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:48m:51s remains)
INFO - root - 2017-12-07 12:25:48.075137: step 8580, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 63h:06m:52s remains)
INFO - root - 2017-12-07 12:25:54.995245: step 8590, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 63h:36m:37s remains)
INFO - root - 2017-12-07 12:26:01.921439: step 8600, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 63h:42m:44s remains)
2017-12-07 12:26:02.693351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3278341 -4.3290062 -4.3261361 -4.32029 -4.3145757 -4.3110032 -4.3065906 -4.3001671 -4.2937312 -4.2848148 -4.2666368 -4.2406316 -4.2110786 -4.1852446 -4.1658449][-4.33041 -4.3279772 -4.3191223 -4.3066926 -4.2924504 -4.2811003 -4.2727132 -4.2675276 -4.2687836 -4.2711143 -4.2613869 -4.2471232 -4.2336116 -4.2185407 -4.20429][-4.3318377 -4.3232994 -4.3060493 -4.2858167 -4.2627106 -4.2451315 -4.2329493 -4.2279768 -4.2362127 -4.2462649 -4.2413406 -4.2333274 -4.2359619 -4.2385345 -4.2366815][-4.3244815 -4.3109946 -4.2898073 -4.26549 -4.2394094 -4.2166309 -4.1947703 -4.1882267 -4.2035165 -4.2194171 -4.2140527 -4.2028241 -4.2144785 -4.2363997 -4.2505383][-4.3137732 -4.2962894 -4.2701211 -4.2397165 -4.2073536 -4.1686764 -4.1246123 -4.1117291 -4.1454358 -4.1781178 -4.1708827 -4.1531668 -4.1692839 -4.2122626 -4.2448812][-4.2960367 -4.2694588 -4.2324972 -4.1946754 -4.1508336 -4.0806451 -3.9944854 -3.9717953 -4.047658 -4.1132283 -4.1066689 -4.0836468 -4.1059775 -4.1728306 -4.2263656][-4.2752023 -4.2426338 -4.1970925 -4.1526208 -4.09463 -3.9861698 -3.8542962 -3.822248 -3.9401298 -4.0366554 -4.032681 -4.0162749 -4.0563807 -4.1449652 -4.215229][-4.2574129 -4.225575 -4.1778059 -4.1323543 -4.0782194 -3.9693658 -3.8412623 -3.8142817 -3.9241915 -4.0139117 -4.01355 -4.012794 -4.0638952 -4.1564593 -4.2291327][-4.2475119 -4.2179217 -4.1733909 -4.1367111 -4.1063509 -4.0411754 -3.9676654 -3.9560821 -4.0214076 -4.0761604 -4.0777583 -4.0865893 -4.1283 -4.2012234 -4.26041][-4.2472515 -4.2207541 -4.1854863 -4.1599784 -4.1533518 -4.1311951 -4.103797 -4.1049042 -4.1388474 -4.1681409 -4.1714411 -4.1818285 -4.207603 -4.2513418 -4.2894869][-4.2603283 -4.239553 -4.2144589 -4.1997786 -4.2075205 -4.2040458 -4.1979995 -4.2078838 -4.2277226 -4.2427254 -4.2465911 -4.2544332 -4.2659297 -4.2870388 -4.3077765][-4.281857 -4.2681623 -4.2521763 -4.246748 -4.2572279 -4.2576518 -4.2574706 -4.2677298 -4.2809858 -4.2912908 -4.2927027 -4.2952657 -4.2999635 -4.3102593 -4.3197813][-4.3045793 -4.2981491 -4.2910171 -4.2925692 -4.3016949 -4.3032746 -4.3024273 -4.3040762 -4.3079433 -4.3138418 -4.3129482 -4.3134618 -4.3171873 -4.3219829 -4.3250856][-4.32302 -4.3215737 -4.3195424 -4.3233514 -4.3293767 -4.3300562 -4.3273039 -4.3244891 -4.3229871 -4.3251381 -4.3241549 -4.3254104 -4.3275075 -4.3275719 -4.3275342][-4.3331971 -4.3349552 -4.3352962 -4.3376794 -4.3398938 -4.3395052 -4.3365526 -4.3321781 -4.3280931 -4.3278956 -4.327673 -4.3280058 -4.3289595 -4.3287997 -4.3294005]]...]
INFO - root - 2017-12-07 12:26:09.310646: step 8610, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 60h:53m:46s remains)
INFO - root - 2017-12-07 12:26:15.965749: step 8620, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 61h:16m:20s remains)
INFO - root - 2017-12-07 12:26:22.655767: step 8630, loss = 2.03, batch loss = 1.97 (12.8 examples/sec; 0.623 sec/batch; 56h:02m:57s remains)
INFO - root - 2017-12-07 12:26:29.547265: step 8640, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 58h:12m:41s remains)
INFO - root - 2017-12-07 12:26:36.255658: step 8650, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 62h:31m:38s remains)
INFO - root - 2017-12-07 12:26:43.112449: step 8660, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:47m:56s remains)
INFO - root - 2017-12-07 12:26:49.910655: step 8670, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 58h:28m:27s remains)
INFO - root - 2017-12-07 12:26:56.706537: step 8680, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 60h:59m:30s remains)
INFO - root - 2017-12-07 12:27:03.511340: step 8690, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:44m:16s remains)
INFO - root - 2017-12-07 12:27:10.333362: step 8700, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.683 sec/batch; 61h:26m:05s remains)
2017-12-07 12:27:11.035306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009086 -4.2967577 -4.2917504 -4.2871981 -4.2895837 -4.2952332 -4.2923265 -4.2836366 -4.2707043 -4.2522569 -4.2403917 -4.2368479 -4.2423387 -4.2496834 -4.2630053][-4.3029585 -4.2968812 -4.2931027 -4.2857633 -4.2864375 -4.2964749 -4.2932725 -4.2824297 -4.2656469 -4.2455482 -4.2376761 -4.2353368 -4.2393723 -4.2456517 -4.2576213][-4.2993774 -4.29194 -4.2879496 -4.2749534 -4.2672558 -4.2758565 -4.2743917 -4.2622242 -4.2417111 -4.2256389 -4.2260237 -4.2259707 -4.2318187 -4.2422447 -4.2561193][-4.2888103 -4.2787824 -4.2734361 -4.2580886 -4.2437267 -4.2459407 -4.2373328 -4.215322 -4.1947622 -4.1892996 -4.2016244 -4.2063303 -4.218008 -4.2326264 -4.250824][-4.2736459 -4.2636766 -4.2528138 -4.2366443 -4.2184067 -4.2057033 -4.1779127 -4.1397462 -4.1173105 -4.1270213 -4.1571312 -4.1747088 -4.1961403 -4.2183418 -4.243381][-4.2577882 -4.245822 -4.227695 -4.201077 -4.1703582 -4.1321321 -4.0746775 -4.0156326 -4.0015025 -4.0378776 -4.0915494 -4.1289535 -4.1648984 -4.1987691 -4.2328687][-4.2491155 -4.2304335 -4.2014503 -4.1631756 -4.1175938 -4.0488987 -3.9493084 -3.8570766 -3.8627303 -3.9365647 -4.0149889 -4.0752606 -4.1286497 -4.1766 -4.2188454][-4.2476645 -4.2265773 -4.1919565 -4.1489854 -4.088491 -3.9841628 -3.8397341 -3.7177253 -3.7537687 -3.8712864 -3.969903 -4.0455074 -4.110148 -4.1673765 -4.2130542][-4.2492318 -4.2357221 -4.2091856 -4.1670175 -4.1034589 -3.9919 -3.8405037 -3.730144 -3.7891157 -3.91744 -4.0067439 -4.0723743 -4.1268311 -4.1789212 -4.220675][-4.2498732 -4.24302 -4.2208009 -4.1809216 -4.1233535 -4.0294914 -3.9206152 -3.8553877 -3.9138253 -4.0204639 -4.0871363 -4.1321306 -4.1675768 -4.2059183 -4.2384467][-4.2635183 -4.2605739 -4.242322 -4.2079334 -4.16214 -4.0974393 -4.0372591 -4.0125217 -4.0585451 -4.1305771 -4.1712108 -4.1972365 -4.2188382 -4.24166 -4.263628][-4.2845178 -4.2881713 -4.2787 -4.2560492 -4.2269058 -4.1909494 -4.1661234 -4.1640859 -4.1926112 -4.2315507 -4.2515554 -4.2660227 -4.2766848 -4.2856107 -4.2983217][-4.3104267 -4.3139868 -4.3101621 -4.3006377 -4.2942157 -4.2799406 -4.2724395 -4.2760606 -4.2871909 -4.3025689 -4.3094964 -4.3127704 -4.3161325 -4.3188171 -4.3260441][-4.3268356 -4.3253269 -4.324636 -4.3259482 -4.33536 -4.3358917 -4.3364348 -4.3408694 -4.3389149 -4.3391747 -4.3365226 -4.3341675 -4.33223 -4.3310709 -4.335731][-4.3210692 -4.3166757 -4.3154902 -4.318469 -4.3339496 -4.3459649 -4.3508615 -4.3518424 -4.3448095 -4.3384123 -4.3349414 -4.3303928 -4.3256087 -4.3228006 -4.3274312]]...]
INFO - root - 2017-12-07 12:27:17.714072: step 8710, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 57h:05m:55s remains)
INFO - root - 2017-12-07 12:27:24.301194: step 8720, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 61h:49m:54s remains)
INFO - root - 2017-12-07 12:27:31.242846: step 8730, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 64h:36m:43s remains)
INFO - root - 2017-12-07 12:27:38.105397: step 8740, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:45m:52s remains)
INFO - root - 2017-12-07 12:27:44.905307: step 8750, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 58h:19m:11s remains)
INFO - root - 2017-12-07 12:27:51.773482: step 8760, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 59h:13m:16s remains)
INFO - root - 2017-12-07 12:27:58.532081: step 8770, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 62h:58m:59s remains)
INFO - root - 2017-12-07 12:28:05.226299: step 8780, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.724 sec/batch; 65h:03m:52s remains)
INFO - root - 2017-12-07 12:28:12.038467: step 8790, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.693 sec/batch; 62h:18m:10s remains)
INFO - root - 2017-12-07 12:28:18.781693: step 8800, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 56h:20m:06s remains)
2017-12-07 12:28:19.482621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1922035 -4.2109008 -4.2208438 -4.2147379 -4.1991339 -4.1821661 -4.1741614 -4.1811552 -4.2047114 -4.2339048 -4.2680206 -4.2988391 -4.3193154 -4.3304796 -4.3204293][-4.1767139 -4.2057405 -4.2266388 -4.2319193 -4.225709 -4.2102757 -4.2000632 -4.19887 -4.2130795 -4.237216 -4.2669888 -4.2952547 -4.3155007 -4.3253531 -4.313375][-4.1713047 -4.2125239 -4.2380037 -4.2460022 -4.2416725 -4.2287779 -4.217042 -4.2084084 -4.2110434 -4.2285643 -4.2519679 -4.2741184 -4.2922597 -4.3021331 -4.2920556][-4.1850119 -4.2304621 -4.2559214 -4.2593112 -4.2483282 -4.2316656 -4.2142658 -4.1974854 -4.1898837 -4.2035933 -4.2267537 -4.2457824 -4.2667127 -4.2799668 -4.2733455][-4.2161522 -4.2569733 -4.2757344 -4.2683215 -4.2431593 -4.2116184 -4.1775494 -4.1477056 -4.136723 -4.1598272 -4.1933689 -4.2222791 -4.25191 -4.268507 -4.2630067][-4.2522283 -4.2877216 -4.2973156 -4.2761626 -4.2350717 -4.1790261 -4.1121697 -4.0556965 -4.0493684 -4.1012764 -4.1613946 -4.2082758 -4.245935 -4.2632594 -4.2569394][-4.2768822 -4.3045454 -4.3061414 -4.2777004 -4.226151 -4.143856 -4.0314307 -3.9341166 -3.9406755 -4.03772 -4.1355276 -4.2014313 -4.2431955 -4.2590518 -4.2493711][-4.283175 -4.3016391 -4.2986884 -4.2721348 -4.2184839 -4.1158323 -3.9600503 -3.8268003 -3.8587713 -4.0036092 -4.1287169 -4.204761 -4.2435308 -4.2529287 -4.242013][-4.2779822 -4.288734 -4.2888417 -4.2688375 -4.2208781 -4.1130033 -3.947916 -3.8174026 -3.8653433 -4.0187588 -4.1393194 -4.2112627 -4.2414813 -4.2452297 -4.2359986][-4.2687106 -4.2744164 -4.2814636 -4.2728643 -4.2366958 -4.1452427 -4.0112629 -3.9201813 -3.9576619 -4.0709295 -4.161387 -4.2166324 -4.2367582 -4.23884 -4.2369046][-4.2533717 -4.2564883 -4.2734046 -4.2819653 -4.2653947 -4.2027736 -4.1124768 -4.0550976 -4.0694056 -4.1303606 -4.1872678 -4.2261076 -4.2366953 -4.2355776 -4.2417412][-4.2391572 -4.2416906 -4.2676516 -4.2943249 -4.2990532 -4.2643185 -4.2064638 -4.1614828 -4.1529093 -4.1791425 -4.2169785 -4.243515 -4.2445345 -4.2404809 -4.2500706][-4.2314973 -4.2369614 -4.2672205 -4.3058538 -4.3231 -4.3069749 -4.2678032 -4.2251005 -4.2044888 -4.2154031 -4.24246 -4.2595582 -4.2571964 -4.2530556 -4.2595587][-4.2333074 -4.2429 -4.2734203 -4.3119879 -4.3347087 -4.329144 -4.3009753 -4.2617984 -4.2346697 -4.235311 -4.252008 -4.2607074 -4.256516 -4.2528567 -4.25677][-4.2494612 -4.2614622 -4.2890429 -4.3206787 -4.3406796 -4.338366 -4.3135405 -4.2739182 -4.2377768 -4.227695 -4.2369366 -4.2450566 -4.2401485 -4.2349806 -4.2371578]]...]
INFO - root - 2017-12-07 12:28:26.190958: step 8810, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 64h:54m:13s remains)
INFO - root - 2017-12-07 12:28:32.968000: step 8820, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 62h:43m:01s remains)
INFO - root - 2017-12-07 12:28:39.798271: step 8830, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 60h:04m:28s remains)
INFO - root - 2017-12-07 12:28:46.628107: step 8840, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 58h:15m:44s remains)
INFO - root - 2017-12-07 12:28:53.462058: step 8850, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 61h:24m:01s remains)
INFO - root - 2017-12-07 12:29:00.352259: step 8860, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 63h:00m:22s remains)
INFO - root - 2017-12-07 12:29:07.106098: step 8870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 60h:44m:15s remains)
INFO - root - 2017-12-07 12:29:13.870366: step 8880, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 58h:10m:22s remains)
INFO - root - 2017-12-07 12:29:20.600339: step 8890, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 61h:52m:53s remains)
INFO - root - 2017-12-07 12:29:27.378594: step 8900, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 62h:54m:39s remains)
2017-12-07 12:29:28.091826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1473069 -4.1496444 -4.1740236 -4.2061763 -4.2231426 -4.225328 -4.2267928 -4.2217813 -4.2270241 -4.23853 -4.2292366 -4.1958308 -4.1529927 -4.1163416 -4.0950003][-4.0938997 -4.1231961 -4.1700072 -4.2159162 -4.2406163 -4.2422152 -4.2348928 -4.2224617 -4.2156014 -4.2164969 -4.1998076 -4.1561174 -4.1010747 -4.060514 -4.0408425][-4.0329871 -4.0974083 -4.16957 -4.2240815 -4.2444377 -4.2317624 -4.2105966 -4.1888614 -4.17089 -4.1624675 -4.1472912 -4.106545 -4.0576978 -4.0301814 -4.0251441][-4.0204892 -4.1082263 -4.1840954 -4.2281756 -4.23241 -4.1973295 -4.1525702 -4.1205325 -4.1006532 -4.097434 -4.0981464 -4.0819235 -4.0633626 -4.0601206 -4.0697117][-4.1104703 -4.18461 -4.2281537 -4.2341743 -4.2071557 -4.1425929 -4.0691624 -4.0307269 -4.0285845 -4.0475268 -4.0750937 -4.0899992 -4.102457 -4.1148162 -4.1265588][-4.196949 -4.2376919 -4.2401738 -4.2055535 -4.1477208 -4.0561662 -3.951781 -3.9184766 -3.9620776 -4.0206633 -4.07774 -4.1217127 -4.1518993 -4.1628981 -4.1605964][-4.2210784 -4.2345581 -4.2021856 -4.13228 -4.0450783 -3.9304447 -3.7973895 -3.7871928 -3.8922551 -3.9917531 -4.0714955 -4.1357346 -4.17128 -4.1697464 -4.1472187][-4.221127 -4.2107363 -4.1524434 -4.0560541 -3.9538603 -3.8417184 -3.7178507 -3.7390671 -3.8805809 -3.993819 -4.0711608 -4.1372976 -4.1706476 -4.1556139 -4.1191673][-4.2082734 -4.1848311 -4.1226063 -4.0301409 -3.9455423 -3.8716178 -3.8073354 -3.8443518 -3.9592736 -4.0455217 -4.0979724 -4.1511617 -4.1786962 -4.1605186 -4.1275115][-4.1945877 -4.1687927 -4.1103506 -4.0300479 -3.9698379 -3.9334462 -3.9142587 -3.9502914 -4.0289888 -4.0890541 -4.1234179 -4.16168 -4.1833749 -4.1702905 -4.1492939][-4.1992555 -4.1722736 -4.12064 -4.0588484 -4.0241032 -4.0158238 -4.0215206 -4.0504 -4.0993285 -4.1353559 -4.1541348 -4.1768556 -4.1917682 -4.1843228 -4.1737208][-4.2341347 -4.2134757 -4.1801519 -4.14438 -4.1269689 -4.1290636 -4.140656 -4.1554894 -4.1771197 -4.1941018 -4.2036004 -4.2156072 -4.2260718 -4.2266297 -4.2246809][-4.2791309 -4.26748 -4.2504025 -4.2344189 -4.2259874 -4.2293072 -4.2399511 -4.24567 -4.2514505 -4.2585726 -4.263999 -4.2709713 -4.277698 -4.2803016 -4.2821164][-4.3100009 -4.3024077 -4.2956519 -4.292254 -4.2913184 -4.294179 -4.3023162 -4.3057365 -4.3059382 -4.3085871 -4.3111663 -4.3137445 -4.3167496 -4.3174958 -4.3181977][-4.3264961 -4.3230476 -4.3212008 -4.3215885 -4.3210235 -4.3220739 -4.3256917 -4.326386 -4.3245797 -4.3234978 -4.3250155 -4.3261957 -4.3271656 -4.3276019 -4.3273249]]...]
INFO - root - 2017-12-07 12:29:34.721636: step 8910, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 57h:58m:17s remains)
INFO - root - 2017-12-07 12:29:41.303933: step 8920, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 62h:38m:20s remains)
INFO - root - 2017-12-07 12:29:48.106917: step 8930, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 59h:31m:57s remains)
INFO - root - 2017-12-07 12:29:54.868124: step 8940, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 58h:58m:48s remains)
INFO - root - 2017-12-07 12:30:01.646421: step 8950, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 56h:14m:58s remains)
INFO - root - 2017-12-07 12:30:08.376967: step 8960, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:28m:51s remains)
INFO - root - 2017-12-07 12:30:15.275311: step 8970, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:44m:33s remains)
INFO - root - 2017-12-07 12:30:22.162768: step 8980, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 62h:09m:22s remains)
INFO - root - 2017-12-07 12:30:28.925948: step 8990, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.621 sec/batch; 55h:48m:51s remains)
INFO - root - 2017-12-07 12:30:35.755403: step 9000, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 60h:19m:43s remains)
2017-12-07 12:30:36.522441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.151526 -4.1669884 -4.1738014 -4.2018733 -4.2349892 -4.2486625 -4.2351484 -4.212153 -4.2031131 -4.2032995 -4.2064981 -4.2014475 -4.1832829 -4.1718082 -4.1899133][-4.1885457 -4.20325 -4.21328 -4.23426 -4.2512259 -4.2455006 -4.2134061 -4.1776323 -4.164803 -4.17095 -4.1940918 -4.2099128 -4.2011552 -4.1868992 -4.2031369][-4.2154813 -4.2337356 -4.243916 -4.2521229 -4.24788 -4.2259908 -4.1862922 -4.1460567 -4.1275978 -4.1378407 -4.1815739 -4.2235603 -4.2350068 -4.225141 -4.2345858][-4.2349062 -4.2558775 -4.2589731 -4.2510934 -4.2297697 -4.1994095 -4.161087 -4.11774 -4.0921955 -4.1083827 -4.1661363 -4.2298007 -4.2661357 -4.2732129 -4.2787647][-4.2138567 -4.2442203 -4.2451739 -4.22683 -4.197135 -4.1662512 -4.1322069 -4.0841064 -4.0488038 -4.0778852 -4.150619 -4.2282581 -4.2835741 -4.3056688 -4.308269][-4.1682415 -4.2154126 -4.2232561 -4.2035756 -4.1676111 -4.132463 -4.0985594 -4.0412312 -3.9952705 -4.0417023 -4.1283855 -4.2071271 -4.26993 -4.3015738 -4.3065915][-4.1360335 -4.1997623 -4.2154593 -4.1955385 -4.1522341 -4.1130881 -4.0771723 -4.0099673 -3.9587731 -4.0138054 -4.0999093 -4.1772833 -4.2432914 -4.281291 -4.2951555][-4.127748 -4.2020431 -4.2260184 -4.2040825 -4.1579995 -4.1214108 -4.0805345 -4.0117311 -3.9758635 -4.0360632 -4.1059794 -4.1673088 -4.2195864 -4.2558632 -4.274446][-4.1345739 -4.2116528 -4.2375031 -4.2160268 -4.1756611 -4.1439047 -4.1007457 -4.04382 -4.0399885 -4.1043577 -4.1531124 -4.187819 -4.2145343 -4.2353458 -4.2456503][-4.1583171 -4.2267 -4.2497082 -4.2323465 -4.2001619 -4.174293 -4.1355042 -4.0996008 -4.1173692 -4.1683006 -4.1992679 -4.2158055 -4.2243981 -4.2283745 -4.2272749][-4.1859779 -4.2402296 -4.262681 -4.2520843 -4.2267852 -4.2020159 -4.1708555 -4.1570745 -4.1835384 -4.2164369 -4.2331576 -4.2445254 -4.2470717 -4.2421679 -4.2373013][-4.2126517 -4.2543283 -4.2749729 -4.2652884 -4.2405043 -4.2175241 -4.1983423 -4.202558 -4.2310963 -4.2525115 -4.2602448 -4.2691464 -4.2705784 -4.2654619 -4.2626042][-4.2427568 -4.2702069 -4.2828808 -4.2707167 -4.2473526 -4.2299008 -4.224628 -4.2412844 -4.2634153 -4.2763195 -4.2818885 -4.2904854 -4.2890387 -4.2833219 -4.2814918][-4.2719765 -4.2849722 -4.2884274 -4.2772112 -4.2579374 -4.2459478 -4.2489681 -4.2675571 -4.2805672 -4.283124 -4.2854757 -4.2922568 -4.2910933 -4.2886295 -4.2871561][-4.2917271 -4.2982688 -4.2983222 -4.286942 -4.2721462 -4.2630811 -4.2677145 -4.28132 -4.2872829 -4.2835507 -4.2833815 -4.28755 -4.2857623 -4.2837605 -4.2827449]]...]
INFO - root - 2017-12-07 12:30:43.184534: step 9010, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 62h:18m:01s remains)
INFO - root - 2017-12-07 12:30:49.803743: step 9020, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 60h:56m:25s remains)
INFO - root - 2017-12-07 12:30:56.605127: step 9030, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 56h:45m:36s remains)
INFO - root - 2017-12-07 12:31:03.469064: step 9040, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 61h:21m:12s remains)
INFO - root - 2017-12-07 12:31:10.318961: step 9050, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 63h:45m:12s remains)
INFO - root - 2017-12-07 12:31:17.224658: step 9060, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 60h:37m:03s remains)
INFO - root - 2017-12-07 12:31:23.992036: step 9070, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 58h:58m:36s remains)
INFO - root - 2017-12-07 12:31:30.814044: step 9080, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 59h:52m:57s remains)
INFO - root - 2017-12-07 12:31:37.651915: step 9090, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 63h:55m:26s remains)
INFO - root - 2017-12-07 12:31:44.463117: step 9100, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 63h:29m:12s remains)
2017-12-07 12:31:45.163246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2155385 -4.2273631 -4.2438445 -4.2596321 -4.2759032 -4.2853093 -4.282681 -4.2710423 -4.2608428 -4.2478375 -4.2327495 -4.2281642 -4.2264647 -4.2272944 -4.2305632][-4.196363 -4.2121854 -4.2299914 -4.2389755 -4.2478771 -4.2567182 -4.2511635 -4.2389188 -4.2314548 -4.2151523 -4.1922588 -4.1877575 -4.1912384 -4.1979814 -4.2083478][-4.1814313 -4.1971712 -4.212276 -4.2170854 -4.219656 -4.2252808 -4.2095256 -4.1938944 -4.1972427 -4.18892 -4.1657805 -4.1552458 -4.1602287 -4.1748586 -4.191905][-4.1648383 -4.1771903 -4.1904092 -4.1934605 -4.1895914 -4.1838841 -4.1464534 -4.1217017 -4.1497083 -4.1670041 -4.1572714 -4.1499367 -4.1562982 -4.1729064 -4.1930628][-4.147213 -4.1559262 -4.1655297 -4.16794 -4.155055 -4.1303611 -4.0550904 -4.0165277 -4.0833211 -4.1443939 -4.1630654 -4.1742277 -4.18831 -4.207118 -4.2277184][-4.1183739 -4.1223412 -4.1287141 -4.1313019 -4.110858 -4.0612488 -3.9374702 -3.8726656 -3.9907608 -4.1078815 -4.1649504 -4.2027025 -4.2316756 -4.2567363 -4.2749109][-4.0799475 -4.0792451 -4.091404 -4.1013041 -4.0797882 -4.0088797 -3.8477883 -3.7516031 -3.9078653 -4.0668 -4.1530819 -4.2119751 -4.256753 -4.2859807 -4.2996507][-4.0600357 -4.0538568 -4.0765381 -4.102396 -4.0942116 -4.0332355 -3.8955379 -3.8057365 -3.9302943 -4.070559 -4.1534328 -4.216876 -4.2676568 -4.2949524 -4.29701][-4.0784707 -4.0689735 -4.0986772 -4.1330366 -4.1390753 -4.1005497 -4.01167 -3.9527192 -4.0290751 -4.1282578 -4.1911125 -4.2415833 -4.2828612 -4.2975869 -4.2859588][-4.1224771 -4.1141462 -4.1408052 -4.1736479 -4.1863508 -4.1607184 -4.1028466 -4.0663695 -4.1133871 -4.1842642 -4.231792 -4.265729 -4.2899022 -4.2900791 -4.2714429][-4.173913 -4.1719737 -4.1946034 -4.2204165 -4.2313156 -4.2110353 -4.1708617 -4.1455374 -4.1752787 -4.2264814 -4.2615619 -4.282465 -4.2952809 -4.2890587 -4.2657547][-4.2085295 -4.2141914 -4.2384028 -4.2632833 -4.2744541 -4.2606168 -4.2325144 -4.2110171 -4.2259912 -4.2608433 -4.28543 -4.29669 -4.3016472 -4.292459 -4.2658682][-4.2192416 -4.2341018 -4.265584 -4.2923536 -4.3077679 -4.3037906 -4.2832737 -4.2628126 -4.266098 -4.28373 -4.2968183 -4.2998443 -4.3012328 -4.294642 -4.2747889][-4.2200933 -4.2352114 -4.2689576 -4.2944689 -4.3134151 -4.3203077 -4.3094497 -4.2922316 -4.2901683 -4.2970219 -4.2996106 -4.2975149 -4.2974815 -4.295289 -4.2859216][-4.228137 -4.2391748 -4.2699704 -4.2918425 -4.3079567 -4.3174639 -4.3127346 -4.2976661 -4.292419 -4.2967653 -4.29808 -4.2948227 -4.294 -4.29468 -4.294354]]...]
INFO - root - 2017-12-07 12:31:51.796411: step 9110, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:47m:11s remains)
INFO - root - 2017-12-07 12:31:58.395282: step 9120, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 62h:14m:45s remains)
INFO - root - 2017-12-07 12:32:05.231007: step 9130, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:39m:32s remains)
INFO - root - 2017-12-07 12:32:12.229574: step 9140, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 57h:45m:17s remains)
INFO - root - 2017-12-07 12:32:19.148049: step 9150, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 58h:44m:06s remains)
INFO - root - 2017-12-07 12:32:25.989862: step 9160, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 63h:36m:20s remains)
INFO - root - 2017-12-07 12:32:32.862706: step 9170, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 64h:20m:52s remains)
INFO - root - 2017-12-07 12:32:39.608508: step 9180, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 61h:03m:39s remains)
INFO - root - 2017-12-07 12:32:46.463995: step 9190, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 58h:05m:03s remains)
INFO - root - 2017-12-07 12:32:53.298515: step 9200, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 58h:09m:27s remains)
2017-12-07 12:32:54.020795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3124208 -4.3025236 -4.2963386 -4.2869411 -4.2716889 -4.2604294 -4.2628803 -4.2699871 -4.2744427 -4.2770486 -4.2818031 -4.2887111 -4.2906041 -4.2932143 -4.2938151][-4.3047671 -4.2930903 -4.2786269 -4.2554946 -4.228159 -4.2124338 -4.2153563 -4.2268958 -4.236711 -4.2458076 -4.2555842 -4.2646542 -4.2646904 -4.2641716 -4.2611156][-4.3045416 -4.2901349 -4.2626786 -4.2215862 -4.1814971 -4.1603675 -4.158329 -4.1724672 -4.1930881 -4.212141 -4.2276673 -4.2371335 -4.2389107 -4.2367868 -4.229959][-4.30554 -4.2847276 -4.2453532 -4.1929507 -4.1469584 -4.1200113 -4.1042767 -4.1160746 -4.1464505 -4.1761994 -4.198494 -4.2080293 -4.2094407 -4.2057638 -4.1994667][-4.3079624 -4.2813396 -4.2341919 -4.1764836 -4.1258688 -4.0806084 -4.0356455 -4.0379543 -4.0864534 -4.1352735 -4.1665092 -4.1750641 -4.1743608 -4.1699748 -4.1673851][-4.3100343 -4.2844086 -4.2334118 -4.1745076 -4.115067 -4.0452075 -3.9590065 -3.9391634 -4.0079145 -4.0817766 -4.1258125 -4.138607 -4.1409707 -4.1388474 -4.1415081][-4.3070216 -4.2830529 -4.2282863 -4.1624184 -4.0901394 -3.9931691 -3.8549545 -3.7917297 -3.8833127 -4.0011153 -4.0694723 -4.0960183 -4.1041369 -4.1059031 -4.1185431][-4.3001552 -4.2767954 -4.2202325 -4.1454158 -4.0579343 -3.9312563 -3.7465913 -3.64257 -3.7667248 -3.9303052 -4.0214262 -4.0612206 -4.0767908 -4.0858083 -4.1065016][-4.2903185 -4.2640929 -4.205812 -4.1272655 -4.0369668 -3.9137402 -3.7561984 -3.6871417 -3.8116264 -3.9578559 -4.0345545 -4.06517 -4.0740533 -4.0834641 -4.1044617][-4.2795892 -4.2566919 -4.2056088 -4.1408339 -4.0697622 -3.9794064 -3.8826857 -3.858475 -3.9429464 -4.0310545 -4.0772018 -4.0939198 -4.0948224 -4.0991 -4.117486][-4.2745113 -4.2585397 -4.2224402 -4.1823244 -4.1352005 -4.068325 -4.0035319 -3.994144 -4.0396466 -4.0820112 -4.1049976 -4.1206388 -4.1268711 -4.1313825 -4.1458125][-4.2772474 -4.2694621 -4.2520328 -4.2254972 -4.185956 -4.1321349 -4.0854621 -4.0834608 -4.1047983 -4.1211047 -4.1330938 -4.1501536 -4.1653013 -4.1736474 -4.184618][-4.2891536 -4.2858882 -4.2749038 -4.2537527 -4.2220054 -4.1860538 -4.1588054 -4.1631727 -4.1710486 -4.1703677 -4.1712656 -4.1863279 -4.206799 -4.2176013 -4.2245946][-4.3091397 -4.3076267 -4.3003306 -4.2864046 -4.2656159 -4.2450285 -4.2328949 -4.236084 -4.2358818 -4.22934 -4.2252545 -4.2352886 -4.2547636 -4.2626004 -4.2632332][-4.3290653 -4.328361 -4.3250179 -4.3171797 -4.3031635 -4.2928982 -4.2916169 -4.2977481 -4.2974029 -4.2898064 -4.2822509 -4.2851949 -4.2989197 -4.3029866 -4.2996988]]...]
INFO - root - 2017-12-07 12:33:00.768534: step 9210, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 61h:09m:07s remains)
INFO - root - 2017-12-07 12:33:07.402561: step 9220, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 58h:13m:21s remains)
INFO - root - 2017-12-07 12:33:14.072924: step 9230, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 59h:00m:57s remains)
INFO - root - 2017-12-07 12:33:20.886632: step 9240, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 63h:31m:00s remains)
INFO - root - 2017-12-07 12:33:27.703114: step 9250, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 63h:11m:37s remains)
INFO - root - 2017-12-07 12:33:34.465316: step 9260, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 58h:10m:59s remains)
INFO - root - 2017-12-07 12:33:41.272532: step 9270, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 60h:06m:12s remains)
INFO - root - 2017-12-07 12:33:48.057618: step 9280, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.698 sec/batch; 62h:39m:49s remains)
INFO - root - 2017-12-07 12:33:54.924620: step 9290, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 64h:23m:18s remains)
INFO - root - 2017-12-07 12:34:01.706765: step 9300, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 58h:39m:22s remains)
2017-12-07 12:34:02.521212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2557688 -4.25288 -4.244205 -4.2281475 -4.2051997 -4.1864338 -4.1646667 -4.1498275 -4.1350603 -4.1161547 -4.1041341 -4.1293 -4.17199 -4.1968222 -4.2105379][-4.2384887 -4.2426152 -4.239912 -4.2284188 -4.203918 -4.1829066 -4.1629353 -4.1546144 -4.1562729 -4.1462641 -4.12603 -4.1335607 -4.161428 -4.1775236 -4.1952806][-4.2143 -4.2179904 -4.2189054 -4.2149906 -4.1926942 -4.1714849 -4.156024 -4.1578574 -4.1785254 -4.1813965 -4.160387 -4.1513672 -4.1602011 -4.1640296 -4.1777296][-4.2064786 -4.2034349 -4.2002468 -4.1960163 -4.1729412 -4.1468635 -4.1324625 -4.1416984 -4.1794229 -4.1974058 -4.1840944 -4.1647706 -4.153636 -4.1443892 -4.1540356][-4.2096338 -4.1983385 -4.1876931 -4.1773276 -4.1471634 -4.1083956 -4.0843172 -4.0955257 -4.1478767 -4.1849933 -4.189198 -4.1684875 -4.1411939 -4.1196256 -4.1278958][-4.2113547 -4.1958456 -4.1787744 -4.1596937 -4.1176591 -4.0591626 -4.0109043 -4.016417 -4.086637 -4.1494141 -4.1750007 -4.1629024 -4.1331911 -4.1073141 -4.1148396][-4.1963034 -4.1788883 -4.1591315 -4.1341782 -4.0857787 -4.0154653 -3.9390388 -3.92591 -4.0075803 -4.0939212 -4.1423569 -4.1461191 -4.1239052 -4.1045327 -4.1139531][-4.1795969 -4.158834 -4.1381321 -4.1137276 -4.0713053 -4.0124636 -3.9403589 -3.9149747 -3.9829743 -4.0654421 -4.1156917 -4.1268978 -4.1126447 -4.0991707 -4.1085305][-4.1653461 -4.1396885 -4.1167197 -4.0958862 -4.0646863 -4.0324759 -3.9968548 -3.9798188 -4.0164981 -4.0694356 -4.1003456 -4.1038718 -4.0942683 -4.0865383 -4.092268][-4.1680765 -4.1402259 -4.1133852 -4.093317 -4.0687284 -4.0547266 -4.049211 -4.0469208 -4.0650697 -4.0880075 -4.095912 -4.0863962 -4.0787182 -4.0801625 -4.0849495][-4.1796365 -4.1538835 -4.1268377 -4.1071277 -4.0925932 -4.09448 -4.1069736 -4.1164961 -4.1265459 -4.1304603 -4.1216927 -4.1019697 -4.0913081 -4.0935807 -4.0937762][-4.1755743 -4.1526437 -4.1286383 -4.1139779 -4.1109738 -4.1253767 -4.1470003 -4.16151 -4.1680322 -4.1651506 -4.1507511 -4.1302862 -4.1187711 -4.1182628 -4.1141119][-4.179987 -4.1610909 -4.1416979 -4.1308656 -4.1344581 -4.1529207 -4.1762209 -4.1918797 -4.198308 -4.1960797 -4.1844921 -4.1698546 -4.1611323 -4.1599874 -4.156487][-4.2042518 -4.1923018 -4.1813359 -4.1765718 -4.1817579 -4.1968093 -4.2146268 -4.2274637 -4.2326336 -4.2309427 -4.2234168 -4.2146187 -4.2088308 -4.2077675 -4.2057085][-4.2400761 -4.2380848 -4.2354321 -4.2344141 -4.2364893 -4.2440114 -4.2555456 -4.2639141 -4.2669735 -4.2660456 -4.2630091 -4.2592831 -4.255888 -4.2547655 -4.2533884]]...]
INFO - root - 2017-12-07 12:34:09.303689: step 9310, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 62h:31m:54s remains)
INFO - root - 2017-12-07 12:34:15.877170: step 9320, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 60h:49m:20s remains)
INFO - root - 2017-12-07 12:34:22.584962: step 9330, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 55h:43m:25s remains)
INFO - root - 2017-12-07 12:34:29.368932: step 9340, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 60h:07m:38s remains)
INFO - root - 2017-12-07 12:34:36.213475: step 9350, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 60h:55m:23s remains)
INFO - root - 2017-12-07 12:34:43.013006: step 9360, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 63h:22m:37s remains)
INFO - root - 2017-12-07 12:34:49.842038: step 9370, loss = 2.09, batch loss = 2.04 (12.9 examples/sec; 0.620 sec/batch; 55h:41m:42s remains)
INFO - root - 2017-12-07 12:34:56.591702: step 9380, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 58h:23m:17s remains)
INFO - root - 2017-12-07 12:35:03.372011: step 9390, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 63h:11m:52s remains)
INFO - root - 2017-12-07 12:35:10.266258: step 9400, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 63h:44m:28s remains)
2017-12-07 12:35:10.964538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2026081 -4.2067175 -4.213892 -4.2237797 -4.2319922 -4.2363515 -4.2451997 -4.2500076 -4.2277937 -4.162961 -4.1046371 -4.1054664 -4.137259 -4.167366 -4.1895752][-4.2026935 -4.2118297 -4.2359147 -4.2532015 -4.2512832 -4.2459416 -4.2542634 -4.2661552 -4.2484856 -4.181695 -4.1175213 -4.112576 -4.1444273 -4.1704082 -4.1873736][-4.2145782 -4.2208953 -4.2478228 -4.2591019 -4.2481737 -4.235558 -4.2445574 -4.2594385 -4.2465105 -4.1800022 -4.113245 -4.1083264 -4.1423316 -4.1668472 -4.181366][-4.2383819 -4.2360296 -4.2512941 -4.2502823 -4.2367077 -4.2302547 -4.2417188 -4.2494721 -4.2329841 -4.1694126 -4.1050644 -4.1083961 -4.1459117 -4.1686473 -4.1818905][-4.2533875 -4.2461538 -4.2440486 -4.2257047 -4.2121286 -4.2145534 -4.2304893 -4.2286277 -4.2044373 -4.1427541 -4.0848212 -4.1010027 -4.1441278 -4.1654811 -4.1778979][-4.2504234 -4.2334366 -4.2063913 -4.1634493 -4.137876 -4.1416149 -4.1611395 -4.1509552 -4.1236234 -4.0705123 -4.0324383 -4.0647688 -4.1114826 -4.1337538 -4.1523538][-4.214252 -4.1856012 -4.1307559 -4.0537391 -4.0079036 -4.0078106 -4.0273242 -4.0045791 -3.97168 -3.9344335 -3.9241309 -3.9823093 -4.04815 -4.08344 -4.1200991][-4.1649523 -4.1311111 -4.0634346 -3.9680536 -3.9110284 -3.9109106 -3.9307165 -3.8978946 -3.8611808 -3.848506 -3.8592 -3.9280157 -4.0069909 -4.0523915 -4.1031966][-4.11922 -4.09242 -4.0342383 -3.950202 -3.8998637 -3.9095929 -3.935061 -3.9004974 -3.8683918 -3.8830521 -3.9088316 -3.9688323 -4.0401158 -4.08306 -4.1293077][-4.1051636 -4.0964451 -4.060267 -4.0019727 -3.9640932 -3.9744785 -3.9924827 -3.9545064 -3.9306178 -3.9633017 -4.0043125 -4.0567694 -4.1183224 -4.1598191 -4.1954279][-4.1366286 -4.138401 -4.1205144 -4.0902257 -4.0678015 -4.0692253 -4.0746374 -4.0424948 -4.0285592 -4.0646534 -4.1080356 -4.1510983 -4.1999016 -4.2360535 -4.2606091][-4.1876893 -4.1930447 -4.1904755 -4.1833024 -4.1735129 -4.1706438 -4.1689878 -4.1439385 -4.1323047 -4.1539459 -4.18204 -4.2144203 -4.2513885 -4.2794409 -4.2954283][-4.2420931 -4.2448316 -4.2474618 -4.2514615 -4.2506328 -4.2488756 -4.2467709 -4.2312331 -4.2189789 -4.2158227 -4.2207546 -4.2432189 -4.27098 -4.2911777 -4.3026967][-4.2798762 -4.2783074 -4.2798924 -4.28369 -4.2848573 -4.2858324 -4.2855353 -4.2754626 -4.2601418 -4.2362909 -4.2211509 -4.2363949 -4.2584209 -4.2766209 -4.2911472][-4.2995272 -4.294775 -4.2926159 -4.2938175 -4.2956128 -4.2970061 -4.2967296 -4.2891841 -4.2711606 -4.2315011 -4.1989803 -4.2081723 -4.2297096 -4.2528658 -4.2752895]]...]
INFO - root - 2017-12-07 12:35:17.613533: step 9410, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 57h:52m:19s remains)
INFO - root - 2017-12-07 12:35:24.273648: step 9420, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 63h:24m:03s remains)
INFO - root - 2017-12-07 12:35:31.147407: step 9430, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 66h:40m:40s remains)
INFO - root - 2017-12-07 12:35:37.891804: step 9440, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 59h:42m:21s remains)
INFO - root - 2017-12-07 12:35:44.764727: step 9450, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 58h:49m:41s remains)
INFO - root - 2017-12-07 12:35:51.555719: step 9460, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 59h:01m:06s remains)
INFO - root - 2017-12-07 12:35:58.363535: step 9470, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 63h:14m:32s remains)
INFO - root - 2017-12-07 12:36:05.271831: step 9480, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 65h:51m:34s remains)
INFO - root - 2017-12-07 12:36:12.106694: step 9490, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.669 sec/batch; 60h:04m:13s remains)
INFO - root - 2017-12-07 12:36:18.879829: step 9500, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 58h:13m:00s remains)
2017-12-07 12:36:19.573514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1842713 -4.210393 -4.2512813 -4.2670746 -4.2260184 -4.136344 -4.0486474 -4.0137362 -4.0599103 -4.1614556 -4.241168 -4.2699661 -4.2826881 -4.2900209 -4.2870269][-4.208231 -4.2308211 -4.2633734 -4.2743063 -4.2266932 -4.1352406 -4.0484118 -4.011961 -4.0542912 -4.1470528 -4.2268686 -4.2633772 -4.285418 -4.2991376 -4.297873][-4.23467 -4.2476044 -4.2644496 -4.268158 -4.2201066 -4.1328316 -4.0556774 -4.0207629 -4.053165 -4.1306939 -4.1980677 -4.2398133 -4.2735152 -4.2959232 -4.3007121][-4.2479796 -4.2493887 -4.2515135 -4.2411194 -4.189538 -4.1152611 -4.0511479 -4.0244832 -4.052299 -4.1124935 -4.1625423 -4.2059445 -4.2485857 -4.2791967 -4.2931523][-4.2317934 -4.2259407 -4.2183523 -4.1924672 -4.1331143 -4.0680637 -4.0187144 -4.0010977 -4.0252957 -4.0720878 -4.1176214 -4.1660137 -4.2136421 -4.2471795 -4.2707644][-4.2011566 -4.1881881 -4.1742606 -4.1336751 -4.0697908 -4.0151939 -3.9823189 -3.9715302 -3.9891505 -4.0248518 -4.0711684 -4.1296778 -4.1814914 -4.2156668 -4.2453752][-4.1787477 -4.1656046 -4.1506605 -4.1068163 -4.0476313 -4.0053511 -3.9902451 -3.9856112 -3.9959507 -4.0180249 -4.0658603 -4.1293325 -4.17929 -4.2110305 -4.240047][-4.1913223 -4.1776805 -4.1685672 -4.1338568 -4.0832663 -4.0443387 -4.0339918 -4.0299654 -4.0397449 -4.0586905 -4.1021166 -4.1615949 -4.2101111 -4.2359581 -4.2559485][-4.2257185 -4.21353 -4.2128038 -4.1896834 -4.1476355 -4.1122518 -4.1041293 -4.1007738 -4.1110945 -4.12681 -4.1590929 -4.2085328 -4.2512136 -4.2738481 -4.2856712][-4.2590108 -4.2530794 -4.2594628 -4.2459641 -4.2147875 -4.1854973 -4.1854219 -4.1886153 -4.1916819 -4.1983209 -4.221199 -4.254283 -4.2841396 -4.3017864 -4.309474][-4.2749348 -4.2734151 -4.2814083 -4.2745285 -4.2530017 -4.230587 -4.2357235 -4.2464557 -4.2497969 -4.2505622 -4.2612033 -4.2801118 -4.2993364 -4.3106651 -4.315742][-4.2735276 -4.2730274 -4.2801924 -4.2790456 -4.2660184 -4.2526932 -4.260736 -4.2754245 -4.2773652 -4.2746987 -4.2829847 -4.2932711 -4.3014622 -4.3071761 -4.3107376][-4.2761292 -4.2715917 -4.2754092 -4.2749496 -4.2647338 -4.2561207 -4.263278 -4.2742186 -4.2772141 -4.2764168 -4.2827878 -4.2915549 -4.298028 -4.3034344 -4.3081322][-4.2870383 -4.2825074 -4.2821198 -4.2803106 -4.2705956 -4.2629375 -4.2663894 -4.27273 -4.2770963 -4.2792449 -4.2848396 -4.2906837 -4.2965741 -4.3030949 -4.3086004][-4.2976971 -4.2947536 -4.2931504 -4.291615 -4.2852607 -4.2790761 -4.2786732 -4.2816315 -4.2861433 -4.2899585 -4.295186 -4.3002238 -4.3037748 -4.3080072 -4.312007]]...]
INFO - root - 2017-12-07 12:36:26.237812: step 9510, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 63h:21m:23s remains)
INFO - root - 2017-12-07 12:36:32.891226: step 9520, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:08m:42s remains)
INFO - root - 2017-12-07 12:36:39.663695: step 9530, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 59h:21m:00s remains)
INFO - root - 2017-12-07 12:36:46.442527: step 9540, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 60h:01m:47s remains)
INFO - root - 2017-12-07 12:36:53.285424: step 9550, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 62h:39m:22s remains)
INFO - root - 2017-12-07 12:37:00.043188: step 9560, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 57h:44m:18s remains)
INFO - root - 2017-12-07 12:37:06.911352: step 9570, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.682 sec/batch; 61h:12m:41s remains)
INFO - root - 2017-12-07 12:37:13.655031: step 9580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 62h:23m:38s remains)
INFO - root - 2017-12-07 12:37:20.522838: step 9590, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 63h:21m:53s remains)
INFO - root - 2017-12-07 12:37:27.385334: step 9600, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 60h:18m:57s remains)
2017-12-07 12:37:28.066894: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2890973 -4.2905874 -4.2953835 -4.2977533 -4.2888417 -4.2674775 -4.2343621 -4.2005405 -4.18393 -4.1741691 -4.1603613 -4.1397758 -4.1265335 -4.1028247 -4.0606227][-4.264523 -4.2688661 -4.2779317 -4.2853527 -4.2785721 -4.2552128 -4.2187138 -4.1866589 -4.1785412 -4.1765027 -4.1665049 -4.1491451 -4.1447892 -4.1255603 -4.0838633][-4.2300115 -4.2395849 -4.2529635 -4.268393 -4.2678714 -4.2460685 -4.2128744 -4.1903 -4.1950316 -4.2022905 -4.1966066 -4.1856923 -4.1916747 -4.1843 -4.1533213][-4.1760273 -4.1902089 -4.2078443 -4.2343125 -4.2440939 -4.2293081 -4.2053618 -4.1953497 -4.2100081 -4.2256923 -4.2250495 -4.2215228 -4.2360883 -4.2411351 -4.2237525][-4.1161966 -4.1302547 -4.149538 -4.18681 -4.2097306 -4.203938 -4.1899023 -4.1868048 -4.2031493 -4.2249107 -4.2330775 -4.240654 -4.2625852 -4.2756205 -4.267159][-4.0796037 -4.0919261 -4.1085691 -4.14265 -4.1710296 -4.1719503 -4.1616135 -4.1597242 -4.1735907 -4.1998472 -4.2183352 -4.2369485 -4.269918 -4.2908254 -4.290338][-4.0864897 -4.0965495 -4.1011157 -4.1175613 -4.135994 -4.134891 -4.125535 -4.1217189 -4.1299758 -4.1584659 -4.1900272 -4.2199917 -4.2611947 -4.2862921 -4.29353][-4.1156788 -4.1205797 -4.109992 -4.1008005 -4.0945587 -4.0757694 -4.0559125 -4.0453176 -4.0506864 -4.0910954 -4.1409645 -4.1837535 -4.2361917 -4.2702551 -4.2858448][-4.1405482 -4.1348615 -4.1137242 -4.08743 -4.0573082 -4.0162535 -3.9854136 -3.9745302 -3.9786375 -4.0276151 -4.0981359 -4.1506252 -4.2063131 -4.2452512 -4.2682281][-4.1703978 -4.1581721 -4.1324515 -4.0982108 -4.0592351 -4.0162306 -3.9919071 -3.9944086 -4.0004959 -4.0443969 -4.1179643 -4.166615 -4.2062931 -4.2353559 -4.256331][-4.193006 -4.1781759 -4.1537976 -4.123414 -4.0939455 -4.0673671 -4.0581093 -4.069633 -4.0771623 -4.107872 -4.1669035 -4.205646 -4.2272015 -4.2425981 -4.2556806][-4.2139821 -4.1950197 -4.17553 -4.1574311 -4.1439877 -4.1363096 -4.139915 -4.1528096 -4.161387 -4.1798077 -4.2227015 -4.2509775 -4.2594733 -4.2628684 -4.2659073][-4.2475734 -4.2250843 -4.2123008 -4.2099853 -4.2111568 -4.2179365 -4.2323194 -4.2454715 -4.2496858 -4.256587 -4.27886 -4.2926884 -4.2918539 -4.2872033 -4.27985][-4.2899017 -4.2692122 -4.263216 -4.2709522 -4.2805352 -4.2938075 -4.3085623 -4.3170447 -4.3167486 -4.3162737 -4.3234315 -4.325274 -4.3183603 -4.3081918 -4.2926245][-4.3206778 -4.3062372 -4.3046608 -4.3176103 -4.329308 -4.3422642 -4.3495827 -4.3468451 -4.3392963 -4.3326535 -4.3312383 -4.3275771 -4.3193216 -4.3072586 -4.2875533]]...]
INFO - root - 2017-12-07 12:37:34.807778: step 9610, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:18m:41s remains)
INFO - root - 2017-12-07 12:37:41.481911: step 9620, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:03m:48s remains)
INFO - root - 2017-12-07 12:37:48.251540: step 9630, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:28m:11s remains)
INFO - root - 2017-12-07 12:37:55.036055: step 9640, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 59h:09m:33s remains)
INFO - root - 2017-12-07 12:38:01.887362: step 9650, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 65h:00m:49s remains)
INFO - root - 2017-12-07 12:38:08.687619: step 9660, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 64h:17m:18s remains)
INFO - root - 2017-12-07 12:38:15.535762: step 9670, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 60h:06m:31s remains)
INFO - root - 2017-12-07 12:38:22.305183: step 9680, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 60h:35m:35s remains)
INFO - root - 2017-12-07 12:38:29.098171: step 9690, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 59h:18m:49s remains)
INFO - root - 2017-12-07 12:38:35.785840: step 9700, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 60h:35m:26s remains)
2017-12-07 12:38:36.514131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2706213 -4.2860155 -4.2849317 -4.2793326 -4.2784162 -4.2889543 -4.299057 -4.2926207 -4.284091 -4.2821555 -4.2888274 -4.3012719 -4.3141665 -4.3165812 -4.3121939][-4.2515988 -4.2686934 -4.2641029 -4.2541418 -4.2503452 -4.261189 -4.2689457 -4.2528677 -4.2418542 -4.2467127 -4.263166 -4.2864885 -4.3087983 -4.3145542 -4.3108034][-4.2363477 -4.2533145 -4.241024 -4.2227783 -4.2143912 -4.22194 -4.2207632 -4.1908889 -4.1794071 -4.1952996 -4.2252994 -4.2615685 -4.2912426 -4.3016181 -4.300252][-4.2321515 -4.2444668 -4.2219257 -4.194365 -4.17904 -4.1748395 -4.15381 -4.099298 -4.0882812 -4.125525 -4.1744752 -4.2275062 -4.2660437 -4.2842884 -4.28858][-4.2249541 -4.2286668 -4.198483 -4.1659031 -4.1419616 -4.1245832 -4.073689 -3.982686 -3.9744279 -4.0415311 -4.1116323 -4.1836925 -4.2349634 -4.26431 -4.2775946][-4.2232308 -4.2216516 -4.1893 -4.1522565 -4.1198492 -4.0841265 -3.9934156 -3.8564851 -3.8659153 -3.9768841 -4.0690417 -4.1520824 -4.21132 -4.2500825 -4.2706017][-4.2349849 -4.229527 -4.2013369 -4.1626277 -4.1152081 -4.0488329 -3.8970697 -3.7008724 -3.7468874 -3.9230974 -4.0423388 -4.1345959 -4.2006636 -4.2427621 -4.2678986][-4.2339668 -4.2317095 -4.2168069 -4.1845675 -4.1227164 -4.0212216 -3.814105 -3.5719175 -3.6607792 -3.8906863 -4.0292931 -4.1271286 -4.1977954 -4.2435155 -4.26866][-4.2326322 -4.237422 -4.23809 -4.2155571 -4.1510506 -4.0449219 -3.8573065 -3.6585536 -3.7477417 -3.9486425 -4.0636687 -4.1462789 -4.2081513 -4.2508731 -4.2720966][-4.2271228 -4.2324252 -4.2381148 -4.225853 -4.1669612 -4.0724196 -3.9197049 -3.7793808 -3.8561583 -4.0130858 -4.1016803 -4.1673408 -4.2205443 -4.2583656 -4.2768207][-4.21476 -4.2215204 -4.235086 -4.2321911 -4.18725 -4.1098857 -3.9886034 -3.8888178 -3.9556856 -4.0773315 -4.1460323 -4.1962996 -4.2395282 -4.2688808 -4.282804][-4.1993279 -4.2075768 -4.2333026 -4.243185 -4.2177405 -4.1673474 -4.0803461 -4.0134006 -4.0647736 -4.1469269 -4.1933813 -4.226964 -4.2611604 -4.2829885 -4.2910395][-4.2021604 -4.2120547 -4.2441626 -4.2626457 -4.254797 -4.229557 -4.1711712 -4.1239877 -4.1572132 -4.2085629 -4.2388749 -4.261766 -4.2865076 -4.2986135 -4.2992315][-4.2025037 -4.2145824 -4.2509618 -4.2735376 -4.2793808 -4.2734871 -4.2364407 -4.2004747 -4.2176666 -4.2486362 -4.2697039 -4.2845855 -4.3002372 -4.305769 -4.3025155][-4.2089381 -4.2254124 -4.26072 -4.283761 -4.2966394 -4.2991657 -4.2749176 -4.2476482 -4.2552605 -4.272758 -4.285018 -4.2924185 -4.3015008 -4.3031774 -4.2999797]]...]
INFO - root - 2017-12-07 12:38:43.378907: step 9710, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 57h:43m:48s remains)
INFO - root - 2017-12-07 12:38:49.954034: step 9720, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 58h:41m:33s remains)
INFO - root - 2017-12-07 12:38:56.736187: step 9730, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:06m:57s remains)
INFO - root - 2017-12-07 12:39:03.529172: step 9740, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 60h:09m:02s remains)
INFO - root - 2017-12-07 12:39:10.342271: step 9750, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 60h:54m:44s remains)
INFO - root - 2017-12-07 12:39:17.119937: step 9760, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.669 sec/batch; 59h:56m:49s remains)
INFO - root - 2017-12-07 12:39:23.933317: step 9770, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.664 sec/batch; 59h:30m:12s remains)
INFO - root - 2017-12-07 12:39:30.756341: step 9780, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 63h:09m:29s remains)
INFO - root - 2017-12-07 12:39:37.535900: step 9790, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 64h:57m:54s remains)
INFO - root - 2017-12-07 12:39:44.318760: step 9800, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:20m:30s remains)
2017-12-07 12:39:45.041754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3173032 -4.3242149 -4.3256831 -4.3204 -4.3060169 -4.2934494 -4.2891088 -4.2907906 -4.2960749 -4.3007278 -4.3016152 -4.2966208 -4.2861137 -4.2787833 -4.2835755][-4.3081312 -4.3107448 -4.3090062 -4.2991214 -4.278213 -4.2605228 -4.2550783 -4.25873 -4.2684851 -4.2790289 -4.2893062 -4.2926311 -4.2892714 -4.2881546 -4.296308][-4.2985168 -4.2967081 -4.2917185 -4.277544 -4.2521358 -4.2304621 -4.2221723 -4.2253094 -4.2363987 -4.251606 -4.2706323 -4.2829633 -4.28817 -4.2926083 -4.3028326][-4.2993927 -4.2955303 -4.2877831 -4.2708592 -4.2386351 -4.2057848 -4.1832876 -4.1760125 -4.1840305 -4.205194 -4.2356133 -4.2591805 -4.2743845 -4.2853184 -4.2974119][-4.3051043 -4.2988181 -4.285975 -4.2611918 -4.2167215 -4.1651464 -4.1224704 -4.1012697 -4.1091905 -4.1415892 -4.1832948 -4.2169228 -4.24248 -4.2633829 -4.2805552][-4.2980609 -4.2892714 -4.2686834 -4.2310925 -4.1720304 -4.1040463 -4.0426679 -4.0081153 -4.0201769 -4.0647058 -4.114737 -4.1551223 -4.190341 -4.2201667 -4.2474589][-4.2719393 -4.2600751 -4.2302837 -4.176188 -4.1020641 -4.0217237 -3.9450679 -3.8940158 -3.912941 -3.9755845 -4.0351849 -4.0813761 -4.1246915 -4.165575 -4.2069931][-4.2295537 -4.213604 -4.1779141 -4.1113806 -4.0276775 -3.9427168 -3.8507266 -3.778316 -3.7987709 -3.8784564 -3.9492323 -4.0039625 -4.0563641 -4.111588 -4.17015][-4.1902041 -4.169858 -4.1315651 -4.06032 -3.9790137 -3.9011643 -3.8122025 -3.7365227 -3.752202 -3.8335519 -3.9055674 -3.9664593 -4.0249891 -4.0903916 -4.1596208][-4.1865005 -4.1651297 -4.1272621 -4.0648026 -4.00249 -3.9507113 -3.8976097 -3.8521962 -3.8639164 -3.914912 -3.9645157 -4.0125389 -4.0629778 -4.1228065 -4.1849508][-4.2201195 -4.2036757 -4.1742997 -4.1346378 -4.1008754 -4.0762029 -4.0562181 -4.0408058 -4.0498819 -4.0711913 -4.0921278 -4.1160855 -4.145299 -4.1851759 -4.2274327][-4.2647872 -4.2548871 -4.2382298 -4.2214308 -4.2094221 -4.202219 -4.1984048 -4.196063 -4.1996546 -4.2045474 -4.2106376 -4.2177615 -4.2277617 -4.2461438 -4.2683196][-4.2970533 -4.290709 -4.2813845 -4.2769456 -4.2757368 -4.2782688 -4.2815485 -4.2827826 -4.2821016 -4.2801938 -4.2800422 -4.2794452 -4.2809334 -4.287518 -4.29739][-4.3147831 -4.3100719 -4.3055325 -4.3063474 -4.3106675 -4.3171182 -4.3223362 -4.3230247 -4.3196287 -4.3149991 -4.3113866 -4.3087091 -4.3088245 -4.3119092 -4.3159924][-4.3263268 -4.3241415 -4.3223515 -4.323523 -4.326961 -4.3315182 -4.3349595 -4.3355927 -4.3336143 -4.3294382 -4.3255992 -4.3229589 -4.3232136 -4.3249526 -4.3265142]]...]
INFO - root - 2017-12-07 12:39:51.727182: step 9810, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 63h:00m:39s remains)
INFO - root - 2017-12-07 12:39:58.403978: step 9820, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.713 sec/batch; 63h:54m:22s remains)
INFO - root - 2017-12-07 12:40:05.259129: step 9830, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 60h:50m:48s remains)
INFO - root - 2017-12-07 12:40:12.025331: step 9840, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 57h:38m:06s remains)
INFO - root - 2017-12-07 12:40:18.735097: step 9850, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 57h:49m:31s remains)
INFO - root - 2017-12-07 12:40:25.541480: step 9860, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 62h:59m:33s remains)
INFO - root - 2017-12-07 12:40:32.350259: step 9870, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.693 sec/batch; 62h:06m:39s remains)
INFO - root - 2017-12-07 12:40:39.123943: step 9880, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 59h:00m:59s remains)
INFO - root - 2017-12-07 12:40:46.002882: step 9890, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 62h:23m:02s remains)
INFO - root - 2017-12-07 12:40:52.884575: step 9900, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 62h:50m:19s remains)
2017-12-07 12:40:53.606568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3169737 -4.3068838 -4.2989459 -4.297545 -4.3027897 -4.3068113 -4.3060842 -4.30059 -4.2956495 -4.2979226 -4.3033566 -4.31086 -4.319675 -4.3230367 -4.3197646][-4.3134542 -4.2981119 -4.2851524 -4.2807884 -4.2846341 -4.289238 -4.2881603 -4.2796946 -4.2710538 -4.2765894 -4.2875695 -4.2995257 -4.3114228 -4.3179164 -4.3171172][-4.3163023 -4.2940927 -4.2726388 -4.2603 -4.2563086 -4.2556214 -4.250186 -4.2395086 -4.2293062 -4.2389116 -4.2568784 -4.2761831 -4.2950826 -4.3069773 -4.3125715][-4.3191051 -4.2920952 -4.263948 -4.2415428 -4.2288055 -4.2201962 -4.2054381 -4.1871033 -4.1759806 -4.1895847 -4.2133708 -4.2408605 -4.2702675 -4.2908969 -4.3037539][-4.3106108 -4.2814012 -4.2471285 -4.2163625 -4.1978931 -4.1856523 -4.1589994 -4.1262188 -4.114419 -4.1317015 -4.1581736 -4.1955228 -4.2355213 -4.2660227 -4.2852125][-4.294508 -4.259388 -4.2167864 -4.1772394 -4.1528931 -4.1320953 -4.0898323 -4.0476975 -4.0445919 -4.0730219 -4.1066732 -4.1530685 -4.2036257 -4.242156 -4.2667241][-4.2843442 -4.2425485 -4.1916456 -4.1447506 -4.1138182 -4.0780096 -4.0160255 -3.9639089 -3.9744856 -4.0209308 -4.0661144 -4.1217175 -4.17957 -4.2244968 -4.2536116][-4.284678 -4.2430124 -4.1915522 -4.14096 -4.1018648 -4.0501018 -3.9690564 -3.9130042 -3.9414494 -4.0077863 -4.0638585 -4.123107 -4.1810212 -4.2262764 -4.2560315][-4.2993617 -4.2632289 -4.2185607 -4.1706834 -4.1304822 -4.0747933 -3.9939969 -3.9459696 -3.9805529 -4.0460787 -4.0982513 -4.1464024 -4.194613 -4.2365327 -4.2654409][-4.3107204 -4.2819066 -4.2456889 -4.2060757 -4.1774855 -4.1357212 -4.0757942 -4.0400863 -4.0565662 -4.1007867 -4.1369905 -4.1685772 -4.2040267 -4.2389336 -4.2681856][-4.3110528 -4.290741 -4.2637725 -4.2381153 -4.2273107 -4.2039771 -4.1640806 -4.1289582 -4.1221251 -4.1448565 -4.1660757 -4.1844583 -4.2086735 -4.2337427 -4.2600155][-4.3194485 -4.3088589 -4.2930617 -4.2786169 -4.275322 -4.2598162 -4.2301307 -4.1956935 -4.1766186 -4.18435 -4.19513 -4.2045588 -4.2189627 -4.2327604 -4.2518649][-4.3373852 -4.3360839 -4.3302751 -4.3196845 -4.3115916 -4.2939863 -4.2681279 -4.2414207 -4.2247586 -4.2251072 -4.2266297 -4.2286673 -4.2359538 -4.2408285 -4.2524376][-4.35479 -4.3593035 -4.3553638 -4.3426833 -4.32945 -4.3094773 -4.2863407 -4.2660685 -4.2521038 -4.2470851 -4.24222 -4.24274 -4.2514119 -4.2584472 -4.2681193][-4.3566437 -4.3621731 -4.3575807 -4.3459668 -4.3326564 -4.314971 -4.2968264 -4.28105 -4.2668667 -4.2571721 -4.2494903 -4.2527843 -4.26527 -4.276854 -4.2866454]]...]
INFO - root - 2017-12-07 12:41:00.270285: step 9910, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 57h:50m:11s remains)
INFO - root - 2017-12-07 12:41:06.888490: step 9920, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 59h:38m:29s remains)
INFO - root - 2017-12-07 12:41:13.685930: step 9930, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 63h:54m:28s remains)
INFO - root - 2017-12-07 12:41:20.520415: step 9940, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 62h:45m:50s remains)
INFO - root - 2017-12-07 12:41:27.410200: step 9950, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 59h:44m:55s remains)
INFO - root - 2017-12-07 12:41:34.245509: step 9960, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 59h:02m:07s remains)
INFO - root - 2017-12-07 12:41:41.054113: step 9970, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 62h:25m:46s remains)
INFO - root - 2017-12-07 12:41:47.804384: step 9980, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 61h:44m:44s remains)
INFO - root - 2017-12-07 12:41:54.690988: step 9990, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 60h:36m:28s remains)
INFO - root - 2017-12-07 12:42:01.536150: step 10000, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 58h:30m:33s remains)
2017-12-07 12:42:02.225277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24014 -4.2598119 -4.2783079 -4.2860627 -4.2830434 -4.2675462 -4.2463517 -4.234302 -4.2313657 -4.2356744 -4.2445755 -4.2516861 -4.2595654 -4.2702942 -4.2736197][-4.2522264 -4.273037 -4.2903495 -4.29489 -4.2876968 -4.270638 -4.2531013 -4.2459745 -4.2509089 -4.2623577 -4.2698889 -4.2739248 -4.2802343 -4.2871532 -4.285058][-4.2533641 -4.2696557 -4.2836738 -4.2829165 -4.2655015 -4.2405457 -4.2190652 -4.2141724 -4.2300186 -4.2540627 -4.27154 -4.2839007 -4.291923 -4.296299 -4.290041][-4.2419209 -4.2504745 -4.255424 -4.2436485 -4.2096548 -4.1657615 -4.1295042 -4.1282444 -4.1614289 -4.2088418 -4.2455792 -4.2678766 -4.2781305 -4.2844577 -4.2784977][-4.22374 -4.2182984 -4.2056723 -4.1774368 -4.1197658 -4.0424819 -3.9818423 -3.9920511 -4.0603075 -4.1365361 -4.1913018 -4.2254744 -4.243319 -4.2585135 -4.2584004][-4.2107773 -4.1830058 -4.1444149 -4.0875087 -3.9971695 -3.8718216 -3.7749455 -3.825078 -3.954669 -4.064311 -4.1314068 -4.1773772 -4.2093806 -4.2360859 -4.2422509][-4.21418 -4.176578 -4.1212158 -4.0422616 -3.9262035 -3.7744846 -3.674566 -3.7657869 -3.9130158 -4.0187793 -4.0795321 -4.1313858 -4.1794453 -4.2189331 -4.2333207][-4.2247791 -4.2051616 -4.1630044 -4.0920815 -3.9909651 -3.8842614 -3.837852 -3.9058359 -3.9947956 -4.0469832 -4.0734611 -4.1129737 -4.1648254 -4.2094922 -4.2274909][-4.2210569 -4.2247372 -4.2111506 -4.1708369 -4.1074524 -4.0475526 -4.0290389 -4.0638065 -4.1020303 -4.1166139 -4.123229 -4.153944 -4.1963277 -4.2287254 -4.2387071][-4.2084846 -4.2317419 -4.2369766 -4.2185874 -4.1806707 -4.1464224 -4.13852 -4.1523442 -4.1595221 -4.1590738 -4.1682663 -4.203402 -4.24243 -4.2649837 -4.2665048][-4.2048759 -4.2354956 -4.2441788 -4.2296929 -4.2006435 -4.1759186 -4.1709538 -4.1757722 -4.1682849 -4.1609888 -4.1781006 -4.21957 -4.2623868 -4.2898011 -4.2911773][-4.2175632 -4.2447705 -4.24877 -4.2309866 -4.197391 -4.1670833 -4.1563978 -4.1546197 -4.1438828 -4.1393118 -4.1644745 -4.2111998 -4.2576237 -4.2903643 -4.2974682][-4.2352171 -4.2546654 -4.250361 -4.2257266 -4.1870503 -4.1554585 -4.1428304 -4.1399679 -4.1303277 -4.1258955 -4.1462955 -4.1856475 -4.2321963 -4.272018 -4.2889967][-4.2376056 -4.2590837 -4.2551932 -4.2296515 -4.1938238 -4.1619291 -4.1495323 -4.1465445 -4.137269 -4.128675 -4.1415372 -4.174293 -4.2174587 -4.2567549 -4.2794857][-4.230319 -4.2529964 -4.2528 -4.2316289 -4.1987715 -4.1692629 -4.1619196 -4.1604834 -4.1488914 -4.1343489 -4.1414208 -4.1704559 -4.212513 -4.250351 -4.2746348]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 12:42:09.492011: step 10010, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 57h:26m:38s remains)
INFO - root - 2017-12-07 12:42:16.109331: step 10020, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 63h:03m:57s remains)
INFO - root - 2017-12-07 12:42:23.038679: step 10030, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 64h:32m:51s remains)
INFO - root - 2017-12-07 12:42:29.879037: step 10040, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 59h:27m:33s remains)
INFO - root - 2017-12-07 12:42:36.671667: step 10050, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 59h:23m:09s remains)
INFO - root - 2017-12-07 12:42:43.523063: step 10060, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 59h:48m:13s remains)
INFO - root - 2017-12-07 12:42:50.362335: step 10070, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 63h:09m:34s remains)
INFO - root - 2017-12-07 12:42:57.072579: step 10080, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 62h:48m:09s remains)
INFO - root - 2017-12-07 12:43:03.880541: step 10090, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 60h:47m:37s remains)
INFO - root - 2017-12-07 12:43:10.708022: step 10100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 60h:15m:28s remains)
2017-12-07 12:43:11.547580: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2159634 -4.2273688 -4.2630987 -4.2980442 -4.321476 -4.3346057 -4.3332033 -4.3211374 -4.3020611 -4.2808528 -4.2720957 -4.2817726 -4.3011236 -4.3141623 -4.3170466][-4.2044687 -4.2067723 -4.2356548 -4.2686114 -4.2945309 -4.3138852 -4.3194437 -4.3109212 -4.2916584 -4.2669392 -4.2544622 -4.2625389 -4.2819967 -4.2962208 -4.3023844][-4.199789 -4.18984 -4.2043128 -4.2285781 -4.2535162 -4.2776732 -4.29028 -4.2881308 -4.2738757 -4.2534184 -4.2408767 -4.2446017 -4.2572989 -4.2682862 -4.2783728][-4.1936431 -4.1712737 -4.1698313 -4.1800117 -4.196517 -4.2173872 -4.2327142 -4.2392864 -4.2374272 -4.2309294 -4.2249966 -4.2265491 -4.2309556 -4.2395573 -4.2534103][-4.1812496 -4.1479645 -4.1338086 -4.1293812 -4.1296487 -4.1366072 -4.1462593 -4.1583333 -4.1730695 -4.1872053 -4.1962352 -4.2042379 -4.2101436 -4.2213931 -4.2384524][-4.1704082 -4.1249189 -4.0964847 -4.07416 -4.050405 -4.0323267 -4.0267935 -4.0447316 -4.0843086 -4.1269732 -4.1613708 -4.1874619 -4.2066183 -4.2240987 -4.2396827][-4.1716132 -4.1161056 -4.0741243 -4.0337038 -3.9804268 -3.9248514 -3.8911884 -3.9150271 -3.9837646 -4.0601072 -4.1252909 -4.17729 -4.2150416 -4.2387428 -4.2502069][-4.1846581 -4.1281714 -4.079288 -4.0262685 -3.9468567 -3.8526525 -3.78442 -3.8105524 -3.9039679 -4.0086155 -4.1002197 -4.1706076 -4.2222853 -4.2492933 -4.2566738][-4.1921606 -4.1456742 -4.1035118 -4.0553217 -3.978276 -3.8794096 -3.8026595 -3.814486 -3.8936892 -3.99132 -4.0851307 -4.1622434 -4.2196608 -4.2473874 -4.2525587][-4.1803951 -4.1516252 -4.1310315 -4.1055927 -4.0593486 -3.9948339 -3.9395413 -3.9285228 -3.9662571 -4.0291386 -4.0983434 -4.161727 -4.2107244 -4.2307682 -4.2311873][-4.1540165 -4.14304 -4.147697 -4.148613 -4.1354156 -4.107718 -4.0775452 -4.0591526 -4.0687418 -4.0991535 -4.134244 -4.1680431 -4.1943951 -4.19937 -4.1893988][-4.1278057 -4.1284637 -4.1541257 -4.1765852 -4.1852069 -4.1823134 -4.1721845 -4.1597533 -4.1593108 -4.1691289 -4.1780186 -4.1830726 -4.1847491 -4.1685066 -4.1413631][-4.1275387 -4.137228 -4.1762838 -4.2102041 -4.2258315 -4.2289453 -4.2252603 -4.2166767 -4.2116671 -4.2099681 -4.2025275 -4.1899905 -4.1758723 -4.1469631 -4.108644][-4.1629481 -4.1791577 -4.2221851 -4.2559891 -4.2664948 -4.2594619 -4.2453408 -4.2294025 -4.2159634 -4.2027068 -4.1871033 -4.1690283 -4.15218 -4.1277618 -4.0977125][-4.2170453 -4.2336864 -4.2717776 -4.297513 -4.2980609 -4.2808962 -4.2551847 -4.2255564 -4.1941185 -4.1643362 -4.1428881 -4.1264453 -4.1153617 -4.1075387 -4.100843]]...]
INFO - root - 2017-12-07 12:43:18.284868: step 10110, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 62h:14m:00s remains)
INFO - root - 2017-12-07 12:43:24.897739: step 10120, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 58h:58m:46s remains)
INFO - root - 2017-12-07 12:43:31.683973: step 10130, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 60h:16m:43s remains)
INFO - root - 2017-12-07 12:43:38.366009: step 10140, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 59h:25m:01s remains)
INFO - root - 2017-12-07 12:43:45.097557: step 10150, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 60h:25m:35s remains)
INFO - root - 2017-12-07 12:43:51.683506: step 10160, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.682 sec/batch; 61h:04m:15s remains)
INFO - root - 2017-12-07 12:43:58.421401: step 10170, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 55h:35m:07s remains)
INFO - root - 2017-12-07 12:44:05.177594: step 10180, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 57h:09m:57s remains)
INFO - root - 2017-12-07 12:44:12.125302: step 10190, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 64h:47m:42s remains)
INFO - root - 2017-12-07 12:44:18.872407: step 10200, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 60h:59m:26s remains)
2017-12-07 12:44:19.530252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2784719 -4.2609453 -4.2512264 -4.247334 -4.2440505 -4.2419171 -4.2333183 -4.2218289 -4.2079306 -4.2018852 -4.2084851 -4.20919 -4.2071614 -4.2055483 -4.1981773][-4.2630959 -4.2398844 -4.2285128 -4.22269 -4.2195606 -4.2204328 -4.2151918 -4.2042842 -4.1848736 -4.172986 -4.1745691 -4.1695614 -4.1655812 -4.16311 -4.154161][-4.2395396 -4.2115345 -4.1953068 -4.1853085 -4.1822915 -4.1859937 -4.1832247 -4.17259 -4.153717 -4.1430035 -4.1445332 -4.1398015 -4.1384459 -4.1378331 -4.1339865][-4.2172937 -4.1918607 -4.1762342 -4.16755 -4.1648188 -4.1700873 -4.1661291 -4.1490426 -4.1296883 -4.1235609 -4.1270528 -4.1244869 -4.1264057 -4.1286306 -4.1330576][-4.2015796 -4.1824937 -4.1712556 -4.1683078 -4.1680455 -4.1741567 -4.1621871 -4.1315327 -4.1072612 -4.1015038 -4.1037617 -4.1003604 -4.1029019 -4.111464 -4.1246533][-4.192543 -4.1748013 -4.1679764 -4.1687384 -4.1677947 -4.1688986 -4.1409636 -4.0917892 -4.06502 -4.0658193 -4.069695 -4.0680304 -4.075171 -4.0886283 -4.1005187][-4.187295 -4.1709151 -4.1665063 -4.1644964 -4.1531315 -4.1374631 -4.083992 -4.0133777 -3.9969501 -4.0194955 -4.0362182 -4.0458865 -4.061007 -4.0766454 -4.084549][-4.1642795 -4.1437626 -4.1353226 -4.1237426 -4.0962906 -4.0580597 -3.981833 -3.9066534 -3.9199462 -3.9742768 -4.0111079 -4.0383391 -4.0637894 -4.0822186 -4.0910006][-4.10715 -4.073175 -4.051383 -4.0295963 -3.9974117 -3.9586105 -3.8921604 -3.8491664 -3.8985574 -3.9721336 -4.0198445 -4.0536895 -4.0787349 -4.0944405 -4.1067786][-4.0568419 -4.009306 -3.9753768 -3.9593072 -3.947526 -3.9328778 -3.9036934 -3.8994005 -3.9519014 -4.0124426 -4.0476518 -4.0715904 -4.0868306 -4.0967183 -4.1110315][-4.0596189 -4.0129938 -3.9786112 -3.97009 -3.9730458 -3.9764609 -3.9704576 -3.9793854 -4.0175734 -4.0540643 -4.0720129 -4.085012 -4.093154 -4.1013308 -4.1176653][-4.1165895 -4.0777783 -4.0449734 -4.0370803 -4.0399103 -4.0463648 -4.0483418 -4.0593395 -4.0841813 -4.10258 -4.1104431 -4.1197391 -4.127697 -4.1365833 -4.1508236][-4.1864963 -4.1546078 -4.1267653 -4.1173224 -4.115345 -4.119204 -4.1225624 -4.132709 -4.148849 -4.1577535 -4.1625509 -4.169384 -4.1776938 -4.1871848 -4.1983128][-4.2457948 -4.223237 -4.2033486 -4.1943703 -4.190721 -4.1913619 -4.1939445 -4.202877 -4.2130218 -4.2175736 -4.2216606 -4.227294 -4.234158 -4.2417145 -4.2487392][-4.2930856 -4.2787619 -4.2660456 -4.2606816 -4.2598348 -4.2622023 -4.2664695 -4.2727885 -4.2779379 -4.2785988 -4.2796559 -4.2820182 -4.284481 -4.2883248 -4.2917686]]...]
INFO - root - 2017-12-07 12:44:26.210359: step 10210, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 58h:42m:09s remains)
INFO - root - 2017-12-07 12:44:32.904729: step 10220, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 64h:54m:08s remains)
INFO - root - 2017-12-07 12:44:39.724267: step 10230, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 60h:43m:11s remains)
INFO - root - 2017-12-07 12:44:46.509703: step 10240, loss = 2.07, batch loss = 2.02 (13.0 examples/sec; 0.617 sec/batch; 55h:16m:13s remains)
INFO - root - 2017-12-07 12:44:53.312425: step 10250, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 57h:41m:46s remains)
INFO - root - 2017-12-07 12:45:00.098471: step 10260, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 63h:25m:32s remains)
INFO - root - 2017-12-07 12:45:06.941052: step 10270, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 66h:41m:18s remains)
INFO - root - 2017-12-07 12:45:13.700942: step 10280, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 62h:07m:55s remains)
INFO - root - 2017-12-07 12:45:20.396618: step 10290, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 57h:41m:05s remains)
INFO - root - 2017-12-07 12:45:27.207476: step 10300, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 62h:07m:46s remains)
2017-12-07 12:45:27.982542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2602777 -4.2618713 -4.2646775 -4.2683883 -4.2702274 -4.2714338 -4.2726936 -4.2724257 -4.2662435 -4.2516193 -4.2349133 -4.2244568 -4.2219329 -4.2345986 -4.2539978][-4.2530475 -4.2529063 -4.2564521 -4.2628622 -4.2664266 -4.2662697 -4.2661452 -4.2655349 -4.2607732 -4.2504735 -4.240572 -4.2368932 -4.237937 -4.2494822 -4.26378][-4.2428207 -4.2411613 -4.2459683 -4.254024 -4.2567554 -4.2529197 -4.2498717 -4.24992 -4.251122 -4.248138 -4.244185 -4.2435541 -4.2437844 -4.25244 -4.2642097][-4.220921 -4.2179337 -4.2246966 -4.2325196 -4.2319264 -4.2217822 -4.2113919 -4.2134666 -4.2267809 -4.2376943 -4.2419581 -4.2417893 -4.2365365 -4.2402763 -4.2514853][-4.1753144 -4.1771426 -4.1902113 -4.200336 -4.1936693 -4.1700644 -4.146811 -4.1501656 -4.1815786 -4.2156572 -4.2332916 -4.2337422 -4.218739 -4.214025 -4.2268414][-4.1081715 -4.1263347 -4.152349 -4.1651053 -4.1446476 -4.09416 -4.0438528 -4.0435033 -4.0994625 -4.1658812 -4.2039046 -4.209692 -4.1904359 -4.1819954 -4.1992607][-4.0447974 -4.0857658 -4.1327028 -4.1514382 -4.1160603 -4.035882 -3.9515719 -3.9352844 -4.0107427 -4.104126 -4.1633911 -4.1812558 -4.1690865 -4.1644106 -4.1875634][-4.00482 -4.0629759 -4.1302876 -4.1630406 -4.1327071 -4.0488367 -3.9556947 -3.9252496 -3.9870374 -4.0713696 -4.1359754 -4.1655626 -4.169055 -4.17711 -4.2064791][-4.0118055 -4.0745292 -4.1498165 -4.1928287 -4.1756897 -4.1111927 -4.0437856 -4.0180707 -4.0539026 -4.10404 -4.1512394 -4.1792927 -4.1949224 -4.2141294 -4.2440586][-4.0548143 -4.1123233 -4.1784444 -4.2186666 -4.2081647 -4.1634459 -4.1221147 -4.1071887 -4.1268473 -4.1500072 -4.1762338 -4.1976404 -4.218338 -4.2449713 -4.2746553][-4.1065388 -4.1582551 -4.2130551 -4.2493486 -4.2450771 -4.2163 -4.1904969 -4.1793318 -4.1832733 -4.1810083 -4.183815 -4.1964235 -4.2193546 -4.2511983 -4.2821193][-4.1266432 -4.1753564 -4.2274823 -4.2657108 -4.2705736 -4.2569771 -4.2457895 -4.2376533 -4.2292104 -4.2096496 -4.1953177 -4.1982594 -4.2167954 -4.2456331 -4.2754607][-4.1262255 -4.1681061 -4.2220573 -4.2697706 -4.2885356 -4.2892942 -4.2903175 -4.2862844 -4.2729697 -4.24918 -4.2273583 -4.220367 -4.2281628 -4.2471457 -4.2722578][-4.1179166 -4.152174 -4.207726 -4.2631388 -4.2933555 -4.3035197 -4.3115649 -4.3095036 -4.2951384 -4.2714076 -4.2502089 -4.2372618 -4.2348642 -4.2454276 -4.2679443][-4.1113658 -4.1410222 -4.1937313 -4.2490053 -4.2848482 -4.3015332 -4.312695 -4.3103838 -4.2944951 -4.2702103 -4.2518315 -4.2368016 -4.227325 -4.2338161 -4.2566934]]...]
INFO - root - 2017-12-07 12:45:34.727998: step 10310, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 59h:54m:35s remains)
INFO - root - 2017-12-07 12:45:40.976202: step 10320, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 55h:06m:43s remains)
INFO - root - 2017-12-07 12:45:47.797567: step 10330, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 57h:24m:04s remains)
INFO - root - 2017-12-07 12:45:54.659273: step 10340, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 62h:23m:19s remains)
INFO - root - 2017-12-07 12:46:01.450608: step 10350, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 62h:52m:55s remains)
INFO - root - 2017-12-07 12:46:08.270847: step 10360, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 61h:51m:18s remains)
INFO - root - 2017-12-07 12:46:14.991440: step 10370, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.633 sec/batch; 56h:36m:22s remains)
INFO - root - 2017-12-07 12:46:21.798226: step 10380, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 59h:17m:12s remains)
INFO - root - 2017-12-07 12:46:28.623970: step 10390, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 64h:48m:29s remains)
INFO - root - 2017-12-07 12:46:35.440043: step 10400, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 63h:04m:16s remains)
2017-12-07 12:46:36.190976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3475485 -4.34872 -4.3430748 -4.3260822 -4.29784 -4.2692866 -4.2550793 -4.2635617 -4.29147 -4.3203669 -4.3385124 -4.3465796 -4.3490477 -4.348258 -4.3469043][-4.3493567 -4.3492022 -4.3395014 -4.316391 -4.2821331 -4.250422 -4.236917 -4.2499228 -4.2835646 -4.3158078 -4.3364863 -4.3472657 -4.3501763 -4.3489685 -4.3475747][-4.3490825 -4.3475137 -4.3333321 -4.3030963 -4.2610669 -4.225142 -4.2129021 -4.2299685 -4.2680368 -4.30424 -4.3283324 -4.3431387 -4.3490233 -4.3490052 -4.3481636][-4.3401504 -4.3367147 -4.3159976 -4.2751465 -4.2220984 -4.1835914 -4.1765118 -4.2007561 -4.2449389 -4.287962 -4.3184166 -4.3381791 -4.3483276 -4.350564 -4.3500695][-4.3145504 -4.30634 -4.2728014 -4.2152333 -4.1521869 -4.1153083 -4.1175742 -4.14969 -4.1987963 -4.2503986 -4.2918968 -4.3207674 -4.3390985 -4.3473444 -4.3500028][-4.2687769 -4.2500725 -4.1997018 -4.1264243 -4.060854 -4.0332174 -4.0456114 -4.0821648 -4.1309915 -4.1877027 -4.243546 -4.2877645 -4.3180118 -4.3347406 -4.3433967][-4.2169375 -4.1849775 -4.1186185 -4.0352755 -3.9747291 -3.9610729 -3.9836996 -4.0198736 -4.059638 -4.1138439 -4.1821389 -4.24258 -4.2876706 -4.3168821 -4.3333812][-4.1822844 -4.1415281 -4.0692706 -3.9878256 -3.9390967 -3.9361217 -3.96132 -3.9897451 -4.0147562 -4.061017 -4.1352715 -4.2046065 -4.2612262 -4.3021359 -4.326746][-4.1690068 -4.1266813 -4.061162 -3.9937329 -3.9588063 -3.9615703 -3.9827173 -4.0007186 -4.0125914 -4.0515676 -4.1233835 -4.1931071 -4.2534146 -4.2991571 -4.3272605][-4.17359 -4.1349678 -4.0835323 -4.0341015 -4.0131245 -4.0187545 -4.0329204 -4.0390549 -4.0408144 -4.0720577 -4.13366 -4.1965055 -4.2555218 -4.3019123 -4.3303242][-4.1966543 -4.1655354 -4.1289816 -4.0979981 -4.0892172 -4.098134 -4.1082234 -4.1068697 -4.1017342 -4.1227193 -4.1669474 -4.2158952 -4.2650204 -4.306035 -4.3315735][-4.2284346 -4.2091365 -4.1886086 -4.1744161 -4.1752496 -4.1862555 -4.1929536 -4.1880703 -4.1806035 -4.190824 -4.2177615 -4.2520847 -4.2881756 -4.317873 -4.3347211][-4.2575536 -4.2465391 -4.2370391 -4.2345276 -4.2425294 -4.25462 -4.2605438 -4.2569189 -4.250442 -4.252923 -4.2667794 -4.2893949 -4.3132834 -4.3307505 -4.3375359][-4.2844715 -4.278192 -4.2737565 -4.2753553 -4.2835612 -4.2930675 -4.2977576 -4.2953095 -4.2898421 -4.2879882 -4.2944622 -4.30951 -4.3250766 -4.3343349 -4.334363][-4.3076696 -4.3043108 -4.3016963 -4.3023038 -4.306138 -4.3105307 -4.3128419 -4.3110151 -4.3070135 -4.3050556 -4.3077793 -4.3164616 -4.3250494 -4.3287225 -4.3265295]]...]
INFO - root - 2017-12-07 12:46:42.962614: step 10410, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 63h:24m:34s remains)
INFO - root - 2017-12-07 12:46:49.651704: step 10420, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 65h:55m:05s remains)
INFO - root - 2017-12-07 12:46:56.469712: step 10430, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 60h:59m:10s remains)
INFO - root - 2017-12-07 12:47:03.189178: step 10440, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 56h:46m:25s remains)
INFO - root - 2017-12-07 12:47:09.976549: step 10450, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 60h:05m:32s remains)
INFO - root - 2017-12-07 12:47:16.834206: step 10460, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 63h:53m:28s remains)
INFO - root - 2017-12-07 12:47:23.473076: step 10470, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 60h:11m:42s remains)
INFO - root - 2017-12-07 12:47:30.370133: step 10480, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 57h:46m:34s remains)
INFO - root - 2017-12-07 12:47:37.117464: step 10490, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 62h:13m:32s remains)
INFO - root - 2017-12-07 12:47:43.918987: step 10500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 63h:49m:14s remains)
2017-12-07 12:47:44.627691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0536108 -4.0318885 -4.0590854 -4.1039824 -4.1280704 -4.1229925 -4.1116791 -4.1133513 -4.1436839 -4.1795421 -4.2115932 -4.2329397 -4.23413 -4.2316947 -4.2431517][-4.0511994 -4.0178242 -4.0473166 -4.0997338 -4.1360059 -4.1445704 -4.1395373 -4.1368356 -4.162581 -4.19831 -4.2256126 -4.239388 -4.2373629 -4.2355957 -4.248168][-4.0671782 -4.0347533 -4.0640106 -4.1135917 -4.1480513 -4.1530161 -4.1480508 -4.145865 -4.1668262 -4.1999125 -4.22534 -4.2352276 -4.2335153 -4.2341704 -4.25086][-4.1007032 -4.071703 -4.0897264 -4.1248946 -4.1428504 -4.1333876 -4.123827 -4.1260858 -4.1482296 -4.1803265 -4.2107234 -4.2230725 -4.2252827 -4.2314363 -4.2511058][-4.1387296 -4.1170511 -4.1239209 -4.1378279 -4.1339397 -4.1089196 -4.09285 -4.1028328 -4.129693 -4.1599064 -4.1934004 -4.2114034 -4.2184348 -4.2284226 -4.2489333][-4.1664147 -4.1580687 -4.1619139 -4.156899 -4.1302938 -4.0943975 -4.0777974 -4.0957165 -4.1236629 -4.1464629 -4.1765924 -4.1977034 -4.2086987 -4.2218604 -4.2448792][-4.1830206 -4.1838293 -4.1834445 -4.1651812 -4.1309571 -4.0994854 -4.0898271 -4.1094971 -4.1302781 -4.1401548 -4.1634078 -4.1860881 -4.1992006 -4.2137227 -4.240078][-4.2053704 -4.2077718 -4.1990595 -4.1728368 -4.1409292 -4.1228533 -4.1239944 -4.1394176 -4.1485734 -4.14854 -4.161829 -4.1808753 -4.191504 -4.2059417 -4.2354116][-4.2206912 -4.2227254 -4.2117877 -4.1854563 -4.1575336 -4.1501203 -4.1603947 -4.1741734 -4.1753345 -4.1692281 -4.1715393 -4.1783714 -4.1810894 -4.1946039 -4.2278938][-4.2168069 -4.214385 -4.2047982 -4.1860857 -4.1674581 -4.1691175 -4.1869025 -4.19978 -4.1951218 -4.1839256 -4.1765208 -4.1730347 -4.1693387 -4.1805215 -4.2159748][-4.189188 -4.1841221 -4.181067 -4.1778345 -4.1765471 -4.1871 -4.2079105 -4.2176576 -4.2065096 -4.1851392 -4.1680937 -4.1584835 -4.1528511 -4.1657157 -4.20481][-4.1624513 -4.1502628 -4.1480083 -4.1600723 -4.1761589 -4.1928711 -4.2133555 -4.223289 -4.2083578 -4.1789608 -4.1532683 -4.1408572 -4.1397257 -4.1561713 -4.19862][-4.149117 -4.1253943 -4.114748 -4.1376076 -4.1692548 -4.1876531 -4.206141 -4.2174706 -4.2052865 -4.1765513 -4.147666 -4.1361504 -4.1383667 -4.1551485 -4.1959791][-4.1451521 -4.1196632 -4.1076193 -4.1309052 -4.1612825 -4.17449 -4.1896925 -4.2077718 -4.206584 -4.1856174 -4.1637769 -4.15326 -4.1493764 -4.1600361 -4.1955595][-4.1479144 -4.1307874 -4.1240764 -4.1423669 -4.1598783 -4.1615591 -4.177156 -4.2042785 -4.21126 -4.1979289 -4.18687 -4.177897 -4.1662459 -4.1680989 -4.1968307]]...]
INFO - root - 2017-12-07 12:47:51.320501: step 10510, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.651 sec/batch; 58h:14m:10s remains)
INFO - root - 2017-12-07 12:47:58.052557: step 10520, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 62h:48m:46s remains)
INFO - root - 2017-12-07 12:48:04.946371: step 10530, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 63h:00m:11s remains)
INFO - root - 2017-12-07 12:48:11.748854: step 10540, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 60h:06m:25s remains)
INFO - root - 2017-12-07 12:48:18.509016: step 10550, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 57h:21m:58s remains)
INFO - root - 2017-12-07 12:48:25.326331: step 10560, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 57h:38m:22s remains)
INFO - root - 2017-12-07 12:48:32.259738: step 10570, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 64h:05m:26s remains)
INFO - root - 2017-12-07 12:48:39.034268: step 10580, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 61h:21m:55s remains)
INFO - root - 2017-12-07 12:48:45.872123: step 10590, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 60h:56m:15s remains)
INFO - root - 2017-12-07 12:48:52.528202: step 10600, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.632 sec/batch; 56h:28m:14s remains)
2017-12-07 12:48:53.240837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1478033 -4.0646124 -4.0035625 -4.0200868 -4.0948153 -4.1513228 -4.170073 -4.1701474 -4.1668777 -4.1678534 -4.1781816 -4.1809349 -4.1902857 -4.1972804 -4.1948442][-4.1426249 -4.0683966 -4.0184445 -4.0347266 -4.0926967 -4.1290088 -4.1349792 -4.1322393 -4.1322479 -4.1452017 -4.1675024 -4.18053 -4.1971216 -4.2143173 -4.2238379][-4.1519332 -4.0958471 -4.0634494 -4.0767317 -4.11118 -4.1246433 -4.1170754 -4.1134024 -4.1233268 -4.1507092 -4.181633 -4.1990957 -4.2144461 -4.2270007 -4.2376914][-4.1815233 -4.1444483 -4.1275272 -4.1357369 -4.1513171 -4.1495123 -4.1372 -4.13357 -4.1468716 -4.1776552 -4.2071495 -4.2238245 -4.2309756 -4.2355137 -4.2428031][-4.2120209 -4.1818037 -4.1704416 -4.1750679 -4.1800003 -4.1785932 -4.1774163 -4.1870513 -4.2084184 -4.2272391 -4.2347579 -4.2351012 -4.2309403 -4.2315187 -4.2378349][-4.2348471 -4.2022877 -4.1869006 -4.1844506 -4.1807008 -4.1772261 -4.1872272 -4.2143211 -4.2487636 -4.2604961 -4.24626 -4.2274075 -4.2127261 -4.213532 -4.2271953][-4.2371864 -4.1928396 -4.1654377 -4.1506672 -4.1351976 -4.123004 -4.13285 -4.1713963 -4.215013 -4.2276239 -4.2089052 -4.1909223 -4.1846571 -4.2003345 -4.2256346][-4.215138 -4.1604352 -4.1200504 -4.0890265 -4.0592666 -4.03774 -4.0453296 -4.0852137 -4.13195 -4.1461153 -4.1382012 -4.1408005 -4.1625514 -4.2024384 -4.23992][-4.1840105 -4.123858 -4.0819626 -4.0458026 -4.0081286 -3.977706 -3.9764695 -4.0118656 -4.0599265 -4.0714478 -4.0758142 -4.1060309 -4.1593456 -4.2157464 -4.25539][-4.1634135 -4.09998 -4.0594425 -4.0279727 -3.9945607 -3.9659553 -3.9594269 -3.9881649 -4.0283313 -4.031477 -4.0507641 -4.1123548 -4.1876545 -4.2416725 -4.2662449][-4.166337 -4.1046362 -4.0694065 -4.04887 -4.03087 -4.019937 -4.0190167 -4.03504 -4.0529985 -4.0487938 -4.0752692 -4.1493244 -4.2287788 -4.2695947 -4.2717762][-4.1883297 -4.1391315 -4.1155262 -4.1088643 -4.105598 -4.1108894 -4.11211 -4.116744 -4.1260324 -4.1262789 -4.1499338 -4.2065535 -4.2646356 -4.2821755 -4.2588391][-4.2096882 -4.1752791 -4.1652246 -4.1721663 -4.1796007 -4.1857176 -4.1805158 -4.179894 -4.1931281 -4.2052584 -4.2254939 -4.2588925 -4.2847133 -4.2772422 -4.2380552][-4.2261448 -4.2010946 -4.200398 -4.2160282 -4.2295289 -4.2331953 -4.2248783 -4.222928 -4.2384295 -4.2553573 -4.2732725 -4.288692 -4.2907987 -4.2689261 -4.2239804][-4.2332606 -4.2127581 -4.2165413 -4.2372723 -4.2508254 -4.2530446 -4.2478075 -4.2501245 -4.2656589 -4.280272 -4.2930007 -4.29983 -4.2914486 -4.26635 -4.2237997]]...]
INFO - root - 2017-12-07 12:49:00.013155: step 10610, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 64h:12m:47s remains)
INFO - root - 2017-12-07 12:49:06.636940: step 10620, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 62h:54m:35s remains)
INFO - root - 2017-12-07 12:49:13.416312: step 10630, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 60h:08m:29s remains)
INFO - root - 2017-12-07 12:49:20.249940: step 10640, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 59h:17m:55s remains)
INFO - root - 2017-12-07 12:49:27.089657: step 10650, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 63h:25m:17s remains)
INFO - root - 2017-12-07 12:49:33.905771: step 10660, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 63h:35m:11s remains)
INFO - root - 2017-12-07 12:49:40.673820: step 10670, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 63h:37m:23s remains)
INFO - root - 2017-12-07 12:49:47.418157: step 10680, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 57h:10m:02s remains)
INFO - root - 2017-12-07 12:49:54.193694: step 10690, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 60h:35m:47s remains)
INFO - root - 2017-12-07 12:50:00.994744: step 10700, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 62h:09m:29s remains)
2017-12-07 12:50:01.750691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3353972 -4.3392539 -4.333797 -4.3230042 -4.3023748 -4.2660513 -4.2313294 -4.1970983 -4.1704721 -4.1822286 -4.2112961 -4.2466683 -4.2784715 -4.2883496 -4.2696424][-4.3390765 -4.3489056 -4.3492851 -4.3355083 -4.2993345 -4.2463365 -4.2024121 -4.1528006 -4.1177421 -4.1394229 -4.187459 -4.2357678 -4.2683058 -4.2781119 -4.2577][-4.339818 -4.3561592 -4.3627872 -4.3492451 -4.3043113 -4.2357512 -4.1785092 -4.113039 -4.076364 -4.1124959 -4.173461 -4.2282715 -4.2576966 -4.2655067 -4.2407184][-4.3362484 -4.3542438 -4.3577337 -4.3379683 -4.2815256 -4.1931348 -4.1142006 -4.0394735 -4.0208945 -4.0824585 -4.1596 -4.2165017 -4.2392979 -4.2371569 -4.1970658][-4.3312988 -4.35105 -4.3476481 -4.3172894 -4.2469816 -4.139595 -4.0316553 -3.9442945 -3.9470644 -4.0356913 -4.1275721 -4.1832442 -4.2062168 -4.1913786 -4.1238604][-4.3264256 -4.3477464 -4.3395481 -4.3013196 -4.2184792 -4.0965595 -3.9668722 -3.8600569 -3.8657629 -3.9703937 -4.0768895 -4.1385632 -4.1625328 -4.1383419 -4.0375028][-4.3195157 -4.3391061 -4.3262606 -4.2811952 -4.188138 -4.0573955 -3.916851 -3.7886829 -3.7777081 -3.883589 -3.9974368 -4.0706296 -4.106638 -4.0857768 -3.9787395][-4.3093963 -4.3225822 -4.3030481 -4.2508278 -4.1473994 -4.0144863 -3.8742962 -3.737505 -3.7074516 -3.7990372 -3.909961 -3.9922848 -4.04567 -4.0442934 -3.9536943][-4.2971253 -4.3013015 -4.2741356 -4.2148356 -4.1075182 -3.9805212 -3.8548629 -3.7320867 -3.7003157 -3.7717519 -3.8757372 -3.9614289 -4.0158854 -4.0246592 -3.9570069][-4.2862897 -4.2815738 -4.2512074 -4.1946554 -4.0980248 -3.9857936 -3.881351 -3.7832332 -3.7539196 -3.8018813 -3.8902051 -3.9716446 -4.0250292 -4.0454249 -3.9988005][-4.2816763 -4.2709527 -4.2424254 -4.19667 -4.1204982 -4.0340338 -3.959729 -3.8951147 -3.8762133 -3.9036951 -3.9663529 -4.0293436 -4.0738873 -4.094346 -4.0663433][-4.2894197 -4.2777643 -4.2550621 -4.2212892 -4.16622 -4.1087146 -4.0649877 -4.0333676 -4.0294189 -4.0474148 -4.0859108 -4.1217747 -4.1486259 -4.1592937 -4.1469913][-4.3035488 -4.2942052 -4.2788444 -4.2561836 -4.2218356 -4.1895566 -4.1697273 -4.16014 -4.1645255 -4.1746612 -4.197299 -4.2166691 -4.2312465 -4.2350049 -4.2324953][-4.3185773 -4.3131313 -4.3047175 -4.2938004 -4.2791939 -4.2662096 -4.2617865 -4.2625265 -4.2670703 -4.2689428 -4.2780619 -4.2870321 -4.2912993 -4.2906613 -4.2913122][-4.3330812 -4.3324437 -4.3287697 -4.3253174 -4.3220267 -4.3196464 -4.3226142 -4.32728 -4.3309479 -4.3302178 -4.3307943 -4.3321447 -4.3314476 -4.3282537 -4.3283949]]...]
INFO - root - 2017-12-07 12:50:08.402141: step 10710, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 56h:59m:51s remains)
INFO - root - 2017-12-07 12:50:15.083396: step 10720, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 58h:12m:08s remains)
INFO - root - 2017-12-07 12:50:21.897307: step 10730, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 63h:18m:23s remains)
INFO - root - 2017-12-07 12:50:28.701536: step 10740, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 61h:15m:44s remains)
INFO - root - 2017-12-07 12:50:35.479685: step 10750, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 61h:19m:41s remains)
INFO - root - 2017-12-07 12:50:42.250226: step 10760, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:46m:57s remains)
INFO - root - 2017-12-07 12:50:49.040737: step 10770, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 61h:46m:21s remains)
INFO - root - 2017-12-07 12:50:55.589522: step 10780, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 59h:29m:12s remains)
INFO - root - 2017-12-07 12:51:02.339893: step 10790, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 57h:58m:33s remains)
INFO - root - 2017-12-07 12:51:09.068397: step 10800, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 57h:36m:03s remains)
2017-12-07 12:51:09.761765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2337852 -4.2470365 -4.2572803 -4.2593541 -4.2479882 -4.2376709 -4.2461619 -4.252717 -4.2396469 -4.2200308 -4.2026663 -4.18531 -4.1840377 -4.1872892 -4.1924553][-4.1984005 -4.2154241 -4.231914 -4.241385 -4.2350335 -4.2274332 -4.2336588 -4.2389336 -4.229414 -4.2146964 -4.2000675 -4.1845875 -4.1904569 -4.2024021 -4.2081079][-4.1705384 -4.1910715 -4.2130246 -4.2259974 -4.2220607 -4.2144217 -4.2178693 -4.2246022 -4.2203083 -4.2092452 -4.1954026 -4.1798859 -4.1845202 -4.2028079 -4.2106709][-4.1679611 -4.1907997 -4.2157946 -4.2307458 -4.2273674 -4.218647 -4.2194004 -4.2267871 -4.2236052 -4.210835 -4.1912174 -4.1686697 -4.1665239 -4.1841688 -4.1946158][-4.1888127 -4.2066522 -4.2304835 -4.2436538 -4.2382612 -4.2283163 -4.2274046 -4.2349563 -4.2302537 -4.212945 -4.1895151 -4.1632733 -4.153193 -4.1613307 -4.1697941][-4.2133303 -4.2222962 -4.2399554 -4.2452321 -4.2352157 -4.2236314 -4.221694 -4.2262182 -4.2173328 -4.1983085 -4.1810784 -4.1628389 -4.1495171 -4.14154 -4.1418405][-4.2292514 -4.226625 -4.2341309 -4.231338 -4.2153697 -4.1979346 -4.1871157 -4.1802506 -4.1606207 -4.1394043 -4.1401834 -4.1452861 -4.1410627 -4.127409 -4.1253195][-4.2337322 -4.2209306 -4.2179027 -4.2091007 -4.185708 -4.1565948 -4.133462 -4.1132474 -4.0791535 -4.0538774 -4.0744729 -4.1073141 -4.1180553 -4.1096258 -4.1166267][-4.2256441 -4.2119436 -4.2057171 -4.1941977 -4.1668148 -4.1360912 -4.1126251 -4.0921679 -4.0569754 -4.0339289 -4.0633192 -4.1034064 -4.1177034 -4.1111236 -4.1268721][-4.2163019 -4.2126803 -4.2126122 -4.2050333 -4.1854224 -4.16843 -4.1619916 -4.1531835 -4.1267805 -4.1121206 -4.1310043 -4.1500721 -4.1487203 -4.1359043 -4.1507063][-4.2165952 -4.2214961 -4.2278409 -4.2256904 -4.2135129 -4.2057528 -4.2103896 -4.2126594 -4.197866 -4.1906528 -4.1987233 -4.1967821 -4.1789284 -4.158051 -4.1654611][-4.2350254 -4.2410331 -4.2497745 -4.2491927 -4.2378469 -4.2295494 -4.2317262 -4.2340631 -4.225718 -4.2250957 -4.2331276 -4.2257667 -4.2041736 -4.1785307 -4.1746535][-4.2661605 -4.2708373 -4.2775545 -4.2769651 -4.2639174 -4.2511873 -4.2404642 -4.2272739 -4.2199197 -4.2311182 -4.246078 -4.2384191 -4.217803 -4.1928148 -4.1821375][-4.2811494 -4.2866735 -4.292767 -4.2919388 -4.2793112 -4.2619681 -4.2391267 -4.2094259 -4.1984482 -4.2191806 -4.2461882 -4.24667 -4.2296429 -4.2092524 -4.1984172][-4.2832451 -4.2862978 -4.289103 -4.2867208 -4.2769804 -4.2606568 -4.2324104 -4.194109 -4.1826859 -4.2107491 -4.2434969 -4.2501259 -4.2363248 -4.2223225 -4.2156844]]...]
INFO - root - 2017-12-07 12:51:16.475952: step 10810, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 61h:54m:02s remains)
INFO - root - 2017-12-07 12:51:23.039891: step 10820, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 59h:59m:00s remains)
INFO - root - 2017-12-07 12:51:29.852318: step 10830, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 58h:54m:13s remains)
INFO - root - 2017-12-07 12:51:36.711637: step 10840, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 65h:42m:00s remains)
INFO - root - 2017-12-07 12:51:43.615743: step 10850, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 64h:51m:41s remains)
INFO - root - 2017-12-07 12:51:50.402737: step 10860, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 59h:10m:57s remains)
INFO - root - 2017-12-07 12:51:57.259097: step 10870, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:31m:29s remains)
INFO - root - 2017-12-07 12:52:03.991666: step 10880, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 62h:58m:38s remains)
INFO - root - 2017-12-07 12:52:10.864291: step 10890, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.747 sec/batch; 66h:45m:08s remains)
INFO - root - 2017-12-07 12:52:17.662874: step 10900, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 61h:02m:56s remains)
2017-12-07 12:52:18.346849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2047968 -4.1948705 -4.215796 -4.2537913 -4.2931337 -4.3184366 -4.33737 -4.348382 -4.3270431 -4.281126 -4.2333136 -4.18879 -4.1580262 -4.161809 -4.1923184][-4.1681833 -4.1595864 -4.1806293 -4.2146587 -4.246942 -4.2749424 -4.3022962 -4.3253403 -4.3142748 -4.2740803 -4.2282009 -4.1848559 -4.1569037 -4.1637945 -4.2007618][-4.1344538 -4.1311264 -4.1518946 -4.1795273 -4.2036848 -4.2311707 -4.2642341 -4.2946 -4.2897849 -4.2570739 -4.2193918 -4.1860433 -4.1655111 -4.1745992 -4.2111683][-4.1155853 -4.1231046 -4.1461415 -4.1670866 -4.1839218 -4.2104564 -4.2450256 -4.2713985 -4.2646809 -4.2377067 -4.2123718 -4.1939569 -4.1834059 -4.1956363 -4.2287793][-4.098331 -4.1122618 -4.1374617 -4.1505594 -4.1603436 -4.1853242 -4.2148452 -4.2296429 -4.2166462 -4.1968284 -4.19186 -4.190413 -4.1910439 -4.2067552 -4.2390838][-4.1031084 -4.116786 -4.1375103 -4.1433039 -4.1505322 -4.1667671 -4.1793323 -4.1703696 -4.1419678 -4.12597 -4.1421618 -4.1581807 -4.1702709 -4.1889377 -4.22498][-4.1273222 -4.1361046 -4.1512594 -4.157197 -4.1606669 -4.1616144 -4.151258 -4.1165004 -4.0647044 -4.0467539 -4.0851274 -4.1219039 -4.1414595 -4.1599436 -4.1993427][-4.1534772 -4.1645322 -4.1817436 -4.1899619 -4.1860304 -4.1711483 -4.1421008 -4.0854721 -4.0120792 -3.9887269 -4.0442576 -4.1048746 -4.1338048 -4.1517715 -4.1911182][-4.167717 -4.1873817 -4.2115726 -4.2230892 -4.2148185 -4.1924019 -4.1581616 -4.0984526 -4.0196238 -3.9955952 -4.0548439 -4.1231203 -4.1525435 -4.1683564 -4.2048392][-4.1808867 -4.2043481 -4.2282271 -4.2401075 -4.2335315 -4.2130723 -4.1852341 -4.13908 -4.0772333 -4.0624194 -4.1120467 -4.1645746 -4.1835966 -4.1923981 -4.2225261][-4.1907253 -4.2135983 -4.2330813 -4.2430763 -4.2425985 -4.2313 -4.2116818 -4.178894 -4.1403623 -4.1384811 -4.1782165 -4.2105937 -4.21443 -4.2139192 -4.2358222][-4.19656 -4.2193565 -4.2370706 -4.2494879 -4.2531257 -4.2454934 -4.2240462 -4.193037 -4.1719427 -4.1850858 -4.2240224 -4.2433162 -4.2309265 -4.2161593 -4.229569][-4.200017 -4.2251081 -4.2441645 -4.2587724 -4.2620883 -4.250083 -4.2202091 -4.1888571 -4.1769428 -4.1994061 -4.2366905 -4.2438192 -4.2130265 -4.1816778 -4.190958][-4.1990876 -4.2277408 -4.2474394 -4.2597847 -4.2572141 -4.2389274 -4.2046576 -4.1760473 -4.1705832 -4.1965222 -4.2266736 -4.2213354 -4.173686 -4.1302562 -4.1411948][-4.209446 -4.2365847 -4.2506 -4.2548194 -4.2496133 -4.2308722 -4.1985183 -4.1730218 -4.1709437 -4.1950192 -4.2161036 -4.1981916 -4.138864 -4.0914879 -4.1086459]]...]
INFO - root - 2017-12-07 12:52:25.017938: step 10910, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 59h:40m:03s remains)
INFO - root - 2017-12-07 12:52:31.520240: step 10920, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:12m:20s remains)
INFO - root - 2017-12-07 12:52:38.368898: step 10930, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 59h:20m:23s remains)
INFO - root - 2017-12-07 12:52:45.192207: step 10940, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 56h:06m:48s remains)
INFO - root - 2017-12-07 12:52:52.008362: step 10950, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 62h:57m:22s remains)
INFO - root - 2017-12-07 12:52:58.902919: step 10960, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 62h:31m:15s remains)
INFO - root - 2017-12-07 12:53:05.655738: step 10970, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 61h:12m:30s remains)
INFO - root - 2017-12-07 12:53:12.371848: step 10980, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 58h:13m:00s remains)
INFO - root - 2017-12-07 12:53:19.198417: step 10990, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 57h:36m:19s remains)
INFO - root - 2017-12-07 12:53:26.008984: step 11000, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 61h:24m:40s remains)
2017-12-07 12:53:26.714767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32496 -4.3244004 -4.3246408 -4.3261571 -4.3251739 -4.3230128 -4.3222752 -4.3236113 -4.3258662 -4.3276196 -4.3292127 -4.3309484 -4.3339763 -4.3390684 -4.3447227][-4.3131089 -4.3113704 -4.3137822 -4.3183703 -4.3194666 -4.319479 -4.320025 -4.320363 -4.3209314 -4.3201013 -4.3185635 -4.3171144 -4.3188863 -4.3260903 -4.3347955][-4.2954683 -4.2946482 -4.299767 -4.30846 -4.3135161 -4.3152933 -4.3165059 -4.3154697 -4.3149881 -4.3129897 -4.3081732 -4.3005238 -4.2970691 -4.3045363 -4.317132][-4.2619171 -4.2627459 -4.2717457 -4.2862887 -4.2945976 -4.2967076 -4.294908 -4.2918563 -4.2901053 -4.2897229 -4.2865658 -4.2757874 -4.2675633 -4.2751789 -4.290947][-4.2178621 -4.2153935 -4.2246809 -4.2413278 -4.2493453 -4.2513418 -4.2450323 -4.2353249 -4.2317295 -4.235146 -4.2363691 -4.2294831 -4.2251153 -4.2374644 -4.2585778][-4.1756668 -4.1649876 -4.1679549 -4.181705 -4.1874313 -4.1862965 -4.1744008 -4.1530452 -4.1449485 -4.1507945 -4.1565 -4.15947 -4.1696439 -4.191514 -4.2227397][-4.1334147 -4.1145134 -4.108789 -4.1151791 -4.1121387 -4.0956583 -4.0691414 -4.036027 -4.0251985 -4.0399551 -4.0571132 -4.0806856 -4.1111069 -4.1458726 -4.1906137][-4.1135607 -4.0880976 -4.0706205 -4.0627174 -4.0409021 -4.0006242 -3.962234 -3.9306545 -3.9352829 -3.9705317 -4.004632 -4.0459933 -4.0909452 -4.1333447 -4.1842341][-4.1338563 -4.1036348 -4.0776882 -4.0559921 -4.0132928 -3.9567151 -3.9208026 -3.9151721 -3.9458773 -3.9966853 -4.0399656 -4.0872707 -4.1315751 -4.1674819 -4.206975][-4.182673 -4.1616297 -4.1410012 -4.1116157 -4.0617447 -4.0115352 -3.9956334 -4.0138483 -4.0516653 -4.0980825 -4.1378045 -4.1744514 -4.2047815 -4.225852 -4.246088][-4.2303452 -4.2207279 -4.210187 -4.1851721 -4.145443 -4.1147151 -4.1140633 -4.13615 -4.1632652 -4.1971154 -4.2285895 -4.250227 -4.2675014 -4.2742815 -4.2767677][-4.2586513 -4.253993 -4.252337 -4.2389669 -4.2175436 -4.2063603 -4.2095175 -4.2217035 -4.233223 -4.2534947 -4.2746825 -4.2858853 -4.2959318 -4.2945986 -4.2893829][-4.2779155 -4.2724586 -4.2723751 -4.2662706 -4.2552605 -4.248806 -4.2515931 -4.2576647 -4.2619252 -4.2703047 -4.28288 -4.2915812 -4.3010578 -4.2997928 -4.2948513][-4.2873764 -4.2813196 -4.2814689 -4.2798166 -4.2735314 -4.2702723 -4.2725997 -4.2752624 -4.2741036 -4.276484 -4.2844105 -4.2920108 -4.3028908 -4.3090596 -4.3101873][-4.2962728 -4.2908163 -4.2892389 -4.2896967 -4.2901325 -4.2906108 -4.2903776 -4.28892 -4.2860851 -4.2873168 -4.2935815 -4.3010836 -4.3113537 -4.3212 -4.3258257]]...]
INFO - root - 2017-12-07 12:53:33.429346: step 11010, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 57h:30m:51s remains)
INFO - root - 2017-12-07 12:53:40.223135: step 11020, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 59h:36m:26s remains)
INFO - root - 2017-12-07 12:53:46.978434: step 11030, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 62h:01m:15s remains)
INFO - root - 2017-12-07 12:53:53.776826: step 11040, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 64h:17m:52s remains)
INFO - root - 2017-12-07 12:54:00.633690: step 11050, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 56h:34m:24s remains)
INFO - root - 2017-12-07 12:54:07.455038: step 11060, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 56h:49m:56s remains)
INFO - root - 2017-12-07 12:54:14.257622: step 11070, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 63h:14m:57s remains)
INFO - root - 2017-12-07 12:54:21.137858: step 11080, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 65h:31m:52s remains)
INFO - root - 2017-12-07 12:54:27.784594: step 11090, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 57h:19m:39s remains)
INFO - root - 2017-12-07 12:54:34.690583: step 11100, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 62h:27m:10s remains)
2017-12-07 12:54:35.456304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1666641 -4.1600623 -4.1647167 -4.1923041 -4.2269831 -4.2488942 -4.2585139 -4.2633858 -4.2689629 -4.2611251 -4.2430296 -4.2288404 -4.2226658 -4.2176185 -4.1960454][-4.1792645 -4.1788936 -4.1843162 -4.2069821 -4.2361383 -4.2578096 -4.2684426 -4.2729993 -4.2778172 -4.2704158 -4.2559767 -4.2454391 -4.2400484 -4.234108 -4.2101784][-4.1961946 -4.1970129 -4.1982527 -4.212018 -4.2286625 -4.2447362 -4.2557535 -4.2638912 -4.2730632 -4.2723522 -4.2647543 -4.2627649 -4.2606063 -4.2542362 -4.2319665][-4.2027249 -4.2037787 -4.2006388 -4.2002797 -4.200016 -4.2064996 -4.2185097 -4.233017 -4.2470675 -4.2541628 -4.2568231 -4.2650623 -4.2706795 -4.2691941 -4.253541][-4.1974111 -4.1937995 -4.1877031 -4.1741962 -4.1550431 -4.1515141 -4.1682587 -4.1925263 -4.2129889 -4.2238708 -4.2336764 -4.2491617 -4.2626204 -4.271121 -4.2652774][-4.1930776 -4.1827874 -4.1724191 -4.1476903 -4.1145029 -4.1047053 -4.1259284 -4.1614294 -4.1870131 -4.19672 -4.2056718 -4.2191443 -4.2340679 -4.2513971 -4.25819][-4.1970267 -4.1834068 -4.1648893 -4.1298523 -4.0855889 -4.0725884 -4.099628 -4.1479216 -4.1769161 -4.1820774 -4.1859641 -4.19179 -4.203476 -4.2279773 -4.24422][-4.1971045 -4.1817255 -4.1593475 -4.1226711 -4.074882 -4.0575066 -4.0883551 -4.1459131 -4.1749024 -4.1764965 -4.1774893 -4.1785374 -4.186317 -4.2138691 -4.2351775][-4.1975241 -4.1858435 -4.1682162 -4.1385875 -4.0970831 -4.0740085 -4.0972672 -4.1483526 -4.1744518 -4.1754627 -4.1762571 -4.1791768 -4.184104 -4.2071166 -4.2284074][-4.1905427 -4.1902127 -4.1860271 -4.1725678 -4.1466994 -4.1216345 -4.1278872 -4.1594954 -4.1787491 -4.1795869 -4.1822128 -4.1878538 -4.1931114 -4.2115631 -4.2300029][-4.1670423 -4.1764188 -4.1888061 -4.1939445 -4.1849637 -4.1674805 -4.1635885 -4.1715789 -4.1741791 -4.1711774 -4.1779394 -4.1892295 -4.1967492 -4.2100325 -4.2222595][-4.1430383 -4.1510859 -4.1661658 -4.1803255 -4.1815987 -4.1735992 -4.1676955 -4.1589484 -4.144412 -4.1376176 -4.1505108 -4.1714005 -4.1847987 -4.1978331 -4.207078][-4.1311398 -4.1248126 -4.1293373 -4.1408958 -4.1459675 -4.1455517 -4.1442394 -4.1329832 -4.1165857 -4.1140914 -4.1334939 -4.1616263 -4.1807981 -4.1946044 -4.2039728][-4.1393228 -4.1172571 -4.107367 -4.1124077 -4.1149926 -4.1202631 -4.1295209 -4.12763 -4.1190286 -4.1213489 -4.1420193 -4.1693268 -4.1865377 -4.1975393 -4.2070141][-4.1584311 -4.1303387 -4.1109667 -4.1056352 -4.0976686 -4.1029854 -4.1250982 -4.13661 -4.1360073 -4.1414227 -4.1607924 -4.1831403 -4.194725 -4.2000365 -4.2088742]]...]
INFO - root - 2017-12-07 12:54:42.151908: step 11110, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.693 sec/batch; 61h:54m:30s remains)
INFO - root - 2017-12-07 12:54:48.718852: step 11120, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 57h:23m:35s remains)
INFO - root - 2017-12-07 12:54:55.471101: step 11130, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 57h:54m:00s remains)
INFO - root - 2017-12-07 12:55:02.265705: step 11140, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 61h:35m:32s remains)
INFO - root - 2017-12-07 12:55:09.000477: step 11150, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 65h:24m:31s remains)
INFO - root - 2017-12-07 12:55:15.891859: step 11160, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 62h:42m:52s remains)
INFO - root - 2017-12-07 12:55:22.706574: step 11170, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 56h:51m:51s remains)
INFO - root - 2017-12-07 12:55:29.613852: step 11180, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 60h:07m:16s remains)
INFO - root - 2017-12-07 12:55:36.563883: step 11190, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.757 sec/batch; 67h:35m:44s remains)
INFO - root - 2017-12-07 12:55:43.345696: step 11200, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 59h:09m:32s remains)
2017-12-07 12:55:43.988306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043858 -4.2127857 -4.2280135 -4.2441673 -4.24385 -4.2227921 -4.1870227 -4.1656775 -4.1768961 -4.2018127 -4.223577 -4.2365084 -4.2442288 -4.2574968 -4.2816353][-4.2106414 -4.2152181 -4.225244 -4.2398386 -4.2424097 -4.2231827 -4.1843953 -4.1624312 -4.176271 -4.2040534 -4.23046 -4.247447 -4.2569532 -4.2686563 -4.29009][-4.2032084 -4.2043381 -4.2123175 -4.2280259 -4.2326407 -4.2157164 -4.1737947 -4.147191 -4.1631765 -4.195714 -4.2267365 -4.2488971 -4.2632518 -4.277431 -4.2980866][-4.1864405 -4.1857691 -4.1943035 -4.2107048 -4.2146006 -4.1964765 -4.1478868 -4.1151152 -4.1324134 -4.1732411 -4.213541 -4.2433739 -4.2658744 -4.2852397 -4.306457][-4.169951 -4.1738553 -4.1832209 -4.1946826 -4.1883459 -4.1582403 -4.09568 -4.0497384 -4.0695043 -4.127964 -4.1865587 -4.2278008 -4.2594886 -4.2853065 -4.3094611][-4.1601729 -4.1728458 -4.1831074 -4.1830649 -4.1643929 -4.1235075 -4.0459862 -3.9786119 -3.9926198 -4.0712347 -4.1513019 -4.205893 -4.2464128 -4.2785387 -4.3067355][-4.1655407 -4.1850724 -4.1954379 -4.1877122 -4.165916 -4.1230674 -4.0406141 -3.9598293 -3.9634 -4.04706 -4.135633 -4.1964812 -4.2401352 -4.2741418 -4.3035178][-4.19388 -4.2097783 -4.2193747 -4.2184639 -4.2013922 -4.1623039 -4.0916953 -4.0213838 -4.0201921 -4.0875015 -4.1637282 -4.2158065 -4.2518973 -4.2803082 -4.3058548][-4.218802 -4.2272272 -4.2382622 -4.2467675 -4.2374215 -4.2015028 -4.1432576 -4.0905952 -4.0902262 -4.13954 -4.1983027 -4.2393589 -4.2689233 -4.2912107 -4.3117833][-4.2218785 -4.2274003 -4.24137 -4.2563844 -4.2543159 -4.221909 -4.1702938 -4.1284194 -4.1279073 -4.1639194 -4.2124686 -4.2483835 -4.2761831 -4.2967691 -4.3150454][-4.2145791 -4.2207088 -4.236814 -4.2561736 -4.2583218 -4.2295656 -4.1804414 -4.1440954 -4.146131 -4.1752105 -4.2147436 -4.2459264 -4.27286 -4.2944112 -4.3139882][-4.2065091 -4.2096868 -4.2253513 -4.245924 -4.2505779 -4.2281971 -4.1892595 -4.161366 -4.167964 -4.1922159 -4.2201328 -4.2439432 -4.2666721 -4.2879486 -4.3093643][-4.2084246 -4.20596 -4.2186522 -4.2387037 -4.246233 -4.23011 -4.1998687 -4.1822162 -4.1932158 -4.2118483 -4.2280564 -4.2439914 -4.2608132 -4.2801208 -4.3024969][-4.2253509 -4.2186637 -4.2243052 -4.2405925 -4.248208 -4.2379622 -4.214097 -4.2037034 -4.2181821 -4.2323308 -4.2406607 -4.2509894 -4.2626634 -4.277926 -4.2978878][-4.2548847 -4.2468486 -4.2476029 -4.257359 -4.2604308 -4.2508783 -4.2285166 -4.2193322 -4.2349534 -4.2489185 -4.2568426 -4.2655239 -4.2738647 -4.2832689 -4.2968435]]...]
INFO - root - 2017-12-07 12:55:50.710803: step 11210, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:25m:42s remains)
INFO - root - 2017-12-07 12:55:57.458176: step 11220, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 61h:27m:49s remains)
INFO - root - 2017-12-07 12:56:04.297367: step 11230, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 62h:45m:44s remains)
INFO - root - 2017-12-07 12:56:11.008165: step 11240, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:46m:45s remains)
INFO - root - 2017-12-07 12:56:17.765819: step 11250, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:45m:30s remains)
INFO - root - 2017-12-07 12:56:24.617997: step 11260, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 63h:34m:51s remains)
INFO - root - 2017-12-07 12:56:31.450672: step 11270, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 62h:25m:33s remains)
INFO - root - 2017-12-07 12:56:38.193513: step 11280, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 59h:54m:29s remains)
INFO - root - 2017-12-07 12:56:44.918122: step 11290, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 59h:32m:29s remains)
INFO - root - 2017-12-07 12:56:51.699954: step 11300, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 61h:50m:51s remains)
2017-12-07 12:56:52.480283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1913586 -4.1744671 -4.168519 -4.1668787 -4.1568289 -4.14975 -4.1509457 -4.1670885 -4.174437 -4.1644578 -4.138092 -4.0942187 -4.0655212 -4.0906978 -4.1244845][-4.1677666 -4.1498704 -4.1466737 -4.1453447 -4.1331034 -4.1246853 -4.1305361 -4.1502643 -4.1618304 -4.1580706 -4.1308179 -4.0780678 -4.049243 -4.0838943 -4.121758][-4.1318369 -4.1200538 -4.1244755 -4.123847 -4.1143985 -4.1101227 -4.1203628 -4.1389546 -4.1503744 -4.1528935 -4.1312056 -4.080399 -4.0574617 -4.0957747 -4.132937][-4.1083961 -4.1025505 -4.112854 -4.1128244 -4.1044455 -4.1024351 -4.1107726 -4.1231885 -4.1303797 -4.136704 -4.1251187 -4.0912538 -4.0822129 -4.1204243 -4.1526256][-4.102891 -4.1033959 -4.1159272 -4.1140485 -4.103436 -4.093956 -4.0914111 -4.0924354 -4.0955305 -4.1046581 -4.1077552 -4.09713 -4.1053028 -4.1450925 -4.1705613][-4.103529 -4.1079407 -4.1209764 -4.1175542 -4.1030626 -4.0789466 -4.0606742 -4.05172 -4.05491 -4.0688462 -4.0797062 -4.0776682 -4.0954742 -4.1398745 -4.1652246][-4.114562 -4.1209631 -4.1316309 -4.1202278 -4.0945706 -4.0539322 -4.0210638 -4.0047894 -4.0126605 -4.026638 -4.0295463 -4.0156536 -4.0309896 -4.0848308 -4.1184568][-4.1318712 -4.1389675 -4.1443648 -4.12284 -4.0855293 -4.0341315 -3.9941785 -3.9779632 -3.98987 -4.0026965 -3.9938228 -3.9607236 -3.9652183 -4.026021 -4.0676994][-4.1471415 -4.1499386 -4.1482687 -4.1208386 -4.0820975 -4.0306964 -3.9924188 -3.9834168 -4.0029488 -4.02122 -4.017971 -3.9907165 -3.9912798 -4.040627 -4.0763683][-4.1481347 -4.1437311 -4.1384373 -4.1135712 -4.0841627 -4.042944 -4.0128121 -4.0140858 -4.0437164 -4.0696812 -4.0730796 -4.0530796 -4.0535641 -4.0935268 -4.1204882][-4.1381464 -4.1313353 -4.1313648 -4.1181231 -4.0979505 -4.0687866 -4.046216 -4.0500484 -4.0791516 -4.1049151 -4.1134353 -4.1024666 -4.1062965 -4.138886 -4.1594796][-4.1261473 -4.1207523 -4.1245155 -4.1241846 -4.1128025 -4.0986977 -4.084002 -4.0827336 -4.0995703 -4.1143169 -4.1202383 -4.1154361 -4.1238766 -4.1543245 -4.1758313][-4.1251664 -4.1195288 -4.1223178 -4.126543 -4.1253457 -4.1268311 -4.1198459 -4.11587 -4.1210322 -4.1235847 -4.1190066 -4.1115317 -4.1213255 -4.1520348 -4.1757588][-4.1252685 -4.1172233 -4.1192031 -4.1275353 -4.1387873 -4.1531448 -4.1514664 -4.1454077 -4.147469 -4.1476765 -4.1370497 -4.1242895 -4.1312985 -4.1590776 -4.1783228][-4.1249566 -4.116725 -4.1211896 -4.1362233 -4.1525621 -4.1716962 -4.1711512 -4.1649365 -4.1684361 -4.1730385 -4.1688652 -4.1612287 -4.1677713 -4.1877284 -4.1946769]]...]
INFO - root - 2017-12-07 12:56:59.139037: step 11310, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 58h:31m:09s remains)
INFO - root - 2017-12-07 12:57:05.692553: step 11320, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 58h:30m:37s remains)
INFO - root - 2017-12-07 12:57:12.443512: step 11330, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.675 sec/batch; 60h:10m:43s remains)
INFO - root - 2017-12-07 12:57:19.233286: step 11340, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 60h:42m:04s remains)
INFO - root - 2017-12-07 12:57:25.978249: step 11350, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 62h:45m:36s remains)
INFO - root - 2017-12-07 12:57:32.713510: step 11360, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 62h:05m:06s remains)
INFO - root - 2017-12-07 12:57:39.467644: step 11370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 60h:30m:30s remains)
INFO - root - 2017-12-07 12:57:46.364748: step 11380, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 59h:27m:22s remains)
INFO - root - 2017-12-07 12:57:53.103565: step 11390, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 65h:50m:17s remains)
INFO - root - 2017-12-07 12:57:59.833042: step 11400, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 59h:16m:17s remains)
2017-12-07 12:58:00.605029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2667351 -4.280077 -4.2943215 -4.2975326 -4.2996707 -4.3017969 -4.302599 -4.3029594 -4.300704 -4.2970953 -4.2915163 -4.2818952 -4.2635417 -4.2449446 -4.2356229][-4.2230582 -4.2407036 -4.2640185 -4.2751884 -4.2828841 -4.2872868 -4.2878237 -4.287035 -4.2824979 -4.2751918 -4.2610836 -4.2371826 -4.2044539 -4.1716485 -4.1556163][-4.1988683 -4.2121253 -4.2364945 -4.2499409 -4.257935 -4.2619133 -4.2628713 -4.2650933 -4.2652855 -4.2628026 -4.2476449 -4.2171597 -4.1777959 -4.1351829 -4.1098871][-4.2027335 -4.2058163 -4.2219629 -4.2270389 -4.2237048 -4.2202482 -4.2179236 -4.2235084 -4.234364 -4.2412658 -4.2323046 -4.2106676 -4.18161 -4.14159 -4.1134033][-4.219728 -4.2157497 -4.2150316 -4.1981945 -4.173121 -4.1527333 -4.1387405 -4.1457276 -4.1668315 -4.1825137 -4.18397 -4.1819782 -4.1759448 -4.1492958 -4.1273355][-4.2272868 -4.2179508 -4.1939292 -4.1456418 -4.0931988 -4.0468559 -4.0173492 -4.0359015 -4.0795107 -4.1098971 -4.1295905 -4.1510115 -4.1647964 -4.1530042 -4.1414094][-4.227706 -4.2075381 -4.1580215 -4.0782237 -3.9969411 -3.9129586 -3.8612642 -3.9015231 -3.977663 -4.0321865 -4.0778112 -4.1263423 -4.160851 -4.1638274 -4.16222][-4.2267647 -4.1973667 -4.125392 -4.0168834 -3.9066038 -3.7855809 -3.7157054 -3.7866375 -3.8978152 -3.9770281 -4.0436735 -4.1104183 -4.1594396 -4.1749129 -4.1811576][-4.2348151 -4.2048621 -4.1287394 -4.0200839 -3.9164591 -3.8092153 -3.7562296 -3.83259 -3.9378049 -4.0134296 -4.0750146 -4.1323767 -4.1754618 -4.1900425 -4.1979852][-4.2523079 -4.2353725 -4.1780243 -4.0949945 -4.0282965 -3.9663851 -3.9462295 -3.9992609 -4.0599961 -4.104085 -4.1394138 -4.1704774 -4.1931181 -4.1974378 -4.1999726][-4.2645526 -4.2656426 -4.2356453 -4.1838884 -4.1488843 -4.1198335 -4.121634 -4.1522455 -4.1758752 -4.1913571 -4.1999574 -4.2047157 -4.202899 -4.1887012 -4.1798849][-4.26643 -4.2850304 -4.2813554 -4.2555728 -4.2364831 -4.222147 -4.2270079 -4.2389736 -4.2382431 -4.2350459 -4.2277369 -4.2200823 -4.20218 -4.1715493 -4.1524549][-4.2633395 -4.2873921 -4.2991815 -4.2928433 -4.2842879 -4.2769423 -4.278832 -4.278512 -4.2635622 -4.2487769 -4.2369981 -4.2235546 -4.1978211 -4.1596565 -4.1336355][-4.2639756 -4.2855458 -4.3026085 -4.3071547 -4.3046746 -4.2981806 -4.2924814 -4.282238 -4.2604747 -4.2441788 -4.2341948 -4.2174253 -4.1893568 -4.1539474 -4.1299644][-4.2752461 -4.2866688 -4.298173 -4.3032656 -4.3020039 -4.2968349 -4.2887163 -4.2765093 -4.2575455 -4.2446904 -4.2379832 -4.22263 -4.2013354 -4.178997 -4.1664934]]...]
INFO - root - 2017-12-07 12:58:07.299039: step 11410, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 61h:58m:18s remains)
INFO - root - 2017-12-07 12:58:13.893509: step 11420, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 59h:57m:05s remains)
INFO - root - 2017-12-07 12:58:20.689639: step 11430, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 60h:29m:29s remains)
INFO - root - 2017-12-07 12:58:27.488379: step 11440, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 60h:09m:02s remains)
INFO - root - 2017-12-07 12:58:34.354401: step 11450, loss = 2.02, batch loss = 1.97 (11.8 examples/sec; 0.681 sec/batch; 60h:41m:36s remains)
INFO - root - 2017-12-07 12:58:41.224011: step 11460, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 63h:37m:10s remains)
INFO - root - 2017-12-07 12:58:48.055223: step 11470, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 64h:07m:48s remains)
INFO - root - 2017-12-07 12:58:54.890298: step 11480, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 56h:01m:55s remains)
INFO - root - 2017-12-07 12:59:01.691967: step 11490, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 57h:10m:11s remains)
INFO - root - 2017-12-07 12:59:08.584754: step 11500, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 61h:33m:03s remains)
2017-12-07 12:59:09.276002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3107791 -4.309413 -4.3016696 -4.29208 -4.2781496 -4.2671766 -4.2609982 -4.2595906 -4.2580934 -4.2540956 -4.2538543 -4.2558446 -4.2617059 -4.2705436 -4.2806396][-4.3071356 -4.3038111 -4.2951722 -4.2842588 -4.2658458 -4.2495089 -4.2412019 -4.2396035 -4.2402916 -4.24096 -4.2487431 -4.2576313 -4.2661195 -4.273303 -4.2808089][-4.3025007 -4.2970228 -4.2862973 -4.2728567 -4.2502503 -4.2271609 -4.2124715 -4.2092648 -4.2163472 -4.2276359 -4.2491403 -4.2684565 -4.2795796 -4.2826929 -4.2840219][-4.3016734 -4.2949629 -4.2811027 -4.26315 -4.2328558 -4.1987925 -4.175024 -4.17064 -4.1850576 -4.2099609 -4.2472758 -4.27742 -4.290987 -4.2910323 -4.2894711][-4.3045211 -4.2973919 -4.2788806 -4.2531371 -4.2096772 -4.159368 -4.1243081 -4.1210232 -4.1464915 -4.1866145 -4.2361145 -4.2724056 -4.2876921 -4.2891603 -4.2885914][-4.2996087 -4.2849345 -4.2552371 -4.2163873 -4.1577387 -4.0872941 -4.0352411 -4.0376749 -4.0859222 -4.1471677 -4.208303 -4.2482777 -4.2672105 -4.2722669 -4.2706461][-4.2782664 -4.2487192 -4.2035365 -4.1501818 -4.0757346 -3.9773421 -3.8923359 -3.8992422 -3.9893737 -4.0846448 -4.1589193 -4.2053065 -4.2342496 -4.2444482 -4.2420449][-4.2436738 -4.1979861 -4.1342535 -4.0671039 -3.9812393 -3.8634803 -3.7479191 -3.7560189 -3.8882318 -4.0163016 -4.1034694 -4.1612959 -4.2029147 -4.2219992 -4.2220006][-4.2154732 -4.1611242 -4.0900245 -4.0208969 -3.944958 -3.850606 -3.758702 -3.7727482 -3.8939939 -4.0136428 -4.0927 -4.1506948 -4.1995287 -4.2282085 -4.2327223][-4.2156873 -4.1646585 -4.1027145 -4.04913 -3.998009 -3.9472466 -3.9077787 -3.9264309 -4.0035267 -4.0860367 -4.1415749 -4.1863031 -4.2293077 -4.2587256 -4.2658978][-4.2407079 -4.1969109 -4.1459069 -4.1080642 -4.0815349 -4.0633583 -4.060225 -4.0827222 -4.1263342 -4.1750846 -4.21069 -4.2440753 -4.2769346 -4.2991424 -4.303463][-4.2730088 -4.2387714 -4.1992884 -4.1748409 -4.1651487 -4.16567 -4.1784172 -4.1986785 -4.2212429 -4.2444043 -4.264534 -4.2887225 -4.3127923 -4.327425 -4.3297071][-4.2995214 -4.2785082 -4.2535625 -4.2395263 -4.2388649 -4.2458014 -4.2595243 -4.27342 -4.2845917 -4.2935319 -4.3036537 -4.3184485 -4.3328791 -4.3410277 -4.3414035][-4.3159294 -4.3083963 -4.2960162 -4.2895236 -4.2928481 -4.3003273 -4.3104277 -4.3183846 -4.3217359 -4.3229337 -4.3257661 -4.3333611 -4.3411984 -4.3449717 -4.3429804][-4.3251376 -4.3240767 -4.3176565 -4.3132577 -4.3152037 -4.3203225 -4.327013 -4.331089 -4.3321586 -4.3317885 -4.3324189 -4.3357797 -4.3401461 -4.3421469 -4.3407779]]...]
INFO - root - 2017-12-07 12:59:16.165483: step 11510, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 58h:07m:15s remains)
INFO - root - 2017-12-07 12:59:22.763355: step 11520, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 57h:49m:57s remains)
INFO - root - 2017-12-07 12:59:29.635124: step 11530, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 64h:14m:12s remains)
INFO - root - 2017-12-07 12:59:36.472011: step 11540, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 61h:14m:11s remains)
INFO - root - 2017-12-07 12:59:43.312474: step 11550, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 62h:26m:11s remains)
INFO - root - 2017-12-07 12:59:50.158227: step 11560, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 58h:48m:37s remains)
INFO - root - 2017-12-07 12:59:56.960150: step 11570, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.674 sec/batch; 60h:06m:19s remains)
INFO - root - 2017-12-07 13:00:03.786984: step 11580, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.705 sec/batch; 62h:52m:57s remains)
INFO - root - 2017-12-07 13:00:10.588819: step 11590, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 62h:38m:36s remains)
INFO - root - 2017-12-07 13:00:17.376153: step 11600, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 60h:01m:34s remains)
2017-12-07 13:00:18.054304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.319098 -4.3174434 -4.316206 -4.3180485 -4.3225532 -4.3268938 -4.3314948 -4.3357463 -4.3402157 -4.3447275 -4.3467789 -4.3452783 -4.3429995 -4.3410068 -4.3409076][-4.2868419 -4.2834296 -4.2784772 -4.2774873 -4.2795281 -4.2857952 -4.2928338 -4.2991552 -4.3051662 -4.3122578 -4.3177309 -4.3199472 -4.3211641 -4.3220463 -4.3246713][-4.2389612 -4.2317982 -4.2286358 -4.2280226 -4.2296166 -4.2371111 -4.2461929 -4.2536011 -4.2618246 -4.2721019 -4.2818503 -4.2895851 -4.2952385 -4.3008814 -4.3089523][-4.181653 -4.1673775 -4.1679235 -4.1724367 -4.1778922 -4.1860452 -4.1943517 -4.2017117 -4.2113929 -4.2264233 -4.2394075 -4.2528033 -4.2631693 -4.271162 -4.2838469][-4.1385803 -4.1167126 -4.1136255 -4.1185942 -4.1232929 -4.1266789 -4.134469 -4.1416626 -4.154109 -4.1775103 -4.1964064 -4.2148805 -4.2276754 -4.2371206 -4.2566266][-4.1115074 -4.0823617 -4.0697088 -4.067224 -4.0623159 -4.0543242 -4.0562344 -4.0611062 -4.07952 -4.1137843 -4.1439385 -4.1702309 -4.1918263 -4.2073493 -4.2328882][-4.0947371 -4.0587997 -4.0366669 -4.0178771 -3.9961071 -3.9704936 -3.951674 -3.9448254 -3.9708438 -4.0232906 -4.0750508 -4.1204243 -4.1591635 -4.1869164 -4.2173295][-4.099401 -4.0650377 -4.0397744 -4.0095315 -3.9714336 -3.9218106 -3.8687584 -3.8394222 -3.8708026 -3.9384856 -4.0098095 -4.0757408 -4.1311355 -4.1698427 -4.2063732][-4.1176839 -4.0941596 -4.077282 -4.0486741 -4.0074553 -3.9478614 -3.8774354 -3.834039 -3.8604004 -3.92042 -3.9891849 -4.0564747 -4.1171279 -4.1625447 -4.2010064][-4.1343288 -4.1180944 -4.11252 -4.0950131 -4.0595093 -4.0054855 -3.9410021 -3.9004416 -3.9123635 -3.9525702 -4.0050721 -4.0631542 -4.1211433 -4.1663256 -4.2020817][-4.1575551 -4.1421371 -4.1414771 -4.1321559 -4.1048408 -4.0635886 -4.0120139 -3.9785607 -3.9776795 -3.9989688 -4.0348206 -4.0811577 -4.1295853 -4.1701951 -4.2046385][-4.1913786 -4.17717 -4.1764116 -4.1702657 -4.1504612 -4.123384 -4.0866947 -4.0625148 -4.0562873 -4.06528 -4.0867262 -4.11919 -4.1546478 -4.1881123 -4.2205567][-4.2335515 -4.22512 -4.2256317 -4.2223797 -4.2096314 -4.191999 -4.1646514 -4.1455083 -4.1363297 -4.1386108 -4.1524243 -4.1720948 -4.1964664 -4.2239609 -4.2520256][-4.2814989 -4.2804222 -4.2814355 -4.27889 -4.2701406 -4.2584643 -4.2388215 -4.2223959 -4.2141953 -4.2151246 -4.2240934 -4.2350945 -4.2505493 -4.2695928 -4.2906914][-4.3182931 -4.3218036 -4.322145 -4.3190064 -4.3121104 -4.3050613 -4.293726 -4.2836494 -4.2802329 -4.2819695 -4.2879796 -4.2937517 -4.3013749 -4.3114829 -4.3251691]]...]
INFO - root - 2017-12-07 13:00:24.861894: step 11610, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.681 sec/batch; 60h:39m:59s remains)
INFO - root - 2017-12-07 13:00:31.497842: step 11620, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 59h:06m:18s remains)
INFO - root - 2017-12-07 13:00:38.367706: step 11630, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:10m:19s remains)
INFO - root - 2017-12-07 13:00:45.167990: step 11640, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 57h:22m:37s remains)
INFO - root - 2017-12-07 13:00:52.015707: step 11650, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 58h:14m:12s remains)
INFO - root - 2017-12-07 13:00:58.836427: step 11660, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 63h:33m:12s remains)
INFO - root - 2017-12-07 13:01:05.624339: step 11670, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.674 sec/batch; 60h:06m:12s remains)
INFO - root - 2017-12-07 13:01:12.504699: step 11680, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 59h:56m:25s remains)
INFO - root - 2017-12-07 13:01:19.366367: step 11690, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 57h:12m:04s remains)
INFO - root - 2017-12-07 13:01:26.102909: step 11700, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 61h:12m:37s remains)
2017-12-07 13:01:26.837978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3059244 -4.3077288 -4.3092322 -4.306879 -4.2960277 -4.2771969 -4.2500243 -4.2351041 -4.2365913 -4.2501211 -4.2672009 -4.2835784 -4.2964458 -4.304265 -4.3114886][-4.2709436 -4.2710834 -4.2716 -4.2680769 -4.2544684 -4.2302241 -4.1936045 -4.1751018 -4.1791859 -4.1967154 -4.2235985 -4.2510643 -4.2717085 -4.2847009 -4.2959728][-4.2311735 -4.2353191 -4.2359228 -4.226759 -4.2065725 -4.1775336 -4.1353416 -4.1211019 -4.1316652 -4.1545243 -4.191741 -4.2263432 -4.2525954 -4.271718 -4.2861204][-4.1890016 -4.1985283 -4.1980996 -4.1802626 -4.1531692 -4.1148763 -4.0649385 -4.0663424 -4.0992737 -4.1299291 -4.1716704 -4.2095404 -4.2408657 -4.2668886 -4.2833853][-4.1383319 -4.1565456 -4.1610527 -4.1407471 -4.1065578 -4.0463624 -3.9696064 -3.9946277 -4.0692883 -4.1193933 -4.16529 -4.2047706 -4.2363319 -4.263586 -4.2808156][-4.0746446 -4.1071653 -4.1201138 -4.0988336 -4.0539632 -3.9565601 -3.819644 -3.862119 -4.0025311 -4.0847335 -4.1434026 -4.1833754 -4.2159672 -4.2474127 -4.2688904][-3.9994512 -4.0454645 -4.0720091 -4.05419 -3.9945614 -3.854799 -3.6371884 -3.6873953 -3.8980031 -4.0122867 -4.0886846 -4.1383972 -4.1802621 -4.2229109 -4.2535582][-3.9735756 -4.0239668 -4.0659418 -4.0536928 -3.99984 -3.8779604 -3.6854267 -3.7174451 -3.9010162 -3.9998271 -4.0725827 -4.1256013 -4.1677766 -4.2145677 -4.2520108][-4.0172749 -4.0567183 -4.1049581 -4.1022139 -4.0664015 -4.0073633 -3.9100032 -3.915904 -4.0058646 -4.0584226 -4.1098576 -4.1543202 -4.1879516 -4.2302332 -4.2662239][-4.0982151 -4.1239347 -4.1667123 -4.1692486 -4.1497679 -4.127645 -4.0853314 -4.0765839 -4.1107655 -4.1394839 -4.1738663 -4.20812 -4.23189 -4.2628932 -4.2905846][-4.17616 -4.1923771 -4.2217298 -4.2262182 -4.2147427 -4.2055893 -4.1827545 -4.1663408 -4.1737556 -4.1940331 -4.2220545 -4.2509737 -4.2716217 -4.2956839 -4.3129649][-4.2313275 -4.2420597 -4.2592092 -4.2646532 -4.2616882 -4.2608562 -4.2450433 -4.2216311 -4.2143283 -4.22537 -4.247612 -4.2704549 -4.2930441 -4.3169184 -4.3287759][-4.276896 -4.2854838 -4.2952952 -4.3021388 -4.3006768 -4.2964096 -4.2802157 -4.2542658 -4.242835 -4.2510228 -4.2737169 -4.2946329 -4.3147655 -4.3315539 -4.3379569][-4.3055964 -4.3149166 -4.3240771 -4.332139 -4.3286748 -4.3153248 -4.2965775 -4.2750273 -4.2654815 -4.2725344 -4.2934313 -4.3135753 -4.3298931 -4.3405571 -4.3433228][-4.3214946 -4.3278618 -4.3369231 -4.3446488 -4.3410063 -4.3264394 -4.309155 -4.29479 -4.2877069 -4.2918711 -4.3064933 -4.321979 -4.3340821 -4.341434 -4.3437696]]...]
INFO - root - 2017-12-07 13:01:33.351072: step 11710, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 57h:15m:53s remains)
INFO - root - 2017-12-07 13:01:40.060338: step 11720, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 61h:34m:19s remains)
INFO - root - 2017-12-07 13:01:46.876894: step 11730, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 63h:31m:25s remains)
INFO - root - 2017-12-07 13:01:53.620572: step 11740, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 62h:28m:46s remains)
INFO - root - 2017-12-07 13:02:00.526945: step 11750, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 59h:59m:48s remains)
INFO - root - 2017-12-07 13:02:07.289310: step 11760, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 55h:51m:01s remains)
INFO - root - 2017-12-07 13:02:14.131055: step 11770, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 62h:05m:37s remains)
INFO - root - 2017-12-07 13:02:20.943357: step 11780, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 63h:17m:31s remains)
INFO - root - 2017-12-07 13:02:27.725915: step 11790, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 60h:59m:54s remains)
INFO - root - 2017-12-07 13:02:34.448419: step 11800, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 57h:25m:34s remains)
2017-12-07 13:02:35.153104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393928 -4.2496467 -4.2466478 -4.2065296 -4.1718407 -4.1692777 -4.1575274 -4.1094351 -4.0718021 -4.1042342 -4.160902 -4.1999283 -4.2056141 -4.181159 -4.1402197][-4.2201595 -4.2266078 -4.2169056 -4.1694121 -4.1247759 -4.1145859 -4.0853524 -4.0131583 -3.9608836 -4.0097947 -4.1043119 -4.17616 -4.1955643 -4.1661768 -4.1042366][-4.1985588 -4.1991396 -4.1837282 -4.1341195 -4.0860267 -4.0715585 -4.0329666 -3.9485004 -3.8892457 -3.9498847 -4.0692534 -4.1601338 -4.1875534 -4.1566324 -4.0851941][-4.2086115 -4.1941295 -4.1691217 -4.12097 -4.0819383 -4.0722985 -4.0398316 -3.9656398 -3.9179866 -3.9735346 -4.0781088 -4.161057 -4.1859517 -4.16362 -4.1118727][-4.2300897 -4.201 -4.1704717 -4.1290507 -4.101346 -4.0991983 -4.0796304 -4.0290155 -3.9971271 -4.0269156 -4.0939693 -4.1590528 -4.1871848 -4.1853037 -4.1684556][-4.2488694 -4.2174616 -4.1852098 -4.1491365 -4.1252327 -4.1172228 -4.1018023 -4.064528 -4.0286107 -4.027801 -4.0704565 -4.1307006 -4.1721 -4.1934457 -4.211133][-4.2647667 -4.2365413 -4.2035127 -4.1663284 -4.1282926 -4.096952 -4.0728574 -4.0400424 -3.9993286 -3.9809594 -4.0189877 -4.0857754 -4.1426558 -4.1859922 -4.2279673][-4.2736068 -4.24613 -4.2094164 -4.1631455 -4.1069984 -4.0548205 -4.0181193 -3.985909 -3.946712 -3.9315946 -3.976259 -4.0508857 -4.1173587 -4.1713314 -4.2188268][-4.2616057 -4.2358947 -4.2014718 -4.1552105 -4.096 -4.0363841 -3.99035 -3.9567342 -3.9261031 -3.9268994 -3.9752879 -4.0460591 -4.1109524 -4.1621857 -4.2025909][-4.2296715 -4.2102566 -4.188797 -4.1587129 -4.1153 -4.0641122 -4.0164576 -3.9796114 -3.9574075 -3.9715707 -4.01793 -4.0757952 -4.1248503 -4.1645522 -4.1870227][-4.20331 -4.1985269 -4.1949778 -4.1830168 -4.1591654 -4.126245 -4.086267 -4.0512991 -4.0351372 -4.047967 -4.0773454 -4.1107812 -4.1392608 -4.1624517 -4.1700673][-4.2053313 -4.2119279 -4.2192063 -4.2174606 -4.2093024 -4.193934 -4.16842 -4.1392045 -4.1251841 -4.1299982 -4.14027 -4.1516643 -4.1605892 -4.1683655 -4.1617517][-4.2163277 -4.225955 -4.236064 -4.2391572 -4.2402315 -4.2372079 -4.2245936 -4.2014289 -4.1886191 -4.1892724 -4.1909904 -4.19174 -4.1886778 -4.1816797 -4.1631818][-4.2181406 -4.22398 -4.2322097 -4.2396436 -4.2473273 -4.2502728 -4.242312 -4.2226677 -4.2150626 -4.2232294 -4.2299795 -4.2302637 -4.2225366 -4.207315 -4.1836829][-4.214643 -4.2130127 -4.2161512 -4.2252312 -4.2368088 -4.2415504 -4.2325983 -4.2149453 -4.2157516 -4.2397623 -4.2628279 -4.2713475 -4.2639256 -4.2436137 -4.2154679]]...]
INFO - root - 2017-12-07 13:02:41.979011: step 11810, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 63h:11m:42s remains)
INFO - root - 2017-12-07 13:02:48.604508: step 11820, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 62h:40m:48s remains)
INFO - root - 2017-12-07 13:02:55.350887: step 11830, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 57h:21m:44s remains)
INFO - root - 2017-12-07 13:03:02.150564: step 11840, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 57h:28m:20s remains)
INFO - root - 2017-12-07 13:03:09.050102: step 11850, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 63h:15m:22s remains)
INFO - root - 2017-12-07 13:03:15.831031: step 11860, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 63h:53m:57s remains)
INFO - root - 2017-12-07 13:03:22.699064: step 11870, loss = 2.03, batch loss = 1.98 (11.6 examples/sec; 0.692 sec/batch; 61h:40m:11s remains)
INFO - root - 2017-12-07 13:03:29.449929: step 11880, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 57h:27m:41s remains)
INFO - root - 2017-12-07 13:03:36.166515: step 11890, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:12m:27s remains)
INFO - root - 2017-12-07 13:03:42.918881: step 11900, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 62h:21m:46s remains)
2017-12-07 13:03:43.697041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3016653 -4.2871761 -4.278204 -4.2803192 -4.2801461 -4.2686644 -4.2557497 -4.2345862 -4.1969419 -4.1660676 -4.1634183 -4.1873007 -4.2277665 -4.2664175 -4.2940283][-4.3130355 -4.3010993 -4.2875972 -4.2801938 -4.2725439 -4.2527385 -4.2318668 -4.2139683 -4.185348 -4.1681418 -4.1736784 -4.1971602 -4.2382088 -4.2784963 -4.3015523][-4.3125386 -4.3036733 -4.2908697 -4.2810125 -4.2678289 -4.2366304 -4.2009468 -4.1762524 -4.1525474 -4.1526971 -4.1695127 -4.1964908 -4.2402048 -4.2800941 -4.2989092][-4.2927837 -4.2835636 -4.27185 -4.2629023 -4.247961 -4.20811 -4.1582246 -4.1200776 -4.0986738 -4.1149311 -4.1522622 -4.1872149 -4.2343121 -4.2736 -4.287292][-4.268991 -4.25187 -4.23661 -4.2229514 -4.1992869 -4.148479 -4.082921 -4.0348072 -4.0298796 -4.0708971 -4.1283479 -4.1713791 -4.2207932 -4.2618022 -4.2729878][-4.2567763 -4.2337217 -4.2147202 -4.1892319 -4.1457553 -4.0730791 -3.9775465 -3.9102919 -3.9276516 -4.0096855 -4.0903506 -4.1493239 -4.205296 -4.2506733 -4.2627225][-4.2452631 -4.2262907 -4.2097335 -4.1746573 -4.1170783 -4.0269766 -3.903698 -3.8110709 -3.8452709 -3.9631553 -4.06128 -4.1306481 -4.1895003 -4.2382336 -4.2575068][-4.2377338 -4.22927 -4.2190852 -4.1802487 -4.1199064 -4.03352 -3.9073932 -3.8078263 -3.8477437 -3.9728744 -4.0649071 -4.13005 -4.183454 -4.2309785 -4.2585988][-4.2521105 -4.2544384 -4.24944 -4.2143483 -4.1596913 -4.0869842 -3.9793856 -3.8893263 -3.9206507 -4.0249991 -4.0938897 -4.1450486 -4.1891451 -4.2308426 -4.2631016][-4.2719584 -4.2821937 -4.28269 -4.2561688 -4.2154474 -4.1631413 -4.0844145 -4.0113678 -4.0262504 -4.097003 -4.1406903 -4.1748557 -4.2094483 -4.2426114 -4.2720222][-4.2925534 -4.3034391 -4.3038988 -4.2865376 -4.2622361 -4.2297697 -4.1765633 -4.1218824 -4.1301804 -4.1783981 -4.204463 -4.2236757 -4.2478795 -4.2690964 -4.2875185][-4.3170648 -4.3206968 -4.3165021 -4.3044515 -4.2893677 -4.2700071 -4.2372866 -4.19982 -4.2069225 -4.2407737 -4.2588634 -4.2674093 -4.2830396 -4.2929282 -4.2997746][-4.3334045 -4.3279939 -4.3204474 -4.314528 -4.3084173 -4.2992353 -4.28109 -4.256659 -4.2585268 -4.27511 -4.2836132 -4.2844715 -4.29707 -4.3028169 -4.3045306][-4.3293056 -4.3195314 -4.3131847 -4.3150744 -4.3176594 -4.3178391 -4.3123789 -4.2999196 -4.2932849 -4.2890625 -4.2826633 -4.2790937 -4.2936268 -4.301403 -4.3045025][-4.3247356 -4.318831 -4.3144517 -4.3171239 -4.322072 -4.325923 -4.3231 -4.314218 -4.3007298 -4.2818146 -4.2651935 -4.2622595 -4.2829337 -4.29663 -4.3033018]]...]
INFO - root - 2017-12-07 13:03:50.435742: step 11910, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 60h:43m:24s remains)
INFO - root - 2017-12-07 13:03:57.015944: step 11920, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 57h:42m:47s remains)
INFO - root - 2017-12-07 13:04:03.881161: step 11930, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 64h:13m:21s remains)
INFO - root - 2017-12-07 13:04:10.813283: step 11940, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.751 sec/batch; 66h:51m:32s remains)
INFO - root - 2017-12-07 13:04:17.625672: step 11950, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 59h:15m:56s remains)
INFO - root - 2017-12-07 13:04:24.491720: step 11960, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 59h:16m:27s remains)
INFO - root - 2017-12-07 13:04:31.345916: step 11970, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:13m:11s remains)
INFO - root - 2017-12-07 13:04:38.139397: step 11980, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 61h:32m:49s remains)
INFO - root - 2017-12-07 13:04:45.019547: step 11990, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:24m:58s remains)
INFO - root - 2017-12-07 13:04:51.798028: step 12000, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 61h:16m:28s remains)
2017-12-07 13:04:52.526931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1843648 -4.1907573 -4.1964331 -4.2104931 -4.2293787 -4.2398376 -4.2298975 -4.206676 -4.1861243 -4.1875577 -4.2020874 -4.2135453 -4.2140188 -4.2023106 -4.189075][-4.154213 -4.1503572 -4.1517382 -4.1741385 -4.2065682 -4.2280774 -4.2234159 -4.2031078 -4.1866646 -4.1899772 -4.2027731 -4.2100677 -4.2073746 -4.1971207 -4.1868196][-4.1623092 -4.1502209 -4.1425433 -4.1618156 -4.1954155 -4.2170572 -4.2128158 -4.1940613 -4.1832347 -4.1923513 -4.2093568 -4.2163019 -4.2114468 -4.20068 -4.193325][-4.2027955 -4.1842027 -4.1633492 -4.1665292 -4.1841984 -4.1948032 -4.18918 -4.1765151 -4.1787233 -4.2001338 -4.2218456 -4.2282376 -4.2216582 -4.2103806 -4.2051682][-4.2404218 -4.2183628 -4.1856246 -4.166574 -4.1571078 -4.1433349 -4.1286507 -4.1204028 -4.1425405 -4.1893568 -4.22651 -4.2363186 -4.2273574 -4.214148 -4.2081943][-4.2517738 -4.2324724 -4.1951537 -4.1535811 -4.10836 -4.0587564 -4.0150442 -3.9922605 -4.0318966 -4.1202512 -4.1940961 -4.2293587 -4.22895 -4.2164474 -4.207366][-4.2362294 -4.2232652 -4.1915355 -4.1413269 -4.0688128 -3.9768271 -3.8821836 -3.8138657 -3.8579006 -3.99511 -4.1169167 -4.1880889 -4.2136545 -4.2144547 -4.2099509][-4.2156968 -4.2093034 -4.1903791 -4.1475792 -4.0715 -3.9634514 -3.8335192 -3.7147889 -3.7369509 -3.8960159 -4.0441618 -4.1379013 -4.1837592 -4.2028384 -4.2129645][-4.2082434 -4.2067885 -4.2029538 -4.1821513 -4.1279211 -4.0426288 -3.9358916 -3.8351789 -3.8317034 -3.9424052 -4.0576444 -4.1355567 -4.1764288 -4.1976185 -4.2138648][-4.2162223 -4.2151713 -4.22095 -4.2188568 -4.1881833 -4.134953 -4.0689311 -4.0102577 -4.002284 -4.065856 -4.1355333 -4.1789236 -4.1985183 -4.2076941 -4.2162671][-4.2352571 -4.233387 -4.2426958 -4.2501755 -4.2385406 -4.2122889 -4.1764636 -4.1462412 -4.1392288 -4.1733928 -4.2095284 -4.2218771 -4.2181139 -4.2146258 -4.2141442][-4.2555552 -4.2514186 -4.2567534 -4.2657857 -4.2699823 -4.266233 -4.2518873 -4.2381287 -4.2321248 -4.2442565 -4.2548747 -4.2451797 -4.224957 -4.2110281 -4.2041736][-4.261282 -4.255301 -4.2499971 -4.2502761 -4.2590933 -4.2684846 -4.2709584 -4.276813 -4.2801213 -4.2813287 -4.2725568 -4.2430153 -4.2071433 -4.1835895 -4.1740212][-4.2402492 -4.2348967 -4.2212391 -4.2156863 -4.2240305 -4.2378092 -4.2491059 -4.2665553 -4.2798262 -4.2787614 -4.2581086 -4.21521 -4.1658726 -4.1358767 -4.1270728][-4.2008886 -4.1898141 -4.1692519 -4.1654496 -4.1825175 -4.2031178 -4.2197914 -4.2427173 -4.2612581 -4.261517 -4.2398672 -4.1967053 -4.1463804 -4.1181793 -4.1098647]]...]
INFO - root - 2017-12-07 13:04:59.392395: step 12010, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.727 sec/batch; 64h:45m:39s remains)
INFO - root - 2017-12-07 13:05:05.894065: step 12020, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 59h:36m:40s remains)
INFO - root - 2017-12-07 13:05:12.638229: step 12030, loss = 2.04, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 56h:40m:15s remains)
INFO - root - 2017-12-07 13:05:19.517153: step 12040, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 61h:22m:05s remains)
INFO - root - 2017-12-07 13:05:26.333450: step 12050, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 63h:20m:20s remains)
INFO - root - 2017-12-07 13:05:33.216822: step 12060, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 61h:23m:09s remains)
INFO - root - 2017-12-07 13:05:40.041368: step 12070, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 57h:45m:52s remains)
INFO - root - 2017-12-07 13:05:46.954084: step 12080, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 59h:13m:26s remains)
INFO - root - 2017-12-07 13:05:53.817656: step 12090, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.728 sec/batch; 64h:46m:48s remains)
INFO - root - 2017-12-07 13:06:00.623795: step 12100, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 65h:57m:50s remains)
2017-12-07 13:06:01.341137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2075367 -4.2073984 -4.2040825 -4.1906881 -4.173871 -4.1654859 -4.1781487 -4.2096643 -4.2352514 -4.2454982 -4.2407913 -4.232532 -4.2357817 -4.2459326 -4.2581038][-4.197422 -4.1994581 -4.1953859 -4.1857047 -4.1751442 -4.1733727 -4.1888847 -4.2237163 -4.2525868 -4.2609029 -4.2539315 -4.240993 -4.2376122 -4.2392168 -4.2418342][-4.1948938 -4.1966667 -4.1933265 -4.1874375 -4.1768751 -4.169961 -4.1827621 -4.2214646 -4.257061 -4.2718754 -4.2701154 -4.2580709 -4.2477436 -4.23864 -4.2319756][-4.2180672 -4.2170897 -4.2108669 -4.20131 -4.1833515 -4.1579938 -4.1510243 -4.1818118 -4.2272725 -4.2618747 -4.2739606 -4.2683969 -4.258935 -4.2476068 -4.2372169][-4.2518535 -4.24867 -4.2401357 -4.2235341 -4.1929631 -4.1435642 -4.1009045 -4.1056437 -4.1574244 -4.2186766 -4.2538142 -4.2625208 -4.2629323 -4.2578568 -4.251565][-4.27923 -4.2779346 -4.2693481 -4.24753 -4.2049909 -4.1312933 -4.0495443 -4.0125456 -4.0588417 -4.1481051 -4.2125249 -4.2416091 -4.2586942 -4.2653265 -4.2644162][-4.2988682 -4.3048415 -4.2990561 -4.2787495 -4.2356758 -4.1579232 -4.05248 -3.9677498 -3.978714 -4.0683265 -4.1555414 -4.2076902 -4.242722 -4.2607913 -4.2663145][-4.3033624 -4.3211927 -4.3221784 -4.3050389 -4.2702026 -4.2130146 -4.12511 -4.0330458 -3.9961674 -4.0370779 -4.1077008 -4.1656408 -4.2112837 -4.239676 -4.2568851][-4.2927647 -4.3172612 -4.3272128 -4.3207364 -4.2989812 -4.2633557 -4.2052784 -4.1348696 -4.0866632 -4.086741 -4.1156569 -4.1500869 -4.1834526 -4.2097716 -4.2330894][-4.2719235 -4.2991614 -4.3173866 -4.3239827 -4.3151846 -4.2939358 -4.2563081 -4.208817 -4.1703582 -4.1604142 -4.1686282 -4.1815481 -4.1945677 -4.2034678 -4.2197561][-4.2622929 -4.2861338 -4.3050394 -4.3175535 -4.3184586 -4.303092 -4.2738075 -4.2391329 -4.2096119 -4.2044258 -4.2137785 -4.2237959 -4.2293854 -4.2258835 -4.2288823][-4.268024 -4.2837405 -4.2944984 -4.30348 -4.3058691 -4.2913313 -4.2625761 -4.23111 -4.2058315 -4.2067428 -4.2248359 -4.2450657 -4.2540092 -4.2489271 -4.2451644][-4.2820349 -4.2871518 -4.2887497 -4.2908678 -4.2898083 -4.2724242 -4.2422695 -4.2061682 -4.1789131 -4.1833081 -4.2102394 -4.2405105 -4.2543159 -4.254004 -4.2517128][-4.289175 -4.2863064 -4.2846775 -4.2839494 -4.2805452 -4.2626271 -4.2309566 -4.1921797 -4.1645794 -4.1721249 -4.2016273 -4.231513 -4.2441163 -4.2431259 -4.2425222][-4.2853208 -4.2789192 -4.277741 -4.2789755 -4.2776613 -4.2615929 -4.232501 -4.1997442 -4.1793294 -4.1890922 -4.2132215 -4.2319789 -4.2333145 -4.2256107 -4.2247024]]...]
INFO - root - 2017-12-07 13:06:08.083429: step 12110, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 58h:47m:24s remains)
INFO - root - 2017-12-07 13:06:14.734248: step 12120, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 61h:50m:02s remains)
INFO - root - 2017-12-07 13:06:21.546301: step 12130, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 58h:41m:06s remains)
INFO - root - 2017-12-07 13:06:28.352452: step 12140, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 60h:12m:10s remains)
INFO - root - 2017-12-07 13:06:35.249058: step 12150, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 58h:07m:44s remains)
INFO - root - 2017-12-07 13:06:41.992365: step 12160, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 62h:20m:46s remains)
INFO - root - 2017-12-07 13:06:48.864865: step 12170, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 63h:55m:11s remains)
INFO - root - 2017-12-07 13:06:55.687297: step 12180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 61h:53m:27s remains)
INFO - root - 2017-12-07 13:07:02.460939: step 12190, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 59h:53m:59s remains)
INFO - root - 2017-12-07 13:07:09.272358: step 12200, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 59h:16m:22s remains)
2017-12-07 13:07:09.985847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24316 -4.2303672 -4.204761 -4.1720533 -4.1394706 -4.1559234 -4.2061148 -4.234705 -4.2511334 -4.2453136 -4.2330055 -4.2161665 -4.216085 -4.2257342 -4.2354741][-4.2422056 -4.2308888 -4.2005959 -4.1687055 -4.1397429 -4.1549478 -4.1981721 -4.2228951 -4.2369385 -4.22285 -4.2036867 -4.1868825 -4.1867166 -4.1999016 -4.2124691][-4.2368269 -4.2241793 -4.1940012 -4.1698837 -4.1481786 -4.1579523 -4.1878238 -4.207665 -4.2198744 -4.2037272 -4.1815782 -4.1653757 -4.1637063 -4.1772842 -4.1894813][-4.2305617 -4.212153 -4.1811213 -4.1657577 -4.1521606 -4.1574492 -4.174684 -4.1936688 -4.208272 -4.1990976 -4.1785679 -4.1581111 -4.1499271 -4.1579986 -4.1674905][-4.224133 -4.2008686 -4.1679244 -4.1564283 -4.1488991 -4.1513953 -4.1602459 -4.18062 -4.1976914 -4.1948643 -4.1798949 -4.1607971 -4.1461911 -4.1469154 -4.15407][-4.2120438 -4.1928535 -4.1619768 -4.1505995 -4.141943 -4.1371627 -4.1360226 -4.1576738 -4.1768274 -4.181623 -4.17718 -4.1633615 -4.1443 -4.1406422 -4.1515975][-4.1971169 -4.1886687 -4.1659756 -4.1489153 -4.1306081 -4.1094742 -4.0934525 -4.1160245 -4.13856 -4.153595 -4.1658478 -4.16294 -4.1464348 -4.1450424 -4.163692][-4.1803765 -4.1876755 -4.1748347 -4.1512322 -4.1221213 -4.0817223 -4.0472579 -4.0684977 -4.0955043 -4.1211162 -4.150465 -4.165494 -4.1623516 -4.1681676 -4.1918335][-4.1584888 -4.18172 -4.1835575 -4.1604729 -4.1278181 -4.0726523 -4.0213585 -4.039032 -4.0704618 -4.1049161 -4.1479468 -4.1788568 -4.1887178 -4.20174 -4.2270956][-4.1368747 -4.1721234 -4.1861539 -4.1703362 -4.1439023 -4.0906157 -4.0389538 -4.0501633 -4.0829735 -4.1199536 -4.1657023 -4.2015133 -4.2161293 -4.2317057 -4.2570028][-4.1253557 -4.1643987 -4.1844697 -4.1790838 -4.167275 -4.1298862 -4.0903654 -4.0970883 -4.12374 -4.1568813 -4.1956711 -4.2265153 -4.2412844 -4.2563472 -4.2797594][-4.1274071 -4.1621976 -4.1842852 -4.1907396 -4.1918964 -4.1696582 -4.1408248 -4.1433411 -4.1638308 -4.1907558 -4.2202325 -4.2465544 -4.2631097 -4.2790575 -4.2975168][-4.1523833 -4.1816893 -4.2024136 -4.2149105 -4.2232375 -4.2104259 -4.188395 -4.1868563 -4.2023387 -4.2246943 -4.2472053 -4.2692947 -4.2861209 -4.3009291 -4.3140821][-4.1962376 -4.2203274 -4.23709 -4.2489915 -4.2579231 -4.2506652 -4.2346978 -4.2328324 -4.2431245 -4.2605276 -4.2775011 -4.294281 -4.3089204 -4.3204303 -4.3275871][-4.2388768 -4.2568121 -4.2697153 -4.2799163 -4.2863245 -4.282217 -4.2730713 -4.2724891 -4.279767 -4.2926621 -4.3048916 -4.3160281 -4.3265047 -4.3342004 -4.3352265]]...]
INFO - root - 2017-12-07 13:07:16.695473: step 12210, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:12m:49s remains)
INFO - root - 2017-12-07 13:07:23.319871: step 12220, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 58h:53m:27s remains)
INFO - root - 2017-12-07 13:07:30.225764: step 12230, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 60h:21m:48s remains)
INFO - root - 2017-12-07 13:07:37.184123: step 12240, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.683 sec/batch; 60h:47m:21s remains)
INFO - root - 2017-12-07 13:07:43.989279: step 12250, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.693 sec/batch; 61h:36m:38s remains)
INFO - root - 2017-12-07 13:07:50.745620: step 12260, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 56h:46m:37s remains)
INFO - root - 2017-12-07 13:07:57.445826: step 12270, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 57h:06m:56s remains)
INFO - root - 2017-12-07 13:08:04.281427: step 12280, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 63h:43m:08s remains)
INFO - root - 2017-12-07 13:08:11.139669: step 12290, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 59h:52m:54s remains)
INFO - root - 2017-12-07 13:08:17.858731: step 12300, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 59h:37m:32s remains)
2017-12-07 13:08:18.581519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3616066 -4.3220744 -4.2684569 -4.2195549 -4.2014923 -4.2270026 -4.26781 -4.2928033 -4.303597 -4.2916765 -4.2773604 -4.2726026 -4.2778044 -4.2977104 -4.3212719][-4.3605442 -4.3102546 -4.2431693 -4.1792741 -4.1649356 -4.2071233 -4.2561383 -4.2806296 -4.2922564 -4.2811527 -4.2652049 -4.2619829 -4.2754507 -4.3026247 -4.3257694][-4.3497343 -4.2961917 -4.2253284 -4.1527443 -4.1406984 -4.1929622 -4.2431126 -4.264184 -4.2738976 -4.2631865 -4.2488976 -4.2548871 -4.2763429 -4.3063283 -4.3282485][-4.3312545 -4.2791047 -4.2183 -4.1521649 -4.1388812 -4.1821542 -4.221004 -4.2322516 -4.2355881 -4.2285504 -4.2231612 -4.2425261 -4.2739577 -4.3072777 -4.3286023][-4.3166585 -4.2651091 -4.2119141 -4.1559358 -4.1362205 -4.1557679 -4.1741357 -4.1817403 -4.1814418 -4.1842432 -4.1931815 -4.2262721 -4.2609372 -4.2955713 -4.3174753][-4.3123827 -4.2656879 -4.2175255 -4.1638837 -4.1265783 -4.1090164 -4.1039243 -4.1191907 -4.1307869 -4.1510372 -4.1785994 -4.2231913 -4.2580185 -4.288897 -4.3092747][-4.3299928 -4.2940173 -4.2433748 -4.1790738 -4.112215 -4.0419111 -4.0058446 -4.0394993 -4.0842967 -4.135848 -4.1865706 -4.2369528 -4.270061 -4.2943926 -4.3097835][-4.354445 -4.3306479 -4.2773414 -4.1923828 -4.0863004 -3.9639921 -3.9016237 -3.9724073 -4.0656533 -4.1459737 -4.2100391 -4.2587886 -4.287612 -4.3055406 -4.3169227][-4.3729863 -4.3536968 -4.2959461 -4.2003536 -4.0757842 -3.9358745 -3.8721242 -3.9651103 -4.0833292 -4.1697369 -4.2316351 -4.2761774 -4.3009491 -4.31518 -4.325511][-4.3896742 -4.3704329 -4.3121109 -4.2216444 -4.1036291 -3.9773169 -3.9308581 -4.0161581 -4.1262374 -4.2058725 -4.2575183 -4.295022 -4.3149247 -4.3258185 -4.333777][-4.4040408 -4.3840594 -4.324811 -4.2416534 -4.1429911 -4.0487785 -4.0292139 -4.0978203 -4.1865478 -4.252984 -4.290669 -4.3141522 -4.3272672 -4.3358984 -4.3423138][-4.4088368 -4.3931017 -4.3382506 -4.262403 -4.1810393 -4.1216021 -4.1201625 -4.1755552 -4.2423458 -4.2928538 -4.3168116 -4.3263764 -4.3318 -4.3395386 -4.3481126][-4.3988171 -4.3879008 -4.3450193 -4.2832909 -4.2204995 -4.1867828 -4.1932182 -4.2334542 -4.2822113 -4.317152 -4.32718 -4.3241811 -4.3228045 -4.3319521 -4.343967][-4.3792958 -4.3719678 -4.3423328 -4.2992349 -4.2571282 -4.2387495 -4.2485714 -4.2767334 -4.3120942 -4.3317227 -4.32697 -4.3143497 -4.309515 -4.3215785 -4.3381028][-4.3632154 -4.3584995 -4.3402581 -4.3141022 -4.2914209 -4.2826681 -4.29325 -4.3130341 -4.3341861 -4.3338494 -4.3140831 -4.2941489 -4.2910089 -4.3091478 -4.33006]]...]
INFO - root - 2017-12-07 13:08:25.301292: step 12310, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 62h:07m:25s remains)
INFO - root - 2017-12-07 13:08:31.950264: step 12320, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 64h:24m:21s remains)
INFO - root - 2017-12-07 13:08:38.553716: step 12330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 58h:49m:27s remains)
INFO - root - 2017-12-07 13:08:45.359934: step 12340, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 57h:34m:21s remains)
INFO - root - 2017-12-07 13:08:52.172985: step 12350, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 63h:07m:41s remains)
INFO - root - 2017-12-07 13:08:58.908852: step 12360, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 61h:36m:40s remains)
INFO - root - 2017-12-07 13:09:05.691487: step 12370, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 59h:03m:26s remains)
INFO - root - 2017-12-07 13:09:12.469641: step 12380, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 56h:56m:17s remains)
INFO - root - 2017-12-07 13:09:19.167398: step 12390, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.648 sec/batch; 57h:36m:21s remains)
INFO - root - 2017-12-07 13:09:26.103901: step 12400, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 64h:07m:17s remains)
2017-12-07 13:09:26.806895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1972108 -4.2258267 -4.259635 -4.2782459 -4.2776208 -4.2745824 -4.2572322 -4.2281981 -4.2003117 -4.19054 -4.19731 -4.1912966 -4.1764488 -4.142045 -4.0849271][-4.1346769 -4.1851563 -4.2378426 -4.2628779 -4.2626181 -4.2553244 -4.2355843 -4.2068481 -4.1834917 -4.1772676 -4.1846671 -4.1806021 -4.1663795 -4.1304994 -4.066699][-4.080976 -4.1604137 -4.2268381 -4.2478237 -4.2385087 -4.2223177 -4.207386 -4.1936584 -4.1844473 -4.1839771 -4.1927195 -4.1929159 -4.1820369 -4.1457667 -4.0733957][-4.0568004 -4.1574893 -4.2244849 -4.2323217 -4.2094197 -4.1836133 -4.1764946 -4.1830783 -4.1915293 -4.1992993 -4.2092209 -4.2131047 -4.2015457 -4.1660218 -4.0924492][-4.065331 -4.1682291 -4.2265635 -4.2222528 -4.1896224 -4.1629977 -4.1621337 -4.1790743 -4.19434 -4.2079344 -4.2190528 -4.228313 -4.2216973 -4.1856689 -4.10552][-4.09343 -4.1866164 -4.23334 -4.2130213 -4.1717739 -4.1483936 -4.1499352 -4.1712966 -4.1883254 -4.20345 -4.2158122 -4.2322631 -4.2335091 -4.1980162 -4.1114836][-4.1484165 -4.216105 -4.2458982 -4.21092 -4.162374 -4.136837 -4.136054 -4.154901 -4.1689405 -4.1826081 -4.19869 -4.2237844 -4.2351127 -4.2045016 -4.1230011][-4.2088037 -4.245 -4.2590661 -4.2221041 -4.1758866 -4.1480007 -4.1382565 -4.1450448 -4.1489134 -4.1579423 -4.178906 -4.2116246 -4.2283688 -4.2012072 -4.1342278][-4.2592382 -4.2722812 -4.2804976 -4.2526751 -4.2139554 -4.1872911 -4.170434 -4.1627493 -4.1539388 -4.1543331 -4.1762223 -4.2120285 -4.2269487 -4.2009668 -4.1472187][-4.2901845 -4.2927756 -4.2995906 -4.2836275 -4.2561312 -4.23548 -4.21929 -4.2034888 -4.1832886 -4.1750665 -4.1904778 -4.2215858 -4.2319155 -4.2073097 -4.1628408][-4.2962956 -4.2956395 -4.3025646 -4.294713 -4.2797332 -4.2687321 -4.258985 -4.2459364 -4.226059 -4.2128935 -4.2171793 -4.2375112 -4.2427478 -4.2227578 -4.1864443][-4.2993245 -4.2981439 -4.3019209 -4.2964492 -4.2869744 -4.280652 -4.2769742 -4.2731729 -4.261024 -4.24973 -4.2489009 -4.258934 -4.2597342 -4.24617 -4.219573][-4.3090429 -4.3092346 -4.3100867 -4.3050914 -4.2970338 -4.2923388 -4.2905984 -4.2903571 -4.2848806 -4.2782769 -4.2783232 -4.2826915 -4.2814903 -4.2735391 -4.255003][-4.3172317 -4.3181548 -4.3194156 -4.3153734 -4.3091455 -4.3040781 -4.3022981 -4.30276 -4.3015466 -4.2994452 -4.3010616 -4.3036208 -4.3031163 -4.29954 -4.2894268][-4.3220258 -4.3228679 -4.3248587 -4.3240609 -4.3219414 -4.3195825 -4.3184881 -4.3184752 -4.3184571 -4.3183417 -4.3190575 -4.318996 -4.3182397 -4.3175082 -4.31501]]...]
INFO - root - 2017-12-07 13:09:33.549280: step 12410, loss = 2.08, batch loss = 2.03 (12.9 examples/sec; 0.621 sec/batch; 55h:10m:57s remains)
INFO - root - 2017-12-07 13:09:40.222197: step 12420, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 59h:49m:59s remains)
INFO - root - 2017-12-07 13:09:46.973397: step 12430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 61h:47m:29s remains)
INFO - root - 2017-12-07 13:09:53.930271: step 12440, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 65h:22m:07s remains)
INFO - root - 2017-12-07 13:10:00.799038: step 12450, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 62h:17m:19s remains)
INFO - root - 2017-12-07 13:10:07.640021: step 12460, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 56h:25m:43s remains)
INFO - root - 2017-12-07 13:10:14.494753: step 12470, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 57h:54m:23s remains)
INFO - root - 2017-12-07 13:10:21.376001: step 12480, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 64h:49m:24s remains)
INFO - root - 2017-12-07 13:10:28.156137: step 12490, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 60h:39m:35s remains)
INFO - root - 2017-12-07 13:10:34.963275: step 12500, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 59h:45m:44s remains)
2017-12-07 13:10:35.738683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1474 -4.1875386 -4.2221417 -4.2393856 -4.2316217 -4.2070389 -4.1802926 -4.1778774 -4.1948485 -4.2142253 -4.2382536 -4.2619762 -4.2646914 -4.2487183 -4.2194242][-4.1156378 -4.1564746 -4.1910629 -4.2174845 -4.2185903 -4.1979103 -4.1779203 -4.181509 -4.195507 -4.2099829 -4.230711 -4.2490239 -4.2457285 -4.2269073 -4.194232][-4.1035786 -4.1342087 -4.1654329 -4.1988654 -4.2135811 -4.2039013 -4.1877122 -4.1931071 -4.2068639 -4.2168627 -4.2299366 -4.2403283 -4.2324138 -4.21249 -4.1762528][-4.1296372 -4.141324 -4.162611 -4.1911469 -4.2124214 -4.2115216 -4.1974435 -4.2006927 -4.2182007 -4.2324319 -4.2403417 -4.2445092 -4.2323246 -4.2106528 -4.1709433][-4.1792612 -4.1702824 -4.1764536 -4.1903248 -4.199842 -4.1912017 -4.1705232 -4.1679945 -4.1935472 -4.22102 -4.2370515 -4.2428036 -4.2340465 -4.2133927 -4.1766186][-4.2213922 -4.2060733 -4.201663 -4.1981549 -4.1804433 -4.1456885 -4.1015048 -4.0851955 -4.1245279 -4.1815453 -4.2212405 -4.2413287 -4.2394037 -4.2230105 -4.1931286][-4.2244239 -4.2083454 -4.2004757 -4.1829333 -4.1355972 -4.0657964 -3.9822061 -3.9448097 -4.0133367 -4.1185675 -4.195353 -4.240788 -4.2516594 -4.2411742 -4.2180676][-4.1946082 -4.1778555 -4.1684384 -4.1444716 -4.0837245 -3.989516 -3.8681829 -3.8045948 -3.9078064 -4.057734 -4.1660142 -4.2298126 -4.2498212 -4.2453184 -4.2304168][-4.1694059 -4.1551876 -4.1484532 -4.1279049 -4.0787191 -3.9945107 -3.8739727 -3.7969913 -3.8950012 -4.0472107 -4.1575389 -4.2184997 -4.2339826 -4.2320776 -4.2257032][-4.1771851 -4.1673951 -4.1661835 -4.1601634 -4.1373267 -4.0869613 -4.00422 -3.9432771 -4.0002046 -4.1032996 -4.1807079 -4.2215624 -4.2246637 -4.2172585 -4.2128057][-4.2069087 -4.20036 -4.2017756 -4.2095504 -4.2074294 -4.1837587 -4.1368108 -4.0923448 -4.1150932 -4.1694026 -4.2109666 -4.2327905 -4.2239852 -4.2068038 -4.1973004][-4.2384472 -4.230257 -4.2277117 -4.2370496 -4.2423744 -4.229177 -4.2022638 -4.1717491 -4.1814289 -4.2107625 -4.2288136 -4.2361069 -4.2174854 -4.1919618 -4.1765966][-4.2615891 -4.2471447 -4.238266 -4.2416525 -4.24502 -4.2312121 -4.2130637 -4.1987619 -4.2112832 -4.2306747 -4.2374725 -4.2361526 -4.2133431 -4.1850371 -4.1678843][-4.2740097 -4.2572536 -4.241076 -4.2331295 -4.2314005 -4.218976 -4.206605 -4.2054043 -4.2232842 -4.240643 -4.2459173 -4.2413154 -4.2199183 -4.1952929 -4.1800771][-4.2806158 -4.2650576 -4.2457161 -4.2297468 -4.2248039 -4.2185292 -4.2103119 -4.2136445 -4.2314596 -4.24615 -4.2517028 -4.2475777 -4.2326241 -4.2155285 -4.2028532]]...]
INFO - root - 2017-12-07 13:10:42.467535: step 12510, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 60h:25m:16s remains)
INFO - root - 2017-12-07 13:10:49.121661: step 12520, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 59h:07m:13s remains)
INFO - root - 2017-12-07 13:10:55.911892: step 12530, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 58h:50m:53s remains)
INFO - root - 2017-12-07 13:11:02.775370: step 12540, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 56h:52m:51s remains)
INFO - root - 2017-12-07 13:11:09.602455: step 12550, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 63h:37m:34s remains)
INFO - root - 2017-12-07 13:11:16.433282: step 12560, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 62h:25m:29s remains)
INFO - root - 2017-12-07 13:11:23.251172: step 12570, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 59h:56m:10s remains)
INFO - root - 2017-12-07 13:11:29.965577: step 12580, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 56h:08m:50s remains)
INFO - root - 2017-12-07 13:11:36.761196: step 12590, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 59h:16m:27s remains)
INFO - root - 2017-12-07 13:11:43.457114: step 12600, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 60h:32m:11s remains)
2017-12-07 13:11:44.140730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2878728 -4.2879887 -4.29392 -4.3000135 -4.3050194 -4.3063855 -4.3041925 -4.2990308 -4.2932682 -4.2855334 -4.2856822 -4.2875481 -4.2869782 -4.2809215 -4.2730575][-4.2821903 -4.2817588 -4.2914886 -4.3004017 -4.3089857 -4.3141146 -4.3108597 -4.3012643 -4.2911677 -4.2824311 -4.2851825 -4.2927146 -4.2925377 -4.2830977 -4.2719889][-4.2645159 -4.2652531 -4.2784796 -4.2915277 -4.30527 -4.3135223 -4.3088555 -4.294044 -4.2821293 -4.2786646 -4.28866 -4.3039842 -4.3043561 -4.2923317 -4.2784381][-4.2277889 -4.2302308 -4.2451196 -4.2564783 -4.2681618 -4.2747726 -4.2726331 -4.2628884 -4.2589507 -4.2687807 -4.2880907 -4.3099113 -4.3136177 -4.3034868 -4.2883024][-4.1693511 -4.1776757 -4.1960692 -4.2061653 -4.2102222 -4.2068796 -4.204073 -4.2028847 -4.2148619 -4.2429271 -4.2744169 -4.3037014 -4.3150387 -4.3130536 -4.300292][-4.0984249 -4.1118555 -4.1328282 -4.1412363 -4.1335158 -4.1157937 -4.1099315 -4.1183991 -4.1470351 -4.1898141 -4.2330728 -4.2707934 -4.2935171 -4.3050413 -4.3013754][-4.0614538 -4.0703416 -4.0854526 -4.0842628 -4.0595927 -4.0253167 -4.0142694 -4.0267496 -4.0635934 -4.1170168 -4.170331 -4.2164426 -4.2524943 -4.2786283 -4.2888136][-4.0862932 -4.0839128 -4.0852475 -4.0737128 -4.0401487 -4.0000663 -3.9798934 -3.9783876 -4.0058131 -4.0574017 -4.1137323 -4.1668386 -4.2163706 -4.2541189 -4.2753949][-4.1463437 -4.1320891 -4.1211472 -4.1079144 -4.0832448 -4.0551076 -4.0338616 -4.0138683 -4.0173445 -4.0535622 -4.1035738 -4.1585584 -4.2129464 -4.2529325 -4.2743497][-4.2069292 -4.1852283 -4.1653795 -4.1542625 -4.1398296 -4.1253104 -4.1118021 -4.0885887 -4.0807109 -4.1043496 -4.145227 -4.1942611 -4.2392855 -4.2707796 -4.2837791][-4.2626019 -4.2425494 -4.2213635 -4.2130737 -4.2026806 -4.1924648 -4.1825047 -4.1631413 -4.153636 -4.1713376 -4.2037807 -4.2417021 -4.2718506 -4.290668 -4.2937331][-4.3006105 -4.2893119 -4.2754412 -4.2738395 -4.2673793 -4.2589827 -4.2503052 -4.2364516 -4.2278895 -4.2382545 -4.258101 -4.2801371 -4.294385 -4.3023295 -4.297204][-4.3215976 -4.3193283 -4.3141875 -4.3183632 -4.3151679 -4.3076553 -4.3014369 -4.293808 -4.2864633 -4.2903671 -4.2962379 -4.3009577 -4.3020573 -4.3020492 -4.2923093][-4.3314486 -4.3349156 -4.3345861 -4.3403783 -4.338295 -4.3321347 -4.3290939 -4.3266435 -4.3232608 -4.3244686 -4.3213892 -4.3134742 -4.3047304 -4.2984738 -4.2854967][-4.3324313 -4.3374405 -4.3383346 -4.3428216 -4.3412223 -4.3387756 -4.3398533 -4.340816 -4.3409872 -4.3403425 -4.332108 -4.3196068 -4.306262 -4.2965355 -4.2812328]]...]
INFO - root - 2017-12-07 13:11:50.796032: step 12610, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 57h:07m:22s remains)
INFO - root - 2017-12-07 13:11:57.449986: step 12620, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 60h:58m:30s remains)
INFO - root - 2017-12-07 13:12:04.299644: step 12630, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 60h:51m:14s remains)
INFO - root - 2017-12-07 13:12:10.927402: step 12640, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.663 sec/batch; 58h:52m:01s remains)
INFO - root - 2017-12-07 13:12:17.694819: step 12650, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 56h:05m:30s remains)
INFO - root - 2017-12-07 13:12:24.542470: step 12660, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 56h:37m:17s remains)
INFO - root - 2017-12-07 13:12:31.400213: step 12670, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 64h:12m:42s remains)
INFO - root - 2017-12-07 13:12:38.206209: step 12680, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 63h:34m:03s remains)
INFO - root - 2017-12-07 13:12:44.993795: step 12690, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 62h:57m:38s remains)
INFO - root - 2017-12-07 13:12:51.717841: step 12700, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 58h:28m:30s remains)
2017-12-07 13:12:52.497902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2922144 -4.3023262 -4.3129187 -4.3113546 -4.3064861 -4.3068666 -4.3106313 -4.3185649 -4.330265 -4.3384781 -4.3378134 -4.3208518 -4.2865372 -4.2582021 -4.2423964][-4.3033037 -4.3152142 -4.3247128 -4.3238335 -4.3205442 -4.3207231 -4.3202105 -4.3224325 -4.32751 -4.3281193 -4.3223748 -4.3038011 -4.2678657 -4.238595 -4.2185974][-4.3002558 -4.3130579 -4.3255816 -4.3270926 -4.3209162 -4.3128834 -4.3017669 -4.295692 -4.2958021 -4.2932339 -4.2846522 -4.2681761 -4.2391915 -4.2167225 -4.2038212][-4.2994676 -4.3140993 -4.327868 -4.3264275 -4.3086166 -4.2824912 -4.253901 -4.2345262 -4.2320452 -4.229609 -4.2226214 -4.2126489 -4.1925035 -4.1820393 -4.1846576][-4.299181 -4.30714 -4.3092651 -4.2908015 -4.2547159 -4.2083073 -4.1548486 -4.1153312 -4.1215835 -4.1365838 -4.1428576 -4.1490006 -4.1428237 -4.1420207 -4.1570373][-4.2801681 -4.2737713 -4.2568712 -4.2181396 -4.1672206 -4.1079712 -4.0235286 -3.9555352 -3.9814954 -4.028532 -4.0583596 -4.0889888 -4.100153 -4.106658 -4.1251335][-4.2421246 -4.2113128 -4.1713767 -4.1145954 -4.0572634 -3.9917178 -3.8766727 -3.7841818 -3.8517916 -3.9512515 -4.0127439 -4.0665607 -4.0909667 -4.094738 -4.1008244][-4.1931062 -4.1387811 -4.0857987 -4.0281868 -3.9769509 -3.9212494 -3.8300729 -3.7797251 -3.8779919 -3.9865506 -4.0463519 -4.097846 -4.1241097 -4.1209669 -4.1090307][-4.1522155 -4.0922155 -4.0532351 -4.017591 -3.9891896 -3.96896 -3.9472964 -3.958019 -4.032423 -4.0999393 -4.1376247 -4.1760559 -4.1931658 -4.1842113 -4.1673303][-4.1342845 -4.0960822 -4.0932174 -4.0892258 -4.0809669 -4.0818896 -4.092916 -4.1208467 -4.1644692 -4.1959872 -4.2135787 -4.235261 -4.2464352 -4.2413287 -4.2335987][-4.142477 -4.1313691 -4.1576371 -4.1775141 -4.1819987 -4.1923161 -4.210536 -4.2296109 -4.2491012 -4.2606969 -4.26544 -4.2780185 -4.2886381 -4.2891712 -4.2894807][-4.1532497 -4.16024 -4.201674 -4.2343636 -4.2454267 -4.2599483 -4.2755194 -4.2856245 -4.2923775 -4.2927504 -4.2872787 -4.2887573 -4.2952056 -4.2972107 -4.300252][-4.1303945 -4.1479983 -4.1973667 -4.2366967 -4.2513537 -4.2643189 -4.2743359 -4.27772 -4.2761168 -4.2702084 -4.2615757 -4.2559729 -4.2551947 -4.2580838 -4.2661438][-4.0789757 -4.0966849 -4.14762 -4.1874623 -4.2031341 -4.21285 -4.2161369 -4.2139134 -4.20886 -4.201405 -4.1932597 -4.1842723 -4.1795812 -4.1852183 -4.1980062][-4.0371051 -4.0450816 -4.0904455 -4.1269913 -4.1407375 -4.1467071 -4.1467147 -4.1433949 -4.1394515 -4.1347632 -4.1289778 -4.1211286 -4.1164002 -4.1228881 -4.1366262]]...]
INFO - root - 2017-12-07 13:12:59.304507: step 12710, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 62h:52m:38s remains)
INFO - root - 2017-12-07 13:13:05.880513: step 12720, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 60h:29m:55s remains)
INFO - root - 2017-12-07 13:13:12.664761: step 12730, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 60h:56m:24s remains)
INFO - root - 2017-12-07 13:13:19.460765: step 12740, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 58h:21m:48s remains)
INFO - root - 2017-12-07 13:13:26.299278: step 12750, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 63h:04m:38s remains)
INFO - root - 2017-12-07 13:13:33.102837: step 12760, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.739 sec/batch; 65h:37m:12s remains)
INFO - root - 2017-12-07 13:13:39.910647: step 12770, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 58h:49m:29s remains)
INFO - root - 2017-12-07 13:13:46.599426: step 12780, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 58h:06m:06s remains)
INFO - root - 2017-12-07 13:13:53.354988: step 12790, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 56h:27m:50s remains)
INFO - root - 2017-12-07 13:14:00.093235: step 12800, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 62h:41m:11s remains)
2017-12-07 13:14:00.860372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2894869 -4.2900229 -4.2787194 -4.2631788 -4.2509365 -4.2461634 -4.2461028 -4.2494555 -4.2544408 -4.259975 -4.2645717 -4.2685838 -4.2712922 -4.2683473 -4.25501][-4.3004265 -4.2987795 -4.2774615 -4.2493086 -4.2300873 -4.2229042 -4.2225194 -4.2323852 -4.2462792 -4.258667 -4.2668056 -4.2729096 -4.2778587 -4.2770224 -4.267097][-4.3036418 -4.2975373 -4.2650723 -4.2240849 -4.1962729 -4.1828442 -4.1796613 -4.1961951 -4.2211628 -4.2416015 -4.2537761 -4.2626066 -4.26963 -4.2737689 -4.2722874][-4.2940145 -4.2817807 -4.2417426 -4.1931672 -4.1543369 -4.1279135 -4.1215286 -4.1488619 -4.1851 -4.2127137 -4.2306342 -4.2417994 -4.2514029 -4.2611146 -4.267602][-4.2842636 -4.2663488 -4.2199035 -4.165236 -4.1136365 -4.0696173 -4.0562849 -4.0966644 -4.147553 -4.1842923 -4.2070684 -4.2212629 -4.23248 -4.2457552 -4.2575655][-4.256001 -4.2309947 -4.178853 -4.1183238 -4.0511513 -3.9745054 -3.9416966 -4.0061278 -4.0900249 -4.1455612 -4.1760387 -4.1945467 -4.2072325 -4.2237191 -4.2401552][-4.2025766 -4.1705813 -4.1177449 -4.0573268 -3.9790826 -3.8646803 -3.7974596 -3.8889735 -4.0135007 -4.0873623 -4.1259103 -4.1469803 -4.1618857 -4.1834373 -4.2078934][-4.1813307 -4.1447639 -4.0992408 -4.0477428 -3.9773021 -3.8586385 -3.7776084 -3.8697953 -3.996829 -4.0694208 -4.1069174 -4.1244869 -4.1382165 -4.1669211 -4.1991868][-4.181541 -4.1453524 -4.1102505 -4.0738068 -4.0249162 -3.9361441 -3.8723812 -3.9369349 -4.0294113 -4.0791168 -4.1056752 -4.1147394 -4.1286907 -4.1637697 -4.20147][-4.1840286 -4.1584296 -4.1372004 -4.1164646 -4.0877719 -4.02726 -3.9763141 -4.02188 -4.0878344 -4.1158276 -4.1267729 -4.12508 -4.135108 -4.1683831 -4.2044544][-4.1969023 -4.1840043 -4.175211 -4.165318 -4.1492815 -4.1021638 -4.0572481 -4.089283 -4.1398678 -4.1591287 -4.1615658 -4.1531878 -4.1579103 -4.1832714 -4.2102332][-4.228869 -4.222518 -4.220614 -4.2190328 -4.2098627 -4.1701 -4.1311507 -4.1503415 -4.1862497 -4.2021165 -4.2039514 -4.1918426 -4.1913061 -4.2062416 -4.2203918][-4.2705407 -4.2662845 -4.264925 -4.265738 -4.2603669 -4.2310824 -4.1987367 -4.2089195 -4.2326069 -4.2461858 -4.2473326 -4.2348218 -4.2319078 -4.2376475 -4.2375321][-4.2947855 -4.2914762 -4.2889304 -4.2928295 -4.2921543 -4.2749071 -4.2519207 -4.256578 -4.2722492 -4.2788844 -4.276813 -4.266428 -4.263762 -4.26415 -4.2546039][-4.30037 -4.2976875 -4.295507 -4.3002095 -4.30339 -4.29732 -4.2868257 -4.2920847 -4.3028784 -4.303863 -4.2988892 -4.29122 -4.2881861 -4.2844386 -4.2698784]]...]
INFO - root - 2017-12-07 13:14:07.638237: step 12810, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 57h:31m:41s remains)
INFO - root - 2017-12-07 13:14:14.194115: step 12820, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 57h:24m:10s remains)
INFO - root - 2017-12-07 13:14:20.826451: step 12830, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 58h:06m:02s remains)
INFO - root - 2017-12-07 13:14:27.636031: step 12840, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 60h:42m:03s remains)
INFO - root - 2017-12-07 13:14:34.433138: step 12850, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 61h:04m:00s remains)
INFO - root - 2017-12-07 13:14:41.199838: step 12860, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 57h:00m:23s remains)
INFO - root - 2017-12-07 13:14:48.064022: step 12870, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.664 sec/batch; 58h:55m:48s remains)
INFO - root - 2017-12-07 13:14:54.857142: step 12880, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 62h:06m:02s remains)
INFO - root - 2017-12-07 13:15:01.688253: step 12890, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 63h:19m:11s remains)
INFO - root - 2017-12-07 13:15:08.501363: step 12900, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 62h:37m:04s remains)
2017-12-07 13:15:09.228799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2728181 -4.2829871 -4.2898889 -4.2908831 -4.2884331 -4.2902355 -4.2961926 -4.3063092 -4.3140597 -4.3209209 -4.3176875 -4.2990832 -4.2700086 -4.2473545 -4.2445173][-4.238842 -4.255476 -4.2678432 -4.270494 -4.2657733 -4.2638087 -4.2661 -4.2730141 -4.2823277 -4.2962027 -4.2967153 -4.2732453 -4.2321196 -4.1999702 -4.1947536][-4.19368 -4.2162881 -4.2358027 -4.24401 -4.2389874 -4.230907 -4.22226 -4.2176142 -4.2229905 -4.2443223 -4.2562518 -4.2374678 -4.1947422 -4.1622934 -4.1558361][-4.1537414 -4.17953 -4.2039962 -4.2159982 -4.2094278 -4.1902843 -4.1635485 -4.1415658 -4.1483116 -4.1857023 -4.2153058 -4.2073169 -4.1715446 -4.1428528 -4.1331758][-4.1277132 -4.1554656 -4.1799922 -4.1912441 -4.1777086 -4.1386638 -4.083992 -4.0383043 -4.0550375 -4.1231523 -4.1789341 -4.1896782 -4.1659508 -4.1425962 -4.1289558][-4.1180787 -4.1468534 -4.171865 -4.1839743 -4.1631317 -4.10059 -4.0081387 -3.9267564 -3.9560976 -4.0597973 -4.14226 -4.1744633 -4.1629505 -4.1470757 -4.1343961][-4.1264071 -4.1571126 -4.1870193 -4.2016554 -4.1780438 -4.1007161 -3.9776146 -3.8635836 -3.9020307 -4.0260792 -4.1213937 -4.1678128 -4.1657124 -4.1545143 -4.1404328][-4.135951 -4.1694937 -4.2084322 -4.2297764 -4.21087 -4.134408 -4.0109019 -3.8999474 -3.9311538 -4.0401487 -4.1247516 -4.1702504 -4.1758909 -4.171412 -4.1567035][-4.1386914 -4.1715431 -4.2163477 -4.2478161 -4.2386842 -4.1785727 -4.0835667 -4.00599 -4.0255513 -4.0985904 -4.1568909 -4.1899366 -4.2011404 -4.2013221 -4.1878595][-4.1416903 -4.1669979 -4.2078128 -4.24288 -4.2449713 -4.2081747 -4.1482205 -4.1038747 -4.1159215 -4.1582975 -4.1896935 -4.2091312 -4.2222853 -4.2267394 -4.2189322][-4.1538644 -4.1658726 -4.1906404 -4.2163291 -4.2239823 -4.2083635 -4.1781545 -4.1563768 -4.1637816 -4.1870861 -4.2027464 -4.2122641 -4.2236357 -4.2292891 -4.2260065][-4.173418 -4.1744723 -4.1832733 -4.1973047 -4.2051654 -4.2049661 -4.1958361 -4.187901 -4.1894636 -4.1943808 -4.1943407 -4.195787 -4.2068586 -4.21662 -4.2175879][-4.187686 -4.1845345 -4.1925926 -4.2048478 -4.2126508 -4.216517 -4.2179384 -4.2176018 -4.2145653 -4.2019992 -4.1891894 -4.1886425 -4.20328 -4.2184095 -4.2243185][-4.1842575 -4.1800265 -4.1973081 -4.2201972 -4.2331324 -4.2372022 -4.2375212 -4.238307 -4.2341452 -4.2143769 -4.1956706 -4.1940136 -4.2092905 -4.2270951 -4.2362213][-4.1730943 -4.1615572 -4.1836886 -4.2169266 -4.2347317 -4.2397718 -4.2385068 -4.2419353 -4.2412152 -4.224092 -4.2052627 -4.2020626 -4.2143555 -4.22984 -4.2385826]]...]
INFO - root - 2017-12-07 13:15:15.991491: step 12910, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.688 sec/batch; 61h:07m:09s remains)
INFO - root - 2017-12-07 13:15:22.615410: step 12920, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 58h:29m:27s remains)
INFO - root - 2017-12-07 13:15:29.369259: step 12930, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 60h:15m:10s remains)
INFO - root - 2017-12-07 13:15:36.265729: step 12940, loss = 2.03, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 59h:19m:25s remains)
INFO - root - 2017-12-07 13:15:42.970615: step 12950, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 61h:32m:44s remains)
INFO - root - 2017-12-07 13:15:49.811811: step 12960, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 63h:13m:50s remains)
INFO - root - 2017-12-07 13:15:56.567018: step 12970, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 56h:46m:52s remains)
INFO - root - 2017-12-07 13:16:03.333230: step 12980, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 58h:07m:49s remains)
INFO - root - 2017-12-07 13:16:10.216884: step 12990, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 59h:54m:33s remains)
INFO - root - 2017-12-07 13:16:17.063656: step 13000, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 62h:58m:26s remains)
2017-12-07 13:16:17.798925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3095684 -4.3075652 -4.3008823 -4.295897 -4.29438 -4.2936273 -4.2925067 -4.293468 -4.2961979 -4.2991133 -4.3008695 -4.3023496 -4.3048134 -4.3077307 -4.3091593][-4.30663 -4.30457 -4.2987351 -4.2940407 -4.2918148 -4.2896471 -4.2865834 -4.2852688 -4.2857289 -4.2871704 -4.2887783 -4.2919416 -4.297771 -4.30454 -4.3088837][-4.3073912 -4.3049164 -4.2988148 -4.2912087 -4.2827187 -4.2724767 -4.2617784 -4.2540822 -4.2508674 -4.2525716 -4.2579756 -4.2666521 -4.2794666 -4.2937579 -4.3045335][-4.3065667 -4.3007159 -4.2905746 -4.2747693 -4.2523742 -4.2251625 -4.1999397 -4.1827459 -4.1761189 -4.1816053 -4.1965647 -4.2179437 -4.2456632 -4.273571 -4.2948389][-4.30742 -4.297318 -4.278862 -4.2477307 -4.2020545 -4.1484113 -4.10202 -4.0738888 -4.0660968 -4.0795784 -4.111227 -4.1534657 -4.2023973 -4.2478971 -4.2812781][-4.2940187 -4.2841759 -4.2618651 -4.2183194 -4.149292 -4.0667572 -3.9970474 -3.9597793 -3.955539 -3.9813545 -4.0352297 -4.1026053 -4.1713338 -4.2304311 -4.2713647][-4.2574453 -4.2547197 -4.2389259 -4.1972265 -4.1230826 -4.0279093 -3.9446898 -3.9035506 -3.906517 -3.9469829 -4.0210452 -4.1044364 -4.1787839 -4.236383 -4.2727389][-4.210259 -4.2213969 -4.2233162 -4.2026925 -4.1527739 -4.0790315 -4.0082283 -3.9725854 -3.976994 -4.0166097 -4.0864825 -4.160243 -4.219502 -4.2608528 -4.2835336][-4.1463828 -4.1751442 -4.2042685 -4.218329 -4.2081676 -4.1716661 -4.1258211 -4.0988722 -4.1004348 -4.1290007 -4.1781917 -4.2283649 -4.2645693 -4.2866073 -4.2953348][-4.09792 -4.1360106 -4.1821318 -4.223938 -4.2496586 -4.2480707 -4.2269545 -4.2080407 -4.2048664 -4.2205896 -4.2489891 -4.2779217 -4.2963519 -4.3035235 -4.3022723][-4.0750222 -4.1103368 -4.1585426 -4.21451 -4.2652059 -4.2905736 -4.2903562 -4.2804184 -4.2738342 -4.2785687 -4.2923594 -4.3072257 -4.3150673 -4.3136878 -4.3060293][-4.0708675 -4.0888605 -4.1266227 -4.1847749 -4.2489219 -4.2956686 -4.3140359 -4.314167 -4.3055043 -4.2990623 -4.3014789 -4.3077979 -4.3120441 -4.3087325 -4.2993956][-4.0728817 -4.0729308 -4.0984807 -4.152565 -4.2217426 -4.2801929 -4.3087387 -4.3121452 -4.2950234 -4.2745962 -4.2669892 -4.2709851 -4.280066 -4.2844014 -4.2800994][-4.0824676 -4.0729361 -4.0921226 -4.1413388 -4.2089734 -4.2664089 -4.2913985 -4.2836843 -4.2491727 -4.2121048 -4.1951194 -4.2015457 -4.2226191 -4.2411184 -4.2442493][-4.1109772 -4.09992 -4.1154447 -4.1567068 -4.2159166 -4.2629132 -4.2722487 -4.2442017 -4.186677 -4.1287613 -4.1018081 -4.113956 -4.1523137 -4.1881781 -4.1983161]]...]
INFO - root - 2017-12-07 13:16:24.537008: step 13010, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 58h:52m:42s remains)
INFO - root - 2017-12-07 13:16:31.258330: step 13020, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 62h:15m:35s remains)
INFO - root - 2017-12-07 13:16:38.185885: step 13030, loss = 2.04, batch loss = 1.98 (10.5 examples/sec; 0.759 sec/batch; 67h:21m:00s remains)
INFO - root - 2017-12-07 13:16:44.890928: step 13040, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 61h:20m:16s remains)
INFO - root - 2017-12-07 13:16:51.578070: step 13050, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 56h:35m:28s remains)
INFO - root - 2017-12-07 13:16:58.467240: step 13060, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 59h:07m:51s remains)
INFO - root - 2017-12-07 13:17:05.377350: step 13070, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 63h:23m:34s remains)
INFO - root - 2017-12-07 13:17:12.162391: step 13080, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 60h:38m:45s remains)
INFO - root - 2017-12-07 13:17:18.993099: step 13090, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 58h:24m:07s remains)
INFO - root - 2017-12-07 13:17:25.768337: step 13100, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 56h:48m:55s remains)
2017-12-07 13:17:26.483802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3485637 -4.3373837 -4.3224287 -4.3104444 -4.3021865 -4.2946162 -4.2878976 -4.28277 -4.2793751 -4.2737856 -4.26566 -4.2621131 -4.2648458 -4.2726297 -4.2775974][-4.3339906 -4.31675 -4.294827 -4.2738705 -4.2568712 -4.2426209 -4.2300563 -4.2232542 -4.2261658 -4.2247529 -4.2174053 -4.2173839 -4.2243776 -4.2346344 -4.2360353][-4.3123569 -4.2847357 -4.2518187 -4.216681 -4.1836553 -4.1549535 -4.1342597 -4.1295319 -4.1453161 -4.1488905 -4.1434703 -4.1517029 -4.1650863 -4.1775808 -4.1786132][-4.2864118 -4.24658 -4.1985774 -4.1459641 -4.0940375 -4.0497632 -4.024838 -4.0278168 -4.0554714 -4.0583782 -4.0539436 -4.067533 -4.0846343 -4.1016583 -4.1098771][-4.2620692 -4.2119479 -4.1462717 -4.0764751 -4.0074978 -3.9462659 -3.9184449 -3.9313407 -3.9657252 -3.9661894 -3.9589269 -3.9726918 -3.9930258 -4.0187492 -4.0405641][-4.2405648 -4.1808481 -4.0975418 -4.0103211 -3.9281402 -3.8467009 -3.810379 -3.8266287 -3.8629482 -3.8674126 -3.8677239 -3.8938723 -3.9244256 -3.9591255 -3.9909461][-4.2278371 -4.1576276 -4.0578747 -3.9580085 -3.8618624 -3.7531142 -3.692718 -3.7086315 -3.76266 -3.7866058 -3.805687 -3.8538313 -3.9068456 -3.9504068 -3.9816418][-4.220993 -4.1415434 -4.0287004 -3.9243782 -3.8204556 -3.7018704 -3.6326363 -3.6614244 -3.7476819 -3.801631 -3.836915 -3.8923624 -3.954601 -3.9989762 -4.0170212][-4.2157464 -4.1332865 -4.0252471 -3.9360852 -3.8542395 -3.7675009 -3.7230036 -3.7630033 -3.8503828 -3.907469 -3.9445343 -3.9910796 -4.045176 -4.0833659 -4.09567][-4.2194476 -4.1517081 -4.0688353 -4.0055165 -3.9494011 -3.8940082 -3.8675208 -3.8953879 -3.9584095 -3.9987278 -4.0296631 -4.0690923 -4.1211295 -4.1602392 -4.1797438][-4.2365842 -4.19105 -4.1374192 -4.0959811 -4.0547829 -4.0171294 -3.9987731 -4.0100193 -4.0451083 -4.0696878 -4.0921164 -4.1283908 -4.1783576 -4.2188654 -4.242487][-4.2631259 -4.2358422 -4.2035275 -4.174984 -4.1434326 -4.1166649 -4.1046233 -4.1064873 -4.1314063 -4.1535993 -4.1732726 -4.20298 -4.2394376 -4.2672181 -4.284287][-4.2848682 -4.2653036 -4.2423425 -4.2218924 -4.2005463 -4.1892061 -4.1903896 -4.1998606 -4.2236738 -4.2436972 -4.2560983 -4.2727084 -4.2898111 -4.3026276 -4.3103065][-4.2988105 -4.2805347 -4.2619143 -4.2504854 -4.2405415 -4.24091 -4.2525983 -4.2697721 -4.2916121 -4.3048129 -4.3068161 -4.3102131 -4.3158727 -4.3212509 -4.3266716][-4.3121786 -4.2937312 -4.2760663 -4.2689257 -4.2663674 -4.269383 -4.2801809 -4.2971654 -4.3162165 -4.3263292 -4.3266788 -4.3281527 -4.32961 -4.3318143 -4.3372169]]...]
INFO - root - 2017-12-07 13:17:33.218588: step 13110, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 61h:04m:41s remains)
INFO - root - 2017-12-07 13:17:39.902757: step 13120, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.658 sec/batch; 58h:20m:25s remains)
INFO - root - 2017-12-07 13:17:46.855365: step 13130, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 60h:24m:57s remains)
INFO - root - 2017-12-07 13:17:53.684824: step 13140, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.695 sec/batch; 61h:41m:52s remains)
INFO - root - 2017-12-07 13:18:00.509674: step 13150, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 62h:32m:50s remains)
INFO - root - 2017-12-07 13:18:07.389658: step 13160, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 60h:18m:47s remains)
INFO - root - 2017-12-07 13:18:14.260442: step 13170, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 57h:09m:04s remains)
INFO - root - 2017-12-07 13:18:21.074394: step 13180, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 61h:59m:58s remains)
INFO - root - 2017-12-07 13:18:27.920979: step 13190, loss = 2.04, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 63h:45m:56s remains)
INFO - root - 2017-12-07 13:18:34.707707: step 13200, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 62h:46m:56s remains)
2017-12-07 13:18:35.418973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2888775 -4.2824054 -4.278789 -4.2759857 -4.2729278 -4.2731595 -4.2774715 -4.28578 -4.2886353 -4.2830348 -4.2738423 -4.2705688 -4.2728004 -4.27054 -4.2719736][-4.2782879 -4.2691369 -4.263937 -4.2592607 -4.2544956 -4.2551451 -4.2599006 -4.2671881 -4.269619 -4.2666216 -4.2585278 -4.2572141 -4.258893 -4.2504039 -4.250093][-4.2577248 -4.2423806 -4.2300944 -4.2212529 -4.2138896 -4.2149987 -4.2209353 -4.228014 -4.229424 -4.2333694 -4.2322507 -4.235076 -4.2309046 -4.2151508 -4.214087][-4.2332358 -4.2095184 -4.1885896 -4.1719065 -4.1608481 -4.1583705 -4.1682849 -4.1790709 -4.1818557 -4.195272 -4.2070975 -4.2162704 -4.203361 -4.1776896 -4.1734166][-4.2131219 -4.1826539 -4.1552258 -4.1305285 -4.1131911 -4.103229 -4.1133289 -4.12628 -4.1317139 -4.150929 -4.1769028 -4.1949058 -4.1760325 -4.139842 -4.1290359][-4.2009883 -4.1694894 -4.1379642 -4.1039085 -4.0761147 -4.0565715 -4.0650678 -4.083075 -4.09205 -4.108768 -4.1417947 -4.165318 -4.1422057 -4.0987034 -4.0896811][-4.2004585 -4.1731253 -4.1385732 -4.0960808 -4.0542955 -4.0232768 -4.0274329 -4.0493932 -4.0653505 -4.0819678 -4.1138368 -4.1346664 -4.0981455 -4.0484076 -4.0555954][-4.2059803 -4.1848364 -4.1572838 -4.1146169 -4.0647416 -4.0282416 -4.0255189 -4.0444388 -4.0654693 -4.0716372 -4.0857587 -4.0981793 -4.0474257 -3.9923968 -4.0205464][-4.2101116 -4.1962872 -4.1825666 -4.1508842 -4.1108851 -4.078227 -4.0678 -4.0784969 -4.0935454 -4.0854864 -4.0826836 -4.0817761 -4.016758 -3.9621067 -4.0082436][-4.2069464 -4.1967583 -4.1993532 -4.1878176 -4.1668468 -4.1418314 -4.1239567 -4.1225214 -4.1275921 -4.1133904 -4.103858 -4.0978589 -4.0431557 -4.0026846 -4.0455689][-4.2027297 -4.1943855 -4.2019386 -4.2014742 -4.202991 -4.1870389 -4.15924 -4.1439013 -4.1400781 -4.1331115 -4.1297216 -4.1258802 -4.0855217 -4.0555243 -4.0848351][-4.2128811 -4.2054553 -4.2126155 -4.2107573 -4.2184987 -4.2075553 -4.1727366 -4.1467953 -4.1428313 -4.1523972 -4.1570449 -4.1584249 -4.1273975 -4.0998187 -4.1165161][-4.227284 -4.2216916 -4.2276235 -4.2223988 -4.2267079 -4.2193952 -4.1823082 -4.1582227 -4.1624141 -4.1792374 -4.1864514 -4.1878686 -4.1602821 -4.1301527 -4.1337843][-4.2404661 -4.2385006 -4.2458839 -4.2408876 -4.2390304 -4.2324133 -4.2019472 -4.1902547 -4.2044477 -4.2207417 -4.2268558 -4.2275758 -4.2030849 -4.1691966 -4.1632671][-4.2522564 -4.2543859 -4.26449 -4.2648139 -4.2636042 -4.2585444 -4.2393537 -4.2368665 -4.2525043 -4.2678308 -4.274663 -4.2738304 -4.2537727 -4.2253237 -4.21489]]...]
INFO - root - 2017-12-07 13:18:42.265016: step 13210, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 61h:34m:40s remains)
INFO - root - 2017-12-07 13:18:48.979950: step 13220, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.722 sec/batch; 64h:00m:47s remains)
INFO - root - 2017-12-07 13:18:55.769793: step 13230, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.677 sec/batch; 60h:00m:19s remains)
INFO - root - 2017-12-07 13:19:02.524892: step 13240, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 58h:04m:31s remains)
INFO - root - 2017-12-07 13:19:09.344015: step 13250, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 58h:11m:03s remains)
INFO - root - 2017-12-07 13:19:16.090022: step 13260, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 59h:23m:24s remains)
INFO - root - 2017-12-07 13:19:22.939782: step 13270, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 63h:02m:29s remains)
INFO - root - 2017-12-07 13:19:29.634299: step 13280, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 55h:29m:40s remains)
INFO - root - 2017-12-07 13:19:36.435167: step 13290, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 59h:04m:05s remains)
INFO - root - 2017-12-07 13:19:43.258492: step 13300, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.716 sec/batch; 63h:28m:08s remains)
2017-12-07 13:19:43.949822: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2313123 -4.2319231 -4.2345157 -4.2242341 -4.2097354 -4.1958146 -4.1723776 -4.1198483 -4.050045 -4.0058236 -4.0211234 -4.0593081 -4.0884237 -4.0975547 -4.0748777][-4.2568283 -4.2577977 -4.2559309 -4.2394733 -4.2189441 -4.2025385 -4.18735 -4.1491485 -4.0936503 -4.0594282 -4.0710945 -4.0918827 -4.1057234 -4.1097455 -4.0957637][-4.263195 -4.2641459 -4.2574968 -4.2380376 -4.2119284 -4.1927748 -4.1834135 -4.15674 -4.1151528 -4.0867181 -4.0866618 -4.0937438 -4.1001697 -4.1040878 -4.1010447][-4.2522178 -4.25252 -4.2453647 -4.2243085 -4.1907454 -4.1667347 -4.1587768 -4.1335874 -4.0995913 -4.0744405 -4.0707588 -4.0791192 -4.0895023 -4.1017942 -4.1033859][-4.2404795 -4.2405181 -4.2354245 -4.2138262 -4.170507 -4.1403384 -4.1296334 -4.1023917 -4.0676317 -4.037272 -4.0301747 -4.0506721 -4.0778041 -4.1000266 -4.1089468][-4.2238822 -4.2236772 -4.2160811 -4.1870351 -4.1352482 -4.1062841 -4.09958 -4.0752516 -4.0388737 -3.9950807 -3.9835 -4.0176377 -4.0650616 -4.0997343 -4.1165314][-4.1964264 -4.1971464 -4.1867085 -4.1523681 -4.0992289 -4.0772281 -4.0840788 -4.0672631 -4.0283489 -3.9755816 -3.9571486 -3.9982882 -4.0484734 -4.0847259 -4.1051598][-4.16334 -4.1595793 -4.1457109 -4.1115766 -4.0673385 -4.0599027 -4.075932 -4.0660143 -4.0304179 -3.9821866 -3.9657831 -3.995856 -4.0311589 -4.0596666 -4.0819154][-4.1370683 -4.1330404 -4.1206331 -4.0890083 -4.0586429 -4.0664945 -4.0907645 -4.0913444 -4.0654583 -4.0325246 -4.017971 -4.0259948 -4.0354643 -4.0444803 -4.0601363][-4.1416621 -4.13997 -4.1274872 -4.0953646 -4.0749722 -4.0897813 -4.1191278 -4.1261687 -4.1123013 -4.1004267 -4.0869412 -4.0743375 -4.05821 -4.046083 -4.052001][-4.1585283 -4.1549983 -4.1382465 -4.104177 -4.0925636 -4.1061134 -4.1255445 -4.1297951 -4.1242714 -4.1305943 -4.125463 -4.1056876 -4.0779877 -4.056881 -4.0560961][-4.1672435 -4.1625385 -4.1415362 -4.1066284 -4.0949588 -4.0973463 -4.1007419 -4.1041226 -4.1112604 -4.1299868 -4.1318879 -4.11205 -4.0903716 -4.0743346 -4.0735564][-4.1906896 -4.1809831 -4.1534657 -4.1169243 -4.0929489 -4.07875 -4.0752821 -4.083818 -4.1052423 -4.1293635 -4.1325889 -4.1150107 -4.1014709 -4.0938759 -4.094224][-4.20904 -4.1956549 -4.1704116 -4.1387768 -4.1063104 -4.0816832 -4.0697484 -4.076755 -4.1013618 -4.1247783 -4.1287208 -4.1174817 -4.1088619 -4.1049633 -4.1049838][-4.2055426 -4.1917491 -4.1730213 -4.1495757 -4.1189427 -4.0924268 -4.0785713 -4.0828633 -4.1047688 -4.1247754 -4.1310368 -4.1273484 -4.1222334 -4.1194654 -4.1199179]]...]
INFO - root - 2017-12-07 13:19:50.581662: step 13310, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.612 sec/batch; 54h:16m:13s remains)
INFO - root - 2017-12-07 13:19:57.261895: step 13320, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 59h:44m:33s remains)
INFO - root - 2017-12-07 13:20:04.121234: step 13330, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 63h:38m:51s remains)
INFO - root - 2017-12-07 13:20:10.950599: step 13340, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 59h:58m:57s remains)
INFO - root - 2017-12-07 13:20:17.645105: step 13350, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.617 sec/batch; 54h:40m:29s remains)
INFO - root - 2017-12-07 13:20:24.522797: step 13360, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 59h:19m:08s remains)
INFO - root - 2017-12-07 13:20:31.288415: step 13370, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 62h:26m:59s remains)
INFO - root - 2017-12-07 13:20:38.048692: step 13380, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 60h:37m:41s remains)
INFO - root - 2017-12-07 13:20:44.756377: step 13390, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 60h:59m:09s remains)
INFO - root - 2017-12-07 13:20:51.506812: step 13400, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 57h:28m:18s remains)
2017-12-07 13:20:52.226583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2436762 -4.2379794 -4.242034 -4.2490916 -4.2393837 -4.22293 -4.2109542 -4.2066221 -4.2015443 -4.201458 -4.2071795 -4.2107248 -4.2072225 -4.2005033 -4.2015405][-4.2181945 -4.2020912 -4.201931 -4.211524 -4.2095323 -4.2008557 -4.196465 -4.1966152 -4.2003417 -4.2065864 -4.2185221 -4.2293053 -4.2254672 -4.21028 -4.1997447][-4.1960483 -4.1739287 -4.174901 -4.1928883 -4.203052 -4.2008815 -4.1992412 -4.2007017 -4.2143612 -4.2258577 -4.2394509 -4.2474914 -4.242898 -4.2276368 -4.2143369][-4.1911697 -4.171638 -4.1776452 -4.2015376 -4.2144251 -4.2093158 -4.1973939 -4.2007732 -4.2268424 -4.2441154 -4.2569985 -4.2632251 -4.2599421 -4.2462673 -4.227869][-4.2022672 -4.1830282 -4.1889138 -4.2097249 -4.2152658 -4.1998124 -4.1703272 -4.1730103 -4.2160931 -4.2468896 -4.2585969 -4.2642236 -4.2618365 -4.2500057 -4.2256441][-4.2138863 -4.1936021 -4.1903529 -4.19396 -4.187603 -4.1544738 -4.0981727 -4.0956469 -4.1656637 -4.21686 -4.2319603 -4.2392931 -4.2416115 -4.2300844 -4.20133][-4.2051487 -4.18921 -4.1753068 -4.1597962 -4.1360288 -4.0774794 -3.9861102 -3.9703848 -4.0754 -4.1596351 -4.1956434 -4.2127585 -4.2195892 -4.2065215 -4.1728644][-4.1824408 -4.16713 -4.1431203 -4.1084037 -4.0671263 -3.9895234 -3.8745856 -3.8535771 -3.9894145 -4.1036296 -4.1622224 -4.1938262 -4.2052875 -4.1936517 -4.158391][-4.1637444 -4.1491671 -4.1237173 -4.0861678 -4.0528841 -4.005043 -3.9322603 -3.9203613 -4.0189576 -4.1113706 -4.1664209 -4.20047 -4.2110081 -4.2015324 -4.1674752][-4.1622462 -4.1579132 -4.14268 -4.1205077 -4.1104255 -4.1009927 -4.075923 -4.0669484 -4.1104188 -4.165082 -4.1999068 -4.2208686 -4.2257342 -4.2182178 -4.1899714][-4.1794991 -4.1869812 -4.1868486 -4.1830444 -4.1805158 -4.1821795 -4.1748347 -4.1662073 -4.180234 -4.2073197 -4.2233267 -4.2296672 -4.231113 -4.2271457 -4.2089019][-4.19623 -4.2083397 -4.2204943 -4.2245312 -4.2222047 -4.22088 -4.2132955 -4.2039657 -4.2079487 -4.2186041 -4.2229743 -4.223002 -4.2288709 -4.2348986 -4.2300639][-4.2073336 -4.2172055 -4.2328343 -4.241015 -4.2425795 -4.2416577 -4.2346621 -4.2256331 -4.2266159 -4.23021 -4.2304139 -4.2313256 -4.2413955 -4.2545094 -4.2605591][-4.2160439 -4.2211733 -4.2383041 -4.2533045 -4.2624483 -4.26615 -4.2651191 -4.2647071 -4.2675824 -4.2691731 -4.2671728 -4.2686872 -4.2742105 -4.2842822 -4.2954874][-4.2440839 -4.2459087 -4.2584209 -4.2729235 -4.285038 -4.2917538 -4.2962546 -4.3013458 -4.3033948 -4.3027816 -4.2987375 -4.2956128 -4.2943625 -4.2983971 -4.3082814]]...]
INFO - root - 2017-12-07 13:20:58.924261: step 13410, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 61h:07m:40s remains)
INFO - root - 2017-12-07 13:21:05.526334: step 13420, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 60h:20m:55s remains)
INFO - root - 2017-12-07 13:21:12.318312: step 13430, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 58h:44m:47s remains)
INFO - root - 2017-12-07 13:21:19.089607: step 13440, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 57h:20m:35s remains)
INFO - root - 2017-12-07 13:21:25.872108: step 13450, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 61h:44m:25s remains)
INFO - root - 2017-12-07 13:21:32.649256: step 13460, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 63h:06m:33s remains)
INFO - root - 2017-12-07 13:21:39.467214: step 13470, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 62h:49m:23s remains)
INFO - root - 2017-12-07 13:21:46.153294: step 13480, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 54h:23m:15s remains)
INFO - root - 2017-12-07 13:21:52.906203: step 13490, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 59h:07m:08s remains)
INFO - root - 2017-12-07 13:21:59.796193: step 13500, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 63h:08m:18s remains)
2017-12-07 13:22:00.441844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2723346 -4.2748919 -4.2739825 -4.27399 -4.2749076 -4.2759547 -4.2771182 -4.2787576 -4.2812023 -4.2830753 -4.2837172 -4.2834735 -4.2823243 -4.2806559 -4.2789321][-4.275104 -4.2782092 -4.2786241 -4.2797494 -4.2812929 -4.2826295 -4.2836022 -4.284955 -4.28759 -4.2900648 -4.2912335 -4.2910085 -4.2890973 -4.285953 -4.2822056][-4.274766 -4.2787476 -4.2804751 -4.2815733 -4.2818651 -4.281311 -4.2800527 -4.2793803 -4.2804 -4.2834725 -4.2875366 -4.2908893 -4.2913642 -4.2891979 -4.2856531][-4.2705736 -4.2745028 -4.2749267 -4.2725468 -4.2679877 -4.2619443 -4.2559419 -4.25193 -4.2515721 -4.2560315 -4.2650948 -4.2756233 -4.2831063 -4.2864494 -4.2863655][-4.2580686 -4.2602243 -4.2568669 -4.2490444 -4.2385859 -4.2287526 -4.2203751 -4.2156858 -4.2157779 -4.2217064 -4.2344456 -4.2496533 -4.2626977 -4.2721081 -4.2776332][-4.2374463 -4.2383766 -4.2335887 -4.224309 -4.2136803 -4.20533 -4.1991982 -4.1952748 -4.1937594 -4.19632 -4.2050123 -4.2167282 -4.2292695 -4.2423558 -4.2528229][-4.2105017 -4.2129545 -4.2108026 -4.2042847 -4.1963511 -4.1911068 -4.1877451 -4.1849713 -4.1803064 -4.1751781 -4.1743217 -4.1759024 -4.1813455 -4.193367 -4.2063766][-4.1813254 -4.1850257 -4.1854706 -4.1818972 -4.1745977 -4.1686897 -4.1645112 -4.1618857 -4.1557913 -4.1448655 -4.1363025 -4.128437 -4.1257324 -4.1342506 -4.1465669][-4.1542764 -4.1549931 -4.1541572 -4.1516366 -4.1454163 -4.1406655 -4.1382742 -4.1358404 -4.1290765 -4.1150365 -4.101326 -4.087779 -4.0802855 -4.0870166 -4.0978532][-4.1446977 -4.1415186 -4.1392984 -4.1377993 -4.1334271 -4.1309748 -4.1313372 -4.1295338 -4.122066 -4.1077046 -4.0932822 -4.0788078 -4.07107 -4.0777512 -4.0878396][-4.1611404 -4.1571155 -4.1551552 -4.1544042 -4.1511841 -4.14955 -4.1488771 -4.1435356 -4.1317668 -4.1160293 -4.103282 -4.0924478 -4.0896144 -4.0994425 -4.1103234][-4.1867771 -4.1851792 -4.1844621 -4.1838946 -4.180068 -4.1766567 -4.17237 -4.1623344 -4.1463652 -4.1291604 -4.1178045 -4.1111274 -4.1126056 -4.1243491 -4.1359859][-4.2079296 -4.2085037 -4.2090321 -4.2090983 -4.2067962 -4.2033372 -4.1975255 -4.1855445 -4.1689639 -4.1517768 -4.1403613 -4.1351314 -4.1371365 -4.1474481 -4.1569619][-4.2202258 -4.2210913 -4.2221842 -4.2232585 -4.2239318 -4.22289 -4.2192764 -4.210072 -4.1967397 -4.1825113 -4.1708841 -4.1643806 -4.1641221 -4.1701365 -4.175704][-4.2281437 -4.2285638 -4.2296443 -4.2315307 -4.2333913 -4.23351 -4.2316494 -4.2253261 -4.2148275 -4.2033758 -4.1935344 -4.1881471 -4.1878824 -4.1908393 -4.1930513]]...]
INFO - root - 2017-12-07 13:22:07.227231: step 13510, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 59h:35m:08s remains)
INFO - root - 2017-12-07 13:22:14.008002: step 13520, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 60h:34m:05s remains)
INFO - root - 2017-12-07 13:22:20.769561: step 13530, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 62h:38m:24s remains)
INFO - root - 2017-12-07 13:22:27.516751: step 13540, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 58h:59m:51s remains)
INFO - root - 2017-12-07 13:22:34.489105: step 13550, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 60h:21m:34s remains)
INFO - root - 2017-12-07 13:22:41.136616: step 13560, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.626 sec/batch; 55h:28m:41s remains)
INFO - root - 2017-12-07 13:22:47.749284: step 13570, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 57h:36m:35s remains)
INFO - root - 2017-12-07 13:22:54.610920: step 13580, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 62h:08m:33s remains)
INFO - root - 2017-12-07 13:23:01.407696: step 13590, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 61h:21m:52s remains)
INFO - root - 2017-12-07 13:23:08.129055: step 13600, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 57h:25m:54s remains)
2017-12-07 13:23:08.841748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2536545 -4.2497411 -4.2256503 -4.1907096 -4.1646495 -4.1388807 -4.1290421 -4.128788 -4.0960879 -4.0570297 -4.0560875 -4.0834227 -4.1111889 -4.1314154 -4.13766][-4.2605648 -4.2583394 -4.2324848 -4.1933317 -4.1607847 -4.1289907 -4.113431 -4.1126714 -4.0960393 -4.075604 -4.0805006 -4.0923481 -4.0907707 -4.0943036 -4.1009388][-4.2568588 -4.2539506 -4.227881 -4.1858945 -4.1449041 -4.1045966 -4.0802655 -4.0789266 -4.0845175 -4.0918789 -4.106267 -4.10413 -4.0748472 -4.0602837 -4.0690475][-4.2469778 -4.2403049 -4.2138491 -4.1684108 -4.1184335 -4.0707126 -4.0393376 -4.0417175 -4.0696921 -4.10523 -4.1286545 -4.1196542 -4.0759988 -4.05331 -4.0640883][-4.2428336 -4.2371669 -4.209095 -4.1619897 -4.1068788 -4.0498414 -4.0083809 -4.0101137 -4.0591688 -4.1125393 -4.1446743 -4.136343 -4.0893326 -4.0575995 -4.0653586][-4.2414613 -4.2377577 -4.2103553 -4.1625886 -4.0982718 -4.0219159 -3.9567866 -3.9616537 -4.0416317 -4.1161542 -4.1534872 -4.1528978 -4.1147056 -4.0773573 -4.08143][-4.2332191 -4.2307596 -4.2068534 -4.1575856 -4.0811186 -3.9783022 -3.8735547 -3.8818817 -4.0090079 -4.1105242 -4.1584783 -4.1681223 -4.1463661 -4.1190777 -4.1180735][-4.2266717 -4.2245226 -4.2004642 -4.1465297 -4.0631218 -3.9425385 -3.7984593 -3.8109422 -3.9782095 -4.0978603 -4.1524448 -4.1751103 -4.1733713 -4.1605048 -4.1538115][-4.2221155 -4.2157764 -4.1897831 -4.1363239 -4.0627508 -3.9633737 -3.8472981 -3.8695545 -4.0055881 -4.0988851 -4.145833 -4.1788926 -4.195899 -4.1965942 -4.1866231][-4.2109723 -4.19948 -4.1717992 -4.1257763 -4.0755196 -4.0180054 -3.9607451 -3.9855354 -4.0613904 -4.1099987 -4.1445184 -4.1808991 -4.2088757 -4.2194171 -4.2068272][-4.2018943 -4.1842318 -4.1555839 -4.1201062 -4.0912132 -4.0663304 -4.0452662 -4.0635219 -4.0979466 -4.1223149 -4.148 -4.181519 -4.2118711 -4.2248421 -4.2067914][-4.1934104 -4.1785564 -4.1507969 -4.1237936 -4.1065707 -4.101058 -4.0968022 -4.1052108 -4.1189828 -4.1333251 -4.1522493 -4.179193 -4.2045655 -4.2148032 -4.1948729][-4.1802883 -4.1723456 -4.1483812 -4.1271992 -4.118114 -4.1286793 -4.1322155 -4.1317768 -4.1325822 -4.1407108 -4.1533866 -4.1703157 -4.1881022 -4.1942329 -4.1751504][-4.1552467 -4.1540871 -4.1401367 -4.1270928 -4.1249151 -4.1439075 -4.15281 -4.1506114 -4.1479449 -4.1544623 -4.1625586 -4.1675892 -4.1711574 -4.1705713 -4.1528144][-4.1346822 -4.1384645 -4.1322365 -4.1219397 -4.1233892 -4.1419816 -4.1461511 -4.1433854 -4.1459641 -4.1606584 -4.1735606 -4.1763854 -4.1688232 -4.16422 -4.1492462]]...]
INFO - root - 2017-12-07 13:23:15.695066: step 13610, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 61h:40m:41s remains)
INFO - root - 2017-12-07 13:23:22.264346: step 13620, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 60h:37m:23s remains)
INFO - root - 2017-12-07 13:23:29.074805: step 13630, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 57h:42m:28s remains)
INFO - root - 2017-12-07 13:23:35.758687: step 13640, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 57h:48m:48s remains)
INFO - root - 2017-12-07 13:23:42.570344: step 13650, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 61h:54m:41s remains)
INFO - root - 2017-12-07 13:23:49.370735: step 13660, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 61h:13m:24s remains)
INFO - root - 2017-12-07 13:23:56.232957: step 13670, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 59h:04m:25s remains)
INFO - root - 2017-12-07 13:24:03.104808: step 13680, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 57h:17m:21s remains)
INFO - root - 2017-12-07 13:24:09.909980: step 13690, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 62h:04m:35s remains)
INFO - root - 2017-12-07 13:24:16.686723: step 13700, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 60h:51m:57s remains)
2017-12-07 13:24:17.337561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1276522 -4.1288843 -4.1479774 -4.1752615 -4.1910739 -4.1960149 -4.1997719 -4.1996365 -4.1942248 -4.1945872 -4.2036881 -4.2047124 -4.1993766 -4.1927471 -4.1960459][-4.1270275 -4.1343522 -4.15932 -4.1908956 -4.2041779 -4.204679 -4.2026167 -4.1971688 -4.1834679 -4.1714745 -4.167583 -4.16964 -4.1756816 -4.1777411 -4.1862769][-4.1352448 -4.1562629 -4.1838765 -4.2105403 -4.2182198 -4.210391 -4.200109 -4.1920772 -4.1743979 -4.1532946 -4.1369643 -4.1341386 -4.1410389 -4.1483264 -4.1623192][-4.1448936 -4.1737518 -4.1995196 -4.2155814 -4.21719 -4.2054515 -4.1851654 -4.1706333 -4.1547737 -4.1348991 -4.116189 -4.1118059 -4.1202936 -4.136107 -4.1553073][-4.1554441 -4.1816163 -4.2018461 -4.2074513 -4.1991315 -4.1736503 -4.1401434 -4.1181903 -4.1116357 -4.1013865 -4.0907006 -4.0960956 -4.1121855 -4.1340704 -4.157002][-4.1672125 -4.1854811 -4.200109 -4.1948123 -4.1681819 -4.1158137 -4.0527878 -4.0271716 -4.0483532 -4.0644217 -4.0699329 -4.0888405 -4.1100049 -4.1359186 -4.1630955][-4.1672635 -4.1778336 -4.190742 -4.1763635 -4.1267185 -4.037456 -3.9313009 -3.9077644 -3.9800344 -4.0416622 -4.0693884 -4.0893908 -4.1021171 -4.1228609 -4.1486869][-4.16035 -4.16531 -4.1792178 -4.1616726 -4.1037045 -4.0053077 -3.8920796 -3.8810554 -3.981355 -4.062007 -4.0949583 -4.1008425 -4.095892 -4.1069241 -4.1309237][-4.1667056 -4.1690764 -4.1794634 -4.1618428 -4.1176772 -4.0586562 -3.9969735 -4.0005713 -4.0722423 -4.1288857 -4.1459355 -4.1330605 -4.114562 -4.115242 -4.13327][-4.1851473 -4.1876593 -4.1989622 -4.1875033 -4.1624937 -4.1347442 -4.105269 -4.1105881 -4.1484084 -4.1780214 -4.1804461 -4.1618261 -4.136868 -4.1319761 -4.14756][-4.2050357 -4.2109928 -4.2215462 -4.2160783 -4.200973 -4.1824961 -4.1634974 -4.1631436 -4.1803427 -4.1914458 -4.19003 -4.1768193 -4.1516614 -4.1429892 -4.1542239][-4.225071 -4.2238026 -4.2226362 -4.2165713 -4.2034211 -4.1853375 -4.1714616 -4.1716428 -4.1814528 -4.1889687 -4.1940989 -4.1902952 -4.1713505 -4.1597342 -4.1613126][-4.2330389 -4.2198043 -4.2028589 -4.19134 -4.1786842 -4.1628728 -4.1587458 -4.1625032 -4.1746778 -4.1880507 -4.2052441 -4.2132721 -4.2075243 -4.2003202 -4.1935534][-4.2246351 -4.2033548 -4.1786871 -4.1622534 -4.1551476 -4.1527104 -4.1592808 -4.1672115 -4.1817789 -4.1953578 -4.2137742 -4.2299538 -4.2363014 -4.2365255 -4.230886][-4.208034 -4.1891103 -4.1699772 -4.1605806 -4.1648955 -4.1698866 -4.1749992 -4.1823392 -4.1960449 -4.2039108 -4.2181025 -4.2322278 -4.2415538 -4.2469325 -4.2437372]]...]
INFO - root - 2017-12-07 13:24:24.092037: step 13710, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 58h:23m:50s remains)
INFO - root - 2017-12-07 13:24:30.724220: step 13720, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.705 sec/batch; 62h:23m:20s remains)
INFO - root - 2017-12-07 13:24:37.625112: step 13730, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 61h:01m:23s remains)
INFO - root - 2017-12-07 13:24:44.474492: step 13740, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 60h:55m:00s remains)
INFO - root - 2017-12-07 13:24:51.255249: step 13750, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 56h:56m:42s remains)
INFO - root - 2017-12-07 13:24:58.003488: step 13760, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 59h:22m:58s remains)
INFO - root - 2017-12-07 13:25:04.882231: step 13770, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 65h:39m:44s remains)
INFO - root - 2017-12-07 13:25:11.778875: step 13780, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 65h:19m:21s remains)
INFO - root - 2017-12-07 13:25:18.603891: step 13790, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 58h:07m:06s remains)
INFO - root - 2017-12-07 13:25:25.319360: step 13800, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 57h:52m:30s remains)
2017-12-07 13:25:26.008796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.261024 -4.2563744 -4.2620416 -4.2726121 -4.2715292 -4.2627263 -4.25732 -4.2588434 -4.2682147 -4.2824249 -4.301877 -4.3156824 -4.3172536 -4.3089242 -4.296957][-4.2444253 -4.2335215 -4.2378087 -4.2548628 -4.2579055 -4.246233 -4.2349682 -4.2323375 -4.2413311 -4.2589607 -4.28579 -4.302742 -4.30384 -4.296073 -4.2858825][-4.2285318 -4.209281 -4.20827 -4.2299504 -4.2393551 -4.2287235 -4.2120895 -4.1970654 -4.1940265 -4.2113485 -4.2462749 -4.2702661 -4.2732682 -4.2707949 -4.2673969][-4.2187762 -4.1956568 -4.1911416 -4.2101526 -4.2175622 -4.2057819 -4.1837239 -4.154027 -4.129724 -4.1424861 -4.1934204 -4.2328906 -4.2443728 -4.2500868 -4.25307][-4.2145548 -4.1933622 -4.1868362 -4.1939154 -4.1926184 -4.1781025 -4.1501226 -4.1018233 -4.04811 -4.0573111 -4.1352258 -4.1957684 -4.2207532 -4.2353163 -4.2430463][-4.21143 -4.1933813 -4.1873822 -4.184443 -4.1720715 -4.1550136 -4.1214023 -4.0385628 -3.9418545 -3.9503746 -4.0680981 -4.1608863 -4.2052236 -4.2280293 -4.2358584][-4.2161493 -4.2055511 -4.2017541 -4.1892924 -4.1668491 -4.1458817 -4.1001306 -3.9790707 -3.8358226 -3.8437376 -4.0109487 -4.140676 -4.2055941 -4.2333355 -4.2364335][-4.2304683 -4.2298341 -4.2264762 -4.2034421 -4.17105 -4.1436014 -4.0917344 -3.9588909 -3.8026323 -3.8029664 -3.9910269 -4.1362939 -4.211813 -4.2383652 -4.2357044][-4.2435274 -4.2500815 -4.2517128 -4.2224717 -4.1822872 -4.1515055 -4.1125097 -4.0058556 -3.8893814 -3.8923187 -4.0388155 -4.1529098 -4.2191362 -4.2375984 -4.2332296][-4.2501469 -4.2632103 -4.2698874 -4.2384825 -4.1960983 -4.1737385 -4.1608462 -4.0918417 -4.0252514 -4.0342326 -4.1198649 -4.1835823 -4.2221756 -4.2297168 -4.2248259][-4.2547026 -4.2735405 -4.2897248 -4.2598882 -4.2195768 -4.2015095 -4.2094259 -4.1815 -4.1523628 -4.1610641 -4.2022929 -4.2254171 -4.2321415 -4.2270756 -4.2228932][-4.2656054 -4.285603 -4.3063722 -4.2864485 -4.2577162 -4.2462773 -4.2616906 -4.2627935 -4.2540565 -4.2548213 -4.268518 -4.2626505 -4.2462521 -4.232513 -4.2327719][-4.284441 -4.3017464 -4.317657 -4.3054981 -4.2881393 -4.2822509 -4.2928734 -4.3035221 -4.3041749 -4.2994976 -4.2951603 -4.2745628 -4.2474856 -4.2343392 -4.2456074][-4.2996769 -4.3107448 -4.317884 -4.3087163 -4.3013577 -4.3003616 -4.3086967 -4.3201327 -4.3235826 -4.322156 -4.314302 -4.2892194 -4.2600708 -4.2501431 -4.2705221][-4.3052249 -4.3081608 -4.304297 -4.2950659 -4.2959127 -4.3051586 -4.3178082 -4.3323112 -4.3401041 -4.34376 -4.3328176 -4.3065414 -4.2802258 -4.27335 -4.292737]]...]
INFO - root - 2017-12-07 13:25:32.800530: step 13810, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 59h:53m:04s remains)
INFO - root - 2017-12-07 13:25:39.502552: step 13820, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 60h:56m:14s remains)
INFO - root - 2017-12-07 13:25:46.213675: step 13830, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 55h:51m:12s remains)
INFO - root - 2017-12-07 13:25:52.935765: step 13840, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 58h:45m:59s remains)
INFO - root - 2017-12-07 13:25:59.885825: step 13850, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 62h:30m:05s remains)
INFO - root - 2017-12-07 13:26:06.767154: step 13860, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 59h:00m:07s remains)
INFO - root - 2017-12-07 13:26:13.611767: step 13870, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 58h:47m:28s remains)
INFO - root - 2017-12-07 13:26:20.327423: step 13880, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 59h:11m:41s remains)
INFO - root - 2017-12-07 13:26:27.129893: step 13890, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 60h:16m:25s remains)
INFO - root - 2017-12-07 13:26:33.898666: step 13900, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 60h:27m:34s remains)
2017-12-07 13:26:34.624322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2686138 -4.2498693 -4.2347093 -4.2299724 -4.2307115 -4.2361526 -4.2470317 -4.2531557 -4.2545438 -4.2551408 -4.2522674 -4.2537961 -4.2597623 -4.2663169 -4.2690215][-4.260344 -4.2421656 -4.2270174 -4.2196059 -4.2179456 -4.2248421 -4.2403693 -4.2531123 -4.2584791 -4.2582846 -4.2555242 -4.2571015 -4.2601261 -4.266006 -4.270257][-4.2590909 -4.2377658 -4.2159843 -4.2018261 -4.194943 -4.2015419 -4.2230268 -4.2460237 -4.2613354 -4.2661281 -4.2662644 -4.26714 -4.2655668 -4.2675362 -4.2689457][-4.2583852 -4.2309275 -4.1973248 -4.1691127 -4.1488118 -4.14496 -4.1628447 -4.1931286 -4.2205386 -4.2386103 -4.2532129 -4.2641644 -4.267436 -4.26927 -4.2683449][-4.2567878 -4.2227359 -4.1770906 -4.1297059 -4.08882 -4.0658231 -4.0671425 -4.0931425 -4.1282296 -4.1679068 -4.2101369 -4.2437644 -4.2610292 -4.2659149 -4.2657385][-4.2612906 -4.2243805 -4.1746097 -4.115634 -4.05779 -4.0115037 -3.9831374 -3.9870434 -4.0191112 -4.0784945 -4.1485357 -4.2074256 -4.2424612 -4.2543092 -4.257082][-4.2673759 -4.2362556 -4.1949596 -4.1425228 -4.0840344 -4.0248504 -3.9720285 -3.9463735 -3.9599638 -4.0199022 -4.1005898 -4.1732049 -4.220098 -4.2394304 -4.2468724][-4.2766395 -4.255856 -4.229413 -4.1943884 -4.151371 -4.1019025 -4.0451407 -4.0003028 -3.9903128 -4.0287123 -4.0937939 -4.1589851 -4.2070312 -4.2306662 -4.2420397][-4.2778392 -4.2675643 -4.2540121 -4.2374954 -4.21702 -4.1895366 -4.1474652 -4.1028886 -4.0780358 -4.09219 -4.1297393 -4.1743941 -4.2134995 -4.2370615 -4.2481279][-4.2665906 -4.2636027 -4.25832 -4.2545481 -4.2525439 -4.2430639 -4.2201395 -4.1901855 -4.1696682 -4.1735148 -4.1909413 -4.2136321 -4.239388 -4.2568655 -4.26221][-4.2498136 -4.2426734 -4.2348623 -4.2349954 -4.2440205 -4.2484155 -4.2445383 -4.2331491 -4.2245564 -4.2284207 -4.2370024 -4.2452416 -4.2605071 -4.2699046 -4.2675753][-4.2338505 -4.21823 -4.2027607 -4.2018557 -4.2163134 -4.2278733 -4.2358866 -4.2369518 -4.2375531 -4.2434392 -4.2481704 -4.2513313 -4.2606244 -4.2642651 -4.25731][-4.2293754 -4.2083979 -4.1875629 -4.1839247 -4.1971917 -4.2106018 -4.2223268 -4.2273808 -4.230792 -4.2364011 -4.2387176 -4.2393432 -4.24374 -4.2435012 -4.2361288][-4.2519097 -4.2344 -4.2153034 -4.2105346 -4.2165718 -4.2227855 -4.2301788 -4.2343063 -4.2384996 -4.2437572 -4.2453566 -4.2457752 -4.2478533 -4.2461839 -4.2402911][-4.288764 -4.2806792 -4.2690105 -4.2661161 -4.2676582 -4.2684 -4.2691293 -4.2699637 -4.2720032 -4.2744164 -4.2754326 -4.2764807 -4.2779651 -4.2768879 -4.2732973]]...]
INFO - root - 2017-12-07 13:26:41.330202: step 13910, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 59h:17m:36s remains)
INFO - root - 2017-12-07 13:26:48.027331: step 13920, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 64h:27m:52s remains)
INFO - root - 2017-12-07 13:26:54.880425: step 13930, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 60h:26m:46s remains)
INFO - root - 2017-12-07 13:27:01.678147: step 13940, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 55h:40m:46s remains)
INFO - root - 2017-12-07 13:27:08.549202: step 13950, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 61h:53m:13s remains)
INFO - root - 2017-12-07 13:27:15.382086: step 13960, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 62h:35m:25s remains)
INFO - root - 2017-12-07 13:27:22.130755: step 13970, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 62h:49m:20s remains)
INFO - root - 2017-12-07 13:27:28.898271: step 13980, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 59h:09m:39s remains)
INFO - root - 2017-12-07 13:27:35.684032: step 13990, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 58h:29m:37s remains)
INFO - root - 2017-12-07 13:27:42.535372: step 14000, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 58h:18m:04s remains)
2017-12-07 13:27:43.330847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0827184 -4.0957913 -4.1213436 -4.1180048 -4.1101012 -4.1080842 -4.1168385 -4.128643 -4.1465325 -4.1817079 -4.1950655 -4.1871915 -4.1871514 -4.2057924 -4.2279811][-4.0606174 -4.0676913 -4.0986323 -4.1038036 -4.1071472 -4.109787 -4.1196117 -4.1323705 -4.1477981 -4.1809769 -4.1915326 -4.1835623 -4.1860342 -4.209949 -4.2328849][-4.0590668 -4.0618787 -4.0933313 -4.1032329 -4.1152821 -4.11787 -4.1249371 -4.1312609 -4.1387033 -4.1605082 -4.1694064 -4.1663718 -4.1782246 -4.2091327 -4.2346134][-4.0728946 -4.0774517 -4.1104016 -4.1225309 -4.1299963 -4.11997 -4.1190858 -4.116148 -4.1124468 -4.1196961 -4.12449 -4.1299763 -4.1576076 -4.1997347 -4.2314134][-4.0915346 -4.0982533 -4.1282163 -4.1388416 -4.1337 -4.1014771 -4.0909171 -4.0843916 -4.076705 -4.0788722 -4.0783138 -4.0869861 -4.1297588 -4.1817284 -4.2198915][-4.1016417 -4.1061769 -4.1285639 -4.1346388 -4.1108761 -4.0599785 -4.0404997 -4.0340824 -4.032537 -4.0473895 -4.0557261 -4.0728655 -4.1259408 -4.1811142 -4.2208][-4.1062131 -4.1110649 -4.126513 -4.126235 -4.0927 -4.0398884 -4.0197139 -4.0141826 -4.0246024 -4.0549641 -4.0721326 -4.0914497 -4.1411738 -4.191062 -4.2258606][-4.1158986 -4.1209426 -4.1332583 -4.1352153 -4.108263 -4.0630531 -4.0434704 -4.040298 -4.0576353 -4.0859513 -4.0961967 -4.1068721 -4.144558 -4.186626 -4.2164664][-4.1344328 -4.1399379 -4.1551714 -4.164001 -4.1500726 -4.1191673 -4.0991144 -4.0926442 -4.1027875 -4.1204114 -4.1250219 -4.1304336 -4.1586409 -4.1899714 -4.2146173][-4.16791 -4.1785593 -4.1972442 -4.2077322 -4.2009883 -4.1804357 -4.1605458 -4.1493545 -4.1525702 -4.1679311 -4.177218 -4.1823897 -4.2008924 -4.22374 -4.2423077][-4.2106285 -4.222887 -4.2437367 -4.2543631 -4.2500911 -4.2399349 -4.223434 -4.2061396 -4.2056293 -4.2209125 -4.2353773 -4.2391524 -4.2511177 -4.2697706 -4.2839413][-4.2436109 -4.2573085 -4.2816229 -4.2939095 -4.2902904 -4.2833247 -4.2690196 -4.2496653 -4.2488995 -4.2598977 -4.2738709 -4.2791672 -4.2902794 -4.3059378 -4.3162203][-4.2513075 -4.2655911 -4.2908244 -4.3034 -4.3018532 -4.2990484 -4.2924027 -4.2772522 -4.27448 -4.2786 -4.2856412 -4.29012 -4.3004394 -4.3150434 -4.325459][-4.2528086 -4.2641215 -4.2828345 -4.29271 -4.2906356 -4.2900739 -4.2887526 -4.2844834 -4.2850151 -4.2879157 -4.2913485 -4.295074 -4.3030162 -4.3140121 -4.3224897][-4.2722025 -4.27725 -4.2851186 -4.2893229 -4.2868595 -4.2861595 -4.2875695 -4.2914882 -4.2975793 -4.3018012 -4.304347 -4.3064876 -4.3108759 -4.3167772 -4.3219357]]...]
INFO - root - 2017-12-07 13:27:50.044947: step 14010, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 61h:18m:19s remains)
INFO - root - 2017-12-07 13:27:56.667064: step 14020, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 57h:12m:59s remains)
INFO - root - 2017-12-07 13:28:03.492834: step 14030, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 60h:58m:11s remains)
INFO - root - 2017-12-07 13:28:10.349618: step 14040, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.716 sec/batch; 63h:22m:20s remains)
INFO - root - 2017-12-07 13:28:17.250166: step 14050, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 63h:25m:49s remains)
INFO - root - 2017-12-07 13:28:24.030958: step 14060, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 60h:23m:29s remains)
INFO - root - 2017-12-07 13:28:30.759838: step 14070, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 55h:47m:49s remains)
INFO - root - 2017-12-07 13:28:37.604428: step 14080, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 64h:29m:16s remains)
INFO - root - 2017-12-07 13:28:44.450372: step 14090, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 60h:35m:47s remains)
INFO - root - 2017-12-07 13:28:51.294121: step 14100, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 60h:51m:59s remains)
2017-12-07 13:28:52.144712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.257637 -4.2539372 -4.2517371 -4.2528067 -4.254055 -4.2525663 -4.2508354 -4.2486067 -4.2438531 -4.2377338 -4.235085 -4.2427268 -4.2539186 -4.2626615 -4.2716274][-4.2272682 -4.2197137 -4.2155752 -4.2148561 -4.2143078 -4.2116933 -4.2077804 -4.2028995 -4.1942797 -4.1871438 -4.1872072 -4.2039809 -4.2248583 -4.2391734 -4.2503872][-4.1949162 -4.1821604 -4.1773891 -4.1767154 -4.1774874 -4.1737561 -4.1682115 -4.161871 -4.1524105 -4.1497846 -4.155447 -4.1777773 -4.2020578 -4.2210259 -4.2366843][-4.1446381 -4.1296983 -4.1299286 -4.1328425 -4.1380377 -4.1366034 -4.13353 -4.13165 -4.127316 -4.1302152 -4.13927 -4.1589646 -4.1785326 -4.1974669 -4.2177267][-4.0913649 -4.0772295 -4.0818958 -4.0879674 -4.095542 -4.0967479 -4.0985732 -4.1042728 -4.10915 -4.1202474 -4.1325903 -4.1482463 -4.1621766 -4.1817145 -4.203475][-4.0594268 -4.0455866 -4.0520525 -4.0587811 -4.0613308 -4.0567122 -4.0559921 -4.0597548 -4.0719666 -4.0957308 -4.1170111 -4.1380377 -4.1542692 -4.1782465 -4.2019329][-4.0434885 -4.0227027 -4.0203118 -4.0182343 -4.0094757 -3.992708 -3.9787226 -3.9698858 -3.9888029 -4.0353122 -4.0780759 -4.1117253 -4.137588 -4.1708579 -4.2010374][-4.0195541 -3.9879751 -3.9661207 -3.9419484 -3.9167938 -3.8893232 -3.8568296 -3.8291612 -3.8601105 -3.9356911 -4.0019407 -4.0492439 -4.0937819 -4.14399 -4.1879997][-4.0028043 -3.9693105 -3.9343963 -3.8911233 -3.8544414 -3.8274691 -3.7877209 -3.7519031 -3.787694 -3.8753657 -3.9482279 -3.9959762 -4.0520878 -4.1153502 -4.1699243][-4.0401244 -4.0181413 -3.9842043 -3.9396842 -3.9063115 -3.8884387 -3.8582945 -3.8321433 -3.8627493 -3.9291701 -3.9795971 -4.0114264 -4.0607643 -4.1186004 -4.167922][-4.1177878 -4.1103106 -4.0871925 -4.0544777 -4.031198 -4.0201364 -3.9995928 -3.9857266 -4.0057211 -4.0409517 -4.06505 -4.0803165 -4.1160393 -4.1582465 -4.1924987][-4.2006803 -4.2039876 -4.19685 -4.180788 -4.1699271 -4.163343 -4.1494513 -4.1435671 -4.1523247 -4.1654172 -4.1731191 -4.177659 -4.2007194 -4.2246313 -4.2431407][-4.2654605 -4.274682 -4.2780037 -4.272038 -4.2681341 -4.2653093 -4.256609 -4.2535133 -4.2566919 -4.2617912 -4.2637081 -4.2639627 -4.2766733 -4.2869072 -4.2932248][-4.2889214 -4.2970209 -4.3021889 -4.3013773 -4.3017511 -4.3023195 -4.2966208 -4.2923965 -4.2924309 -4.2965417 -4.3009825 -4.3025351 -4.3097172 -4.3154778 -4.3180156][-4.2821198 -4.2883158 -4.292 -4.2937465 -4.2956209 -4.2956581 -4.2906928 -4.2864952 -4.2859311 -4.290164 -4.297493 -4.3024888 -4.3099575 -4.3164992 -4.3205113]]...]
INFO - root - 2017-12-07 13:28:58.971275: step 14110, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 61h:46m:29s remains)
INFO - root - 2017-12-07 13:29:05.629277: step 14120, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.685 sec/batch; 60h:33m:22s remains)
INFO - root - 2017-12-07 13:29:12.379923: step 14130, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 60h:28m:27s remains)
INFO - root - 2017-12-07 13:29:19.154971: step 14140, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 56h:02m:38s remains)
INFO - root - 2017-12-07 13:29:25.987391: step 14150, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 58h:45m:37s remains)
INFO - root - 2017-12-07 13:29:32.769841: step 14160, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 59h:58m:12s remains)
INFO - root - 2017-12-07 13:29:39.568522: step 14170, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 62h:07m:15s remains)
INFO - root - 2017-12-07 13:29:46.302030: step 14180, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 58h:53m:37s remains)
INFO - root - 2017-12-07 13:29:52.966223: step 14190, loss = 2.08, batch loss = 2.02 (14.7 examples/sec; 0.546 sec/batch; 48h:14m:19s remains)
INFO - root - 2017-12-07 13:29:59.684095: step 14200, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 62h:07m:04s remains)
2017-12-07 13:30:00.417613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3382888 -4.3351374 -4.3335395 -4.3328314 -4.3308663 -4.3263726 -4.3189716 -4.3130674 -4.3126831 -4.3155289 -4.3200874 -4.324811 -4.327527 -4.3292933 -4.3303475][-4.3278928 -4.3199849 -4.3148456 -4.311029 -4.3062286 -4.2997351 -4.2897692 -4.2835736 -4.284112 -4.2880354 -4.2956443 -4.3037496 -4.3078403 -4.3100524 -4.3112111][-4.3123193 -4.2984376 -4.2900572 -4.2850618 -4.2783933 -4.2714071 -4.2623181 -4.2559094 -4.2558131 -4.2586551 -4.2673211 -4.2775283 -4.2835011 -4.2877717 -4.2883854][-4.2895865 -4.2699771 -4.2610106 -4.2576289 -4.2517862 -4.2448049 -4.2376995 -4.2317953 -4.228591 -4.2246456 -4.2294393 -4.2397881 -4.2495031 -4.2588367 -4.2621884][-4.2576957 -4.2330561 -4.2261205 -4.2264371 -4.2240868 -4.220386 -4.2147241 -4.2094584 -4.2030816 -4.1923366 -4.190331 -4.1983566 -4.2118921 -4.2256417 -4.2320786][-4.22969 -4.2018762 -4.1963363 -4.1990337 -4.198597 -4.1920576 -4.1830139 -4.1728511 -4.1601257 -4.1427941 -4.1390538 -4.1529341 -4.1740623 -4.1898031 -4.1957211][-4.2145109 -4.1883931 -4.1856561 -4.1878719 -4.1828914 -4.1646056 -4.1410589 -4.1137114 -4.0859118 -4.0602489 -4.0581656 -4.083806 -4.1198416 -4.1439047 -4.1516356][-4.2146459 -4.1922216 -4.1915803 -4.1917119 -4.181674 -4.1541419 -4.1121879 -4.0592532 -4.007515 -3.9680974 -3.9654479 -4.0058131 -4.0582209 -4.0921984 -4.1047053][-4.2198515 -4.2029595 -4.205421 -4.2058983 -4.1951995 -4.1648726 -4.1174579 -4.0487361 -3.97518 -3.9205823 -3.9129922 -3.9601865 -4.0227509 -4.0624657 -4.0776377][-4.2222538 -4.2141147 -4.2216539 -4.2239828 -4.215826 -4.1914129 -4.1530504 -4.09365 -4.0241623 -3.9679065 -3.9509432 -3.9886932 -4.0451989 -4.0782294 -4.089417][-4.2199078 -4.2166395 -4.2263546 -4.2308674 -4.2276764 -4.2132645 -4.18895 -4.1506243 -4.1025105 -4.0595732 -4.0417452 -4.0649443 -4.1058879 -4.1252222 -4.1317372][-4.2122154 -4.2077518 -4.2165661 -4.2252183 -4.2294025 -4.2250204 -4.2146764 -4.1955576 -4.1692057 -4.14273 -4.1318483 -4.1456308 -4.1692843 -4.1764297 -4.1735148][-4.2059951 -4.1993608 -4.2041988 -4.2141404 -4.222302 -4.2276192 -4.2307134 -4.2249904 -4.2120733 -4.1996026 -4.1963162 -4.2051711 -4.2176151 -4.2135754 -4.2009978][-4.2142682 -4.2028446 -4.2005124 -4.2060924 -4.2141976 -4.225172 -4.2339535 -4.2311645 -4.2220936 -4.2155514 -4.2156968 -4.2227597 -4.2309775 -4.2243147 -4.2079268][-4.2314253 -4.2137933 -4.2048607 -4.2065935 -4.2136288 -4.224369 -4.2310529 -4.2227659 -4.2094445 -4.2016363 -4.2020378 -4.2080069 -4.2167282 -4.2139277 -4.2004371]]...]
INFO - root - 2017-12-07 13:30:07.116558: step 14210, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 58h:44m:19s remains)
INFO - root - 2017-12-07 13:30:13.624622: step 14220, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 58h:23m:35s remains)
INFO - root - 2017-12-07 13:30:20.359646: step 14230, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 61h:27m:29s remains)
INFO - root - 2017-12-07 13:30:27.242352: step 14240, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.748 sec/batch; 66h:06m:07s remains)
INFO - root - 2017-12-07 13:30:34.095540: step 14250, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 59h:10m:45s remains)
INFO - root - 2017-12-07 13:30:40.839557: step 14260, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 58h:18m:10s remains)
INFO - root - 2017-12-07 13:30:47.743781: step 14270, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 61h:54m:45s remains)
INFO - root - 2017-12-07 13:30:54.510207: step 14280, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 58h:16m:40s remains)
INFO - root - 2017-12-07 13:31:01.377546: step 14290, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 60h:38m:12s remains)
INFO - root - 2017-12-07 13:31:08.143232: step 14300, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 60h:34m:52s remains)
2017-12-07 13:31:08.856406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2906914 -4.2837443 -4.2781239 -4.2646713 -4.2540574 -4.2463059 -4.2406125 -4.2374992 -4.2360392 -4.2397542 -4.2457056 -4.250742 -4.2571321 -4.2650676 -4.2719173][-4.2669911 -4.253582 -4.2401032 -4.2237234 -4.2141414 -4.202868 -4.19236 -4.1888981 -4.1940389 -4.2041197 -4.2183819 -4.2320018 -4.2455611 -4.2602243 -4.272336][-4.25309 -4.2304583 -4.2084689 -4.1944504 -4.1890631 -4.1715608 -4.1505775 -4.1502771 -4.1661224 -4.182344 -4.2019839 -4.2196512 -4.2371259 -4.2543678 -4.270112][-4.2428665 -4.2152514 -4.1904316 -4.1796446 -4.1727386 -4.1436453 -4.1063662 -4.1103468 -4.1412239 -4.1668239 -4.1895781 -4.2107415 -4.2312465 -4.2488084 -4.2667274][-4.2175584 -4.1879845 -4.1622376 -4.1483288 -4.1273079 -4.0735431 -4.01523 -4.0311794 -4.0850687 -4.1262155 -4.1583428 -4.1891489 -4.2172742 -4.2401795 -4.2621169][-4.1756611 -4.1438332 -4.1207581 -4.1005073 -4.0577545 -3.9728255 -3.8958323 -3.9353857 -4.0230389 -4.0853605 -4.1352448 -4.178452 -4.2087641 -4.2313337 -4.2556262][-4.1364622 -4.1043344 -4.0850167 -4.0550113 -3.995316 -3.8923781 -3.8145435 -3.8782933 -3.9896553 -4.0702481 -4.1370015 -4.18999 -4.2166958 -4.2330775 -4.2558331][-4.1219354 -4.096704 -4.08406 -4.0547886 -4.0016003 -3.9149673 -3.866236 -3.9323163 -4.030951 -4.1020665 -4.1605005 -4.2044015 -4.222446 -4.2332234 -4.2521882][-4.13209 -4.1131272 -4.1124816 -4.1001458 -4.0649781 -4.0075717 -3.9908874 -4.0440226 -4.1121159 -4.1555538 -4.1932583 -4.223557 -4.2314525 -4.2352724 -4.2503662][-4.1512985 -4.1328073 -4.142951 -4.1460004 -4.1271062 -4.0910611 -4.08677 -4.1219544 -4.1643162 -4.1907291 -4.2152824 -4.2368493 -4.2373066 -4.233995 -4.2478843][-4.1850643 -4.1640139 -4.1794262 -4.1891665 -4.1776185 -4.15411 -4.1471052 -4.16127 -4.1886058 -4.2103744 -4.2338767 -4.2520056 -4.2459183 -4.2363777 -4.2471147][-4.2286229 -4.2066245 -4.2205071 -4.2299757 -4.2173271 -4.1981716 -4.1884527 -4.1850395 -4.1990328 -4.2188325 -4.241838 -4.2609754 -4.2577257 -4.2506113 -4.2598348][-4.2722321 -4.2525735 -4.2604971 -4.2680731 -4.2566957 -4.2409644 -4.2269607 -4.2137508 -4.2138028 -4.2278705 -4.2499466 -4.2686753 -4.2720981 -4.271348 -4.2803383][-4.3141265 -4.2992063 -4.2997141 -4.3023105 -4.2940211 -4.2835164 -4.2694182 -4.252017 -4.2444124 -4.2544231 -4.2749243 -4.2905006 -4.2946358 -4.2947903 -4.2993526][-4.3235626 -4.3126607 -4.3090944 -4.3069439 -4.3017378 -4.2979693 -4.2907844 -4.2802458 -4.2759762 -4.2849126 -4.2992458 -4.3097267 -4.3123231 -4.3110232 -4.3123341]]...]
INFO - root - 2017-12-07 13:31:15.599399: step 14310, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 61h:13m:23s remains)
INFO - root - 2017-12-07 13:31:22.215948: step 14320, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 62h:53m:04s remains)
INFO - root - 2017-12-07 13:31:28.956846: step 14330, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 63h:05m:16s remains)
INFO - root - 2017-12-07 13:31:35.714231: step 14340, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 55h:34m:40s remains)
INFO - root - 2017-12-07 13:31:42.453839: step 14350, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 56h:16m:23s remains)
INFO - root - 2017-12-07 13:31:49.302173: step 14360, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 64h:51m:02s remains)
INFO - root - 2017-12-07 13:31:55.996509: step 14370, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 61h:47m:00s remains)
INFO - root - 2017-12-07 13:32:02.862433: step 14380, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 61h:34m:53s remains)
INFO - root - 2017-12-07 13:32:09.600370: step 14390, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 56h:39m:04s remains)
INFO - root - 2017-12-07 13:32:16.416381: step 14400, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 58h:10m:28s remains)
2017-12-07 13:32:17.231046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.309473 -4.2894983 -4.2692657 -4.253221 -4.2376003 -4.2303185 -4.2476606 -4.2669163 -4.2839322 -4.2940884 -4.299696 -4.2998142 -4.29395 -4.2859335 -4.2759042][-4.3120608 -4.2855644 -4.2618847 -4.2387276 -4.2107129 -4.1930075 -4.2069054 -4.2267241 -4.2500591 -4.2690644 -4.28439 -4.291265 -4.2849326 -4.2734294 -4.2598519][-4.3056188 -4.2736921 -4.2469845 -4.2160311 -4.1765203 -4.1501679 -4.1580787 -4.1811795 -4.2176356 -4.2519593 -4.2761478 -4.2834725 -4.2725296 -4.2561622 -4.2349453][-4.2924747 -4.2578626 -4.2313418 -4.1988263 -4.1575146 -4.1280384 -4.1292562 -4.1542335 -4.2050276 -4.249938 -4.2727947 -4.2724066 -4.2578216 -4.2352452 -4.2014794][-4.2872295 -4.2509656 -4.2249303 -4.1955104 -4.156908 -4.1241593 -4.1103954 -4.1300573 -4.193027 -4.2454534 -4.2603574 -4.2482195 -4.2274332 -4.1979566 -4.1580229][-4.2877431 -4.2503419 -4.2222948 -4.1908469 -4.1485643 -4.1003952 -4.0606861 -4.0745335 -4.1549611 -4.2188578 -4.2323451 -4.2136021 -4.1889892 -4.1542377 -4.1101956][-4.2796588 -4.2369995 -4.1982288 -4.1515241 -4.0912676 -4.0107079 -3.9283032 -3.9366913 -4.0453563 -4.1368575 -4.165709 -4.1569281 -4.136548 -4.1033025 -4.061059][-4.2633657 -4.2123342 -4.1629019 -4.1033211 -4.0314164 -3.9264073 -3.8049207 -3.8066132 -3.9363306 -4.050252 -4.0985451 -4.107832 -4.0963693 -4.0682721 -4.033217][-4.2667689 -4.2195635 -4.1751781 -4.126111 -4.071898 -3.9921579 -3.8954227 -3.8978834 -3.9996142 -4.0895495 -4.1312051 -4.1398249 -4.1228261 -4.0868645 -4.0465746][-4.2826128 -4.2455716 -4.2129703 -4.1803703 -4.1461945 -4.0982018 -4.037118 -4.0421214 -4.1086879 -4.1676321 -4.1952548 -4.1952047 -4.17058 -4.1301365 -4.0870686][-4.2887278 -4.2556915 -4.2276106 -4.2024107 -4.1800556 -4.150003 -4.1118946 -4.1198225 -4.164155 -4.2020597 -4.2186165 -4.2143769 -4.1896319 -4.1543283 -4.1174288][-4.2892089 -4.2525129 -4.2212281 -4.1966023 -4.1808157 -4.1636963 -4.1458769 -4.1595268 -4.1936822 -4.2185154 -4.226192 -4.2186174 -4.1958542 -4.1662312 -4.1361609][-4.2887721 -4.2496061 -4.2156377 -4.1919236 -4.1818643 -4.1770239 -4.1769161 -4.1939015 -4.2199841 -4.2364488 -4.2368922 -4.226696 -4.2068377 -4.1836071 -4.1619129][-4.2802539 -4.2404504 -4.2071924 -4.1864996 -4.1816921 -4.1895204 -4.2032108 -4.2208209 -4.2384691 -4.247704 -4.2432652 -4.2325649 -4.2183061 -4.2033358 -4.1892872][-4.2798467 -4.2410655 -4.2083426 -4.1893945 -4.1864982 -4.2006283 -4.2200503 -4.2348452 -4.244585 -4.2480378 -4.2433548 -4.2354021 -4.2268286 -4.21787 -4.2085052]]...]
INFO - root - 2017-12-07 13:32:23.991394: step 14410, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 58h:52m:31s remains)
INFO - root - 2017-12-07 13:32:30.504277: step 14420, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 55h:08m:07s remains)
INFO - root - 2017-12-07 13:32:37.197088: step 14430, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 58h:13m:29s remains)
INFO - root - 2017-12-07 13:32:43.941337: step 14440, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 61h:37m:02s remains)
INFO - root - 2017-12-07 13:32:50.805131: step 14450, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 65h:24m:52s remains)
INFO - root - 2017-12-07 13:32:57.618225: step 14460, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 62h:46m:53s remains)
INFO - root - 2017-12-07 13:33:04.378568: step 14470, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 56h:06m:31s remains)
INFO - root - 2017-12-07 13:33:11.262473: step 14480, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 59h:18m:45s remains)
INFO - root - 2017-12-07 13:33:18.086264: step 14490, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 62h:32m:41s remains)
INFO - root - 2017-12-07 13:33:24.898440: step 14500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 63h:09m:44s remains)
2017-12-07 13:33:25.587016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2709727 -4.2610512 -4.2594357 -4.2582779 -4.2537785 -4.2468691 -4.2442555 -4.2557836 -4.2750173 -4.2912784 -4.2982922 -4.3037167 -4.314311 -4.3209891 -4.3162422][-4.2579789 -4.2532353 -4.2612171 -4.2694178 -4.27401 -4.270669 -4.2638164 -4.2688875 -4.2832747 -4.2948637 -4.302145 -4.3080854 -4.3176365 -4.3211756 -4.3085294][-4.2514248 -4.2462473 -4.2571983 -4.2692132 -4.2783523 -4.2727122 -4.2542896 -4.2442226 -4.2512393 -4.2633324 -4.2754068 -4.2854447 -4.2955952 -4.3003755 -4.2879434][-4.2516913 -4.2410488 -4.2454009 -4.2570415 -4.2662106 -4.2512727 -4.2113724 -4.1771693 -4.1757522 -4.1953015 -4.21826 -4.2377005 -4.2518988 -4.2560415 -4.2443137][-4.2499681 -4.2314444 -4.224627 -4.2313561 -4.2340684 -4.2006712 -4.1319323 -4.0722022 -4.0711884 -4.10809 -4.1481524 -4.1798897 -4.1992044 -4.1999531 -4.187305][-4.217216 -4.1864104 -4.1655135 -4.1637974 -4.1582117 -4.1071172 -4.0142045 -3.9446654 -3.9634247 -4.0294552 -4.0883169 -4.1319404 -4.1564403 -4.1526752 -4.1352654][-4.1401405 -4.0999451 -4.0741158 -4.06897 -4.0592074 -4.0029902 -3.9124174 -3.8635674 -3.9052467 -3.9847856 -4.0500193 -4.0985913 -4.1265745 -4.1207442 -4.0984497][-4.075954 -4.0386286 -4.0227771 -4.0245886 -4.0198393 -3.9761405 -3.9137993 -3.894129 -3.940567 -4.0087085 -4.0602741 -4.0995436 -4.1239967 -4.11517 -4.0900226][-4.0637412 -4.0454178 -4.05095 -4.063066 -4.0641832 -4.037014 -4.0032721 -4.0028872 -4.04054 -4.0868721 -4.1178055 -4.1428561 -4.1581268 -4.1443467 -4.1175141][-4.1240673 -4.1226439 -4.1392417 -4.1539683 -4.1564755 -4.1417351 -4.1285415 -4.1372375 -4.1638966 -4.1923532 -4.2100348 -4.22696 -4.233633 -4.215529 -4.1881771][-4.2163963 -4.2218728 -4.2384267 -4.2506566 -4.25268 -4.247478 -4.24774 -4.2596636 -4.2789836 -4.2973523 -4.3065228 -4.31546 -4.3149204 -4.2945809 -4.2674994][-4.290885 -4.2965622 -4.3078537 -4.3164787 -4.3194513 -4.3209877 -4.3283563 -4.34097 -4.3554006 -4.3663969 -4.3689256 -4.36966 -4.3639579 -4.3463049 -4.3241916][-4.3367672 -4.3401494 -4.3448844 -4.3491559 -4.35175 -4.3555493 -4.3635306 -4.3740954 -4.3844004 -4.3900666 -4.3895288 -4.3863449 -4.3792725 -4.3669653 -4.35308][-4.3539696 -4.356318 -4.3574986 -4.3589506 -4.3602991 -4.362668 -4.36741 -4.3734231 -4.3789039 -4.3819308 -4.3818064 -4.379425 -4.3748231 -4.36825 -4.3615065][-4.3564219 -4.3575535 -4.3573461 -4.3576293 -4.3576655 -4.358099 -4.3593135 -4.361062 -4.363029 -4.3644223 -4.3648777 -4.3645768 -4.3637285 -4.3626218 -4.3608861]]...]
INFO - root - 2017-12-07 13:33:32.446425: step 14510, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 63h:51m:03s remains)
INFO - root - 2017-12-07 13:33:39.083904: step 14520, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 58h:33m:25s remains)
INFO - root - 2017-12-07 13:33:45.963099: step 14530, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 56h:05m:34s remains)
INFO - root - 2017-12-07 13:33:52.787553: step 14540, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 56h:58m:09s remains)
INFO - root - 2017-12-07 13:33:59.609880: step 14550, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 62h:41m:12s remains)
INFO - root - 2017-12-07 13:34:06.452281: step 14560, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 62h:10m:18s remains)
INFO - root - 2017-12-07 13:34:13.323656: step 14570, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 63h:13m:54s remains)
INFO - root - 2017-12-07 13:34:20.103540: step 14580, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 58h:36m:48s remains)
INFO - root - 2017-12-07 13:34:26.896434: step 14590, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 60h:05m:00s remains)
INFO - root - 2017-12-07 13:34:33.834576: step 14600, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 62h:52m:35s remains)
2017-12-07 13:34:34.629624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1827126 -4.2163219 -4.2474737 -4.2551956 -4.2205191 -4.1521964 -4.0829248 -4.0445428 -4.0713696 -4.123538 -4.1581235 -4.1739497 -4.1785903 -4.1714196 -4.1726236][-4.2143769 -4.2545185 -4.2889557 -4.2975698 -4.2646675 -4.2029886 -4.1356087 -4.08144 -4.0795588 -4.107163 -4.1316662 -4.1432223 -4.1471872 -4.1492939 -4.1637106][-4.2473607 -4.2854772 -4.3143649 -4.3194137 -4.2895169 -4.2351241 -4.1738281 -4.1150074 -4.0953546 -4.1027431 -4.1143637 -4.1202226 -4.1249819 -4.1370449 -4.1610236][-4.2694468 -4.2975292 -4.3179307 -4.3171582 -4.282999 -4.2237697 -4.1597342 -4.1085134 -4.0989466 -4.1085486 -4.1130962 -4.1144886 -4.1169386 -4.1355348 -4.1665487][-4.2823472 -4.3006678 -4.3103132 -4.2960176 -4.24409 -4.1656651 -4.0889359 -4.047822 -4.0722656 -4.1117635 -4.127789 -4.1349564 -4.1427965 -4.1610947 -4.1859941][-4.2936392 -4.3035555 -4.303021 -4.2681394 -4.1866903 -4.0770979 -3.9755969 -3.9358568 -3.9998853 -4.086041 -4.1322689 -4.1614213 -4.1872258 -4.2063584 -4.2224894][-4.2989383 -4.3028083 -4.2917795 -4.2367415 -4.1260557 -3.9787083 -3.8421116 -3.7903917 -3.890105 -4.025682 -4.1069856 -4.1649489 -4.2133951 -4.2422237 -4.259356][-4.2959981 -4.2990928 -4.2837725 -4.221643 -4.1002574 -3.9343231 -3.7762332 -3.7029834 -3.8105769 -3.9732492 -4.0817409 -4.1626029 -4.2263384 -4.2620811 -4.2847576][-4.2582369 -4.2746725 -4.2771993 -4.237052 -4.1387467 -3.9988079 -3.8673391 -3.8045382 -3.8815351 -4.0147872 -4.1150861 -4.1959829 -4.2584634 -4.2906609 -4.311583][-4.1934237 -4.2348709 -4.2701344 -4.2673855 -4.2119904 -4.1178741 -4.0294313 -3.9825993 -4.0213752 -4.1046047 -4.1779037 -4.2407207 -4.2892728 -4.3124728 -4.3261676][-4.1276641 -4.1940351 -4.2586203 -4.2898397 -4.2771387 -4.2284441 -4.1762533 -4.1425624 -4.1530185 -4.1965733 -4.2448545 -4.2858148 -4.3161769 -4.3288827 -4.3352818][-4.0892878 -4.1789737 -4.2635813 -4.3121662 -4.3262672 -4.3112841 -4.2852035 -4.2598786 -4.2554708 -4.2732463 -4.3014793 -4.3258386 -4.3418779 -4.3470588 -4.3489037][-4.0962081 -4.1948957 -4.2804008 -4.3283315 -4.353447 -4.3573689 -4.3469496 -4.3295784 -4.3200407 -4.3220711 -4.3323154 -4.34354 -4.3509321 -4.3520603 -4.3528996][-4.1435666 -4.2297721 -4.2972155 -4.3334932 -4.3593073 -4.3701873 -4.3667345 -4.3552594 -4.3459206 -4.343287 -4.3450341 -4.3483777 -4.3527079 -4.3536444 -4.3531494][-4.2062521 -4.2673168 -4.3082938 -4.3287325 -4.34674 -4.3554072 -4.3529105 -4.3450685 -4.3375397 -4.3355141 -4.335959 -4.3378878 -4.3414087 -4.3422141 -4.3407607]]...]
INFO - root - 2017-12-07 13:34:41.365977: step 14610, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 58h:16m:15s remains)
INFO - root - 2017-12-07 13:34:48.074274: step 14620, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 65h:04m:56s remains)
INFO - root - 2017-12-07 13:34:54.940734: step 14630, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 65h:06m:01s remains)
INFO - root - 2017-12-07 13:35:01.736846: step 14640, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 58h:04m:07s remains)
INFO - root - 2017-12-07 13:35:08.553608: step 14650, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 58h:19m:52s remains)
INFO - root - 2017-12-07 13:35:15.379981: step 14660, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 60h:31m:42s remains)
INFO - root - 2017-12-07 13:35:22.340851: step 14670, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 64h:22m:05s remains)
INFO - root - 2017-12-07 13:35:29.210149: step 14680, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 62h:15m:47s remains)
INFO - root - 2017-12-07 13:35:36.099500: step 14690, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 57h:36m:16s remains)
INFO - root - 2017-12-07 13:35:42.952055: step 14700, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 55h:41m:06s remains)
2017-12-07 13:35:43.672454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2704091 -4.2672987 -4.274477 -4.2770648 -4.2731171 -4.2738023 -4.2758923 -4.2714105 -4.2676497 -4.2699318 -4.2760253 -4.2837896 -4.291635 -4.2982163 -4.3029389][-4.2925215 -4.2929893 -4.3034139 -4.3088813 -4.3064809 -4.3051081 -4.3066583 -4.3022618 -4.2966094 -4.2949014 -4.3000255 -4.3077149 -4.3151436 -4.3199711 -4.3221412][-4.2813993 -4.2814097 -4.2915583 -4.2969227 -4.2946377 -4.2921386 -4.292769 -4.2883873 -4.28265 -4.2805204 -4.2850804 -4.2927065 -4.2983871 -4.3006864 -4.3013296][-4.2351036 -4.2319584 -4.241 -4.24873 -4.2486005 -4.2463946 -4.2437992 -4.2366681 -4.2281532 -4.2228374 -4.2234573 -4.2271771 -4.2297263 -4.2310152 -4.2327285][-4.1655917 -4.1535954 -4.1576071 -4.1662116 -4.1708503 -4.1714554 -4.1676278 -4.1586914 -4.1477833 -4.1368761 -4.1303406 -4.1268573 -4.1242867 -4.1215711 -4.1210938][-4.0785365 -4.0482659 -4.0384011 -4.0422554 -4.0498776 -4.0547366 -4.0528383 -4.0459161 -4.0353603 -4.0225482 -4.0106525 -3.998312 -3.9879854 -3.980422 -3.9783089][-4.0266929 -3.9796743 -3.9547031 -3.9494772 -3.95056 -3.9488192 -3.9430077 -3.9378948 -3.9356358 -3.9323728 -3.9249189 -3.9137988 -3.90517 -3.8993516 -3.8974001][-4.0676818 -4.021265 -3.990555 -3.9783382 -3.9684274 -3.9533381 -3.939255 -3.9376445 -3.9504919 -3.9663327 -3.9738526 -3.9726849 -3.9714191 -3.9696326 -3.9682467][-4.1466289 -4.1102934 -4.0821486 -4.0678034 -4.0511055 -4.0269852 -4.0048461 -4.0043974 -4.0281472 -4.0588508 -4.082376 -4.0945568 -4.1018786 -4.10189 -4.0984693][-4.1855059 -4.15617 -4.1331787 -4.1240911 -4.1114278 -4.0905519 -4.0699368 -4.068923 -4.0922561 -4.1229982 -4.1500025 -4.1659222 -4.1751442 -4.1723452 -4.1653996][-4.1898155 -4.1619239 -4.1428103 -4.1393471 -4.135591 -4.1258888 -4.1135893 -4.1111054 -4.125051 -4.1461377 -4.1669526 -4.17965 -4.1869955 -4.1845303 -4.1801925][-4.1896095 -4.1614566 -4.1425233 -4.1398516 -4.1438918 -4.1454968 -4.1417222 -4.13535 -4.1354694 -4.1433406 -4.1549063 -4.164916 -4.1729774 -4.1724806 -4.171102][-4.2024021 -4.1726413 -4.1503887 -4.1449919 -4.1535444 -4.1626878 -4.16436 -4.1552329 -4.1438947 -4.1394258 -4.1418071 -4.1479044 -4.1562481 -4.1588922 -4.1600151][-4.2247877 -4.1937079 -4.1699419 -4.1624765 -4.1684313 -4.1776557 -4.1787658 -4.1677361 -4.1508775 -4.1381221 -4.1319475 -4.1314096 -4.1376538 -4.1415219 -4.1456981][-4.2441249 -4.2128968 -4.1911774 -4.1843333 -4.18551 -4.1914163 -4.1922655 -4.1823449 -4.1668568 -4.1520033 -4.1414886 -4.1367588 -4.139955 -4.144187 -4.150517]]...]
INFO - root - 2017-12-07 13:35:50.494168: step 14710, loss = 2.11, batch loss = 2.05 (11.6 examples/sec; 0.689 sec/batch; 60h:47m:24s remains)
INFO - root - 2017-12-07 13:35:56.973110: step 14720, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.623 sec/batch; 54h:58m:55s remains)
INFO - root - 2017-12-07 13:36:03.862063: step 14730, loss = 2.10, batch loss = 2.05 (11.8 examples/sec; 0.680 sec/batch; 59h:59m:16s remains)
INFO - root - 2017-12-07 13:36:10.754799: step 14740, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 61h:24m:57s remains)
INFO - root - 2017-12-07 13:36:17.615908: step 14750, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 60h:49m:34s remains)
INFO - root - 2017-12-07 13:36:24.415001: step 14760, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 60h:36m:59s remains)
INFO - root - 2017-12-07 13:36:31.224147: step 14770, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 56h:51m:53s remains)
INFO - root - 2017-12-07 13:36:38.010807: step 14780, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 62h:24m:44s remains)
INFO - root - 2017-12-07 13:36:44.880159: step 14790, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.736 sec/batch; 64h:59m:49s remains)
INFO - root - 2017-12-07 13:36:51.709757: step 14800, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 65h:10m:55s remains)
2017-12-07 13:36:52.412213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0443368 -4.1186714 -4.1977878 -4.2461691 -4.2690187 -4.2787657 -4.2849207 -4.2871289 -4.2865548 -4.28846 -4.2898889 -4.2943697 -4.2996325 -4.3020115 -4.3034725][-4.0580096 -4.1300292 -4.2033272 -4.2451057 -4.2617712 -4.2699628 -4.2793827 -4.2872396 -4.294754 -4.3054862 -4.3150878 -4.3249664 -4.3316388 -4.3342533 -4.3336296][-4.0945644 -4.1598172 -4.2195196 -4.250083 -4.2570848 -4.2589688 -4.2670789 -4.2782121 -4.290657 -4.3051529 -4.317986 -4.3301196 -4.3382497 -4.3413248 -4.3379211][-4.13167 -4.1857119 -4.2251949 -4.2368088 -4.2274156 -4.2163005 -4.2194605 -4.2317052 -4.2437534 -4.2605443 -4.2791615 -4.299758 -4.3176656 -4.328248 -4.3276286][-4.1448851 -4.1836705 -4.1983252 -4.1837974 -4.1548452 -4.1285014 -4.11967 -4.123992 -4.1327224 -4.159471 -4.197422 -4.2407784 -4.2780404 -4.3023691 -4.3117876][-4.1264219 -4.1527209 -4.1468258 -4.1114035 -4.064189 -4.02094 -3.9870358 -3.9630702 -3.9562752 -4.0029206 -4.0761766 -4.1538348 -4.2165647 -4.260922 -4.2897167][-4.1080017 -4.1300225 -4.1147914 -4.0695248 -4.0119672 -3.9526222 -3.8815172 -3.8036022 -3.7610364 -3.8310084 -3.9442019 -4.054018 -4.1390276 -4.2018371 -4.2545872][-4.1240506 -4.1465063 -4.1333036 -4.0940623 -4.0363293 -3.9706848 -3.8774798 -3.7640615 -3.69131 -3.7643542 -3.8823886 -3.989471 -4.0713339 -4.1395168 -4.2091584][-4.1635365 -4.1871815 -4.1849136 -4.1617575 -4.1189628 -4.06174 -3.9764018 -3.8838863 -3.8266993 -3.8654389 -3.9299595 -3.9889479 -4.0414205 -4.0993843 -4.1733189][-4.2152405 -4.2342343 -4.2337379 -4.2184324 -4.1855383 -4.13678 -4.072464 -4.0156169 -3.9865794 -3.9982696 -4.0130558 -4.022872 -4.0363488 -4.0729594 -4.1376543][-4.262435 -4.2774062 -4.2774959 -4.2604957 -4.2258215 -4.1823678 -4.1381874 -4.1109233 -4.105988 -4.1079183 -4.1001649 -4.0863419 -4.0724764 -4.0799484 -4.1167626][-4.2965617 -4.3089342 -4.3123064 -4.2982664 -4.2665071 -4.2286453 -4.2000842 -4.1878438 -4.1936464 -4.1957335 -4.1872725 -4.1701293 -4.1456604 -4.1305637 -4.1325636][-4.3145809 -4.3251858 -4.3297539 -4.322072 -4.3004761 -4.2713933 -4.253603 -4.2472692 -4.2569222 -4.2646165 -4.264782 -4.2556672 -4.233665 -4.2092118 -4.1872511][-4.3172679 -4.3287191 -4.3354435 -4.3341112 -4.3215318 -4.2995315 -4.2828059 -4.27486 -4.2825923 -4.2947783 -4.3051496 -4.3098617 -4.3002553 -4.2798128 -4.25199][-4.3088121 -4.3234758 -4.3344951 -4.3384314 -4.3305659 -4.3114023 -4.2896833 -4.2742662 -4.276536 -4.2888188 -4.3042269 -4.3193784 -4.3246274 -4.3193727 -4.3006382]]...]
INFO - root - 2017-12-07 13:36:59.063852: step 14810, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 59h:07m:39s remains)
INFO - root - 2017-12-07 13:37:05.710620: step 14820, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 59h:43m:26s remains)
INFO - root - 2017-12-07 13:37:12.659036: step 14830, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 56h:02m:56s remains)
INFO - root - 2017-12-07 13:37:19.638979: step 14840, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 65h:09m:47s remains)
INFO - root - 2017-12-07 13:37:26.526972: step 14850, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 62h:18m:06s remains)
INFO - root - 2017-12-07 13:37:33.364951: step 14860, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 60h:15m:32s remains)
INFO - root - 2017-12-07 13:37:40.033992: step 14870, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 55h:31m:49s remains)
INFO - root - 2017-12-07 13:37:46.891680: step 14880, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 57h:43m:51s remains)
INFO - root - 2017-12-07 13:37:53.878805: step 14890, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 64h:48m:40s remains)
INFO - root - 2017-12-07 13:38:00.658210: step 14900, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 63h:06m:26s remains)
2017-12-07 13:38:01.372358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3718758 -4.3731422 -4.3735814 -4.3746738 -4.3753581 -4.375556 -4.3758636 -4.3765707 -4.3769355 -4.3765135 -4.3761277 -4.3763165 -4.37693 -4.3778267 -4.3789067][-4.3588128 -4.3596916 -4.3609066 -4.3624344 -4.3635645 -4.36385 -4.364037 -4.3657913 -4.3674068 -4.3672843 -4.367743 -4.3691335 -4.3703833 -4.370717 -4.3715382][-4.3338108 -4.3352766 -4.3371782 -4.3388462 -4.3412085 -4.3420224 -4.3421936 -4.3453536 -4.3488331 -4.3504367 -4.3533154 -4.3565531 -4.3589807 -4.3590336 -4.358263][-4.3058524 -4.309052 -4.3105178 -4.3098888 -4.3121157 -4.3141432 -4.3148904 -4.3193636 -4.323843 -4.3289852 -4.3370914 -4.3445892 -4.3493133 -4.3476353 -4.3435454][-4.2764511 -4.2807922 -4.28082 -4.2792172 -4.2814884 -4.2846689 -4.2868638 -4.2921233 -4.2965655 -4.3059759 -4.3184233 -4.3299561 -4.3343291 -4.3258924 -4.3161478][-4.2321305 -4.2379041 -4.2366953 -4.2352648 -4.2392831 -4.2439842 -4.2470689 -4.2528462 -4.2545333 -4.2651734 -4.2834954 -4.30137 -4.3036556 -4.284512 -4.2666235][-4.1881819 -4.1963735 -4.1936469 -4.1931715 -4.2011805 -4.2070646 -4.206996 -4.209527 -4.2080555 -4.2221332 -4.2494869 -4.273324 -4.2713656 -4.2419252 -4.2179079][-4.1521521 -4.1594877 -4.1554718 -4.1586418 -4.1732669 -4.1833668 -4.1810679 -4.177495 -4.1724477 -4.1885352 -4.2188544 -4.2429118 -4.2354388 -4.2015128 -4.1760015][-4.144556 -4.146091 -4.1367221 -4.1402946 -4.1630058 -4.1778884 -4.1753125 -4.1678967 -4.1609278 -4.1756639 -4.2024274 -4.2226815 -4.2121811 -4.17945 -4.1562705][-4.1552448 -4.1481977 -4.1321282 -4.1343074 -4.163712 -4.1837788 -4.1843758 -4.1751842 -4.1663384 -4.1793976 -4.1994209 -4.2120051 -4.2001281 -4.1719379 -4.1543808][-4.18848 -4.1738486 -4.1490893 -4.1443033 -4.175631 -4.2031827 -4.2093649 -4.201951 -4.1920762 -4.1994667 -4.2116642 -4.2153659 -4.2005424 -4.1782269 -4.1666441][-4.2335739 -4.2099309 -4.1743298 -4.1580181 -4.18772 -4.2235217 -4.2382894 -4.2338991 -4.2246451 -4.2270613 -4.2304726 -4.2263293 -4.2103 -4.193727 -4.1891222][-4.254745 -4.2242951 -4.1793184 -4.1520462 -4.1799269 -4.2239308 -4.2477922 -4.2491393 -4.2464752 -4.2481122 -4.2449884 -4.2347403 -4.2207618 -4.2138362 -4.2180996][-4.2602496 -4.2311306 -4.1842456 -4.1514292 -4.1779046 -4.226656 -4.256566 -4.2639637 -4.2673974 -4.2696638 -4.2626853 -4.2500796 -4.2417707 -4.2462935 -4.2588716][-4.2739754 -4.2524047 -4.2118917 -4.180254 -4.20224 -4.2495 -4.2827468 -4.2940822 -4.2989054 -4.2989655 -4.2876148 -4.2728424 -4.2685318 -4.2798433 -4.2955532]]...]
INFO - root - 2017-12-07 13:38:08.193167: step 14910, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 58h:46m:43s remains)
INFO - root - 2017-12-07 13:38:14.863991: step 14920, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.721 sec/batch; 63h:36m:12s remains)
INFO - root - 2017-12-07 13:38:21.735730: step 14930, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 59h:44m:11s remains)
INFO - root - 2017-12-07 13:38:28.516338: step 14940, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 55h:28m:46s remains)
INFO - root - 2017-12-07 13:38:35.311635: step 14950, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 56h:02m:26s remains)
INFO - root - 2017-12-07 13:38:42.151462: step 14960, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 62h:30m:43s remains)
INFO - root - 2017-12-07 13:38:49.023803: step 14970, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 61h:14m:14s remains)
INFO - root - 2017-12-07 13:38:55.861562: step 14980, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 59h:45m:57s remains)
INFO - root - 2017-12-07 13:39:02.624852: step 14990, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 60h:17m:28s remains)
INFO - root - 2017-12-07 13:39:09.572474: step 15000, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 60h:33m:47s remains)
2017-12-07 13:39:10.294801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3306589 -4.3325505 -4.3302298 -4.3246231 -4.3134127 -4.297544 -4.2790861 -4.2618265 -4.2463217 -4.2374129 -4.2360663 -4.2466831 -4.2674389 -4.2904925 -4.3121462][-4.3312674 -4.3337884 -4.3332825 -4.3271642 -4.3111486 -4.287868 -4.2618985 -4.2367363 -4.2150679 -4.2060909 -4.2078757 -4.219451 -4.239274 -4.2633114 -4.2921815][-4.3324471 -4.3353667 -4.3365474 -4.3310461 -4.3071809 -4.2733393 -4.2425938 -4.2163453 -4.1960177 -4.1923938 -4.2034521 -4.2154231 -4.2282495 -4.2465978 -4.2743611][-4.3361425 -4.3386464 -4.3411679 -4.3320861 -4.2957897 -4.2477684 -4.209734 -4.18919 -4.1827941 -4.1894107 -4.2074089 -4.2217159 -4.2288423 -4.2395196 -4.2617979][-4.33272 -4.3328271 -4.3320503 -4.313767 -4.2624631 -4.1957021 -4.142705 -4.1299362 -4.1508851 -4.1730995 -4.1945381 -4.2104888 -4.217886 -4.2262087 -4.244112][-4.3149166 -4.30528 -4.2877717 -4.2523313 -4.185914 -4.0978189 -4.020184 -4.0128641 -4.0721169 -4.1239734 -4.1551504 -4.1733084 -4.1873507 -4.2034836 -4.222959][-4.2730513 -4.2411079 -4.1968083 -4.1390958 -4.0534596 -3.9414749 -3.8335218 -3.8281288 -3.933063 -4.0324287 -4.084619 -4.1118231 -4.139143 -4.1667662 -4.1933212][-4.2089047 -4.1470094 -4.0751657 -3.9941726 -3.8933167 -3.7735794 -3.6570022 -3.6595116 -3.8034875 -3.9372275 -4.0063615 -4.0472665 -4.0883169 -4.1252584 -4.1618066][-4.1570358 -4.0764804 -3.9950628 -3.91077 -3.8263812 -3.7441678 -3.6693881 -3.6834211 -3.8068166 -3.922827 -3.9878471 -4.0341463 -4.0823178 -4.1226263 -4.1648474][-4.1561165 -4.0787916 -4.0081172 -3.9473634 -3.9033794 -3.8780465 -3.8569245 -3.8651078 -3.9276366 -3.9983764 -4.0464811 -4.0889468 -4.1376004 -4.1738315 -4.2101393][-4.2016153 -4.1432953 -4.0956059 -4.0668468 -4.0601521 -4.0693727 -4.071876 -4.070044 -4.0890417 -4.1222582 -4.1529756 -4.1879063 -4.2262797 -4.2502604 -4.2740531][-4.2565885 -4.2186489 -4.1932945 -4.1890779 -4.2015562 -4.2180376 -4.2226477 -4.2173538 -4.2181258 -4.2305713 -4.2518396 -4.2791524 -4.3035355 -4.3170691 -4.3293533][-4.2949939 -4.2739391 -4.2634978 -4.2691894 -4.2844124 -4.296505 -4.2998018 -4.2966127 -4.2935677 -4.29546 -4.30794 -4.3271747 -4.3428006 -4.351881 -4.3570123][-4.3185945 -4.3089848 -4.3059654 -4.3117337 -4.3217564 -4.328135 -4.3304405 -4.3289447 -4.3252707 -4.3229308 -4.328156 -4.33928 -4.351182 -4.3597407 -4.361587][-4.3327541 -4.3283029 -4.3265405 -4.3290987 -4.334177 -4.3377619 -4.3385863 -4.3368187 -4.3340292 -4.3319459 -4.3339229 -4.3398867 -4.3479972 -4.354033 -4.3547082]]...]
INFO - root - 2017-12-07 13:39:17.124235: step 15010, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 59h:51m:01s remains)
INFO - root - 2017-12-07 13:39:23.807028: step 15020, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 58h:53m:36s remains)
INFO - root - 2017-12-07 13:39:30.612950: step 15030, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 58h:50m:51s remains)
INFO - root - 2017-12-07 13:39:37.467936: step 15040, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 62h:35m:38s remains)
INFO - root - 2017-12-07 13:39:44.262909: step 15050, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.670 sec/batch; 59h:06m:15s remains)
INFO - root - 2017-12-07 13:39:51.128938: step 15060, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 58h:09m:13s remains)
INFO - root - 2017-12-07 13:39:58.043248: step 15070, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.649 sec/batch; 57h:11m:33s remains)
INFO - root - 2017-12-07 13:40:04.693871: step 15080, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 60h:17m:57s remains)
INFO - root - 2017-12-07 13:40:11.534937: step 15090, loss = 2.12, batch loss = 2.06 (11.5 examples/sec; 0.694 sec/batch; 61h:11m:39s remains)
INFO - root - 2017-12-07 13:40:18.443865: step 15100, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 58h:59m:55s remains)
2017-12-07 13:40:19.137677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3373618 -4.3390603 -4.3398938 -4.3417864 -4.3444519 -4.3441038 -4.3409357 -4.3371224 -4.3349648 -4.3347478 -4.3403506 -4.348176 -4.3550515 -4.3632364 -4.3723779][-4.3068347 -4.3058791 -4.3045907 -4.3039231 -4.3061047 -4.3065987 -4.3041654 -4.300581 -4.2977171 -4.2962141 -4.3013453 -4.3109732 -4.3217072 -4.3367438 -4.3544297][-4.2604 -4.2543879 -4.248065 -4.2400136 -4.2370729 -4.2364631 -4.236629 -4.2375879 -4.2374687 -4.2370181 -4.2441673 -4.2568078 -4.2730093 -4.2976317 -4.3268442][-4.2037454 -4.1928267 -4.1814771 -4.1653242 -4.1535382 -4.1469493 -4.1492786 -4.1592984 -4.1675978 -4.1731195 -4.1854448 -4.2008095 -4.2215714 -4.2537694 -4.2928753][-4.1482692 -4.1356578 -4.121419 -4.0998716 -4.0801897 -4.0682421 -4.0732079 -4.0922737 -4.1062784 -4.1137047 -4.1271167 -4.1417646 -4.16531 -4.2028642 -4.2514348][-4.1125069 -4.0996242 -4.0854521 -4.0666175 -4.0466909 -4.0312085 -4.0380588 -4.06226 -4.0769453 -4.0844965 -4.0974216 -4.1090555 -4.1297808 -4.1676364 -4.2211342][-4.1179461 -4.1075015 -4.0995178 -4.0871463 -4.0704141 -4.0537248 -4.056818 -4.0775366 -4.0924473 -4.1006846 -4.1120539 -4.1189227 -4.132031 -4.1629691 -4.2117581][-4.1413412 -4.1298351 -4.1249442 -4.1191144 -4.1095271 -4.09573 -4.0894637 -4.1026068 -4.1173754 -4.1293979 -4.1397572 -4.1440916 -4.1529269 -4.1768961 -4.2157731][-4.1674132 -4.1542268 -4.1535974 -4.1582742 -4.1562638 -4.1414495 -4.1219196 -4.121799 -4.129354 -4.1441512 -4.1571531 -4.1657085 -4.17595 -4.1962228 -4.2272444][-4.1882968 -4.1738853 -4.1735125 -4.1836987 -4.1884871 -4.1757684 -4.1525092 -4.1421342 -4.1407461 -4.1557245 -4.1757846 -4.1903048 -4.2014689 -4.2178326 -4.2424331][-4.208744 -4.1922092 -4.1857166 -4.1938429 -4.2003541 -4.19133 -4.176753 -4.1715603 -4.17575 -4.1942577 -4.2141809 -4.2255673 -4.2307372 -4.2389188 -4.2566423][-4.2390342 -4.2249823 -4.2137957 -4.2166061 -4.2203507 -4.21163 -4.2047834 -4.2074847 -4.2184477 -4.238204 -4.2557278 -4.2613482 -4.2591505 -4.2608023 -4.2729788][-4.2721186 -4.2647123 -4.2547531 -4.2522817 -4.2526755 -4.2444711 -4.2407613 -4.2457294 -4.25767 -4.2762494 -4.2910709 -4.2943387 -4.2887611 -4.2869816 -4.2957859][-4.3138623 -4.3123736 -4.304915 -4.2994385 -4.296339 -4.2884917 -4.2856455 -4.2911129 -4.3029556 -4.3189259 -4.3306127 -4.3309569 -4.323081 -4.3188696 -4.3240924][-4.35732 -4.3598437 -4.3535271 -4.3454332 -4.3391395 -4.3314319 -4.3284984 -4.3329625 -4.3433824 -4.357017 -4.3653092 -4.3631396 -4.3548217 -4.3498917 -4.3513989]]...]
INFO - root - 2017-12-07 13:40:25.874748: step 15110, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 60h:28m:19s remains)
INFO - root - 2017-12-07 13:40:32.284170: step 15120, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.637 sec/batch; 56h:11m:50s remains)
INFO - root - 2017-12-07 13:40:39.085006: step 15130, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 57h:37m:01s remains)
INFO - root - 2017-12-07 13:40:45.874134: step 15140, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 61h:19m:39s remains)
INFO - root - 2017-12-07 13:40:52.714286: step 15150, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 62h:46m:53s remains)
INFO - root - 2017-12-07 13:40:59.507243: step 15160, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 60h:12m:51s remains)
INFO - root - 2017-12-07 13:41:06.296786: step 15170, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 58h:35m:32s remains)
INFO - root - 2017-12-07 13:41:13.060346: step 15180, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 55h:26m:13s remains)
INFO - root - 2017-12-07 13:41:19.820484: step 15190, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 60h:31m:26s remains)
INFO - root - 2017-12-07 13:41:26.647359: step 15200, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 62h:56m:11s remains)
2017-12-07 13:41:27.346875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2949171 -4.2718468 -4.2506952 -4.2445703 -4.245779 -4.2601147 -4.270093 -4.2600203 -4.2509451 -4.2606215 -4.2757721 -4.2875986 -4.2951875 -4.2978578 -4.300096][-4.2923622 -4.2740622 -4.2552042 -4.2465563 -4.2417984 -4.2517195 -4.2628016 -4.2557774 -4.2474418 -4.2584562 -4.2747583 -4.2851782 -4.2896276 -4.2927575 -4.297596][-4.2778091 -4.2690344 -4.2548327 -4.242815 -4.2304807 -4.2322659 -4.2391081 -4.2319827 -4.2223334 -4.2307777 -4.2466469 -4.2583013 -4.2649446 -4.2764916 -4.2891574][-4.2568674 -4.2579231 -4.2490144 -4.2342243 -4.2128949 -4.2023597 -4.2004871 -4.1895804 -4.1773725 -4.1817765 -4.1991363 -4.21585 -4.2323971 -4.2557878 -4.2797279][-4.2282043 -4.2357802 -4.23004 -4.2134666 -4.191186 -4.17359 -4.1611547 -4.1402907 -4.11941 -4.1153984 -4.1338792 -4.1678214 -4.2053089 -4.2436867 -4.2763591][-4.1967797 -4.2087793 -4.206512 -4.1940269 -4.17394 -4.1519904 -4.1275058 -4.0949087 -4.0652432 -4.0588021 -4.087234 -4.1454592 -4.2025666 -4.2490549 -4.2820673][-4.167522 -4.1832561 -4.1882648 -4.1847506 -4.169539 -4.144516 -4.1059027 -4.0610662 -4.0261393 -4.0272155 -4.0749731 -4.1525445 -4.2187543 -4.2661 -4.2936382][-4.1405869 -4.1568923 -4.1677809 -4.1703439 -4.1601048 -4.1321087 -4.0866914 -4.0353308 -4.0040059 -4.0211306 -4.0885162 -4.1737943 -4.240221 -4.2827535 -4.3046][-4.1211281 -4.1372719 -4.14898 -4.1504703 -4.1342211 -4.1006784 -4.0544062 -4.0071826 -3.9948037 -4.0375104 -4.1158795 -4.1961961 -4.2546005 -4.2913904 -4.3096871][-4.1212916 -4.1351304 -4.1422076 -4.134726 -4.10843 -4.0764632 -4.039341 -4.016871 -4.0340533 -4.0908885 -4.1612549 -4.2207918 -4.2640066 -4.293951 -4.3091245][-4.1508851 -4.1604042 -4.1594791 -4.14154 -4.1119027 -4.0868421 -4.0694942 -4.0752497 -4.1089487 -4.1635571 -4.2153006 -4.2513 -4.280488 -4.3014946 -4.3117085][-4.1957378 -4.1966434 -4.1858907 -4.1615391 -4.1366978 -4.1233425 -4.125186 -4.1446238 -4.1787343 -4.2260828 -4.2595248 -4.2803745 -4.3015895 -4.316967 -4.321548][-4.2503328 -4.2453513 -4.2283072 -4.2064013 -4.1908216 -4.1888671 -4.1988106 -4.2154016 -4.2431 -4.2785978 -4.2996049 -4.311142 -4.3263392 -4.3358383 -4.3334832][-4.29518 -4.2856054 -4.2679729 -4.2503319 -4.2444935 -4.2502179 -4.2606692 -4.2730713 -4.2933364 -4.3186212 -4.3332715 -4.3392544 -4.3473425 -4.3495016 -4.3419976][-4.3227768 -4.3110886 -4.2932539 -4.2783179 -4.2763581 -4.2829013 -4.2920332 -4.3054767 -4.3238316 -4.3450007 -4.3565736 -4.360003 -4.3614869 -4.3566246 -4.3451123]]...]
INFO - root - 2017-12-07 13:41:34.078161: step 15210, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 56h:32m:57s remains)
INFO - root - 2017-12-07 13:41:40.814851: step 15220, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 62h:15m:54s remains)
INFO - root - 2017-12-07 13:41:47.755558: step 15230, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 62h:52m:29s remains)
INFO - root - 2017-12-07 13:41:54.621327: step 15240, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 60h:11m:14s remains)
INFO - root - 2017-12-07 13:42:01.528014: step 15250, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 57h:17m:36s remains)
INFO - root - 2017-12-07 13:42:08.375010: step 15260, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 57h:38m:11s remains)
INFO - root - 2017-12-07 13:42:15.129814: step 15270, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 61h:30m:34s remains)
INFO - root - 2017-12-07 13:42:21.902251: step 15280, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 61h:06m:12s remains)
INFO - root - 2017-12-07 13:42:28.826012: step 15290, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 59h:29m:06s remains)
INFO - root - 2017-12-07 13:42:35.624831: step 15300, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 58h:36m:31s remains)
2017-12-07 13:42:36.340074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3308563 -4.3305807 -4.327704 -4.3245177 -4.3206968 -4.3180594 -4.3187184 -4.3206644 -4.3216791 -4.3229804 -4.3239527 -4.325624 -4.3288555 -4.3323116 -4.3364949][-4.3203831 -4.3186426 -4.314806 -4.3103132 -4.3057222 -4.3032432 -4.3051758 -4.3087411 -4.3109164 -4.3124304 -4.3114047 -4.31034 -4.3127346 -4.3159947 -4.3208489][-4.2988644 -4.2933064 -4.2874875 -4.2821794 -4.2777572 -4.2745266 -4.27641 -4.2826643 -4.289865 -4.2964921 -4.2969337 -4.2943296 -4.2936578 -4.2953143 -4.2994218][-4.2704372 -4.2555976 -4.2421103 -4.2374964 -4.2343407 -4.2300081 -4.2294579 -4.238801 -4.2565641 -4.2746859 -4.28098 -4.2780733 -4.273284 -4.2722931 -4.2743778][-4.23261 -4.2051749 -4.1824727 -4.1803236 -4.1806049 -4.1735258 -4.1653757 -4.1731434 -4.2047305 -4.2374382 -4.2512536 -4.24816 -4.242034 -4.2388759 -4.2407856][-4.1937251 -4.1529155 -4.1223893 -4.1215377 -4.1201553 -4.1013446 -4.0750937 -4.0723858 -4.1194539 -4.1721807 -4.1958113 -4.1971498 -4.1951022 -4.194376 -4.200768][-4.154109 -4.1013432 -4.0620866 -4.0574369 -4.0493031 -4.0103121 -3.9484627 -3.9220166 -3.9897902 -4.0730038 -4.1125522 -4.1284575 -4.1412306 -4.1512136 -4.1661515][-4.1237173 -4.0582695 -4.0052352 -3.9953589 -3.9764583 -3.9108019 -3.8005428 -3.7344158 -3.8264065 -3.9495177 -4.0135183 -4.0538254 -4.0892024 -4.11264 -4.1362314][-4.1207356 -4.0461988 -3.9803448 -3.9672177 -3.9496975 -3.8741293 -3.7399402 -3.6462617 -3.7402611 -3.8740182 -3.944845 -3.9967084 -4.046463 -4.0787411 -4.1095433][-4.1365471 -4.0619655 -4.0022669 -4.0018878 -4.0039115 -3.9496007 -3.8424783 -3.7621679 -3.8200345 -3.9101191 -3.9496136 -3.9840808 -4.029325 -4.0596666 -4.0896978][-4.167707 -4.1017942 -4.0573854 -4.0710626 -4.0904045 -4.0612068 -3.9911251 -3.9362943 -3.9657323 -4.010231 -4.0193868 -4.0300465 -4.056191 -4.0746088 -4.0958281][-4.2283578 -4.1777544 -4.1495061 -4.167799 -4.1909738 -4.1767974 -4.1324997 -4.09874 -4.1159019 -4.1371827 -4.1312337 -4.1272259 -4.1363192 -4.1406403 -4.1476955][-4.2937117 -4.2634139 -4.2483582 -4.2626963 -4.2823076 -4.2794318 -4.2518721 -4.2296319 -4.2379827 -4.2497826 -4.2427449 -4.2347994 -4.2343516 -4.2302513 -4.2268319][-4.3307681 -4.3177619 -4.3121219 -4.32139 -4.334465 -4.3370895 -4.32342 -4.3087773 -4.3109236 -4.3173146 -4.3126321 -4.3071609 -4.3046188 -4.2974162 -4.2897754][-4.3473577 -4.3429389 -4.340836 -4.3455844 -4.3531132 -4.3572 -4.3528094 -4.345645 -4.3456054 -4.3489132 -4.3460965 -4.3429265 -4.3397064 -4.3321247 -4.3238869]]...]
INFO - root - 2017-12-07 13:42:43.044346: step 15310, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 62h:25m:25s remains)
INFO - root - 2017-12-07 13:42:49.700775: step 15320, loss = 2.09, batch loss = 2.04 (12.7 examples/sec; 0.630 sec/batch; 55h:28m:36s remains)
INFO - root - 2017-12-07 13:42:56.530036: step 15330, loss = 2.03, batch loss = 1.97 (12.3 examples/sec; 0.651 sec/batch; 57h:19m:30s remains)
INFO - root - 2017-12-07 13:43:03.337746: step 15340, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 62h:22m:00s remains)
INFO - root - 2017-12-07 13:43:10.161837: step 15350, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.690 sec/batch; 60h:49m:45s remains)
INFO - root - 2017-12-07 13:43:16.882403: step 15360, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.701 sec/batch; 61h:45m:22s remains)
INFO - root - 2017-12-07 13:43:23.555467: step 15370, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 55h:29m:58s remains)
INFO - root - 2017-12-07 13:43:30.379962: step 15380, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 62h:29m:40s remains)
INFO - root - 2017-12-07 13:43:37.071129: step 15390, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 64h:13m:32s remains)
INFO - root - 2017-12-07 13:43:43.822082: step 15400, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 60h:45m:21s remains)
2017-12-07 13:43:44.573253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3428349 -4.3441544 -4.3438168 -4.3425474 -4.339829 -4.3323317 -4.320209 -4.3069468 -4.3064365 -4.3231387 -4.3417268 -4.3493853 -4.3446198 -4.3273787 -4.2935882][-4.343 -4.3437624 -4.3425245 -4.3394632 -4.3331685 -4.3207827 -4.3018365 -4.2812657 -4.2780881 -4.2974215 -4.3208904 -4.3310676 -4.3272386 -4.3041697 -4.2547684][-4.3469086 -4.3471527 -4.3436303 -4.3366795 -4.3241978 -4.3025904 -4.2753172 -4.2509904 -4.2500615 -4.2751875 -4.3050966 -4.320312 -4.3161488 -4.2805605 -4.2056146][-4.3506131 -4.350441 -4.3429246 -4.3287611 -4.3056645 -4.2683854 -4.2279325 -4.2004981 -4.2066035 -4.24177 -4.2842484 -4.3082361 -4.3016691 -4.24932 -4.1410427][-4.3498774 -4.3488712 -4.3355703 -4.31189 -4.2744393 -4.2200303 -4.1608086 -4.1284003 -4.1508684 -4.2090316 -4.2685804 -4.2981014 -4.2876463 -4.2207203 -4.08725][-4.3451171 -4.3414712 -4.3208346 -4.28523 -4.2318931 -4.1522403 -4.0599947 -4.0139532 -4.06143 -4.1574965 -4.2389035 -4.2789283 -4.272552 -4.2090082 -4.0928779][-4.3376284 -4.3270621 -4.2951217 -4.2447324 -4.1713524 -4.0594487 -3.9274509 -3.8624785 -3.9484975 -4.0876904 -4.1960864 -4.2561607 -4.269968 -4.2302384 -4.153409][-4.3355389 -4.3160162 -4.2705474 -4.2034497 -4.1133814 -3.9840829 -3.8277516 -3.7531347 -3.8732917 -4.046248 -4.1769791 -4.253684 -4.2839327 -4.2671089 -4.2239914][-4.3375053 -4.3137765 -4.2632623 -4.1919508 -4.1053581 -4.0006618 -3.8886271 -3.8508253 -3.9536488 -4.1016121 -4.2147751 -4.2771363 -4.2988753 -4.28684 -4.2580285][-4.3402638 -4.3201728 -4.2792592 -4.2206573 -4.1531129 -4.0871739 -4.0340018 -4.03361 -4.1021008 -4.1918511 -4.260922 -4.2998405 -4.3100162 -4.2978997 -4.2703509][-4.3482032 -4.3378282 -4.3131056 -4.273273 -4.2239442 -4.1844568 -4.16736 -4.1845145 -4.2233734 -4.26244 -4.2926369 -4.3118 -4.3091278 -4.2843971 -4.2443914][-4.3575768 -4.3514686 -4.3344846 -4.3042879 -4.2643304 -4.2377844 -4.2379017 -4.2596889 -4.2826204 -4.3014588 -4.318996 -4.323771 -4.3002968 -4.2497187 -4.185276][-4.3606181 -4.3557868 -4.3422127 -4.3132896 -4.2762818 -4.2511339 -4.2541471 -4.2771425 -4.3011713 -4.3203864 -4.3360682 -4.3295403 -4.2768908 -4.1842103 -4.0787816][-4.3496685 -4.3442111 -4.3278332 -4.2897425 -4.2461262 -4.2180848 -4.2232032 -4.2521143 -4.2836132 -4.3117309 -4.3277254 -4.3066831 -4.2220564 -4.0840292 -3.9295936][-4.3358746 -4.3265696 -4.2975268 -4.2424331 -4.18225 -4.143786 -4.1561761 -4.2063265 -4.2572737 -4.2955756 -4.3069291 -4.2718921 -4.1711636 -4.0183 -3.8520098]]...]
INFO - root - 2017-12-07 13:43:51.345766: step 15410, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 60h:17m:10s remains)
INFO - root - 2017-12-07 13:43:58.008531: step 15420, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.714 sec/batch; 62h:54m:30s remains)
INFO - root - 2017-12-07 13:44:04.534967: step 15430, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 59h:49m:02s remains)
INFO - root - 2017-12-07 13:44:11.229800: step 15440, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 54h:52m:40s remains)
INFO - root - 2017-12-07 13:44:17.992855: step 15450, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 59h:13m:56s remains)
INFO - root - 2017-12-07 13:44:24.745114: step 15460, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 62h:31m:58s remains)
INFO - root - 2017-12-07 13:44:31.581586: step 15470, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 61h:27m:40s remains)
INFO - root - 2017-12-07 13:44:38.335770: step 15480, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 56h:33m:02s remains)
INFO - root - 2017-12-07 13:44:45.065478: step 15490, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 57h:00m:20s remains)
INFO - root - 2017-12-07 13:44:51.945911: step 15500, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 59h:24m:04s remains)
2017-12-07 13:44:52.745214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1990314 -4.1886916 -4.1909757 -4.1990533 -4.2099609 -4.2185621 -4.2213707 -4.219192 -4.2156997 -4.2153606 -4.2212868 -4.2291942 -4.2397079 -4.2529874 -4.2636776][-4.2051167 -4.1846762 -4.1784563 -4.1863241 -4.2032633 -4.2189536 -4.2258935 -4.2228475 -4.2151003 -4.2106452 -4.2121177 -4.2153592 -4.2241964 -4.2375126 -4.2483797][-4.213973 -4.1873693 -4.1713152 -4.1723723 -4.1887956 -4.2088284 -4.2214518 -4.2220917 -4.215941 -4.2107878 -4.2075129 -4.2039733 -4.2075291 -4.2174959 -4.2259288][-4.21923 -4.1918478 -4.1700516 -4.1623597 -4.1712928 -4.1897345 -4.2047853 -4.2094865 -4.2083855 -4.2080383 -4.2054968 -4.1986709 -4.1966743 -4.2001467 -4.2027588][-4.210763 -4.1856174 -4.1619358 -4.1488752 -4.1504021 -4.16415 -4.1792054 -4.1874743 -4.1919212 -4.1985803 -4.202672 -4.2010021 -4.1993456 -4.1968951 -4.1895618][-4.1924005 -4.168201 -4.1426063 -4.1252885 -4.1210818 -4.1309981 -4.1458516 -4.1575294 -4.167726 -4.1805134 -4.1937504 -4.2036204 -4.2091331 -4.2048645 -4.1894059][-4.1849256 -4.1601672 -4.1302242 -4.1057858 -4.0935354 -4.0964255 -4.10728 -4.1202412 -4.1362662 -4.1563959 -4.1795468 -4.2007408 -4.2132177 -4.20887 -4.1888576][-4.1959467 -4.1735106 -4.1422987 -4.1137547 -4.0933433 -4.0850449 -4.0843925 -4.0899076 -4.1052828 -4.1314454 -4.1639948 -4.1921811 -4.2075443 -4.2018423 -4.17947][-4.2176137 -4.2013559 -4.1750212 -4.148447 -4.1243386 -4.10653 -4.09237 -4.0836124 -4.0894427 -4.114603 -4.1515884 -4.1835427 -4.1989722 -4.1926222 -4.1712289][-4.2409945 -4.2324443 -4.2137494 -4.1922069 -4.1693859 -4.1481862 -4.1263695 -4.106214 -4.0996742 -4.1149592 -4.1476483 -4.1791329 -4.1953039 -4.1921253 -4.1766291][-4.2615337 -4.2597561 -4.2489557 -4.234643 -4.2153397 -4.1926527 -4.1678505 -4.1443777 -4.1305966 -4.133976 -4.1546211 -4.179163 -4.194 -4.1946096 -4.1862206][-4.2747364 -4.2775869 -4.27381 -4.2668033 -4.2528896 -4.231998 -4.2072477 -4.1843085 -4.1677666 -4.1618652 -4.1672821 -4.1789756 -4.1881442 -4.1905222 -4.1893835][-4.2744904 -4.2807622 -4.2809739 -4.2782474 -4.2708054 -4.2565308 -4.2364025 -4.2161269 -4.2004323 -4.1895385 -4.1841655 -4.1828251 -4.1829982 -4.1837244 -4.1878476][-4.2601357 -4.2708163 -4.2732668 -4.272181 -4.2713857 -4.2671309 -4.25493 -4.2396173 -4.2260871 -4.2141938 -4.2047973 -4.1967845 -4.18862 -4.1847539 -4.1898322][-4.2361507 -4.2508936 -4.2559166 -4.2553296 -4.2599282 -4.2668791 -4.2661767 -4.2580047 -4.24656 -4.2351 -4.2259207 -4.2163696 -4.2034116 -4.1948876 -4.1972747]]...]
INFO - root - 2017-12-07 13:44:59.444475: step 15510, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 57h:04m:50s remains)
INFO - root - 2017-12-07 13:45:06.150541: step 15520, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 57h:57m:17s remains)
INFO - root - 2017-12-07 13:45:13.017086: step 15530, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 60h:58m:17s remains)
INFO - root - 2017-12-07 13:45:19.781299: step 15540, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 62h:19m:30s remains)
INFO - root - 2017-12-07 13:45:26.629211: step 15550, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 61h:33m:27s remains)
INFO - root - 2017-12-07 13:45:33.327700: step 15560, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 54h:38m:21s remains)
INFO - root - 2017-12-07 13:45:40.139122: step 15570, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 56h:07m:40s remains)
INFO - root - 2017-12-07 13:45:46.840930: step 15580, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.653 sec/batch; 57h:31m:04s remains)
INFO - root - 2017-12-07 13:45:53.759096: step 15590, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 61h:30m:35s remains)
INFO - root - 2017-12-07 13:46:00.528190: step 15600, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 60h:36m:21s remains)
2017-12-07 13:46:01.146886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1433764 -4.1759944 -4.2154803 -4.2414622 -4.2416515 -4.2201138 -4.2023678 -4.1986623 -4.194037 -4.182724 -4.1767135 -4.1831555 -4.200263 -4.212893 -4.2106404][-4.1398115 -4.17024 -4.2128725 -4.2434521 -4.2455797 -4.2213597 -4.1964231 -4.1874595 -4.1863227 -4.1821713 -4.1860895 -4.1981807 -4.2152543 -4.2254181 -4.2185836][-4.1647491 -4.182723 -4.2134237 -4.234242 -4.2279868 -4.1983957 -4.1692953 -4.1616049 -4.1733131 -4.1877432 -4.2021551 -4.2173877 -4.2334857 -4.2450814 -4.2400084][-4.194406 -4.1991615 -4.2110815 -4.2122688 -4.1898732 -4.1513457 -4.1214132 -4.1252346 -4.1588559 -4.1935949 -4.216435 -4.2310991 -4.2448759 -4.2558746 -4.2502685][-4.211472 -4.2074327 -4.2029753 -4.178555 -4.1362209 -4.090414 -4.0681562 -4.0959883 -4.1551337 -4.2073689 -4.2339492 -4.2424688 -4.2431445 -4.2423654 -4.231451][-4.2069311 -4.1990557 -4.1801739 -4.1342483 -4.0757866 -4.025589 -4.0178628 -4.0732651 -4.1536107 -4.214942 -4.2450123 -4.24537 -4.2297773 -4.2115817 -4.1958394][-4.1838355 -4.1700706 -4.1401663 -4.0814748 -4.0174603 -3.9719722 -3.9780047 -4.0496573 -4.140152 -4.2085395 -4.2404189 -4.2328677 -4.2026634 -4.1717877 -4.1532578][-4.1463032 -4.123991 -4.0897613 -4.0350437 -3.9797528 -3.9473133 -3.9572594 -4.0219455 -4.1087718 -4.1787868 -4.2077789 -4.190774 -4.1527929 -4.1227031 -4.10984][-4.1259637 -4.0939722 -4.0622416 -4.0216694 -3.9850855 -3.9680433 -3.9771221 -4.0251832 -4.0953856 -4.1563411 -4.1708264 -4.1393061 -4.0941191 -4.0682063 -4.0644646][-4.1454215 -4.1074624 -4.0802073 -4.0536547 -4.0341315 -4.0296869 -4.0371971 -4.0665483 -4.1132073 -4.1562033 -4.1543956 -4.1110721 -4.0610514 -4.0335608 -4.0341182][-4.1871848 -4.1528492 -4.1321645 -4.1184564 -4.110415 -4.1124296 -4.1163073 -4.1295285 -4.1552758 -4.1804013 -4.1666369 -4.1246057 -4.078331 -4.0437136 -4.03581][-4.2246327 -4.2059059 -4.1968141 -4.1947641 -4.1958303 -4.1975336 -4.1943831 -4.1969028 -4.2073526 -4.2176652 -4.2003636 -4.164547 -4.1250792 -4.0895238 -4.0741758][-4.2537551 -4.2492247 -4.2460709 -4.2469397 -4.2514772 -4.2543359 -4.25117 -4.2495856 -4.2532825 -4.2563729 -4.2423716 -4.2124834 -4.177352 -4.1481266 -4.1314034][-4.2801561 -4.279326 -4.27474 -4.2732997 -4.2779489 -4.2841549 -4.2862778 -4.2853141 -4.2852521 -4.2826161 -4.2675848 -4.2394018 -4.2107787 -4.1902723 -4.1783228][-4.2841644 -4.2820592 -4.2770267 -4.2769117 -4.2857013 -4.296216 -4.3003387 -4.2991118 -4.29486 -4.2868338 -4.2710285 -4.2486753 -4.2272091 -4.2118216 -4.2008452]]...]
INFO - root - 2017-12-07 13:46:07.904832: step 15610, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 57h:32m:20s remains)
INFO - root - 2017-12-07 13:46:14.552485: step 15620, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 63h:15m:15s remains)
INFO - root - 2017-12-07 13:46:21.357234: step 15630, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 57h:11m:45s remains)
INFO - root - 2017-12-07 13:46:28.151125: step 15640, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 58h:41m:43s remains)
INFO - root - 2017-12-07 13:46:34.863673: step 15650, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 56h:22m:55s remains)
INFO - root - 2017-12-07 13:46:41.652896: step 15660, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 62h:19m:02s remains)
INFO - root - 2017-12-07 13:46:48.425506: step 15670, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 62h:21m:03s remains)
INFO - root - 2017-12-07 13:46:55.109221: step 15680, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 59h:53m:32s remains)
INFO - root - 2017-12-07 13:47:01.920334: step 15690, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 56h:23m:34s remains)
INFO - root - 2017-12-07 13:47:08.752705: step 15700, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 58h:39m:25s remains)
2017-12-07 13:47:09.514620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2187123 -4.2274818 -4.2256184 -4.2211518 -4.219151 -4.2143435 -4.1985912 -4.1766887 -4.1640525 -4.1741195 -4.1986179 -4.2254233 -4.2522545 -4.2597713 -4.255693][-4.2160025 -4.2348046 -4.2433567 -4.2421689 -4.2364554 -4.2209182 -4.1937857 -4.16575 -4.1555815 -4.1735229 -4.2031507 -4.2313175 -4.2556329 -4.2605715 -4.2560759][-4.2128057 -4.2414508 -4.2582116 -4.2601156 -4.2502441 -4.2180305 -4.174211 -4.1428032 -4.1487107 -4.1842141 -4.2165146 -4.2387195 -4.2546363 -4.2544374 -4.2504439][-4.1952734 -4.2381935 -4.2639256 -4.2683325 -4.2527361 -4.1984782 -4.1289916 -4.0899267 -4.1190162 -4.18158 -4.2248039 -4.2464862 -4.2554722 -4.2485595 -4.241931][-4.1623626 -4.2214131 -4.2605762 -4.268065 -4.2412281 -4.1599851 -4.0562644 -3.9960289 -4.0454063 -4.1419134 -4.2098126 -4.2440643 -4.2549458 -4.2451448 -4.2363815][-4.1310949 -4.2035055 -4.2571883 -4.2665091 -4.2250419 -4.1100349 -3.9604356 -3.8603315 -3.9226956 -4.0635672 -4.1717644 -4.2303004 -4.2483492 -4.2385783 -4.2277794][-4.1117077 -4.1956453 -4.2612963 -4.2702403 -4.2161345 -4.0663004 -3.8625896 -3.7054143 -3.7670891 -3.9527185 -4.1102214 -4.201057 -4.2336297 -4.2272353 -4.2128606][-4.1038265 -4.1849213 -4.2573271 -4.2752481 -4.2222843 -4.074791 -3.8678446 -3.6935406 -3.7285762 -3.9128246 -4.0859251 -4.1864395 -4.2248521 -4.2190056 -4.2014775][-4.1130209 -4.1849 -4.2514052 -4.275404 -4.2419872 -4.1354942 -3.9818234 -3.8420069 -3.8441677 -3.9784627 -4.1196494 -4.2002139 -4.2311306 -4.2226696 -4.2049847][-4.1172781 -4.1804667 -4.23496 -4.2614422 -4.2520671 -4.1958294 -4.0999794 -3.9943507 -3.9737175 -4.0586047 -4.1589303 -4.2192187 -4.2440548 -4.2344289 -4.2193856][-4.112175 -4.1601844 -4.1988525 -4.2261667 -4.2389913 -4.2231174 -4.1669755 -4.0817504 -4.0486617 -4.1044497 -4.1788049 -4.22689 -4.251142 -4.2446003 -4.232954][-4.0933418 -4.1337385 -4.1618009 -4.1913805 -4.2162733 -4.2208571 -4.1876497 -4.1236367 -4.0908537 -4.1296358 -4.1875792 -4.2273722 -4.2506809 -4.248157 -4.241631][-4.0644908 -4.1050949 -4.1340766 -4.1663675 -4.1981468 -4.212501 -4.1988974 -4.1638885 -4.1418672 -4.1645212 -4.2017384 -4.2294016 -4.2491221 -4.2507162 -4.2468596][-4.0350957 -4.0689454 -4.1003661 -4.1401644 -4.180057 -4.203629 -4.209146 -4.199573 -4.1879392 -4.1967807 -4.2138681 -4.2312527 -4.2478852 -4.2525969 -4.2511816][-4.0333509 -4.0535841 -4.08271 -4.1283879 -4.1742315 -4.2019958 -4.2176805 -4.22127 -4.2151752 -4.215364 -4.2186136 -4.2283196 -4.2436666 -4.2522588 -4.25328]]...]
INFO - root - 2017-12-07 13:47:16.193623: step 15710, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 60h:07m:06s remains)
INFO - root - 2017-12-07 13:47:22.702805: step 15720, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 55h:09m:28s remains)
INFO - root - 2017-12-07 13:47:29.483461: step 15730, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 57h:42m:03s remains)
INFO - root - 2017-12-07 13:47:36.154478: step 15740, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 60h:17m:42s remains)
INFO - root - 2017-12-07 13:47:43.036948: step 15750, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 62h:09m:07s remains)
INFO - root - 2017-12-07 13:47:49.803980: step 15760, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 56h:16m:17s remains)
INFO - root - 2017-12-07 13:47:56.728458: step 15770, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 60h:19m:53s remains)
INFO - root - 2017-12-07 13:48:03.666443: step 15780, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.750 sec/batch; 65h:57m:47s remains)
INFO - root - 2017-12-07 13:48:10.402981: step 15790, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 58h:47m:07s remains)
INFO - root - 2017-12-07 13:48:17.232405: step 15800, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 56h:33m:56s remains)
2017-12-07 13:48:17.902294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1482968 -4.1491055 -4.1581006 -4.1788678 -4.2093639 -4.2411456 -4.26672 -4.281527 -4.2773986 -4.2581291 -4.229238 -4.190258 -4.1605024 -4.1606231 -4.1840286][-4.1745105 -4.1694155 -4.1714473 -4.1876011 -4.2162766 -4.2472134 -4.2690177 -4.2745094 -4.2610545 -4.2370057 -4.2106724 -4.1812735 -4.16495 -4.173913 -4.2013354][-4.1983089 -4.1891384 -4.1863389 -4.1985826 -4.2204828 -4.2390256 -4.2470207 -4.2389569 -4.2195787 -4.1972704 -4.1809926 -4.1688352 -4.1688313 -4.1856318 -4.2138624][-4.2227144 -4.2168326 -4.214664 -4.2229848 -4.2294025 -4.2263012 -4.2123113 -4.1898293 -4.1662812 -4.1503263 -4.1529942 -4.1657705 -4.1818972 -4.2024183 -4.2222323][-4.2391415 -4.2372313 -4.2413821 -4.2444882 -4.2319388 -4.204442 -4.1675434 -4.1274972 -4.0975871 -4.0964408 -4.12741 -4.1666965 -4.1944566 -4.2136912 -4.2219148][-4.2274776 -4.2260013 -4.2315974 -4.2283268 -4.2027969 -4.1570854 -4.0997324 -4.0417824 -4.0128741 -4.0393848 -4.104115 -4.1671429 -4.2055588 -4.2252817 -4.2234092][-4.1971383 -4.1940875 -4.1993623 -4.1918449 -4.1592674 -4.1013317 -4.0323453 -3.9690003 -3.957247 -4.0186744 -4.1100416 -4.1861014 -4.227037 -4.2398987 -4.2246385][-4.17142 -4.15994 -4.158884 -4.1470447 -4.1108189 -4.0528655 -3.9954615 -3.9561734 -3.97495 -4.0519133 -4.1427283 -4.2095771 -4.2390203 -4.2414112 -4.2178488][-4.1613274 -4.1344237 -4.1170197 -4.0959105 -4.0621533 -4.0198059 -3.9937029 -3.9919944 -4.0286922 -4.0950522 -4.16338 -4.2106524 -4.22722 -4.2225375 -4.1976609][-4.1667709 -4.1303453 -4.0985537 -4.070025 -4.0429039 -4.0191374 -4.0157828 -4.0303035 -4.0643959 -4.113008 -4.1604695 -4.1937847 -4.2032371 -4.1958075 -4.1738143][-4.1744118 -4.1436739 -4.1167321 -4.0954185 -4.0798788 -4.0691638 -4.07234 -4.0832925 -4.1019979 -4.1320581 -4.1623526 -4.183753 -4.1857891 -4.172936 -4.1495209][-4.1769495 -4.1634173 -4.1509719 -4.1380315 -4.1284943 -4.1238823 -4.1273861 -4.1344304 -4.14469 -4.1637821 -4.1820612 -4.1916823 -4.1837029 -4.1619072 -4.1326942][-4.1889858 -4.1940584 -4.1913881 -4.1781216 -4.1646309 -4.1570978 -4.157351 -4.16186 -4.1707444 -4.1858258 -4.1977577 -4.1982212 -4.18384 -4.1603742 -4.1325808][-4.21496 -4.2271466 -4.2234573 -4.2061791 -4.189867 -4.1812081 -4.1798253 -4.1827049 -4.1898093 -4.1986241 -4.2007461 -4.1919127 -4.1748023 -4.1565466 -4.1382904][-4.2484894 -4.2537856 -4.2439175 -4.2258387 -4.2128668 -4.2081838 -4.2088656 -4.2116475 -4.2143803 -4.2135849 -4.2057724 -4.1905708 -4.173727 -4.1622224 -4.153482]]...]
INFO - root - 2017-12-07 13:48:24.713296: step 15810, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 62h:03m:01s remains)
INFO - root - 2017-12-07 13:48:31.377009: step 15820, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 62h:51m:48s remains)
INFO - root - 2017-12-07 13:48:38.249393: step 15830, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 57h:55m:39s remains)
INFO - root - 2017-12-07 13:48:45.025693: step 15840, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 58h:06m:44s remains)
INFO - root - 2017-12-07 13:48:51.904409: step 15850, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.713 sec/batch; 62h:44m:14s remains)
INFO - root - 2017-12-07 13:48:58.698256: step 15860, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 62h:19m:45s remains)
INFO - root - 2017-12-07 13:49:05.473200: step 15870, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 55h:36m:42s remains)
INFO - root - 2017-12-07 13:49:12.235140: step 15880, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 58h:04m:13s remains)
INFO - root - 2017-12-07 13:49:19.023093: step 15890, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.695 sec/batch; 61h:08m:40s remains)
INFO - root - 2017-12-07 13:49:25.873239: step 15900, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 63h:22m:16s remains)
2017-12-07 13:49:26.527270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3205314 -4.3223 -4.3207788 -4.3160558 -4.31626 -4.3216538 -4.3275919 -4.3287082 -4.3224726 -4.3102751 -4.296175 -4.2876506 -4.2872152 -4.2950597 -4.3093643][-4.286191 -4.2920308 -4.2925668 -4.28875 -4.2897954 -4.2968378 -4.3049622 -4.30801 -4.2997885 -4.2805128 -4.2567472 -4.2410483 -4.2393517 -4.2534318 -4.2766509][-4.2308874 -4.2372789 -4.2374945 -4.2339616 -4.2376924 -4.24938 -4.261229 -4.2676153 -4.2591772 -4.2378612 -4.2106934 -4.192143 -4.1926842 -4.21389 -4.2442431][-4.1677747 -4.1682611 -4.1639543 -4.159287 -4.16594 -4.1808305 -4.1925845 -4.2022815 -4.2023129 -4.1922231 -4.1756172 -4.16527 -4.1719174 -4.1950088 -4.2240458][-4.1269455 -4.121489 -4.1096611 -4.0997958 -4.1002045 -4.1026673 -4.0985823 -4.1036959 -4.1209507 -4.1394835 -4.150805 -4.1603708 -4.1761975 -4.1994505 -4.2188363][-4.1227684 -4.1185503 -4.1001792 -4.0792418 -4.0581737 -4.0259147 -3.9814391 -3.9656029 -4.0073938 -4.0710912 -4.1194181 -4.1500363 -4.1753354 -4.2003837 -4.2152781][-4.1419563 -4.1457291 -4.1303215 -4.1013875 -4.0568395 -3.9831171 -3.8878207 -3.840158 -3.9048338 -4.0083804 -4.079689 -4.1181507 -4.1478848 -4.1772866 -4.1983948][-4.1618404 -4.1791306 -4.1779704 -4.1516113 -4.0981793 -4.0099344 -3.9042625 -3.8501148 -3.9078257 -4.0001955 -4.0538125 -4.078104 -4.10225 -4.1354856 -4.1708813][-4.1692934 -4.205174 -4.2240028 -4.2125134 -4.1702785 -4.0984583 -4.0215745 -3.9869781 -4.0145264 -4.0535378 -4.0602875 -4.0546546 -4.0651593 -4.0991573 -4.1433983][-4.1748834 -4.2200093 -4.2534151 -4.258678 -4.2371373 -4.1930614 -4.1480384 -4.1253834 -4.1254854 -4.1245103 -4.0987506 -4.0736094 -4.0743079 -4.1011314 -4.137157][-4.193893 -4.2315731 -4.268743 -4.2864966 -4.2821703 -4.2631612 -4.2412 -4.2236147 -4.210073 -4.1942368 -4.1626048 -4.1375208 -4.1352248 -4.1505384 -4.167552][-4.219923 -4.2416129 -4.2756667 -4.3009953 -4.3088045 -4.3045239 -4.2954845 -4.2809072 -4.2648854 -4.2508054 -4.2297935 -4.2151933 -4.2150683 -4.2212586 -4.2224941][-4.252202 -4.2610469 -4.2874961 -4.3130217 -4.3244357 -4.3251843 -4.3225765 -4.3128738 -4.3020639 -4.2951 -4.2866268 -4.2822728 -4.2846088 -4.2862163 -4.280477][-4.2882681 -4.29224 -4.3105206 -4.3300958 -4.33961 -4.339972 -4.3373518 -4.3296814 -4.325254 -4.3263092 -4.3277245 -4.3292589 -4.3316231 -4.3297381 -4.3204041][-4.3194337 -4.3233061 -4.334548 -4.3460264 -4.3510323 -4.3499813 -4.3462276 -4.3390775 -4.3367462 -4.3423977 -4.3501348 -4.3542895 -4.356812 -4.3526335 -4.3428411]]...]
INFO - root - 2017-12-07 13:49:33.228737: step 15910, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 57h:48m:47s remains)
INFO - root - 2017-12-07 13:49:39.872505: step 15920, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 60h:13m:17s remains)
INFO - root - 2017-12-07 13:49:46.837487: step 15930, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 62h:38m:34s remains)
INFO - root - 2017-12-07 13:49:53.661446: step 15940, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 59h:55m:09s remains)
INFO - root - 2017-12-07 13:50:00.439370: step 15950, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.619 sec/batch; 54h:27m:18s remains)
INFO - root - 2017-12-07 13:50:07.272076: step 15960, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 57h:20m:32s remains)
INFO - root - 2017-12-07 13:50:14.047856: step 15970, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 61h:43m:51s remains)
INFO - root - 2017-12-07 13:50:20.834014: step 15980, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 60h:21m:55s remains)
INFO - root - 2017-12-07 13:50:27.759858: step 15990, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 63h:59m:57s remains)
INFO - root - 2017-12-07 13:50:34.469929: step 16000, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 55h:32m:36s remains)
2017-12-07 13:50:35.219201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2013874 -4.1917372 -4.1938853 -4.2050681 -4.2267628 -4.2383509 -4.2380023 -4.2413945 -4.2424178 -4.2534237 -4.2599282 -4.2408018 -4.2241597 -4.2242584 -4.2311392][-4.1803422 -4.1722932 -4.1782556 -4.1984019 -4.2274895 -4.2418385 -4.2408495 -4.2471929 -4.250689 -4.2581511 -4.2584081 -4.2389402 -4.2201023 -4.2193723 -4.2362967][-4.1800227 -4.1800737 -4.1908741 -4.2161436 -4.244442 -4.2531476 -4.2496128 -4.2478271 -4.2497463 -4.2550173 -4.2490325 -4.227643 -4.1986327 -4.1875987 -4.207778][-4.2067084 -4.2117829 -4.2274776 -4.2556615 -4.2787986 -4.2748766 -4.2555318 -4.2301846 -4.2186108 -4.2132568 -4.209722 -4.1999059 -4.1743197 -4.154017 -4.1615644][-4.2130113 -4.2192574 -4.2436781 -4.276185 -4.2893081 -4.2648592 -4.2168279 -4.1559291 -4.1160936 -4.1069016 -4.1271663 -4.1439018 -4.137207 -4.1185393 -4.1102324][-4.1873441 -4.1866865 -4.2113986 -4.2498474 -4.2556753 -4.2084594 -4.1295485 -4.0314221 -3.9581242 -3.9542289 -4.0140219 -4.0652637 -4.080524 -4.0695477 -4.0542369][-4.1179042 -4.0986052 -4.1198273 -4.1707077 -4.1839027 -4.1258883 -4.0257607 -3.9082246 -3.8136883 -3.8229091 -3.9281843 -4.0089769 -4.041028 -4.0316639 -4.011868][-4.0121269 -3.9696724 -3.9856758 -4.0564828 -4.0977316 -4.0582752 -3.969543 -3.8700082 -3.7976384 -3.8290057 -3.9378374 -4.01648 -4.0506425 -4.0370812 -4.0126553][-3.9415896 -3.8823736 -3.8910477 -3.9764609 -4.0509567 -4.0497437 -4.0034719 -3.9477987 -3.9129605 -3.9473855 -4.0234504 -4.0790157 -4.1054215 -4.0901613 -4.0647783][-3.9838853 -3.9375098 -3.9408815 -4.0071254 -4.0802064 -4.1011124 -4.0856667 -4.0591936 -4.0505934 -4.0823951 -4.1315651 -4.1688709 -4.1876431 -4.1726766 -4.1474137][-4.0838075 -4.0603905 -4.0596042 -4.0938215 -4.1341619 -4.1499228 -4.1463656 -4.136704 -4.1468062 -4.1824379 -4.2203236 -4.2444615 -4.2523108 -4.2340307 -4.210237][-4.1584697 -4.1481776 -4.1459551 -4.1555314 -4.1629038 -4.17093 -4.1750755 -4.17717 -4.1971188 -4.2312412 -4.2596006 -4.2741327 -4.275104 -4.257412 -4.2384644][-4.1970248 -4.1838007 -4.1782017 -4.1772871 -4.1737051 -4.1822891 -4.1945972 -4.2036104 -4.2199836 -4.2423768 -4.2578287 -4.2666297 -4.2664218 -4.2542963 -4.2413311][-4.2049251 -4.1895628 -4.1845622 -4.1866331 -4.1894946 -4.2044997 -4.2195911 -4.2275038 -4.2337694 -4.2386932 -4.2405038 -4.2444482 -4.2442489 -4.2347703 -4.2248487][-4.1976562 -4.1856265 -4.185636 -4.1939192 -4.2048521 -4.22317 -4.2352834 -4.238265 -4.2380676 -4.2320638 -4.2226295 -4.2224336 -4.222281 -4.2139897 -4.2050033]]...]
INFO - root - 2017-12-07 13:50:41.922965: step 16010, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 61h:58m:35s remains)
INFO - root - 2017-12-07 13:50:48.596448: step 16020, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 63h:00m:55s remains)
INFO - root - 2017-12-07 13:50:55.519147: step 16030, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 55h:38m:51s remains)
INFO - root - 2017-12-07 13:51:02.327982: step 16040, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 61h:47m:59s remains)
INFO - root - 2017-12-07 13:51:09.002576: step 16050, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 60h:10m:56s remains)
INFO - root - 2017-12-07 13:51:15.757625: step 16060, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 58h:03m:52s remains)
INFO - root - 2017-12-07 13:51:22.699254: step 16070, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 56h:55m:42s remains)
INFO - root - 2017-12-07 13:51:29.571525: step 16080, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 64h:26m:06s remains)
INFO - root - 2017-12-07 13:51:36.289252: step 16090, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 62h:48m:34s remains)
INFO - root - 2017-12-07 13:51:43.152360: step 16100, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 60h:05m:46s remains)
2017-12-07 13:51:43.863044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3062453 -4.31177 -4.3117681 -4.3111777 -4.3106575 -4.3098578 -4.3100061 -4.309689 -4.3084965 -4.30708 -4.3063774 -4.3068552 -4.3113956 -4.3207603 -4.3307519][-4.2857995 -4.2926941 -4.2946725 -4.2972593 -4.3003831 -4.3025265 -4.3046346 -4.3054576 -4.304379 -4.3021841 -4.30039 -4.298461 -4.3010235 -4.3107777 -4.3226743][-4.2660027 -4.271265 -4.2716169 -4.272541 -4.2758102 -4.2785945 -4.2814379 -4.2843866 -4.2864566 -4.2875237 -4.2885432 -4.2868257 -4.2871022 -4.2958512 -4.308682][-4.24203 -4.2407265 -4.2339945 -4.2281251 -4.2263336 -4.2253022 -4.2253022 -4.2294431 -4.2376575 -4.2466445 -4.2556224 -4.2572136 -4.2556481 -4.2632842 -4.2785616][-4.2046704 -4.193728 -4.1762242 -4.1603012 -4.1491261 -4.1361346 -4.1249285 -4.1285763 -4.1485181 -4.1725774 -4.1963859 -4.2063265 -4.205411 -4.2136216 -4.2331715][-4.1631947 -4.1430268 -4.1137676 -4.0863066 -4.0640545 -4.0322948 -3.9978447 -3.9941773 -4.0280995 -4.0712385 -4.1135168 -4.1386666 -4.1461954 -4.1598396 -4.1871057][-4.1443124 -4.1160574 -4.0757194 -4.0342321 -3.9977422 -3.9462883 -3.8802946 -3.8638263 -3.9161558 -3.9814305 -4.0393119 -4.07745 -4.0965891 -4.1181808 -4.1532693][-4.163064 -4.1340079 -4.0871181 -4.0324578 -3.9786038 -3.907341 -3.8130586 -3.7797408 -3.8466289 -3.9287493 -3.9936512 -4.0394983 -4.0721045 -4.10248 -4.1411829][-4.1988521 -4.1817045 -4.1474261 -4.0995855 -4.0435667 -3.9708035 -3.8804724 -3.8417661 -3.8885388 -3.9519169 -4.0051827 -4.0518613 -4.0915103 -4.1246233 -4.1603069][-4.2217894 -4.2234745 -4.212285 -4.1844788 -4.1461134 -4.0941453 -4.0332918 -4.0043688 -4.0213642 -4.0472131 -4.0722713 -4.1042652 -4.1378031 -4.1659102 -4.1942086][-4.2176208 -4.234468 -4.243855 -4.2381868 -4.224802 -4.20022 -4.1687679 -4.1516066 -4.1508465 -4.1518154 -4.1532607 -4.1645355 -4.1833868 -4.203083 -4.2244606][-4.1863189 -4.2145691 -4.2396917 -4.253335 -4.26143 -4.2595782 -4.2488375 -4.2397718 -4.2318268 -4.220798 -4.2100682 -4.2066426 -4.2111669 -4.2227116 -4.2396159][-4.1668134 -4.1960897 -4.2244611 -4.2476125 -4.2709179 -4.2870059 -4.2911863 -4.2876983 -4.2785378 -4.262259 -4.2450533 -4.2329149 -4.2289777 -4.2358532 -4.2505665][-4.2009349 -4.219636 -4.2375488 -4.2547073 -4.279572 -4.3009081 -4.3114886 -4.312602 -4.3077874 -4.2962337 -4.2812915 -4.2686424 -4.2619185 -4.2650175 -4.2751946][-4.2616391 -4.2709713 -4.278749 -4.288094 -4.3053856 -4.3206987 -4.3278003 -4.3288655 -4.3271589 -4.3216028 -4.3132677 -4.3053312 -4.3002687 -4.3007512 -4.3054919]]...]
INFO - root - 2017-12-07 13:51:50.610778: step 16110, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 62h:12m:09s remains)
INFO - root - 2017-12-07 13:51:57.279342: step 16120, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 60h:39m:21s remains)
INFO - root - 2017-12-07 13:52:04.163505: step 16130, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 55h:59m:01s remains)
INFO - root - 2017-12-07 13:52:10.912495: step 16140, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 57h:42m:57s remains)
INFO - root - 2017-12-07 13:52:17.801055: step 16150, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 61h:10m:40s remains)
INFO - root - 2017-12-07 13:52:24.730463: step 16160, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 64h:38m:19s remains)
INFO - root - 2017-12-07 13:52:31.621547: step 16170, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 58h:21m:07s remains)
INFO - root - 2017-12-07 13:52:38.434199: step 16180, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 57h:11m:21s remains)
INFO - root - 2017-12-07 13:52:45.287140: step 16190, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 57h:28m:27s remains)
INFO - root - 2017-12-07 13:52:52.099707: step 16200, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 61h:39m:50s remains)
2017-12-07 13:52:52.856800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0850973 -4.0765281 -4.0983682 -4.1323748 -4.1634793 -4.1845293 -4.1705747 -4.144598 -4.1191368 -4.0969162 -4.0821667 -4.0833488 -4.1042418 -4.1243439 -4.1395731][-4.0828366 -4.0706267 -4.0855837 -4.1118841 -4.1450963 -4.1668711 -4.1531596 -4.1305885 -4.1099906 -4.0851641 -4.06928 -4.0720882 -4.095849 -4.121963 -4.1380172][-4.0785127 -4.0692468 -4.0760064 -4.0895591 -4.1213508 -4.1420469 -4.1300879 -4.11283 -4.1036134 -4.0893617 -4.0829268 -4.0919108 -4.1100464 -4.131248 -4.1426682][-4.07612 -4.0766463 -4.0829554 -4.0918179 -4.1154633 -4.1278257 -4.112545 -4.0973091 -4.0943232 -4.0895514 -4.0904179 -4.1026883 -4.1136217 -4.1287842 -4.1406121][-4.0752163 -4.0852966 -4.0988116 -4.1129031 -4.1308231 -4.1314063 -4.1081119 -4.0833306 -4.0735054 -4.0721397 -4.0786037 -4.0908203 -4.09614 -4.1067476 -4.1181068][-4.0705323 -4.0866156 -4.107233 -4.134212 -4.1512 -4.1390543 -4.1029553 -4.0556812 -4.0349388 -4.0395417 -4.0526476 -4.0710888 -4.0756316 -4.0810966 -4.0871167][-4.0570254 -4.0722036 -4.1014967 -4.139946 -4.15312 -4.1262665 -4.0684829 -3.9961984 -3.9745641 -4.0022678 -4.0347896 -4.0664663 -4.0744348 -4.0727758 -4.068862][-4.0473013 -4.0602689 -4.0944037 -4.1328492 -4.1367693 -4.0919809 -4.011364 -3.9193988 -3.9072578 -3.9691563 -4.02984 -4.0733609 -4.0832167 -4.0737314 -4.0619149][-4.0602217 -4.0733705 -4.1049376 -4.1326427 -4.1241894 -4.06902 -3.9807737 -3.8889503 -3.885361 -3.9634893 -4.0372057 -4.0870976 -4.0982671 -4.0848413 -4.0708151][-4.0904279 -4.1031246 -4.124404 -4.1382494 -4.1207633 -4.0690169 -3.9976056 -3.9398909 -3.9455 -4.0075803 -4.0693655 -4.1154132 -4.1279516 -4.1143451 -4.0987034][-4.1330943 -4.1443133 -4.1563082 -4.1598215 -4.1367397 -4.0970855 -4.0504041 -4.028007 -4.0392704 -4.07959 -4.1245503 -4.1596379 -4.1698513 -4.1584973 -4.14569][-4.1671886 -4.1766438 -4.179976 -4.1751881 -4.1522946 -4.1264215 -4.1066375 -4.1087742 -4.1227946 -4.1511412 -4.1808233 -4.2001266 -4.2032247 -4.1935306 -4.183269][-4.2011533 -4.2050686 -4.2011943 -4.1938977 -4.1800032 -4.1691775 -4.1654267 -4.1788054 -4.1934605 -4.2119517 -4.2272511 -4.2338381 -4.2323418 -4.2262907 -4.2211747][-4.238399 -4.2351427 -4.2264128 -4.219667 -4.2145605 -4.2087746 -4.2093263 -4.2239761 -4.2386866 -4.2524037 -4.2602539 -4.2622743 -4.2617869 -4.26093 -4.2592235][-4.2634339 -4.2547679 -4.24439 -4.2405376 -4.2408962 -4.2388568 -4.2409115 -4.2504377 -4.2614293 -4.2701964 -4.2740936 -4.2764211 -4.2787352 -4.281065 -4.2816734]]...]
INFO - root - 2017-12-07 13:52:59.623980: step 16210, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 56h:48m:52s remains)
INFO - root - 2017-12-07 13:53:06.376297: step 16220, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 57h:31m:02s remains)
INFO - root - 2017-12-07 13:53:13.236200: step 16230, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 61h:20m:36s remains)
INFO - root - 2017-12-07 13:53:20.125991: step 16240, loss = 2.04, batch loss = 1.98 (10.7 examples/sec; 0.744 sec/batch; 65h:22m:46s remains)
INFO - root - 2017-12-07 13:53:26.913176: step 16250, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 58h:19m:55s remains)
INFO - root - 2017-12-07 13:53:33.606845: step 16260, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 57h:02m:42s remains)
INFO - root - 2017-12-07 13:53:40.443819: step 16270, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 57h:04m:38s remains)
INFO - root - 2017-12-07 13:53:47.271261: step 16280, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 61h:13m:19s remains)
INFO - root - 2017-12-07 13:53:54.195698: step 16290, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.744 sec/batch; 65h:18m:26s remains)
INFO - root - 2017-12-07 13:54:01.024183: step 16300, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 59h:29m:15s remains)
2017-12-07 13:54:01.701057: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.317296 -4.3103471 -4.3035927 -4.2964797 -4.2923613 -4.2927742 -4.2972569 -4.3067 -4.3152771 -4.3147173 -4.3048124 -4.2915297 -4.2846971 -4.2903481 -4.3017588][-4.293891 -4.2834625 -4.2754498 -4.2657576 -4.2571664 -4.2523775 -4.2542162 -4.2677913 -4.2822213 -4.2839522 -4.2682328 -4.2464814 -4.2357631 -4.2465348 -4.2705231][-4.2611055 -4.2531729 -4.2491646 -4.2372093 -4.2183347 -4.2004347 -4.1958704 -4.2149816 -4.2390232 -4.2467108 -4.2273483 -4.1953826 -4.1763711 -4.1878877 -4.2216487][-4.2352767 -4.23465 -4.2380462 -4.2240553 -4.1895952 -4.1498017 -4.1357851 -4.1649628 -4.2058015 -4.2244363 -4.2077036 -4.1671014 -4.1321592 -4.1352811 -4.1730537][-4.218215 -4.2231464 -4.2304688 -4.2127891 -4.1614752 -4.0945091 -4.066359 -4.108367 -4.1753759 -4.2124596 -4.2059402 -4.1654878 -4.1160612 -4.10415 -4.1383615][-4.1978807 -4.2083611 -4.2181835 -4.1961131 -4.1259837 -4.027812 -3.9778388 -4.0330257 -4.1325126 -4.1961432 -4.2094817 -4.1811976 -4.1261253 -4.098166 -4.1221323][-4.1684051 -4.1828184 -4.1951623 -4.1715837 -4.0881991 -3.9620249 -3.8859351 -3.9466128 -4.0767636 -4.1733475 -4.2129831 -4.2008672 -4.1463952 -4.1037841 -4.1126342][-4.1461639 -4.1641846 -4.1815372 -4.1628575 -4.0817914 -3.9523137 -3.8651781 -3.9153919 -4.0493712 -4.1633892 -4.2236152 -4.2270317 -4.1790018 -4.1264863 -4.116785][-4.145977 -4.1666446 -4.1912117 -4.186903 -4.1287394 -4.0281453 -3.9532187 -3.9759943 -4.0767384 -4.17663 -4.2405138 -4.253242 -4.2149158 -4.1577129 -4.1321607][-4.1583939 -4.1801124 -4.2106733 -4.2236381 -4.196723 -4.1335893 -4.0757322 -4.0716195 -4.1289196 -4.20164 -4.2562065 -4.2713361 -4.2406669 -4.1850271 -4.1522694][-4.1809373 -4.1998115 -4.2307868 -4.2561064 -4.2547908 -4.2248492 -4.1822243 -4.1632009 -4.1886854 -4.2363529 -4.2788062 -4.2927418 -4.2664638 -4.2165446 -4.1857328][-4.2166033 -4.2305622 -4.2595525 -4.2897015 -4.3015356 -4.2954626 -4.2687974 -4.2471447 -4.2552958 -4.2826204 -4.312079 -4.3229656 -4.3009677 -4.2583838 -4.2313542][-4.2594972 -4.2702141 -4.2951851 -4.322227 -4.33617 -4.338306 -4.3246274 -4.308392 -4.3082542 -4.3217311 -4.3397026 -4.3457265 -4.3266244 -4.2933049 -4.2710009][-4.2923803 -4.2995934 -4.3170614 -4.3361282 -4.3457313 -4.3491216 -4.3442378 -4.3356619 -4.3329716 -4.33728 -4.3452549 -4.3461366 -4.3305821 -4.3071451 -4.2907386][-4.3073258 -4.3111596 -4.3195434 -4.3279963 -4.3310862 -4.3334723 -4.3335896 -4.3311043 -4.3295469 -4.3300796 -4.3322005 -4.3308992 -4.3213534 -4.3082843 -4.2988029]]...]
INFO - root - 2017-12-07 13:54:08.475862: step 16310, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 59h:04m:49s remains)
INFO - root - 2017-12-07 13:54:15.114189: step 16320, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 59h:51m:57s remains)
INFO - root - 2017-12-07 13:54:22.016407: step 16330, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 57h:24m:54s remains)
INFO - root - 2017-12-07 13:54:28.846095: step 16340, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 56h:21m:09s remains)
INFO - root - 2017-12-07 13:54:35.639520: step 16350, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 56h:15m:50s remains)
INFO - root - 2017-12-07 13:54:42.225482: step 16360, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 56h:17m:22s remains)
INFO - root - 2017-12-07 13:54:49.030756: step 16370, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 56h:14m:56s remains)
INFO - root - 2017-12-07 13:54:55.792540: step 16380, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.632 sec/batch; 55h:32m:04s remains)
INFO - root - 2017-12-07 13:55:02.557441: step 16390, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 59h:53m:50s remains)
INFO - root - 2017-12-07 13:55:09.421001: step 16400, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 63h:52m:01s remains)
2017-12-07 13:55:10.123245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2527471 -4.2407742 -4.2269678 -4.1990185 -4.1384068 -4.0719194 -4.0317712 -4.0277042 -4.0622015 -4.1152363 -4.1655684 -4.2004819 -4.2254767 -4.2471642 -4.2739148][-4.2388148 -4.2374249 -4.2377715 -4.2186155 -4.1610732 -4.0946374 -4.0509925 -4.0408573 -4.0699062 -4.1164103 -4.1603012 -4.1888571 -4.2129312 -4.2372041 -4.2673554][-4.2116671 -4.2212548 -4.238584 -4.241992 -4.2054749 -4.1413789 -4.0805597 -4.0494595 -4.0718074 -4.1167626 -4.1594963 -4.1866837 -4.2074547 -4.2330408 -4.2643213][-4.1647711 -4.1781063 -4.2094889 -4.2304878 -4.2103114 -4.1442814 -4.0519676 -3.9960468 -4.0265203 -4.0939851 -4.1537795 -4.190846 -4.2089057 -4.2353635 -4.2651968][-4.1104641 -4.1261282 -4.169045 -4.1962209 -4.1733861 -4.0889249 -3.9477282 -3.8565338 -3.9139893 -4.0301323 -4.1288729 -4.1847491 -4.2102056 -4.2429829 -4.2709494][-4.067441 -4.0931482 -4.1441431 -4.1646008 -4.1223264 -4.0042114 -3.8009472 -3.6666174 -3.7780404 -3.9659019 -4.1082053 -4.1812563 -4.2126184 -4.2465615 -4.2737956][-4.0600781 -4.0931091 -4.1390705 -4.1448951 -4.0810285 -3.9420955 -3.7243 -3.596096 -3.7547576 -3.9699259 -4.1158752 -4.1892319 -4.2180972 -4.2458882 -4.27105][-4.0979595 -4.1252465 -4.1586571 -4.1535168 -4.0934591 -3.9852605 -3.8368649 -3.77268 -3.8952808 -4.0505261 -4.1590462 -4.2127547 -4.2291083 -4.2477427 -4.2698808][-4.1730809 -4.1924047 -4.2136731 -4.2018213 -4.1466036 -4.0738344 -3.9919884 -3.9683824 -4.0451388 -4.1343589 -4.1996765 -4.2306533 -4.2402649 -4.2543507 -4.2743535][-4.2248406 -4.2369957 -4.2518773 -4.2413039 -4.191052 -4.1370554 -4.0914803 -4.0873427 -4.13753 -4.1869187 -4.2257028 -4.243094 -4.250545 -4.2633319 -4.2805533][-4.2274933 -4.2393456 -4.2530913 -4.2503076 -4.2165875 -4.1772385 -4.1458755 -4.1426406 -4.1719832 -4.1973715 -4.2202435 -4.2374568 -4.2509861 -4.2652035 -4.2836471][-4.20925 -4.2179637 -4.22997 -4.2302537 -4.2054334 -4.1716032 -4.1437473 -4.1374083 -4.1565361 -4.1760716 -4.2004023 -4.2288246 -4.2529869 -4.2705064 -4.2910547][-4.1906004 -4.19215 -4.1991949 -4.2050462 -4.1919641 -4.1696978 -4.1461043 -4.1343904 -4.1495314 -4.1705256 -4.204772 -4.2436175 -4.2705951 -4.285924 -4.3036904][-4.1804361 -4.1760383 -4.1781187 -4.1926537 -4.1959715 -4.1878161 -4.1707392 -4.1579933 -4.1689363 -4.19411 -4.2386947 -4.2818303 -4.3015795 -4.30962 -4.3195362][-4.1957235 -4.1884532 -4.1878114 -4.2077513 -4.2196693 -4.2203403 -4.2083831 -4.1957874 -4.2069902 -4.235569 -4.2797122 -4.31594 -4.3274727 -4.3302689 -4.3342085]]...]
INFO - root - 2017-12-07 13:55:16.878085: step 16410, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 57h:55m:53s remains)
INFO - root - 2017-12-07 13:55:23.590858: step 16420, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 60h:47m:29s remains)
INFO - root - 2017-12-07 13:55:30.427049: step 16430, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 60h:29m:00s remains)
INFO - root - 2017-12-07 13:55:37.205379: step 16440, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 57h:37m:06s remains)
INFO - root - 2017-12-07 13:55:44.191101: step 16450, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 60h:26m:12s remains)
INFO - root - 2017-12-07 13:55:50.953928: step 16460, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 60h:42m:06s remains)
INFO - root - 2017-12-07 13:55:57.803678: step 16470, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 61h:00m:38s remains)
INFO - root - 2017-12-07 13:56:04.541414: step 16480, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 57h:04m:34s remains)
INFO - root - 2017-12-07 13:56:11.322834: step 16490, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 55h:40m:29s remains)
INFO - root - 2017-12-07 13:56:18.115949: step 16500, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 62h:31m:36s remains)
2017-12-07 13:56:18.825159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3073039 -4.2918272 -4.2760744 -4.265039 -4.2542186 -4.2453938 -4.2453756 -4.2535849 -4.2525868 -4.2464547 -4.2502003 -4.250926 -4.241219 -4.2374935 -4.2466593][-4.2818642 -4.2585521 -4.2344489 -4.2170076 -4.2042136 -4.1993775 -4.2056556 -4.2219572 -4.2231417 -4.2129431 -4.2118988 -4.2140536 -4.2118564 -4.2125778 -4.2244058][-4.2543268 -4.2260852 -4.1981792 -4.1776013 -4.1625571 -4.1609764 -4.1734333 -4.1952963 -4.1948218 -4.1780043 -4.1686277 -4.1700368 -4.1717024 -4.1760821 -4.191041][-4.2421794 -4.2140608 -4.1875758 -4.1666079 -4.1499295 -4.1450386 -4.1570873 -4.176403 -4.1703897 -4.14371 -4.1211343 -4.1170511 -4.121489 -4.1326408 -4.1517634][-4.2308879 -4.2038121 -4.1861949 -4.16817 -4.14818 -4.1345944 -4.1325569 -4.136652 -4.1205859 -4.0845146 -4.0556488 -4.0553603 -4.0696483 -4.0904093 -4.1156368][-4.2182088 -4.1943564 -4.1883612 -4.1739435 -4.1481185 -4.1177516 -4.08718 -4.0679092 -4.0386868 -4.000453 -3.9800768 -4.0004997 -4.0337772 -4.0663524 -4.1007137][-4.2119465 -4.1883187 -4.1894264 -4.1806469 -4.1471491 -4.0894313 -4.0252175 -3.983614 -3.9531584 -3.9230375 -3.9216585 -3.9668865 -4.0189991 -4.0625262 -4.1087828][-4.1977873 -4.1728067 -4.1745892 -4.1691904 -4.1349216 -4.0582118 -3.9668512 -3.9167547 -3.9100542 -3.9114366 -3.9276502 -3.9776344 -4.0341406 -4.0834551 -4.1352448][-4.1781044 -4.1501079 -4.1472263 -4.1429844 -4.1174908 -4.0512414 -3.9678354 -3.9299691 -3.95302 -3.9834459 -4.0059576 -4.0439763 -4.0904388 -4.1349592 -4.1819887][-4.1812758 -4.1540947 -4.1434312 -4.1361327 -4.1230617 -4.0856304 -4.0255761 -3.9990759 -4.0321965 -4.0685687 -4.0882478 -4.1122689 -4.148839 -4.1854935 -4.2221432][-4.2154651 -4.1952686 -4.1800718 -4.1675525 -4.1629171 -4.1509795 -4.1197057 -4.1004734 -4.1238027 -4.150825 -4.1638327 -4.181036 -4.2070026 -4.2312531 -4.2546639][-4.2537346 -4.2431021 -4.2281871 -4.2117152 -4.214416 -4.2240615 -4.218246 -4.2080126 -4.2175608 -4.2319961 -4.2390032 -4.2476525 -4.2604342 -4.2712278 -4.28238][-4.2898731 -4.2874093 -4.2755418 -4.2615652 -4.2697916 -4.2896872 -4.2957458 -4.2923803 -4.2952619 -4.3006711 -4.3032823 -4.3060584 -4.3096361 -4.3116827 -4.3152452][-4.3215837 -4.3219352 -4.314044 -4.3056636 -4.3133945 -4.3291368 -4.3342667 -4.3343096 -4.3353653 -4.3372145 -4.3409243 -4.3437214 -4.3428011 -4.3398705 -4.3386445][-4.3266516 -4.3286042 -4.3259153 -4.3224683 -4.3274627 -4.3356729 -4.3358951 -4.3345203 -4.3360429 -4.3396034 -4.3459778 -4.3490906 -4.3460879 -4.3404527 -4.3362207]]...]
INFO - root - 2017-12-07 13:56:25.522125: step 16510, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 54h:35m:58s remains)
INFO - root - 2017-12-07 13:56:32.112779: step 16520, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 60h:25m:59s remains)
INFO - root - 2017-12-07 13:56:38.951936: step 16530, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 64h:08m:04s remains)
INFO - root - 2017-12-07 13:56:45.690380: step 16540, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 60h:56m:40s remains)
INFO - root - 2017-12-07 13:56:52.504710: step 16550, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 59h:13m:43s remains)
INFO - root - 2017-12-07 13:56:59.256825: step 16560, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 56h:45m:56s remains)
INFO - root - 2017-12-07 13:57:06.087817: step 16570, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 58h:20m:13s remains)
INFO - root - 2017-12-07 13:57:12.909814: step 16580, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 60h:48m:52s remains)
INFO - root - 2017-12-07 13:57:19.791976: step 16590, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 62h:08m:21s remains)
INFO - root - 2017-12-07 13:57:26.469416: step 16600, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 57h:21m:46s remains)
2017-12-07 13:57:27.168460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516713 -4.2356472 -4.2269716 -4.23882 -4.255784 -4.2662029 -4.2791343 -4.2939806 -4.3052897 -4.3148479 -4.3249664 -4.3283138 -4.3216186 -4.3145504 -4.3052278][-4.2679105 -4.25473 -4.2446842 -4.2509956 -4.2624621 -4.2673464 -4.2759366 -4.2908325 -4.3056026 -4.319253 -4.3313847 -4.3355875 -4.3312817 -4.3244929 -4.3154783][-4.2769017 -4.262856 -4.2477894 -4.2442827 -4.2473893 -4.2493949 -4.2577605 -4.2748127 -4.2950864 -4.313765 -4.3285275 -4.3360686 -4.3360009 -4.3309989 -4.3218555][-4.2813139 -4.263823 -4.2416143 -4.22736 -4.221168 -4.2192173 -4.22748 -4.2471466 -4.2734284 -4.2982297 -4.3169789 -4.3284888 -4.3329425 -4.3312793 -4.3226566][-4.291882 -4.268919 -4.2381511 -4.2108254 -4.1923904 -4.1853986 -4.1926985 -4.2127056 -4.2426248 -4.2725711 -4.2946405 -4.3096018 -4.317606 -4.31914 -4.3122573][-4.3045211 -4.2759805 -4.2355666 -4.1927438 -4.1572194 -4.139854 -4.1460843 -4.1710091 -4.2056293 -4.2381196 -4.2610431 -4.2771912 -4.2875886 -4.2931108 -4.290482][-4.3099394 -4.2769165 -4.2280946 -4.1717706 -4.1213984 -4.0919304 -4.0952835 -4.127018 -4.1690512 -4.2049074 -4.2287707 -4.2446842 -4.2549729 -4.26309 -4.2644014][-4.3028345 -4.270227 -4.2188234 -4.1569986 -4.1008034 -4.0666995 -4.0690269 -4.1058378 -4.152741 -4.1889849 -4.2117286 -4.2266965 -4.235703 -4.2447033 -4.24796][-4.2847795 -4.2596273 -4.2164221 -4.163455 -4.1168585 -4.0873671 -4.086762 -4.1189489 -4.163775 -4.1962838 -4.2155266 -4.227921 -4.2348266 -4.2423434 -4.2459078][-4.2627978 -4.247118 -4.2161188 -4.1775894 -4.1453958 -4.1253238 -4.1236115 -4.1448536 -4.1792407 -4.2047582 -4.2204084 -4.2325907 -4.2412853 -4.2486973 -4.2512722][-4.2387905 -4.2306781 -4.2116957 -4.189136 -4.1701927 -4.1562834 -4.1532741 -4.161664 -4.1804352 -4.1968856 -4.2110143 -4.2258515 -4.2401867 -4.2502437 -4.2534876][-4.2248082 -4.2195439 -4.20744 -4.1965437 -4.1862555 -4.173737 -4.1665287 -4.1629419 -4.1651807 -4.1712275 -4.1839571 -4.2037892 -4.2250452 -4.2408733 -4.2492576][-4.2326608 -4.2285542 -4.2184753 -4.2101541 -4.200428 -4.1837773 -4.1699524 -4.1549096 -4.1445489 -4.1441135 -4.1566505 -4.1811323 -4.2092466 -4.2312813 -4.2459326][-4.2449026 -4.242547 -4.2340937 -4.226635 -4.2166452 -4.1977062 -4.1784272 -4.156703 -4.1392221 -4.134068 -4.145041 -4.1706591 -4.2010827 -4.2254272 -4.2440114][-4.2477455 -4.24544 -4.2395921 -4.2351055 -4.2304783 -4.2186046 -4.2046466 -4.186142 -4.1702042 -4.1625261 -4.1651282 -4.1814036 -4.2054691 -4.2247829 -4.2391019]]...]
INFO - root - 2017-12-07 13:57:33.874579: step 16610, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 59h:26m:48s remains)
INFO - root - 2017-12-07 13:57:40.530077: step 16620, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 62h:21m:06s remains)
INFO - root - 2017-12-07 13:57:47.244236: step 16630, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 58h:14m:33s remains)
INFO - root - 2017-12-07 13:57:54.029377: step 16640, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 61h:04m:51s remains)
INFO - root - 2017-12-07 13:58:00.781177: step 16650, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 59h:42m:45s remains)
INFO - root - 2017-12-07 13:58:07.511622: step 16660, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 60h:20m:52s remains)
INFO - root - 2017-12-07 13:58:14.076232: step 16670, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 55h:27m:22s remains)
INFO - root - 2017-12-07 13:58:20.922699: step 16680, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 56h:43m:35s remains)
INFO - root - 2017-12-07 13:58:27.660319: step 16690, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 60h:04m:33s remains)
INFO - root - 2017-12-07 13:58:34.433026: step 16700, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 60h:41m:22s remains)
2017-12-07 13:58:35.171202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.354228 -4.3497043 -4.3510895 -4.3539166 -4.3542047 -4.35175 -4.3481035 -4.3460565 -4.3459387 -4.3430424 -4.3364177 -4.3275785 -4.3126178 -4.2904835 -4.2745924][-4.3506107 -4.3423715 -4.3417993 -4.34528 -4.3483829 -4.3483253 -4.345294 -4.3431997 -4.3422995 -4.3389039 -4.3324156 -4.3240409 -4.3112769 -4.2918339 -4.274828][-4.3526754 -4.3414745 -4.3376274 -4.3393211 -4.3422222 -4.3424473 -4.3397861 -4.3378015 -4.3354259 -4.3304615 -4.3241844 -4.3175697 -4.3095994 -4.295433 -4.2782769][-4.3614149 -4.3466845 -4.336555 -4.3340664 -4.3349128 -4.3329663 -4.3285532 -4.3251395 -4.3207335 -4.3137312 -4.3088784 -4.3052154 -4.3030276 -4.2958183 -4.2823529][-4.3651609 -4.3455238 -4.3267221 -4.3154488 -4.3083067 -4.2974858 -4.2836628 -4.2713079 -4.2640209 -4.2606521 -4.259634 -4.262651 -4.2717543 -4.2752361 -4.2694836][-4.3519893 -4.3212266 -4.2901182 -4.2668724 -4.2448583 -4.2164917 -4.1806841 -4.1427383 -4.1311145 -4.1422868 -4.1540093 -4.1698885 -4.1945882 -4.2122684 -4.2208648][-4.3105812 -4.2647347 -4.2239962 -4.1915975 -4.1550331 -4.1003003 -4.0215669 -3.9383898 -3.9347003 -3.9856477 -4.0271583 -4.0675812 -4.1083751 -4.1356592 -4.1575747][-4.2675781 -4.2145042 -4.1743336 -4.1447144 -4.1015325 -4.0237455 -3.9038756 -3.783911 -3.8039668 -3.9023438 -3.9720521 -4.0275106 -4.0680676 -4.0892625 -4.1105738][-4.249918 -4.2037177 -4.1724319 -4.1513162 -4.1218042 -4.0575829 -3.9484143 -3.8486075 -3.8782868 -3.9651532 -4.0193453 -4.0547895 -4.0683336 -4.0672588 -4.0767193][-4.2401266 -4.2007146 -4.176815 -4.164722 -4.1568117 -4.1209726 -4.0516315 -3.9916105 -4.0141912 -4.0648389 -4.0895386 -4.0988483 -4.0907812 -4.0765281 -4.0767565][-4.2364154 -4.202487 -4.1840253 -4.1784687 -4.1853542 -4.1686287 -4.1292028 -4.0955415 -4.107688 -4.1340418 -4.1445732 -4.145545 -4.1369281 -4.1265769 -4.1299133][-4.2483578 -4.2190785 -4.207077 -4.2056837 -4.2153411 -4.2085519 -4.1885285 -4.1715684 -4.1796374 -4.19576 -4.201993 -4.2026553 -4.1993628 -4.1958055 -4.2033372][-4.2801285 -4.2594919 -4.25305 -4.2542677 -4.2628679 -4.2625351 -4.2558308 -4.2506375 -4.25762 -4.267139 -4.27055 -4.2709084 -4.2696657 -4.2681589 -4.2735968][-4.3156633 -4.3028774 -4.3008566 -4.3038778 -4.3118668 -4.3165145 -4.3190031 -4.320612 -4.3265653 -4.3332815 -4.3356729 -4.3353176 -4.3331676 -4.3301983 -4.331543][-4.3440971 -4.3376837 -4.3390441 -4.3437619 -4.3523636 -4.3591442 -4.3641028 -4.3659024 -4.3691916 -4.3730979 -4.375226 -4.3762851 -4.37486 -4.3722939 -4.371469]]...]
INFO - root - 2017-12-07 13:58:41.867719: step 16710, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 56h:17m:48s remains)
INFO - root - 2017-12-07 13:58:48.553681: step 16720, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.699 sec/batch; 61h:16m:30s remains)
INFO - root - 2017-12-07 13:58:55.371400: step 16730, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 59h:25m:54s remains)
INFO - root - 2017-12-07 13:59:02.210088: step 16740, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 61h:34m:22s remains)
INFO - root - 2017-12-07 13:59:08.997535: step 16750, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 58h:05m:25s remains)
INFO - root - 2017-12-07 13:59:15.776146: step 16760, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 58h:45m:18s remains)
INFO - root - 2017-12-07 13:59:22.605151: step 16770, loss = 2.09, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 58h:56m:24s remains)
INFO - root - 2017-12-07 13:59:29.482647: step 16780, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 59h:02m:33s remains)
INFO - root - 2017-12-07 13:59:36.322995: step 16790, loss = 2.10, batch loss = 2.04 (13.0 examples/sec; 0.613 sec/batch; 53h:47m:44s remains)
INFO - root - 2017-12-07 13:59:43.225542: step 16800, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 58h:27m:43s remains)
2017-12-07 13:59:43.913685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2916374 -4.2774711 -4.2657962 -4.2592816 -4.249342 -4.2329078 -4.2111354 -4.1957784 -4.1935048 -4.2045527 -4.2184119 -4.2229748 -4.2293463 -4.2393804 -4.2434778][-4.29276 -4.2773256 -4.2649703 -4.257587 -4.2446632 -4.2179022 -4.1848149 -4.1648602 -4.170547 -4.1960535 -4.2216711 -4.2260218 -4.2283015 -4.2326584 -4.2334666][-4.2895527 -4.2700281 -4.2546067 -4.2510896 -4.24449 -4.2140656 -4.1740494 -4.152339 -4.1678352 -4.2055588 -4.2338843 -4.23716 -4.2347145 -4.2326269 -4.2270141][-4.2732763 -4.2462358 -4.2285957 -4.2310224 -4.2359295 -4.21046 -4.1689157 -4.1461091 -4.1696424 -4.2158942 -4.2398987 -4.2392316 -4.2354851 -4.232338 -4.224894][-4.2453027 -4.21243 -4.19265 -4.1998153 -4.2123728 -4.1892872 -4.1429639 -4.1172209 -4.146698 -4.1995096 -4.221343 -4.2143664 -4.2089691 -4.2062635 -4.20336][-4.209877 -4.1687417 -4.1437478 -4.1493196 -4.1617675 -4.1300259 -4.0603323 -4.0141139 -4.0484819 -4.1240091 -4.1614442 -4.1605239 -4.1623349 -4.1632385 -4.165904][-4.1831827 -4.1348042 -4.1018891 -4.1005354 -4.1034155 -4.0541954 -3.9479008 -3.8625693 -3.9051645 -4.0222664 -4.0898757 -4.1086683 -4.1218219 -4.1298566 -4.1380949][-4.1747289 -4.124629 -4.0881648 -4.0831437 -4.0777435 -4.0150208 -3.8892736 -3.7805943 -3.8355286 -3.9803958 -4.0713525 -4.0992408 -4.1128798 -4.1261063 -4.1398458][-4.1954093 -4.1550093 -4.1304555 -4.1299086 -4.1222777 -4.0669384 -3.9683025 -3.8956587 -3.9495387 -4.0615654 -4.1277895 -4.1449351 -4.1495566 -4.1585894 -4.1728067][-4.2260842 -4.1962543 -4.184875 -4.1906538 -4.1895514 -4.154068 -4.0944366 -4.059639 -4.0961518 -4.1633229 -4.2035708 -4.2103205 -4.2062964 -4.21015 -4.2186556][-4.2553072 -4.2359924 -4.235013 -4.2449331 -4.2471504 -4.2230315 -4.1867723 -4.1709719 -4.1953187 -4.2378664 -4.2618074 -4.2650347 -4.2610092 -4.2627378 -4.2662148][-4.2826362 -4.2732072 -4.2763405 -4.2849684 -4.2850285 -4.2642 -4.2392664 -4.2261958 -4.2388229 -4.2673936 -4.2878866 -4.2943735 -4.29181 -4.2912931 -4.2934451][-4.2962551 -4.293911 -4.2987289 -4.3068781 -4.3071465 -4.2907109 -4.2696161 -4.2543316 -4.259232 -4.2805333 -4.2997885 -4.30871 -4.3054128 -4.302464 -4.30366][-4.294908 -4.2935081 -4.2988448 -4.3066683 -4.3082891 -4.2972341 -4.2800121 -4.2666144 -4.268681 -4.2841258 -4.3020706 -4.3116713 -4.3120403 -4.309494 -4.3082843][-4.2945204 -4.2909341 -4.2920275 -4.2961631 -4.2980685 -4.2916274 -4.2805548 -4.2738042 -4.2773786 -4.2882271 -4.2997589 -4.3065243 -4.3099866 -4.3097558 -4.3087821]]...]
INFO - root - 2017-12-07 13:59:50.576123: step 16810, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 56h:36m:26s remains)
INFO - root - 2017-12-07 13:59:57.392366: step 16820, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 62h:55m:06s remains)
INFO - root - 2017-12-07 14:00:04.285581: step 16830, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 57h:16m:48s remains)
INFO - root - 2017-12-07 14:00:11.136979: step 16840, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 62h:37m:36s remains)
INFO - root - 2017-12-07 14:00:18.016714: step 16850, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 63h:22m:14s remains)
INFO - root - 2017-12-07 14:00:24.747929: step 16860, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 58h:16m:38s remains)
INFO - root - 2017-12-07 14:00:31.481269: step 16870, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 58h:24m:32s remains)
INFO - root - 2017-12-07 14:00:38.315687: step 16880, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.696 sec/batch; 60h:59m:24s remains)
INFO - root - 2017-12-07 14:00:45.088112: step 16890, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 61h:51m:12s remains)
INFO - root - 2017-12-07 14:00:51.852265: step 16900, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 59h:46m:18s remains)
2017-12-07 14:00:52.681700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1182294 -4.1119103 -4.1401629 -4.1747756 -4.1921988 -4.1878729 -4.1694932 -4.1521931 -4.1265798 -4.076056 -4.0255613 -4.0044012 -4.0315118 -4.0841503 -4.13174][-4.1079774 -4.1075335 -4.1377416 -4.1695695 -4.1874666 -4.1847329 -4.1650958 -4.1454782 -4.1154275 -4.0531073 -3.9986746 -3.9791555 -4.0167966 -4.0843935 -4.1441488][-4.1026521 -4.108016 -4.1369591 -4.1619453 -4.1758947 -4.177897 -4.1623721 -4.1487203 -4.1266937 -4.0755887 -4.024899 -4.0001435 -4.0357676 -4.1015697 -4.159564][-4.1090231 -4.1168656 -4.1394587 -4.1577039 -4.1700697 -4.1758914 -4.1643109 -4.1535721 -4.1512618 -4.130249 -4.0942717 -4.0667219 -4.0795393 -4.1264353 -4.178278][-4.0992203 -4.1055651 -4.122498 -4.1400719 -4.1534162 -4.1575661 -4.1399751 -4.1225133 -4.1319075 -4.1457343 -4.1401954 -4.1215196 -4.1220732 -4.1505747 -4.186367][-4.0785408 -4.0807886 -4.1001291 -4.1212296 -4.1328559 -4.1239285 -4.0743132 -4.018013 -4.0217233 -4.0793824 -4.1228118 -4.1303029 -4.1343575 -4.1506615 -4.1654387][-4.0697122 -4.0635977 -4.0851507 -4.1058292 -4.1082973 -4.0758548 -3.9776084 -3.8394647 -3.8049645 -3.9222121 -4.0397053 -4.0944881 -4.1123371 -4.1198859 -4.1179848][-4.0912542 -4.0778189 -4.095119 -4.1164036 -4.1111732 -4.0672693 -3.9476542 -3.7564073 -3.6665771 -3.7959428 -3.9647985 -4.062675 -4.0962877 -4.1051741 -4.099247][-4.1392651 -4.1236577 -4.1424451 -4.1665993 -4.1627989 -4.1299419 -4.0421267 -3.8975081 -3.8103662 -3.8720562 -3.9963875 -4.084269 -4.1181803 -4.1269269 -4.1183591][-4.1905427 -4.1758428 -4.1984615 -4.2247663 -4.2284646 -4.21276 -4.159657 -4.0728397 -4.0228968 -4.043972 -4.1025968 -4.150353 -4.1653414 -4.1583481 -4.1388106][-4.2199044 -4.2100348 -4.2321239 -4.256547 -4.2667513 -4.2609191 -4.2302666 -4.1811967 -4.1621361 -4.1717124 -4.1934347 -4.2065053 -4.2016196 -4.1798687 -4.1574607][-4.2208323 -4.217505 -4.2350054 -4.2508388 -4.2559452 -4.25205 -4.2366233 -4.2145753 -4.2141089 -4.22602 -4.231606 -4.2254419 -4.2176013 -4.2014766 -4.1822014][-4.2125735 -4.2120423 -4.2192822 -4.222466 -4.2172036 -4.2081409 -4.1982479 -4.189754 -4.20098 -4.2194409 -4.2215977 -4.2139788 -4.2125292 -4.2112708 -4.2091846][-4.230597 -4.225059 -4.2168803 -4.2017488 -4.1825104 -4.1666956 -4.1545072 -4.1489105 -4.1599054 -4.1791878 -4.1824841 -4.1808114 -4.1895251 -4.1994767 -4.2064452][-4.2756696 -4.2658463 -4.2470794 -4.2179856 -4.1862588 -4.1625552 -4.1457705 -4.1336536 -4.1329679 -4.1418877 -4.1411419 -4.1390729 -4.1509466 -4.1674843 -4.1788483]]...]
INFO - root - 2017-12-07 14:00:59.458619: step 16910, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 61h:48m:39s remains)
INFO - root - 2017-12-07 14:01:06.088070: step 16920, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 61h:37m:47s remains)
INFO - root - 2017-12-07 14:01:12.851756: step 16930, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 58h:59m:00s remains)
INFO - root - 2017-12-07 14:01:19.491803: step 16940, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 58h:12m:51s remains)
INFO - root - 2017-12-07 14:01:26.292620: step 16950, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 58h:53m:28s remains)
INFO - root - 2017-12-07 14:01:33.108339: step 16960, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 61h:01m:47s remains)
INFO - root - 2017-12-07 14:01:39.894789: step 16970, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 63h:35m:48s remains)
INFO - root - 2017-12-07 14:01:46.421560: step 16980, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 56h:22m:41s remains)
INFO - root - 2017-12-07 14:01:53.261955: step 16990, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 60h:43m:05s remains)
INFO - root - 2017-12-07 14:02:00.166041: step 17000, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 63h:59m:42s remains)
2017-12-07 14:02:00.801742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2801452 -4.271276 -4.2615094 -4.2451038 -4.2211976 -4.2041025 -4.2038603 -4.2226295 -4.2429562 -4.2501721 -4.2450194 -4.2488575 -4.2597289 -4.26022 -4.2506409][-4.2713623 -4.2561393 -4.2374358 -4.2067947 -4.166028 -4.1359115 -4.13643 -4.1582117 -4.1834273 -4.1988311 -4.202837 -4.2210393 -4.2445889 -4.2478013 -4.2384248][-4.2625656 -4.2400689 -4.2079048 -4.1600485 -4.0947757 -4.0467696 -4.0486484 -4.0780497 -4.1089511 -4.1332316 -4.1572785 -4.1970677 -4.237762 -4.2482204 -4.2376304][-4.2535424 -4.22554 -4.1789951 -4.1094823 -4.0171885 -3.9531074 -3.9580288 -4.0017934 -4.0426183 -4.0809207 -4.1269684 -4.186636 -4.240674 -4.2613926 -4.2570524][-4.2497559 -4.2166476 -4.1611257 -4.0780177 -3.9721944 -3.9000134 -3.9047012 -3.9617436 -4.018321 -4.0709734 -4.1292129 -4.1910667 -4.2450004 -4.2729807 -4.27727][-4.2516923 -4.215632 -4.1569219 -4.0761557 -3.9762325 -3.8982863 -3.8770437 -3.9292212 -4.0106869 -4.083776 -4.1471157 -4.2013755 -4.2398829 -4.2597046 -4.2684288][-4.2591209 -4.2243581 -4.1667714 -4.089642 -3.9923873 -3.8862827 -3.8068581 -3.8425307 -3.9701123 -4.080544 -4.1473074 -4.1844349 -4.1978073 -4.2015033 -4.2119226][-4.2653403 -4.2329493 -4.1788335 -4.101491 -3.9940135 -3.8512816 -3.7192841 -3.7441349 -3.9167433 -4.0535078 -4.1152959 -4.1285172 -4.1151237 -4.1000676 -4.1189218][-4.2740016 -4.2447834 -4.1956115 -4.1214862 -4.0166311 -3.8793724 -3.7633753 -3.7937193 -3.9479678 -4.0574117 -4.0861859 -4.063632 -4.0235729 -4.0005178 -4.0408783][-4.2830338 -4.2577538 -4.2155395 -4.1579137 -4.0786295 -3.9837112 -3.9216576 -3.9564383 -4.0540738 -4.1107521 -4.1008887 -4.0441937 -3.9901819 -3.9795191 -4.0392251][-4.2880182 -4.2692337 -4.237524 -4.2014432 -4.1538596 -4.100441 -4.0780807 -4.1088543 -4.1595321 -4.178545 -4.146884 -4.0822172 -4.0354238 -4.0452261 -4.1070938][-4.288681 -4.2752285 -4.2522326 -4.2331605 -4.2122784 -4.1900387 -4.1895175 -4.2144618 -4.23841 -4.2370238 -4.2029829 -4.152401 -4.12702 -4.1494646 -4.199244][-4.2941227 -4.2839756 -4.2667708 -4.2588067 -4.2569256 -4.2560897 -4.2669582 -4.2869091 -4.2980881 -4.2914863 -4.26477 -4.231091 -4.2196746 -4.24031 -4.2732573][-4.3067946 -4.2988896 -4.2877493 -4.28871 -4.2982569 -4.305274 -4.3144817 -4.3281817 -4.3338189 -4.3266425 -4.3066163 -4.2853646 -4.2810884 -4.2952094 -4.3170309][-4.3177834 -4.3130779 -4.30755 -4.3129439 -4.3253551 -4.3338122 -4.3407254 -4.3489814 -4.3504763 -4.3428574 -4.3286991 -4.3153219 -4.3136883 -4.3229113 -4.3361492]]...]
INFO - root - 2017-12-07 14:02:07.590435: step 17010, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 58h:15m:20s remains)
INFO - root - 2017-12-07 14:02:14.478884: step 17020, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 64h:14m:14s remains)
INFO - root - 2017-12-07 14:02:21.330459: step 17030, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 61h:57m:41s remains)
INFO - root - 2017-12-07 14:02:28.081064: step 17040, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 60h:28m:05s remains)
INFO - root - 2017-12-07 14:02:34.863921: step 17050, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.637 sec/batch; 55h:51m:38s remains)
INFO - root - 2017-12-07 14:02:41.694292: step 17060, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 61h:20m:02s remains)
INFO - root - 2017-12-07 14:02:48.503755: step 17070, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 60h:26m:33s remains)
INFO - root - 2017-12-07 14:02:55.297019: step 17080, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 60h:07m:59s remains)
INFO - root - 2017-12-07 14:03:02.159934: step 17090, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 59h:47m:41s remains)
INFO - root - 2017-12-07 14:03:08.869632: step 17100, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 54h:53m:40s remains)
2017-12-07 14:03:09.594296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2415977 -4.2436609 -4.2496052 -4.2537622 -4.2590051 -4.2640328 -4.2695122 -4.2794023 -4.2873964 -4.2860422 -4.2780671 -4.2743907 -4.27732 -4.2847981 -4.288888][-4.2311645 -4.2301588 -4.2268472 -4.2192497 -4.2166686 -4.2178183 -4.2259469 -4.24726 -4.2687988 -4.2775526 -4.2766809 -4.2763915 -4.2766252 -4.282392 -4.2850752][-4.2175946 -4.2170053 -4.205761 -4.1808176 -4.1595511 -4.1413507 -4.14171 -4.1751418 -4.2165561 -4.2415223 -4.2534122 -4.262692 -4.2642021 -4.2640867 -4.2619572][-4.2187858 -4.2201133 -4.2030182 -4.1657796 -4.1158452 -4.0647173 -4.0436077 -4.0819821 -4.1457148 -4.190412 -4.2186828 -4.2349138 -4.2371664 -4.2306037 -4.2193866][-4.2179775 -4.2213821 -4.2032375 -4.1552262 -4.0732317 -3.9795527 -3.9249051 -3.9688299 -4.0624743 -4.1301365 -4.17176 -4.1887145 -4.1843777 -4.1673822 -4.1566334][-4.2106676 -4.2136788 -4.1932354 -4.1397934 -4.0397539 -3.9143424 -3.821841 -3.8645504 -3.991132 -4.0868979 -4.1387873 -4.1537695 -4.141211 -4.1202869 -4.1162844][-4.196692 -4.1986403 -4.1782937 -4.1258054 -4.0308676 -3.9051783 -3.7880204 -3.8122211 -3.9582794 -4.0752168 -4.1357045 -4.150291 -4.1351585 -4.1119552 -4.1115847][-4.1987166 -4.201611 -4.1886129 -4.1462245 -4.0778232 -3.9835856 -3.8756042 -3.8635411 -3.9903336 -4.1038823 -4.1589217 -4.1630397 -4.1406474 -4.1130285 -4.109098][-4.2098322 -4.2148809 -4.2145872 -4.189559 -4.1534462 -4.1045275 -4.0378866 -4.0085235 -4.0782108 -4.156414 -4.19307 -4.1821342 -4.1485815 -4.1135263 -4.1012859][-4.2216587 -4.2225542 -4.2254138 -4.219759 -4.2065315 -4.1883645 -4.1581068 -4.1335464 -4.1653895 -4.2100253 -4.2269058 -4.2017016 -4.1575446 -4.1158028 -4.0937018][-4.2180409 -4.2167339 -4.2179828 -4.2180891 -4.2187138 -4.2188053 -4.2122951 -4.20076 -4.2193375 -4.2463508 -4.2552567 -4.2297778 -4.1805725 -4.137639 -4.1107826][-4.2167492 -4.2169738 -4.2164583 -4.2189474 -4.2249579 -4.2345147 -4.2395926 -4.2360992 -4.2457805 -4.2614436 -4.2704368 -4.2554812 -4.2170048 -4.1853948 -4.1632962][-4.21367 -4.2240467 -4.2341518 -4.241735 -4.2465367 -4.2563334 -4.2619033 -4.2595153 -4.2625375 -4.2720733 -4.2806664 -4.2733221 -4.2491126 -4.2264905 -4.2110467][-4.2120104 -4.2294555 -4.2493539 -4.2604313 -4.2628074 -4.2691441 -4.2742567 -4.270597 -4.2690468 -4.2763772 -4.2842712 -4.2782025 -4.2576637 -4.2430925 -4.2349939][-4.24063 -4.2536659 -4.2701545 -4.278152 -4.2768807 -4.2803655 -4.2846618 -4.2831359 -4.2839904 -4.2918162 -4.2972403 -4.2912736 -4.2745214 -4.2677054 -4.2653484]]...]
INFO - root - 2017-12-07 14:03:16.370771: step 17110, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 63h:21m:53s remains)
INFO - root - 2017-12-07 14:03:23.029832: step 17120, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 56h:41m:35s remains)
INFO - root - 2017-12-07 14:03:29.867225: step 17130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 56h:19m:40s remains)
INFO - root - 2017-12-07 14:03:36.753803: step 17140, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 61h:03m:59s remains)
INFO - root - 2017-12-07 14:03:43.641996: step 17150, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.746 sec/batch; 65h:22m:11s remains)
INFO - root - 2017-12-07 14:03:50.518731: step 17160, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 60h:34m:09s remains)
INFO - root - 2017-12-07 14:03:57.233203: step 17170, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 56h:29m:26s remains)
INFO - root - 2017-12-07 14:04:04.092696: step 17180, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 59h:14m:15s remains)
INFO - root - 2017-12-07 14:04:11.016184: step 17190, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.737 sec/batch; 64h:32m:21s remains)
INFO - root - 2017-12-07 14:04:17.848732: step 17200, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 59h:29m:44s remains)
2017-12-07 14:04:18.555136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2212844 -4.2241015 -4.2237616 -4.2171464 -4.202951 -4.1841836 -4.1688132 -4.1685166 -4.1853814 -4.2115722 -4.235137 -4.2471786 -4.2532845 -4.2568345 -4.2580576][-4.2245355 -4.2266536 -4.2250767 -4.2145038 -4.1923537 -4.1628642 -4.1350965 -4.1273718 -4.1481566 -4.1882019 -4.2278552 -4.2513366 -4.2622452 -4.2663159 -4.2653885][-4.2241211 -4.2255216 -4.2239623 -4.2119069 -4.1840863 -4.1446209 -4.102911 -4.0825148 -4.1014538 -4.1529837 -4.2088714 -4.2450848 -4.2624 -4.26871 -4.2677617][-4.2231317 -4.2238441 -4.2234974 -4.2126546 -4.1820817 -4.1357651 -4.0827179 -4.0468988 -4.05762 -4.1157279 -4.1862397 -4.2348185 -4.2587733 -4.2679811 -4.2680588][-4.2216763 -4.2216592 -4.2219958 -4.2120671 -4.180562 -4.1320057 -4.0732675 -4.0249228 -4.0245962 -4.0850635 -4.1663828 -4.2258172 -4.2560387 -4.2674079 -4.2680025][-4.2191415 -4.2186937 -4.218648 -4.2091665 -4.179328 -4.1334548 -4.0763049 -4.0227551 -4.0119352 -4.0687823 -4.1546707 -4.2218232 -4.2570567 -4.2690492 -4.2687917][-4.2257729 -4.2229013 -4.2190976 -4.2080088 -4.1802578 -4.1391134 -4.0880055 -4.0368662 -4.018723 -4.0658298 -4.148787 -4.2195692 -4.2588406 -4.27149 -4.2702165][-4.2380042 -4.2328854 -4.224174 -4.2105551 -4.1847305 -4.1477284 -4.1017962 -4.0565538 -4.0359406 -4.0710897 -4.1444445 -4.2141871 -4.25668 -4.2717957 -4.2711773][-4.2479196 -4.2426524 -4.2311721 -4.2164569 -4.1945186 -4.1632447 -4.1210647 -4.0800319 -4.0587263 -4.0814061 -4.1393757 -4.2023706 -4.2471957 -4.2675014 -4.2697878][-4.2494197 -4.2465467 -4.2362018 -4.224329 -4.2090774 -4.1858892 -4.1484594 -4.1088605 -4.0829997 -4.0912523 -4.1316867 -4.1842461 -4.2300096 -4.257185 -4.264833][-4.2410116 -4.2417288 -4.2356753 -4.2303171 -4.2242217 -4.2114873 -4.1821957 -4.143446 -4.1104774 -4.1033936 -4.1273 -4.1690912 -4.2132106 -4.2450304 -4.25813][-4.2254138 -4.2298279 -4.2290487 -4.2305851 -4.2332754 -4.2315702 -4.2137389 -4.18017 -4.1440773 -4.1256461 -4.1354609 -4.1652908 -4.2031875 -4.23491 -4.2518167][-4.2074242 -4.2151804 -4.2188778 -4.2248611 -4.23302 -4.2400475 -4.23515 -4.2124009 -4.18125 -4.1598077 -4.1602664 -4.17784 -4.2053447 -4.2316823 -4.2485361][-4.1986523 -4.2077961 -4.2128086 -4.2190952 -4.227797 -4.2383943 -4.2436013 -4.2326708 -4.2114887 -4.1944418 -4.1915574 -4.2001967 -4.2168484 -4.236165 -4.2493911][-4.2050962 -4.211709 -4.2138863 -4.21712 -4.2230821 -4.2319388 -4.2411022 -4.2392716 -4.2284803 -4.2191229 -4.2182956 -4.2232785 -4.2329569 -4.24555 -4.2540045]]...]
INFO - root - 2017-12-07 14:04:25.340685: step 17210, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 61h:07m:50s remains)
INFO - root - 2017-12-07 14:04:32.034200: step 17220, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 60h:38m:15s remains)
INFO - root - 2017-12-07 14:04:38.964794: step 17230, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 58h:43m:17s remains)
INFO - root - 2017-12-07 14:04:45.784627: step 17240, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 57h:22m:05s remains)
INFO - root - 2017-12-07 14:04:52.523080: step 17250, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 59h:33m:49s remains)
INFO - root - 2017-12-07 14:04:59.388833: step 17260, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 62h:47m:48s remains)
INFO - root - 2017-12-07 14:05:06.202110: step 17270, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 58h:54m:08s remains)
INFO - root - 2017-12-07 14:05:12.928282: step 17280, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 55h:21m:23s remains)
INFO - root - 2017-12-07 14:05:19.511513: step 17290, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 56h:48m:45s remains)
INFO - root - 2017-12-07 14:05:26.271945: step 17300, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 60h:35m:49s remains)
2017-12-07 14:05:26.964416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3242311 -4.3189359 -4.3132706 -4.3099432 -4.3076644 -4.3065276 -4.30688 -4.3082652 -4.3126879 -4.3233008 -4.3352232 -4.3476396 -4.3581581 -4.3637462 -4.3635869][-4.3074708 -4.2999611 -4.2938557 -4.2931275 -4.2951722 -4.2961559 -4.2956405 -4.2939539 -4.2962575 -4.3066077 -4.3186851 -4.3326359 -4.3477111 -4.3568487 -4.3578277][-4.296948 -4.2884183 -4.2820492 -4.2805405 -4.282311 -4.2806263 -4.27382 -4.2633414 -4.2618918 -4.274334 -4.2865176 -4.3028269 -4.325006 -4.3394585 -4.3431258][-4.285542 -4.2721739 -4.258934 -4.250432 -4.2493138 -4.2422109 -4.2246461 -4.202198 -4.2006073 -4.2227435 -4.2395864 -4.2607265 -4.2922568 -4.313983 -4.3222036][-4.2623491 -4.2367358 -4.2091441 -4.1854472 -4.1746182 -4.1625943 -4.1398849 -4.1078844 -4.1127977 -4.1537685 -4.1815476 -4.2092886 -4.2507305 -4.2811666 -4.2972188][-4.2253475 -4.1826844 -4.132277 -4.0827122 -4.0473413 -4.0257483 -4.0040402 -3.9719214 -3.9949379 -4.0684228 -4.1155372 -4.1527357 -4.2024717 -4.2428389 -4.2688494][-4.1755486 -4.1179304 -4.0430913 -3.9629264 -3.8924632 -3.8538337 -3.8357291 -3.8121183 -3.8613062 -3.9762983 -4.0519853 -4.1033516 -4.1604118 -4.20938 -4.2444348][-4.1494265 -4.0840559 -4.0005155 -3.9065056 -3.8186917 -3.7697358 -3.7547545 -3.7362728 -3.7977681 -3.9332361 -4.0293469 -4.0906696 -4.1506405 -4.2006536 -4.2386923][-4.1769853 -4.1230884 -4.0561886 -3.9840574 -3.9149516 -3.8762181 -3.8563235 -3.8291326 -3.871305 -3.9849615 -4.0723252 -4.1278791 -4.1802607 -4.2232943 -4.2558022][-4.2281241 -4.1959519 -4.1545444 -4.114584 -4.0736852 -4.0477114 -4.0246973 -3.9891045 -4.0043645 -4.0777454 -4.14221 -4.1870861 -4.2300463 -4.2644753 -4.2871284][-4.2734256 -4.2569132 -4.2368708 -4.2219472 -4.2011333 -4.1867933 -4.1689043 -4.1390209 -4.1411457 -4.1816292 -4.2196894 -4.249238 -4.2813878 -4.3071856 -4.3192453][-4.2953687 -4.2874417 -4.2789168 -4.2770638 -4.2714696 -4.2695503 -4.2642145 -4.2470479 -4.2448168 -4.2631712 -4.2800593 -4.296205 -4.3184881 -4.3360434 -4.3403025][-4.3006287 -4.2992139 -4.2974849 -4.2996721 -4.30053 -4.3054085 -4.3068342 -4.3009043 -4.2988186 -4.3066659 -4.3145823 -4.324769 -4.339241 -4.3508029 -4.3519678][-4.3086948 -4.3098912 -4.3117747 -4.3152294 -4.31736 -4.3223796 -4.32423 -4.3217292 -4.3198848 -4.3233719 -4.3289495 -4.3367815 -4.34723 -4.3555861 -4.3573442][-4.3252664 -4.3276367 -4.3315792 -4.3354049 -4.3371339 -4.3388119 -4.3386469 -4.3363991 -4.3344712 -4.3355465 -4.3388171 -4.3439245 -4.3508725 -4.3569407 -4.3594613]]...]
INFO - root - 2017-12-07 14:05:33.575998: step 17310, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 57h:02m:15s remains)
INFO - root - 2017-12-07 14:05:40.329888: step 17320, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 59h:28m:45s remains)
INFO - root - 2017-12-07 14:05:47.301545: step 17330, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.731 sec/batch; 64h:01m:54s remains)
INFO - root - 2017-12-07 14:05:54.049231: step 17340, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 58h:57m:48s remains)
INFO - root - 2017-12-07 14:06:00.940575: step 17350, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 59h:46m:29s remains)
INFO - root - 2017-12-07 14:06:07.776213: step 17360, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 56h:48m:30s remains)
INFO - root - 2017-12-07 14:06:14.645952: step 17370, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 60h:16m:00s remains)
INFO - root - 2017-12-07 14:06:21.481474: step 17380, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 62h:01m:52s remains)
INFO - root - 2017-12-07 14:06:28.329719: step 17390, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 59h:01m:48s remains)
INFO - root - 2017-12-07 14:06:35.070055: step 17400, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 56h:43m:41s remains)
2017-12-07 14:06:35.792096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1836805 -4.17656 -4.1654778 -4.1603451 -4.1731443 -4.2005925 -4.2271695 -4.2430496 -4.2647529 -4.2808208 -4.2815337 -4.2758651 -4.2731924 -4.2737808 -4.2779369][-4.1981034 -4.1910524 -4.1837325 -4.1807923 -4.1919107 -4.2151332 -4.2407455 -4.2556443 -4.2697945 -4.281044 -4.2842069 -4.2842069 -4.2845259 -4.2881126 -4.2957292][-4.24403 -4.2359476 -4.2295752 -4.2239227 -4.2274346 -4.2380362 -4.2517033 -4.2602816 -4.2671957 -4.2746339 -4.2789259 -4.2807965 -4.2826219 -4.2887235 -4.2988458][-4.2970424 -4.2851248 -4.276897 -4.2679777 -4.2560339 -4.2454662 -4.241827 -4.2437072 -4.2481389 -4.2577572 -4.2669554 -4.2678251 -4.2645197 -4.2681069 -4.2735314][-4.3138676 -4.2992892 -4.292944 -4.2786083 -4.2476153 -4.2135825 -4.1882491 -4.185298 -4.2048178 -4.2344928 -4.252193 -4.2531195 -4.2416387 -4.2333431 -4.2287779][-4.2951694 -4.278194 -4.2701902 -4.2450705 -4.1903696 -4.1189413 -4.0625539 -4.0598235 -4.1172104 -4.1855054 -4.2199631 -4.2236214 -4.202291 -4.1806855 -4.172255][-4.2542396 -4.2348027 -4.2203317 -4.180635 -4.097105 -3.9784658 -3.8783493 -3.8936081 -4.0155096 -4.1231155 -4.1675706 -4.1688404 -4.1351857 -4.1001492 -4.1050677][-4.2130179 -4.1919765 -4.1659255 -4.1062789 -3.9958594 -3.8365741 -3.6884518 -3.7438023 -3.9381354 -4.0718784 -4.1163354 -4.1101637 -4.0677834 -4.0345006 -4.0614429][-4.2088757 -4.1861753 -4.1498423 -4.0846815 -3.9781814 -3.825423 -3.6883507 -3.769263 -3.9662483 -4.0840225 -4.1121044 -4.0890474 -4.0448451 -4.0299511 -4.0772738][-4.2316418 -4.2149153 -4.1830392 -4.1319003 -4.0546422 -3.95084 -3.8747633 -3.9428227 -4.0780973 -4.1519151 -4.1544695 -4.1111727 -4.0670457 -4.0752144 -4.1319079][-4.24774 -4.2387466 -4.2205858 -4.1933265 -4.1501918 -4.0960417 -4.0675359 -4.1108403 -4.1825132 -4.2201891 -4.2055407 -4.154561 -4.1174126 -4.13111 -4.1764264][-4.2471805 -4.243772 -4.2384715 -4.2340236 -4.2196288 -4.19856 -4.1937141 -4.2163725 -4.2493825 -4.2632332 -4.2415724 -4.1975913 -4.17332 -4.1855607 -4.211309][-4.2377896 -4.2379022 -4.2414827 -4.2524328 -4.2591629 -4.2580028 -4.261426 -4.2689939 -4.2780166 -4.2803988 -4.261735 -4.2331696 -4.2230577 -4.2322149 -4.2416534][-4.22635 -4.2287631 -4.2371178 -4.2538824 -4.2669034 -4.2767506 -4.2871122 -4.2920175 -4.2926888 -4.2915335 -4.2801461 -4.2652907 -4.2643652 -4.2695293 -4.2699361][-4.2305727 -4.2350063 -4.2421513 -4.2553253 -4.2683635 -4.2838378 -4.2992139 -4.3038611 -4.2983141 -4.2962646 -4.2940221 -4.2900677 -4.2910733 -4.2935476 -4.2905149]]...]
INFO - root - 2017-12-07 14:06:42.541107: step 17410, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 60h:09m:11s remains)
INFO - root - 2017-12-07 14:06:49.207290: step 17420, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 60h:01m:20s remains)
INFO - root - 2017-12-07 14:06:56.055292: step 17430, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 57h:11m:01s remains)
INFO - root - 2017-12-07 14:07:02.905705: step 17440, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.744 sec/batch; 65h:08m:10s remains)
INFO - root - 2017-12-07 14:07:09.716136: step 17450, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 63h:14m:56s remains)
INFO - root - 2017-12-07 14:07:16.461014: step 17460, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 57h:23m:19s remains)
INFO - root - 2017-12-07 14:07:23.292760: step 17470, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 57h:33m:14s remains)
INFO - root - 2017-12-07 14:07:30.114414: step 17480, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 56h:50m:31s remains)
INFO - root - 2017-12-07 14:07:36.897364: step 17490, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 61h:55m:53s remains)
INFO - root - 2017-12-07 14:07:43.712682: step 17500, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 60h:49m:19s remains)
2017-12-07 14:07:44.426060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1795478 -4.18644 -4.1964054 -4.2022729 -4.2110004 -4.2151251 -4.2089891 -4.2050195 -4.2067881 -4.2084222 -4.2130213 -4.2181816 -4.2187481 -4.2079453 -4.199234][-4.1492209 -4.1689978 -4.1890435 -4.1966891 -4.2001882 -4.1919746 -4.1715837 -4.1550922 -4.1561651 -4.1650877 -4.175581 -4.1812825 -4.1763248 -4.1667185 -4.1694536][-4.1281223 -4.1608672 -4.19296 -4.2053175 -4.2001004 -4.1748443 -4.1363878 -4.1036253 -4.1078153 -4.1304464 -4.1483207 -4.1452346 -4.1319242 -4.120687 -4.1276755][-4.1050797 -4.1476955 -4.193687 -4.2085876 -4.1909428 -4.1434679 -4.0846686 -4.0392294 -4.0546842 -4.1032786 -4.12961 -4.1191416 -4.09443 -4.074861 -4.0794077][-4.1034703 -4.1464405 -4.1954427 -4.2088275 -4.17446 -4.1002865 -4.0051446 -3.9410019 -3.9826162 -4.0735555 -4.1208625 -4.113977 -4.083087 -4.0516295 -4.0498924][-4.12215 -4.1528692 -4.19135 -4.1966553 -4.1469121 -4.0407572 -3.8902922 -3.8017244 -3.8901663 -4.0351429 -4.1121435 -4.1187654 -4.0869908 -4.0425286 -4.0286431][-4.1431823 -4.1636658 -4.1887302 -4.1825728 -4.1164193 -3.9682138 -3.7483938 -3.6422479 -3.8018847 -4.0039096 -4.1078753 -4.1287642 -4.0967507 -4.0375476 -4.0135436][-4.1606231 -4.1765976 -4.19032 -4.1721478 -4.0904889 -3.9106412 -3.6489494 -3.5556028 -3.7786453 -4.0030479 -4.1122 -4.1336517 -4.0980768 -4.0312858 -4.009728][-4.1645775 -4.1802254 -4.1892366 -4.1647058 -4.0863953 -3.9258046 -3.7183716 -3.6789987 -3.8667138 -4.041079 -4.1294522 -4.1453 -4.1016812 -4.0294738 -4.0164852][-4.1578989 -4.173842 -4.1842732 -4.161828 -4.1023121 -3.993083 -3.880208 -3.8832111 -4.001471 -4.1039982 -4.1626987 -4.1675715 -4.11615 -4.0421824 -4.034832][-4.1426034 -4.1655087 -4.1757579 -4.153316 -4.1115785 -4.0547676 -4.0119104 -4.0310416 -4.0995078 -4.1541762 -4.1885471 -4.1783252 -4.1239719 -4.0541263 -4.0489216][-4.1212149 -4.1537838 -4.1677065 -4.1477919 -4.12041 -4.1031294 -4.1006055 -4.125349 -4.1605282 -4.1825 -4.1915245 -4.1678448 -4.1175385 -4.0602098 -4.0573277][-4.1103611 -4.1530967 -4.1683674 -4.1560578 -4.142221 -4.144062 -4.1581278 -4.1819291 -4.1938782 -4.18785 -4.1731668 -4.1489477 -4.1116095 -4.0698032 -4.0719709][-4.1294394 -4.1755815 -4.1907325 -4.1843376 -4.1798949 -4.1885862 -4.20467 -4.2193418 -4.2125983 -4.1859059 -4.1595263 -4.1460752 -4.1262631 -4.0987144 -4.1082449][-4.1798387 -4.2194786 -4.2361078 -4.235651 -4.2355738 -4.2415915 -4.2510042 -4.2555132 -4.238986 -4.2053728 -4.1830497 -4.18062 -4.1768055 -4.1603069 -4.1738687]]...]
INFO - root - 2017-12-07 14:07:51.152947: step 17510, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 58h:54m:18s remains)
INFO - root - 2017-12-07 14:07:57.782136: step 17520, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 61h:01m:35s remains)
INFO - root - 2017-12-07 14:08:04.593407: step 17530, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 61h:57m:26s remains)
INFO - root - 2017-12-07 14:08:11.420521: step 17540, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 58h:08m:16s remains)
INFO - root - 2017-12-07 14:08:18.219933: step 17550, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 57h:19m:49s remains)
INFO - root - 2017-12-07 14:08:25.153498: step 17560, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 64h:20m:24s remains)
INFO - root - 2017-12-07 14:08:31.943033: step 17570, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 63h:10m:21s remains)
INFO - root - 2017-12-07 14:08:38.617825: step 17580, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 58h:45m:18s remains)
INFO - root - 2017-12-07 14:08:45.368842: step 17590, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 55h:38m:01s remains)
INFO - root - 2017-12-07 14:08:52.113189: step 17600, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 63h:02m:29s remains)
2017-12-07 14:08:52.873833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2283106 -4.2374725 -4.2456141 -4.2499967 -4.2533937 -4.2581983 -4.2606716 -4.25961 -4.2601242 -4.2620096 -4.2640061 -4.2692661 -4.2734818 -4.2721605 -4.2704282][-4.2083678 -4.2229404 -4.239809 -4.2495823 -4.2529497 -4.2557964 -4.2589569 -4.2599654 -4.26219 -4.2623606 -4.2624469 -4.2637954 -4.2643085 -4.2628441 -4.2620964][-4.207109 -4.2250667 -4.2444878 -4.2542968 -4.2557635 -4.2565594 -4.2603116 -4.2645345 -4.2689357 -4.2697668 -4.2674322 -4.2635727 -4.2593575 -4.2572017 -4.2573128][-4.2260728 -4.2417345 -4.2543311 -4.2573218 -4.253427 -4.2495265 -4.2503 -4.2555151 -4.2629414 -4.2674503 -4.26547 -4.2593541 -4.2523775 -4.2480326 -4.2483258][-4.2589297 -4.2655487 -4.2665896 -4.2591605 -4.2505536 -4.2412243 -4.233984 -4.233952 -4.2422209 -4.250041 -4.250464 -4.2428994 -4.2349958 -4.2308006 -4.2328963][-4.2648568 -4.2578211 -4.2448192 -4.2250104 -4.2095671 -4.190866 -4.1666327 -4.1574 -4.1735997 -4.1937895 -4.2049131 -4.2008219 -4.1926541 -4.1882505 -4.1900558][-4.2231331 -4.1982689 -4.1695638 -4.1359878 -4.1042008 -4.0627604 -4.0120282 -3.9983015 -4.0390368 -4.085865 -4.1169095 -4.11928 -4.1090074 -4.1042509 -4.1024952][-4.151649 -4.1114044 -4.0662012 -4.0118818 -3.9498627 -3.8744867 -3.7929451 -3.780539 -3.8582449 -3.9388094 -3.9928145 -4.0053005 -4.0018868 -4.0073957 -4.0105567][-4.0980425 -4.05187 -4.00483 -3.95011 -3.8837547 -3.8066616 -3.7298782 -3.7298625 -3.8171511 -3.8936739 -3.942348 -3.9573071 -3.9594994 -3.9761043 -3.9898765][-4.102107 -4.0701065 -4.0419149 -4.0113811 -3.9749687 -3.9362025 -3.900135 -3.9074652 -3.9620688 -4.0031185 -4.0253267 -4.0300851 -4.02835 -4.042922 -4.0566616][-4.1515265 -4.1372304 -4.128458 -4.119204 -4.1084113 -4.0988488 -4.0891762 -4.0959282 -4.1237893 -4.141871 -4.1494112 -4.14909 -4.1466651 -4.1550312 -4.1624789][-4.2046113 -4.2016311 -4.2030706 -4.2049832 -4.2066054 -4.20957 -4.2094393 -4.2120404 -4.2229514 -4.2302079 -4.23428 -4.2347636 -4.2328668 -4.234169 -4.2339559][-4.2510538 -4.2498579 -4.2527032 -4.257493 -4.2616282 -4.2659397 -4.2667122 -4.265625 -4.2690477 -4.2729316 -4.2772536 -4.2784605 -4.2768922 -4.2741003 -4.2705007][-4.2703667 -4.2696528 -4.272315 -4.2759881 -4.2787576 -4.2811828 -4.2805877 -4.2779331 -4.2783504 -4.2806644 -4.2839394 -4.2850556 -4.2839222 -4.2819042 -4.2796283][-4.2596836 -4.2591925 -4.2593689 -4.2599506 -4.2598386 -4.2600961 -4.2594795 -4.2580442 -4.2583113 -4.260231 -4.2622352 -4.2632947 -4.2635169 -4.2637777 -4.2641873]]...]
INFO - root - 2017-12-07 14:08:59.651512: step 17610, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 56h:17m:52s remains)
INFO - root - 2017-12-07 14:09:06.406822: step 17620, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 59h:28m:46s remains)
INFO - root - 2017-12-07 14:09:13.331548: step 17630, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 62h:53m:01s remains)
INFO - root - 2017-12-07 14:09:20.186828: step 17640, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 60h:33m:09s remains)
INFO - root - 2017-12-07 14:09:26.875852: step 17650, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 54h:11m:54s remains)
INFO - root - 2017-12-07 14:09:33.625468: step 17660, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 58h:24m:18s remains)
INFO - root - 2017-12-07 14:09:40.372346: step 17670, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 62h:54m:19s remains)
INFO - root - 2017-12-07 14:09:47.248869: step 17680, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 60h:39m:39s remains)
INFO - root - 2017-12-07 14:09:54.014932: step 17690, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 60h:09m:36s remains)
INFO - root - 2017-12-07 14:10:00.724279: step 17700, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 56h:44m:41s remains)
2017-12-07 14:10:01.522844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1863317 -4.1838708 -4.1913524 -4.2000628 -4.2043295 -4.222043 -4.2382808 -4.2478456 -4.2440567 -4.2108974 -4.1576195 -4.1211748 -4.1245255 -4.1465168 -4.1501923][-4.1793771 -4.1833782 -4.1958141 -4.2121139 -4.2182713 -4.2294989 -4.241785 -4.2474556 -4.2388444 -4.1998796 -4.1439748 -4.1067457 -4.1133046 -4.136169 -4.1348295][-4.1860332 -4.1915865 -4.200604 -4.2159848 -4.223875 -4.2270284 -4.2321258 -4.2385244 -4.2317915 -4.1918941 -4.1368017 -4.0980644 -4.1030831 -4.1313424 -4.1360006][-4.202424 -4.203454 -4.2021365 -4.2118082 -4.2227168 -4.223217 -4.2216053 -4.2252207 -4.221241 -4.1867685 -4.1422315 -4.1057272 -4.1060491 -4.1347404 -4.144917][-4.2183385 -4.2149358 -4.2074652 -4.2117844 -4.2188253 -4.2177477 -4.2125449 -4.2144909 -4.2125769 -4.1842494 -4.1520762 -4.12369 -4.1179886 -4.1353354 -4.1401644][-4.2244625 -4.2209659 -4.2085471 -4.2013192 -4.1960196 -4.193192 -4.1888442 -4.1907129 -4.1855259 -4.158051 -4.129847 -4.1105084 -4.1076407 -4.1177683 -4.1198955][-4.2188582 -4.2131529 -4.1935077 -4.1710958 -4.1474423 -4.136507 -4.1358895 -4.1435719 -4.1345081 -4.1039715 -4.07637 -4.0679975 -4.0755949 -4.0856781 -4.0889692][-4.1989131 -4.1858006 -4.1539669 -4.1109767 -4.0571609 -4.0349693 -4.0451903 -4.0662551 -4.0569057 -4.0204463 -3.9975317 -4.0095935 -4.0393858 -4.057446 -4.0603919][-4.1693573 -4.1509204 -4.1101146 -4.0469685 -3.9659133 -3.9387698 -3.9645481 -4.0003133 -3.9964888 -3.9650605 -3.9562302 -3.9904263 -4.0414424 -4.0673428 -4.0717449][-4.1594725 -4.148499 -4.1166339 -4.057313 -3.9808524 -3.9571972 -3.986196 -4.0252233 -4.0329485 -4.0206537 -4.02049 -4.0517387 -4.0971041 -4.1211963 -4.1280832][-4.17853 -4.1802487 -4.1643548 -4.1256204 -4.0765142 -4.0610118 -4.0821285 -4.1129637 -4.12441 -4.1204624 -4.118463 -4.133657 -4.1612453 -4.1785378 -4.1884913][-4.173027 -4.1824861 -4.1759005 -4.1517739 -4.1257439 -4.12638 -4.1539125 -4.1852317 -4.1976085 -4.1918483 -4.1873894 -4.1925025 -4.2075176 -4.2209916 -4.2308311][-4.1084366 -4.12299 -4.1194134 -4.1078024 -4.1109991 -4.1413994 -4.1863146 -4.2243505 -4.2373037 -4.2306361 -4.2251339 -4.2259974 -4.2350755 -4.2452054 -4.2529335][-4.0213585 -4.0417805 -4.0425782 -4.0451975 -4.0760555 -4.1326265 -4.1911693 -4.23291 -4.2476263 -4.2436514 -4.2398477 -4.238903 -4.2406077 -4.2472425 -4.2563291][-3.9881268 -4.01774 -4.0296087 -4.0416956 -4.0777235 -4.13478 -4.1918287 -4.2326922 -4.2514906 -4.254096 -4.2531385 -4.2506371 -4.2456975 -4.2465119 -4.2566643]]...]
INFO - root - 2017-12-07 14:10:08.287294: step 17710, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 54h:44m:09s remains)
INFO - root - 2017-12-07 14:10:14.935086: step 17720, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 55h:55m:27s remains)
INFO - root - 2017-12-07 14:10:21.752439: step 17730, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 58h:29m:16s remains)
INFO - root - 2017-12-07 14:10:28.577041: step 17740, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 60h:24m:05s remains)
INFO - root - 2017-12-07 14:10:35.416558: step 17750, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 60h:28m:53s remains)
INFO - root - 2017-12-07 14:10:42.106013: step 17760, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 59h:18m:08s remains)
INFO - root - 2017-12-07 14:10:48.773410: step 17770, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 56h:16m:21s remains)
INFO - root - 2017-12-07 14:10:55.530829: step 17780, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 57h:25m:39s remains)
INFO - root - 2017-12-07 14:11:02.282163: step 17790, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 60h:51m:45s remains)
INFO - root - 2017-12-07 14:11:09.067975: step 17800, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 62h:33m:23s remains)
2017-12-07 14:11:09.734374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2687335 -4.2347407 -4.2124209 -4.2058649 -4.2142959 -4.2263055 -4.2367311 -4.2480979 -4.2580495 -4.2664475 -4.2737732 -4.27334 -4.265799 -4.2586365 -4.2589226][-4.2587156 -4.2221241 -4.2041841 -4.2066975 -4.2183676 -4.2261977 -4.2304111 -4.236867 -4.2442293 -4.2503114 -4.2535315 -4.2464423 -4.2329979 -4.2251554 -4.2315707][-4.2557106 -4.2227721 -4.2134109 -4.2218142 -4.229104 -4.2261205 -4.2184153 -4.2158537 -4.2178254 -4.2220693 -4.2226586 -4.2129874 -4.2011671 -4.1997943 -4.2152596][-4.2595186 -4.2344666 -4.2317104 -4.2396259 -4.2369289 -4.2174807 -4.1925659 -4.1777472 -4.1760707 -4.1851282 -4.1904945 -4.1862469 -4.1835647 -4.1934114 -4.2163763][-4.2717805 -4.2544832 -4.2539411 -4.2546678 -4.2382221 -4.1994786 -4.1529613 -4.1219926 -4.1242838 -4.1481161 -4.16737 -4.176302 -4.1875086 -4.2065287 -4.2306542][-4.281909 -4.2699203 -4.2677379 -4.2577758 -4.2242212 -4.1661639 -4.0963659 -4.0554867 -4.07856 -4.128407 -4.1638894 -4.1840749 -4.2030191 -4.2238932 -4.2437243][-4.2790461 -4.2676296 -4.2617512 -4.24013 -4.1908641 -4.1178532 -4.0374436 -4.0079546 -4.0640159 -4.1339941 -4.1744723 -4.1938124 -4.2091422 -4.22609 -4.2402072][-4.266521 -4.2529583 -4.2411156 -4.2098069 -4.1526985 -4.0833144 -4.0237975 -4.0267544 -4.094676 -4.1580157 -4.1880846 -4.1967516 -4.2016082 -4.2141519 -4.2270813][-4.2616353 -4.2447476 -4.2260261 -4.1895862 -4.1360126 -4.0880008 -4.0677457 -4.0927525 -4.1441255 -4.1842403 -4.1983891 -4.1931763 -4.1873283 -4.1967397 -4.2118225][-4.2604442 -4.239048 -4.214221 -4.1781111 -4.1379414 -4.1143246 -4.1212277 -4.1507816 -4.1812887 -4.1986232 -4.2000346 -4.1872921 -4.1741505 -4.1809564 -4.2005963][-4.255229 -4.2284374 -4.1998472 -4.1679749 -4.1423397 -4.1350355 -4.153686 -4.1812778 -4.1990938 -4.2018862 -4.1952238 -4.1799588 -4.1647997 -4.1704473 -4.1921511][-4.24114 -4.2113857 -4.1831264 -4.1564183 -4.1391263 -4.1374168 -4.1585784 -4.184875 -4.1979508 -4.1947756 -4.1835623 -4.168282 -4.1555996 -4.1604533 -4.1789012][-4.2257318 -4.1976948 -4.1713581 -4.1473026 -4.130096 -4.127552 -4.1461735 -4.1706548 -4.1825275 -4.1783781 -4.1674023 -4.1548004 -4.1451468 -4.1488585 -4.162662][-4.21561 -4.1898766 -4.1651559 -4.1425161 -4.1256437 -4.120811 -4.1345057 -4.1552005 -4.1662974 -4.16303 -4.1556044 -4.1472025 -4.1404576 -4.1448288 -4.1551356][-4.2183022 -4.1951323 -4.1725016 -4.1512852 -4.1371469 -4.1308122 -4.1384368 -4.1541414 -4.163393 -4.1607652 -4.1566849 -4.1521521 -4.1488066 -4.1540847 -4.1619139]]...]
INFO - root - 2017-12-07 14:11:16.462310: step 17810, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 55h:16m:29s remains)
INFO - root - 2017-12-07 14:11:23.083387: step 17820, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 60h:03m:44s remains)
INFO - root - 2017-12-07 14:11:29.900343: step 17830, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 64h:37m:36s remains)
INFO - root - 2017-12-07 14:11:36.727715: step 17840, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 60h:50m:55s remains)
INFO - root - 2017-12-07 14:11:43.501057: step 17850, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 55h:23m:27s remains)
INFO - root - 2017-12-07 14:11:50.420976: step 17860, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 59h:39m:38s remains)
INFO - root - 2017-12-07 14:11:57.297684: step 17870, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 61h:20m:27s remains)
INFO - root - 2017-12-07 14:12:04.090850: step 17880, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 63h:17m:49s remains)
INFO - root - 2017-12-07 14:12:10.927324: step 17890, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 57h:41m:39s remains)
INFO - root - 2017-12-07 14:12:17.775894: step 17900, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 55h:54m:09s remains)
2017-12-07 14:12:18.474924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2114692 -4.2121463 -4.2304749 -4.2501922 -4.255208 -4.2434568 -4.22751 -4.2084188 -4.1964569 -4.1866808 -4.1718478 -4.158433 -4.1590686 -4.1792469 -4.2038603][-4.2342653 -4.2282534 -4.237545 -4.2524862 -4.2553973 -4.2456069 -4.2356443 -4.2232256 -4.2167387 -4.2122135 -4.20383 -4.1935372 -4.1911407 -4.2059221 -4.2235107][-4.2710223 -4.2619138 -4.2599444 -4.2627497 -4.2572136 -4.2469296 -4.2444091 -4.2410336 -4.2404213 -4.2448549 -4.246377 -4.2390165 -4.2322373 -4.2392507 -4.2481766][-4.2857933 -4.2823477 -4.2757397 -4.268796 -4.2564664 -4.2438955 -4.240335 -4.2389579 -4.2423234 -4.2527757 -4.2638435 -4.263989 -4.258297 -4.2618165 -4.26751][-4.2616477 -4.2689948 -4.268229 -4.25859 -4.2420411 -4.2294803 -4.2257309 -4.2233119 -4.2257619 -4.2374158 -4.25371 -4.2598939 -4.2600741 -4.2656689 -4.2731547][-4.2057667 -4.22666 -4.2394452 -4.2265134 -4.2050977 -4.1950269 -4.1956744 -4.1975393 -4.2023659 -4.214231 -4.2322493 -4.2435789 -4.2500792 -4.2573833 -4.2663541][-4.134717 -4.1746287 -4.1991491 -4.1828556 -4.1600685 -4.1523352 -4.1580138 -4.1677971 -4.181283 -4.1968021 -4.2137923 -4.2265339 -4.2357736 -4.2419219 -4.2506824][-4.1036911 -4.1438375 -4.16675 -4.1445384 -4.1200686 -4.11927 -4.1319456 -4.1486893 -4.172092 -4.1936722 -4.2066469 -4.2146931 -4.2211556 -4.2217312 -4.228301][-4.1240215 -4.147788 -4.1598167 -4.1371908 -4.117557 -4.1218238 -4.1346717 -4.1498461 -4.1741438 -4.1914778 -4.1983414 -4.20386 -4.2042561 -4.2001338 -4.2046266][-4.1685648 -4.1793003 -4.1830559 -4.1683679 -4.160182 -4.1666851 -4.1715636 -4.1784678 -4.1940947 -4.2009854 -4.1958508 -4.1866293 -4.1776509 -4.1699238 -4.1765957][-4.1993546 -4.2037487 -4.2049851 -4.2050219 -4.2132726 -4.2219486 -4.2192035 -4.2162805 -4.2193475 -4.2141571 -4.1944094 -4.1691308 -4.1459231 -4.131238 -4.1425982][-4.1828027 -4.1900287 -4.1954012 -4.2058015 -4.2207885 -4.2294865 -4.2244854 -4.2162795 -4.21247 -4.1994271 -4.1709747 -4.1343713 -4.0998325 -4.0795546 -4.0945888][-4.1783257 -4.1876059 -4.1939635 -4.2019086 -4.2101111 -4.2123513 -4.2058988 -4.19575 -4.18839 -4.1748152 -4.1454034 -4.1078815 -4.072176 -4.0528059 -4.0707717][-4.2092957 -4.2174373 -4.2208734 -4.2232423 -4.2246156 -4.220706 -4.2116547 -4.2011704 -4.1930156 -4.1817503 -4.1562824 -4.1199775 -4.0858192 -4.069663 -4.0884581][-4.2258706 -4.2313037 -4.2329907 -4.2343059 -4.2340212 -4.2286773 -4.2191968 -4.2116437 -4.2067528 -4.1992264 -4.1784849 -4.1465988 -4.1175742 -4.1074433 -4.1254587]]...]
INFO - root - 2017-12-07 14:12:25.042424: step 17910, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 59h:24m:06s remains)
INFO - root - 2017-12-07 14:12:31.728777: step 17920, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 57h:03m:43s remains)
INFO - root - 2017-12-07 14:12:38.548990: step 17930, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 61h:53m:20s remains)
INFO - root - 2017-12-07 14:12:45.417132: step 17940, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 63h:52m:43s remains)
INFO - root - 2017-12-07 14:12:52.276162: step 17950, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 57h:28m:48s remains)
INFO - root - 2017-12-07 14:12:59.109821: step 17960, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 55h:39m:39s remains)
INFO - root - 2017-12-07 14:13:05.938436: step 17970, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 61h:41m:01s remains)
INFO - root - 2017-12-07 14:13:12.831198: step 17980, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 64h:28m:30s remains)
INFO - root - 2017-12-07 14:13:19.665790: step 17990, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 58h:50m:41s remains)
INFO - root - 2017-12-07 14:13:26.420685: step 18000, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.632 sec/batch; 55h:13m:32s remains)
2017-12-07 14:13:27.166229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1972 -4.2030888 -4.2216792 -4.2373533 -4.2485003 -4.2544737 -4.2604885 -4.268342 -4.2751679 -4.2826748 -4.2904916 -4.29372 -4.290267 -4.2829676 -4.2782445][-4.147121 -4.1510653 -4.1698885 -4.1888256 -4.204699 -4.2144461 -4.2224603 -4.2340789 -4.2439885 -4.2534013 -4.2611604 -4.2625079 -4.2569642 -4.2473631 -4.242559][-4.1175947 -4.1148911 -4.1265082 -4.1426325 -4.1594815 -4.1736307 -4.1860776 -4.2012014 -4.2125015 -4.2227893 -4.2297325 -4.23021 -4.2229643 -4.2123938 -4.210783][-4.1073322 -4.0903883 -4.0867577 -4.0940619 -4.1086068 -4.1268268 -4.1439662 -4.1589551 -4.1693535 -4.1797462 -4.1873918 -4.1924086 -4.1893172 -4.1847229 -4.1928563][-4.1165266 -4.0802445 -4.0557733 -4.0481195 -4.056077 -4.0786839 -4.103219 -4.1189961 -4.127346 -4.13419 -4.1405039 -4.1493974 -4.153163 -4.1576524 -4.1796231][-4.1325231 -4.0803828 -4.0351868 -4.0078778 -4.00379 -4.0276456 -4.0607696 -4.0809555 -4.0888014 -4.0886774 -4.083704 -4.0872345 -4.0956688 -4.1112103 -4.1481891][-4.1426764 -4.084549 -4.0270882 -3.9815121 -3.9639404 -3.9808762 -4.01434 -4.0372944 -4.0477042 -4.0416775 -4.0230913 -4.0169187 -4.0259461 -4.0494075 -4.0964165][-4.1628013 -4.1126275 -4.0573659 -4.0050492 -3.9755116 -3.9786956 -4.0002327 -4.0211477 -4.0325832 -4.0253782 -4.0014491 -3.9883797 -3.9922402 -4.0123777 -4.0571423][-4.1933417 -4.1585379 -4.1175423 -4.0749207 -4.0434036 -4.0348597 -4.0439978 -4.0592957 -4.0696278 -4.0649471 -4.0419159 -4.0248756 -4.0177188 -4.0267243 -4.0596733][-4.2156682 -4.1962953 -4.1725826 -4.1455464 -4.1194453 -4.1050763 -4.1058588 -4.1150875 -4.1218648 -4.1154814 -4.09733 -4.0804853 -4.0673442 -4.0697713 -4.0912809][-4.2282834 -4.2172446 -4.2060485 -4.1925535 -4.1757374 -4.1614881 -4.1583557 -4.165309 -4.1701012 -4.164444 -4.1538959 -4.1398025 -4.1230273 -4.11921 -4.1307888][-4.2289252 -4.2231293 -4.2203765 -4.2171931 -4.2096949 -4.1985569 -4.1927223 -4.1984458 -4.2033248 -4.2010493 -4.19954 -4.1921959 -4.1778917 -4.1705613 -4.1745567][-4.2303538 -4.2265887 -4.2281518 -4.2301011 -4.2269692 -4.2187033 -4.2125177 -4.2153344 -4.2188015 -4.2189765 -4.2237244 -4.2238445 -4.2157426 -4.209465 -4.2116218][-4.2445679 -4.2422976 -4.2439594 -4.2463293 -4.2445297 -4.2385058 -4.23207 -4.2310066 -4.2312751 -4.2307158 -4.2349505 -4.2391181 -4.2373157 -4.2359157 -4.2418451][-4.267499 -4.2674627 -4.26924 -4.2718687 -4.27252 -4.2709661 -4.2664351 -4.2621326 -4.2585192 -4.25514 -4.2556324 -4.2590542 -4.2616692 -4.2656789 -4.2744341]]...]
INFO - root - 2017-12-07 14:13:33.898466: step 18010, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:47m:58s remains)
INFO - root - 2017-12-07 14:13:40.590856: step 18020, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 60h:25m:17s remains)
INFO - root - 2017-12-07 14:13:47.331999: step 18030, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 55h:50m:46s remains)
INFO - root - 2017-12-07 14:13:54.193387: step 18040, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.679 sec/batch; 59h:16m:12s remains)
INFO - root - 2017-12-07 14:14:01.029537: step 18050, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 62h:14m:25s remains)
INFO - root - 2017-12-07 14:14:07.744348: step 18060, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 58h:36m:47s remains)
INFO - root - 2017-12-07 14:14:14.418815: step 18070, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 56h:18m:55s remains)
INFO - root - 2017-12-07 14:14:21.212016: step 18080, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 58h:32m:57s remains)
INFO - root - 2017-12-07 14:14:28.005862: step 18090, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 61h:08m:28s remains)
INFO - root - 2017-12-07 14:14:34.830533: step 18100, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 61h:07m:22s remains)
2017-12-07 14:14:35.601462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2641597 -4.2593179 -4.2597461 -4.2582417 -4.2575612 -4.2579584 -4.2607102 -4.2709527 -4.2754955 -4.2674341 -4.2589641 -4.2558656 -4.2561836 -4.2571177 -4.2535014][-4.230104 -4.2212086 -4.2190642 -4.2189417 -4.221148 -4.2206907 -4.2218518 -4.2333107 -4.2396364 -4.2313652 -4.2210412 -4.2176232 -4.2208972 -4.2244482 -4.2193012][-4.2182932 -4.2082181 -4.2063413 -4.21233 -4.2161026 -4.2074366 -4.1980071 -4.2057953 -4.2153811 -4.2105746 -4.2018833 -4.2023878 -4.2124505 -4.2200227 -4.2119126][-4.2167525 -4.2097778 -4.2139864 -4.2242007 -4.2255015 -4.2106643 -4.1920967 -4.1965089 -4.2081618 -4.2058668 -4.2001338 -4.2050242 -4.220161 -4.231163 -4.2244229][-4.2254186 -4.216959 -4.2208195 -4.2296648 -4.228065 -4.2125812 -4.193068 -4.1957908 -4.2018018 -4.1961141 -4.1918154 -4.1998429 -4.2164383 -4.2258248 -4.2235208][-4.2291055 -4.2152781 -4.2099342 -4.2122655 -4.2059913 -4.1904774 -4.1721635 -4.1671233 -4.16367 -4.1568623 -4.1560755 -4.1652269 -4.1785221 -4.1844392 -4.1865196][-4.20034 -4.1820889 -4.1673641 -4.1610222 -4.1494112 -4.1356759 -4.1196065 -4.1089487 -4.0995383 -4.0937047 -4.0958209 -4.1012516 -4.1059484 -4.103889 -4.1058092][-4.1579332 -4.144834 -4.12433 -4.1080184 -4.0872703 -4.0679612 -4.0518827 -4.0435095 -4.0398626 -4.045476 -4.0576525 -4.063098 -4.061511 -4.0488644 -4.0438914][-4.1338215 -4.1260872 -4.1042523 -4.0860195 -4.0666714 -4.0460739 -4.0316319 -4.0293145 -4.0329328 -4.0442305 -4.0601339 -4.0678043 -4.0676489 -4.0548387 -4.0470209][-4.1459832 -4.1436887 -4.1272383 -4.11536 -4.1050186 -4.0905828 -4.07802 -4.0765018 -4.0781426 -4.0843058 -4.0965142 -4.1055608 -4.1038451 -4.0899339 -4.0811057][-4.1765919 -4.180861 -4.1769571 -4.1733012 -4.1702685 -4.1618905 -4.1508327 -4.1465282 -4.1430688 -4.1428485 -4.1468792 -4.150938 -4.1444993 -4.1302609 -4.1231894][-4.1835027 -4.194613 -4.2064695 -4.215734 -4.2225561 -4.2225032 -4.2183919 -4.2136803 -4.2065134 -4.1992016 -4.1923223 -4.1865268 -4.1674666 -4.1473637 -4.1438088][-4.1634684 -4.1797895 -4.1986022 -4.2147512 -4.229537 -4.2402062 -4.243845 -4.240325 -4.2316055 -4.2211528 -4.2082176 -4.19456 -4.167717 -4.143733 -4.1382165][-4.1547232 -4.1712108 -4.1906705 -4.2059932 -4.217999 -4.2261105 -4.2296491 -4.2270813 -4.2204161 -4.2147388 -4.2054029 -4.1885958 -4.1568794 -4.1291976 -4.1215615][-4.1658225 -4.17348 -4.1843796 -4.1931753 -4.1975737 -4.2004514 -4.2047653 -4.2066 -4.2048941 -4.2058697 -4.2045813 -4.1932125 -4.1636944 -4.137804 -4.1287045]]...]
INFO - root - 2017-12-07 14:14:42.261519: step 18110, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.608 sec/batch; 53h:07m:14s remains)
INFO - root - 2017-12-07 14:14:49.011516: step 18120, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 59h:17m:21s remains)
INFO - root - 2017-12-07 14:14:55.759963: step 18130, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 61h:42m:20s remains)
INFO - root - 2017-12-07 14:15:02.468833: step 18140, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.643 sec/batch; 56h:06m:32s remains)
INFO - root - 2017-12-07 14:15:09.202708: step 18150, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 58h:01m:55s remains)
INFO - root - 2017-12-07 14:15:15.998770: step 18160, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 57h:59m:56s remains)
INFO - root - 2017-12-07 14:15:22.727125: step 18170, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 59h:41m:43s remains)
INFO - root - 2017-12-07 14:15:29.579158: step 18180, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.712 sec/batch; 62h:10m:11s remains)
INFO - root - 2017-12-07 14:15:36.377231: step 18190, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 56h:36m:04s remains)
INFO - root - 2017-12-07 14:15:43.150176: step 18200, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.655 sec/batch; 57h:09m:10s remains)
2017-12-07 14:15:43.826511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104911 -4.3016467 -4.2999535 -4.3003516 -4.2964439 -4.2919388 -4.29237 -4.2965465 -4.3018708 -4.3077087 -4.3106031 -4.3103976 -4.3071952 -4.3034835 -4.3031483][-4.298635 -4.2857895 -4.2840681 -4.2851524 -4.2751222 -4.2665658 -4.2675419 -4.2745595 -4.28302 -4.2923126 -4.2981281 -4.3017449 -4.2996159 -4.2934585 -4.2907958][-4.2820787 -4.2665372 -4.2642736 -4.2634168 -4.2475457 -4.2337742 -4.2365241 -4.2479854 -4.2604885 -4.2744169 -4.28334 -4.2925525 -4.2942023 -4.2891498 -4.2871823][-4.2601857 -4.2411203 -4.2336311 -4.2283545 -4.2087965 -4.1929188 -4.1948762 -4.2086959 -4.2267814 -4.2487712 -4.262115 -4.275516 -4.2789416 -4.2785087 -4.2790508][-4.2340131 -4.20951 -4.1929297 -4.1847491 -4.1683416 -4.1512055 -4.1456857 -4.1575527 -4.1828704 -4.2150359 -4.234653 -4.2542415 -4.2611575 -4.2659287 -4.2681451][-4.2050066 -4.1698847 -4.13757 -4.1239591 -4.1164632 -4.1036119 -4.0873847 -4.091547 -4.1237144 -4.16739 -4.1937695 -4.2209659 -4.2401133 -4.2538481 -4.2583051][-4.1857362 -4.135983 -4.0779738 -4.0505414 -4.0503259 -4.0479937 -4.0270991 -4.0192332 -4.0565748 -4.1075444 -4.1408343 -4.174511 -4.2048249 -4.2320142 -4.2443595][-4.1850162 -4.1211414 -4.0380783 -3.9910011 -3.991854 -3.9988451 -3.9725704 -3.9459054 -3.9797068 -4.0377541 -4.0777059 -4.1166873 -4.1545134 -4.190093 -4.2160106][-4.2057438 -4.1409211 -4.0488939 -3.9900963 -3.9837608 -3.9902315 -3.9528239 -3.9067416 -3.9253614 -3.984885 -4.0257015 -4.0626817 -4.1048717 -4.1464162 -4.1861544][-4.2321534 -4.1797695 -4.1038141 -4.0477881 -4.0323253 -4.0299106 -3.9870152 -3.9369564 -3.9357784 -3.9812303 -4.0072417 -4.0336528 -4.0695095 -4.1101937 -4.1575356][-4.2388258 -4.1982212 -4.1483264 -4.1107345 -4.0979438 -4.0925765 -4.0568113 -4.0149345 -4.0030208 -4.0227094 -4.026248 -4.0370541 -4.0616932 -4.0913935 -4.1321115][-4.2284527 -4.1903639 -4.1580663 -4.1409979 -4.1378117 -4.132865 -4.1092587 -4.08494 -4.07737 -4.0768313 -4.0619712 -4.05612 -4.0652008 -4.0818648 -4.1083107][-4.2110119 -4.17106 -4.1465211 -4.1419458 -4.1433921 -4.1414561 -4.1322994 -4.1260109 -4.1292982 -4.1237307 -4.098516 -4.0775447 -4.07488 -4.0809226 -4.0992365][-4.1961336 -4.158412 -4.1413989 -4.1437292 -4.148283 -4.1502185 -4.1507435 -4.1527052 -4.1573358 -4.1526241 -4.129035 -4.1032147 -4.0967565 -4.1014047 -4.1203742][-4.2034254 -4.1727743 -4.1649141 -4.1727548 -4.1819186 -4.1871967 -4.1891375 -4.1888227 -4.1893682 -4.1842012 -4.1682882 -4.1497545 -4.1464915 -4.1542058 -4.1722217]]...]
INFO - root - 2017-12-07 14:15:50.644896: step 18210, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 63h:58m:46s remains)
INFO - root - 2017-12-07 14:15:57.212937: step 18220, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 55h:42m:52s remains)
INFO - root - 2017-12-07 14:16:04.098927: step 18230, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.681 sec/batch; 59h:25m:20s remains)
INFO - root - 2017-12-07 14:16:11.028910: step 18240, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 60h:19m:01s remains)
INFO - root - 2017-12-07 14:16:17.796061: step 18250, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 59h:45m:59s remains)
INFO - root - 2017-12-07 14:16:24.672441: step 18260, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 57h:34m:34s remains)
INFO - root - 2017-12-07 14:16:31.389629: step 18270, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 57h:06m:49s remains)
INFO - root - 2017-12-07 14:16:38.215398: step 18280, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 63h:35m:03s remains)
INFO - root - 2017-12-07 14:16:44.936492: step 18290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 60h:24m:15s remains)
INFO - root - 2017-12-07 14:16:51.637209: step 18300, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 56h:06m:15s remains)
2017-12-07 14:16:52.320332: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2421837 -4.2599635 -4.268959 -4.2776623 -4.26888 -4.2457423 -4.2231288 -4.2175422 -4.2315731 -4.2499547 -4.2686071 -4.2841654 -4.2986083 -4.3111186 -4.3173289][-4.2682652 -4.2801085 -4.2763233 -4.2656221 -4.2339287 -4.1867604 -4.1506352 -4.1485839 -4.1783142 -4.2143054 -4.2494655 -4.2765579 -4.2963858 -4.3115225 -4.3179183][-4.2801952 -4.2844276 -4.2692471 -4.2399874 -4.1845093 -4.1116691 -4.0589814 -4.0615234 -4.1105857 -4.1671219 -4.2207036 -4.26256 -4.2906389 -4.3115263 -4.3204994][-4.2890077 -4.2881594 -4.2635107 -4.2175679 -4.1395106 -4.0392885 -3.9670775 -3.9732914 -4.0439172 -4.1221766 -4.194222 -4.2494383 -4.2833743 -4.3079414 -4.3188858][-4.28894 -4.2827291 -4.2495995 -4.1897478 -4.091969 -3.9663229 -3.8718505 -3.8802416 -3.9771566 -4.0819664 -4.1731668 -4.2394772 -4.2771268 -4.3011775 -4.3115859][-4.2771873 -4.2648973 -4.2224913 -4.1504717 -4.0388012 -3.8905511 -3.7691579 -3.7760174 -3.9063447 -4.0440636 -4.1556025 -4.2322593 -4.2744484 -4.2965755 -4.3053432][-4.2571831 -4.2390685 -4.1916738 -4.1142926 -3.9997809 -3.8400884 -3.6953762 -3.6922283 -3.8496373 -4.0141835 -4.1393428 -4.2230625 -4.2693539 -4.2919083 -4.3018041][-4.2440886 -4.2277246 -4.1865158 -4.1200738 -4.0221615 -3.8762529 -3.7351267 -3.71619 -3.8608294 -4.0200486 -4.1384249 -4.2185745 -4.2648153 -4.285285 -4.2937064][-4.239006 -4.2319617 -4.2075667 -4.1665182 -4.1020088 -3.9936855 -3.8849552 -3.858855 -3.9574354 -4.0764933 -4.1633649 -4.2247658 -4.2642059 -4.2793021 -4.28302][-4.2253366 -4.23011 -4.2275767 -4.2158346 -4.18676 -4.1216807 -4.0508423 -4.0259695 -4.0799856 -4.1513367 -4.1988316 -4.2322559 -4.2590089 -4.26938 -4.2692394][-4.1944761 -4.2067246 -4.2215071 -4.2340631 -4.232933 -4.2017064 -4.160243 -4.1368074 -4.15862 -4.1927443 -4.2099328 -4.2194843 -4.23224 -4.2393785 -4.2378516][-4.1379004 -4.1560411 -4.1858397 -4.2188811 -4.2405553 -4.2344818 -4.2152591 -4.1959271 -4.1970429 -4.2000771 -4.1873608 -4.1757345 -4.1781225 -4.183548 -4.1851535][-4.0888672 -4.109231 -4.1457624 -4.190134 -4.2254829 -4.2360845 -4.2305636 -4.2142072 -4.2023163 -4.184145 -4.1507349 -4.1281886 -4.1281185 -4.13505 -4.1410346][-4.0833163 -4.100369 -4.131701 -4.1789517 -4.2203264 -4.2403178 -4.2434082 -4.2270308 -4.2060032 -4.1775904 -4.1385288 -4.1184034 -4.1238074 -4.1346974 -4.1424623][-4.1245718 -4.1357579 -4.15566 -4.194726 -4.2336273 -4.2554922 -4.2606835 -4.2459583 -4.2269163 -4.2025285 -4.1715956 -4.162498 -4.1752868 -4.1878085 -4.19181]]...]
INFO - root - 2017-12-07 14:16:58.987765: step 18310, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 58h:42m:01s remains)
INFO - root - 2017-12-07 14:17:05.681476: step 18320, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 61h:01m:46s remains)
INFO - root - 2017-12-07 14:17:12.542471: step 18330, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 60h:09m:04s remains)
INFO - root - 2017-12-07 14:17:19.353370: step 18340, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 59h:18m:04s remains)
INFO - root - 2017-12-07 14:17:26.290387: step 18350, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 59h:30m:14s remains)
INFO - root - 2017-12-07 14:17:33.096980: step 18360, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 60h:28m:42s remains)
INFO - root - 2017-12-07 14:17:39.996413: step 18370, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 62h:14m:01s remains)
INFO - root - 2017-12-07 14:17:46.782000: step 18380, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 58h:46m:37s remains)
INFO - root - 2017-12-07 14:17:53.576177: step 18390, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 58h:41m:54s remains)
INFO - root - 2017-12-07 14:18:00.413653: step 18400, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 59h:47m:17s remains)
2017-12-07 14:18:01.130481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1999063 -4.1782165 -4.1697159 -4.1700053 -4.1769023 -4.1893744 -4.2073183 -4.2115068 -4.2003136 -4.1920514 -4.1967092 -4.2088056 -4.2209072 -4.2364731 -4.2590775][-4.2410545 -4.2224059 -4.2093463 -4.1993246 -4.198585 -4.2089696 -4.2262163 -4.2342792 -4.2335 -4.2377005 -4.2487187 -4.255599 -4.252646 -4.2524352 -4.2654452][-4.2642322 -4.2586408 -4.2476597 -4.2336388 -4.2280107 -4.2378664 -4.2542987 -4.2657647 -4.2718697 -4.2774105 -4.280519 -4.27325 -4.2541022 -4.2453566 -4.2555962][-4.2638693 -4.27471 -4.2744222 -4.263793 -4.2590733 -4.2629981 -4.2689719 -4.2727151 -4.2801523 -4.2849331 -4.2795968 -4.2622423 -4.2351542 -4.2250361 -4.23957][-4.243227 -4.2634907 -4.271831 -4.268856 -4.2690678 -4.2682266 -4.2641826 -4.2609129 -4.2615819 -4.2605333 -4.2474341 -4.2276549 -4.205605 -4.2048035 -4.2292504][-4.2108455 -4.2322373 -4.2464695 -4.2500243 -4.2517142 -4.2495213 -4.2417383 -4.2293305 -4.2220669 -4.2136564 -4.1997881 -4.1866059 -4.1788664 -4.1949849 -4.228478][-4.1754265 -4.1998377 -4.2202067 -4.2247324 -4.2241869 -4.2182212 -4.2058611 -4.1859732 -4.1742973 -4.1629848 -4.1550865 -4.1542473 -4.1644783 -4.1977539 -4.2356663][-4.1527467 -4.1800442 -4.2009211 -4.1992459 -4.187799 -4.1731024 -4.1567488 -4.1311526 -4.1146126 -4.1071854 -4.1134782 -4.1317859 -4.1632757 -4.2082558 -4.2470193][-4.1525908 -4.1729951 -4.1804314 -4.16267 -4.1371355 -4.1139936 -4.0945516 -4.0718822 -4.0558977 -4.052103 -4.0710635 -4.1103735 -4.160543 -4.2124519 -4.2518506][-4.1491671 -4.1612329 -4.1581683 -4.131753 -4.0974455 -4.0677037 -4.0503869 -4.0405865 -4.0356092 -4.0373278 -4.0649743 -4.1113563 -4.1645436 -4.2151508 -4.2536764][-4.1482782 -4.1619735 -4.1569347 -4.1320925 -4.0989871 -4.0689297 -4.0571389 -4.0625577 -4.0673881 -4.073832 -4.100286 -4.1376724 -4.17902 -4.2205009 -4.2555923][-4.1561332 -4.1713572 -4.1692085 -4.1517506 -4.1286392 -4.109992 -4.1118841 -4.1299787 -4.1404395 -4.1466827 -4.1604705 -4.1792793 -4.2023926 -4.233674 -4.265152][-4.1731076 -4.1854224 -4.18839 -4.1827288 -4.1745963 -4.1702156 -4.181952 -4.2022929 -4.2107368 -4.2103648 -4.2111921 -4.2151122 -4.2247448 -4.249033 -4.2774954][-4.2092915 -4.2163577 -4.2202168 -4.222146 -4.2252893 -4.2292819 -4.2424688 -4.2573633 -4.2606072 -4.2537694 -4.2458215 -4.2408619 -4.2434716 -4.2643552 -4.2894859][-4.261229 -4.2612662 -4.2627583 -4.2647481 -4.270267 -4.2766037 -4.2853909 -4.2912712 -4.2878747 -4.2770305 -4.2676682 -4.2618356 -4.2615204 -4.2801 -4.3025131]]...]
INFO - root - 2017-12-07 14:18:07.802446: step 18410, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 59h:42m:42s remains)
INFO - root - 2017-12-07 14:18:14.396004: step 18420, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 56h:01m:04s remains)
INFO - root - 2017-12-07 14:18:21.275086: step 18430, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 59h:43m:13s remains)
INFO - root - 2017-12-07 14:18:28.054432: step 18440, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 60h:03m:27s remains)
INFO - root - 2017-12-07 14:18:34.828142: step 18450, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 61h:39m:32s remains)
INFO - root - 2017-12-07 14:18:41.552697: step 18460, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 57h:40m:36s remains)
INFO - root - 2017-12-07 14:18:48.346696: step 18470, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 57h:23m:36s remains)
INFO - root - 2017-12-07 14:18:55.146188: step 18480, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 62h:33m:42s remains)
INFO - root - 2017-12-07 14:19:01.932318: step 18490, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 61h:47m:28s remains)
INFO - root - 2017-12-07 14:19:08.743576: step 18500, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 61h:10m:20s remains)
2017-12-07 14:19:09.460706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2629781 -4.2745442 -4.2874212 -4.2997561 -4.3108125 -4.3190131 -4.3226185 -4.325356 -4.3277826 -4.3275256 -4.3238091 -4.3134155 -4.2950697 -4.27112 -4.2528234][-4.2555108 -4.2654691 -4.2762547 -4.2869225 -4.2988462 -4.3083544 -4.3137155 -4.3211932 -4.330368 -4.3366804 -4.3366313 -4.3270879 -4.3067431 -4.2795405 -4.2579732][-4.2467613 -4.2499 -4.2534003 -4.2575159 -4.2676668 -4.2766161 -4.2822208 -4.2940731 -4.3116903 -4.3262215 -4.3324904 -4.3267455 -4.3076305 -4.2801495 -4.2577043][-4.2356253 -4.2261872 -4.2169685 -4.2103052 -4.2154169 -4.2209959 -4.2228146 -4.235642 -4.2622128 -4.2872028 -4.3025355 -4.3037934 -4.2908888 -4.267992 -4.2489247][-4.2207413 -4.1974554 -4.1739593 -4.155488 -4.1525526 -4.1503744 -4.1431608 -4.1483974 -4.1790638 -4.2151651 -4.2416425 -4.2532468 -4.2514682 -4.2393632 -4.2293687][-4.2016716 -4.1679239 -4.1338072 -4.1054039 -4.0920095 -4.0794005 -4.0580869 -4.0451822 -4.0717425 -4.1187367 -4.1587863 -4.1830597 -4.1961164 -4.1995478 -4.20303][-4.1779094 -4.1401539 -4.1014776 -4.0678473 -4.0451274 -4.0214753 -3.9857717 -3.9522059 -3.9682469 -4.0243416 -4.0783195 -4.1147127 -4.141705 -4.1610746 -4.1790485][-4.1582966 -4.1218123 -4.0841846 -4.049674 -4.0226183 -3.9917293 -3.9486032 -3.9043872 -3.9100404 -3.9683571 -4.0309768 -4.0756645 -4.1101246 -4.1397061 -4.1672497][-4.1520905 -4.1178713 -4.0829268 -4.0507116 -4.0257282 -3.9976387 -3.9598503 -3.9208307 -3.9202375 -3.9692221 -4.0295057 -4.0754747 -4.1098824 -4.1412559 -4.170939][-4.1658273 -4.1341667 -4.101603 -4.0736032 -4.0549765 -4.0374274 -4.0132589 -3.9871361 -3.9845197 -4.0186625 -4.0671225 -4.1055255 -4.132421 -4.1580954 -4.183311][-4.2001166 -4.1745114 -4.1466503 -4.1235247 -4.1108513 -4.1031332 -4.0917349 -4.0765195 -4.07287 -4.0917888 -4.1246214 -4.1510663 -4.1673 -4.1833911 -4.1991143][-4.2399912 -4.2273717 -4.2100239 -4.1940756 -4.1854448 -4.18296 -4.1782403 -4.1681614 -4.161746 -4.167316 -4.1831474 -4.1961923 -4.2022195 -4.2075262 -4.2106428][-4.2612238 -4.2678232 -4.2677631 -4.2633247 -4.2592192 -4.2580218 -4.2537656 -4.2432146 -4.2325993 -4.2275343 -4.2292829 -4.2314711 -4.2288494 -4.2231011 -4.211277][-4.2447414 -4.2706418 -4.2888212 -4.2979851 -4.3017154 -4.3037672 -4.3000031 -4.2894359 -4.2765059 -4.2646046 -4.2565355 -4.2500305 -4.2399373 -4.2241821 -4.1979361][-4.1876812 -4.2266169 -4.2611351 -4.2863355 -4.3032928 -4.3144145 -4.3167524 -4.3110366 -4.2991776 -4.2838387 -4.2681789 -4.2537766 -4.2359443 -4.2106266 -4.1723304]]...]
INFO - root - 2017-12-07 14:19:16.219103: step 18510, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 59h:34m:36s remains)
INFO - root - 2017-12-07 14:19:22.934205: step 18520, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 61h:45m:22s remains)
INFO - root - 2017-12-07 14:19:29.582151: step 18530, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 59h:50m:35s remains)
INFO - root - 2017-12-07 14:19:36.444054: step 18540, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 58h:22m:05s remains)
INFO - root - 2017-12-07 14:19:43.240663: step 18550, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.692 sec/batch; 60h:23m:07s remains)
INFO - root - 2017-12-07 14:19:50.163942: step 18560, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 64h:39m:40s remains)
INFO - root - 2017-12-07 14:19:57.022937: step 18570, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 59h:28m:13s remains)
INFO - root - 2017-12-07 14:20:03.843105: step 18580, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 56h:45m:40s remains)
INFO - root - 2017-12-07 14:20:10.754955: step 18590, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 58h:42m:04s remains)
INFO - root - 2017-12-07 14:20:17.584990: step 18600, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 63h:49m:30s remains)
2017-12-07 14:20:18.274787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2692528 -4.2572904 -4.2336264 -4.2091112 -4.2015171 -4.2136221 -4.2256594 -4.2313485 -4.2385936 -4.2556376 -4.2766294 -4.2868204 -4.2926564 -4.2934866 -4.2782664][-4.2754884 -4.2604537 -4.2313008 -4.2010732 -4.1909657 -4.1999712 -4.20944 -4.2105012 -4.2139831 -4.2305756 -4.2569094 -4.2731071 -4.2826295 -4.2844968 -4.2702928][-4.292511 -4.2768087 -4.2441578 -4.2121286 -4.2008753 -4.2017636 -4.201273 -4.1876969 -4.1806479 -4.1961012 -4.2319021 -4.2616448 -4.2821164 -4.2897711 -4.2808876][-4.3070421 -4.2918673 -4.25915 -4.2275448 -4.2133074 -4.2014828 -4.1825624 -4.148109 -4.1299734 -4.1516337 -4.20196 -4.2491465 -4.2837963 -4.297894 -4.292388][-4.305943 -4.2979527 -4.2729516 -4.2432914 -4.2244554 -4.1979427 -4.1595 -4.1018462 -4.0771961 -4.1139765 -4.1810594 -4.2427 -4.289464 -4.3093519 -4.3048253][-4.2812991 -4.2771859 -4.2622852 -4.23813 -4.2137256 -4.1710486 -4.1113839 -4.0342393 -4.014327 -4.0727372 -4.1576018 -4.2287555 -4.2835512 -4.3072147 -4.3029413][-4.2423534 -4.2340159 -4.2293086 -4.2120938 -4.1847453 -4.1286044 -4.0479121 -3.9547138 -3.9424694 -4.0234108 -4.1274891 -4.2067747 -4.2648807 -4.2885838 -4.2800255][-4.1973343 -4.1804457 -4.1791592 -4.1709452 -4.1540594 -4.102663 -4.0120273 -3.9060063 -3.8995395 -3.99098 -4.1019549 -4.1814332 -4.2380509 -4.2612214 -4.2489257][-4.1621041 -4.1411638 -4.143209 -4.1491661 -4.1487589 -4.1148691 -4.0362196 -3.9415863 -3.9391346 -4.0184865 -4.1119714 -4.1742244 -4.2177906 -4.2355962 -4.2205243][-4.1499891 -4.1394043 -4.1480474 -4.1643457 -4.174962 -4.1580234 -4.104775 -4.0398068 -4.0433345 -4.0976605 -4.1586347 -4.1938515 -4.21482 -4.2208066 -4.2017026][-4.168889 -4.16466 -4.1723313 -4.1881175 -4.1991992 -4.1936269 -4.1629887 -4.1276894 -4.1352949 -4.1668382 -4.2000518 -4.21561 -4.2211876 -4.2166519 -4.1945186][-4.2014861 -4.1974611 -4.2021565 -4.2143974 -4.2260356 -4.2299781 -4.2159457 -4.1963706 -4.2016182 -4.2165837 -4.2332134 -4.2420049 -4.2458534 -4.2409258 -4.2207131][-4.219624 -4.2183743 -4.2265882 -4.2416129 -4.2560344 -4.2628622 -4.2556515 -4.2439318 -4.2457442 -4.2510357 -4.2620773 -4.27105 -4.2783365 -4.2774372 -4.2623544][-4.2299047 -4.2309623 -4.2411189 -4.2578735 -4.2743597 -4.2808003 -4.2752366 -4.2664819 -4.2654057 -4.2682543 -4.2793841 -4.2915716 -4.3038621 -4.3089695 -4.299006][-4.2380748 -4.240068 -4.2494345 -4.2636361 -4.2772307 -4.2821488 -4.2779069 -4.2714791 -4.2688446 -4.2705936 -4.2823615 -4.2971416 -4.3121448 -4.3206816 -4.3155551]]...]
INFO - root - 2017-12-07 14:20:25.078280: step 18610, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 57h:29m:28s remains)
INFO - root - 2017-12-07 14:20:31.771484: step 18620, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 62h:59m:44s remains)
INFO - root - 2017-12-07 14:20:38.595802: step 18630, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 63h:30m:16s remains)
INFO - root - 2017-12-07 14:20:45.349155: step 18640, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 56h:50m:47s remains)
INFO - root - 2017-12-07 14:20:52.217234: step 18650, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 55h:41m:18s remains)
INFO - root - 2017-12-07 14:20:59.054596: step 18660, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 61h:27m:58s remains)
INFO - root - 2017-12-07 14:21:05.958239: step 18670, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 61h:45m:42s remains)
INFO - root - 2017-12-07 14:21:12.821583: step 18680, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.682 sec/batch; 59h:24m:40s remains)
INFO - root - 2017-12-07 14:21:19.513190: step 18690, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 55h:16m:03s remains)
INFO - root - 2017-12-07 14:21:26.374763: step 18700, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 57h:20m:40s remains)
2017-12-07 14:21:27.032166: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2583904 -4.2329292 -4.2086864 -4.1724553 -4.1249828 -4.1269274 -4.1606379 -4.1996436 -4.2333755 -4.2396 -4.2220669 -4.1955209 -4.1668105 -4.1512256 -4.1471786][-4.2448697 -4.2145824 -4.1896477 -4.1489687 -4.0954766 -4.0880146 -4.1178331 -4.157548 -4.198451 -4.2168636 -4.2186632 -4.2034836 -4.1801581 -4.1623716 -4.152884][-4.2331958 -4.2034817 -4.1772327 -4.1340847 -4.0758996 -4.0542331 -4.0720549 -4.1083345 -4.1538839 -4.1842036 -4.2036338 -4.2029772 -4.1867647 -4.1685491 -4.1568975][-4.2243266 -4.199904 -4.1735754 -4.1324224 -4.0745196 -4.0399942 -4.0415678 -4.069665 -4.1187124 -4.1599178 -4.1941266 -4.2088313 -4.2012577 -4.1843376 -4.168251][-4.2218547 -4.2011237 -4.1789827 -4.1469488 -4.0997887 -4.0552435 -4.0304136 -4.0407472 -4.0879436 -4.1345596 -4.176662 -4.2038584 -4.2099724 -4.2015529 -4.1853423][-4.2236056 -4.1972141 -4.1745005 -4.147572 -4.1096029 -4.0655861 -4.0269766 -4.0251913 -4.0678763 -4.1136742 -4.1525259 -4.182858 -4.202085 -4.2038326 -4.1965566][-4.2337413 -4.2001934 -4.1712289 -4.1389656 -4.102634 -4.0638947 -4.0312052 -4.0262871 -4.055594 -4.0850563 -4.1061063 -4.1360273 -4.1723256 -4.1911712 -4.1985188][-4.2408648 -4.2118196 -4.1869607 -4.1571674 -4.1208839 -4.0880537 -4.0630412 -4.046771 -4.044858 -4.0406251 -4.0324345 -4.0526567 -4.1038837 -4.1487908 -4.1772833][-4.2409487 -4.2231021 -4.2101464 -4.1953354 -4.1747346 -4.1549687 -4.1298132 -4.0905428 -4.0479436 -4.002284 -3.9611974 -3.9677567 -4.0289645 -4.0963988 -4.1438503][-4.24605 -4.2386656 -4.2365479 -4.2367945 -4.2348018 -4.2287283 -4.204421 -4.1528678 -4.0900912 -4.0255461 -3.9747493 -3.9742293 -4.0300379 -4.0943089 -4.1396041][-4.2594213 -4.25998 -4.2643952 -4.2731214 -4.2808142 -4.280314 -4.2593946 -4.2140374 -4.15579 -4.1004596 -4.0612078 -4.0595207 -4.0949278 -4.1353617 -4.1606][-4.2784209 -4.2822051 -4.2875171 -4.2959814 -4.3042617 -4.3076596 -4.2941871 -4.2619681 -4.2174754 -4.1775832 -4.1540675 -4.1547661 -4.1753383 -4.1957841 -4.2027693][-4.2944527 -4.2997646 -4.3033757 -4.3075252 -4.3145328 -4.321166 -4.3143458 -4.295455 -4.2697868 -4.24493 -4.2293363 -4.2282143 -4.2371144 -4.2455549 -4.2436643][-4.3029251 -4.309371 -4.3107295 -4.3102913 -4.3135657 -4.3192348 -4.3176274 -4.3106394 -4.2997971 -4.2864304 -4.2754412 -4.2735076 -4.2745733 -4.2732191 -4.2658157][-4.3016443 -4.308579 -4.3101773 -4.3093209 -4.3105621 -4.3139658 -4.3134618 -4.3121638 -4.3087382 -4.3020453 -4.2938609 -4.2908597 -4.2895741 -4.2855349 -4.278698]]...]
INFO - root - 2017-12-07 14:21:33.846616: step 18710, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 59h:05m:48s remains)
INFO - root - 2017-12-07 14:21:40.589130: step 18720, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 56h:18m:32s remains)
INFO - root - 2017-12-07 14:21:47.435443: step 18730, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 56h:55m:23s remains)
INFO - root - 2017-12-07 14:21:54.292356: step 18740, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 60h:40m:13s remains)
INFO - root - 2017-12-07 14:22:01.100126: step 18750, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 61h:47m:59s remains)
INFO - root - 2017-12-07 14:22:07.960830: step 18760, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 56h:37m:43s remains)
INFO - root - 2017-12-07 14:22:14.809354: step 18770, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 58h:10m:32s remains)
INFO - root - 2017-12-07 14:22:21.638920: step 18780, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 61h:49m:15s remains)
INFO - root - 2017-12-07 14:22:28.461478: step 18790, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.724 sec/batch; 63h:06m:18s remains)
INFO - root - 2017-12-07 14:22:35.269601: step 18800, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 58h:24m:09s remains)
2017-12-07 14:22:36.011389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2307868 -4.2549009 -4.2771969 -4.2784495 -4.2630792 -4.2389388 -4.2212372 -4.2144876 -4.2050633 -4.1864152 -4.152914 -4.136097 -4.1303606 -4.1445017 -4.1763873][-4.2190394 -4.2354393 -4.2515211 -4.2467384 -4.2341766 -4.2204618 -4.2106471 -4.2054672 -4.1949129 -4.1797285 -4.1533446 -4.1401739 -4.1299834 -4.130754 -4.1479988][-4.2102189 -4.211669 -4.2168412 -4.2055249 -4.196826 -4.1905856 -4.1854434 -4.179214 -4.1689796 -4.164032 -4.1536555 -4.1475649 -4.1360254 -4.1279926 -4.1303267][-4.2095289 -4.1948266 -4.1884131 -4.1711354 -4.1598678 -4.1496687 -4.1403008 -4.1268859 -4.1173172 -4.12565 -4.1330853 -4.1396594 -4.1336513 -4.1272902 -4.123714][-4.219964 -4.1962175 -4.1846247 -4.1611142 -4.1356316 -4.1092148 -4.0884428 -4.06463 -4.050519 -4.0682464 -4.0917292 -4.1131124 -4.1190429 -4.1228108 -4.1296015][-4.2347307 -4.2150917 -4.20547 -4.1756444 -4.1301694 -4.0820107 -4.0400047 -3.9950781 -3.9650581 -3.9911299 -4.0355816 -4.0749097 -4.0984592 -4.1178956 -4.1378818][-4.2403426 -4.2365022 -4.2328434 -4.2010846 -4.1388922 -4.065063 -3.9917686 -3.9115233 -3.8504579 -3.8906054 -3.9726076 -4.040246 -4.0875463 -4.1261 -4.158216][-4.2365313 -4.2429824 -4.2440143 -4.216661 -4.1549196 -4.0724945 -3.9797294 -3.8734539 -3.7868249 -3.8406658 -3.9517903 -4.0361428 -4.096458 -4.145443 -4.1850338][-4.2382503 -4.2501059 -4.2520885 -4.2292552 -4.179152 -4.1159458 -4.0426674 -3.9614379 -3.8942547 -3.929657 -4.01244 -4.07489 -4.1180329 -4.1546187 -4.1893096][-4.2349758 -4.2518983 -4.2537293 -4.2335372 -4.1962113 -4.1586013 -4.1191154 -4.0782976 -4.0397673 -4.0516191 -4.0924277 -4.1195817 -4.1354513 -4.1548967 -4.1817565][-4.2175951 -4.2413321 -4.24538 -4.2318473 -4.209548 -4.191998 -4.1773357 -4.162971 -4.138422 -4.1312342 -4.1399441 -4.1424127 -4.1450315 -4.1573124 -4.1784692][-4.2089257 -4.2376246 -4.2452116 -4.2381568 -4.2291179 -4.225801 -4.224474 -4.2195554 -4.2007923 -4.1849089 -4.1750832 -4.1640739 -4.1596889 -4.1666565 -4.1807013][-4.2163353 -4.2479281 -4.2546139 -4.2487869 -4.2458878 -4.2510695 -4.2535424 -4.246953 -4.2324867 -4.2156591 -4.2002697 -4.1866584 -4.1862249 -4.1989636 -4.2126465][-4.2350879 -4.2657142 -4.2738581 -4.2713814 -4.2717052 -4.279119 -4.280086 -4.2678556 -4.2521896 -4.2352333 -4.2237735 -4.2172132 -4.2264166 -4.2489491 -4.2652516][-4.2594986 -4.2877803 -4.2949543 -4.29333 -4.295939 -4.3020973 -4.3015318 -4.2894573 -4.27731 -4.2673769 -4.26116 -4.2603164 -4.2698288 -4.2910471 -4.3028836]]...]
INFO - root - 2017-12-07 14:22:42.715266: step 18810, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 60h:43m:38s remains)
INFO - root - 2017-12-07 14:22:49.393974: step 18820, loss = 2.07, batch loss = 2.02 (10.6 examples/sec; 0.754 sec/batch; 65h:39m:44s remains)
INFO - root - 2017-12-07 14:22:56.172306: step 18830, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 61h:36m:49s remains)
INFO - root - 2017-12-07 14:23:02.888227: step 18840, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 62h:24m:56s remains)
INFO - root - 2017-12-07 14:23:09.870880: step 18850, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 61h:41m:35s remains)
INFO - root - 2017-12-07 14:23:16.653011: step 18860, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 57h:13m:52s remains)
INFO - root - 2017-12-07 14:23:23.352112: step 18870, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 55h:56m:16s remains)
INFO - root - 2017-12-07 14:23:30.132356: step 18880, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 59h:38m:34s remains)
INFO - root - 2017-12-07 14:23:36.895288: step 18890, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 61h:06m:07s remains)
INFO - root - 2017-12-07 14:23:43.796347: step 18900, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 61h:04m:45s remains)
2017-12-07 14:23:44.518903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026638 -4.3067656 -4.3160791 -4.3214121 -4.3134208 -4.2923293 -4.2750411 -4.2616482 -4.2608 -4.2708 -4.28582 -4.2994084 -4.3045812 -4.3019156 -4.2957191][-4.2725282 -4.2861834 -4.3072639 -4.3222575 -4.3150916 -4.2883658 -4.2615638 -4.2398095 -4.2362733 -4.2502131 -4.2747612 -4.2933779 -4.2971516 -4.2933865 -4.2875638][-4.2461233 -4.2648392 -4.2933655 -4.3114982 -4.3038249 -4.2739062 -4.2405295 -4.2115321 -4.2071152 -4.2245336 -4.2545257 -4.273097 -4.2751174 -4.2740211 -4.2742457][-4.2127309 -4.2280283 -4.2638292 -4.2902932 -4.2805843 -4.2474627 -4.2033935 -4.1661429 -4.16717 -4.1905432 -4.2232428 -4.2455277 -4.2521515 -4.2580023 -4.2694139][-4.1712976 -4.1745548 -4.2127881 -4.2459726 -4.2322292 -4.1972647 -4.1487989 -4.117908 -4.1333694 -4.1650167 -4.2028084 -4.2360115 -4.2510514 -4.26319 -4.2792711][-4.14286 -4.1244388 -4.1541772 -4.185873 -4.169704 -4.12586 -4.0715075 -4.0613794 -4.1029148 -4.1493607 -4.1922278 -4.2337718 -4.2540603 -4.26884 -4.2824831][-4.1482897 -4.1121683 -4.1166091 -4.1324177 -4.1070828 -4.03427 -3.9509137 -3.9656732 -4.06003 -4.1445794 -4.2003851 -4.2468085 -4.2698131 -4.2748609 -4.2791338][-4.1719503 -4.136054 -4.1269236 -4.119627 -4.0719857 -3.9559672 -3.819926 -3.8603117 -4.0260944 -4.1537209 -4.2269106 -4.2726073 -4.2899737 -4.2795954 -4.2712584][-4.1916518 -4.1616187 -4.148335 -4.1358361 -4.0910988 -3.9859302 -3.8663144 -3.8965888 -4.0489955 -4.1785388 -4.25298 -4.2933683 -4.3030357 -4.2826223 -4.2690291][-4.1908584 -4.1670818 -4.1530252 -4.1428761 -4.1179075 -4.0623465 -4.0003738 -4.0137725 -4.1066771 -4.199718 -4.2599664 -4.2980757 -4.3055015 -4.2839146 -4.2728047][-4.1811986 -4.1676722 -4.1611032 -4.1553626 -4.1429052 -4.1162915 -4.0901718 -4.1029916 -4.1604877 -4.217186 -4.2583895 -4.2885032 -4.2955785 -4.2807989 -4.2771535][-4.1705475 -4.169775 -4.1764851 -4.1765275 -4.170383 -4.1586404 -4.1521568 -4.1711755 -4.209249 -4.2406621 -4.2626023 -4.285152 -4.2897015 -4.2806735 -4.2846475][-4.1676178 -4.1778197 -4.1977682 -4.2083726 -4.2118478 -4.2118473 -4.2123928 -4.22532 -4.2430682 -4.2545147 -4.2618847 -4.2788053 -4.2832413 -4.27974 -4.2930098][-4.1797214 -4.1928644 -4.2173209 -4.2376962 -4.2546029 -4.2637725 -4.2651348 -4.26801 -4.2703123 -4.2654357 -4.263999 -4.2751141 -4.2804813 -4.2830305 -4.303287][-4.1963549 -4.2043028 -4.2294021 -4.2533531 -4.2765083 -4.2890453 -4.2896872 -4.2879524 -4.2865305 -4.2756805 -4.2658143 -4.2692356 -4.2753129 -4.2849865 -4.3081861]]...]
INFO - root - 2017-12-07 14:23:51.267923: step 18910, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 58h:34m:33s remains)
INFO - root - 2017-12-07 14:23:57.955337: step 18920, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 61h:37m:31s remains)
INFO - root - 2017-12-07 14:24:04.786144: step 18930, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 58h:43m:57s remains)
INFO - root - 2017-12-07 14:24:11.553149: step 18940, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 59h:13m:59s remains)
INFO - root - 2017-12-07 14:24:18.375908: step 18950, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.654 sec/batch; 56h:59m:13s remains)
INFO - root - 2017-12-07 14:24:25.227452: step 18960, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 61h:23m:23s remains)
INFO - root - 2017-12-07 14:24:32.110714: step 18970, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 62h:31m:26s remains)
INFO - root - 2017-12-07 14:24:39.023789: step 18980, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 59h:04m:56s remains)
INFO - root - 2017-12-07 14:24:45.809382: step 18990, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 56h:23m:28s remains)
INFO - root - 2017-12-07 14:24:52.509735: step 19000, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 56h:53m:48s remains)
2017-12-07 14:24:53.261307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2855959 -4.259141 -4.218143 -4.1588073 -4.1130323 -4.087615 -4.0729332 -4.0762997 -4.1034794 -4.1284914 -4.1462235 -4.1707067 -4.1924434 -4.2175679 -4.2434311][-4.2877445 -4.2620873 -4.2220564 -4.1628551 -4.1141176 -4.0876756 -4.0709896 -4.0660634 -4.08128 -4.09892 -4.1195693 -4.1548996 -4.188642 -4.222333 -4.24964][-4.2989297 -4.2794523 -4.2432508 -4.1886334 -4.1407223 -4.1092253 -4.0834789 -4.0617218 -4.0610385 -4.0716481 -4.1002316 -4.1461315 -4.1875186 -4.2257843 -4.2522383][-4.3089075 -4.2932062 -4.2589064 -4.207139 -4.158627 -4.1122932 -4.0679684 -4.0258055 -4.017077 -4.0326037 -4.0763855 -4.1376224 -4.1854444 -4.22372 -4.2492356][-4.3146453 -4.2977362 -4.2568746 -4.198523 -4.1381025 -4.0683355 -4.0009933 -3.9469335 -3.951242 -3.9913998 -4.0570278 -4.1338263 -4.1850667 -4.221282 -4.2450995][-4.3129635 -4.2890511 -4.2355447 -4.1615448 -4.0806975 -3.9789407 -3.8782368 -3.8230069 -3.8745811 -3.963623 -4.0544491 -4.1379819 -4.1883454 -4.2204041 -4.2413988][-4.304533 -4.2711883 -4.2026019 -4.1136823 -4.0108876 -3.8772988 -3.7403033 -3.696636 -3.8201976 -3.9613528 -4.06786 -4.1445341 -4.1863103 -4.2125654 -4.2325892][-4.2940845 -4.2518516 -4.1733384 -4.0759735 -3.9653215 -3.8302915 -3.7042089 -3.7025409 -3.8568068 -4.0017753 -4.0953193 -4.1541848 -4.1815958 -4.20401 -4.2266145][-4.2838387 -4.2391958 -4.16765 -4.0787873 -3.9874289 -3.8962774 -3.8317702 -3.8544798 -3.9709506 -4.0713096 -4.132432 -4.1712394 -4.1893516 -4.2125163 -4.2356834][-4.2770782 -4.2365789 -4.1802964 -4.1130204 -4.0537534 -4.0077596 -3.9796982 -4.0005589 -4.0775094 -4.1409903 -4.1766615 -4.2017303 -4.2173696 -4.2390761 -4.2577534][-4.2675228 -4.2307258 -4.1871214 -4.1413255 -4.1075449 -4.0848618 -4.0676823 -4.0825086 -4.1378207 -4.1829748 -4.2091374 -4.2303576 -4.2459292 -4.2642183 -4.2767487][-4.2571125 -4.2188869 -4.1820259 -4.1520686 -4.1338525 -4.1221023 -4.1085129 -4.1197319 -4.1661849 -4.2071567 -4.2302589 -4.2487335 -4.2634926 -4.2772431 -4.2855606][-4.2479882 -4.2070384 -4.1710362 -4.1491709 -4.1422949 -4.1409888 -4.134829 -4.1465211 -4.1886268 -4.2246757 -4.2448659 -4.2615571 -4.27413 -4.2833276 -4.2893705][-4.2444334 -4.20352 -4.1697645 -4.1522064 -4.1515346 -4.1593075 -4.1613693 -4.1750112 -4.211472 -4.2426252 -4.2602038 -4.2732034 -4.2824593 -4.2891321 -4.296308][-4.248364 -4.2096844 -4.1805735 -4.1672783 -4.1693897 -4.1810718 -4.188426 -4.2024608 -4.2307014 -4.2553105 -4.2688303 -4.2797408 -4.2890115 -4.2977161 -4.3061905]]...]
INFO - root - 2017-12-07 14:24:59.925603: step 19010, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 60h:51m:09s remains)
INFO - root - 2017-12-07 14:25:06.611304: step 19020, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.620 sec/batch; 54h:00m:09s remains)
INFO - root - 2017-12-07 14:25:13.511965: step 19030, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 60h:36m:05s remains)
INFO - root - 2017-12-07 14:25:20.407608: step 19040, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 59h:30m:33s remains)
INFO - root - 2017-12-07 14:25:27.193677: step 19050, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 59h:06m:29s remains)
INFO - root - 2017-12-07 14:25:33.984949: step 19060, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 56h:25m:05s remains)
INFO - root - 2017-12-07 14:25:40.820379: step 19070, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 59h:43m:45s remains)
INFO - root - 2017-12-07 14:25:47.633408: step 19080, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 60h:59m:31s remains)
INFO - root - 2017-12-07 14:25:54.562187: step 19090, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 59h:53m:13s remains)
INFO - root - 2017-12-07 14:26:01.397920: step 19100, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 57h:09m:45s remains)
2017-12-07 14:26:02.248406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3107643 -4.306253 -4.3020105 -4.3011041 -4.3027992 -4.3046775 -4.3053193 -4.3061228 -4.3088536 -4.3147902 -4.3214793 -4.3279572 -4.3346996 -4.3388329 -4.3417559][-4.296175 -4.2898107 -4.2830009 -4.2792492 -4.2788525 -4.2789259 -4.275466 -4.2731643 -4.2770915 -4.2868118 -4.2967768 -4.3074927 -4.3200936 -4.3276334 -4.33149][-4.2841711 -4.2729592 -4.2634 -4.2576208 -4.2569022 -4.2565556 -4.2482615 -4.2407641 -4.2444062 -4.2575784 -4.2697883 -4.2847691 -4.3034968 -4.3148427 -4.3196278][-4.2653561 -4.24771 -4.235034 -4.2270856 -4.2270479 -4.2269349 -4.2170238 -4.2067776 -4.2119589 -4.2263284 -4.2385345 -4.2580075 -4.2820024 -4.2980461 -4.3061795][-4.2392349 -4.217577 -4.2023616 -4.1924624 -4.1940665 -4.1910238 -4.1766024 -4.1647348 -4.17064 -4.1867018 -4.2018633 -4.2274294 -4.2561722 -4.2781539 -4.291296][-4.2048297 -4.1799498 -4.160759 -4.1506624 -4.1529593 -4.143486 -4.1200638 -4.1073985 -4.1191134 -4.1436009 -4.1688137 -4.2038765 -4.2363453 -4.2597952 -4.2758913][-4.1645021 -4.1319418 -4.1066861 -4.0956187 -4.0961356 -4.0806971 -4.0453329 -4.030817 -4.0568743 -4.1001825 -4.1424966 -4.1875119 -4.219811 -4.24318 -4.2619195][-4.1308732 -4.0851564 -4.0442839 -4.0207992 -4.0096269 -3.983464 -3.9477851 -3.9499979 -4.0021229 -4.0660939 -4.1237812 -4.1758795 -4.2116914 -4.2369919 -4.2590356][-4.1293793 -4.0682874 -4.0101862 -3.9733503 -3.9482081 -3.9147136 -3.8979223 -3.926029 -3.9947238 -4.0678859 -4.1291852 -4.1827545 -4.221839 -4.2476029 -4.2688484][-4.1566525 -4.0949907 -4.037962 -4.00367 -3.9792714 -3.9502203 -3.9479399 -3.9822545 -4.0432358 -4.1086335 -4.1618047 -4.2088456 -4.248601 -4.2733955 -4.2900758][-4.1865487 -4.1372266 -4.0978236 -4.0830159 -4.0700512 -4.0485048 -4.0492578 -4.0735688 -4.1165557 -4.1667614 -4.209549 -4.2521482 -4.2878342 -4.306695 -4.3169074][-4.2320151 -4.1990972 -4.1797366 -4.1815658 -4.17795 -4.16614 -4.1670804 -4.1783276 -4.2030344 -4.2372928 -4.2703319 -4.3050227 -4.3302083 -4.3405118 -4.342514][-4.2857533 -4.2700276 -4.2661738 -4.2738252 -4.275413 -4.2712426 -4.273448 -4.2760019 -4.2829032 -4.2998114 -4.3202462 -4.3419237 -4.3553357 -4.3597026 -4.3587232][-4.3150892 -4.3097634 -4.3130331 -4.3237796 -4.3306503 -4.333663 -4.3370156 -4.3348455 -4.331151 -4.3348064 -4.3428 -4.3529682 -4.3612494 -4.3663535 -4.3668208][-4.3238249 -4.3217936 -4.3261714 -4.3363285 -4.3439255 -4.348021 -4.3506484 -4.3493595 -4.3461428 -4.3457994 -4.3483129 -4.3531528 -4.3590493 -4.3642411 -4.366334]]...]
INFO - root - 2017-12-07 14:26:08.969991: step 19110, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 57h:33m:21s remains)
INFO - root - 2017-12-07 14:26:15.680723: step 19120, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 60h:49m:48s remains)
INFO - root - 2017-12-07 14:26:22.371409: step 19130, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 57h:11m:57s remains)
INFO - root - 2017-12-07 14:26:29.217979: step 19140, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 59h:57m:12s remains)
INFO - root - 2017-12-07 14:26:35.843353: step 19150, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 61h:29m:17s remains)
INFO - root - 2017-12-07 14:26:42.583871: step 19160, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 59h:17m:46s remains)
INFO - root - 2017-12-07 14:26:49.323351: step 19170, loss = 2.04, batch loss = 1.99 (13.1 examples/sec; 0.610 sec/batch; 53h:05m:05s remains)
INFO - root - 2017-12-07 14:26:56.253736: step 19180, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 61h:31m:50s remains)
INFO - root - 2017-12-07 14:27:03.158034: step 19190, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 60h:08m:45s remains)
INFO - root - 2017-12-07 14:27:10.045392: step 19200, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.722 sec/batch; 62h:49m:19s remains)
2017-12-07 14:27:10.729505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3482757 -4.344183 -4.3386474 -4.3315048 -4.3261852 -4.3220034 -4.3198662 -4.3206673 -4.3240972 -4.3285031 -4.3336167 -4.3377848 -4.3392549 -4.3413653 -4.3463593][-4.3305578 -4.3214054 -4.3132687 -4.3042827 -4.2988672 -4.2936988 -4.2900443 -4.2879081 -4.2915483 -4.2962527 -4.304028 -4.3125157 -4.3163576 -4.319829 -4.3286095][-4.30645 -4.2930751 -4.2831717 -4.2773933 -4.2762294 -4.2717905 -4.2648578 -4.2586513 -4.2629418 -4.2678 -4.2777219 -4.2910743 -4.2997303 -4.3051171 -4.3148351][-4.2903433 -4.2749081 -4.2656074 -4.2659988 -4.2740879 -4.2716789 -4.2602882 -4.2513857 -4.2595868 -4.2652516 -4.2742414 -4.2873783 -4.296454 -4.2995586 -4.3083034][-4.2834849 -4.2683978 -4.2606215 -4.2635131 -4.2788725 -4.2834625 -4.2688088 -4.2563872 -4.2738843 -4.2871141 -4.2939405 -4.298501 -4.2991524 -4.2973928 -4.3026237][-4.2755837 -4.2570167 -4.2469 -4.2459707 -4.2618093 -4.268024 -4.2512264 -4.2336087 -4.2607756 -4.2819219 -4.2883534 -4.2850409 -4.2756653 -4.2700119 -4.2768369][-4.2562113 -4.2264509 -4.202775 -4.1883416 -4.1936622 -4.1938605 -4.1703315 -4.142509 -4.1743789 -4.2018113 -4.2165165 -4.2184877 -4.2092986 -4.2093415 -4.2301555][-4.2286038 -4.181 -4.1328278 -4.0949597 -4.0810151 -4.0667553 -4.0299296 -3.9883826 -4.0171943 -4.0540876 -4.0934815 -4.1191468 -4.1248465 -4.1427317 -4.1825562][-4.2034225 -4.1416974 -4.0731144 -4.0129657 -3.9812543 -3.9516847 -3.9016113 -3.8493385 -3.8728254 -3.920783 -3.993778 -4.0563812 -4.0877018 -4.1223793 -4.1729555][-4.2046518 -4.1449046 -4.0804372 -4.0244346 -3.9932847 -3.963094 -3.9201317 -3.8815031 -3.9103713 -3.9571681 -4.0294566 -4.0989294 -4.1381855 -4.1740212 -4.2153959][-4.2333937 -4.1906471 -4.1482835 -4.1174822 -4.1040936 -4.0894437 -4.0644846 -4.0447497 -4.0717678 -4.1046591 -4.1524053 -4.2047582 -4.2355776 -4.2573552 -4.27723][-4.27522 -4.2521524 -4.2363405 -4.2300491 -4.2326775 -4.2304564 -4.2185955 -4.2091832 -4.2246308 -4.2383237 -4.2616911 -4.2918425 -4.3090835 -4.3159981 -4.3190322][-4.313971 -4.302412 -4.301178 -4.3038325 -4.309144 -4.31011 -4.3048873 -4.3006363 -4.3060632 -4.3089104 -4.3187671 -4.3328791 -4.340488 -4.3396707 -4.3372288][-4.3380995 -4.3299179 -4.3309298 -4.3337512 -4.3349643 -4.3336878 -4.3302984 -4.327168 -4.3278761 -4.3268361 -4.3309669 -4.3389573 -4.3453531 -4.3454208 -4.3437][-4.354187 -4.3490896 -4.3482285 -4.3488154 -4.3480458 -4.3465686 -4.343514 -4.3398485 -4.3370419 -4.3345671 -4.3362832 -4.3405647 -4.3455753 -4.3480077 -4.3488631]]...]
INFO - root - 2017-12-07 14:27:17.485338: step 19210, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 60h:40m:45s remains)
INFO - root - 2017-12-07 14:27:24.280778: step 19220, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 63h:18m:53s remains)
INFO - root - 2017-12-07 14:27:31.174451: step 19230, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.681 sec/batch; 59h:17m:28s remains)
INFO - root - 2017-12-07 14:27:37.957826: step 19240, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 56h:17m:06s remains)
INFO - root - 2017-12-07 14:27:44.851019: step 19250, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.662 sec/batch; 57h:35m:18s remains)
INFO - root - 2017-12-07 14:27:51.625718: step 19260, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 56h:05m:39s remains)
INFO - root - 2017-12-07 14:27:58.462596: step 19270, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 61h:24m:05s remains)
INFO - root - 2017-12-07 14:28:05.321819: step 19280, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 55h:28m:37s remains)
INFO - root - 2017-12-07 14:28:12.184633: step 19290, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 58h:18m:28s remains)
INFO - root - 2017-12-07 14:28:18.910817: step 19300, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 60h:47m:17s remains)
2017-12-07 14:28:19.741948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2277131 -4.25681 -4.2726307 -4.2732353 -4.2638969 -4.2533083 -4.2486939 -4.2499237 -4.2566824 -4.2648535 -4.2690554 -4.2676148 -4.2627692 -4.2570591 -4.2514524][-4.1922765 -4.23396 -4.258409 -4.2629056 -4.2519183 -4.23572 -4.2238731 -4.2194719 -4.2265716 -4.24058 -4.2520804 -4.2563429 -4.2556109 -4.2511039 -4.2441611][-4.161314 -4.2058606 -4.2340488 -4.2399373 -4.2246656 -4.197062 -4.168983 -4.1516819 -4.1580515 -4.1839504 -4.2116518 -4.2297926 -4.2396054 -4.240293 -4.2343326][-4.1565289 -4.195374 -4.2168021 -4.2177095 -4.1928487 -4.1471882 -4.0948491 -4.0588565 -4.0655384 -4.1095963 -4.1583681 -4.1936207 -4.2158713 -4.2238278 -4.2218904][-4.1719279 -4.201745 -4.213975 -4.2074981 -4.1714773 -4.1065636 -4.0297108 -3.9769578 -3.9880147 -4.0494261 -4.1153975 -4.1656423 -4.199141 -4.2143049 -4.2157903][-4.178721 -4.2042117 -4.2146506 -4.2040834 -4.1608229 -4.0850163 -3.9963412 -3.9398673 -3.9590232 -4.0277877 -4.0950489 -4.1451964 -4.1786804 -4.194664 -4.1962266][-4.1649561 -4.1890092 -4.2056751 -4.2002172 -4.1578088 -4.0843287 -4.0021176 -3.9572935 -3.981596 -4.0431461 -4.093977 -4.1271448 -4.1467061 -4.1549973 -4.1550775][-4.1567669 -4.1822 -4.2066422 -4.2093349 -4.1742735 -4.1156774 -4.0532928 -4.0239468 -4.0457382 -4.0880146 -4.1149387 -4.1274624 -4.1316657 -4.1302657 -4.1263638][-4.1734843 -4.1972036 -4.2213092 -4.2266378 -4.2019048 -4.1632824 -4.122622 -4.1027551 -4.11405 -4.1347842 -4.1407852 -4.1384163 -4.1344242 -4.130404 -4.1228151][-4.2017732 -4.2178993 -4.2344627 -4.238009 -4.224041 -4.20307 -4.1763659 -4.1589346 -4.1605606 -4.1669388 -4.1626472 -4.1568828 -4.1537228 -4.1530113 -4.1458111][-4.22623 -4.234457 -4.2451916 -4.245831 -4.2385054 -4.2272139 -4.209178 -4.1927366 -4.1914058 -4.1942978 -4.1901979 -4.1872115 -4.1883259 -4.1920705 -4.1870041][-4.2426796 -4.2475638 -4.2545109 -4.2552609 -4.2532654 -4.2479019 -4.2349586 -4.2179704 -4.2145324 -4.2172341 -4.2154489 -4.213748 -4.21725 -4.2243524 -4.2250137][-4.2672911 -4.2731919 -4.2770205 -4.2773938 -4.278049 -4.2742276 -4.2611551 -4.2426028 -4.2360988 -4.2378459 -4.2368746 -4.2343011 -4.2375546 -4.2464123 -4.2519875][-4.293148 -4.3004165 -4.3022337 -4.3021255 -4.3038578 -4.300952 -4.288856 -4.2719712 -4.2635212 -4.26299 -4.2593637 -4.2544723 -4.2553992 -4.263083 -4.2691827][-4.3125324 -4.3173904 -4.3178186 -4.3179479 -4.3208466 -4.319046 -4.310586 -4.2982178 -4.2903156 -4.2867765 -4.2791786 -4.2701426 -4.2656093 -4.2663922 -4.26853]]...]
INFO - root - 2017-12-07 14:28:26.449540: step 19310, loss = 2.07, batch loss = 2.01 (14.2 examples/sec; 0.565 sec/batch; 49h:08m:59s remains)
INFO - root - 2017-12-07 14:28:33.080997: step 19320, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 57h:01m:22s remains)
INFO - root - 2017-12-07 14:28:39.881408: step 19330, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 62h:14m:54s remains)
INFO - root - 2017-12-07 14:28:46.631122: step 19340, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 62h:30m:44s remains)
INFO - root - 2017-12-07 14:28:53.425326: step 19350, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 61h:15m:07s remains)
INFO - root - 2017-12-07 14:29:00.085412: step 19360, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 57h:36m:01s remains)
INFO - root - 2017-12-07 14:29:06.955462: step 19370, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 58h:45m:20s remains)
INFO - root - 2017-12-07 14:29:13.775997: step 19380, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 61h:28m:00s remains)
INFO - root - 2017-12-07 14:29:20.490489: step 19390, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 58h:34m:24s remains)
INFO - root - 2017-12-07 14:29:27.295145: step 19400, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 57h:32m:25s remains)
2017-12-07 14:29:28.046210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1998639 -4.207583 -4.2271 -4.2310038 -4.2195535 -4.2080255 -4.1964951 -4.17855 -4.1727519 -4.1668429 -4.1560211 -4.1514969 -4.1591468 -4.1728334 -4.1779246][-4.1897864 -4.2056627 -4.2286348 -4.236413 -4.2282381 -4.2160974 -4.2084217 -4.1942286 -4.1894069 -4.1849952 -4.1779423 -4.172318 -4.1735315 -4.1748261 -4.1661186][-4.174809 -4.202518 -4.2271247 -4.2348018 -4.2267995 -4.2098136 -4.2003689 -4.1909146 -4.1905794 -4.1931267 -4.1960011 -4.1922116 -4.1834197 -4.1662793 -4.1393661][-4.1628575 -4.1920624 -4.2176342 -4.2277532 -4.2210059 -4.1984239 -4.1836529 -4.1764059 -4.1770234 -4.1876206 -4.2081008 -4.2193575 -4.2148581 -4.1905718 -4.1550727][-4.1821733 -4.1948223 -4.2037177 -4.2056246 -4.1906133 -4.1622829 -4.1444216 -4.1420674 -4.1458879 -4.164639 -4.2036834 -4.2351613 -4.2441339 -4.2292867 -4.2049303][-4.2075696 -4.1998553 -4.18889 -4.1753654 -4.146842 -4.108079 -4.0827336 -4.0824556 -4.0937076 -4.1239829 -4.1783423 -4.2273936 -4.2534986 -4.2548423 -4.2468348][-4.2198105 -4.1983252 -4.1761007 -4.1492233 -4.1073427 -4.0530386 -4.0121794 -4.0080285 -4.0307198 -4.075418 -4.1414123 -4.20392 -4.2485256 -4.2687597 -4.2720876][-4.2209158 -4.1942916 -4.1671352 -4.1319442 -4.0783281 -4.0066662 -3.9381359 -3.9202766 -3.9544857 -4.019474 -4.0981097 -4.168241 -4.2249889 -4.2580361 -4.2678409][-4.21877 -4.1921172 -4.1664319 -4.1351681 -4.0847144 -4.0127797 -3.9289052 -3.8912401 -3.9215724 -3.99358 -4.076458 -4.1488771 -4.2058024 -4.2387185 -4.2471189][-4.2319756 -4.2043486 -4.1879444 -4.1716647 -4.1415272 -4.094451 -4.0320621 -3.9940982 -4.0069194 -4.05635 -4.1211352 -4.1810365 -4.224607 -4.2463746 -4.2490115][-4.2597404 -4.2349157 -4.22514 -4.2221956 -4.2085261 -4.1833754 -4.1481509 -4.1230254 -4.1272168 -4.15489 -4.19631 -4.2388182 -4.2695775 -4.2813578 -4.2774386][-4.2909989 -4.2718229 -4.265337 -4.2663789 -4.2600684 -4.2474689 -4.2315187 -4.2197781 -4.2215018 -4.2353387 -4.2583847 -4.2850561 -4.3047686 -4.3107491 -4.3065081][-4.31892 -4.3060346 -4.3013363 -4.3030996 -4.2999959 -4.2935987 -4.2878704 -4.2846489 -4.2836561 -4.2879496 -4.2990661 -4.3136611 -4.3254552 -4.3298426 -4.3285675][-4.3380413 -4.3303094 -4.3269124 -4.3259554 -4.3228111 -4.3190804 -4.3159895 -4.3150516 -4.314292 -4.3155675 -4.322331 -4.3317261 -4.3399458 -4.3443942 -4.3448944][-4.3459907 -4.3420572 -4.3399978 -4.337842 -4.33462 -4.3322244 -4.3305545 -4.3306746 -4.3312449 -4.3327937 -4.3375664 -4.343379 -4.3480396 -4.3509722 -4.3508549]]...]
INFO - root - 2017-12-07 14:29:34.798138: step 19410, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 59h:39m:48s remains)
INFO - root - 2017-12-07 14:29:41.390789: step 19420, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 60h:05m:25s remains)
INFO - root - 2017-12-07 14:29:48.237338: step 19430, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 60h:57m:48s remains)
INFO - root - 2017-12-07 14:29:55.016805: step 19440, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 57h:18m:45s remains)
INFO - root - 2017-12-07 14:30:01.783186: step 19450, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 62h:40m:56s remains)
INFO - root - 2017-12-07 14:30:08.379067: step 19460, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 62h:11m:34s remains)
INFO - root - 2017-12-07 14:30:15.318301: step 19470, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 60h:05m:55s remains)
INFO - root - 2017-12-07 14:30:22.044211: step 19480, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 56h:25m:02s remains)
INFO - root - 2017-12-07 14:30:28.764287: step 19490, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 59h:02m:00s remains)
INFO - root - 2017-12-07 14:30:35.540214: step 19500, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 61h:14m:52s remains)
2017-12-07 14:30:36.272544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1931376 -4.1854043 -4.1774282 -4.1640224 -4.1504011 -4.1507936 -4.165956 -4.1825256 -4.1924286 -4.1826248 -4.16258 -4.1465893 -4.1496449 -4.1567173 -4.1626396][-4.1974421 -4.1928649 -4.1912332 -4.1829228 -4.1696615 -4.1676164 -4.1811328 -4.1950951 -4.1983614 -4.1877365 -4.1696658 -4.159441 -4.1639295 -4.1712461 -4.168622][-4.2120848 -4.2155728 -4.2220039 -4.2213383 -4.2113566 -4.2072306 -4.2135506 -4.2145267 -4.2060509 -4.19679 -4.1856151 -4.1783118 -4.1758952 -4.1742516 -4.1618505][-4.2419114 -4.2521009 -4.2625833 -4.2636824 -4.2518497 -4.2422204 -4.238224 -4.2226295 -4.20219 -4.1926742 -4.18986 -4.1859446 -4.1763053 -4.1599078 -4.1363764][-4.2670732 -4.2791262 -4.2868719 -4.2851157 -4.2682347 -4.2503796 -4.2342639 -4.2069383 -4.1801796 -4.1738524 -4.181262 -4.1836834 -4.1682367 -4.1383753 -4.1079721][-4.273016 -4.2791481 -4.2796173 -4.2729211 -4.2523508 -4.2265739 -4.1986146 -4.1644959 -4.1388574 -4.1383524 -4.1583624 -4.1729097 -4.1634669 -4.13428 -4.1032329][-4.2563734 -4.2520194 -4.2421312 -4.2288475 -4.2059455 -4.1744933 -4.1375089 -4.1010895 -4.08495 -4.0985079 -4.1341753 -4.1638288 -4.1684632 -4.1500196 -4.1263485][-4.2227964 -4.206933 -4.1875944 -4.1675029 -4.141448 -4.1062684 -4.0674295 -4.0397139 -4.0422854 -4.0746431 -4.1228938 -4.1591411 -4.1719685 -4.1658521 -4.1535912][-4.1788578 -4.1519165 -4.1271157 -4.1065745 -4.0854807 -4.0570908 -4.0302591 -4.0195389 -4.038456 -4.07684 -4.1216254 -4.1541572 -4.1662016 -4.1670551 -4.1640539][-4.1280384 -4.0958495 -4.0765529 -4.0688324 -4.0641484 -4.0528507 -4.0444136 -4.0491958 -4.0725479 -4.103395 -4.1338444 -4.1533866 -4.1578689 -4.1590009 -4.1628923][-4.0928154 -4.0690689 -4.0670385 -4.0757604 -4.0832734 -4.0827684 -4.08083 -4.0889597 -4.1107526 -4.1337752 -4.1515737 -4.1590614 -4.1542144 -4.1534371 -4.1598091][-4.0921645 -4.0806565 -4.0896821 -4.1041832 -4.1132112 -4.1132421 -4.1095543 -4.1144009 -4.1330967 -4.1544151 -4.167901 -4.170011 -4.1623173 -4.1601462 -4.1658058][-4.1235747 -4.1132073 -4.1177611 -4.1258955 -4.1298542 -4.1303349 -4.1286011 -4.132443 -4.1482391 -4.1689281 -4.1819367 -4.1836576 -4.1765575 -4.1758642 -4.1813364][-4.1632829 -4.1440716 -4.1337743 -4.1305637 -4.1308088 -4.135807 -4.1404715 -4.1459513 -4.1600971 -4.1797328 -4.1939082 -4.1956892 -4.1904459 -4.1902137 -4.1947045][-4.1903982 -4.1621437 -4.1378546 -4.1262465 -4.1261749 -4.1345558 -4.1434422 -4.1508079 -4.1646576 -4.18339 -4.1985407 -4.2009373 -4.1963539 -4.1962304 -4.2010942]]...]
INFO - root - 2017-12-07 14:30:43.011874: step 19510, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.648 sec/batch; 56h:18m:58s remains)
INFO - root - 2017-12-07 14:30:49.659950: step 19520, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 55h:21m:33s remains)
INFO - root - 2017-12-07 14:30:56.488109: step 19530, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 62h:33m:21s remains)
INFO - root - 2017-12-07 14:31:03.299263: step 19540, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 62h:38m:28s remains)
INFO - root - 2017-12-07 14:31:10.187914: step 19550, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 57h:26m:19s remains)
INFO - root - 2017-12-07 14:31:17.035523: step 19560, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 56h:02m:11s remains)
INFO - root - 2017-12-07 14:31:23.924821: step 19570, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 62h:00m:01s remains)
INFO - root - 2017-12-07 14:31:30.699972: step 19580, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 60h:01m:38s remains)
INFO - root - 2017-12-07 14:31:37.519790: step 19590, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 60h:17m:04s remains)
INFO - root - 2017-12-07 14:31:44.415453: step 19600, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 55h:43m:31s remains)
2017-12-07 14:31:45.142595: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3271608 -4.3259497 -4.3253837 -4.3278842 -4.3316188 -4.32632 -4.3111706 -4.2877336 -4.2638741 -4.2432346 -4.235599 -4.2440472 -4.2612414 -4.2743907 -4.2832823][-4.3265886 -4.3220654 -4.31863 -4.3204951 -4.325376 -4.3177719 -4.2978659 -4.265913 -4.236351 -4.2140079 -4.2035036 -4.2088428 -4.2253351 -4.241643 -4.2533517][-4.3159046 -4.3071017 -4.3003182 -4.2999225 -4.3035903 -4.2914548 -4.2677293 -4.2284894 -4.1945686 -4.1776137 -4.1713095 -4.177166 -4.1911235 -4.2109914 -4.2247853][-4.2885418 -4.2759347 -4.2657356 -4.262794 -4.2638 -4.2492008 -4.2258477 -4.1861672 -4.1541619 -4.1526518 -4.1597204 -4.1670456 -4.1713972 -4.183702 -4.1945443][-4.2527781 -4.2317524 -4.2134018 -4.2039142 -4.2003713 -4.1859989 -4.1673222 -4.128437 -4.0990872 -4.1245012 -4.1582017 -4.1685677 -4.1630139 -4.1635342 -4.1700816][-4.2163572 -4.1844091 -4.1518817 -4.1284323 -4.1128974 -4.0934744 -4.0763936 -4.0303221 -4.0021324 -4.06734 -4.1417875 -4.1645036 -4.1598053 -4.1594706 -4.1623535][-4.185194 -4.1429343 -4.0931768 -4.0495377 -4.01611 -3.9798453 -3.948343 -3.8826733 -3.8609862 -3.9802525 -4.1074018 -4.1530585 -4.1615167 -4.1692314 -4.1692386][-4.1635323 -4.120739 -4.0618863 -4.0014758 -3.9461234 -3.8836293 -3.8240209 -3.7323329 -3.7183869 -3.8835127 -4.051549 -4.1209412 -4.1441097 -4.1629181 -4.1675038][-4.1629677 -4.1275473 -4.0731769 -4.0141568 -3.9573569 -3.8889961 -3.813374 -3.7131419 -3.6976886 -3.8456569 -4.0044093 -4.0826645 -4.1165509 -4.1415815 -4.1572933][-4.19548 -4.1708703 -4.1317234 -4.0862556 -4.0430903 -3.9846613 -3.9103031 -3.8162608 -3.7819827 -3.8655138 -3.9792881 -4.0524683 -4.0917459 -4.12032 -4.1512971][-4.2509747 -4.2340722 -4.2083869 -4.1773539 -4.1470695 -4.1016645 -4.0378919 -3.9603829 -3.9180598 -3.9475698 -4.0171728 -4.0776491 -4.1172342 -4.1460104 -4.1827374][-4.3028874 -4.2906337 -4.2726583 -4.2543221 -4.23585 -4.2043371 -4.1569567 -4.1057067 -4.0757809 -4.0841594 -4.1249418 -4.1714139 -4.2054749 -4.2257938 -4.2492642][-4.3356586 -4.3282895 -4.3177977 -4.3080707 -4.2971568 -4.2754874 -4.242485 -4.2122769 -4.1995192 -4.2102795 -4.2397189 -4.2731957 -4.2973542 -4.3059 -4.3129325][-4.3466845 -4.3432488 -4.3373437 -4.3325233 -4.3265634 -4.3121276 -4.2885985 -4.2705398 -4.2700472 -4.2834353 -4.3055348 -4.3288255 -4.3437643 -4.34576 -4.3437138][-4.3401589 -4.3402424 -4.3375583 -4.3350139 -4.3317618 -4.3232255 -4.3077374 -4.2956409 -4.295989 -4.3066473 -4.322619 -4.3368368 -4.34553 -4.3452549 -4.3418927]]...]
INFO - root - 2017-12-07 14:31:51.849659: step 19610, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 59h:06m:49s remains)
INFO - root - 2017-12-07 14:31:58.405675: step 19620, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 61h:30m:24s remains)
INFO - root - 2017-12-07 14:32:05.138014: step 19630, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 55h:25m:34s remains)
INFO - root - 2017-12-07 14:32:11.894620: step 19640, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 59h:01m:48s remains)
INFO - root - 2017-12-07 14:32:18.526544: step 19650, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 60h:44m:39s remains)
INFO - root - 2017-12-07 14:32:25.306730: step 19660, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 61h:26m:58s remains)
INFO - root - 2017-12-07 14:32:32.035935: step 19670, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 57h:56m:58s remains)
INFO - root - 2017-12-07 14:32:38.785097: step 19680, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 56h:36m:56s remains)
INFO - root - 2017-12-07 14:32:45.504120: step 19690, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 55h:41m:57s remains)
INFO - root - 2017-12-07 14:32:52.282103: step 19700, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 61h:02m:57s remains)
2017-12-07 14:32:53.002515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2792721 -4.2489214 -4.2117543 -4.178287 -4.1644568 -4.1758604 -4.1872444 -4.176959 -4.14918 -4.13498 -4.1513686 -4.1773133 -4.1912994 -4.2066727 -4.2257466][-4.2943668 -4.269031 -4.2336483 -4.1969733 -4.1714826 -4.169302 -4.1743011 -4.1607184 -4.1299829 -4.1241827 -4.1566291 -4.1918588 -4.208971 -4.224257 -4.2446017][-4.3079915 -4.2896762 -4.2554541 -4.21209 -4.1759872 -4.1610231 -4.1585517 -4.1431303 -4.1125178 -4.1130762 -4.1534619 -4.1940112 -4.2202916 -4.2404985 -4.2615767][-4.3166552 -4.3048167 -4.2711229 -4.2217469 -4.1782928 -4.1555333 -4.1472588 -4.1317649 -4.1036873 -4.1057606 -4.14477 -4.1891832 -4.2255797 -4.2496052 -4.2680106][-4.3160481 -4.3078008 -4.2762613 -4.2276697 -4.1800966 -4.1520872 -4.1448722 -4.1382422 -4.1211052 -4.1213059 -4.1493049 -4.1915069 -4.2296333 -4.2541919 -4.2694254][-4.3034606 -4.2944503 -4.2635865 -4.2164245 -4.1680489 -4.1405449 -4.1395912 -4.1480021 -4.1484327 -4.1520524 -4.1688385 -4.2011294 -4.2382994 -4.2644482 -4.2764068][-4.2799006 -4.2658572 -4.2300062 -4.1818471 -4.1374607 -4.1159768 -4.1230478 -4.1442685 -4.1631131 -4.1770062 -4.1906657 -4.2143788 -4.2519846 -4.2787066 -4.2873383][-4.2523546 -4.2288575 -4.1843877 -4.1327353 -4.0898781 -4.0751181 -4.0948663 -4.1303649 -4.166944 -4.1953731 -4.2142682 -4.235713 -4.2677822 -4.2901115 -4.2937489][-4.2243271 -4.193028 -4.1450419 -4.0949588 -4.0556235 -4.0493217 -4.0822096 -4.1264257 -4.1705179 -4.2095675 -4.2376008 -4.2594194 -4.2806873 -4.2936163 -4.29306][-4.2060981 -4.1726456 -4.1311536 -4.0898008 -4.0576258 -4.0600142 -4.0971751 -4.1394248 -4.1836891 -4.2263575 -4.2570105 -4.2742963 -4.2835326 -4.2860761 -4.2843642][-4.2070007 -4.1749158 -4.1407895 -4.1072268 -4.0834608 -4.0907812 -4.1246209 -4.1647019 -4.2093959 -4.2490349 -4.2711225 -4.2772694 -4.2758141 -4.2722473 -4.2715845][-4.2196107 -4.1945229 -4.165484 -4.1380806 -4.1194615 -4.1230388 -4.1490116 -4.1902528 -4.2349906 -4.2673478 -4.2786865 -4.2763791 -4.26862 -4.2626157 -4.2650709][-4.2332435 -4.21756 -4.1989617 -4.1782908 -4.1615915 -4.158309 -4.1758184 -4.2112656 -4.2490239 -4.27216 -4.2787647 -4.2755113 -4.2688584 -4.2642121 -4.2657533][-4.2401218 -4.2344337 -4.2278152 -4.2172484 -4.2058778 -4.1998649 -4.2077675 -4.2299666 -4.253087 -4.2673755 -4.2728424 -4.2735486 -4.2717738 -4.2697921 -4.271028][-4.2376661 -4.2378197 -4.2405019 -4.2417331 -4.2401137 -4.2369266 -4.2373686 -4.2452707 -4.254889 -4.2615862 -4.2653065 -4.2682462 -4.2707257 -4.2728696 -4.27621]]...]
INFO - root - 2017-12-07 14:32:59.751699: step 19710, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 58h:25m:36s remains)
INFO - root - 2017-12-07 14:33:06.335456: step 19720, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 55h:12m:16s remains)
INFO - root - 2017-12-07 14:33:13.212672: step 19730, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 61h:34m:01s remains)
INFO - root - 2017-12-07 14:33:19.914051: step 19740, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 61h:01m:07s remains)
INFO - root - 2017-12-07 14:33:26.701608: step 19750, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 58h:40m:49s remains)
INFO - root - 2017-12-07 14:33:33.475761: step 19760, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 56h:51m:36s remains)
INFO - root - 2017-12-07 14:33:39.980341: step 19770, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 58h:17m:49s remains)
INFO - root - 2017-12-07 14:33:46.671088: step 19780, loss = 2.12, batch loss = 2.06 (11.8 examples/sec; 0.678 sec/batch; 58h:52m:05s remains)
INFO - root - 2017-12-07 14:33:53.436599: step 19790, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 57h:59m:06s remains)
INFO - root - 2017-12-07 14:34:00.215637: step 19800, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.678 sec/batch; 58h:55m:25s remains)
2017-12-07 14:34:00.932993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2019353 -4.2231736 -4.2255578 -4.2253585 -4.23298 -4.2424426 -4.2520957 -4.2649364 -4.2707243 -4.2652893 -4.2442083 -4.2018294 -4.1593184 -4.1424174 -4.1603055][-4.1999893 -4.2209139 -4.2230577 -4.2224278 -4.2311006 -4.2366052 -4.2376852 -4.2449331 -4.2525115 -4.2485237 -4.2276058 -4.1809421 -4.1250644 -4.0937848 -4.1145868][-4.2114911 -4.2247319 -4.2236834 -4.2199683 -4.223371 -4.2240562 -4.2149167 -4.2105665 -4.2164373 -4.2163405 -4.2009726 -4.1550241 -4.0834403 -4.0353451 -4.05756][-4.2393551 -4.24023 -4.2335658 -4.2239108 -4.2171869 -4.2081838 -4.1845384 -4.1646647 -4.167767 -4.1775 -4.1774163 -4.1415634 -4.0630016 -4.0045018 -4.0242467][-4.2616014 -4.2467132 -4.23212 -4.2165551 -4.202323 -4.1885223 -4.1530371 -4.1166368 -4.114027 -4.1365604 -4.155724 -4.1395636 -4.0670147 -4.0026145 -4.0098414][-4.2637253 -4.2339139 -4.2089667 -4.1866512 -4.1729975 -4.1585994 -4.11712 -4.0646024 -4.0481896 -4.0783911 -4.121316 -4.1326184 -4.0797958 -4.0171146 -4.0030551][-4.2619081 -4.2207255 -4.1848121 -4.15636 -4.1422648 -4.1239452 -4.0768971 -4.0170178 -3.9881763 -4.0203295 -4.0854869 -4.1247659 -4.0958815 -4.0361719 -4.0005279][-4.2657723 -4.2208552 -4.178061 -4.1436191 -4.120399 -4.0953493 -4.0492969 -3.9940517 -3.9664993 -4.0018215 -4.0781331 -4.1298304 -4.1166058 -4.0559144 -4.0044775][-4.2642527 -4.223712 -4.18536 -4.1487732 -4.1161947 -4.0866833 -4.0434446 -3.9978375 -3.9764221 -4.0106869 -4.0891848 -4.1454382 -4.1448431 -4.0857134 -4.0221739][-4.2631345 -4.2266684 -4.1936264 -4.15929 -4.1201921 -4.0856733 -4.0464358 -4.0160322 -4.0050435 -4.0335937 -4.1071615 -4.1672587 -4.176774 -4.1247125 -4.0582581][-4.2666955 -4.2332735 -4.2045627 -4.1750097 -4.1326494 -4.0957365 -4.0633245 -4.0512404 -4.0548744 -4.0784802 -4.1379066 -4.1922112 -4.1996493 -4.1494594 -4.0867672][-4.2722635 -4.2489691 -4.2280059 -4.2065835 -4.1643953 -4.1242342 -4.094398 -4.0916491 -4.1070518 -4.1315289 -4.1779633 -4.2192416 -4.2162685 -4.1601639 -4.0976624][-4.2694893 -4.2589927 -4.2488456 -4.2385068 -4.2030816 -4.1634784 -4.1364155 -4.1349573 -4.1528673 -4.1806417 -4.2182803 -4.2467704 -4.233758 -4.1698551 -4.1077709][-4.2607574 -4.2595625 -4.2595868 -4.2582364 -4.23253 -4.1988883 -4.1770616 -4.1747427 -4.1890707 -4.2130671 -4.2412753 -4.259069 -4.2409778 -4.1736908 -4.1130934][-4.2543135 -4.2593822 -4.2657261 -4.2702193 -4.2557631 -4.2332544 -4.2180915 -4.2115207 -4.2163019 -4.2264581 -4.2395 -4.2488275 -4.2294378 -4.1617932 -4.1000476]]...]
INFO - root - 2017-12-07 14:34:07.610102: step 19810, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 59h:15m:53s remains)
INFO - root - 2017-12-07 14:34:14.252903: step 19820, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 59h:24m:00s remains)
INFO - root - 2017-12-07 14:34:21.002394: step 19830, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 57h:03m:40s remains)
INFO - root - 2017-12-07 14:34:27.787729: step 19840, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 55h:20m:33s remains)
INFO - root - 2017-12-07 14:34:34.501399: step 19850, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 61h:18m:08s remains)
INFO - root - 2017-12-07 14:34:41.386868: step 19860, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 63h:00m:53s remains)
INFO - root - 2017-12-07 14:34:48.181134: step 19870, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 58h:19m:48s remains)
INFO - root - 2017-12-07 14:34:55.009886: step 19880, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 57h:34m:47s remains)
INFO - root - 2017-12-07 14:35:02.023845: step 19890, loss = 2.11, batch loss = 2.05 (11.2 examples/sec; 0.715 sec/batch; 62h:07m:35s remains)
INFO - root - 2017-12-07 14:35:08.874049: step 19900, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.724 sec/batch; 62h:52m:23s remains)
2017-12-07 14:35:09.603990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2686453 -4.2679882 -4.2543783 -4.2384362 -4.21464 -4.204103 -4.2249174 -4.2729907 -4.319273 -4.3303609 -4.3036265 -4.2471757 -4.1887717 -4.1540508 -4.1443987][-4.2709513 -4.2706857 -4.2527461 -4.2245855 -4.1924887 -4.1799393 -4.2062654 -4.265996 -4.3211389 -4.3399844 -4.3180304 -4.2628493 -4.2018156 -4.1579952 -4.1479192][-4.2759743 -4.2726021 -4.2488041 -4.2089849 -4.1698756 -4.1566696 -4.1873584 -4.2520342 -4.3073077 -4.3315306 -4.3159609 -4.2642832 -4.2004328 -4.1501288 -4.1421762][-4.277957 -4.269433 -4.2345057 -4.1810608 -4.1330156 -4.1219349 -4.1574373 -4.2230105 -4.276145 -4.3058681 -4.3004637 -4.2558708 -4.1896744 -4.1305013 -4.1192751][-4.2629056 -4.2477894 -4.201292 -4.1349177 -4.0776076 -4.0676055 -4.1118593 -4.1832051 -4.2375379 -4.2726736 -4.2795753 -4.244606 -4.1751308 -4.1057673 -4.0860748][-4.2341347 -4.2135215 -4.1603508 -4.0878644 -4.0228968 -4.0081072 -4.0592594 -4.1380777 -4.1972885 -4.2377882 -4.2581687 -4.232306 -4.1622787 -4.0848818 -4.0565348][-4.2062831 -4.1831408 -4.1310806 -4.0594621 -3.9872 -3.9619222 -4.0150375 -4.1024408 -4.167161 -4.2082739 -4.2384329 -4.2245312 -4.16465 -4.08929 -4.0546641][-4.1977296 -4.1779275 -4.133966 -4.0698247 -3.9920757 -3.9527469 -3.996397 -4.082376 -4.1455779 -4.1844339 -4.218503 -4.21358 -4.1639934 -4.0943241 -4.055357][-4.2174006 -4.2022858 -4.1674552 -4.1102214 -4.0282159 -3.96912 -3.9885421 -4.0604053 -4.1206455 -4.1638279 -4.2031322 -4.2030768 -4.160243 -4.094327 -4.052032][-4.2603235 -4.2473221 -4.2177238 -4.1619987 -4.0742569 -3.9941278 -3.9857676 -4.039113 -4.0958247 -4.1444283 -4.190064 -4.1970172 -4.1551805 -4.0871673 -4.0433822][-4.3032761 -4.2923203 -4.2657657 -4.2105761 -4.1168818 -4.0182328 -3.9844732 -4.0217566 -4.074873 -4.1285367 -4.1794114 -4.1924295 -4.1477509 -4.0714526 -4.0202003][-4.3323922 -4.3248587 -4.30245 -4.2542195 -4.1682553 -4.06774 -4.018971 -4.0402484 -4.0838294 -4.1338048 -4.1816053 -4.1924977 -4.1400089 -4.0472279 -3.9771066][-4.3512678 -4.3473806 -4.3326125 -4.2987504 -4.2342548 -4.1548324 -4.1090403 -4.1177421 -4.1478696 -4.1846304 -4.2172985 -4.215271 -4.1504192 -4.0388021 -3.9407876][-4.3602614 -4.3591566 -4.3513441 -4.3329391 -4.2926979 -4.239315 -4.2050323 -4.2095289 -4.2310758 -4.2562366 -4.2737818 -4.2610016 -4.1912022 -4.0708623 -3.9457989][-4.3611803 -4.3613987 -4.357491 -4.3486567 -4.3271251 -4.2956166 -4.2740655 -4.2781296 -4.2948093 -4.31275 -4.3218188 -4.3078828 -4.2470317 -4.1347237 -4.0026112]]...]
INFO - root - 2017-12-07 14:35:16.421517: step 19910, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 58h:24m:59s remains)
INFO - root - 2017-12-07 14:35:23.146472: step 19920, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 61h:26m:56s remains)
INFO - root - 2017-12-07 14:35:30.030953: step 19930, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.730 sec/batch; 63h:20m:28s remains)
INFO - root - 2017-12-07 14:35:36.823772: step 19940, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 60h:11m:36s remains)
INFO - root - 2017-12-07 14:35:43.588097: step 19950, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 57h:10m:36s remains)
INFO - root - 2017-12-07 14:35:50.493577: step 19960, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 56h:43m:05s remains)
INFO - root - 2017-12-07 14:35:57.353265: step 19970, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 62h:40m:53s remains)
INFO - root - 2017-12-07 14:36:04.281484: step 19980, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.727 sec/batch; 63h:07m:37s remains)
INFO - root - 2017-12-07 14:36:11.167368: step 19990, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 60h:07m:37s remains)
INFO - root - 2017-12-07 14:36:17.838461: step 20000, loss = 2.03, batch loss = 1.97 (12.6 examples/sec; 0.635 sec/batch; 55h:05m:30s remains)
2017-12-07 14:36:18.540674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30217 -4.2970767 -4.2943377 -4.3000188 -4.3067136 -4.3070521 -4.3015451 -4.2993841 -4.2984982 -4.2897072 -4.2758908 -4.2715812 -4.279563 -4.2898378 -4.3017836][-4.2766619 -4.2694807 -4.264256 -4.270319 -4.2802129 -4.2823772 -4.2720251 -4.2687111 -4.2676296 -4.2560287 -4.2390795 -4.2352018 -4.2461576 -4.2604856 -4.2773924][-4.2408524 -4.2324977 -4.2240252 -4.2292671 -4.2400074 -4.2402096 -4.2203131 -4.2084436 -4.2067494 -4.1961603 -4.1829362 -4.1832018 -4.2000003 -4.2228718 -4.2483015][-4.2126484 -4.2019854 -4.1883988 -4.1874151 -4.1892805 -4.1806779 -4.1435957 -4.1201053 -4.1247063 -4.126472 -4.1236663 -4.134984 -4.1641645 -4.198216 -4.2318983][-4.1848879 -4.1702709 -4.1540761 -4.1431546 -4.1273561 -4.1023397 -4.0394149 -3.9928815 -4.0073824 -4.0323167 -4.0470719 -4.0769038 -4.1251969 -4.1716404 -4.2143903][-4.1623015 -4.1435843 -4.118824 -4.0903735 -4.05 -3.9994717 -3.8905644 -3.8010242 -3.8454075 -3.9165664 -3.9472222 -3.9999056 -4.0740957 -4.1340995 -4.1909056][-4.1636972 -4.1389966 -4.1002398 -4.0439143 -3.9789622 -3.8979268 -3.7214832 -3.5728717 -3.673877 -3.8012183 -3.8421509 -3.9203064 -4.0244131 -4.0970125 -4.1692085][-4.1871152 -4.1617222 -4.1141782 -4.0426321 -3.9736183 -3.8891113 -3.6971841 -3.5397809 -3.6670094 -3.802886 -3.8337033 -3.9079027 -4.0173512 -4.0881367 -4.16062][-4.2260094 -4.2066903 -4.1615553 -4.1000252 -4.0519419 -4.0000997 -3.8659096 -3.7605982 -3.844445 -3.933327 -3.9416227 -3.9850404 -4.0680776 -4.1223717 -4.18101][-4.2606258 -4.2484474 -4.2150369 -4.1731296 -4.1464658 -4.1235 -4.0483031 -3.9858148 -4.023016 -4.0707836 -4.07182 -4.0928268 -4.1474323 -4.1825733 -4.2215204][-4.2839608 -4.2753482 -4.255569 -4.2321095 -4.2206578 -4.2183647 -4.1842957 -4.1445255 -4.14834 -4.1709118 -4.1706333 -4.1806345 -4.2138147 -4.2360282 -4.2604737][-4.2993107 -4.2960224 -4.2840195 -4.2746377 -4.2712636 -4.2749219 -4.2603421 -4.2348738 -4.2235312 -4.2304153 -4.2313871 -4.2359672 -4.2539191 -4.2682085 -4.2847085][-4.3040457 -4.300168 -4.2901649 -4.28354 -4.2813416 -4.2862382 -4.2818704 -4.2691069 -4.260221 -4.2613163 -4.2636166 -4.2653604 -4.2758369 -4.2856069 -4.2956781][-4.3100057 -4.3055563 -4.2951164 -4.2861657 -4.2814889 -4.2844791 -4.2851624 -4.2828727 -4.2792535 -4.278748 -4.2818155 -4.2857418 -4.2934985 -4.2995129 -4.3052459][-4.3163724 -4.3108783 -4.3012562 -4.2937136 -4.2895918 -4.2913275 -4.2944503 -4.2965679 -4.2961974 -4.2974167 -4.3003583 -4.3045535 -4.3105416 -4.3142748 -4.3168726]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 14:36:25.914963: step 20010, loss = 2.05, batch loss = 2.00 (15.5 examples/sec; 0.516 sec/batch; 44h:48m:35s remains)
INFO - root - 2017-12-07 14:36:32.706265: step 20020, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 61h:20m:45s remains)
INFO - root - 2017-12-07 14:36:39.445868: step 20030, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 58h:16m:16s remains)
INFO - root - 2017-12-07 14:36:46.172376: step 20040, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 59h:09m:52s remains)
INFO - root - 2017-12-07 14:36:52.874835: step 20050, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 55h:48m:13s remains)
INFO - root - 2017-12-07 14:36:59.520191: step 20060, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 53h:57m:34s remains)
INFO - root - 2017-12-07 14:37:06.418684: step 20070, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 60h:14m:13s remains)
INFO - root - 2017-12-07 14:37:13.065581: step 20080, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 63h:41m:16s remains)
INFO - root - 2017-12-07 14:37:19.821146: step 20090, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 58h:58m:59s remains)
INFO - root - 2017-12-07 14:37:26.574226: step 20100, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 59h:38m:53s remains)
2017-12-07 14:37:27.343268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808175 -4.2841868 -4.2856731 -4.2848554 -4.2831464 -4.2820735 -4.2799911 -4.2773004 -4.2836156 -4.3049631 -4.3250203 -4.338182 -4.3404045 -4.3379149 -4.335969][-4.2366734 -4.2423487 -4.2489204 -4.2483521 -4.2461772 -4.2451077 -4.24348 -4.2410374 -4.2494044 -4.2768116 -4.3018756 -4.3207054 -4.3220863 -4.3176074 -4.3168526][-4.1948338 -4.204411 -4.2135959 -4.2128668 -4.2092476 -4.20908 -4.2087145 -4.2085638 -4.2205176 -4.2509193 -4.2781143 -4.2993793 -4.2996049 -4.2946072 -4.2974482][-4.1795073 -4.1876779 -4.1880231 -4.1804347 -4.1736269 -4.1781526 -4.183404 -4.1915407 -4.2108951 -4.2408915 -4.2655344 -4.2843204 -4.282371 -4.2767911 -4.2828627][-4.1707911 -4.180625 -4.1763105 -4.1615129 -4.1505837 -4.1567121 -4.1630516 -4.1787066 -4.2060575 -4.2348785 -4.2543521 -4.2698941 -4.26693 -4.2628284 -4.2716007][-4.1650677 -4.1769972 -4.1748047 -4.1587219 -4.1380005 -4.1294956 -4.1282063 -4.1474757 -4.181704 -4.2102151 -4.2305393 -4.24767 -4.2502041 -4.2501082 -4.261775][-4.1657948 -4.1776404 -4.1764708 -4.1574788 -4.1213431 -4.0929875 -4.0932622 -4.1229887 -4.1589704 -4.1835608 -4.2015376 -4.2230587 -4.2307668 -4.2331448 -4.2486258][-4.1442094 -4.1569777 -4.158608 -4.1428909 -4.0948291 -4.04973 -4.0555792 -4.1000547 -4.1409965 -4.1634517 -4.1825352 -4.2091751 -4.218318 -4.2220922 -4.2381868][-4.1249309 -4.1383185 -4.1453924 -4.1386166 -4.0939374 -4.0456462 -4.0561 -4.108541 -4.1533542 -4.1727338 -4.1890268 -4.2143869 -4.2219195 -4.2239037 -4.2367306][-4.1243677 -4.1310983 -4.1388721 -4.1447797 -4.1166153 -4.0777235 -4.09056 -4.1406164 -4.183085 -4.1964297 -4.2100282 -4.2313566 -4.2354259 -4.2340803 -4.240983][-4.1341434 -4.1319132 -4.1345396 -4.1473413 -4.1301293 -4.0979424 -4.108788 -4.1522918 -4.1940851 -4.2101111 -4.2287946 -4.2503824 -4.25404 -4.24923 -4.2499661][-4.1683984 -4.1574078 -4.1540804 -4.1619987 -4.1417813 -4.1069889 -4.1144867 -4.1549711 -4.1981411 -4.2209182 -4.2456913 -4.2706852 -4.2797685 -4.273953 -4.2670312][-4.2111526 -4.2042513 -4.1969051 -4.1948662 -4.1663723 -4.1277509 -4.125504 -4.1614413 -4.2108297 -4.2426295 -4.2714825 -4.296576 -4.3064809 -4.2989292 -4.2850518][-4.225481 -4.2269077 -4.2226582 -4.2173386 -4.1892238 -4.1589284 -4.1536589 -4.18168 -4.2266049 -4.2600932 -4.2897291 -4.3134065 -4.3186235 -4.3100123 -4.2946148][-4.2343144 -4.238483 -4.2378883 -4.2382927 -4.2208872 -4.202579 -4.1999192 -4.2177792 -4.2470174 -4.2745705 -4.3034549 -4.32576 -4.3269062 -4.3157415 -4.3009133]]...]
INFO - root - 2017-12-07 14:37:34.014202: step 20110, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 56h:15m:10s remains)
INFO - root - 2017-12-07 14:37:40.690295: step 20120, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 58h:44m:30s remains)
INFO - root - 2017-12-07 14:37:47.360959: step 20130, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 54h:25m:54s remains)
INFO - root - 2017-12-07 14:37:54.071568: step 20140, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 56h:57m:37s remains)
INFO - root - 2017-12-07 14:38:00.776228: step 20150, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 61h:10m:47s remains)
INFO - root - 2017-12-07 14:38:07.618269: step 20160, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 58h:55m:12s remains)
INFO - root - 2017-12-07 14:38:14.388957: step 20170, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 57h:22m:52s remains)
INFO - root - 2017-12-07 14:38:21.065368: step 20180, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 56h:05m:00s remains)
INFO - root - 2017-12-07 14:38:27.937922: step 20190, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 59h:45m:25s remains)
INFO - root - 2017-12-07 14:38:34.714552: step 20200, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 61h:08m:14s remains)
2017-12-07 14:38:35.391222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2309456 -4.2403612 -4.25803 -4.2720032 -4.2749429 -4.267312 -4.2498856 -4.2256703 -4.2075295 -4.2057309 -4.2159591 -4.22655 -4.2336917 -4.2372689 -4.2357974][-4.2681208 -4.2735863 -4.281311 -4.2832122 -4.2749248 -4.25672 -4.2298727 -4.2001705 -4.182241 -4.1850848 -4.1980133 -4.2085695 -4.2144508 -4.21901 -4.2191215][-4.2961788 -4.2973175 -4.2961121 -4.2883058 -4.2734671 -4.2516308 -4.224122 -4.1969118 -4.1834035 -4.1903996 -4.202631 -4.2084885 -4.2100835 -4.2109437 -4.2100463][-4.2917957 -4.2900844 -4.2859731 -4.2770638 -4.2622366 -4.2426853 -4.2198892 -4.1989436 -4.1935787 -4.2049847 -4.2156763 -4.2180529 -4.2173557 -4.2156825 -4.214613][-4.2523785 -4.2473741 -4.2438607 -4.2367587 -4.2241058 -4.206831 -4.1866655 -4.1711626 -4.1747885 -4.1952534 -4.2106142 -4.2150989 -4.214715 -4.2140408 -4.2143545][-4.2080226 -4.1971641 -4.1909027 -4.1837764 -4.1695881 -4.1508322 -4.1273069 -4.109211 -4.1181836 -4.1522179 -4.1802588 -4.1920171 -4.1942568 -4.1967688 -4.2000437][-4.1766176 -4.1558394 -4.1393504 -4.121408 -4.0910454 -4.0568352 -4.0204649 -3.9973009 -4.0191631 -4.080236 -4.1297331 -4.1510596 -4.1603661 -4.1703386 -4.1812587][-4.1708765 -4.1402283 -4.1147971 -4.0843019 -4.0331373 -3.977592 -3.9209375 -3.8804488 -3.9106922 -3.9991875 -4.0708528 -4.1013322 -4.1156979 -4.130805 -4.148953][-4.18813 -4.1559534 -4.1317782 -4.1015553 -4.0518117 -3.9976537 -3.9422634 -3.8929508 -3.9002748 -3.9659393 -4.0273762 -4.0554833 -4.070951 -4.0902991 -4.1148744][-4.2119226 -4.18697 -4.1723986 -4.1541061 -4.1185966 -4.0798264 -4.0392394 -3.9945636 -3.9827771 -4.0132117 -4.0464482 -4.0581551 -4.0655985 -4.08385 -4.1090331][-4.238441 -4.2231488 -4.2167759 -4.20565 -4.1797833 -4.1526952 -4.1223688 -4.0846353 -4.0683236 -4.0816274 -4.0963116 -4.0948992 -4.0947127 -4.1087551 -4.1297297][-4.2714448 -4.26206 -4.2570543 -4.2461395 -4.2266378 -4.2096791 -4.1908035 -4.1632981 -4.1467333 -4.1474156 -4.148767 -4.1403975 -4.1369185 -4.1478586 -4.1642613][-4.298594 -4.291688 -4.2867031 -4.276063 -4.2623186 -4.2543426 -4.246048 -4.2306423 -4.2187858 -4.2144561 -4.2087522 -4.1982446 -4.1923771 -4.1977472 -4.2073035][-4.316927 -4.3113976 -4.3069673 -4.298697 -4.2899294 -4.2867846 -4.2839403 -4.276556 -4.26898 -4.2648425 -4.2602692 -4.252 -4.2456441 -4.2450242 -4.2463713][-4.3303871 -4.32573 -4.32151 -4.3146138 -4.3077922 -4.3044629 -4.3019528 -4.2967949 -4.2910762 -4.2870665 -4.2830558 -4.277833 -4.2731562 -4.2703233 -4.2678967]]...]
INFO - root - 2017-12-07 14:38:42.149587: step 20210, loss = 2.09, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 55h:40m:35s remains)
INFO - root - 2017-12-07 14:38:48.879011: step 20220, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 58h:43m:08s remains)
INFO - root - 2017-12-07 14:38:55.836187: step 20230, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.737 sec/batch; 63h:57m:05s remains)
INFO - root - 2017-12-07 14:39:02.662260: step 20240, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 60h:55m:29s remains)
INFO - root - 2017-12-07 14:39:09.561119: step 20250, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 58h:55m:57s remains)
INFO - root - 2017-12-07 14:39:16.295092: step 20260, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 56h:02m:45s remains)
INFO - root - 2017-12-07 14:39:23.101582: step 20270, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 57h:54m:38s remains)
INFO - root - 2017-12-07 14:39:29.950236: step 20280, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 63h:12m:48s remains)
INFO - root - 2017-12-07 14:39:36.768212: step 20290, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 60h:51m:52s remains)
INFO - root - 2017-12-07 14:39:43.627100: step 20300, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 59h:13m:19s remains)
2017-12-07 14:39:44.310146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3068156 -4.2900391 -4.2783542 -4.2757998 -4.2820706 -4.2948036 -4.3067718 -4.3137803 -4.3130126 -4.3054404 -4.2933888 -4.2711115 -4.2446847 -4.235909 -4.2539597][-4.3026123 -4.2820134 -4.2677341 -4.2624168 -4.26625 -4.2805119 -4.2963376 -4.3090425 -4.3143158 -4.3087473 -4.2946949 -4.2671332 -4.2357416 -4.2233071 -4.2375946][-4.2903543 -4.2648821 -4.2437906 -4.2306776 -4.2309813 -4.2519655 -4.2761703 -4.297266 -4.3093147 -4.3108597 -4.2980933 -4.266119 -4.2271442 -4.2074471 -4.216517][-4.2815642 -4.2529454 -4.2213912 -4.1903491 -4.1766615 -4.1977339 -4.23375 -4.27048 -4.2962484 -4.3115149 -4.3065357 -4.2782283 -4.2350636 -4.205174 -4.2051773][-4.2784071 -4.2491765 -4.2082429 -4.1567936 -4.1184254 -4.1228404 -4.1606283 -4.2142005 -4.261724 -4.298604 -4.309402 -4.2946787 -4.2588186 -4.2240915 -4.2123876][-4.2805433 -4.2549767 -4.2140384 -4.1494141 -4.083426 -4.0515943 -4.0641103 -4.1189847 -4.1952043 -4.2591977 -4.291605 -4.2978358 -4.2820106 -4.2572093 -4.2406788][-4.2844973 -4.2646527 -4.2307434 -4.1699967 -4.0890989 -4.0107436 -3.9606822 -3.9845419 -4.08263 -4.1833849 -4.2470961 -4.2803087 -4.2916389 -4.2870941 -4.2773728][-4.2865882 -4.2754674 -4.25725 -4.2159944 -4.146708 -4.0558195 -3.9590368 -3.9201388 -3.9890931 -4.0958371 -4.182919 -4.2412314 -4.2765837 -4.296999 -4.3016806][-4.2784748 -4.2768421 -4.2749395 -4.261044 -4.2235179 -4.1633139 -4.0832534 -4.019875 -4.0191741 -4.0663319 -4.1330709 -4.1944265 -4.2411308 -4.2798271 -4.304647][-4.2569842 -4.2573285 -4.2629566 -4.2629375 -4.25156 -4.2301021 -4.1930652 -4.14929 -4.1243887 -4.1249704 -4.1442418 -4.1725345 -4.2042966 -4.246686 -4.2899394][-4.2289352 -4.2223196 -4.2214479 -4.2235684 -4.2276578 -4.2368679 -4.236083 -4.2234864 -4.2074137 -4.200274 -4.1943164 -4.1875014 -4.1878915 -4.2116346 -4.2551007][-4.2119927 -4.1966429 -4.1821985 -4.1750636 -4.1795311 -4.2005816 -4.2259169 -4.2395215 -4.2397714 -4.2382183 -4.23309 -4.2166057 -4.1959853 -4.1935015 -4.2195172][-4.18777 -4.17395 -4.1537833 -4.1338062 -4.1246605 -4.1396794 -4.171803 -4.2051439 -4.2224641 -4.2305517 -4.2315283 -4.2216806 -4.2019439 -4.1868181 -4.193162][-4.1463332 -4.1467247 -4.1374054 -4.1190467 -4.0977135 -4.0971813 -4.1187887 -4.1504703 -4.1729956 -4.1888561 -4.1998587 -4.2054248 -4.198947 -4.1869593 -4.1829104][-4.1174226 -4.1240482 -4.1256976 -4.1185017 -4.1029468 -4.0986109 -4.1068091 -4.1194754 -4.1286345 -4.1396909 -4.1558714 -4.1738205 -4.1852622 -4.1886668 -4.1894364]]...]
INFO - root - 2017-12-07 14:39:51.012790: step 20310, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 58h:58m:26s remains)
INFO - root - 2017-12-07 14:39:57.309149: step 20320, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 54h:55m:21s remains)
INFO - root - 2017-12-07 14:40:04.006608: step 20330, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 57h:46m:16s remains)
INFO - root - 2017-12-07 14:40:10.766990: step 20340, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 57h:43m:46s remains)
INFO - root - 2017-12-07 14:40:17.468372: step 20350, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 56h:17m:03s remains)
INFO - root - 2017-12-07 14:40:24.293119: step 20360, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.692 sec/batch; 59h:59m:18s remains)
INFO - root - 2017-12-07 14:40:30.975817: step 20370, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 61h:15m:20s remains)
INFO - root - 2017-12-07 14:40:37.797223: step 20380, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 59h:51m:26s remains)
INFO - root - 2017-12-07 14:40:44.375910: step 20390, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 57h:57m:07s remains)
INFO - root - 2017-12-07 14:40:51.122917: step 20400, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 60h:23m:06s remains)
2017-12-07 14:40:51.901992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.163835 -4.1741128 -4.1855745 -4.1841879 -4.1756234 -4.1676908 -4.1641531 -4.1629581 -4.16568 -4.1740384 -4.19063 -4.210351 -4.2272568 -4.2422543 -4.2495732][-4.1348553 -4.1348243 -4.1462646 -4.1475544 -4.1406965 -4.1376905 -4.1394215 -4.1414294 -4.14147 -4.1388769 -4.1411686 -4.15053 -4.1680274 -4.1947088 -4.2236834][-4.1291189 -4.1252365 -4.1393156 -4.1472926 -4.145987 -4.14732 -4.1497974 -4.14675 -4.1356335 -4.1179996 -4.1062493 -4.1074 -4.123467 -4.1554585 -4.1993837][-4.1509938 -4.1445413 -4.1584888 -4.1722355 -4.1792426 -4.1824679 -4.1784072 -4.1612992 -4.1339216 -4.1019363 -4.0834274 -4.0885906 -4.112546 -4.1499562 -4.1986895][-4.196279 -4.18629 -4.1968 -4.2118907 -4.2192936 -4.2168546 -4.2007937 -4.1677275 -4.1233625 -4.0792265 -4.0619369 -4.0843139 -4.126389 -4.1724634 -4.2215524][-4.2578897 -4.2466822 -4.2515655 -4.2615476 -4.2621608 -4.2486048 -4.219418 -4.1725559 -4.1159673 -4.069057 -4.062417 -4.1028743 -4.1584916 -4.2075772 -4.2500596][-4.3034611 -4.2963266 -4.298841 -4.3043923 -4.2994018 -4.2773776 -4.2409 -4.1906424 -4.1366816 -4.0973425 -4.1001477 -4.1464343 -4.2026677 -4.2459788 -4.2763376][-4.3225451 -4.3225846 -4.3255763 -4.3276196 -4.3202596 -4.2986407 -4.2664094 -4.2252908 -4.1837883 -4.154808 -4.1595368 -4.1990461 -4.2441645 -4.2750325 -4.2913594][-4.3080316 -4.3150105 -4.3223805 -4.3271012 -4.3248754 -4.3125353 -4.2927446 -4.2674074 -4.2411847 -4.2203894 -4.2184238 -4.2398095 -4.266994 -4.2841029 -4.2897525][-4.2663317 -4.2826996 -4.2984796 -4.3109446 -4.3175983 -4.3168616 -4.3108559 -4.3002758 -4.2858248 -4.2693582 -4.2586493 -4.2617593 -4.27178 -4.2776108 -4.2784367][-4.2329698 -4.2575564 -4.2792292 -4.2955418 -4.30556 -4.3100462 -4.3118067 -4.3104925 -4.3038116 -4.291151 -4.2772603 -4.2703652 -4.26969 -4.2690678 -4.2692909][-4.22251 -4.2482824 -4.2699828 -4.2845325 -4.291995 -4.2955217 -4.2978048 -4.2988186 -4.2959867 -4.287396 -4.2767 -4.2697587 -4.266449 -4.2645283 -4.2661133][-4.2319174 -4.25458 -4.2724786 -4.2815704 -4.283196 -4.2808228 -4.2773337 -4.2743959 -4.2703342 -4.264864 -4.2601204 -4.2584176 -4.2581253 -4.2601476 -4.2665687][-4.250865 -4.2682281 -4.2815194 -4.2864275 -4.283886 -4.2770023 -4.267858 -4.2590919 -4.2516713 -4.2468362 -4.2454743 -4.24764 -4.2508497 -4.2570777 -4.2677851][-4.2726378 -4.2834535 -4.2922282 -4.2952075 -4.2925572 -4.2850642 -4.2736454 -4.2605162 -4.2494583 -4.2431312 -4.2423315 -4.2458882 -4.25064 -4.2589216 -4.271637]]...]
INFO - root - 2017-12-07 14:40:58.487984: step 20410, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.618 sec/batch; 53h:32m:11s remains)
INFO - root - 2017-12-07 14:41:05.204236: step 20420, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.679 sec/batch; 58h:52m:31s remains)
INFO - root - 2017-12-07 14:41:12.034914: step 20430, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 61h:58m:52s remains)
INFO - root - 2017-12-07 14:41:18.734239: step 20440, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 56h:39m:41s remains)
INFO - root - 2017-12-07 14:41:25.565670: step 20450, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 58h:38m:04s remains)
INFO - root - 2017-12-07 14:41:32.383230: step 20460, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 59h:31m:42s remains)
INFO - root - 2017-12-07 14:41:39.177695: step 20470, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 57h:35m:43s remains)
INFO - root - 2017-12-07 14:41:45.916458: step 20480, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 59h:55m:50s remains)
INFO - root - 2017-12-07 14:41:52.775383: step 20490, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 62h:59m:07s remains)
INFO - root - 2017-12-07 14:41:59.553578: step 20500, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 57h:17m:16s remains)
2017-12-07 14:42:00.238093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.18462 -4.1902652 -4.1885982 -4.1832433 -4.2038488 -4.2148595 -4.202004 -4.176744 -4.1464963 -4.1420856 -4.1566486 -4.1661835 -4.1852894 -4.1950088 -4.1865158][-4.1456871 -4.1600223 -4.1722293 -4.1831355 -4.2139168 -4.2327027 -4.2294393 -4.2167497 -4.1903248 -4.1832571 -4.1921091 -4.1931005 -4.1986809 -4.1921449 -4.1822538][-4.125288 -4.1392884 -4.1628218 -4.1860523 -4.2174773 -4.2346907 -4.2369337 -4.2359519 -4.2195864 -4.210721 -4.211834 -4.2048464 -4.2011151 -4.1880407 -4.1789536][-4.1449065 -4.1517658 -4.1753378 -4.1969619 -4.2160511 -4.2181787 -4.2092028 -4.2080917 -4.2027617 -4.2002206 -4.1983747 -4.1837587 -4.1746368 -4.1643395 -4.1632242][-4.1785755 -4.1794224 -4.1989713 -4.2153845 -4.2165604 -4.1956124 -4.1671824 -4.1581736 -4.166503 -4.1777077 -4.1747184 -4.1535783 -4.1382132 -4.1290207 -4.1374464][-4.1954904 -4.1947331 -4.2071891 -4.2151666 -4.1996889 -4.1567311 -4.1032252 -4.0744915 -4.103117 -4.1405311 -4.145144 -4.1245742 -4.1067686 -4.0983009 -4.1097045][-4.1795039 -4.184711 -4.1949439 -4.1923847 -4.1615992 -4.0955477 -4.0092835 -3.948719 -3.9973843 -4.0745454 -4.1018858 -4.0916677 -4.0759783 -4.0690808 -4.08185][-4.1799889 -4.1874928 -4.191462 -4.1740975 -4.1220851 -4.0245094 -3.8915071 -3.7797632 -3.8476973 -3.9760871 -4.0398221 -4.051578 -4.042202 -4.0376439 -4.0482912][-4.1987042 -4.2074265 -4.2073026 -4.1869397 -4.1322551 -4.03231 -3.8829308 -3.732245 -3.7784669 -3.90904 -3.9869318 -4.0146394 -4.0076408 -4.0030003 -4.0156507][-4.2087851 -4.2225485 -4.2246838 -4.2144585 -4.1834555 -4.1203456 -4.0126777 -3.8924642 -3.8844476 -3.94216 -3.98236 -3.9998698 -3.990845 -3.9831376 -3.994252][-4.1927371 -4.2086973 -4.2209473 -4.2306175 -4.2275085 -4.20539 -4.1483707 -4.0716815 -4.0388045 -4.038034 -4.0392051 -4.0399528 -4.0237217 -4.0094953 -4.0142722][-4.1740332 -4.1852837 -4.2011719 -4.2245035 -4.2407327 -4.2475758 -4.231318 -4.1925716 -4.1596313 -4.1359663 -4.1192608 -4.1110353 -4.0956707 -4.0821381 -4.08258][-4.1679463 -4.163568 -4.1758661 -4.2058587 -4.2335458 -4.2522054 -4.2524714 -4.2364488 -4.2212315 -4.2018824 -4.1851811 -4.18331 -4.1809278 -4.1739283 -4.1668906][-4.1861281 -4.1624374 -4.1600151 -4.18602 -4.217989 -4.2394075 -4.24285 -4.2380013 -4.2430129 -4.2380018 -4.2290606 -4.2355614 -4.2452097 -4.2478275 -4.2422566][-4.2153344 -4.1844139 -4.17106 -4.17995 -4.2025089 -4.216825 -4.2160354 -4.2179561 -4.2372036 -4.2447848 -4.2461538 -4.257998 -4.2724266 -4.2782888 -4.2746062]]...]
INFO - root - 2017-12-07 14:42:06.952836: step 20510, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 59h:09m:03s remains)
INFO - root - 2017-12-07 14:42:13.769631: step 20520, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 63h:10m:34s remains)
INFO - root - 2017-12-07 14:42:20.620545: step 20530, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 60h:58m:09s remains)
INFO - root - 2017-12-07 14:42:27.429602: step 20540, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 57h:06m:34s remains)
INFO - root - 2017-12-07 14:42:34.234980: step 20550, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 59h:36m:16s remains)
INFO - root - 2017-12-07 14:42:41.097870: step 20560, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 63h:20m:31s remains)
INFO - root - 2017-12-07 14:42:47.917876: step 20570, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 61h:29m:16s remains)
INFO - root - 2017-12-07 14:42:54.733723: step 20580, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 59h:22m:07s remains)
INFO - root - 2017-12-07 14:43:01.446890: step 20590, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 55h:29m:39s remains)
INFO - root - 2017-12-07 14:43:08.272277: step 20600, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 58h:57m:00s remains)
2017-12-07 14:43:09.046759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3374763 -4.3331718 -4.3278728 -4.3256164 -4.3256049 -4.326591 -4.32901 -4.3297486 -4.32775 -4.3261147 -4.3263307 -4.3275023 -4.3278718 -4.3291292 -4.3300629][-4.3270617 -4.31825 -4.3092737 -4.3037605 -4.3023529 -4.3042603 -4.30954 -4.3088231 -4.3028827 -4.3004346 -4.3029337 -4.3058772 -4.3069572 -4.3114362 -4.3153491][-4.3068886 -4.2908783 -4.276124 -4.26547 -4.264091 -4.2707024 -4.2790146 -4.2742753 -4.2630777 -4.2599239 -4.2660871 -4.2724576 -4.2786713 -4.2894521 -4.2965074][-4.2803884 -4.2552528 -4.2307668 -4.2108192 -4.2088947 -4.2221918 -4.2342491 -4.2264781 -4.2132082 -4.2141252 -4.2267895 -4.2385855 -4.2511334 -4.2678046 -4.2754769][-4.2662106 -4.2325954 -4.1960077 -4.1644855 -4.1562834 -4.171495 -4.1870346 -4.1798415 -4.1732035 -4.1859264 -4.2058444 -4.2217956 -4.2350307 -4.2500749 -4.254952][-4.2648 -4.2243929 -4.1759706 -4.1312962 -4.1123433 -4.1225095 -4.1326022 -4.1221886 -4.1279645 -4.1577086 -4.1885304 -4.2069988 -4.2214408 -4.2365737 -4.239399][-4.2708807 -4.2296672 -4.1796956 -4.1298571 -4.1032934 -4.10168 -4.0953541 -4.0775104 -4.0879083 -4.1288671 -4.1712909 -4.1941481 -4.2094693 -4.2222919 -4.2222853][-4.2826128 -4.2454643 -4.2003303 -4.1565771 -4.1337357 -4.1266937 -4.1087713 -4.0843983 -4.0892467 -4.1274319 -4.1699734 -4.1910472 -4.1994529 -4.20475 -4.20244][-4.2962494 -4.2647891 -4.2273693 -4.1950288 -4.1777425 -4.1708393 -4.1514444 -4.12561 -4.1197224 -4.143229 -4.1709146 -4.1816587 -4.1804237 -4.1759195 -4.1745691][-4.3035932 -4.2763162 -4.2447572 -4.2176986 -4.201961 -4.195076 -4.1786065 -4.15621 -4.1448216 -4.1532106 -4.1665716 -4.1706228 -4.1648574 -4.1564217 -4.1589127][-4.3035746 -4.2774043 -4.2443271 -4.2147093 -4.1983352 -4.1947913 -4.1830935 -4.1599669 -4.1424131 -4.1434445 -4.1570349 -4.1642408 -4.1602664 -4.155695 -4.1592321][-4.2969666 -4.2686129 -4.2318573 -4.1989212 -4.1804819 -4.1793261 -4.1701112 -4.1427479 -4.1203294 -4.1222086 -4.1457109 -4.1655054 -4.1696758 -4.1684761 -4.1674061][-4.2847123 -4.250546 -4.2106223 -4.1763535 -4.1563659 -4.1565213 -4.1541572 -4.1292162 -4.1058974 -4.1101608 -4.1451321 -4.1804905 -4.1928911 -4.1916308 -4.1846442][-4.2787642 -4.241581 -4.199379 -4.1644788 -4.146728 -4.1539326 -4.15965 -4.1391053 -4.115912 -4.1220269 -4.1618376 -4.2039433 -4.2198315 -4.2212486 -4.2138677][-4.2872672 -4.2549677 -4.2186732 -4.1903229 -4.1823263 -4.19398 -4.2011046 -4.1837969 -4.1628804 -4.1700339 -4.2082729 -4.2463417 -4.2595568 -4.2608328 -4.2530556]]...]
INFO - root - 2017-12-07 14:43:15.693403: step 20610, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 57h:27m:19s remains)
INFO - root - 2017-12-07 14:43:22.325837: step 20620, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 56h:38m:38s remains)
INFO - root - 2017-12-07 14:43:29.184825: step 20630, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 62h:09m:40s remains)
INFO - root - 2017-12-07 14:43:35.908991: step 20640, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 61h:04m:57s remains)
INFO - root - 2017-12-07 14:43:42.754609: step 20650, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 58h:07m:54s remains)
INFO - root - 2017-12-07 14:43:49.487650: step 20660, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 57h:33m:29s remains)
INFO - root - 2017-12-07 14:43:56.223940: step 20670, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 55h:53m:22s remains)
INFO - root - 2017-12-07 14:44:03.162815: step 20680, loss = 2.05, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 63h:35m:00s remains)
INFO - root - 2017-12-07 14:44:10.020405: step 20690, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 61h:35m:52s remains)
INFO - root - 2017-12-07 14:44:16.588731: step 20700, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 54h:56m:19s remains)
2017-12-07 14:44:17.333766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2750983 -4.2531805 -4.2422848 -4.2397346 -4.2512989 -4.2771673 -4.3052621 -4.3259358 -4.3350244 -4.3343344 -4.3240252 -4.3168273 -4.321876 -4.3348088 -4.3411636][-4.2950187 -4.2685409 -4.2499852 -4.2392726 -4.244411 -4.2664552 -4.2940187 -4.3141451 -4.3233476 -4.3249111 -4.3219228 -4.3183289 -4.3222842 -4.3354964 -4.3450241][-4.3026943 -4.2619653 -4.2257004 -4.198925 -4.1945119 -4.2163129 -4.2499371 -4.2754025 -4.2864347 -4.2912674 -4.2989006 -4.3045764 -4.3106108 -4.3242011 -4.3358588][-4.279315 -4.2164979 -4.154964 -4.1029415 -4.0831757 -4.1100845 -4.160377 -4.2012219 -4.2213721 -4.233345 -4.2506189 -4.2688851 -4.2823215 -4.2987475 -4.313076][-4.235642 -4.147717 -4.0560374 -3.9708507 -3.9265401 -3.9545314 -4.0272756 -4.0935421 -4.1298232 -4.1530662 -4.1839604 -4.2202897 -4.2496905 -4.2735028 -4.2906127][-4.1989202 -4.0933776 -3.976177 -3.8608649 -3.78852 -3.8065202 -3.8936703 -3.9812298 -4.029983 -4.0650811 -4.1122189 -4.1660666 -4.2137809 -4.2499757 -4.2717762][-4.1887727 -4.0834122 -3.9602408 -3.8341637 -3.7405689 -3.7352819 -3.8133576 -3.9060793 -3.9622631 -4.0063524 -4.067863 -4.1312723 -4.1898727 -4.23721 -4.2660027][-4.2115884 -4.1258817 -4.0212345 -3.9108946 -3.8192563 -3.7873797 -3.8278828 -3.9029262 -3.9557209 -4.0002251 -4.0643692 -4.1297073 -4.1914644 -4.2435451 -4.2769852][-4.258287 -4.2016263 -4.1247997 -4.038816 -3.9635017 -3.9199491 -3.9278064 -3.97886 -4.0206575 -4.0530858 -4.1065164 -4.1660929 -4.22189 -4.2687306 -4.2985816][-4.3090878 -4.273849 -4.2186918 -4.1539803 -4.0972295 -4.0594959 -4.053452 -4.0833569 -4.1109042 -4.1311226 -4.1684036 -4.2153831 -4.2587643 -4.2959704 -4.3208737][-4.3360114 -4.316678 -4.2780771 -4.2296658 -4.188859 -4.1657691 -4.1626472 -4.1791253 -4.1949763 -4.2067828 -4.2277074 -4.2560873 -4.2832818 -4.3081489 -4.324347][-4.3370333 -4.3271823 -4.3014455 -4.265645 -4.2387462 -4.2278781 -4.230937 -4.2426238 -4.2506585 -4.2561741 -4.2656312 -4.2784257 -4.2910032 -4.3029232 -4.3098021][-4.3264475 -4.319787 -4.3048959 -4.2818642 -4.2680736 -4.26701 -4.2752504 -4.2836094 -4.2851987 -4.2860589 -4.2895818 -4.2932858 -4.2943773 -4.2947969 -4.2915773][-4.3086228 -4.2994051 -4.2932086 -4.2845979 -4.2844777 -4.2930989 -4.3044496 -4.3121991 -4.3126311 -4.3109846 -4.3098125 -4.3063564 -4.3001013 -4.29172 -4.2779994][-4.2863884 -4.2716122 -4.27232 -4.2780924 -4.2898932 -4.3051667 -4.3203087 -4.330164 -4.3306561 -4.3255839 -4.3204651 -4.3141437 -4.3034873 -4.28745 -4.2639127]]...]
INFO - root - 2017-12-07 14:44:24.108423: step 20710, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 61h:14m:37s remains)
INFO - root - 2017-12-07 14:44:30.634796: step 20720, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 61h:22m:27s remains)
INFO - root - 2017-12-07 14:44:37.291362: step 20730, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 53h:39m:37s remains)
INFO - root - 2017-12-07 14:44:43.925534: step 20740, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 55h:44m:59s remains)
INFO - root - 2017-12-07 14:44:50.708931: step 20750, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 59h:00m:09s remains)
INFO - root - 2017-12-07 14:44:57.556027: step 20760, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 62h:19m:28s remains)
INFO - root - 2017-12-07 14:45:04.432233: step 20770, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 59h:04m:51s remains)
INFO - root - 2017-12-07 14:45:11.196082: step 20780, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 60h:58m:35s remains)
INFO - root - 2017-12-07 14:45:18.002172: step 20790, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 55h:38m:18s remains)
INFO - root - 2017-12-07 14:45:24.830796: step 20800, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 58h:51m:57s remains)
2017-12-07 14:45:25.625521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2782516 -4.2781911 -4.2763309 -4.2754521 -4.2762756 -4.2775192 -4.2804723 -4.28588 -4.2931533 -4.2993078 -4.3023491 -4.3018293 -4.2993507 -4.2980194 -4.2985134][-4.2883611 -4.2907381 -4.2892814 -4.2849288 -4.2785673 -4.2711573 -4.2685933 -4.2745414 -4.287519 -4.3000154 -4.3060932 -4.3050637 -4.3007846 -4.2971573 -4.2964592][-4.2934647 -4.2982106 -4.2958789 -4.2845206 -4.2657094 -4.2447486 -4.2336411 -4.2404 -4.2635136 -4.2885571 -4.3039522 -4.3065596 -4.30268 -4.2973948 -4.2951136][-4.2799559 -4.2882404 -4.28491 -4.2642426 -4.2285953 -4.1904845 -4.1691175 -4.1775656 -4.2148995 -4.2591438 -4.2906985 -4.3027239 -4.3027496 -4.2974715 -4.293457][-4.2457771 -4.2593479 -4.2564282 -4.2268524 -4.1714859 -4.1116939 -4.0779481 -4.0891147 -4.143537 -4.2122588 -4.2642951 -4.2892652 -4.2954412 -4.2913585 -4.2864733][-4.2025533 -4.2215734 -4.2213426 -4.1862612 -4.1120453 -4.0286245 -3.9816573 -3.9956336 -4.0673141 -4.1602812 -4.2321272 -4.2697268 -4.2816873 -4.2788186 -4.2734771][-4.1781259 -4.1989808 -4.1995649 -4.15905 -4.0687833 -3.9641886 -3.9049132 -3.9209535 -4.00717 -4.1198716 -4.2048535 -4.2495441 -4.2641444 -4.261898 -4.2565303][-4.1976542 -4.2136488 -4.2107282 -4.1676712 -4.0737324 -3.963691 -3.9000006 -3.9144194 -4.0013185 -4.1158218 -4.2013011 -4.2449627 -4.2581553 -4.2557073 -4.2500887][-4.2407312 -4.2495036 -4.2436781 -4.2075105 -4.1300139 -4.03861 -3.9843619 -3.9942107 -4.0641527 -4.1571183 -4.2267694 -4.2612829 -4.2699862 -4.2671633 -4.2617373][-4.277801 -4.2826686 -4.2779951 -4.255424 -4.2067471 -4.1473527 -4.1117229 -4.1170115 -4.1600032 -4.2190065 -4.2637033 -4.2849207 -4.2879357 -4.2843332 -4.279901][-4.2949848 -4.29916 -4.2977343 -4.2887697 -4.2670045 -4.2380366 -4.2196269 -4.2209044 -4.2408309 -4.2698517 -4.2918329 -4.3008142 -4.2991371 -4.2950768 -4.2922215][-4.2951336 -4.3001056 -4.301115 -4.2999907 -4.294219 -4.2843628 -4.277348 -4.276876 -4.283411 -4.29423 -4.3025093 -4.3043337 -4.30065 -4.2974944 -4.2960958][-4.28917 -4.2942343 -4.2964764 -4.2983818 -4.2994223 -4.2985969 -4.2976117 -4.2980418 -4.299727 -4.3023438 -4.30395 -4.3025675 -4.2996321 -4.2979269 -4.2976623][-4.2866664 -4.2906384 -4.2924118 -4.2942777 -4.2966342 -4.2986865 -4.3003759 -4.3017831 -4.3027267 -4.3029695 -4.3024211 -4.3008389 -4.2993016 -4.2986531 -4.2989488][-4.2884459 -4.2903852 -4.2906489 -4.2911692 -4.2927179 -4.2949233 -4.2973919 -4.2996225 -4.3010931 -4.301414 -4.3006735 -4.2994337 -4.2983918 -4.2977915 -4.2977357]]...]
INFO - root - 2017-12-07 14:45:32.342460: step 20810, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 58h:19m:38s remains)
INFO - root - 2017-12-07 14:45:38.971134: step 20820, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 57h:19m:09s remains)
INFO - root - 2017-12-07 14:45:45.857922: step 20830, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 59h:51m:57s remains)
INFO - root - 2017-12-07 14:45:52.743806: step 20840, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.724 sec/batch; 62h:38m:44s remains)
INFO - root - 2017-12-07 14:45:59.540946: step 20850, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.730 sec/batch; 63h:11m:24s remains)
INFO - root - 2017-12-07 14:46:06.420511: step 20860, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 61h:59m:28s remains)
INFO - root - 2017-12-07 14:46:13.177378: step 20870, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 55h:52m:25s remains)
INFO - root - 2017-12-07 14:46:20.098165: step 20880, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 59h:31m:08s remains)
INFO - root - 2017-12-07 14:46:26.916426: step 20890, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 60h:15m:48s remains)
INFO - root - 2017-12-07 14:46:33.778327: step 20900, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 57h:44m:34s remains)
2017-12-07 14:46:34.535454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2825003 -4.2857628 -4.2866025 -4.2851381 -4.2812405 -4.2750459 -4.26863 -4.2631035 -4.259593 -4.2593894 -4.2596922 -4.2593627 -4.2601976 -4.2625842 -4.2652307][-4.2848878 -4.2874479 -4.28774 -4.2861614 -4.2825246 -4.2769365 -4.2711363 -4.264946 -4.2593441 -4.2566934 -4.2560492 -4.2577024 -4.26177 -4.2665992 -4.2696061][-4.2857757 -4.2875972 -4.2878137 -4.2867484 -4.2841873 -4.2799873 -4.2747512 -4.2670965 -4.2576094 -4.2501631 -4.2474279 -4.2517776 -4.260283 -4.2678847 -4.2711806][-4.2849073 -4.2866011 -4.287775 -4.2885962 -4.2886238 -4.2865067 -4.2813249 -4.2704034 -4.2537613 -4.2383294 -4.2325644 -4.2402577 -4.2544732 -4.2655835 -4.269731][-4.282341 -4.2848372 -4.2881632 -4.2924824 -4.2960544 -4.2957158 -4.2889862 -4.2714343 -4.2437811 -4.2182274 -4.2100906 -4.2226267 -4.2441025 -4.2604542 -4.2666774][-4.27976 -4.2844973 -4.2911592 -4.2995882 -4.3059282 -4.3059487 -4.294734 -4.2664185 -4.2252512 -4.1904883 -4.182395 -4.2019777 -4.2321038 -4.2543635 -4.263761][-4.2800946 -4.288094 -4.2985387 -4.3102236 -4.3175311 -4.3153319 -4.2963686 -4.2556481 -4.2025061 -4.1626606 -4.15845 -4.1868854 -4.2246661 -4.2511349 -4.2636523][-4.2838163 -4.295229 -4.3084474 -4.3213239 -4.3274665 -4.3209157 -4.2930546 -4.2414207 -4.181067 -4.1431222 -4.1473784 -4.1843228 -4.2263789 -4.2540507 -4.267879][-4.2894168 -4.3026795 -4.3165617 -4.3288364 -4.3327689 -4.3214469 -4.2864909 -4.228569 -4.1687064 -4.1393433 -4.1539836 -4.1959276 -4.2371864 -4.2626019 -4.275496][-4.2937446 -4.3068495 -4.3194857 -4.3303771 -4.3322263 -4.3178377 -4.2801151 -4.2229152 -4.1707253 -4.1536345 -4.1764736 -4.2171845 -4.252492 -4.2728004 -4.282793][-4.2943368 -4.3059506 -4.3167787 -4.3265672 -4.3280187 -4.3142848 -4.2794533 -4.2291727 -4.1893177 -4.1830435 -4.20809 -4.2420259 -4.2678452 -4.2813039 -4.2871079][-4.2917624 -4.3018456 -4.3121452 -4.3228307 -4.3270631 -4.3174334 -4.2894573 -4.2494249 -4.2212181 -4.2197924 -4.2399306 -4.2637229 -4.2794223 -4.2861624 -4.2872329][-4.2885041 -4.2982092 -4.309835 -4.3233213 -4.3316255 -4.3274584 -4.3078361 -4.2773509 -4.2554417 -4.2526412 -4.2643938 -4.2779264 -4.285398 -4.2872272 -4.2841048][-4.2865992 -4.2965436 -4.3092527 -4.3241658 -4.3347235 -4.3349357 -4.32189 -4.2985926 -4.2795343 -4.2733774 -4.2776771 -4.2840629 -4.2868004 -4.2852759 -4.2792068][-4.2860541 -4.29561 -4.3074722 -4.3211136 -4.331377 -4.3333611 -4.3244987 -4.3068695 -4.2904177 -4.2820983 -4.28162 -4.2836213 -4.2836351 -4.280149 -4.273459]]...]
INFO - root - 2017-12-07 14:46:41.448396: step 20910, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 63h:07m:10s remains)
INFO - root - 2017-12-07 14:46:48.044556: step 20920, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 57h:56m:56s remains)
INFO - root - 2017-12-07 14:46:54.759434: step 20930, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 57h:57m:36s remains)
INFO - root - 2017-12-07 14:47:01.451234: step 20940, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 55h:14m:44s remains)
INFO - root - 2017-12-07 14:47:08.234709: step 20950, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 54h:43m:06s remains)
INFO - root - 2017-12-07 14:47:14.996524: step 20960, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.697 sec/batch; 60h:19m:06s remains)
INFO - root - 2017-12-07 14:47:21.776081: step 20970, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 59h:59m:11s remains)
INFO - root - 2017-12-07 14:47:28.421553: step 20980, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:17m:40s remains)
INFO - root - 2017-12-07 14:47:35.316867: step 20990, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 54h:14m:05s remains)
INFO - root - 2017-12-07 14:47:42.168920: step 21000, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 57h:09m:06s remains)
2017-12-07 14:47:42.977523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2712212 -4.2666831 -4.2677331 -4.27272 -4.2841105 -4.2917519 -4.2917871 -4.2871838 -4.2794366 -4.2734237 -4.2672033 -4.2647986 -4.2688231 -4.2792497 -4.2929621][-4.2254119 -4.2237554 -4.2304869 -4.2375832 -4.250988 -4.2600393 -4.2607985 -4.2515345 -4.2374115 -4.2263803 -4.2190943 -4.21479 -4.2166853 -4.2304835 -4.2516627][-4.1739593 -4.1780024 -4.19091 -4.199151 -4.2120705 -4.2232523 -4.226572 -4.2162838 -4.2028079 -4.1937461 -4.1885653 -4.1799588 -4.1765213 -4.1903791 -4.2140217][-4.1377892 -4.1480441 -4.1696005 -4.1791325 -4.1859384 -4.1895809 -4.1876054 -4.1747231 -4.1682506 -4.1677585 -4.1683979 -4.1606817 -4.1565752 -4.1697845 -4.1894712][-4.1186314 -4.1348825 -4.1604877 -4.1731968 -4.1737847 -4.1632752 -4.1447968 -4.1268535 -4.1271462 -4.1388535 -4.1519227 -4.1544814 -4.1585264 -4.1723995 -4.18392][-4.1026683 -4.1153345 -4.1415472 -4.1598921 -4.1550412 -4.1230254 -4.0775476 -4.047164 -4.0563779 -4.0925283 -4.1324506 -4.1538706 -4.1690807 -4.183301 -4.1893072][-4.08812 -4.0962324 -4.1258173 -4.14387 -4.1224375 -4.0587664 -3.9742155 -3.9219053 -3.9438815 -4.0170374 -4.0936942 -4.1392331 -4.1703396 -4.1896205 -4.198894][-4.0894237 -4.0975246 -4.12852 -4.139967 -4.1023736 -4.0154729 -3.8960917 -3.8149834 -3.8465252 -3.949199 -4.0468216 -4.1114964 -4.1607971 -4.1907082 -4.2106972][-4.0891104 -4.1112404 -4.1474586 -4.1582551 -4.1237068 -4.0505719 -3.9479756 -3.8723629 -3.8885553 -3.9672177 -4.038321 -4.0934892 -4.1454349 -4.1811619 -4.2108359][-4.0815215 -4.1163373 -4.1546874 -4.166522 -4.1490607 -4.1089282 -4.0477867 -4.001008 -4.0030937 -4.0401297 -4.0651321 -4.0871744 -4.1194715 -4.1520615 -4.1881895][-4.0863123 -4.1160479 -4.1480527 -4.1636267 -4.16288 -4.14677 -4.1195478 -4.0974174 -4.0950227 -4.1069489 -4.1007309 -4.0937181 -4.1032348 -4.1278152 -4.1652417][-4.1123724 -4.1312909 -4.1547675 -4.1698837 -4.1746345 -4.1710749 -4.16321 -4.1596012 -4.1631727 -4.1676636 -4.151041 -4.1271496 -4.1196394 -4.1355653 -4.1682158][-4.1701832 -4.182868 -4.1983876 -4.2073827 -4.2109542 -4.2114873 -4.2074537 -4.2101789 -4.2161555 -4.2158012 -4.1972117 -4.1698055 -4.1576366 -4.1701393 -4.1974888][-4.244791 -4.2516084 -4.2582264 -4.2642255 -4.2663522 -4.2640085 -4.2593818 -4.2631817 -4.2693472 -4.2661481 -4.249239 -4.2261667 -4.2149968 -4.22501 -4.2454767][-4.2997203 -4.3021579 -4.3059425 -4.3118167 -4.3115125 -4.3071337 -4.304996 -4.308537 -4.3111997 -4.30649 -4.2945967 -4.2787242 -4.2708282 -4.2768731 -4.2889185]]...]
INFO - root - 2017-12-07 14:47:49.553107: step 21010, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 58h:41m:03s remains)
INFO - root - 2017-12-07 14:47:56.115178: step 21020, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 56h:45m:49s remains)
INFO - root - 2017-12-07 14:48:03.006222: step 21030, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 61h:33m:02s remains)
INFO - root - 2017-12-07 14:48:09.890856: step 21040, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 62h:15m:58s remains)
INFO - root - 2017-12-07 14:48:16.711843: step 21050, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 55h:56m:03s remains)
INFO - root - 2017-12-07 14:48:23.518374: step 21060, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 54h:57m:57s remains)
INFO - root - 2017-12-07 14:48:30.302053: step 21070, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 60h:15m:31s remains)
INFO - root - 2017-12-07 14:48:37.103267: step 21080, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 57h:48m:31s remains)
INFO - root - 2017-12-07 14:48:43.824848: step 21090, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 58h:13m:36s remains)
INFO - root - 2017-12-07 14:48:50.542747: step 21100, loss = 2.06, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 53h:48m:03s remains)
2017-12-07 14:48:51.283715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2657714 -4.2807512 -4.2845535 -4.2827592 -4.275784 -4.26328 -4.2495193 -4.2361403 -4.2232347 -4.2114186 -4.206059 -4.2075129 -4.213027 -4.2206736 -4.2266793][-4.2894793 -4.3031378 -4.3053164 -4.3014994 -4.292366 -4.2785554 -4.2617912 -4.2435641 -4.2268014 -4.2117267 -4.20224 -4.1987882 -4.2004857 -4.2061377 -4.213284][-4.3115973 -4.3204293 -4.3199768 -4.3137283 -4.3007145 -4.2813044 -4.2598844 -4.2393861 -4.2246904 -4.2151532 -4.2104206 -4.2069864 -4.203125 -4.1995416 -4.2012224][-4.3286591 -4.3298917 -4.3219595 -4.3041611 -4.2757149 -4.2399006 -4.2086358 -4.1840105 -4.1749897 -4.1826277 -4.1984119 -4.2109694 -4.2167587 -4.213131 -4.2097092][-4.3346376 -4.3266234 -4.3069177 -4.2723861 -4.2151175 -4.1512513 -4.0995469 -4.0638657 -4.0618658 -4.09202 -4.1373158 -4.1777143 -4.2053494 -4.2173958 -4.2236309][-4.3282394 -4.3091884 -4.2728806 -4.2135062 -4.1174421 -4.0091348 -3.9224277 -3.8761358 -3.8920491 -3.9529912 -4.0326242 -4.106576 -4.1653647 -4.2022219 -4.22637][-4.319243 -4.2921548 -4.2383709 -4.1517706 -4.0233989 -3.8766184 -3.7585149 -3.7044797 -3.7471347 -3.8348746 -3.937268 -4.0380883 -4.1225424 -4.1826057 -4.2228146][-4.3212237 -4.2948208 -4.2410746 -4.1566663 -4.0353618 -3.901581 -3.8051462 -3.7708538 -3.8159709 -3.8907835 -3.970402 -4.0512686 -4.1269689 -4.1857929 -4.2263656][-4.329967 -4.3121295 -4.2753286 -4.2177496 -4.1367078 -4.0508909 -3.9984109 -3.9886041 -4.016705 -4.0559587 -4.0955682 -4.1339269 -4.1776433 -4.2124705 -4.2390051][-4.3290944 -4.3192768 -4.2984762 -4.2659268 -4.221643 -4.1770082 -4.1541967 -4.1522837 -4.1632485 -4.1792622 -4.1963906 -4.2084994 -4.2271695 -4.2420621 -4.2506862][-4.316185 -4.3086238 -4.2948275 -4.2747378 -4.2489514 -4.2272978 -4.2179065 -4.215137 -4.2149959 -4.2171826 -4.2204709 -4.224237 -4.2353482 -4.2423005 -4.241838][-4.2966905 -4.2871351 -4.2734084 -4.2594156 -4.2400541 -4.2239342 -4.2128334 -4.2022238 -4.1926413 -4.1863351 -4.1832461 -4.1864476 -4.196979 -4.2035208 -4.2048287][-4.2729473 -4.2611012 -4.2446432 -4.2324376 -4.21601 -4.1972866 -4.1795912 -4.1575747 -4.1387982 -4.1286745 -4.1288209 -4.1354222 -4.1428208 -4.148294 -4.153214][-4.259881 -4.2498512 -4.2343941 -4.2221675 -4.2048278 -4.1825967 -4.1613154 -4.1334906 -4.1121473 -4.0998111 -4.0976548 -4.1037927 -4.1068454 -4.1073794 -4.1146655][-4.2598109 -4.2542057 -4.2425003 -4.2311568 -4.2148719 -4.1925817 -4.17048 -4.1444807 -4.1262741 -4.1167641 -4.1140714 -4.11689 -4.1162276 -4.1111035 -4.1190295]]...]
INFO - root - 2017-12-07 14:48:58.096735: step 21110, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 59h:28m:31s remains)
INFO - root - 2017-12-07 14:49:04.676310: step 21120, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 58h:32m:26s remains)
INFO - root - 2017-12-07 14:49:11.367376: step 21130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 55h:53m:12s remains)
INFO - root - 2017-12-07 14:49:18.047703: step 21140, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 56h:34m:24s remains)
INFO - root - 2017-12-07 14:49:24.747945: step 21150, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 58h:29m:59s remains)
INFO - root - 2017-12-07 14:49:31.555028: step 21160, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 62h:10m:02s remains)
INFO - root - 2017-12-07 14:49:38.338378: step 21170, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 62h:02m:09s remains)
INFO - root - 2017-12-07 14:49:45.210392: step 21180, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 55h:09m:29s remains)
INFO - root - 2017-12-07 14:49:52.058431: step 21190, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 56h:49m:58s remains)
INFO - root - 2017-12-07 14:49:58.893429: step 21200, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 60h:09m:04s remains)
2017-12-07 14:49:59.599660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2071404 -4.1701565 -4.1379786 -4.1104274 -4.110435 -4.1342349 -4.1637621 -4.1784277 -4.1764469 -4.1688094 -4.1629748 -4.1747632 -4.2030864 -4.2258797 -4.2331343][-4.1850328 -4.1499023 -4.1264644 -4.1116991 -4.1210232 -4.1506481 -4.1863432 -4.2053905 -4.2055058 -4.1952186 -4.1868553 -4.1945772 -4.218297 -4.2349243 -4.2352748][-4.1607113 -4.1360493 -4.1296444 -4.1316977 -4.1450834 -4.1710553 -4.20008 -4.2110224 -4.2060542 -4.1953831 -4.1885715 -4.1944895 -4.2148108 -4.2277522 -4.2267723][-4.134131 -4.134491 -4.1545982 -4.1747818 -4.1881433 -4.2033648 -4.2124233 -4.2030182 -4.1855235 -4.176466 -4.1793194 -4.1908512 -4.2111311 -4.2216268 -4.2175884][-4.1199465 -4.1466269 -4.1881518 -4.2187181 -4.2275748 -4.2258391 -4.2061353 -4.1664896 -4.1308689 -4.1259713 -4.1487989 -4.178709 -4.2061553 -4.2158027 -4.2085648][-4.1429009 -4.1817021 -4.2253547 -4.2511191 -4.2453876 -4.2154016 -4.162147 -4.0938878 -4.0474105 -4.05638 -4.1109233 -4.172555 -4.2107081 -4.219408 -4.2088332][-4.1947465 -4.23525 -4.268249 -4.2786379 -4.2517853 -4.1888719 -4.0956779 -3.996767 -3.941397 -3.9708166 -4.0638385 -4.1620297 -4.2144952 -4.2203383 -4.2053742][-4.2409859 -4.2772651 -4.3009548 -4.3014836 -4.2587929 -4.168992 -4.0451641 -3.924258 -3.8642704 -3.9100814 -4.0252547 -4.143682 -4.2073965 -4.2138939 -4.1937065][-4.2670569 -4.2973423 -4.3141751 -4.3132176 -4.2715583 -4.1832509 -4.0662456 -3.9636707 -3.9227068 -3.9674313 -4.0608792 -4.1569314 -4.2154059 -4.2202187 -4.1959519][-4.2748032 -4.3003969 -4.3163323 -4.3169165 -4.2847204 -4.2164145 -4.1286893 -4.0685949 -4.0580072 -4.0904675 -4.142293 -4.1956286 -4.2345123 -4.2346549 -4.2089806][-4.2677231 -4.2896657 -4.3053317 -4.3061104 -4.2802405 -4.2271891 -4.160687 -4.1302624 -4.1389813 -4.1635685 -4.1898985 -4.2140789 -4.2352042 -4.2345185 -4.2135391][-4.2579226 -4.2772622 -4.2896123 -4.2857113 -4.2618837 -4.2186618 -4.1680317 -4.155993 -4.1749125 -4.1977167 -4.2147479 -4.2246494 -4.2323046 -4.2278342 -4.2053][-4.2603745 -4.2796764 -4.2868042 -4.2767625 -4.2524266 -4.2160578 -4.1809611 -4.1808228 -4.2047105 -4.2255611 -4.2373886 -4.2396235 -4.2338471 -4.2219605 -4.1987371][-4.2757354 -4.29359 -4.2943087 -4.2779918 -4.255301 -4.2283549 -4.2066379 -4.2113504 -4.23719 -4.25539 -4.2653165 -4.264164 -4.2510705 -4.2315993 -4.2053256][-4.2824674 -4.2962203 -4.2908154 -4.2706447 -4.2512145 -4.2351804 -4.2262397 -4.2339983 -4.2603106 -4.2790184 -4.2917953 -4.2922745 -4.2786207 -4.2502289 -4.2128034]]...]
INFO - root - 2017-12-07 14:50:06.320946: step 21210, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 54h:10m:41s remains)
INFO - root - 2017-12-07 14:50:12.973228: step 21220, loss = 2.03, batch loss = 1.97 (12.6 examples/sec; 0.637 sec/batch; 55h:04m:50s remains)
INFO - root - 2017-12-07 14:50:19.805083: step 21230, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 59h:33m:22s remains)
INFO - root - 2017-12-07 14:50:26.636174: step 21240, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 61h:40m:04s remains)
INFO - root - 2017-12-07 14:50:33.553330: step 21250, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 57h:33m:43s remains)
INFO - root - 2017-12-07 14:50:40.209903: step 21260, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 54h:39m:46s remains)
INFO - root - 2017-12-07 14:50:47.125413: step 21270, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 58h:44m:47s remains)
INFO - root - 2017-12-07 14:50:54.055750: step 21280, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.737 sec/batch; 63h:40m:46s remains)
INFO - root - 2017-12-07 14:51:00.873860: step 21290, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 60h:44m:10s remains)
INFO - root - 2017-12-07 14:51:07.685581: step 21300, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.661 sec/batch; 57h:08m:58s remains)
2017-12-07 14:51:08.413375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23629 -4.2491302 -4.2679353 -4.2891045 -4.2944317 -4.2759719 -4.2396965 -4.2158241 -4.2134624 -4.209187 -4.2114072 -4.2249932 -4.2453251 -4.2504597 -4.2667518][-4.2028236 -4.2162228 -4.2384834 -4.2668552 -4.272325 -4.2499995 -4.2097921 -4.1850729 -4.1922574 -4.2018452 -4.2102923 -4.2258339 -4.2494817 -4.2560191 -4.2703724][-4.1859531 -4.20316 -4.2279654 -4.2584162 -4.2580285 -4.2248287 -4.1808066 -4.15489 -4.16621 -4.1887946 -4.2064047 -4.2290955 -4.2555957 -4.2632933 -4.27565][-4.1810355 -4.1969967 -4.2128897 -4.2379827 -4.2327046 -4.1963873 -4.150063 -4.1221118 -4.1343212 -4.1640725 -4.1935568 -4.2277021 -4.2582831 -4.26787 -4.2797961][-4.1931758 -4.2026839 -4.2066226 -4.2216363 -4.2173176 -4.19162 -4.1482129 -4.1121078 -4.1157236 -4.1464682 -4.1833591 -4.2249341 -4.25884 -4.2698088 -4.2819252][-4.2062068 -4.2005863 -4.1920247 -4.198535 -4.2021971 -4.1930056 -4.1650648 -4.1264997 -4.1201572 -4.1484456 -4.1878853 -4.229198 -4.2610669 -4.2708278 -4.2826753][-4.19089 -4.1687021 -4.1437006 -4.1415749 -4.1536708 -4.1631351 -4.1577482 -4.132431 -4.1251411 -4.151319 -4.1889877 -4.2280235 -4.256701 -4.2671442 -4.2805724][-4.1485624 -4.12213 -4.0849218 -4.0777364 -4.1005144 -4.1303563 -4.1393247 -4.1281033 -4.1188993 -4.1415825 -4.1789827 -4.2185783 -4.2462306 -4.2593832 -4.276917][-4.0964384 -4.0798206 -4.0461884 -4.0467167 -4.08315 -4.1277561 -4.1469812 -4.1420708 -4.1306648 -4.1446562 -4.1764607 -4.2146616 -4.2400813 -4.2536807 -4.2744951][-4.0665684 -4.074297 -4.0631704 -4.0742974 -4.1144013 -4.1588016 -4.1771021 -4.1740818 -4.1626415 -4.1649895 -4.1855206 -4.2186246 -4.2416081 -4.25287 -4.2742634][-4.0915928 -4.1181765 -4.1315193 -4.1480174 -4.1745563 -4.2004275 -4.212677 -4.2126403 -4.2052755 -4.1959171 -4.2021022 -4.2292085 -4.2490268 -4.255445 -4.2740431][-4.1472878 -4.1788363 -4.20787 -4.2235036 -4.2352152 -4.2451844 -4.2476172 -4.2475433 -4.2476292 -4.2354684 -4.2304373 -4.24805 -4.2607279 -4.2597079 -4.2742262][-4.2128 -4.2366056 -4.2602582 -4.2727733 -4.269537 -4.2653308 -4.2609591 -4.2624135 -4.2657862 -4.2538004 -4.2439251 -4.2547617 -4.2625256 -4.25938 -4.2729831][-4.264246 -4.2734566 -4.2859316 -4.2943869 -4.2859015 -4.2708225 -4.26312 -4.2677631 -4.269927 -4.254045 -4.2402415 -4.2467656 -4.2557173 -4.2556391 -4.2708044][-4.27282 -4.2767119 -4.2829428 -4.2831864 -4.2681918 -4.246594 -4.2369719 -4.2414093 -4.2410321 -4.2319932 -4.2217517 -4.2305312 -4.2469397 -4.2530651 -4.2698174]]...]
INFO - root - 2017-12-07 14:51:15.146687: step 21310, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 60h:48m:19s remains)
INFO - root - 2017-12-07 14:51:21.726003: step 21320, loss = 2.10, batch loss = 2.05 (12.1 examples/sec; 0.659 sec/batch; 56h:56m:52s remains)
INFO - root - 2017-12-07 14:51:28.518785: step 21330, loss = 2.03, batch loss = 1.98 (12.0 examples/sec; 0.664 sec/batch; 57h:25m:22s remains)
INFO - root - 2017-12-07 14:51:35.326517: step 21340, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 61h:24m:45s remains)
INFO - root - 2017-12-07 14:51:42.198616: step 21350, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 63h:05m:59s remains)
INFO - root - 2017-12-07 14:51:49.006259: step 21360, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 57h:39m:03s remains)
INFO - root - 2017-12-07 14:51:55.899894: step 21370, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 57h:37m:35s remains)
INFO - root - 2017-12-07 14:52:02.786054: step 21380, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 60h:06m:31s remains)
INFO - root - 2017-12-07 14:52:09.672550: step 21390, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 61h:44m:23s remains)
INFO - root - 2017-12-07 14:52:16.497748: step 21400, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 55h:43m:11s remains)
2017-12-07 14:52:17.245707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3420916 -4.3381085 -4.334136 -4.3316526 -4.330615 -4.3289566 -4.3210664 -4.3095884 -4.293046 -4.279964 -4.2778397 -4.2867155 -4.2999439 -4.3096886 -4.3177724][-4.339623 -4.334456 -4.3254304 -4.3178 -4.3121867 -4.3083477 -4.2978129 -4.2838154 -4.2642126 -4.2470579 -4.2399526 -4.2458839 -4.2579904 -4.2692881 -4.2838306][-4.33607 -4.3275328 -4.3104758 -4.2947655 -4.2810526 -4.273653 -4.2615547 -4.2482719 -4.229588 -4.210824 -4.1998034 -4.2002182 -4.20538 -4.2166367 -4.2401719][-4.3234992 -4.3092208 -4.2820182 -4.256763 -4.2341943 -4.2204633 -4.20529 -4.1934657 -4.1822004 -4.1693993 -4.1598811 -4.15661 -4.1546493 -4.1634126 -4.1946411][-4.3006926 -4.280941 -4.245738 -4.21289 -4.1831694 -4.1592536 -4.1323528 -4.1134143 -4.1146851 -4.1199942 -4.1231375 -4.1239257 -4.1220503 -4.1318583 -4.16828][-4.26884 -4.2490263 -4.2138562 -4.1758876 -4.1376386 -4.0987997 -4.0507293 -4.009666 -4.0189333 -4.0567026 -4.0884795 -4.1067858 -4.11574 -4.1322308 -4.169066][-4.2292809 -4.2122731 -4.1873503 -4.1510735 -4.1054249 -4.0492516 -3.9737058 -3.8989606 -3.908041 -3.986207 -4.0551786 -4.0990911 -4.1249189 -4.14955 -4.1862159][-4.1815147 -4.1662803 -4.1574173 -4.1353927 -4.0932093 -4.0279307 -3.9319463 -3.8295343 -3.8338962 -3.9387398 -4.0314488 -4.0931635 -4.1349354 -4.16861 -4.2085385][-4.1454234 -4.130795 -4.1309767 -4.1281614 -4.1037378 -4.0480952 -3.9631979 -3.8759651 -3.8772438 -3.9583035 -4.0321989 -4.085422 -4.1298723 -4.1715965 -4.2182674][-4.120841 -4.1092381 -4.1108418 -4.1242657 -4.1235619 -4.0869141 -4.0293965 -3.9785337 -3.9762959 -4.0126133 -4.0471678 -4.0768752 -4.1140318 -4.1624241 -4.2172637][-4.0952806 -4.0870004 -4.0870886 -4.1109967 -4.1298885 -4.1151175 -4.0832257 -4.0604191 -4.0562744 -4.06054 -4.0626569 -4.0704074 -4.1006737 -4.1545086 -4.2153144][-4.0929217 -4.0863442 -4.0886827 -4.1126766 -4.136621 -4.1376195 -4.1245303 -4.1150122 -4.107244 -4.0930939 -4.0780363 -4.0752068 -4.1017537 -4.1574869 -4.2197452][-4.1207533 -4.1128445 -4.1177368 -4.1345649 -4.1505857 -4.1588216 -4.1570687 -4.1536512 -4.1457906 -4.1285172 -4.1126451 -4.1092434 -4.1325336 -4.1821561 -4.2377234][-4.1550407 -4.1456561 -4.1507678 -4.1598048 -4.1674728 -4.1764774 -4.1811748 -4.1827927 -4.1797419 -4.1704788 -4.1638985 -4.1658607 -4.1846256 -4.2219534 -4.2650375][-4.1911926 -4.1834359 -4.1852093 -4.1867485 -4.1865659 -4.193295 -4.2013168 -4.20729 -4.2117152 -4.2132921 -4.2161875 -4.2231903 -4.2382631 -4.2640047 -4.2940378]]...]
INFO - root - 2017-12-07 14:52:24.031043: step 21410, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 60h:00m:18s remains)
INFO - root - 2017-12-07 14:52:30.673273: step 21420, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 62h:10m:38s remains)
INFO - root - 2017-12-07 14:52:37.384254: step 21430, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 56h:01m:24s remains)
INFO - root - 2017-12-07 14:52:44.190975: step 21440, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 56h:47m:22s remains)
INFO - root - 2017-12-07 14:52:51.012026: step 21450, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 61h:53m:01s remains)
INFO - root - 2017-12-07 14:52:57.867031: step 21460, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 60h:38m:37s remains)
INFO - root - 2017-12-07 14:53:04.589434: step 21470, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 56h:28m:21s remains)
INFO - root - 2017-12-07 14:53:11.326242: step 21480, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 56h:02m:05s remains)
INFO - root - 2017-12-07 14:53:18.110192: step 21490, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 61h:47m:05s remains)
INFO - root - 2017-12-07 14:53:24.931569: step 21500, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 62h:33m:48s remains)
2017-12-07 14:53:25.630851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2127352 -4.2110047 -4.2114797 -4.216485 -4.2217565 -4.216609 -4.2139349 -4.217556 -4.2247949 -4.2334018 -4.2384963 -4.2413735 -4.2425551 -4.2386947 -4.2299619][-4.2456226 -4.2466545 -4.2468009 -4.2545872 -4.2638392 -4.2605238 -4.2567244 -4.2564383 -4.2580876 -4.2591267 -4.2547255 -4.2471232 -4.2395644 -4.2300758 -4.2227974][-4.2826557 -4.2873554 -4.2908211 -4.3001962 -4.3069782 -4.2991643 -4.2881918 -4.2794318 -4.275414 -4.2722826 -4.262548 -4.2485847 -4.2337337 -4.221334 -4.2187166][-4.2995443 -4.3069825 -4.3128142 -4.3191023 -4.3150277 -4.2951159 -4.2747278 -4.2604961 -4.2542648 -4.2504697 -4.2437148 -4.2332678 -4.2194915 -4.2094235 -4.2104387][-4.2990446 -4.3048258 -4.304584 -4.301434 -4.2830896 -4.24983 -4.2192783 -4.2005153 -4.1955118 -4.1942987 -4.1949773 -4.1978197 -4.1928883 -4.1884289 -4.1923728][-4.272428 -4.2657714 -4.2533336 -4.2357349 -4.2044616 -4.1615734 -4.1230454 -4.1045651 -4.1120415 -4.1288157 -4.1479464 -4.1659794 -4.1725335 -4.1741958 -4.178916][-4.2243042 -4.2005434 -4.1729956 -4.1425877 -4.1016455 -4.0500221 -4.0061579 -3.9931905 -4.0197124 -4.0620542 -4.1011691 -4.1337233 -4.1518173 -4.1605024 -4.1670909][-4.1754169 -4.1420016 -4.1040297 -4.0609794 -4.0050874 -3.9403195 -3.8949118 -3.894393 -3.9471331 -4.0170693 -4.0722542 -4.1155438 -4.1441083 -4.1578279 -4.1628051][-4.1560087 -4.1241865 -4.0774775 -4.0141973 -3.9361413 -3.8584189 -3.8226237 -3.8481059 -3.9324489 -4.0184097 -4.0786171 -4.1233678 -4.1521554 -4.1632257 -4.1644893][-4.1648374 -4.1300964 -4.0719919 -3.9883595 -3.8967648 -3.8337126 -3.8326657 -3.8838236 -3.9717205 -4.0489349 -4.1031628 -4.1426196 -4.1657619 -4.1723866 -4.1675797][-4.1770725 -4.141192 -4.0826173 -4.0031624 -3.9304304 -3.8995941 -3.9180593 -3.9659452 -4.0315938 -4.08738 -4.1265206 -4.1542892 -4.169065 -4.1725168 -4.1650791][-4.1941252 -4.1694641 -4.1281395 -4.0716448 -4.0236039 -4.004766 -4.0239372 -4.0605383 -4.1073189 -4.14453 -4.1679468 -4.1815257 -4.1851749 -4.1813927 -4.1701107][-4.2231464 -4.2125487 -4.1868577 -4.1496868 -4.1174984 -4.1023269 -4.1187654 -4.1505 -4.18573 -4.2059035 -4.2118092 -4.2087274 -4.2007961 -4.1898575 -4.1753712][-4.2471128 -4.2448735 -4.2287583 -4.20463 -4.1829653 -4.1714764 -4.182765 -4.2082295 -4.2337809 -4.2397251 -4.2306762 -4.2162657 -4.2050171 -4.1939368 -4.18058][-4.2425127 -4.2430558 -4.2339125 -4.2187324 -4.20463 -4.1963911 -4.2046027 -4.2236791 -4.2400579 -4.23731 -4.2216887 -4.205442 -4.1964731 -4.1884527 -4.1764221]]...]
INFO - root - 2017-12-07 14:53:32.317222: step 21510, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 56h:04m:41s remains)
INFO - root - 2017-12-07 14:53:38.970060: step 21520, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 61h:19m:06s remains)
INFO - root - 2017-12-07 14:53:45.832383: step 21530, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 60h:10m:03s remains)
INFO - root - 2017-12-07 14:53:52.613960: step 21540, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 59h:07m:54s remains)
INFO - root - 2017-12-07 14:53:59.392102: step 21550, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 55h:37m:06s remains)
INFO - root - 2017-12-07 14:54:06.202951: step 21560, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 58h:43m:27s remains)
INFO - root - 2017-12-07 14:54:13.068720: step 21570, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 59h:55m:14s remains)
INFO - root - 2017-12-07 14:54:19.781588: step 21580, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 59h:38m:42s remains)
INFO - root - 2017-12-07 14:54:26.620134: step 21590, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 56h:18m:36s remains)
INFO - root - 2017-12-07 14:54:33.483257: step 21600, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 55h:32m:43s remains)
2017-12-07 14:54:34.193964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2060938 -4.2115116 -4.2191448 -4.2192612 -4.210247 -4.1995344 -4.1966 -4.1984968 -4.1966491 -4.193635 -4.1993475 -4.2069254 -4.2183986 -4.2347918 -4.2545066][-4.2115145 -4.2152376 -4.2199588 -4.2168336 -4.2036042 -4.1894884 -4.1838126 -4.1847773 -4.1846743 -4.1848245 -4.1965313 -4.207675 -4.2180786 -4.2323289 -4.2518044][-4.1822124 -4.1797214 -4.1778822 -4.169014 -4.1513581 -4.1348538 -4.1322312 -4.1409264 -4.1500587 -4.1596522 -4.1818666 -4.2008343 -4.2135572 -4.2262273 -4.2452841][-4.1312442 -4.1234694 -4.1176214 -4.1063147 -4.0870023 -4.0705318 -4.0759864 -4.0980153 -4.1194968 -4.1401258 -4.1710124 -4.1971292 -4.2122455 -4.223196 -4.2409549][-4.0728822 -4.0650849 -4.0612164 -4.052669 -4.0346947 -4.020874 -4.0351562 -4.0694375 -4.1023641 -4.1329751 -4.1680517 -4.1963305 -4.212481 -4.2228508 -4.2398682][-4.0315604 -4.0259767 -4.0246816 -4.0164075 -3.9978995 -3.9836712 -4.001668 -4.0425916 -4.0825276 -4.1209674 -4.1609817 -4.1913295 -4.2109437 -4.2229137 -4.2406139][-4.01524 -4.0128784 -4.0122604 -4.0022731 -3.9798663 -3.9587326 -3.970459 -4.0096903 -4.0506811 -4.093915 -4.14056 -4.1770191 -4.2034082 -4.2204218 -4.2410183][-4.0093708 -4.0091095 -4.0077014 -3.9943511 -3.9675722 -3.9391181 -3.9430022 -3.9775114 -4.018137 -4.0638709 -4.116046 -4.15847 -4.1911373 -4.2139359 -4.2388778][-4.0082293 -4.0057445 -4.0009966 -3.9842789 -3.9560616 -3.9292493 -3.9306633 -3.9601009 -3.9994102 -4.0469656 -4.1016884 -4.1463728 -4.1819859 -4.2084947 -4.2370424][-4.027864 -4.0255046 -4.0213413 -4.0077314 -3.9819961 -3.9566174 -3.9536171 -3.9721079 -4.0026479 -4.0455732 -4.0973759 -4.1407423 -4.1769695 -4.2062583 -4.237339][-4.0603948 -4.0594778 -4.05837 -4.0510511 -4.0318184 -4.0095019 -4.0028691 -4.0127006 -4.034121 -4.0672016 -4.1093392 -4.1454926 -4.1789923 -4.2092872 -4.2403479][-4.1132841 -4.1113658 -4.11119 -4.1080341 -4.0970588 -4.0832577 -4.0785689 -4.0848122 -4.0971618 -4.1172519 -4.1448092 -4.1685882 -4.1945424 -4.2209873 -4.2475667][-4.1684203 -4.1693816 -4.1708059 -4.1700673 -4.1648211 -4.1585245 -4.1574335 -4.1615171 -4.1668196 -4.175735 -4.1898794 -4.2015076 -4.2180867 -4.2377763 -4.2575517][-4.1819873 -4.1863475 -4.1896338 -4.1898556 -4.1853704 -4.181736 -4.18427 -4.1917186 -4.1993365 -4.2082529 -4.2194128 -4.2277308 -4.2396269 -4.253572 -4.2673187][-4.1522903 -4.1570311 -4.1599631 -4.1586027 -4.1521044 -4.1490073 -4.1557817 -4.1704693 -4.1866117 -4.2036595 -4.2220435 -4.236444 -4.2504244 -4.263021 -4.2737937]]...]
INFO - root - 2017-12-07 14:54:41.043107: step 21610, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 61h:34m:01s remains)
INFO - root - 2017-12-07 14:54:47.696510: step 21620, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 58h:10m:22s remains)
INFO - root - 2017-12-07 14:54:54.321009: step 21630, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 59h:33m:07s remains)
INFO - root - 2017-12-07 14:55:01.271003: step 21640, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.726 sec/batch; 62h:41m:43s remains)
INFO - root - 2017-12-07 14:55:08.145120: step 21650, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.721 sec/batch; 62h:12m:48s remains)
INFO - root - 2017-12-07 14:55:14.929507: step 21660, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 56h:37m:24s remains)
INFO - root - 2017-12-07 14:55:21.767513: step 21670, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 59h:06m:15s remains)
INFO - root - 2017-12-07 14:55:28.656913: step 21680, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 58h:49m:10s remains)
INFO - root - 2017-12-07 14:55:35.601051: step 21690, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 60h:51m:46s remains)
INFO - root - 2017-12-07 14:55:42.387855: step 21700, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 61h:06m:48s remains)
2017-12-07 14:55:43.069729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2079253 -4.2069221 -4.2137079 -4.2166972 -4.21342 -4.1990018 -4.1720476 -4.1450171 -4.1315217 -4.1581874 -4.1824832 -4.1980882 -4.2218809 -4.2399483 -4.2530022][-4.1956925 -4.1876297 -4.1907454 -4.1914477 -4.1855125 -4.1682134 -4.1352549 -4.0996337 -4.0840139 -4.1155987 -4.1418171 -4.1609621 -4.1922865 -4.2204533 -4.2399917][-4.1867657 -4.1813493 -4.1877027 -4.1875973 -4.1779408 -4.161067 -4.1271911 -4.0810971 -4.0603328 -4.0960369 -4.1243067 -4.15054 -4.1878548 -4.2188983 -4.2390904][-4.1771345 -4.1779304 -4.1887465 -4.1865 -4.1739717 -4.1577907 -4.1233172 -4.0733895 -4.053123 -4.0901771 -4.1252117 -4.1579041 -4.1982508 -4.2266312 -4.2463717][-4.1690979 -4.1682158 -4.1748075 -4.1648893 -4.1449385 -4.1198668 -4.0729585 -4.0116725 -3.9991114 -4.05864 -4.1144671 -4.1586208 -4.2020407 -4.2315345 -4.2523456][-4.168324 -4.1602979 -4.158915 -4.1415968 -4.1059046 -4.04926 -3.9647267 -3.8740222 -3.8760827 -3.9823744 -4.0741529 -4.1403894 -4.1965632 -4.2346992 -4.2582226][-4.1770611 -4.1661329 -4.1587162 -4.1353245 -4.0799723 -3.9907641 -3.8628244 -3.7397513 -3.7636604 -3.9145055 -4.0368724 -4.122231 -4.1958151 -4.2442288 -4.2684875][-4.1957507 -4.1865129 -4.1820645 -4.16628 -4.1146426 -4.0273323 -3.8974459 -3.7832429 -3.8113661 -3.9512773 -4.0666466 -4.1468081 -4.220521 -4.2667775 -4.285912][-4.2136922 -4.2099071 -4.2121344 -4.209816 -4.1772027 -4.1121607 -4.0118804 -3.9294868 -3.9459298 -4.0486922 -4.1421733 -4.2040682 -4.2587843 -4.29177 -4.3017826][-4.237648 -4.2373137 -4.2420368 -4.2449112 -4.2241907 -4.1747561 -4.0927272 -4.0260811 -4.0319386 -4.1046686 -4.1839223 -4.237309 -4.2806592 -4.303956 -4.3084421][-4.2590942 -4.2615457 -4.2653174 -4.2641411 -4.243299 -4.1983552 -4.1214833 -4.0603771 -4.0633473 -4.1192131 -4.1900358 -4.2450485 -4.2884083 -4.3089108 -4.3107486][-4.2610521 -4.2641258 -4.265758 -4.2614374 -4.238709 -4.1965971 -4.1267138 -4.0759463 -4.0810809 -4.1271839 -4.1937218 -4.2515464 -4.2953124 -4.3146882 -4.3158827][-4.2529449 -4.2514362 -4.2549038 -4.2509813 -4.230474 -4.1955829 -4.1410842 -4.104001 -4.1111045 -4.1493344 -4.2081795 -4.2617846 -4.3004909 -4.3167977 -4.3180194][-4.2500362 -4.2456846 -4.253109 -4.2540617 -4.239284 -4.2102194 -4.1706014 -4.144598 -4.1507282 -4.1813507 -4.2280025 -4.2717361 -4.3033504 -4.3159428 -4.3175278][-4.2380672 -4.2330432 -4.2415395 -4.2454925 -4.2374215 -4.2181 -4.1902137 -4.1698737 -4.17242 -4.1959853 -4.2337494 -4.269989 -4.2972674 -4.3091273 -4.3136473]]...]
INFO - root - 2017-12-07 14:55:49.829284: step 21710, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 59h:12m:13s remains)
INFO - root - 2017-12-07 14:55:56.587843: step 21720, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 61h:47m:47s remains)
INFO - root - 2017-12-07 14:56:03.471681: step 21730, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 56h:42m:48s remains)
INFO - root - 2017-12-07 14:56:10.279994: step 21740, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 55h:10m:00s remains)
INFO - root - 2017-12-07 14:56:17.068747: step 21750, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 61h:30m:09s remains)
INFO - root - 2017-12-07 14:56:23.902428: step 21760, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 60h:00m:23s remains)
INFO - root - 2017-12-07 14:56:30.647712: step 21770, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 59h:11m:43s remains)
INFO - root - 2017-12-07 14:56:37.407468: step 21780, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 54h:04m:41s remains)
INFO - root - 2017-12-07 14:56:44.300208: step 21790, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.698 sec/batch; 60h:15m:39s remains)
INFO - root - 2017-12-07 14:56:51.155002: step 21800, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 58h:03m:53s remains)
2017-12-07 14:56:51.812380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2710414 -4.2637467 -4.24554 -4.1890044 -4.0893416 -4.0173364 -4.0387712 -4.1069431 -4.1625628 -4.2103996 -4.2484255 -4.2716818 -4.2914495 -4.3089342 -4.3268919][-4.2707887 -4.2721572 -4.2562103 -4.2046747 -4.1080418 -4.0200834 -4.0207171 -4.0910482 -4.1586018 -4.2145066 -4.2567606 -4.2804203 -4.2977042 -4.314837 -4.3315086][-4.2674556 -4.2768593 -4.2679462 -4.2256546 -4.14224 -4.0482316 -4.0194254 -4.0757427 -4.1492615 -4.2093763 -4.2541995 -4.2792234 -4.298131 -4.3170471 -4.3339205][-4.2667437 -4.2801032 -4.2765636 -4.2408009 -4.16812 -4.0760884 -4.0177951 -4.050158 -4.1293316 -4.1953154 -4.2420053 -4.270133 -4.2946219 -4.3169427 -4.3346071][-4.2658381 -4.2817688 -4.2778692 -4.2457619 -4.1814866 -4.0877237 -3.9994159 -4.0011425 -4.0894709 -4.1714668 -4.223268 -4.256855 -4.2882538 -4.3149581 -4.3334956][-4.2757311 -4.2890339 -4.2788272 -4.2464643 -4.1898828 -4.0886335 -3.9693992 -3.9350114 -4.0320554 -4.1381803 -4.2018728 -4.2437167 -4.2817273 -4.3127971 -4.3326578][-4.2870855 -4.2916517 -4.2767739 -4.2449932 -4.1953583 -4.094676 -3.9558747 -3.8864913 -3.9739852 -4.0967393 -4.1770477 -4.2293634 -4.274755 -4.3118277 -4.33319][-4.28947 -4.2841043 -4.2682991 -4.2408814 -4.1992588 -4.1081257 -3.97443 -3.8916435 -3.9520698 -4.0681787 -4.1587825 -4.21788 -4.2680736 -4.3104506 -4.333427][-4.2807579 -4.2691436 -4.2538362 -4.2331409 -4.200789 -4.1281552 -4.025815 -3.9593332 -3.9945738 -4.0840349 -4.1656685 -4.2230945 -4.2726126 -4.3132153 -4.3349643][-4.2738872 -4.2564511 -4.2425375 -4.2257161 -4.1989956 -4.146678 -4.0806332 -4.0389991 -4.0613565 -4.1254139 -4.1891994 -4.2387786 -4.2830796 -4.3185968 -4.3373823][-4.2689028 -4.2489433 -4.2327023 -4.2169476 -4.197278 -4.165204 -4.1268597 -4.1031122 -4.120862 -4.1684818 -4.2157235 -4.2549086 -4.2920451 -4.3212643 -4.3386431][-4.2696915 -4.2481585 -4.2296972 -4.2178655 -4.2054024 -4.1867514 -4.1672339 -4.1584992 -4.1740804 -4.2074265 -4.2390332 -4.268507 -4.299212 -4.3234792 -4.3400564][-4.2714334 -4.251204 -4.2343588 -4.2261086 -4.2184777 -4.2087164 -4.2032542 -4.2049208 -4.21782 -4.2379274 -4.2562027 -4.2786231 -4.305047 -4.3259687 -4.341928][-4.2715664 -4.2563791 -4.2451453 -4.2371764 -4.2310171 -4.2273436 -4.2284384 -4.2334476 -4.2435203 -4.2562 -4.2677093 -4.2873144 -4.3117085 -4.3303695 -4.3446827][-4.2714047 -4.2615495 -4.2529669 -4.2444658 -4.2385325 -4.2354336 -4.2356696 -4.2403445 -4.2517633 -4.2648306 -4.2765956 -4.29547 -4.3176222 -4.3353171 -4.3478374]]...]
INFO - root - 2017-12-07 14:56:58.595822: step 21810, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 58h:59m:58s remains)
INFO - root - 2017-12-07 14:57:05.395539: step 21820, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 61h:39m:57s remains)
INFO - root - 2017-12-07 14:57:12.303317: step 21830, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 59h:26m:02s remains)
INFO - root - 2017-12-07 14:57:19.128989: step 21840, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 59h:03m:12s remains)
INFO - root - 2017-12-07 14:57:25.897557: step 21850, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.634 sec/batch; 54h:42m:45s remains)
INFO - root - 2017-12-07 14:57:32.669293: step 21860, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 54h:51m:19s remains)
INFO - root - 2017-12-07 14:57:39.535301: step 21870, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 59h:49m:52s remains)
INFO - root - 2017-12-07 14:57:46.379911: step 21880, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.724 sec/batch; 62h:29m:51s remains)
INFO - root - 2017-12-07 14:57:53.143786: step 21890, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 58h:59m:57s remains)
INFO - root - 2017-12-07 14:57:59.910431: step 21900, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 57h:07m:16s remains)
2017-12-07 14:58:00.648584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2615247 -4.2589569 -4.2718372 -4.2868786 -4.2873673 -4.2774734 -4.2589254 -4.2449951 -4.2403359 -4.2424421 -4.2443795 -4.2503257 -4.2633595 -4.2826943 -4.3033509][-4.2141495 -4.2141542 -4.244801 -4.2770066 -4.2859764 -4.2780943 -4.2547607 -4.2316289 -4.2206125 -4.219368 -4.2206526 -4.2314925 -4.2489386 -4.2701015 -4.2932973][-4.1765614 -4.1781316 -4.2223635 -4.2638788 -4.274231 -4.2645593 -4.2352085 -4.2049103 -4.1937938 -4.1921682 -4.1921959 -4.2064586 -4.228271 -4.2524824 -4.2794][-4.1573853 -4.149848 -4.192287 -4.230135 -4.2356424 -4.2198834 -4.1845641 -4.1507993 -4.1463637 -4.1575255 -4.1643267 -4.1822062 -4.2044611 -4.2302175 -4.2619929][-4.1627588 -4.1478109 -4.1748409 -4.1913137 -4.182229 -4.1550131 -4.107914 -4.07343 -4.0874472 -4.1219764 -4.1438031 -4.1685176 -4.1943026 -4.22242 -4.2562556][-4.1697712 -4.1485462 -4.1529341 -4.1400352 -4.1032195 -4.0492091 -3.9776077 -3.9372318 -3.979037 -4.0490522 -4.0972605 -4.1377478 -4.1779137 -4.2149763 -4.2522035][-4.1713638 -4.136548 -4.1193361 -4.0890532 -4.0323696 -3.9473338 -3.8327196 -3.7624214 -3.8308921 -3.9450388 -4.0269356 -4.0907135 -4.14878 -4.1980824 -4.2434816][-4.1965127 -4.1547332 -4.1296768 -4.1005993 -4.04626 -3.9508541 -3.8135633 -3.7189023 -3.7721505 -3.883487 -3.9747179 -4.0549579 -4.1296921 -4.1899271 -4.2407627][-4.2335443 -4.1987939 -4.1821141 -4.1642108 -4.1241803 -4.0512023 -3.9503026 -3.8768191 -3.88951 -3.9439867 -4.0054464 -4.0758867 -4.1469631 -4.20624 -4.2529817][-4.2692471 -4.2417927 -4.2329922 -4.2194195 -4.1886663 -4.1401105 -4.0803871 -4.0328717 -4.021286 -4.0326953 -4.0643682 -4.1162949 -4.1768389 -4.2303405 -4.2697825][-4.3022275 -4.2802019 -4.2776642 -4.2678041 -4.2449241 -4.2177777 -4.1866951 -4.1536489 -4.1295638 -4.1197639 -4.1302733 -4.1627226 -4.2069445 -4.2511067 -4.2837906][-4.3248305 -4.3065305 -4.3039331 -4.2984 -4.28502 -4.2747321 -4.2614627 -4.23635 -4.2095594 -4.1927428 -4.1919551 -4.2093024 -4.2393837 -4.2716651 -4.2954836][-4.33586 -4.3231282 -4.3224053 -4.3215046 -4.31494 -4.3116241 -4.3064651 -4.288589 -4.2671528 -4.2506976 -4.243762 -4.2518692 -4.2713237 -4.2922111 -4.3071895][-4.3313556 -4.3256178 -4.3269358 -4.3294015 -4.327147 -4.3265009 -4.324379 -4.3140006 -4.3008289 -4.2885752 -4.2800956 -4.2835231 -4.2953439 -4.3071494 -4.3167787][-4.3297563 -4.3272734 -4.3276029 -4.3276286 -4.3262687 -4.3271904 -4.3276582 -4.3232665 -4.3162036 -4.3091187 -4.3030992 -4.3039393 -4.3098412 -4.3158593 -4.3225574]]...]
INFO - root - 2017-12-07 14:58:07.432760: step 21910, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 58h:37m:15s remains)
INFO - root - 2017-12-07 14:58:14.047521: step 21920, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 60h:02m:18s remains)
INFO - root - 2017-12-07 14:58:20.797052: step 21930, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 60h:03m:35s remains)
INFO - root - 2017-12-07 14:58:27.490190: step 21940, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 61h:12m:35s remains)
INFO - root - 2017-12-07 14:58:34.335329: step 21950, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 60h:56m:58s remains)
INFO - root - 2017-12-07 14:58:41.096218: step 21960, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 58h:23m:38s remains)
INFO - root - 2017-12-07 14:58:47.820533: step 21970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 58h:05m:07s remains)
INFO - root - 2017-12-07 14:58:54.693011: step 21980, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 55h:40m:25s remains)
INFO - root - 2017-12-07 14:59:01.461652: step 21990, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 60h:47m:30s remains)
INFO - root - 2017-12-07 14:59:08.348533: step 22000, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 61h:09m:29s remains)
2017-12-07 14:59:09.092432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3468952 -4.3439465 -4.3282857 -4.3018012 -4.2768693 -4.2570295 -4.2454419 -4.2432423 -4.2483363 -4.2640543 -4.2779317 -4.2885413 -4.2978473 -4.2984409 -4.2910752][-4.3469238 -4.3382859 -4.3152552 -4.2809982 -4.25345 -4.2331991 -4.2190332 -4.2171521 -4.2274823 -4.2516747 -4.2696323 -4.2828593 -4.2956791 -4.2969675 -4.2891855][-4.3452568 -4.33296 -4.3047142 -4.2663569 -4.2389307 -4.2194443 -4.197598 -4.1861019 -4.1970978 -4.2312574 -4.2556171 -4.2711153 -4.2876821 -4.2906103 -4.28048][-4.344851 -4.3315887 -4.3001266 -4.2607703 -4.234086 -4.2137494 -4.178288 -4.1486425 -4.1533389 -4.1914973 -4.2217736 -4.2449155 -4.2688923 -4.2726064 -4.2587624][-4.3456864 -4.3342237 -4.3020325 -4.2594352 -4.2300444 -4.2030997 -4.1489725 -4.0955377 -4.093914 -4.1426048 -4.1847892 -4.2168331 -4.2468162 -4.2488108 -4.2276292][-4.3435578 -4.332108 -4.2967706 -4.2464004 -4.2041268 -4.161077 -4.0822506 -4.0066533 -4.0144482 -4.0882044 -4.1483054 -4.1893816 -4.2224307 -4.2232389 -4.1972809][-4.3382139 -4.3249493 -4.2888646 -4.231389 -4.1732335 -4.104301 -3.9927738 -3.8960328 -3.914465 -4.0186324 -4.1041512 -4.1597023 -4.1927752 -4.1943917 -4.1760077][-4.336143 -4.3241858 -4.2905407 -4.23346 -4.1643453 -4.0781846 -3.9479821 -3.8359296 -3.8447497 -3.9582403 -4.062685 -4.1310534 -4.1647863 -4.170372 -4.1662788][-4.3345823 -4.3239393 -4.2954264 -4.2465682 -4.181848 -4.105855 -3.9974487 -3.9010735 -3.8948116 -3.9890614 -4.0862451 -4.1459846 -4.1736979 -4.1782632 -4.1849694][-4.330164 -4.3187208 -4.2951126 -4.255455 -4.2073131 -4.153863 -4.0819983 -4.0182676 -4.0106521 -4.0754037 -4.144587 -4.1857233 -4.2039881 -4.2083898 -4.2156634][-4.326086 -4.3142486 -4.2955856 -4.2655849 -4.234683 -4.2023606 -4.1634259 -4.1251168 -4.1158681 -4.1508884 -4.1931992 -4.2194166 -4.2307167 -4.2357836 -4.241291][-4.3260436 -4.3141437 -4.3001242 -4.2855763 -4.2730703 -4.2583947 -4.2450314 -4.2264562 -4.2179422 -4.2350984 -4.2584872 -4.2735953 -4.2798743 -4.2800469 -4.2836003][-4.3279715 -4.3185544 -4.3100452 -4.3098154 -4.3129773 -4.3133631 -4.3150058 -4.3104053 -4.3054442 -4.3136363 -4.3253489 -4.3342423 -4.3372664 -4.3335614 -4.3322577][-4.33022 -4.3233237 -4.3190627 -4.325418 -4.3360515 -4.3459997 -4.3531294 -4.355238 -4.35345 -4.35803 -4.3628821 -4.3678889 -4.3701029 -4.3675141 -4.3623495][-4.3317528 -4.3263035 -4.3231893 -4.3292136 -4.3397322 -4.3501687 -4.3576927 -4.3605251 -4.359551 -4.3597136 -4.3601084 -4.3608871 -4.3620157 -4.3609633 -4.356317]]...]
INFO - root - 2017-12-07 14:59:15.899748: step 22010, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 59h:10m:45s remains)
INFO - root - 2017-12-07 14:59:22.624909: step 22020, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 62h:05m:48s remains)
INFO - root - 2017-12-07 14:59:29.403734: step 22030, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 59h:37m:52s remains)
INFO - root - 2017-12-07 14:59:36.270556: step 22040, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 56h:50m:28s remains)
INFO - root - 2017-12-07 14:59:43.150303: step 22050, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 54h:31m:48s remains)
INFO - root - 2017-12-07 14:59:49.979089: step 22060, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 60h:28m:16s remains)
INFO - root - 2017-12-07 14:59:56.854234: step 22070, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 63h:10m:23s remains)
INFO - root - 2017-12-07 15:00:03.621926: step 22080, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 56h:15m:45s remains)
INFO - root - 2017-12-07 15:00:10.363085: step 22090, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 57h:07m:40s remains)
INFO - root - 2017-12-07 15:00:17.197128: step 22100, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.663 sec/batch; 57h:11m:46s remains)
2017-12-07 15:00:17.993530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2542534 -4.2335672 -4.2325063 -4.2480626 -4.2641568 -4.27063 -4.2501078 -4.2093139 -4.1727486 -4.1637473 -4.1781487 -4.2025971 -4.2390504 -4.2731295 -4.2991376][-4.2034049 -4.1907153 -4.1996703 -4.2246413 -4.2458892 -4.25275 -4.2292633 -4.1842022 -4.1521263 -4.1544538 -4.1803584 -4.2117772 -4.2475376 -4.2778292 -4.3016038][-4.1498361 -4.1450214 -4.1593261 -4.1858039 -4.2056441 -4.209651 -4.1877761 -4.1470532 -4.1330943 -4.1531019 -4.1916065 -4.2276535 -4.2605743 -4.2853127 -4.3051596][-4.1178288 -4.1209421 -4.1351552 -4.1542807 -4.160934 -4.156302 -4.1331253 -4.1054077 -4.1112909 -4.1474504 -4.1974535 -4.2399559 -4.2711062 -4.2932549 -4.3087945][-4.1050887 -4.1251168 -4.143456 -4.1528497 -4.1412492 -4.11392 -4.0757995 -4.0482635 -4.0688396 -4.1260867 -4.1910882 -4.23953 -4.2746019 -4.2984338 -4.3118677][-4.1001854 -4.1404686 -4.1661792 -4.1698694 -4.1390538 -4.0819969 -4.0229378 -3.9936461 -4.0264821 -4.1047153 -4.1829653 -4.2372479 -4.2764854 -4.3025346 -4.3158565][-4.1122303 -4.16744 -4.20063 -4.1945062 -4.1444364 -4.066977 -3.993937 -3.9678619 -4.0078034 -4.0925045 -4.1744361 -4.2326417 -4.2748203 -4.3021173 -4.3164811][-4.1687713 -4.216958 -4.2445707 -4.2286148 -4.1731467 -4.096796 -4.0266261 -3.9999471 -4.0358686 -4.1081996 -4.1775188 -4.2312088 -4.2723336 -4.2999148 -4.3147731][-4.2253108 -4.2592893 -4.2815752 -4.266531 -4.22424 -4.1686087 -4.115407 -4.0918961 -4.1152458 -4.1611166 -4.2075224 -4.2491746 -4.2837272 -4.3060331 -4.3177223][-4.2608585 -4.2821321 -4.299788 -4.2931137 -4.2712317 -4.2365336 -4.1951122 -4.1749611 -4.1894732 -4.216392 -4.2491913 -4.2815046 -4.3096623 -4.3233104 -4.3273606][-4.2867079 -4.2993627 -4.3072109 -4.3003 -4.2911215 -4.2701573 -4.2384782 -4.223105 -4.2374878 -4.259685 -4.2884865 -4.3152676 -4.3378286 -4.3431282 -4.3384957][-4.3082352 -4.3156781 -4.314641 -4.3011718 -4.2926011 -4.2804022 -4.2591095 -4.2515011 -4.2681379 -4.2882876 -4.3151426 -4.3386259 -4.3555222 -4.3539047 -4.3432627][-4.3189945 -4.3257909 -4.3209691 -4.3051243 -4.2923465 -4.28209 -4.26924 -4.2668848 -4.280982 -4.2975459 -4.3208957 -4.3402681 -4.3512855 -4.3478813 -4.3387465][-4.3220396 -4.3264875 -4.3206749 -4.3104763 -4.3004413 -4.2925129 -4.286181 -4.2854195 -4.2918725 -4.3010845 -4.316206 -4.3300481 -4.3371596 -4.3360276 -4.33226][-4.3160791 -4.3205223 -4.3193345 -4.3163824 -4.3122721 -4.3068991 -4.3010817 -4.298636 -4.2995071 -4.302938 -4.3118043 -4.3211656 -4.3275046 -4.3298297 -4.3299241]]...]
INFO - root - 2017-12-07 15:00:24.741322: step 22110, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 57h:37m:04s remains)
INFO - root - 2017-12-07 15:00:31.381802: step 22120, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 54h:27m:40s remains)
INFO - root - 2017-12-07 15:00:38.139265: step 22130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 55h:29m:14s remains)
INFO - root - 2017-12-07 15:00:44.887828: step 22140, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 59h:01m:16s remains)
INFO - root - 2017-12-07 15:00:51.714525: step 22150, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 62h:06m:50s remains)
INFO - root - 2017-12-07 15:00:58.495504: step 22160, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.675 sec/batch; 58h:08m:48s remains)
INFO - root - 2017-12-07 15:01:05.261184: step 22170, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 53h:47m:34s remains)
INFO - root - 2017-12-07 15:01:12.024829: step 22180, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 57h:36m:49s remains)
INFO - root - 2017-12-07 15:01:18.804050: step 22190, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 60h:31m:00s remains)
INFO - root - 2017-12-07 15:01:25.650231: step 22200, loss = 2.02, batch loss = 1.96 (11.5 examples/sec; 0.696 sec/batch; 59h:57m:23s remains)
2017-12-07 15:01:26.355945: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.179018 -4.1593194 -4.1375127 -4.1313415 -4.1520529 -4.1837306 -4.2158604 -4.2465067 -4.2715387 -4.2880912 -4.2917767 -4.2833748 -4.2745681 -4.273675 -4.2790966][-4.1620808 -4.149713 -4.1272988 -4.1212626 -4.1490331 -4.1877742 -4.2208362 -4.2493482 -4.2776475 -4.3006792 -4.3073096 -4.2993889 -4.2917428 -4.2901168 -4.2924247][-4.1464987 -4.14274 -4.1224337 -4.118979 -4.1485677 -4.1876473 -4.2184391 -4.2434797 -4.2725134 -4.2995625 -4.3086805 -4.3028955 -4.294991 -4.2911015 -4.2901344][-4.14576 -4.1426578 -4.1240644 -4.124167 -4.150835 -4.1827407 -4.205555 -4.223525 -4.251502 -4.27931 -4.2907515 -4.287643 -4.2782569 -4.2718534 -4.2701335][-4.164649 -4.1536574 -4.1352892 -4.1347208 -4.1535072 -4.173636 -4.1834655 -4.1906719 -4.2166667 -4.24837 -4.2641764 -4.2654972 -4.2543921 -4.2438812 -4.2399197][-4.1924276 -4.1744537 -4.1543822 -4.1472831 -4.1522775 -4.1547728 -4.1457906 -4.1392198 -4.1652575 -4.2061934 -4.2320566 -4.2425776 -4.2333126 -4.2187357 -4.2107673][-4.2170725 -4.1939826 -4.17004 -4.1535563 -4.1425533 -4.1250744 -4.09334 -4.0656271 -4.0905423 -4.145802 -4.18692 -4.2100282 -4.2080474 -4.1947222 -4.1851234][-4.2302656 -4.206955 -4.1854477 -4.1671495 -4.1442862 -4.1062117 -4.0494461 -3.9963968 -4.0159879 -4.0837979 -4.1386552 -4.1742244 -4.1814551 -4.1731982 -4.1664114][-4.2316275 -4.2133079 -4.2016387 -4.1918783 -4.1694393 -4.1195507 -4.0459905 -3.974612 -3.979686 -4.0462413 -4.1053867 -4.1480665 -4.1650181 -4.1653013 -4.1636][-4.2191782 -4.2126331 -4.2130775 -4.2161493 -4.2047706 -4.1611595 -4.0908041 -4.0201378 -4.010179 -4.0581632 -4.106348 -4.1461244 -4.1692019 -4.1779733 -4.1788273][-4.2138753 -4.2206421 -4.2308216 -4.24417 -4.2451787 -4.2187657 -4.1642656 -4.1062627 -4.089375 -4.1143961 -4.1435862 -4.1730661 -4.1955786 -4.204772 -4.2013097][-4.2349987 -4.2516861 -4.2664604 -4.28101 -4.2852707 -4.2708459 -4.2333145 -4.1907039 -4.1750464 -4.1837721 -4.1948485 -4.2109666 -4.2272363 -4.2298293 -4.217371][-4.2705517 -4.2886648 -4.3025026 -4.3131118 -4.3181982 -4.3125587 -4.2877316 -4.2563314 -4.2432466 -4.2428432 -4.2438035 -4.2506433 -4.260726 -4.2573915 -4.2381778][-4.3030982 -4.3142228 -4.3208337 -4.3264737 -4.3346424 -4.338088 -4.3258419 -4.3034339 -4.29149 -4.2868137 -4.2834191 -4.2850718 -4.2903905 -4.2840924 -4.2627277][-4.3255558 -4.3264642 -4.3247023 -4.3263493 -4.3369884 -4.3472743 -4.3459682 -4.3334694 -4.3235908 -4.3170433 -4.3126254 -4.3123074 -4.3135514 -4.3055868 -4.2847934]]...]
INFO - root - 2017-12-07 15:01:33.110405: step 22210, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 58h:06m:55s remains)
INFO - root - 2017-12-07 15:01:39.868518: step 22220, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 62h:11m:10s remains)
INFO - root - 2017-12-07 15:01:46.657696: step 22230, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 57h:05m:20s remains)
INFO - root - 2017-12-07 15:01:53.432493: step 22240, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 56h:27m:09s remains)
INFO - root - 2017-12-07 15:02:00.039337: step 22250, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 60h:30m:06s remains)
INFO - root - 2017-12-07 15:02:06.873040: step 22260, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 60h:20m:36s remains)
INFO - root - 2017-12-07 15:02:13.685308: step 22270, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 58h:45m:47s remains)
INFO - root - 2017-12-07 15:02:20.402096: step 22280, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 57h:19m:04s remains)
INFO - root - 2017-12-07 15:02:27.164345: step 22290, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 55h:03m:47s remains)
INFO - root - 2017-12-07 15:02:34.016619: step 22300, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 61h:57m:06s remains)
2017-12-07 15:02:34.735817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2009649 -4.1888852 -4.2002192 -4.2287145 -4.247437 -4.2459636 -4.2257214 -4.2001424 -4.1786418 -4.1674638 -4.1659904 -4.1733522 -4.19972 -4.238327 -4.2664948][-4.2121329 -4.199295 -4.2138519 -4.2501268 -4.2758822 -4.2736282 -4.2442708 -4.205297 -4.1743717 -4.1652689 -4.177619 -4.2037354 -4.2434554 -4.2865124 -4.3105617][-4.22911 -4.2144737 -4.2267303 -4.2645111 -4.2915773 -4.2837257 -4.241468 -4.1852551 -4.1427097 -4.1368966 -4.16477 -4.2118139 -4.2650928 -4.3107104 -4.3321819][-4.2384715 -4.2236576 -4.2339878 -4.269279 -4.2928157 -4.2757359 -4.2178578 -4.1452522 -4.095717 -4.0992737 -4.1448274 -4.2100725 -4.2756457 -4.3245349 -4.3445191][-4.2525921 -4.241384 -4.2507863 -4.2783675 -4.2909961 -4.2593064 -4.1820488 -4.09021 -4.0355172 -4.0546055 -4.1214509 -4.2036443 -4.27945 -4.3327022 -4.354228][-4.2706833 -4.2688208 -4.2787023 -4.2948208 -4.29058 -4.237185 -4.1328621 -4.0151806 -3.955498 -3.9966044 -4.0894375 -4.1883106 -4.2732081 -4.3308406 -4.3559418][-4.2918077 -4.2991624 -4.3079877 -4.3133545 -4.2937164 -4.2179227 -4.0856915 -3.9456234 -3.888978 -3.9554045 -4.0706515 -4.1803451 -4.2669969 -4.322341 -4.3488441][-4.3100872 -4.3204355 -4.3254647 -4.3240209 -4.2955565 -4.2090359 -4.0673084 -3.9296193 -3.8916526 -3.9746771 -4.0931015 -4.1934738 -4.2664413 -4.3118339 -4.3351827][-4.3085966 -4.3148265 -4.3154755 -4.31335 -4.2865105 -4.2065892 -4.0795512 -3.970046 -3.9558458 -4.0346837 -4.135498 -4.2134008 -4.2658486 -4.2991767 -4.3184729][-4.2933025 -4.291595 -4.2895889 -4.292675 -4.276649 -4.2173648 -4.1198616 -4.04233 -4.0425019 -4.1073093 -4.1824026 -4.233592 -4.2627358 -4.2807422 -4.2945142][-4.2697248 -4.2605438 -4.2582178 -4.2677855 -4.2641368 -4.2293825 -4.1632142 -4.1088929 -4.1138077 -4.1624846 -4.2137065 -4.2420769 -4.2498493 -4.251533 -4.2594085][-4.246367 -4.2302055 -4.22866 -4.2434168 -4.2511163 -4.2370973 -4.1953645 -4.155643 -4.1605935 -4.1959324 -4.2271376 -4.2344913 -4.2222562 -4.209691 -4.2154937][-4.2296634 -4.2121158 -4.2131333 -4.2310958 -4.2447753 -4.2393522 -4.2093682 -4.177804 -4.1811876 -4.2056475 -4.2229619 -4.2148237 -4.1880722 -4.1704774 -4.1807985][-4.2226515 -4.2069216 -4.2131038 -4.2341986 -4.2502432 -4.2447815 -4.2165084 -4.1852303 -4.1822481 -4.1959143 -4.20243 -4.1853895 -4.1541371 -4.1411009 -4.1601844][-4.2267146 -4.2159643 -4.2293577 -4.2537575 -4.2686191 -4.2550049 -4.2177496 -4.1800795 -4.1672091 -4.1695104 -4.1687574 -4.1512747 -4.1253448 -4.122508 -4.147881]]...]
INFO - root - 2017-12-07 15:02:41.455892: step 22310, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 56h:19m:01s remains)
INFO - root - 2017-12-07 15:02:48.135391: step 22320, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 55h:57m:47s remains)
INFO - root - 2017-12-07 15:02:54.931644: step 22330, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 61h:28m:39s remains)
INFO - root - 2017-12-07 15:03:01.695760: step 22340, loss = 2.09, batch loss = 2.03 (10.4 examples/sec; 0.770 sec/batch; 66h:20m:30s remains)
INFO - root - 2017-12-07 15:03:08.437036: step 22350, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 57h:16m:33s remains)
INFO - root - 2017-12-07 15:03:15.237457: step 22360, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 56h:43m:59s remains)
INFO - root - 2017-12-07 15:03:21.986918: step 22370, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 54h:23m:31s remains)
INFO - root - 2017-12-07 15:03:28.777524: step 22380, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.705 sec/batch; 60h:46m:03s remains)
INFO - root - 2017-12-07 15:03:35.530422: step 22390, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 56h:07m:03s remains)
INFO - root - 2017-12-07 15:03:42.253035: step 22400, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 58h:13m:33s remains)
2017-12-07 15:03:43.005117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2984419 -4.3126411 -4.3163838 -4.3116088 -4.30278 -4.2909732 -4.2806721 -4.2811022 -4.2897878 -4.2939534 -4.2936707 -4.2909694 -4.2852025 -4.2777591 -4.2738533][-4.2935238 -4.3088222 -4.3106613 -4.3022461 -4.2902255 -4.2768116 -4.268137 -4.2736659 -4.290555 -4.2960372 -4.2883515 -4.2758889 -4.2646961 -4.2543774 -4.247189][-4.2849312 -4.2949228 -4.2903414 -4.2757516 -4.2603126 -4.2443452 -4.2365379 -4.2491751 -4.2732248 -4.278059 -4.2599635 -4.2357841 -4.2184091 -4.2031531 -4.1939421][-4.2761259 -4.2762403 -4.2634263 -4.2440114 -4.22779 -4.2096682 -4.1994376 -4.213181 -4.2387166 -4.2379084 -4.2053981 -4.1697593 -4.148633 -4.1329746 -4.1269622][-4.2742333 -4.2647309 -4.2426276 -4.2188373 -4.1986942 -4.1728568 -4.1563783 -4.1667466 -4.1928539 -4.1892686 -4.144886 -4.1001143 -4.0789256 -4.0674205 -4.0674839][-4.2742548 -4.2558303 -4.2262135 -4.1941366 -4.1599846 -4.1185374 -4.0908127 -4.0961795 -4.1285787 -4.1331739 -4.0905581 -4.0453768 -4.0289221 -4.0241904 -4.0315132][-4.2693987 -4.2423129 -4.2049818 -4.1622591 -4.1123548 -4.0542221 -4.0179806 -4.0245733 -4.0673833 -4.0887284 -4.062047 -4.0209813 -4.0047817 -4.0028253 -4.0169711][-4.2590122 -4.2285047 -4.1879125 -4.1433992 -4.0885415 -4.0197668 -3.9745846 -3.9790077 -4.0281796 -4.0640926 -4.0563645 -4.0258465 -4.0094066 -4.0057464 -4.0207348][-4.2431946 -4.217412 -4.1858997 -4.1519489 -4.1067238 -4.0408354 -3.9895666 -3.9829059 -4.0246835 -4.0559449 -4.0556116 -4.0348496 -4.0254149 -4.0246215 -4.0386143][-4.2236476 -4.2064214 -4.18968 -4.1723657 -4.1442556 -4.0899887 -4.0358982 -4.01454 -4.04135 -4.0625668 -4.0603504 -4.046834 -4.046289 -4.0511842 -4.0655589][-4.2074819 -4.1968651 -4.1931024 -4.191535 -4.1795297 -4.1380205 -4.0879569 -4.058816 -4.0720606 -4.0844483 -4.0785809 -4.0679779 -4.0683603 -4.0748529 -4.085732][-4.2011967 -4.1910195 -4.1924758 -4.1992793 -4.1968665 -4.1679068 -4.1274467 -4.0999312 -4.1061878 -4.1120028 -4.1023192 -4.0920959 -4.0896544 -4.0929708 -4.0994611][-4.2092247 -4.1961913 -4.1972952 -4.2047663 -4.2072845 -4.1898131 -4.162972 -4.1456227 -4.1526537 -4.1578083 -4.1464047 -4.1318889 -4.1217809 -4.1201358 -4.1239619][-4.2245259 -4.2098432 -4.21102 -4.217607 -4.2224989 -4.2155728 -4.2012992 -4.1912723 -4.1968102 -4.2020864 -4.1923704 -4.1757841 -4.1628561 -4.1618876 -4.1705637][-4.2424245 -4.2280674 -4.2303963 -4.2387819 -4.2462621 -4.2454381 -4.2379227 -4.2313075 -4.2329926 -4.2343626 -4.2253146 -4.2103095 -4.1992512 -4.2041764 -4.2203455]]...]
INFO - root - 2017-12-07 15:03:49.701446: step 22410, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 56h:30m:17s remains)
INFO - root - 2017-12-07 15:03:56.493192: step 22420, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 62h:08m:07s remains)
INFO - root - 2017-12-07 15:04:03.276860: step 22430, loss = 2.02, batch loss = 1.96 (11.3 examples/sec; 0.711 sec/batch; 61h:13m:24s remains)
INFO - root - 2017-12-07 15:04:10.189138: step 22440, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 58h:21m:48s remains)
INFO - root - 2017-12-07 15:04:17.000154: step 22450, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 55h:09m:06s remains)
INFO - root - 2017-12-07 15:04:23.876154: step 22460, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 62h:29m:52s remains)
INFO - root - 2017-12-07 15:04:30.750099: step 22470, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 62h:39m:37s remains)
INFO - root - 2017-12-07 15:04:37.501349: step 22480, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.668 sec/batch; 57h:32m:14s remains)
INFO - root - 2017-12-07 15:04:44.308596: step 22490, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 53h:51m:23s remains)
INFO - root - 2017-12-07 15:04:51.157320: step 22500, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 58h:45m:41s remains)
2017-12-07 15:04:51.978489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1674175 -4.1494842 -4.1394696 -4.1415033 -4.1587391 -4.185822 -4.2043881 -4.2043242 -4.1920962 -4.1803041 -4.1713529 -4.1665821 -4.1655092 -4.1701789 -4.1811695][-4.144187 -4.1254392 -4.1152115 -4.1183977 -4.1397824 -4.1707506 -4.192008 -4.1922574 -4.1833773 -4.178803 -4.1747785 -4.16744 -4.1573772 -4.1506696 -4.1556354][-4.1341476 -4.1189213 -4.1115837 -4.1160836 -4.1349125 -4.1627192 -4.1839743 -4.1882296 -4.185503 -4.1911016 -4.1942043 -4.1834164 -4.163013 -4.1440234 -4.143085][-4.142766 -4.13124 -4.125701 -4.130795 -4.1430116 -4.1617036 -4.1813288 -4.1869769 -4.1874332 -4.2003231 -4.2110715 -4.197876 -4.168714 -4.1408277 -4.1398025][-4.1515841 -4.1406679 -4.1340122 -4.1341472 -4.1351767 -4.1444035 -4.1629987 -4.1695275 -4.1755934 -4.196188 -4.2150731 -4.2006941 -4.1647873 -4.1324949 -4.130981][-4.1493115 -4.1386113 -4.1324863 -4.1228042 -4.1117287 -4.1114516 -4.124155 -4.1353931 -4.1575012 -4.192718 -4.2207727 -4.2067966 -4.1636086 -4.1277633 -4.1241064][-4.1499052 -4.1398144 -4.1335626 -4.1187181 -4.1019225 -4.0906858 -4.0856152 -4.0955477 -4.1339607 -4.188086 -4.2223864 -4.2106633 -4.1652775 -4.1301355 -4.1259055][-4.1519337 -4.1420789 -4.1350603 -4.122601 -4.1062212 -4.0784159 -4.0436878 -4.0366154 -4.0831985 -4.1585851 -4.2034564 -4.2012687 -4.163435 -4.1325159 -4.128973][-4.1497593 -4.1418042 -4.1363182 -4.1296086 -4.1125937 -4.069139 -3.9996762 -3.9614651 -4.01293 -4.1117325 -4.1713595 -4.1811409 -4.1572447 -4.1303997 -4.1281562][-4.1562343 -4.1534553 -4.1431451 -4.13581 -4.1202674 -4.0713086 -3.9816976 -3.9199827 -3.9716866 -4.0852628 -4.1581583 -4.1720681 -4.1536279 -4.1294742 -4.12636][-4.1771708 -4.1789465 -4.163156 -4.1501446 -4.13487 -4.0939474 -4.0143342 -3.9592025 -4.007719 -4.1138887 -4.1855812 -4.193615 -4.1676078 -4.137404 -4.1251607][-4.2002535 -4.2087145 -4.1935415 -4.176156 -4.1553845 -4.1204238 -4.0631285 -4.0319443 -4.0761938 -4.1640816 -4.2218761 -4.2206192 -4.1845636 -4.1465993 -4.1269431][-4.2212167 -4.2358108 -4.2235932 -4.2046218 -4.1787229 -4.1462688 -4.1060033 -4.0936618 -4.133801 -4.2023039 -4.2440467 -4.2327456 -4.1922746 -4.154634 -4.13593][-4.2404857 -4.2512994 -4.2416892 -4.2288327 -4.2059913 -4.1744313 -4.1457276 -4.1472387 -4.1860204 -4.2360859 -4.2629404 -4.2435622 -4.2013659 -4.1690764 -4.1539946][-4.2602682 -4.2618423 -4.2568874 -4.2531624 -4.2352443 -4.2049217 -4.1848731 -4.1954226 -4.2334762 -4.2716475 -4.2910466 -4.2679839 -4.2248139 -4.1941228 -4.1785145]]...]
INFO - root - 2017-12-07 15:04:58.763231: step 22510, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 56h:57m:40s remains)
INFO - root - 2017-12-07 15:05:05.280430: step 22520, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 55h:51m:08s remains)
INFO - root - 2017-12-07 15:05:12.004634: step 22530, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 57h:19m:37s remains)
INFO - root - 2017-12-07 15:05:18.840425: step 22540, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.699 sec/batch; 60h:09m:01s remains)
INFO - root - 2017-12-07 15:05:25.628624: step 22550, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 57h:48m:32s remains)
INFO - root - 2017-12-07 15:05:32.209746: step 22560, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 59h:16m:54s remains)
INFO - root - 2017-12-07 15:05:39.045044: step 22570, loss = 2.03, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 59h:42m:41s remains)
INFO - root - 2017-12-07 15:05:45.918476: step 22580, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 60h:05m:06s remains)
INFO - root - 2017-12-07 15:05:52.600553: step 22590, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 57h:34m:45s remains)
INFO - root - 2017-12-07 15:05:59.360842: step 22600, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 56h:27m:21s remains)
2017-12-07 15:06:00.067060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2969446 -4.286377 -4.2750363 -4.2652617 -4.2630043 -4.2644577 -4.26917 -4.2762 -4.2755718 -4.276022 -4.2808027 -4.2877464 -4.2924929 -4.2953572 -4.2972441][-4.2804027 -4.2622118 -4.245 -4.2290626 -4.2261438 -4.2254853 -4.2299027 -4.2394032 -4.2451925 -4.2522674 -4.2588549 -4.2693663 -4.2777581 -4.2784719 -4.2781196][-4.2626023 -4.2342134 -4.2056484 -4.1748276 -4.1661415 -4.160861 -4.1585855 -4.1709023 -4.1924939 -4.2070742 -4.2169342 -4.2312078 -4.2482462 -4.2511911 -4.2515979][-4.2474694 -4.2074919 -4.1663742 -4.1234708 -4.0964184 -4.0760202 -4.0594845 -4.0828066 -4.1338668 -4.1551604 -4.1659851 -4.1840463 -4.2070732 -4.216332 -4.2175145][-4.23458 -4.188941 -4.14339 -4.0876317 -4.0317659 -3.9810584 -3.9390783 -3.9750445 -4.0617909 -4.09391 -4.1074896 -4.123723 -4.15251 -4.1736593 -4.1816115][-4.2080412 -4.151402 -4.0986671 -4.03086 -3.9464688 -3.8493862 -3.7598357 -3.8209331 -3.9602504 -4.0096893 -4.0264091 -4.0443687 -4.08355 -4.1221952 -4.1459422][-4.17803 -4.0989442 -4.0303073 -3.9538684 -3.851006 -3.6948113 -3.5234604 -3.6190169 -3.829932 -3.9078393 -3.9334214 -3.9646444 -4.0213761 -4.0723038 -4.1104345][-4.1631312 -4.0786724 -4.0058622 -3.9373465 -3.8423967 -3.6793978 -3.4958684 -3.5997074 -3.80772 -3.8934603 -3.9298446 -3.973326 -4.0220776 -4.0696096 -4.110487][-4.1790214 -4.1222944 -4.0721612 -4.0243139 -3.9626744 -3.8494241 -3.7466016 -3.8162837 -3.938365 -3.9924705 -4.0147767 -4.0413423 -4.0689464 -4.0996618 -4.1386309][-4.1933432 -4.158699 -4.126018 -4.0903397 -4.0468283 -3.9718184 -3.9196951 -3.9658318 -4.0322003 -4.0622153 -4.0716743 -4.0840468 -4.0976529 -4.1207633 -4.1589422][-4.2136722 -4.1911764 -4.16593 -4.1329355 -4.0925245 -4.0370607 -4.0113916 -4.0505962 -4.0888057 -4.1037884 -4.1129766 -4.1244273 -4.1348047 -4.1536736 -4.184094][-4.2444081 -4.2300925 -4.2113371 -4.1902595 -4.1608977 -4.1186886 -4.1121645 -4.1443944 -4.1663008 -4.16922 -4.1765366 -4.1872129 -4.1990948 -4.2118917 -4.2330341][-4.2831526 -4.27744 -4.2711463 -4.2632451 -4.2481461 -4.2260222 -4.2257886 -4.2445717 -4.2562184 -4.2544613 -4.2566934 -4.2634873 -4.2720962 -4.2798944 -4.2930179][-4.309598 -4.3103867 -4.3118567 -4.3097858 -4.3039265 -4.2955647 -4.2993226 -4.3102 -4.3168077 -4.3154993 -4.3140292 -4.3143182 -4.3174238 -4.3213577 -4.3254323][-4.3210883 -4.3207111 -4.3221936 -4.3207912 -4.3180313 -4.31599 -4.3190293 -4.3252997 -4.3289776 -4.3297386 -4.329525 -4.32815 -4.3284292 -4.3304977 -4.3316817]]...]
INFO - root - 2017-12-07 15:06:06.750642: step 22610, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 55h:07m:21s remains)
INFO - root - 2017-12-07 15:06:13.438247: step 22620, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 57h:28m:33s remains)
INFO - root - 2017-12-07 15:06:20.251670: step 22630, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 55h:12m:32s remains)
INFO - root - 2017-12-07 15:06:27.072341: step 22640, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 56h:45m:34s remains)
INFO - root - 2017-12-07 15:06:33.924025: step 22650, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 61h:05m:49s remains)
INFO - root - 2017-12-07 15:06:40.772168: step 22660, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 63h:06m:53s remains)
INFO - root - 2017-12-07 15:06:47.583150: step 22670, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 57h:31m:11s remains)
INFO - root - 2017-12-07 15:06:54.352110: step 22680, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 55h:09m:32s remains)
INFO - root - 2017-12-07 15:07:01.189838: step 22690, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 61h:20m:01s remains)
INFO - root - 2017-12-07 15:07:08.037956: step 22700, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 60h:15m:25s remains)
2017-12-07 15:07:08.778571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2901721 -4.2747483 -4.259438 -4.2420177 -4.22658 -4.2160535 -4.21209 -4.2120647 -4.2087269 -4.2144332 -4.2266073 -4.2394514 -4.2455797 -4.2458005 -4.243062][-4.2790227 -4.2611966 -4.2415032 -4.2196617 -4.2013521 -4.1910596 -4.1895542 -4.1874313 -4.1815557 -4.1828523 -4.1894035 -4.2024169 -4.2090149 -4.2073803 -4.2010775][-4.271452 -4.2502365 -4.2265186 -4.1998878 -4.1770182 -4.1635108 -4.1585083 -4.1518507 -4.1471629 -4.1523561 -4.1590457 -4.1766372 -4.1888247 -4.1832428 -4.1639614][-4.265192 -4.2386208 -4.206676 -4.1713691 -4.1418767 -4.1212821 -4.1068912 -4.101882 -4.1151938 -4.1364231 -4.152842 -4.1737809 -4.1895986 -4.1776114 -4.1432524][-4.2605076 -4.2269483 -4.1855245 -4.1394467 -4.0977068 -4.0611691 -4.0301223 -4.0266275 -4.0686941 -4.1185422 -4.1498976 -4.1753006 -4.1930943 -4.1832771 -4.1424437][-4.2549305 -4.2151556 -4.1634459 -4.103797 -4.04232 -3.9801898 -3.9204049 -3.9082198 -3.9772706 -4.0630732 -4.1176367 -4.155448 -4.1804767 -4.17892 -4.13867][-4.252882 -4.2083554 -4.1477036 -4.0733304 -3.9966104 -3.9119043 -3.8179018 -3.7671082 -3.8354282 -3.961122 -4.0470667 -4.0941324 -4.1261616 -4.1358781 -4.1076546][-4.2524657 -4.2047162 -4.1370039 -4.0527477 -3.9686887 -3.8769341 -3.7592962 -3.653482 -3.6889224 -3.8418097 -3.9557624 -4.012742 -4.0471907 -4.0681324 -4.055151][-4.2501268 -4.2049222 -4.1413932 -4.0630713 -3.9878469 -3.9141703 -3.8148184 -3.7118874 -3.7109568 -3.8250837 -3.9180212 -3.9664814 -3.9990366 -4.0205946 -4.0205393][-4.2541604 -4.2192788 -4.1716032 -4.110642 -4.0571055 -4.0145445 -3.9587963 -3.9022133 -3.884692 -3.9220278 -3.9672382 -4.003243 -4.027082 -4.036902 -4.0367293][-4.2665648 -4.241818 -4.20807 -4.1658125 -4.1333795 -4.1146092 -4.0920544 -4.0630531 -4.0372696 -4.0298343 -4.0472145 -4.08015 -4.0985537 -4.0964084 -4.0927029][-4.2862239 -4.269516 -4.2454233 -4.2175841 -4.1991925 -4.1915693 -4.1846247 -4.1683049 -4.1469951 -4.1298127 -4.1342978 -4.1582451 -4.172194 -4.1667833 -4.1579165][-4.3034697 -4.29213 -4.2743011 -4.2556696 -4.2451015 -4.2433267 -4.2449903 -4.2412419 -4.2317853 -4.2181139 -4.2157016 -4.2249203 -4.2346072 -4.230967 -4.2221608][-4.3131132 -4.306365 -4.2945867 -4.2815933 -4.2719231 -4.2709017 -4.2749133 -4.2787905 -4.2778554 -4.2711678 -4.265358 -4.2656708 -4.2723751 -4.2743745 -4.2712712][-4.3202324 -4.3164206 -4.3083282 -4.2997746 -4.2927995 -4.2921128 -4.2948484 -4.2990274 -4.3004293 -4.2977509 -4.293673 -4.2932005 -4.2973747 -4.3018541 -4.3036585]]...]
INFO - root - 2017-12-07 15:07:15.529634: step 22710, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 55h:57m:53s remains)
INFO - root - 2017-12-07 15:07:22.159899: step 22720, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:34m:19s remains)
INFO - root - 2017-12-07 15:07:29.053254: step 22730, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 60h:22m:07s remains)
INFO - root - 2017-12-07 15:07:35.956114: step 22740, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.668 sec/batch; 57h:28m:48s remains)
INFO - root - 2017-12-07 15:07:42.720812: step 22750, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 55h:57m:05s remains)
INFO - root - 2017-12-07 15:07:49.538334: step 22760, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.654 sec/batch; 56h:14m:23s remains)
INFO - root - 2017-12-07 15:07:56.350125: step 22770, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 61h:06m:59s remains)
INFO - root - 2017-12-07 15:08:03.191992: step 22780, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 56h:29m:16s remains)
INFO - root - 2017-12-07 15:08:10.001632: step 22790, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 58h:42m:43s remains)
INFO - root - 2017-12-07 15:08:16.717396: step 22800, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 55h:11m:41s remains)
2017-12-07 15:08:17.405739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2858849 -4.266099 -4.2502065 -4.2451377 -4.25234 -4.2724032 -4.2864861 -4.2852983 -4.2839513 -4.27731 -4.2696719 -4.2673364 -4.2705603 -4.2693143 -4.26317][-4.2802014 -4.259831 -4.249115 -4.2517786 -4.263967 -4.2853026 -4.2973247 -4.2952657 -4.2911582 -4.2838717 -4.2787409 -4.2787142 -4.2821026 -4.2819505 -4.28019][-4.2800603 -4.2609878 -4.2546463 -4.2602329 -4.2737627 -4.294096 -4.3060412 -4.3064847 -4.3014221 -4.2958684 -4.2950854 -4.29705 -4.29964 -4.2960792 -4.2933431][-4.2683244 -4.2573218 -4.2578373 -4.2634907 -4.27332 -4.286171 -4.29305 -4.2940254 -4.2924237 -4.2935634 -4.3003168 -4.3066282 -4.3097763 -4.3050666 -4.3021154][-4.2520895 -4.25247 -4.2594914 -4.26381 -4.2646794 -4.2646284 -4.2602291 -4.258626 -4.2625723 -4.272994 -4.2897177 -4.3025312 -4.307714 -4.3042431 -4.3019195][-4.2188048 -4.2339988 -4.2524867 -4.2602978 -4.2544951 -4.2415814 -4.2216339 -4.2093954 -4.2144136 -4.2332897 -4.2608237 -4.2807722 -4.289547 -4.2912493 -4.2917042][-4.1719055 -4.2002773 -4.2319288 -4.2466807 -4.2363086 -4.2120671 -4.1815338 -4.16161 -4.1678271 -4.1902361 -4.2225518 -4.2488985 -4.2622771 -4.2670512 -4.2692094][-4.1195755 -4.1535006 -4.1917691 -4.2090325 -4.1972017 -4.1677189 -4.1341996 -4.1133761 -4.12431 -4.14873 -4.179821 -4.2090483 -4.2266431 -4.2341032 -4.2346988][-4.1163812 -4.1410627 -4.1685996 -4.1784325 -4.1658792 -4.1385069 -4.1070213 -4.084363 -4.0926361 -4.1133289 -4.1394382 -4.167438 -4.1880703 -4.1986737 -4.2002339][-4.1666441 -4.1824536 -4.1968794 -4.1972122 -4.1848917 -4.161346 -4.1307588 -4.104969 -4.1061897 -4.1191778 -4.1371551 -4.157753 -4.1753693 -4.1867003 -4.1895695][-4.2050071 -4.22016 -4.2330642 -4.2340932 -4.2284455 -4.2121243 -4.1846719 -4.1593747 -4.1520548 -4.155972 -4.1658897 -4.1793861 -4.1923819 -4.2004457 -4.2009497][-4.2070575 -4.2243071 -4.23988 -4.248857 -4.2552361 -4.2501035 -4.233067 -4.2153149 -4.2055116 -4.2035308 -4.2069235 -4.2145839 -4.2235107 -4.227962 -4.2236857][-4.1927152 -4.2123156 -4.2342091 -4.2519736 -4.2653503 -4.2673955 -4.2603397 -4.2517409 -4.245327 -4.2427139 -4.2424974 -4.2427511 -4.2453613 -4.2463717 -4.2412448][-4.1806774 -4.2001667 -4.2277064 -4.2521758 -4.2686691 -4.2732768 -4.2696805 -4.2643957 -4.2620649 -4.2628665 -4.2620039 -4.2603765 -4.2597013 -4.2585912 -4.2562556][-4.1836433 -4.1990509 -4.2254992 -4.2514181 -4.27135 -4.2813549 -4.2798533 -4.2743058 -4.2719541 -4.2722864 -4.2708149 -4.2692642 -4.268249 -4.2677031 -4.2680674]]...]
INFO - root - 2017-12-07 15:08:24.173101: step 22810, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 60h:19m:47s remains)
INFO - root - 2017-12-07 15:08:30.746307: step 22820, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 58h:03m:15s remains)
INFO - root - 2017-12-07 15:08:37.516028: step 22830, loss = 2.10, batch loss = 2.05 (12.3 examples/sec; 0.650 sec/batch; 55h:55m:24s remains)
INFO - root - 2017-12-07 15:08:44.339983: step 22840, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 57h:29m:54s remains)
INFO - root - 2017-12-07 15:08:51.193220: step 22850, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.732 sec/batch; 62h:58m:29s remains)
INFO - root - 2017-12-07 15:08:58.052030: step 22860, loss = 2.11, batch loss = 2.05 (11.1 examples/sec; 0.718 sec/batch; 61h:46m:14s remains)
INFO - root - 2017-12-07 15:09:04.706519: step 22870, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.690 sec/batch; 59h:23m:02s remains)
INFO - root - 2017-12-07 15:09:11.569561: step 22880, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 59h:07m:36s remains)
INFO - root - 2017-12-07 15:09:18.469797: step 22890, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.698 sec/batch; 60h:00m:44s remains)
INFO - root - 2017-12-07 15:09:25.402616: step 22900, loss = 2.02, batch loss = 1.96 (10.9 examples/sec; 0.735 sec/batch; 63h:13m:22s remains)
2017-12-07 15:09:26.078919: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3215666 -4.3188376 -4.3151984 -4.3080745 -4.2975545 -4.2875104 -4.28069 -4.2765336 -4.2749619 -4.2781258 -4.2825537 -4.2872319 -4.2931776 -4.2986116 -4.3027358][-4.319416 -4.3116503 -4.304132 -4.2922921 -4.2771978 -4.2628632 -4.2502842 -4.2415681 -4.2385406 -4.2442293 -4.25338 -4.2642789 -4.2772732 -4.2879539 -4.2951479][-4.3064456 -4.2911873 -4.2773252 -4.2587113 -4.2359505 -4.2145057 -4.1974144 -4.1881247 -4.1898212 -4.2032709 -4.2192516 -4.2373428 -4.2569323 -4.2715607 -4.2812181][-4.2755208 -4.2523217 -4.2303686 -4.2032218 -4.1673007 -4.1320839 -4.105144 -4.0954003 -4.1107988 -4.1429319 -4.1722841 -4.2011838 -4.2276864 -4.2440376 -4.25511][-4.2095451 -4.1814108 -4.1553369 -4.1201935 -4.0709734 -4.0201669 -3.9750214 -3.9550834 -3.9878516 -4.0496688 -4.1033306 -4.1493292 -4.1829319 -4.1992717 -4.2088122][-4.1212645 -4.0908365 -4.0621529 -4.0219531 -3.9647255 -3.8967624 -3.8154092 -3.7695718 -3.8225279 -3.9186478 -4.0017357 -4.0709724 -4.1142292 -4.133604 -4.1444206][-4.0444331 -4.0151649 -3.9846337 -3.9454904 -3.8946753 -3.8237574 -3.7171431 -3.6481168 -3.7076092 -3.813978 -3.9090512 -3.9947445 -4.0451646 -4.0696483 -4.0832033][-4.0262547 -4.0040946 -3.9791436 -3.9537513 -3.9336655 -3.9002347 -3.8280911 -3.776715 -3.8051152 -3.8616168 -3.924804 -3.995748 -4.0381284 -4.0617204 -4.0738263][-4.0824103 -4.0738964 -4.0616388 -4.0507174 -4.0500884 -4.0439906 -4.0110078 -3.9824104 -3.9850805 -3.9995198 -4.0318284 -4.0777788 -4.1042347 -4.1180196 -4.1216521][-4.1691265 -4.169744 -4.1658354 -4.1596837 -4.1605158 -4.1611056 -4.1461964 -4.1262579 -4.1169548 -4.1176486 -4.1361475 -4.165246 -4.1825972 -4.1924276 -4.1938725][-4.2273459 -4.23016 -4.229835 -4.2262344 -4.2253366 -4.2260666 -4.2197161 -4.2068009 -4.1953511 -4.1918473 -4.1998162 -4.2156963 -4.2274675 -4.2359548 -4.2395425][-4.2423568 -4.246232 -4.2468119 -4.2437057 -4.240593 -4.241056 -4.2393327 -4.232017 -4.2244287 -4.2215872 -4.22298 -4.2272482 -4.2307959 -4.2345166 -4.2390456][-4.22592 -4.2285166 -4.2294912 -4.2266307 -4.2233505 -4.2255893 -4.2269998 -4.22359 -4.2206607 -4.2171006 -4.2144141 -4.2120175 -4.2084117 -4.2063432 -4.2101593][-4.2215962 -4.2217727 -4.2207623 -4.2177253 -4.2148781 -4.2157736 -4.2162814 -4.2150288 -4.2143407 -4.2108622 -4.2077055 -4.2053118 -4.2015443 -4.1991744 -4.2027278][-4.2440071 -4.2416682 -4.2388973 -4.236299 -4.2355876 -4.2365446 -4.2361655 -4.2365217 -4.2372336 -4.2344222 -4.2318225 -4.2301412 -4.2278891 -4.2275696 -4.2309923]]...]
INFO - root - 2017-12-07 15:09:32.757714: step 22910, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 56h:40m:52s remains)
INFO - root - 2017-12-07 15:09:39.508484: step 22920, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 62h:47m:07s remains)
INFO - root - 2017-12-07 15:09:46.424880: step 22930, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 60h:37m:25s remains)
INFO - root - 2017-12-07 15:09:53.236729: step 22940, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 56h:56m:31s remains)
INFO - root - 2017-12-07 15:10:00.040613: step 22950, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 55h:51m:33s remains)
INFO - root - 2017-12-07 15:10:06.752040: step 22960, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 61h:10m:26s remains)
INFO - root - 2017-12-07 15:10:13.648515: step 22970, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 61h:56m:30s remains)
INFO - root - 2017-12-07 15:10:20.447552: step 22980, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 57h:54m:53s remains)
INFO - root - 2017-12-07 15:10:27.288186: step 22990, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 56h:43m:53s remains)
INFO - root - 2017-12-07 15:10:34.020733: step 23000, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 57h:27m:34s remains)
2017-12-07 15:10:34.738515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2049375 -4.2202392 -4.2252235 -4.2079344 -4.1703091 -4.1277471 -4.1056767 -4.1092877 -4.1286845 -4.1459289 -4.1643972 -4.182199 -4.1841369 -4.1726379 -4.1624236][-4.2160168 -4.2327204 -4.2394309 -4.223866 -4.1902351 -4.149622 -4.1231432 -4.1281037 -4.16026 -4.1926661 -4.2169962 -4.2298732 -4.2245212 -4.2001081 -4.1720719][-4.2162261 -4.23669 -4.2467942 -4.2357745 -4.2061868 -4.17177 -4.1483283 -4.1556239 -4.1874194 -4.2250652 -4.2510219 -4.2664146 -4.263906 -4.2369409 -4.2009759][-4.2146888 -4.2366815 -4.2445393 -4.2359881 -4.2104769 -4.1791134 -4.1547084 -4.1586018 -4.1856914 -4.21949 -4.2490621 -4.2711759 -4.2752223 -4.2592168 -4.2305112][-4.2023516 -4.2212782 -4.2247357 -4.2166934 -4.198843 -4.1737165 -4.1514416 -4.15094 -4.1680951 -4.1951952 -4.2255139 -4.2508264 -4.2602081 -4.2545848 -4.2412591][-4.1969938 -4.2056093 -4.2045226 -4.1968956 -4.1882277 -4.1735334 -4.1575375 -4.1590753 -4.1686187 -4.1880927 -4.2140946 -4.2347775 -4.2404222 -4.2377033 -4.2343531][-4.19289 -4.1910172 -4.1851692 -4.1777244 -4.1759214 -4.1692243 -4.1611495 -4.1660862 -4.1731358 -4.18774 -4.2054148 -4.2159472 -4.2124858 -4.20797 -4.2148671][-4.1898952 -4.1838355 -4.1727371 -4.1611619 -4.1566548 -4.1462936 -4.1386075 -4.1487908 -4.1661229 -4.1838937 -4.196835 -4.1969442 -4.1842566 -4.1794105 -4.1900678][-4.1842694 -4.181952 -4.1696906 -4.1565652 -4.1475554 -4.1259012 -4.1110964 -4.1261053 -4.1593251 -4.1927595 -4.2117124 -4.2082338 -4.1909146 -4.1846423 -4.19081][-4.1761088 -4.1743236 -4.1629133 -4.1554608 -4.1484566 -4.1251483 -4.1070228 -4.1214862 -4.1653175 -4.2093935 -4.2382541 -4.2424083 -4.2334151 -4.228775 -4.2283015][-4.1679811 -4.1585522 -4.1474476 -4.1532946 -4.1670766 -4.1591234 -4.1475692 -4.1551223 -4.1851668 -4.2169886 -4.243422 -4.2559423 -4.2598372 -4.2625551 -4.2632432][-4.1660376 -4.1470194 -4.1339936 -4.150238 -4.1839418 -4.1928225 -4.1904263 -4.1868396 -4.1962295 -4.2110124 -4.2278275 -4.241148 -4.2529135 -4.2620492 -4.2666006][-4.1612911 -4.1340775 -4.1151342 -4.134335 -4.1756039 -4.1974292 -4.2004175 -4.1933169 -4.1947947 -4.2076793 -4.2200975 -4.2253571 -4.2346053 -4.2468104 -4.2560892][-4.1277575 -4.0990748 -4.0838151 -4.1073122 -4.1550822 -4.1858521 -4.1936588 -4.18564 -4.1859593 -4.1983585 -4.2081356 -4.2102308 -4.2150455 -4.2268305 -4.2386355][-4.0948915 -4.0654149 -4.0482917 -4.0753083 -4.1314325 -4.1728811 -4.181211 -4.1718554 -4.1702514 -4.1810956 -4.1864696 -4.1883969 -4.1917453 -4.201654 -4.2131038]]...]
INFO - root - 2017-12-07 15:10:41.448926: step 23010, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 59h:12m:22s remains)
INFO - root - 2017-12-07 15:10:47.991744: step 23020, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 57h:54m:38s remains)
INFO - root - 2017-12-07 15:10:54.944182: step 23030, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 60h:39m:58s remains)
INFO - root - 2017-12-07 15:11:01.839542: step 23040, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 61h:29m:36s remains)
INFO - root - 2017-12-07 15:11:08.825689: step 23050, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 58h:17m:24s remains)
INFO - root - 2017-12-07 15:11:15.625280: step 23060, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 56h:03m:52s remains)
INFO - root - 2017-12-07 15:11:22.437286: step 23070, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 57h:29m:48s remains)
INFO - root - 2017-12-07 15:11:29.323366: step 23080, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 60h:15m:24s remains)
INFO - root - 2017-12-07 15:11:36.151935: step 23090, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 61h:33m:46s remains)
INFO - root - 2017-12-07 15:11:42.947385: step 23100, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 58h:28m:31s remains)
2017-12-07 15:11:43.698467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2342777 -4.2523837 -4.2729912 -4.2918696 -4.3033442 -4.3072705 -4.2975159 -4.2750254 -4.2580891 -4.2634344 -4.2899408 -4.3236 -4.3442903 -4.3497777 -4.3494143][-4.2389359 -4.2546425 -4.2727861 -4.2886257 -4.3004427 -4.3070803 -4.3007445 -4.2809367 -4.2654457 -4.271668 -4.2991266 -4.3305254 -4.3462777 -4.3465366 -4.3447413][-4.2642808 -4.2713509 -4.2817845 -4.2940445 -4.3074636 -4.3170042 -4.3138313 -4.2952433 -4.2766838 -4.2762446 -4.2946205 -4.3193207 -4.3314548 -4.332736 -4.3360238][-4.2925282 -4.2906551 -4.2950783 -4.3036284 -4.3171682 -4.3262939 -4.3195329 -4.295773 -4.2682176 -4.2577581 -4.2655582 -4.2845087 -4.2988129 -4.3094749 -4.3232656][-4.3105621 -4.3072433 -4.3101974 -4.3146939 -4.3208327 -4.3187857 -4.29848 -4.2587943 -4.2193089 -4.2021704 -4.2069273 -4.2279425 -4.2531424 -4.280973 -4.3079152][-4.2742529 -4.278193 -4.287374 -4.2921591 -4.2885842 -4.2689757 -4.2283587 -4.1676292 -4.1207337 -4.1093936 -4.1274028 -4.1651373 -4.2108264 -4.2591758 -4.2977819][-4.1789932 -4.1952057 -4.2169862 -4.2287936 -4.2154455 -4.1755004 -4.1137123 -4.0325589 -3.9893317 -4.0026646 -4.0511246 -4.1184626 -4.1873741 -4.249927 -4.29382][-4.0698218 -4.1109643 -4.1501555 -4.1638713 -4.1348777 -4.0744367 -3.9930789 -3.8939848 -3.8678603 -3.9277661 -4.0154424 -4.1114087 -4.1945052 -4.259068 -4.3025169][-3.9903307 -4.0582085 -4.1119432 -4.1209974 -4.07682 -4.0057578 -3.9234076 -3.8276854 -3.83258 -3.9324036 -4.044507 -4.14828 -4.2284632 -4.2853069 -4.3215842][-3.9949651 -4.0721364 -4.128056 -4.1276655 -4.0750284 -4.010437 -3.9528165 -3.8925502 -3.9184451 -4.01612 -4.1171303 -4.2066054 -4.2723207 -4.3153448 -4.3416796][-4.0903826 -4.1448903 -4.1819854 -4.1745515 -4.1306305 -4.0889788 -4.0624719 -4.0374355 -4.063684 -4.1306777 -4.2016411 -4.2657566 -4.3121686 -4.3411837 -4.3579116][-4.205267 -4.2304835 -4.2455654 -4.2362156 -4.2089806 -4.189857 -4.1849508 -4.1803956 -4.2012839 -4.237236 -4.2770052 -4.315505 -4.34457 -4.3640494 -4.3737435][-4.2884607 -4.2962265 -4.299118 -4.2900267 -4.2751751 -4.271112 -4.2760043 -4.2777176 -4.2894526 -4.3060641 -4.3282161 -4.3513894 -4.3701134 -4.3834276 -4.3873334][-4.3245597 -4.3287969 -4.3291273 -4.3235636 -4.3169017 -4.3184128 -4.3241558 -4.327107 -4.3332739 -4.3419285 -4.3552642 -4.3689837 -4.3801126 -4.3873568 -4.387619][-4.3400764 -4.34234 -4.3427634 -4.3402205 -4.3373227 -4.3379674 -4.34107 -4.3446851 -4.3486671 -4.3537083 -4.3614488 -4.3692856 -4.374948 -4.3785191 -4.3783684]]...]
INFO - root - 2017-12-07 15:11:50.422126: step 23110, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 58h:57m:25s remains)
INFO - root - 2017-12-07 15:11:57.073981: step 23120, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 59h:20m:55s remains)
INFO - root - 2017-12-07 15:12:03.944856: step 23130, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 54h:49m:11s remains)
INFO - root - 2017-12-07 15:12:10.877180: step 23140, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 59h:23m:05s remains)
INFO - root - 2017-12-07 15:12:17.668737: step 23150, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 59h:48m:37s remains)
INFO - root - 2017-12-07 15:12:24.496492: step 23160, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 60h:42m:01s remains)
INFO - root - 2017-12-07 15:12:31.292954: step 23170, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 59h:41m:50s remains)
INFO - root - 2017-12-07 15:12:37.819628: step 23180, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 56h:53m:59s remains)
INFO - root - 2017-12-07 15:12:44.675995: step 23190, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 58h:52m:19s remains)
INFO - root - 2017-12-07 15:12:51.524307: step 23200, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 57h:38m:51s remains)
2017-12-07 15:12:52.228793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3240237 -4.3213234 -4.316668 -4.3121629 -4.3087711 -4.3073578 -4.3063011 -4.3042488 -4.3053126 -4.31014 -4.3148775 -4.32159 -4.331553 -4.3399715 -4.3443561][-4.3241329 -4.3248911 -4.3225584 -4.3188024 -4.3140955 -4.3087354 -4.3024931 -4.2930465 -4.2896185 -4.2949882 -4.302875 -4.3142014 -4.3287849 -4.3407507 -4.3469739][-4.3271747 -4.3296318 -4.326859 -4.3203053 -4.3104053 -4.2979441 -4.2847395 -4.2668495 -4.2583394 -4.2645035 -4.27729 -4.2949615 -4.3165212 -4.3351469 -4.3451657][-4.3317823 -4.3327 -4.3248825 -4.3092031 -4.2886162 -4.2663054 -4.2439518 -4.2166371 -4.2051229 -4.2159586 -4.2399406 -4.2676091 -4.2974453 -4.3242645 -4.3394127][-4.3310809 -4.3265715 -4.3110867 -4.2859321 -4.2549214 -4.2209344 -4.1879969 -4.1526809 -4.1373463 -4.1548276 -4.1925378 -4.230197 -4.2685475 -4.30507 -4.32762][-4.3262987 -4.3154593 -4.2920942 -4.260653 -4.2221031 -4.1776819 -4.132122 -4.0882807 -4.0686064 -4.095149 -4.1463079 -4.19185 -4.2360168 -4.2823415 -4.3146224][-4.32348 -4.3089108 -4.2830534 -4.2506576 -4.2081838 -4.1576 -4.10217 -4.0498638 -4.02659 -4.0599365 -4.1192632 -4.1689925 -4.214273 -4.265264 -4.3043246][-4.3255224 -4.312324 -4.2899451 -4.2629652 -4.2229671 -4.1709127 -4.1146874 -4.0623388 -4.0395169 -4.0703759 -4.1240196 -4.1693144 -4.2117004 -4.2606273 -4.3002644][-4.3333731 -4.3261456 -4.3124471 -4.2957888 -4.2638211 -4.2201252 -4.1733904 -4.1293449 -4.1091003 -4.1290154 -4.1647711 -4.1974239 -4.230423 -4.2709455 -4.3047786][-4.3433275 -4.3426728 -4.3381214 -4.3318782 -4.3107328 -4.27817 -4.2441692 -4.2114725 -4.19617 -4.2037406 -4.2184567 -4.23626 -4.2587247 -4.288343 -4.3132911][-4.3518915 -4.3553853 -4.3557868 -4.3563423 -4.343956 -4.322432 -4.3008513 -4.27831 -4.2664938 -4.2656884 -4.2655749 -4.2728915 -4.28758 -4.3070908 -4.3228941][-4.3556705 -4.36053 -4.363039 -4.3663459 -4.3599324 -4.3466229 -4.334847 -4.3221383 -4.3153982 -4.3124843 -4.3065448 -4.3077693 -4.3143082 -4.3232331 -4.3307533][-4.3527517 -4.3575406 -4.360332 -4.3629313 -4.3586125 -4.3502707 -4.3446031 -4.3389525 -4.3365774 -4.3353457 -4.3298388 -4.3279915 -4.3293834 -4.3323145 -4.3352804][-4.3459873 -4.3487687 -4.3503137 -4.35069 -4.3463945 -4.3400612 -4.337657 -4.3369412 -4.3378592 -4.3387341 -4.3360763 -4.334734 -4.3342094 -4.3359127 -4.338141][-4.3396716 -4.3401823 -4.3402972 -4.3394752 -4.3357716 -4.3306541 -4.3298783 -4.3308058 -4.3329644 -4.33508 -4.3346062 -4.3339496 -4.3343678 -4.3373823 -4.3410573]]...]
INFO - root - 2017-12-07 15:12:58.918971: step 23210, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 55h:54m:37s remains)
INFO - root - 2017-12-07 15:13:05.523388: step 23220, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 58h:12m:39s remains)
INFO - root - 2017-12-07 15:13:12.318842: step 23230, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 54h:04m:21s remains)
INFO - root - 2017-12-07 15:13:19.141837: step 23240, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 58h:06m:08s remains)
INFO - root - 2017-12-07 15:13:26.009329: step 23250, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 56h:54m:46s remains)
INFO - root - 2017-12-07 15:13:32.764345: step 23260, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 60h:10m:37s remains)
INFO - root - 2017-12-07 15:13:39.660165: step 23270, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 59h:59m:29s remains)
INFO - root - 2017-12-07 15:13:46.422879: step 23280, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 56h:48m:51s remains)
INFO - root - 2017-12-07 15:13:53.274459: step 23290, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 58h:30m:39s remains)
INFO - root - 2017-12-07 15:14:00.119512: step 23300, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 56h:41m:46s remains)
2017-12-07 15:14:00.820176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2870431 -4.2920527 -4.29866 -4.2973514 -4.2901449 -4.2703094 -4.265132 -4.2864523 -4.3152094 -4.3263955 -4.3249331 -4.3081493 -4.2864 -4.2953982 -4.3124619][-4.2638655 -4.2568555 -4.2566085 -4.26174 -4.267581 -4.2585464 -4.2585988 -4.2824926 -4.3115005 -4.3223658 -4.3180962 -4.2990355 -4.2752666 -4.283999 -4.3047748][-4.2585158 -4.2423687 -4.2396107 -4.2511635 -4.2656245 -4.2637739 -4.262362 -4.2821312 -4.3074355 -4.3179941 -4.3131957 -4.2908759 -4.2652111 -4.2736635 -4.29879][-4.2707524 -4.2551556 -4.2586479 -4.2736306 -4.2857938 -4.2804427 -4.2701349 -4.2776084 -4.2944651 -4.3053408 -4.3017931 -4.2811837 -4.2596617 -4.2714348 -4.2973866][-4.2888918 -4.2787838 -4.285356 -4.2948027 -4.2931886 -4.2731476 -4.2439375 -4.2343278 -4.2497387 -4.2718668 -4.2789912 -4.2675772 -4.2547855 -4.2718968 -4.2954564][-4.3011613 -4.296896 -4.296124 -4.2818208 -4.2540159 -4.2116828 -4.1600814 -4.1366725 -4.1688995 -4.2267652 -4.2598367 -4.2586169 -4.2496166 -4.2693372 -4.2898][-4.3024497 -4.2969422 -4.2807927 -4.2381573 -4.1782494 -4.1028414 -4.0149717 -3.9749086 -4.0380964 -4.1462231 -4.2120247 -4.2221351 -4.2173815 -4.2436075 -4.270215][-4.2919488 -4.2826853 -4.2535191 -4.1900024 -4.1060615 -4.0003972 -3.8767853 -3.8219929 -3.9106846 -4.0558543 -4.1496 -4.1743627 -4.18027 -4.2189937 -4.2557669][-4.2872539 -4.2788877 -4.2483454 -4.186594 -4.1110425 -4.0160961 -3.9094706 -3.8716109 -3.9544718 -4.0781655 -4.158761 -4.1800103 -4.1874604 -4.2307105 -4.2684817][-4.2964292 -4.2958379 -4.2774234 -4.2379618 -4.1917872 -4.13389 -4.0720692 -4.0561996 -4.1097159 -4.1843567 -4.228045 -4.2327652 -4.2324662 -4.2692881 -4.298974][-4.3111372 -4.3150492 -4.3027983 -4.2810073 -4.2562628 -4.2240357 -4.1919074 -4.1885815 -4.2216897 -4.2616873 -4.2794909 -4.272706 -4.2675848 -4.2970638 -4.3192129][-4.3169703 -4.3193974 -4.31189 -4.3016305 -4.288816 -4.2722349 -4.25693 -4.2603183 -4.279767 -4.2982635 -4.29888 -4.2858729 -4.27823 -4.3004961 -4.3183851][-4.2996182 -4.3009286 -4.2975912 -4.2934551 -4.2870269 -4.2787991 -4.2734804 -4.2784147 -4.2889328 -4.2966337 -4.2892575 -4.2720666 -4.263042 -4.28317 -4.3019953][-4.2947474 -4.2943249 -4.2921786 -4.28954 -4.284111 -4.2768064 -4.2733469 -4.2749448 -4.2793832 -4.2809429 -4.2687726 -4.2484384 -4.2405 -4.2661543 -4.2912588][-4.3152809 -4.3099151 -4.3030019 -4.2955203 -4.2861905 -4.276206 -4.2679811 -4.2632446 -4.2629185 -4.2633443 -4.252233 -4.2320461 -4.228333 -4.2609262 -4.2905159]]...]
INFO - root - 2017-12-07 15:14:07.496181: step 23310, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 58h:33m:43s remains)
INFO - root - 2017-12-07 15:14:14.204720: step 23320, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 57h:25m:46s remains)
INFO - root - 2017-12-07 15:14:21.103357: step 23330, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 58h:35m:12s remains)
INFO - root - 2017-12-07 15:14:28.097237: step 23340, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 61h:42m:37s remains)
INFO - root - 2017-12-07 15:14:34.891657: step 23350, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 56h:44m:34s remains)
INFO - root - 2017-12-07 15:14:41.695654: step 23360, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 53h:31m:24s remains)
INFO - root - 2017-12-07 15:14:48.432633: step 23370, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 55h:16m:25s remains)
INFO - root - 2017-12-07 15:14:55.250793: step 23380, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 55h:50m:32s remains)
INFO - root - 2017-12-07 15:15:02.195812: step 23390, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 60h:45m:02s remains)
INFO - root - 2017-12-07 15:15:09.049825: step 23400, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.738 sec/batch; 63h:23m:49s remains)
2017-12-07 15:15:09.788352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2651567 -4.2883191 -4.2963605 -4.3011341 -4.2943139 -4.2748375 -4.2551236 -4.2420325 -4.2335877 -4.2492537 -4.2688017 -4.2702236 -4.2581038 -4.2412844 -4.2388048][-4.2706804 -4.2882876 -4.2974553 -4.3023748 -4.2874322 -4.2577167 -4.232605 -4.225862 -4.2311497 -4.2537994 -4.2770343 -4.2856941 -4.2780461 -4.2670388 -4.2659612][-4.2600913 -4.2817993 -4.2987885 -4.3047295 -4.2798271 -4.2400784 -4.2086167 -4.2039247 -4.2235093 -4.2505803 -4.2751904 -4.2895527 -4.2904749 -4.2860637 -4.2855878][-4.2410316 -4.2717681 -4.2951155 -4.3008785 -4.2727747 -4.2299304 -4.1942186 -4.1876674 -4.2082572 -4.236372 -4.2624569 -4.2792916 -4.289444 -4.295053 -4.299346][-4.226368 -4.2618461 -4.2849355 -4.2833815 -4.2513785 -4.2083778 -4.1757412 -4.1709571 -4.188138 -4.2131467 -4.2423449 -4.26513 -4.28628 -4.3018842 -4.3109546][-4.2192445 -4.2542858 -4.26963 -4.255928 -4.2163281 -4.1699338 -4.140008 -4.1419806 -4.1623206 -4.1887689 -4.2208071 -4.2526937 -4.2856641 -4.3080635 -4.3188629][-4.2180681 -4.2485628 -4.2561731 -4.2323613 -4.1835966 -4.1279726 -4.0942941 -4.101429 -4.1314268 -4.1678982 -4.2016344 -4.2402573 -4.2824392 -4.308917 -4.3196011][-4.2235017 -4.2463942 -4.247478 -4.2202988 -4.1667757 -4.1046791 -4.0607953 -4.065269 -4.103972 -4.1513143 -4.1883259 -4.2304425 -4.2748752 -4.3040709 -4.3146434][-4.2291985 -4.243866 -4.2409916 -4.2155938 -4.1644053 -4.1009469 -4.0501413 -4.0466766 -4.0863214 -4.1370883 -4.1768703 -4.2212062 -4.26558 -4.2979469 -4.3090558][-4.22809 -4.235652 -4.2310658 -4.2094216 -4.168819 -4.1106586 -4.0585194 -4.0483522 -4.0809546 -4.125433 -4.1655016 -4.212256 -4.2558413 -4.2890935 -4.301218][-4.21721 -4.21376 -4.20519 -4.1925812 -4.1678638 -4.1236081 -4.0807157 -4.0650759 -4.0866723 -4.1246214 -4.1639528 -4.2082515 -4.2482262 -4.2789764 -4.2909594][-4.2041588 -4.1904955 -4.1786594 -4.1741495 -4.166079 -4.1399984 -4.1125312 -4.0982642 -4.1119037 -4.1424327 -4.1764474 -4.2142668 -4.2462921 -4.2737112 -4.284934][-4.1936436 -4.1801238 -4.1709609 -4.171103 -4.1762109 -4.1684971 -4.1528015 -4.1412649 -4.1497397 -4.1719418 -4.1981392 -4.2272038 -4.2520981 -4.2767305 -4.2873769][-4.2034459 -4.196568 -4.1949935 -4.2024779 -4.2166452 -4.2181106 -4.2063475 -4.1962175 -4.1982989 -4.2072282 -4.2232141 -4.2427573 -4.2596459 -4.2812276 -4.2928724][-4.2315135 -4.2311635 -4.2348928 -4.2487965 -4.2681665 -4.2738171 -4.2627726 -4.2499743 -4.2433681 -4.2403088 -4.2430334 -4.252583 -4.2649746 -4.2839031 -4.2974634]]...]
INFO - root - 2017-12-07 15:15:16.584113: step 23410, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.685 sec/batch; 58h:46m:35s remains)
INFO - root - 2017-12-07 15:15:23.246167: step 23420, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 59h:56m:28s remains)
INFO - root - 2017-12-07 15:15:30.101811: step 23430, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 60h:42m:42s remains)
INFO - root - 2017-12-07 15:15:36.932705: step 23440, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 57h:35m:07s remains)
INFO - root - 2017-12-07 15:15:43.696420: step 23450, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 55h:44m:36s remains)
INFO - root - 2017-12-07 15:15:50.443274: step 23460, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 57h:55m:38s remains)
INFO - root - 2017-12-07 15:15:57.221164: step 23470, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 59h:42m:24s remains)
INFO - root - 2017-12-07 15:16:04.131457: step 23480, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 61h:58m:50s remains)
INFO - root - 2017-12-07 15:16:10.753462: step 23490, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 54h:42m:05s remains)
INFO - root - 2017-12-07 15:16:17.550023: step 23500, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:23m:48s remains)
2017-12-07 15:16:18.298175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25988 -4.247726 -4.2408175 -4.2417026 -4.2326264 -4.224349 -4.2250957 -4.2251019 -4.2303996 -4.22646 -4.2191057 -4.2202053 -4.2178636 -4.2143917 -4.2284427][-4.2236624 -4.2164674 -4.2141924 -4.2191582 -4.21159 -4.2007875 -4.1971955 -4.1926246 -4.1943836 -4.184 -4.1718278 -4.1759772 -4.1817384 -4.179461 -4.1950693][-4.2000089 -4.1980863 -4.1990924 -4.2019162 -4.1932755 -4.1883106 -4.1871052 -4.1844969 -4.1879292 -4.1760669 -4.1588621 -4.1592965 -4.1693125 -4.1681719 -4.1809578][-4.1772413 -4.177011 -4.1777563 -4.1748104 -4.166925 -4.1694088 -4.1712494 -4.1728368 -4.185976 -4.181098 -4.158618 -4.1489949 -4.1594615 -4.1643152 -4.1727476][-4.1778145 -4.1703935 -4.16699 -4.1567535 -4.14399 -4.1453929 -4.1402578 -4.1362891 -4.1642113 -4.1837997 -4.1667242 -4.1483445 -4.1535907 -4.1627164 -4.1659751][-4.2146521 -4.2006254 -4.1864338 -4.1660376 -4.1468644 -4.1392751 -4.1114788 -4.0800519 -4.1138458 -4.1684794 -4.17507 -4.1573486 -4.1543689 -4.1595268 -4.1599679][-4.2254987 -4.21344 -4.20072 -4.1777043 -4.1576014 -4.1407871 -4.0838313 -4.00455 -4.0224237 -4.1209331 -4.1682272 -4.1641164 -4.159265 -4.15765 -4.1568508][-4.1978006 -4.1905427 -4.1890621 -4.1728444 -4.15559 -4.1390123 -4.0617127 -3.9325027 -3.9166193 -4.0512476 -4.1420112 -4.1614494 -4.1623 -4.1608877 -4.15922][-4.1745138 -4.1704559 -4.1801271 -4.1717215 -4.1641626 -4.1593275 -4.0924129 -3.9620121 -3.9254255 -4.0452876 -4.1408663 -4.1690693 -4.1759043 -4.1769361 -4.1743031][-4.1707773 -4.1672325 -4.1830955 -4.1826415 -4.182066 -4.1901956 -4.1529088 -4.0618267 -4.0275159 -4.098978 -4.1654987 -4.1861634 -4.1959023 -4.2028804 -4.2012258][-4.185812 -4.1763124 -4.1916142 -4.1981688 -4.2015615 -4.2170358 -4.206409 -4.1540966 -4.1278419 -4.1594372 -4.1935267 -4.2025161 -4.2162242 -4.2303562 -4.233386][-4.2070336 -4.1924772 -4.20347 -4.2191916 -4.2228312 -4.2371964 -4.2385268 -4.21254 -4.1970048 -4.2062573 -4.2177129 -4.2199154 -4.2369537 -4.2547026 -4.2623429][-4.2364011 -4.2243147 -4.2308125 -4.2463655 -4.2511454 -4.2598376 -4.2627306 -4.2525587 -4.2459135 -4.2467537 -4.2478371 -4.2502141 -4.2656264 -4.2789249 -4.2866573][-4.2684236 -4.262629 -4.2674422 -4.2791414 -4.2842278 -4.2867346 -4.2881536 -4.2863021 -4.2854819 -4.2826695 -4.2809868 -4.2850122 -4.2958908 -4.3003993 -4.3051391][-4.2928109 -4.2905121 -4.2953224 -4.3045874 -4.3079405 -4.3055596 -4.3067532 -4.3095474 -4.3126931 -4.3099484 -4.3072152 -4.31115 -4.3166761 -4.3155489 -4.3179808]]...]
INFO - root - 2017-12-07 15:16:25.006283: step 23510, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 56h:22m:24s remains)
INFO - root - 2017-12-07 15:16:31.604469: step 23520, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 54h:17m:22s remains)
INFO - root - 2017-12-07 15:16:38.454852: step 23530, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 56h:55m:58s remains)
INFO - root - 2017-12-07 15:16:45.305785: step 23540, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.688 sec/batch; 59h:00m:15s remains)
INFO - root - 2017-12-07 15:16:52.145825: step 23550, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 60h:11m:59s remains)
INFO - root - 2017-12-07 15:16:58.970652: step 23560, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 60h:37m:57s remains)
INFO - root - 2017-12-07 15:17:05.775278: step 23570, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 56h:22m:07s remains)
INFO - root - 2017-12-07 15:17:12.575973: step 23580, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 57h:10m:09s remains)
INFO - root - 2017-12-07 15:17:19.413338: step 23590, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 62h:02m:21s remains)
INFO - root - 2017-12-07 15:17:26.269572: step 23600, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 60h:10m:47s remains)
2017-12-07 15:17:26.902301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2323456 -4.2312469 -4.2357464 -4.2439446 -4.2482305 -4.2476254 -4.2437186 -4.2376204 -4.22671 -4.2170362 -4.2099681 -4.2026343 -4.2030668 -4.2065921 -4.2065053][-4.22084 -4.2187915 -4.223382 -4.2328596 -4.2387128 -4.2386527 -4.2321048 -4.2207036 -4.2051435 -4.1934748 -4.18591 -4.1811571 -4.186635 -4.193563 -4.1919856][-4.2221503 -4.2170095 -4.21942 -4.2274003 -4.2329755 -4.2332611 -4.2228584 -4.2063804 -4.1885352 -4.1795454 -4.1759658 -4.1800556 -4.19316 -4.2025485 -4.1970048][-4.2277412 -4.2174234 -4.2132969 -4.2157588 -4.2173624 -4.2156558 -4.203526 -4.185503 -4.17045 -4.1686349 -4.1751647 -4.1891966 -4.2066922 -4.2152681 -4.20575][-4.2331634 -4.2157226 -4.2016797 -4.1941833 -4.186718 -4.1778808 -4.1639214 -4.14726 -4.1392841 -4.1480432 -4.166616 -4.1906786 -4.2112122 -4.2195029 -4.2094264][-4.2323585 -4.20982 -4.1874366 -4.1675258 -4.1470833 -4.1250196 -4.1008987 -4.0832539 -4.0865765 -4.1102409 -4.1413684 -4.1773024 -4.2073035 -4.2211514 -4.2148962][-4.228476 -4.2004862 -4.1693478 -4.1354723 -4.09815 -4.0549889 -4.0100918 -3.9892309 -4.0111952 -4.0583558 -4.1076708 -4.1585422 -4.2018318 -4.2229404 -4.2211781][-4.2226973 -4.1907206 -4.1531405 -4.108366 -4.0542111 -3.9892213 -3.9231529 -3.9000721 -3.9443762 -4.0167885 -4.0833988 -4.1459007 -4.1980057 -4.2238097 -4.2241292][-4.2214537 -4.1914244 -4.1535721 -4.1054516 -4.0482936 -3.9822986 -3.9164338 -3.8958535 -3.9471464 -4.0249271 -4.0931187 -4.1537304 -4.205369 -4.2302127 -4.2293377][-4.2296433 -4.2048545 -4.1712575 -4.1278868 -4.0815082 -4.034584 -3.9919157 -3.9775078 -4.0150681 -4.077888 -4.1341457 -4.1853361 -4.2290955 -4.2484426 -4.2425108][-4.2420158 -4.222249 -4.195734 -4.1590223 -4.1243515 -4.0943561 -4.0711403 -4.0627575 -4.0880532 -4.1333923 -4.17586 -4.2171988 -4.2517333 -4.2651849 -4.2551532][-4.2475452 -4.2320595 -4.2121992 -4.1841669 -4.1596746 -4.1418829 -4.1288924 -4.1225505 -4.1412015 -4.173224 -4.2037878 -4.2359066 -4.262217 -4.2719097 -4.2630253][-4.2471972 -4.2350259 -4.2202845 -4.19987 -4.1829948 -4.1710367 -4.1620526 -4.1566296 -4.1705132 -4.1929288 -4.2151518 -4.2404766 -4.2609277 -4.2690239 -4.2643204][-4.2419877 -4.231071 -4.21953 -4.2036633 -4.1905394 -4.1802912 -4.1735435 -4.1707511 -4.1808577 -4.1969476 -4.214767 -4.2363605 -4.25449 -4.2627177 -4.2622867][-4.2375631 -4.22705 -4.2178993 -4.2063651 -4.1961122 -4.1883717 -4.1837215 -4.1832409 -4.1907892 -4.2032146 -4.2172208 -4.2348137 -4.2504883 -4.2579532 -4.2600484]]...]
INFO - root - 2017-12-07 15:17:33.644367: step 23610, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 53h:36m:15s remains)
INFO - root - 2017-12-07 15:17:40.293992: step 23620, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 61h:25m:48s remains)
INFO - root - 2017-12-07 15:17:47.050039: step 23630, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 60h:43m:31s remains)
INFO - root - 2017-12-07 15:17:53.865483: step 23640, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 58h:59m:09s remains)
INFO - root - 2017-12-07 15:18:00.669664: step 23650, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 57h:52m:34s remains)
INFO - root - 2017-12-07 15:18:07.474340: step 23660, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 54h:29m:50s remains)
INFO - root - 2017-12-07 15:18:14.326380: step 23670, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 61h:44m:54s remains)
INFO - root - 2017-12-07 15:18:21.260131: step 23680, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.732 sec/batch; 62h:49m:30s remains)
INFO - root - 2017-12-07 15:18:28.254644: step 23690, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 59h:46m:13s remains)
INFO - root - 2017-12-07 15:18:35.069021: step 23700, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 57h:38m:38s remains)
2017-12-07 15:18:35.805898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0931368 -4.0807743 -4.0441847 -4.0242515 -4.0567489 -4.1126184 -4.1659822 -4.2033992 -4.2163944 -4.2033815 -4.162395 -4.1180067 -4.0916462 -4.0975871 -4.1376181][-4.1092405 -4.0922294 -4.0545688 -4.0331879 -4.0577092 -4.1042938 -4.1510544 -4.183043 -4.1922488 -4.1797214 -4.150558 -4.1252117 -4.1141768 -4.1249318 -4.1580167][-4.1364059 -4.1148696 -4.0727625 -4.0463953 -4.0573926 -4.0903 -4.12692 -4.1540775 -4.1629014 -4.1580267 -4.1470237 -4.1428809 -4.1474614 -4.1640205 -4.1913733][-4.1714668 -4.1460061 -4.1002226 -4.0657806 -4.0591383 -4.0751634 -4.0985508 -4.1177897 -4.1276922 -4.1311603 -4.1358232 -4.146172 -4.1625829 -4.1860285 -4.2139893][-4.2007842 -4.1714554 -4.121469 -4.075551 -4.0517774 -4.0499268 -4.05859 -4.0693355 -4.0808749 -4.0935936 -4.1093993 -4.1279993 -4.147265 -4.1740618 -4.2061625][-4.21154 -4.1727147 -4.1114635 -4.0512214 -4.016139 -4.0022392 -4.0007882 -4.0104313 -4.0315027 -4.0581613 -4.08905 -4.1183453 -4.1423244 -4.1699295 -4.2007313][-4.1930118 -4.1410475 -4.0658884 -3.9918861 -3.944468 -3.9223638 -3.9230559 -3.9417052 -3.9767342 -4.0244932 -4.0749531 -4.1164145 -4.144753 -4.17252 -4.2013941][-4.1646433 -4.1048889 -4.0214877 -3.9396372 -3.8820791 -3.8528419 -3.8521423 -3.8664687 -3.9037232 -3.9689682 -4.0389161 -4.0913711 -4.1248746 -4.1561265 -4.1844835][-4.1428742 -4.084094 -4.0055752 -3.9285126 -3.8725009 -3.8421226 -3.8397279 -3.84656 -3.8770661 -3.9438295 -4.0121922 -4.063592 -4.0991983 -4.132103 -4.1566949][-4.1300735 -4.0801668 -4.0176425 -3.9616914 -3.9247103 -3.9061186 -3.9066155 -3.9101787 -3.9219863 -3.9620118 -4.0050669 -4.0427589 -4.0769553 -4.1124125 -4.1356912][-4.1248326 -4.0859761 -4.0417404 -4.0080509 -3.9883595 -3.980459 -3.9794023 -3.9725928 -3.9694676 -3.9883087 -4.0162058 -4.0485253 -4.0815134 -4.11715 -4.1382952][-4.1339927 -4.1106644 -4.0822449 -4.0567546 -4.0396638 -4.0305009 -4.022748 -4.0055847 -3.9972539 -4.0116172 -4.0372343 -4.0690336 -4.1014948 -4.1337919 -4.1539779][-4.1534905 -4.1476707 -4.133338 -4.1119967 -4.0911255 -4.0721312 -4.0510659 -4.0221272 -4.0108871 -4.0234623 -4.0509877 -4.086535 -4.1232553 -4.1545281 -4.1740422][-4.1760969 -4.1852469 -4.1849976 -4.170548 -4.1443605 -4.1117177 -4.0756259 -4.0386257 -4.0247254 -4.0367975 -4.065969 -4.10517 -4.1438441 -4.170341 -4.1839681][-4.1999907 -4.2159791 -4.2250376 -4.2176571 -4.1910219 -4.1535339 -4.1117616 -4.0728765 -4.0580173 -4.0688009 -4.0967546 -4.1333413 -4.1646671 -4.1811681 -4.18738]]...]
INFO - root - 2017-12-07 15:18:42.519019: step 23710, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 58h:16m:57s remains)
INFO - root - 2017-12-07 15:18:49.161751: step 23720, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 58h:34m:53s remains)
INFO - root - 2017-12-07 15:18:55.817506: step 23730, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 54h:07m:30s remains)
INFO - root - 2017-12-07 15:19:02.602338: step 23740, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 56h:09m:33s remains)
INFO - root - 2017-12-07 15:19:09.266227: step 23750, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 59h:19m:55s remains)
INFO - root - 2017-12-07 15:19:16.139341: step 23760, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 61h:44m:04s remains)
INFO - root - 2017-12-07 15:19:22.966335: step 23770, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 59h:55m:38s remains)
INFO - root - 2017-12-07 15:19:29.747852: step 23780, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 57h:36m:42s remains)
INFO - root - 2017-12-07 15:19:36.634335: step 23790, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 59h:27m:06s remains)
INFO - root - 2017-12-07 15:19:43.011403: step 23800, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 56h:41m:53s remains)
2017-12-07 15:19:43.774746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2490158 -4.2382379 -4.2297845 -4.2170548 -4.2046652 -4.2004704 -4.2011619 -4.2046313 -4.2087574 -4.2159252 -4.2204304 -4.2177114 -4.2054944 -4.1961031 -4.1950159][-4.2468967 -4.2332749 -4.2166314 -4.193687 -4.1719437 -4.1626253 -4.1641755 -4.1768441 -4.1942534 -4.2134953 -4.2286987 -4.2358513 -4.234755 -4.2306409 -4.2281284][-4.2553492 -4.2425117 -4.2229729 -4.1927619 -4.1614232 -4.1437154 -4.14605 -4.1654058 -4.1951904 -4.2255363 -4.2519059 -4.2697692 -4.2777681 -4.2776418 -4.2712765][-4.2522264 -4.2447562 -4.2280169 -4.1947088 -4.1584778 -4.1341252 -4.1329794 -4.1538577 -4.1913157 -4.2294521 -4.2622809 -4.2888093 -4.3039579 -4.3062429 -4.2958369][-4.2339149 -4.2327433 -4.2209859 -4.1887784 -4.1493993 -4.1190214 -4.1120105 -4.1303248 -4.1706452 -4.2130017 -4.2486792 -4.2783966 -4.2938271 -4.2959609 -4.2835245][-4.1878529 -4.1984653 -4.1975737 -4.1714153 -4.1286163 -4.0902185 -4.0754352 -4.0856614 -4.124115 -4.1653185 -4.2018914 -4.2346983 -4.2487459 -4.2478895 -4.2372355][-4.11397 -4.1394091 -4.1544495 -4.1418729 -4.1055017 -4.0658274 -4.0452585 -4.0458584 -4.0781422 -4.1153655 -4.1509738 -4.1845541 -4.1992636 -4.2004023 -4.1981063][-4.0523348 -4.0893121 -4.115531 -4.1179333 -4.0941277 -4.0602989 -4.0406132 -4.0357928 -4.064394 -4.0986233 -4.1325269 -4.1643844 -4.1793504 -4.1851535 -4.1929464][-4.0410228 -4.0783348 -4.1079011 -4.12124 -4.113256 -4.0918236 -4.075458 -4.0683403 -4.0935979 -4.1236591 -4.1531687 -4.178834 -4.1894531 -4.1961174 -4.2092986][-4.1001654 -4.1224608 -4.1418071 -4.15762 -4.1603279 -4.1505027 -4.1381235 -4.1299415 -4.147727 -4.1683488 -4.18809 -4.2040706 -4.2075744 -4.21162 -4.2240038][-4.1821961 -4.1860962 -4.1887283 -4.1994629 -4.2071314 -4.20498 -4.19474 -4.1856146 -4.1943369 -4.2057939 -4.2167964 -4.2224021 -4.2209744 -4.2230082 -4.232234][-4.2511253 -4.2452173 -4.2379251 -4.2407532 -4.2470741 -4.248724 -4.2413225 -4.232872 -4.2354565 -4.2405767 -4.2450557 -4.2428765 -4.2373362 -4.2373819 -4.2422819][-4.2927952 -4.2862339 -4.2794595 -4.278254 -4.2810326 -4.2844057 -4.2814779 -4.2761154 -4.2773137 -4.2780695 -4.2758088 -4.2665238 -4.2549262 -4.2480087 -4.2455516][-4.2967796 -4.2973514 -4.3016839 -4.3061032 -4.3113203 -4.3164811 -4.3167882 -4.3122482 -4.308166 -4.3032994 -4.2963948 -4.2857294 -4.2724266 -4.26091 -4.2522287][-4.2606573 -4.27663 -4.2973785 -4.3139911 -4.3265648 -4.3355131 -4.3384981 -4.3351178 -4.3252149 -4.31529 -4.3086514 -4.3020186 -4.2924776 -4.2808242 -4.26862]]...]
INFO - root - 2017-12-07 15:19:50.463058: step 23810, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 55h:11m:00s remains)
INFO - root - 2017-12-07 15:19:57.187920: step 23820, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 56h:41m:53s remains)
INFO - root - 2017-12-07 15:20:04.041675: step 23830, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 61h:27m:36s remains)
INFO - root - 2017-12-07 15:20:10.902738: step 23840, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 60h:05m:13s remains)
INFO - root - 2017-12-07 15:20:17.771017: step 23850, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 58h:59m:52s remains)
INFO - root - 2017-12-07 15:20:24.599887: step 23860, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 57h:11m:01s remains)
INFO - root - 2017-12-07 15:20:31.372865: step 23870, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 56h:58m:46s remains)
INFO - root - 2017-12-07 15:20:38.112073: step 23880, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 57h:54m:54s remains)
INFO - root - 2017-12-07 15:20:44.889007: step 23890, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 61h:53m:59s remains)
INFO - root - 2017-12-07 15:20:51.668359: step 23900, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 54h:36m:51s remains)
2017-12-07 15:20:52.367458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2876048 -4.2793436 -4.2750087 -4.2748113 -4.2784195 -4.2810788 -4.277205 -4.2696705 -4.2676115 -4.2723379 -4.2769628 -4.2805481 -4.2837839 -4.2874374 -4.2973547][-4.2438264 -4.2302837 -4.2242336 -4.2272968 -4.234261 -4.2392678 -4.2322226 -4.2167568 -4.2195473 -4.2384725 -4.2583666 -4.2725954 -4.2812462 -4.2867885 -4.2952442][-4.1987767 -4.1762414 -4.164907 -4.1702118 -4.1784358 -4.1770005 -4.1511359 -4.1138506 -4.1237984 -4.1656432 -4.2087474 -4.2422228 -4.2642469 -4.2754264 -4.2849755][-4.1726174 -4.1423292 -4.1251454 -4.1295967 -4.1358 -4.1231828 -4.067771 -4.0020313 -4.0215158 -4.0916028 -4.1559982 -4.203989 -4.2383103 -4.2572 -4.2688603][-4.1545296 -4.1173573 -4.0973082 -4.1002989 -4.1069651 -4.0887246 -4.0180607 -3.9343734 -3.9632757 -4.0561986 -4.1311736 -4.1810279 -4.2217164 -4.2445097 -4.2570214][-4.1302061 -4.0875573 -4.07179 -4.0742779 -4.0739937 -4.0451379 -3.9568691 -3.849524 -3.8934553 -4.0231915 -4.1118135 -4.1648712 -4.2110949 -4.2372031 -4.250597][-4.1142426 -4.070312 -4.0593381 -4.056447 -4.03275 -3.9702873 -3.8353071 -3.6865358 -3.7565663 -3.9451227 -4.0649872 -4.135004 -4.1932192 -4.226964 -4.2437234][-4.1250234 -4.0862608 -4.0845747 -4.0796576 -4.0341854 -3.9417014 -3.7772906 -3.6143429 -3.7032547 -3.911963 -4.03999 -4.1150303 -4.1785531 -4.2137828 -4.2345424][-4.1468563 -4.1175046 -4.1310859 -4.1379905 -4.0938396 -4.0092154 -3.8748045 -3.7531123 -3.8261681 -3.9856553 -4.0811095 -4.1375141 -4.1874776 -4.2152123 -4.2335939][-4.1677432 -4.1456614 -4.1693697 -4.1880655 -4.1554723 -4.0901322 -3.9897475 -3.9049878 -3.959341 -4.077621 -4.1446357 -4.183907 -4.2195969 -4.2392206 -4.2487559][-4.1910272 -4.1715622 -4.1898189 -4.2073717 -4.1807642 -4.1235313 -4.0390086 -3.9662261 -4.0031056 -4.1068921 -4.1720867 -4.2119408 -4.2430239 -4.2583747 -4.2635098][-4.2206531 -4.2064562 -4.2181878 -4.2286582 -4.1998849 -4.1492233 -4.0809922 -4.0144477 -4.0331435 -4.1160288 -4.1775208 -4.2180839 -4.2477951 -4.2626405 -4.2702656][-4.2512021 -4.2438135 -4.251832 -4.2560039 -4.2334647 -4.1990457 -4.1511884 -4.0969176 -4.0989761 -4.1502643 -4.1922851 -4.2243195 -4.249434 -4.2664213 -4.2784495][-4.2879553 -4.2845411 -4.2860394 -4.28408 -4.269618 -4.2471385 -4.2128558 -4.1688161 -4.1627827 -4.1898341 -4.2160554 -4.2423983 -4.2637033 -4.2804866 -4.2930202][-4.3167024 -4.3160343 -4.3136783 -4.3080082 -4.2975512 -4.2805929 -4.2535043 -4.2152519 -4.202848 -4.2163987 -4.2338424 -4.2561359 -4.2753959 -4.2903738 -4.3024759]]...]
INFO - root - 2017-12-07 15:20:59.176250: step 23910, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 57h:56m:09s remains)
INFO - root - 2017-12-07 15:21:05.954770: step 23920, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 59h:25m:31s remains)
INFO - root - 2017-12-07 15:21:12.727549: step 23930, loss = 2.03, batch loss = 1.97 (12.6 examples/sec; 0.637 sec/batch; 54h:34m:49s remains)
INFO - root - 2017-12-07 15:21:19.667428: step 23940, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.691 sec/batch; 59h:12m:59s remains)
INFO - root - 2017-12-07 15:21:26.492171: step 23950, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 61h:04m:00s remains)
INFO - root - 2017-12-07 15:21:33.320238: step 23960, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 61h:09m:28s remains)
INFO - root - 2017-12-07 15:21:40.128240: step 23970, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 55h:12m:48s remains)
INFO - root - 2017-12-07 15:21:46.889799: step 23980, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 55h:47m:07s remains)
INFO - root - 2017-12-07 15:21:53.752051: step 23990, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 60h:08m:01s remains)
INFO - root - 2017-12-07 15:22:00.534421: step 24000, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 60h:33m:10s remains)
2017-12-07 15:22:01.201727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282846 -4.285181 -4.2952981 -4.307652 -4.3172126 -4.3197737 -4.3139467 -4.3039236 -4.2930193 -4.2855163 -4.2874155 -4.2922211 -4.2936716 -4.2935057 -4.29377][-4.2403946 -4.2430468 -4.2584777 -4.2782531 -4.2961373 -4.3052144 -4.3062291 -4.3028307 -4.2960796 -4.2884946 -4.2874804 -4.2880392 -4.2861514 -4.2824974 -4.2798924][-4.20027 -4.2022476 -4.2205386 -4.2415586 -4.259654 -4.2703171 -4.2759752 -4.2793126 -4.2770376 -4.2680092 -4.2620773 -4.2596793 -4.2565026 -4.2514009 -4.2478266][-4.1657276 -4.1697569 -4.1917229 -4.2107511 -4.2197542 -4.2204 -4.22098 -4.2261162 -4.2254939 -4.2149878 -4.2070522 -4.2061281 -4.2086735 -4.2092357 -4.2106318][-4.132442 -4.143044 -4.1712413 -4.1905389 -4.1835623 -4.163147 -4.1478763 -4.1457686 -4.1462793 -4.1398039 -4.136642 -4.1431026 -4.1581335 -4.1718593 -4.182941][-4.0926094 -4.1129436 -4.1491556 -4.1657209 -4.1390114 -4.0919137 -4.0538416 -4.0396004 -4.0486956 -4.0614824 -4.0751071 -4.0989194 -4.1312995 -4.1610708 -4.1830893][-4.0578551 -4.0888572 -4.1235609 -4.1258755 -4.0789032 -4.01033 -3.9499512 -3.927824 -3.9622402 -4.0105753 -4.0485873 -4.09182 -4.1381307 -4.178297 -4.2079759][-4.0473948 -4.0778384 -4.0943561 -4.0720668 -4.0093737 -3.93468 -3.8644006 -3.8466058 -3.9117188 -3.9951525 -4.0552011 -4.1134496 -4.1658411 -4.2052908 -4.2324862][-4.0604162 -4.0847411 -4.080307 -4.0347481 -3.9656603 -3.9003944 -3.8440204 -3.845336 -3.9265237 -4.0172811 -4.0794864 -4.1355968 -4.18476 -4.2179117 -4.2376032][-4.0843868 -4.1030421 -4.0887575 -4.0394878 -3.9859438 -3.9444637 -3.9166706 -3.9318476 -4.0021763 -4.0721064 -4.116396 -4.156631 -4.1931105 -4.21625 -4.226213][-4.1245642 -4.1333065 -4.1142936 -4.0768051 -4.0500979 -4.0362287 -4.03284 -4.0540304 -4.09908 -4.1361523 -4.1566162 -4.1778178 -4.1960206 -4.2061844 -4.207911][-4.1853805 -4.18479 -4.164556 -4.14 -4.1342006 -4.1399312 -4.1529775 -4.1741657 -4.1956968 -4.2046995 -4.2040691 -4.2045169 -4.2025123 -4.1996536 -4.1954341][-4.2455425 -4.2407265 -4.2233906 -4.2058649 -4.20734 -4.221796 -4.2433844 -4.2638297 -4.2727928 -4.268033 -4.251895 -4.2340274 -4.2165089 -4.2039285 -4.195734][-4.2739091 -4.2709284 -4.2616816 -4.2520008 -4.2528176 -4.2663069 -4.2899914 -4.308744 -4.3141561 -4.307622 -4.2873416 -4.2621284 -4.2368493 -4.2182527 -4.2059345][-4.2570052 -4.2588234 -4.25765 -4.25249 -4.2485623 -4.255331 -4.2776365 -4.2970467 -4.307147 -4.3099756 -4.2992468 -4.2774973 -4.25287 -4.2335005 -4.2180896]]...]
INFO - root - 2017-12-07 15:22:07.902862: step 24010, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 55h:27m:40s remains)
INFO - root - 2017-12-07 15:22:14.515408: step 24020, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.697 sec/batch; 59h:45m:38s remains)
INFO - root - 2017-12-07 15:22:21.365338: step 24030, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 60h:02m:53s remains)
INFO - root - 2017-12-07 15:22:28.282116: step 24040, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 59h:18m:45s remains)
INFO - root - 2017-12-07 15:22:35.067913: step 24050, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 57h:14m:24s remains)
INFO - root - 2017-12-07 15:22:41.978535: step 24060, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 60h:17m:56s remains)
INFO - root - 2017-12-07 15:22:48.829938: step 24070, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 60h:45m:22s remains)
INFO - root - 2017-12-07 15:22:55.617977: step 24080, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 57h:09m:04s remains)
INFO - root - 2017-12-07 15:23:02.491045: step 24090, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 56h:41m:24s remains)
INFO - root - 2017-12-07 15:23:09.291984: step 24100, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 54h:39m:21s remains)
2017-12-07 15:23:09.980630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23585 -4.2129469 -4.182188 -4.1430917 -4.1100688 -4.1080017 -4.1351337 -4.1471081 -4.1471453 -4.15173 -4.1513519 -4.1493044 -4.1521916 -4.13878 -4.1219335][-4.2398739 -4.2196469 -4.1858721 -4.1479049 -4.1244206 -4.1278863 -4.1519241 -4.1551743 -4.14422 -4.1488109 -4.1529846 -4.1562204 -4.1568155 -4.1370883 -4.115788][-4.2322149 -4.217361 -4.1853676 -4.1544948 -4.1478491 -4.1587248 -4.1785975 -4.1799669 -4.1655903 -4.1682081 -4.1727958 -4.1770535 -4.174078 -4.1483111 -4.1179214][-4.2269707 -4.2170539 -4.18334 -4.16064 -4.1650829 -4.1793437 -4.1898894 -4.1886153 -4.1784563 -4.1825624 -4.1875753 -4.1942883 -4.1889763 -4.1568809 -4.1148982][-4.2262354 -4.2213645 -4.1821938 -4.1573176 -4.1614981 -4.1706057 -4.1625319 -4.1550694 -4.1596575 -4.1775608 -4.1912975 -4.2034636 -4.2002568 -4.1654119 -4.1148434][-4.2083926 -4.214046 -4.1751184 -4.1444197 -4.143456 -4.1413946 -4.1039004 -4.0771976 -4.1050549 -4.1499381 -4.1788468 -4.19573 -4.1949372 -4.1625233 -4.1088367][-4.1741543 -4.1868954 -4.1589031 -4.1268973 -4.1202388 -4.1025605 -4.0271354 -3.9680192 -4.0196548 -4.0962095 -4.1447983 -4.1693249 -4.174902 -4.1449895 -4.0912056][-4.1552086 -4.168407 -4.1495256 -4.1184816 -4.104497 -4.06737 -3.9599643 -3.8695073 -3.9322865 -4.0326247 -4.1029415 -4.1443129 -4.1599073 -4.1315274 -4.0758038][-4.1678 -4.1774325 -4.1638875 -4.1355395 -4.1119094 -4.0581675 -3.9410639 -3.8404369 -3.8973284 -4.0009117 -4.080461 -4.1355414 -4.1559486 -4.1356664 -4.0837145][-4.2041941 -4.2143703 -4.2065525 -4.1841893 -4.1542225 -4.0959697 -3.99223 -3.9064083 -3.9452257 -4.0280309 -4.0981946 -4.1559677 -4.1785574 -4.1706657 -4.1323204][-4.2505326 -4.2616377 -4.25646 -4.2383232 -4.2077851 -4.1569033 -4.079174 -4.0182066 -4.0425019 -4.1004953 -4.1554418 -4.2024717 -4.2258472 -4.2312956 -4.2042503][-4.2915196 -4.3021488 -4.3005366 -4.2850537 -4.2574162 -4.2185817 -4.1669011 -4.12711 -4.1437035 -4.1827865 -4.2209959 -4.2529964 -4.2753119 -4.2868075 -4.2685847][-4.3174448 -4.3266907 -4.3307343 -4.3223119 -4.300869 -4.2752819 -4.2461085 -4.2221403 -4.2318516 -4.2541656 -4.2755451 -4.2906189 -4.3070478 -4.3203664 -4.3097496][-4.3262787 -4.3348789 -4.3398604 -4.3374944 -4.3252311 -4.3118734 -4.2979703 -4.285821 -4.2899094 -4.3000813 -4.3077359 -4.3095846 -4.3173933 -4.3281317 -4.3249784][-4.3252797 -4.3313265 -4.3327947 -4.3318233 -4.3263016 -4.3208852 -4.3174634 -4.3136997 -4.3171549 -4.3217192 -4.3223934 -4.3180995 -4.319571 -4.3244076 -4.3243613]]...]
INFO - root - 2017-12-07 15:23:16.422256: step 24110, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 58h:30m:17s remains)
INFO - root - 2017-12-07 15:23:23.064257: step 24120, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 55h:09m:08s remains)
INFO - root - 2017-12-07 15:23:29.839118: step 24130, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 56h:14m:39s remains)
INFO - root - 2017-12-07 15:23:36.628410: step 24140, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 61h:36m:42s remains)
INFO - root - 2017-12-07 15:23:43.480624: step 24150, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 62h:48m:33s remains)
INFO - root - 2017-12-07 15:23:50.322509: step 24160, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 57h:05m:24s remains)
INFO - root - 2017-12-07 15:23:57.094015: step 24170, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 54h:49m:50s remains)
INFO - root - 2017-12-07 15:24:04.061559: step 24180, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 58h:20m:08s remains)
INFO - root - 2017-12-07 15:24:10.938266: step 24190, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 63h:01m:16s remains)
INFO - root - 2017-12-07 15:24:17.771433: step 24200, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 57h:28m:53s remains)
2017-12-07 15:24:18.523710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1915178 -4.1965909 -4.2093377 -4.2219334 -4.2259426 -4.2089081 -4.1868911 -4.1831961 -4.1946816 -4.2091556 -4.22065 -4.2292757 -4.2344294 -4.2392287 -4.2402129][-4.1992249 -4.2036467 -4.212923 -4.2238536 -4.2254858 -4.2037268 -4.1816082 -4.1830149 -4.1998715 -4.21737 -4.2257996 -4.2286086 -4.2320561 -4.235817 -4.2359428][-4.2017655 -4.2092004 -4.2207618 -4.23433 -4.2369 -4.2160816 -4.1964788 -4.2000628 -4.2173357 -4.2356668 -4.2433195 -4.2423205 -4.2430825 -4.2432289 -4.2406726][-4.2028484 -4.2136931 -4.2263 -4.241991 -4.2498307 -4.2364564 -4.2224779 -4.2269173 -4.24058 -4.2536583 -4.2611961 -4.2598972 -4.2599955 -4.2567291 -4.2511811][-4.207345 -4.2133145 -4.2206645 -4.2300377 -4.2417226 -4.2415695 -4.23958 -4.2476034 -4.2565908 -4.2652364 -4.2684479 -4.266716 -4.2670364 -4.2651644 -4.2605333][-4.2119107 -4.2104511 -4.2083564 -4.2095861 -4.2189584 -4.2293086 -4.2418604 -4.2554326 -4.2608957 -4.2622437 -4.2574139 -4.2555122 -4.2587829 -4.2597318 -4.2536216][-4.2009149 -4.1942024 -4.184545 -4.1776919 -4.1802015 -4.1915288 -4.2106514 -4.22719 -4.2324429 -4.2324624 -4.2264166 -4.230032 -4.2386436 -4.2401676 -4.2287273][-4.1703882 -4.1660433 -4.1527734 -4.1418324 -4.1333513 -4.1292973 -4.1380863 -4.1535788 -4.1672087 -4.1760979 -4.1780515 -4.1891723 -4.2047162 -4.2052732 -4.187891][-4.1234007 -4.1293955 -4.1189847 -4.104094 -4.076292 -4.0434518 -4.0328274 -4.0489664 -4.0802655 -4.11056 -4.1319714 -4.1522913 -4.1698627 -4.1705408 -4.1471605][-4.0908 -4.104495 -4.099165 -4.07521 -4.0254664 -3.9660282 -3.9361718 -3.950912 -3.9991689 -4.0523486 -4.0942917 -4.1208153 -4.1408958 -4.1424646 -4.1194711][-4.1010885 -4.1122146 -4.1020937 -4.0675478 -4.0127354 -3.9489584 -3.9095526 -3.9139092 -3.9599109 -4.0210781 -4.0753579 -4.1088929 -4.131012 -4.13578 -4.120368][-4.1512508 -4.1514225 -4.1350451 -4.0988169 -4.0514688 -4.0012627 -3.9653344 -3.9642942 -3.9961846 -4.0448108 -4.0949821 -4.1287231 -4.1507249 -4.1601968 -4.1566157][-4.2217107 -4.2121582 -4.1908622 -4.15787 -4.1246667 -4.0944805 -4.0714912 -4.06827 -4.0863247 -4.1171832 -4.1537824 -4.1844168 -4.2048984 -4.2155633 -4.2169237][-4.2866449 -4.2759109 -4.2582545 -4.23459 -4.213697 -4.1967845 -4.1840582 -4.1810722 -4.1906157 -4.2090077 -4.230886 -4.2508054 -4.2648754 -4.2739081 -4.2772284][-4.3268714 -4.3217049 -4.312479 -4.2993245 -4.2876253 -4.2799921 -4.2752986 -4.2737656 -4.278306 -4.2858734 -4.29472 -4.304317 -4.3125176 -4.3191752 -4.32183]]...]
INFO - root - 2017-12-07 15:24:25.298953: step 24210, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 60h:20m:23s remains)
INFO - root - 2017-12-07 15:24:32.036440: step 24220, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.735 sec/batch; 62h:54m:21s remains)
INFO - root - 2017-12-07 15:24:38.814121: step 24230, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 59h:06m:10s remains)
INFO - root - 2017-12-07 15:24:45.541186: step 24240, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 54h:44m:14s remains)
INFO - root - 2017-12-07 15:24:52.353469: step 24250, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 58h:20m:14s remains)
INFO - root - 2017-12-07 15:24:59.204858: step 24260, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 58h:02m:21s remains)
INFO - root - 2017-12-07 15:25:05.975768: step 24270, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 61h:59m:19s remains)
INFO - root - 2017-12-07 15:25:12.900188: step 24280, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 59h:09m:06s remains)
INFO - root - 2017-12-07 15:25:19.665316: step 24290, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 57h:32m:18s remains)
INFO - root - 2017-12-07 15:25:26.431609: step 24300, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 55h:30m:58s remains)
2017-12-07 15:25:27.172006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3220243 -4.3393574 -4.3525276 -4.3518043 -4.3270941 -4.2834063 -4.2265973 -4.1759448 -4.1570311 -4.1733742 -4.2059307 -4.2405796 -4.2821145 -4.3130269 -4.3240733][-4.3205152 -4.3442364 -4.3586106 -4.3538289 -4.3175931 -4.2562466 -4.1736884 -4.0956435 -4.0697017 -4.0988021 -4.1489172 -4.203917 -4.2627773 -4.3060269 -4.3212008][-4.3064694 -4.33541 -4.3492632 -4.3413067 -4.2963195 -4.2188821 -4.1096525 -4.0064473 -3.9779205 -4.0266805 -4.1003957 -4.1759825 -4.2447066 -4.2957129 -4.3153005][-4.2847595 -4.3172607 -4.3289509 -4.3167934 -4.2627306 -4.1696458 -4.0396061 -3.9217317 -3.89088 -3.9603617 -4.0544934 -4.1468735 -4.2234974 -4.2801247 -4.3065743][-4.2661529 -4.3018346 -4.312449 -4.2928009 -4.2282372 -4.1209345 -3.9779611 -3.8515406 -3.8175755 -3.8964782 -3.9989514 -4.1054749 -4.1961713 -4.2628937 -4.298018][-4.2592044 -4.29366 -4.3027868 -4.2753654 -4.207397 -4.1005955 -3.9612608 -3.8395755 -3.8070512 -3.8828049 -3.9757888 -4.0825844 -4.1809292 -4.2541437 -4.2946391][-4.2616968 -4.2910562 -4.2979121 -4.2654772 -4.2050972 -4.1155825 -3.9975169 -3.8952963 -3.8672032 -3.932586 -4.0068235 -4.0951409 -4.1848893 -4.25513 -4.2953362][-4.2539835 -4.2748241 -4.2791729 -4.2455368 -4.1948118 -4.124238 -4.020731 -3.9316792 -3.9049702 -3.9724913 -4.0429659 -4.1179619 -4.1987004 -4.2627807 -4.2988954][-4.2439551 -4.2508941 -4.2481079 -4.2173667 -4.1758642 -4.1139708 -4.0158768 -3.9351726 -3.9150724 -3.9905334 -4.0677505 -4.1394715 -4.2155337 -4.2733021 -4.3038273][-4.2419462 -4.2332129 -4.2267118 -4.2089047 -4.1806808 -4.128593 -4.0459089 -3.9807148 -3.9619374 -4.0203395 -4.0883565 -4.152957 -4.2247853 -4.2782378 -4.3063655][-4.243134 -4.2250605 -4.2183142 -4.2145977 -4.2033191 -4.1728473 -4.1149144 -4.06378 -4.0426359 -4.0692587 -4.1136794 -4.1641769 -4.2268753 -4.2777429 -4.3065395][-4.2416091 -4.2197676 -4.2111282 -4.2174907 -4.2176995 -4.2041941 -4.1634974 -4.1224928 -4.1042242 -4.1194663 -4.1500797 -4.187067 -4.2365189 -4.2803473 -4.3072896][-4.2291694 -4.2027197 -4.191144 -4.200377 -4.2082758 -4.2064676 -4.1749449 -4.1415343 -4.1390467 -4.1633863 -4.1924257 -4.22099 -4.2580838 -4.2902584 -4.3106232][-4.2090268 -4.1857982 -4.1744671 -4.1811152 -4.1928797 -4.196053 -4.1685934 -4.1431 -4.1547995 -4.1883802 -4.2196484 -4.247674 -4.2785826 -4.3023839 -4.3145986][-4.2081637 -4.1936173 -4.1848412 -4.1852279 -4.192996 -4.1947465 -4.1694865 -4.1520896 -4.1678915 -4.2008033 -4.2317696 -4.2605777 -4.2891068 -4.3081408 -4.3158183]]...]
INFO - root - 2017-12-07 15:25:33.941101: step 24310, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 58h:31m:07s remains)
INFO - root - 2017-12-07 15:25:40.605221: step 24320, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 59h:46m:08s remains)
INFO - root - 2017-12-07 15:25:47.438701: step 24330, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 58h:30m:14s remains)
INFO - root - 2017-12-07 15:25:54.340436: step 24340, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 60h:34m:56s remains)
INFO - root - 2017-12-07 15:26:01.142978: step 24350, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 62h:33m:57s remains)
INFO - root - 2017-12-07 15:26:07.965493: step 24360, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 58h:02m:51s remains)
INFO - root - 2017-12-07 15:26:14.747896: step 24370, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 56h:31m:09s remains)
INFO - root - 2017-12-07 15:26:21.573406: step 24380, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 55h:27m:01s remains)
INFO - root - 2017-12-07 15:26:28.423357: step 24390, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 60h:00m:25s remains)
INFO - root - 2017-12-07 15:26:35.145706: step 24400, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 59h:40m:57s remains)
2017-12-07 15:26:35.870536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2489729 -4.2466073 -4.2613277 -4.2812595 -4.2994804 -4.3128467 -4.3140025 -4.3037391 -4.304956 -4.31067 -4.3104615 -4.3103485 -4.2995629 -4.281271 -4.2553759][-4.2252016 -4.2261271 -4.2444744 -4.2663021 -4.28733 -4.30306 -4.3018384 -4.2871876 -4.2887559 -4.2946339 -4.2913527 -4.2871251 -4.270668 -4.2465539 -4.2148094][-4.2340908 -4.2398705 -4.2601709 -4.2777023 -4.2920532 -4.2992864 -4.2905912 -4.2709765 -4.2712808 -4.2753568 -4.269649 -4.2660074 -4.2479849 -4.2229862 -4.1925211][-4.2672729 -4.2712522 -4.2846684 -4.2912273 -4.291182 -4.2827382 -4.26338 -4.2407432 -4.2400274 -4.240593 -4.2346077 -4.2316856 -4.2187128 -4.2015839 -4.181232][-4.2821174 -4.2794909 -4.2811871 -4.2708039 -4.248879 -4.2171445 -4.1850553 -4.162147 -4.1697993 -4.1762166 -4.1798315 -4.190124 -4.1933026 -4.1918736 -4.18566][-4.2644796 -4.2483759 -4.2332172 -4.1993942 -4.1488485 -4.0875325 -4.0322185 -4.0020733 -4.02789 -4.0592594 -4.09175 -4.1299338 -4.1581297 -4.17742 -4.1850381][-4.2403412 -4.2152829 -4.1882515 -4.136672 -4.0616107 -3.9697902 -3.882601 -3.8427505 -3.8920796 -3.9586329 -4.0253024 -4.0889359 -4.12944 -4.1564012 -4.1704912][-4.2322712 -4.214252 -4.1966963 -4.1557178 -4.0901165 -4.007555 -3.9327216 -3.8978791 -3.9406655 -4.0075941 -4.0725303 -4.1265368 -4.1527238 -4.1631413 -4.1662312][-4.2114677 -4.2045779 -4.2081103 -4.1979475 -4.1663141 -4.1220884 -4.0843363 -4.0676064 -4.0938816 -4.137815 -4.1748414 -4.1965489 -4.1933784 -4.1790466 -4.1610384][-4.1721168 -4.1688995 -4.1930847 -4.2158833 -4.2175975 -4.2066712 -4.1919856 -4.1833572 -4.1967726 -4.2184987 -4.2278962 -4.2259078 -4.2027383 -4.1732392 -4.1424413][-4.1458025 -4.146606 -4.1817169 -4.2215133 -4.245873 -4.2546458 -4.2485275 -4.2400465 -4.2438536 -4.2475085 -4.23831 -4.2246346 -4.193296 -4.1586742 -4.1264005][-4.1634145 -4.1681466 -4.203382 -4.2417064 -4.270483 -4.285841 -4.2819858 -4.2714491 -4.2666831 -4.2600284 -4.2418513 -4.2228785 -4.1906562 -4.1613474 -4.1391678][-4.2278881 -4.2349224 -4.2634711 -4.2910061 -4.31346 -4.3224387 -4.3140039 -4.3017278 -4.2957129 -4.28821 -4.2707763 -4.2536535 -4.2286749 -4.2092323 -4.1993766][-4.3116789 -4.31564 -4.3323317 -4.3467021 -4.356492 -4.3567834 -4.34336 -4.3314924 -4.3279266 -4.3269415 -4.3178205 -4.3074679 -4.292501 -4.2808142 -4.280437][-4.3623176 -4.3632984 -4.3711491 -4.3775892 -4.3810687 -4.3780918 -4.3657432 -4.3558784 -4.353919 -4.3562307 -4.3533621 -4.3485885 -4.3395171 -4.3329329 -4.3354383]]...]
INFO - root - 2017-12-07 15:26:42.586535: step 24410, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 57h:05m:16s remains)
INFO - root - 2017-12-07 15:26:49.097927: step 24420, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 60h:22m:09s remains)
INFO - root - 2017-12-07 15:26:56.041473: step 24430, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 59h:11m:27s remains)
INFO - root - 2017-12-07 15:27:02.868778: step 24440, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 55h:51m:57s remains)
INFO - root - 2017-12-07 15:27:09.751217: step 24450, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 57h:58m:42s remains)
INFO - root - 2017-12-07 15:27:16.614966: step 24460, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 60h:41m:58s remains)
INFO - root - 2017-12-07 15:27:23.486851: step 24470, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 62h:01m:22s remains)
INFO - root - 2017-12-07 15:27:30.324201: step 24480, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 56h:00m:23s remains)
INFO - root - 2017-12-07 15:27:37.186098: step 24490, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 59h:17m:10s remains)
INFO - root - 2017-12-07 15:27:43.960285: step 24500, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 60h:50m:18s remains)
2017-12-07 15:27:44.675738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2201128 -4.2182717 -4.2222776 -4.2208214 -4.2268925 -4.232574 -4.2256889 -4.2171817 -4.2070165 -4.1877818 -4.1640515 -4.1560082 -4.1514945 -4.1504836 -4.1664963][-4.2060137 -4.1940737 -4.1888728 -4.1825614 -4.1902738 -4.1971459 -4.1888013 -4.1800289 -4.1709867 -4.1527162 -4.1273928 -4.1157694 -4.107069 -4.1076941 -4.1312881][-4.2022195 -4.1848869 -4.1723285 -4.16018 -4.1681867 -4.174901 -4.1729774 -4.1752892 -4.1741939 -4.1616955 -4.1428223 -4.138144 -4.1371779 -4.1435771 -4.1670709][-4.207643 -4.18853 -4.1738105 -4.1626635 -4.1718879 -4.1782064 -4.1862216 -4.2012863 -4.2086449 -4.2034125 -4.1935306 -4.1991682 -4.2095342 -4.221828 -4.2402954][-4.2129893 -4.1963072 -4.1835046 -4.1741967 -4.1804562 -4.1779213 -4.1780005 -4.193368 -4.206254 -4.2114592 -4.2175775 -4.236845 -4.2610836 -4.278604 -4.293963][-4.2184958 -4.195869 -4.1813421 -4.1745887 -4.1768126 -4.16453 -4.1525445 -4.160758 -4.1713681 -4.1892586 -4.213562 -4.2416344 -4.2713761 -4.2916069 -4.3090115][-4.2035575 -4.1652083 -4.1488705 -4.1442661 -4.1456418 -4.1295161 -4.111712 -4.1125607 -4.1224952 -4.142405 -4.178669 -4.2198896 -4.2573237 -4.2839303 -4.3089304][-4.1729 -4.1225872 -4.10361 -4.0992579 -4.0972943 -4.0796485 -4.056705 -4.0539961 -4.0663648 -4.0843306 -4.12235 -4.1798763 -4.2302885 -4.2625556 -4.2931571][-4.1481233 -4.1050076 -4.0909896 -4.0868406 -4.0795188 -4.0570364 -4.0339103 -4.0345016 -4.0484352 -4.05739 -4.0840645 -4.1429963 -4.1962242 -4.2325454 -4.2673488][-4.1323261 -4.1091752 -4.1062131 -4.1047158 -4.1009226 -4.0873609 -4.0682006 -4.0667934 -4.0740056 -4.0645323 -4.0695395 -4.109446 -4.1555762 -4.19435 -4.2337976][-4.1265879 -4.1175208 -4.1253681 -4.1325006 -4.1417027 -4.149024 -4.1413989 -4.1312432 -4.1190324 -4.08952 -4.0677366 -4.078701 -4.1088367 -4.1448216 -4.189734][-4.1340408 -4.1283269 -4.1374955 -4.1522622 -4.178782 -4.2045903 -4.2063904 -4.191216 -4.1655421 -4.1239839 -4.0887547 -4.0789504 -4.093452 -4.1219244 -4.1647573][-4.1635303 -4.1530967 -4.1593194 -4.1738029 -4.2058272 -4.2376409 -4.2459583 -4.2349949 -4.2135086 -4.1767173 -4.1416054 -4.122479 -4.1233473 -4.1389365 -4.1723785][-4.2238555 -4.2141929 -4.2167592 -4.2256961 -4.2506313 -4.2758765 -4.2827229 -4.2738719 -4.2630091 -4.2411704 -4.2158933 -4.1990871 -4.1939206 -4.2023849 -4.2259579][-4.2846332 -4.2810974 -4.2835426 -4.2869411 -4.2993145 -4.3134036 -4.3165607 -4.3111372 -4.3040376 -4.2922387 -4.27905 -4.2723956 -4.2726021 -4.2816472 -4.2997828]]...]
INFO - root - 2017-12-07 15:27:51.401530: step 24510, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 55h:00m:12s remains)
INFO - root - 2017-12-07 15:27:58.053922: step 24520, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 55h:07m:21s remains)
INFO - root - 2017-12-07 15:28:04.895160: step 24530, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.709 sec/batch; 60h:40m:26s remains)
INFO - root - 2017-12-07 15:28:11.720006: step 24540, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.749 sec/batch; 64h:02m:42s remains)
INFO - root - 2017-12-07 15:28:18.549329: step 24550, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 58h:36m:56s remains)
INFO - root - 2017-12-07 15:28:25.337996: step 24560, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 56h:37m:12s remains)
INFO - root - 2017-12-07 15:28:32.150886: step 24570, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 59h:18m:23s remains)
INFO - root - 2017-12-07 15:28:38.928416: step 24580, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 60h:11m:07s remains)
INFO - root - 2017-12-07 15:28:45.656527: step 24590, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 59h:34m:07s remains)
INFO - root - 2017-12-07 15:28:52.518255: step 24600, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 56h:08m:17s remains)
2017-12-07 15:28:53.271967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3203616 -4.3127184 -4.3030243 -4.2950063 -4.2916455 -4.2939796 -4.2969246 -4.3010573 -4.3079643 -4.3111453 -4.3087997 -4.3048186 -4.3011703 -4.2977223 -4.2963619][-4.3148775 -4.3001885 -4.2830105 -4.2704062 -4.2667441 -4.2727876 -4.27884 -4.282393 -4.2909827 -4.3010392 -4.3046985 -4.3025079 -4.2958965 -4.2848411 -4.27582][-4.3069749 -4.2840209 -4.2560163 -4.2325659 -4.2199979 -4.2210226 -4.2263765 -4.2281809 -4.2382483 -4.2587934 -4.2756486 -4.2836971 -4.2801809 -4.2639427 -4.2462912][-4.2996492 -4.2671118 -4.2265506 -4.1862087 -4.1604466 -4.1528645 -4.151577 -4.1509442 -4.1650863 -4.1997023 -4.2320819 -4.2508717 -4.24992 -4.2307434 -4.2092485][-4.2930202 -4.2545233 -4.2036643 -4.1480813 -4.11124 -4.0924091 -4.0742269 -4.0594735 -4.07409 -4.1236072 -4.1778502 -4.2061291 -4.2078705 -4.1923132 -4.1722155][-4.2876382 -4.2474818 -4.1923051 -4.1247835 -4.0756831 -4.0402169 -3.9900162 -3.9436493 -3.953531 -4.0257468 -4.104238 -4.1417084 -4.1514616 -4.1471848 -4.1380868][-4.283361 -4.2459588 -4.19194 -4.1171303 -4.0528975 -3.9919884 -3.9018188 -3.8170333 -3.8302886 -3.933821 -4.0369382 -4.0759354 -4.0822082 -4.09269 -4.1009016][-4.2809439 -4.2496028 -4.2030306 -4.1312771 -4.0593948 -3.9784551 -3.8643434 -3.7590706 -3.7819564 -3.9049244 -4.0097208 -4.0271163 -4.0114288 -4.0269928 -4.052465][-4.2791257 -4.2552118 -4.2225122 -4.1660342 -4.1063995 -4.0386586 -3.9477096 -3.866189 -3.8820806 -3.9729037 -4.03851 -4.0170226 -3.9677892 -3.9713988 -4.0008173][-4.2793059 -4.2604837 -4.2391052 -4.2035 -4.16836 -4.1288738 -4.0765867 -4.0286946 -4.0273023 -4.0689487 -4.0886025 -4.0396667 -3.9747798 -3.9668055 -3.9848526][-4.2816191 -4.2630386 -4.2476683 -4.2284665 -4.2147117 -4.199131 -4.1742125 -4.14481 -4.1300187 -4.1396785 -4.1383319 -4.0934677 -4.0442472 -4.03062 -4.0320935][-4.2838039 -4.2626524 -4.2467442 -4.2344923 -4.233953 -4.2375875 -4.2324338 -4.2136869 -4.1935492 -4.188848 -4.1864533 -4.1656246 -4.1487284 -4.1391306 -4.1255517][-4.2868614 -4.2633371 -4.2435946 -4.2331829 -4.2395191 -4.2568569 -4.2681332 -4.2572393 -4.236445 -4.2268991 -4.2253566 -4.222527 -4.223928 -4.2152476 -4.194694][-4.2902417 -4.2634978 -4.2405009 -4.2304564 -4.2418828 -4.2678823 -4.2883339 -4.2827024 -4.2627611 -4.2543354 -4.2555637 -4.2608914 -4.2693024 -4.2649121 -4.2509637][-4.2956104 -4.2686486 -4.243577 -4.2333994 -4.2456908 -4.2717557 -4.2928047 -4.2902 -4.2766643 -4.2728 -4.274404 -4.280973 -4.2907853 -4.2921109 -4.2865438]]...]
INFO - root - 2017-12-07 15:28:59.917380: step 24610, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 57h:35m:58s remains)
INFO - root - 2017-12-07 15:29:06.485651: step 24620, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 56h:06m:55s remains)
INFO - root - 2017-12-07 15:29:13.294932: step 24630, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 56h:44m:24s remains)
INFO - root - 2017-12-07 15:29:20.062280: step 24640, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 54h:49m:55s remains)
INFO - root - 2017-12-07 15:29:26.872830: step 24650, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 60h:25m:23s remains)
INFO - root - 2017-12-07 15:29:33.653011: step 24660, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 59h:41m:53s remains)
INFO - root - 2017-12-07 15:29:40.394171: step 24670, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 58h:31m:08s remains)
INFO - root - 2017-12-07 15:29:47.135156: step 24680, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 56h:21m:07s remains)
INFO - root - 2017-12-07 15:29:54.011617: step 24690, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 58h:12m:53s remains)
INFO - root - 2017-12-07 15:30:00.807431: step 24700, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 60h:06m:36s remains)
2017-12-07 15:30:01.544489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3038096 -4.3142891 -4.3147345 -4.301126 -4.281631 -4.2582397 -4.2381563 -4.2336497 -4.2344618 -4.2302961 -4.2236094 -4.2133489 -4.2002096 -4.1911635 -4.1826611][-4.2978659 -4.303194 -4.3007979 -4.2891884 -4.2747746 -4.2561626 -4.2375178 -4.2342286 -4.2351418 -4.2324133 -4.2291074 -4.2229457 -4.2121048 -4.2034993 -4.1974669][-4.2903748 -4.288641 -4.2825456 -4.2717123 -4.2596512 -4.2413788 -4.221982 -4.2198796 -4.2222495 -4.2207065 -4.2215424 -4.2233939 -4.2184558 -4.2143092 -4.2134886][-4.270566 -4.2623391 -4.2554536 -4.2453103 -4.2329822 -4.2113681 -4.1884751 -4.189095 -4.1995339 -4.2037063 -4.2099705 -4.21842 -4.22048 -4.2201195 -4.2201891][-4.2455 -4.2325854 -4.2244992 -4.2124839 -4.1962543 -4.167552 -4.1370997 -4.1392136 -4.1612873 -4.1734753 -4.1858096 -4.1996689 -4.2062626 -4.20562 -4.2012596][-4.2198896 -4.2035379 -4.1927767 -4.1762123 -4.1498556 -4.1080971 -4.0615721 -4.0598712 -4.0964708 -4.1212535 -4.1428747 -4.1631508 -4.1729932 -4.1713371 -4.1608124][-4.1986871 -4.1819663 -4.1693406 -4.14574 -4.1056376 -4.0490232 -3.9870048 -3.9815898 -4.0327744 -4.0717864 -4.1019726 -4.1274757 -4.138339 -4.1359682 -4.1230512][-4.2107019 -4.201683 -4.1946206 -4.1745067 -4.1350975 -4.0809031 -4.0252447 -4.0240197 -4.0720444 -4.1086373 -4.1332645 -4.150599 -4.1529236 -4.1450725 -4.1307778][-4.2436013 -4.2449865 -4.2457366 -4.2342334 -4.203661 -4.1620121 -4.1234 -4.1273527 -4.1620951 -4.187007 -4.2001257 -4.206008 -4.200037 -4.1884165 -4.174818][-4.2733665 -4.2815952 -4.2861629 -4.278842 -4.256464 -4.2266545 -4.2026448 -4.2103081 -4.2337813 -4.2484369 -4.2510948 -4.2478695 -4.2376008 -4.2254024 -4.2156954][-4.2966013 -4.306334 -4.310689 -4.3043275 -4.2872958 -4.26741 -4.2553864 -4.2645206 -4.278564 -4.2851987 -4.2817087 -4.2740393 -4.2629175 -4.2524376 -4.2461843][-4.3074827 -4.3147492 -4.3186936 -4.3156748 -4.3066425 -4.2971268 -4.294147 -4.3021383 -4.3091826 -4.3105879 -4.3049917 -4.296752 -4.2880259 -4.2812266 -4.2777333][-4.3142548 -4.3197536 -4.3234539 -4.3237114 -4.3214493 -4.3186922 -4.318913 -4.3233314 -4.3255544 -4.3247318 -4.3200388 -4.3140006 -4.3084073 -4.3048744 -4.3037329][-4.3169703 -4.3205533 -4.3244338 -4.32742 -4.3291154 -4.3291841 -4.3292055 -4.3294706 -4.3284683 -4.3271432 -4.3251615 -4.3228393 -4.32084 -4.3197103 -4.31975][-4.3226113 -4.3248768 -4.326786 -4.3290911 -4.3309803 -4.33146 -4.3311362 -4.3300881 -4.3288746 -4.3286252 -4.3287544 -4.328815 -4.3289289 -4.328979 -4.3290496]]...]
INFO - root - 2017-12-07 15:30:08.336896: step 24710, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 59h:06m:25s remains)
INFO - root - 2017-12-07 15:30:14.939536: step 24720, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 56h:03m:08s remains)
INFO - root - 2017-12-07 15:30:21.561334: step 24730, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 60h:23m:49s remains)
INFO - root - 2017-12-07 15:30:28.384325: step 24740, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 60h:23m:28s remains)
INFO - root - 2017-12-07 15:30:35.178039: step 24750, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 57h:40m:07s remains)
INFO - root - 2017-12-07 15:30:41.952928: step 24760, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.657 sec/batch; 56h:08m:52s remains)
INFO - root - 2017-12-07 15:30:48.805367: step 24770, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 58h:18m:46s remains)
INFO - root - 2017-12-07 15:30:55.645370: step 24780, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 59h:55m:53s remains)
INFO - root - 2017-12-07 15:31:02.417425: step 24790, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 58h:22m:24s remains)
INFO - root - 2017-12-07 15:31:09.151397: step 24800, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 56h:41m:29s remains)
2017-12-07 15:31:09.880559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2687149 -4.2646542 -4.2518368 -4.2455187 -4.2495723 -4.254066 -4.2557864 -4.2534924 -4.2568932 -4.2563057 -4.2489023 -4.2421761 -4.2399292 -4.2376709 -4.2408328][-4.2498903 -4.2451897 -4.2317839 -4.2256336 -4.2278304 -4.2288976 -4.2247558 -4.2202296 -4.2256107 -4.2299633 -4.2315006 -4.23005 -4.2279162 -4.2211442 -4.2217975][-4.2398357 -4.2359533 -4.2243481 -4.2171226 -4.2173886 -4.2140274 -4.2040935 -4.1954727 -4.1979022 -4.2023869 -4.2126713 -4.2179151 -4.2179976 -4.2086515 -4.2065272][-4.2353983 -4.2300234 -4.2195554 -4.2115579 -4.209671 -4.2017856 -4.1875072 -4.1759305 -4.1750469 -4.1767554 -4.1876369 -4.1952009 -4.1957221 -4.1877666 -4.1888947][-4.2212873 -4.2153263 -4.2068415 -4.1976151 -4.1867948 -4.1679993 -4.1473804 -4.1372571 -4.1380615 -4.1412315 -4.1535892 -4.1598687 -4.1576133 -4.1529074 -4.1604962][-4.2010984 -4.1932473 -4.1849194 -4.1726165 -4.15035 -4.1165628 -4.0889459 -4.0841179 -4.0909424 -4.1016073 -4.1181812 -4.1197152 -4.1100321 -4.101212 -4.1128263][-4.185833 -4.1774068 -4.1678038 -4.1495528 -4.1195245 -4.0798211 -4.0499196 -4.0488791 -4.0605874 -4.076447 -4.0879855 -4.0784969 -4.0604458 -4.0453343 -4.05589][-4.1783 -4.1739173 -4.1665182 -4.141644 -4.1082692 -4.071835 -4.0451093 -4.0442452 -4.0552688 -4.0675249 -4.0715375 -4.056222 -4.0396218 -4.0229058 -4.0306468][-4.1841965 -4.1876678 -4.1839314 -4.1577454 -4.1242476 -4.0906987 -4.0679669 -4.0657377 -4.07296 -4.0790806 -4.0858436 -4.0801029 -4.0655489 -4.0466285 -4.0467224][-4.2016211 -4.2082725 -4.2073789 -4.1829915 -4.1523538 -4.1251588 -4.107563 -4.105072 -4.1051593 -4.1097965 -4.1295066 -4.1343994 -4.1144919 -4.0887566 -4.0826287][-4.201333 -4.2069416 -4.2115021 -4.1931181 -4.1671171 -4.1480036 -4.1398311 -4.1416917 -4.1389136 -4.1411023 -4.1627693 -4.1777558 -4.1583204 -4.1324792 -4.1265693][-4.1746082 -4.17867 -4.1888142 -4.1805482 -4.1620684 -4.1527905 -4.1546483 -4.1613159 -4.1571727 -4.1531281 -4.168283 -4.1901026 -4.1835113 -4.1703181 -4.1696434][-4.1560183 -4.162499 -4.1739087 -4.1693449 -4.1520443 -4.1501894 -4.1591725 -4.1686258 -4.1611323 -4.149281 -4.1535144 -4.1765394 -4.1900339 -4.1988654 -4.2082963][-4.1637554 -4.1712151 -4.1775842 -4.1690454 -4.1494017 -4.1477766 -4.1578994 -4.1733351 -4.1686873 -4.1485019 -4.1380019 -4.1552958 -4.1807213 -4.2063642 -4.2275615][-4.18344 -4.1945519 -4.1992636 -4.1885772 -4.1637883 -4.1530781 -4.1585288 -4.178494 -4.1830177 -4.162653 -4.142477 -4.1442533 -4.1645756 -4.1911349 -4.2151008]]...]
INFO - root - 2017-12-07 15:31:16.604939: step 24810, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 58h:36m:48s remains)
INFO - root - 2017-12-07 15:31:23.281809: step 24820, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 62h:41m:16s remains)
INFO - root - 2017-12-07 15:31:30.080590: step 24830, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 58h:19m:38s remains)
INFO - root - 2017-12-07 15:31:36.808583: step 24840, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 55h:44m:18s remains)
INFO - root - 2017-12-07 15:31:43.636969: step 24850, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 56h:30m:57s remains)
INFO - root - 2017-12-07 15:31:50.372711: step 24860, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 59h:13m:55s remains)
INFO - root - 2017-12-07 15:31:57.119180: step 24870, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 58h:25m:39s remains)
INFO - root - 2017-12-07 15:32:03.845066: step 24880, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:07m:26s remains)
INFO - root - 2017-12-07 15:32:10.654415: step 24890, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 55h:27m:33s remains)
INFO - root - 2017-12-07 15:32:17.526609: step 24900, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 59h:15m:57s remains)
2017-12-07 15:32:18.239156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.312254 -4.3132734 -4.308084 -4.3060651 -4.3018374 -4.2925887 -4.2808814 -4.2691469 -4.2521248 -4.2413669 -4.2351675 -4.2289562 -4.2271695 -4.2249818 -4.2220106][-4.3151793 -4.3198524 -4.3165164 -4.3156834 -4.30999 -4.3000379 -4.2903409 -4.2809496 -4.2658195 -4.2547641 -4.2503109 -4.2474165 -4.2453303 -4.2394867 -4.2353921][-4.3181992 -4.3227639 -4.3192992 -4.3181005 -4.3121858 -4.3021336 -4.2948 -4.2878337 -4.2729654 -4.2652426 -4.2683759 -4.2730522 -4.2727551 -4.2642317 -4.2575088][-4.3143778 -4.3139968 -4.3074288 -4.3031783 -4.2974644 -4.2898207 -4.2815928 -4.2713552 -4.2553568 -4.2562008 -4.2737112 -4.2900734 -4.2938137 -4.2843943 -4.2734528][-4.2929668 -4.2851877 -4.27406 -4.2661428 -4.2571912 -4.247077 -4.2342935 -4.213 -4.1915193 -4.2032933 -4.2385478 -4.2710214 -4.2846894 -4.2809906 -4.2744832][-4.2548232 -4.2444715 -4.2342663 -4.2217722 -4.2055712 -4.1846237 -4.153954 -4.1045103 -4.0669932 -4.0971289 -4.15695 -4.2059522 -4.2289314 -4.2351279 -4.2380738][-4.208456 -4.2018542 -4.198348 -4.1853738 -4.16203 -4.12557 -4.0680528 -3.9769514 -3.9129777 -3.966167 -4.0545363 -4.1210852 -4.1569595 -4.1742067 -4.1837621][-4.1793056 -4.1811233 -4.1869636 -4.1794147 -4.1541953 -4.1109672 -4.0426579 -3.9368021 -3.865766 -3.9213686 -4.0084915 -4.0711761 -4.1107736 -4.132256 -4.1443982][-4.1862965 -4.1950693 -4.2058406 -4.2029524 -4.1804323 -4.1419473 -4.08426 -4.0090566 -3.9711852 -4.0099058 -4.062933 -4.0991 -4.126092 -4.141562 -4.1503296][-4.219532 -4.2313023 -4.2405343 -4.2383122 -4.2221313 -4.1951375 -4.1555905 -4.1143661 -4.1043148 -4.1309257 -4.1567831 -4.1697588 -4.1809754 -4.1877794 -4.1914659][-4.2556953 -4.2689972 -4.2743535 -4.2716141 -4.2627077 -4.2483292 -4.227704 -4.210968 -4.2142148 -4.2341051 -4.2452278 -4.2431979 -4.2422438 -4.2408347 -4.23907][-4.2770877 -4.2919497 -4.2966986 -4.2951207 -4.2906346 -4.2845221 -4.2766728 -4.2741618 -4.2826905 -4.2962089 -4.2970271 -4.2876639 -4.2810755 -4.2762718 -4.2727523][-4.2835875 -4.2966113 -4.3016214 -4.30095 -4.2991071 -4.2975683 -4.2958145 -4.2976627 -4.3057203 -4.3123779 -4.3085895 -4.2995696 -4.2937932 -4.2906532 -4.2886739][-4.2819085 -4.2899704 -4.2936587 -4.2931218 -4.2917042 -4.29125 -4.2931156 -4.2968988 -4.3020372 -4.3032422 -4.2979655 -4.2920294 -4.2898192 -4.2908263 -4.2929125][-4.2765527 -4.2773027 -4.2775116 -4.2759466 -4.2735949 -4.2745814 -4.2808714 -4.2888279 -4.2933168 -4.2926097 -4.287744 -4.2840362 -4.2843404 -4.2883749 -4.2951131]]...]
INFO - root - 2017-12-07 15:32:24.949634: step 24910, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 57h:10m:09s remains)
INFO - root - 2017-12-07 15:32:31.497470: step 24920, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 55h:12m:15s remains)
INFO - root - 2017-12-07 15:32:38.373144: step 24930, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 58h:04m:25s remains)
INFO - root - 2017-12-07 15:32:45.219205: step 24940, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 60h:50m:10s remains)
INFO - root - 2017-12-07 15:32:52.011870: step 24950, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 57h:38m:39s remains)
INFO - root - 2017-12-07 15:32:58.745475: step 24960, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.647 sec/batch; 55h:17m:07s remains)
INFO - root - 2017-12-07 15:33:05.548372: step 24970, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 54h:47m:08s remains)
INFO - root - 2017-12-07 15:33:12.291934: step 24980, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 56h:44m:58s remains)
INFO - root - 2017-12-07 15:33:19.120944: step 24990, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 60h:36m:40s remains)
INFO - root - 2017-12-07 15:33:25.933224: step 25000, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 60h:34m:06s remains)
2017-12-07 15:33:26.621051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2888002 -4.28388 -4.2869062 -4.29444 -4.3003488 -4.2981024 -4.2975922 -4.302032 -4.3082247 -4.3088632 -4.3061624 -4.3047161 -4.2910094 -4.2482338 -4.186862][-4.2909451 -4.2821126 -4.2794557 -4.2805624 -4.2799077 -4.270083 -4.26666 -4.2703762 -4.2751775 -4.2809439 -4.2875772 -4.2894616 -4.2783103 -4.2346473 -4.1709266][-4.294764 -4.2812061 -4.2691135 -4.2577915 -4.2415004 -4.2170219 -4.20541 -4.2030568 -4.2035203 -4.2214842 -4.2491517 -4.2649117 -4.264039 -4.2306728 -4.1746006][-4.295825 -4.275229 -4.2537088 -4.2299342 -4.1954389 -4.1542358 -4.1288018 -4.111052 -4.1020427 -4.1413765 -4.2011538 -4.2390647 -4.2497306 -4.2274861 -4.1826134][-4.2942252 -4.2699342 -4.239315 -4.1985235 -4.1451664 -4.0871058 -4.0391054 -3.9905436 -3.97114 -4.0450025 -4.1469593 -4.2107296 -4.2304626 -4.2136168 -4.173912][-4.289464 -4.2621956 -4.2226624 -4.1652365 -4.0967364 -4.0231023 -3.9447465 -3.8505855 -3.8114843 -3.92734 -4.0788565 -4.1661205 -4.1930676 -4.1793308 -4.144453][-4.2739539 -4.2464886 -4.1987314 -4.1307321 -4.0557284 -3.9702711 -3.8652837 -3.7232997 -3.6586151 -3.8119378 -3.9984732 -4.1019335 -4.1403594 -4.1356091 -4.1096644][-4.2434244 -4.2212734 -4.1795859 -4.1195765 -4.0584917 -3.9889367 -3.8915973 -3.7534733 -3.6871781 -3.8253584 -3.9894228 -4.0829535 -4.1254025 -4.1319723 -4.1201282][-4.2121811 -4.1979713 -4.1724257 -4.1339974 -4.0984926 -4.0629349 -4.0037942 -3.9168253 -3.8790259 -3.9634619 -4.0665383 -4.1248507 -4.1512127 -4.1542935 -4.1483693][-4.1845202 -4.172617 -4.1631627 -4.1494169 -4.1420937 -4.1340308 -4.1068716 -4.0609341 -4.0415282 -4.08688 -4.1452031 -4.1768045 -4.1901722 -4.190722 -4.1896358][-4.170208 -4.1533475 -4.1525445 -4.16048 -4.1779022 -4.1874552 -4.1815057 -4.163528 -4.1530352 -4.1742282 -4.2033753 -4.2165351 -4.2190542 -4.2213712 -4.2252145][-4.17481 -4.1528697 -4.1533704 -4.1744113 -4.2058997 -4.22437 -4.2305446 -4.2319903 -4.2287874 -4.2392492 -4.2544842 -4.2581749 -4.2562556 -4.259367 -4.2611794][-4.2103643 -4.1834607 -4.1769023 -4.1962743 -4.2281246 -4.2485046 -4.2649655 -4.2815847 -4.2884774 -4.2955546 -4.3034606 -4.3033824 -4.2990236 -4.2959337 -4.292572][-4.2495995 -4.2219777 -4.2134118 -4.2271719 -4.2539816 -4.2741938 -4.2933784 -4.31339 -4.3256865 -4.3282166 -4.3249969 -4.3185921 -4.3139739 -4.3103404 -4.30494][-4.2732868 -4.2523713 -4.2471952 -4.2594638 -4.2826424 -4.302043 -4.3176932 -4.331181 -4.3383284 -4.3344817 -4.324594 -4.3156152 -4.3118362 -4.3096685 -4.308351]]...]
INFO - root - 2017-12-07 15:33:33.384400: step 25010, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 56h:37m:09s remains)
INFO - root - 2017-12-07 15:33:39.950092: step 25020, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:10m:00s remains)
INFO - root - 2017-12-07 15:33:46.690528: step 25030, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 60h:27m:38s remains)
INFO - root - 2017-12-07 15:33:53.218649: step 25040, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 54h:34m:13s remains)
INFO - root - 2017-12-07 15:34:00.004266: step 25050, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 55h:00m:25s remains)
INFO - root - 2017-12-07 15:34:06.886358: step 25060, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 60h:43m:46s remains)
INFO - root - 2017-12-07 15:34:13.789196: step 25070, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 62h:21m:11s remains)
INFO - root - 2017-12-07 15:34:20.617393: step 25080, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 57h:33m:32s remains)
INFO - root - 2017-12-07 15:34:27.486720: step 25090, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 56h:25m:15s remains)
INFO - root - 2017-12-07 15:34:34.327456: step 25100, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.711 sec/batch; 60h:43m:35s remains)
2017-12-07 15:34:35.078008: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0606151 -4.069746 -4.0806975 -4.0941567 -4.0968695 -4.1060114 -4.1218815 -4.1148491 -4.079855 -4.0525918 -4.0609 -4.0878434 -4.1357937 -4.1876135 -4.2377663][-4.1577935 -4.1619005 -4.164413 -4.1569762 -4.1422281 -4.1398377 -4.1550183 -4.1589274 -4.1457663 -4.1282935 -4.1342082 -4.1482663 -4.177217 -4.2158065 -4.2574034][-4.2254725 -4.2227473 -4.21589 -4.1944237 -4.1670909 -4.1502194 -4.1550832 -4.1598206 -4.1609478 -4.158967 -4.1720214 -4.1828513 -4.2036943 -4.2345133 -4.268352][-4.2163177 -4.2182302 -4.2113829 -4.181088 -4.1457958 -4.1235013 -4.1154675 -4.1025219 -4.0978165 -4.1092916 -4.1446166 -4.1728859 -4.2044263 -4.2376742 -4.2701783][-4.16041 -4.1783905 -4.1837363 -4.1569123 -4.1247 -4.102241 -4.0758471 -4.02627 -3.984791 -3.9873214 -4.0481629 -4.1128006 -4.1772389 -4.2277651 -4.2657819][-4.0945315 -4.1402545 -4.16693 -4.1544151 -4.1299086 -4.1064749 -4.0588484 -3.9662628 -3.8711319 -3.8394017 -3.9152195 -4.0178251 -4.1236463 -4.2022681 -4.2530813][-4.0521717 -4.1175461 -4.1613765 -4.1608882 -4.1390033 -4.1107631 -4.0391049 -3.9075069 -3.7723258 -3.7163599 -3.8075304 -3.9409909 -4.0756841 -4.1736622 -4.237021][-4.0059929 -4.0718441 -4.1260047 -4.143188 -4.1313663 -4.0991731 -4.0099998 -3.8525758 -3.6931195 -3.6299684 -3.7271328 -3.8725028 -4.0231948 -4.1364436 -4.2129269][-3.9886537 -4.0350432 -4.0901685 -4.1253691 -4.1301332 -4.1054387 -4.0226 -3.8677268 -3.7063913 -3.6406095 -3.7149529 -3.8397346 -3.9818718 -4.0953922 -4.1825862][-4.0322022 -4.0542421 -4.1037612 -4.1476145 -4.1612959 -4.1442266 -4.0852561 -3.9698346 -3.8451135 -3.7905781 -3.8329563 -3.9147902 -4.018918 -4.1077271 -4.1826305][-4.0904164 -4.1004353 -4.1423454 -4.1889491 -4.2059507 -4.1951723 -4.1613832 -4.0927634 -4.0122123 -3.9712467 -3.9925923 -4.0397253 -4.1054263 -4.1626263 -4.21446][-4.1399584 -4.1406946 -4.1712141 -4.2138581 -4.2323456 -4.2286015 -4.2138581 -4.185101 -4.1446838 -4.1176667 -4.1234913 -4.1428261 -4.179008 -4.2135477 -4.248179][-4.1598825 -4.1562719 -4.177825 -4.2113719 -4.2274542 -4.22637 -4.2243624 -4.2219505 -4.2060189 -4.1919923 -4.1934505 -4.1984324 -4.2179804 -4.2403502 -4.2672019][-4.1523452 -4.1452279 -4.1576729 -4.1818581 -4.1930289 -4.1928849 -4.1942325 -4.2000113 -4.1924086 -4.1857066 -4.1866608 -4.1896076 -4.2105265 -4.2350221 -4.2660012][-4.1391511 -4.123827 -4.1288462 -4.1434746 -4.1493044 -4.1511812 -4.1473546 -4.1445808 -4.1338558 -4.1324148 -4.1428275 -4.1535912 -4.1824379 -4.2151957 -4.256197]]...]
INFO - root - 2017-12-07 15:34:41.790737: step 25110, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 56h:59m:11s remains)
INFO - root - 2017-12-07 15:34:48.379678: step 25120, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 56h:04m:56s remains)
INFO - root - 2017-12-07 15:34:55.200165: step 25130, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 60h:04m:56s remains)
INFO - root - 2017-12-07 15:35:01.990749: step 25140, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.722 sec/batch; 61h:37m:36s remains)
INFO - root - 2017-12-07 15:35:08.782070: step 25150, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 57h:35m:32s remains)
INFO - root - 2017-12-07 15:35:15.580124: step 25160, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 54h:55m:02s remains)
INFO - root - 2017-12-07 15:35:22.356442: step 25170, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 55h:21m:42s remains)
INFO - root - 2017-12-07 15:35:29.218418: step 25180, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 58h:27m:14s remains)
INFO - root - 2017-12-07 15:35:36.072979: step 25190, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 61h:57m:05s remains)
INFO - root - 2017-12-07 15:35:42.938660: step 25200, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 60h:06m:57s remains)
2017-12-07 15:35:43.633777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2939506 -4.2800817 -4.2471743 -4.1996045 -4.1477408 -4.1030369 -4.0952749 -4.1262112 -4.144546 -4.1471229 -4.163197 -4.1946259 -4.2240267 -4.2352052 -4.2335949][-4.2941728 -4.2767992 -4.2367992 -4.1772828 -4.1082516 -4.0401993 -4.0252194 -4.0656509 -4.09698 -4.1092987 -4.1342783 -4.1757121 -4.2097454 -4.2261133 -4.2253318][-4.2890944 -4.2651377 -4.2196112 -4.1574893 -4.0793128 -3.99323 -3.9668133 -4.0122418 -4.0564771 -4.0739307 -4.1037459 -4.1577597 -4.2019525 -4.2192941 -4.2181377][-4.2820597 -4.252964 -4.20798 -4.1510196 -4.07404 -3.9796562 -3.9458494 -3.9904337 -4.0369406 -4.058116 -4.0958133 -4.1621127 -4.212379 -4.22496 -4.2202353][-4.27969 -4.2495489 -4.2074962 -4.1556745 -4.0807481 -3.9842045 -3.9449573 -3.988935 -4.0444531 -4.0749788 -4.1193085 -4.1892948 -4.2319603 -4.2283816 -4.2187619][-4.2776308 -4.2487459 -4.2055507 -4.149477 -4.0699511 -3.9627457 -3.9035332 -3.9495304 -4.0348334 -4.0913224 -4.1473551 -4.2087708 -4.2284307 -4.2069597 -4.1920543][-4.2696548 -4.2378273 -4.1918373 -4.1272912 -4.0320735 -3.8937881 -3.7910976 -3.8486066 -3.9874415 -4.0759039 -4.1333303 -4.1783338 -4.1806011 -4.1465278 -4.1272583][-4.2676578 -4.2338662 -4.1852078 -4.111495 -4.000886 -3.8465235 -3.7291503 -3.8088274 -3.9750147 -4.0649223 -4.1012688 -4.1229086 -4.1128125 -4.0706935 -4.0492473][-4.27742 -4.2440491 -4.1924767 -4.1163692 -4.01486 -3.8977175 -3.8271172 -3.9028606 -4.0273342 -4.082212 -4.0851564 -4.0819116 -4.0598335 -4.0137405 -3.992487][-4.2884445 -4.2575078 -4.2083945 -4.1405129 -4.0621624 -3.9877129 -3.9539089 -4.0115156 -4.0868897 -4.1043715 -4.0823274 -4.0689836 -4.0452356 -4.0064678 -3.9961891][-4.2951221 -4.2681303 -4.22692 -4.1746111 -4.11882 -4.0730042 -4.0539732 -4.0886078 -4.1249251 -4.1189036 -4.0926023 -4.0871511 -4.0789523 -4.0567355 -4.0535517][-4.2969389 -4.2730269 -4.2420111 -4.2062159 -4.1673212 -4.1388803 -4.1285868 -4.1454554 -4.15445 -4.1406393 -4.1226935 -4.124795 -4.1232324 -4.1127934 -4.1136389][-4.2989154 -4.2803092 -4.2603521 -4.237144 -4.2097759 -4.1888056 -4.18203 -4.1895242 -4.1863732 -4.1740627 -4.1622062 -4.1633134 -4.162 -4.1565442 -4.1598687][-4.30098 -4.2880039 -4.2791977 -4.2666955 -4.2485123 -4.2318015 -4.224607 -4.22754 -4.2216129 -4.2080469 -4.1975193 -4.1955495 -4.1911783 -4.1836362 -4.1833811][-4.3027282 -4.2938876 -4.2906737 -4.2851162 -4.2723827 -4.2572775 -4.24996 -4.255434 -4.2499137 -4.2373185 -4.2271533 -4.2221856 -4.2144179 -4.2041564 -4.1983943]]...]
INFO - root - 2017-12-07 15:35:50.411097: step 25210, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 61h:15m:35s remains)
INFO - root - 2017-12-07 15:35:57.018552: step 25220, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 59h:15m:49s remains)
INFO - root - 2017-12-07 15:36:03.902852: step 25230, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.707 sec/batch; 60h:21m:09s remains)
INFO - root - 2017-12-07 15:36:10.913970: step 25240, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 57h:13m:35s remains)
INFO - root - 2017-12-07 15:36:17.749954: step 25250, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 59h:33m:29s remains)
INFO - root - 2017-12-07 15:36:24.571190: step 25260, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 59h:07m:54s remains)
INFO - root - 2017-12-07 15:36:31.304496: step 25270, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 59h:30m:17s remains)
INFO - root - 2017-12-07 15:36:38.086250: step 25280, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 58h:53m:05s remains)
INFO - root - 2017-12-07 15:36:44.901024: step 25290, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.648 sec/batch; 55h:18m:10s remains)
INFO - root - 2017-12-07 15:36:51.626326: step 25300, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 59h:51m:16s remains)
2017-12-07 15:36:52.384946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2825208 -4.278729 -4.26967 -4.2570696 -4.2512445 -4.2578855 -4.2663794 -4.2679338 -4.2738938 -4.2799344 -4.2844462 -4.2868819 -4.2898045 -4.300199 -4.312448][-4.2641563 -4.2579842 -4.243022 -4.22338 -4.216908 -4.228672 -4.2426066 -4.2455821 -4.2517743 -4.2552252 -4.2544885 -4.255085 -4.2596612 -4.2745171 -4.2940025][-4.2550473 -4.2490816 -4.2292838 -4.2010136 -4.1897306 -4.2045684 -4.2228618 -4.2266955 -4.2337852 -4.2354307 -4.2311821 -4.231297 -4.2367949 -4.2542024 -4.27897][-4.2496271 -4.2408733 -4.2146411 -4.1779714 -4.1592984 -4.1727037 -4.1930103 -4.198729 -4.2101383 -4.2144279 -4.2111392 -4.2127924 -4.2196908 -4.2392588 -4.2681222][-4.2306142 -4.2156067 -4.1819243 -4.1387777 -4.1128907 -4.1191077 -4.13549 -4.1431017 -4.1626577 -4.1780438 -4.1825628 -4.189302 -4.2019305 -4.22624 -4.2595859][-4.2076535 -4.1865916 -4.1474104 -4.0975895 -4.0617504 -4.0521021 -4.0551596 -4.0631561 -4.0945578 -4.1267333 -4.1456418 -4.16201 -4.1815166 -4.2102737 -4.248271][-4.1825266 -4.1563625 -4.1094241 -4.0502958 -3.9992585 -3.9690378 -3.9541388 -3.9641492 -4.0131683 -4.064898 -4.102005 -4.1312814 -4.1602845 -4.1961112 -4.2388277][-4.1436472 -4.1149454 -4.0671897 -4.0084152 -3.9549747 -3.9160557 -3.891943 -3.9062119 -3.9660826 -4.0294709 -4.0819879 -4.1207261 -4.155231 -4.1950808 -4.2379079][-4.1147666 -4.0955138 -4.0602031 -4.0130935 -3.9707923 -3.9396489 -3.920681 -3.93994 -3.9978132 -4.0552163 -4.1045113 -4.1387987 -4.168232 -4.204638 -4.2436633][-4.1209526 -4.1141443 -4.0939608 -4.0609608 -4.0278749 -4.0017171 -3.9864824 -4.006886 -4.0572343 -4.102149 -4.1418533 -4.1693935 -4.1928072 -4.2225776 -4.2556424][-4.1439648 -4.146081 -4.139102 -4.1214838 -4.1003976 -4.0814037 -4.0695896 -4.0865703 -4.1266556 -4.1600842 -4.1881189 -4.20649 -4.2217612 -4.2440171 -4.2701912][-4.1842508 -4.1912642 -4.1930709 -4.1865945 -4.1761556 -4.1641541 -4.156045 -4.1678267 -4.1960249 -4.21902 -4.2368255 -4.246459 -4.2542477 -4.2683425 -4.286202][-4.2307539 -4.2370296 -4.2407932 -4.2401395 -4.2354341 -4.2273216 -4.2217083 -4.22826 -4.246593 -4.2622604 -4.27324 -4.2783022 -4.2820115 -4.2907524 -4.3019271][-4.2647676 -4.2681813 -4.27061 -4.2704706 -4.2672462 -4.2616258 -4.2578115 -4.2607288 -4.2716532 -4.2828679 -4.2920461 -4.297821 -4.3023386 -4.3091393 -4.3156118][-4.2851119 -4.2853971 -4.2861838 -4.2862825 -4.2852955 -4.2832389 -4.2816911 -4.283288 -4.2896948 -4.297616 -4.3047934 -4.3101206 -4.3148289 -4.3205853 -4.3247938]]...]
INFO - root - 2017-12-07 15:36:59.099586: step 25310, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 55h:47m:20s remains)
INFO - root - 2017-12-07 15:37:05.819435: step 25320, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 55h:30m:43s remains)
INFO - root - 2017-12-07 15:37:12.565574: step 25330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 56h:19m:26s remains)
INFO - root - 2017-12-07 15:37:19.505692: step 25340, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 58h:03m:39s remains)
INFO - root - 2017-12-07 15:37:25.947104: step 25350, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:50m:20s remains)
INFO - root - 2017-12-07 15:37:32.729264: step 25360, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 56h:20m:58s remains)
INFO - root - 2017-12-07 15:37:39.543561: step 25370, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 60h:01m:44s remains)
INFO - root - 2017-12-07 15:37:46.338466: step 25380, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 60h:56m:05s remains)
INFO - root - 2017-12-07 15:37:53.109818: step 25390, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 59h:41m:34s remains)
INFO - root - 2017-12-07 15:38:00.075358: step 25400, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 59h:39m:47s remains)
2017-12-07 15:38:00.777577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.975538 -4.0023384 -4.0741096 -4.1460824 -4.2052469 -4.2552996 -4.2950749 -4.3100796 -4.3003407 -4.2714124 -4.2372293 -4.2199616 -4.2330737 -4.2654557 -4.3049288][-3.943953 -3.988343 -4.0659895 -4.1345825 -4.19044 -4.2351704 -4.2655129 -4.2741847 -4.2672062 -4.2585735 -4.2540884 -4.2623243 -4.2879057 -4.3160048 -4.34019][-4.0067315 -4.0544686 -4.1215954 -4.1740589 -4.2130489 -4.2366614 -4.2426953 -4.2352891 -4.2267013 -4.235568 -4.2566528 -4.2868767 -4.3224864 -4.3483343 -4.36025][-4.1030855 -4.1441412 -4.1914935 -4.2226915 -4.2344556 -4.2254853 -4.1977782 -4.1689391 -4.1589332 -4.1856122 -4.2310963 -4.2819843 -4.3284221 -4.3565984 -4.3631158][-4.1938229 -4.2211494 -4.2432323 -4.2438521 -4.21815 -4.1673932 -4.1034317 -4.0555954 -4.048934 -4.0973964 -4.1689024 -4.2411065 -4.3018456 -4.3392849 -4.3477788][-4.2710547 -4.2797527 -4.2687664 -4.2269235 -4.1576343 -4.0691915 -3.978265 -3.9232175 -3.9337621 -4.0089645 -4.1022196 -4.1900625 -4.2602444 -4.3064227 -4.31921][-4.3301849 -4.3224945 -4.28443 -4.2094364 -4.1060262 -3.9893944 -3.8875754 -3.841819 -3.8776572 -3.9709418 -4.0705552 -4.1606274 -4.2305708 -4.273767 -4.2845716][-4.3603039 -4.3465376 -4.3020282 -4.2205911 -4.1133237 -3.9973776 -3.9112916 -3.8899317 -3.9409389 -4.0285382 -4.11148 -4.1818075 -4.2318406 -4.2548547 -4.254312][-4.3654222 -4.352766 -4.3154359 -4.25004 -4.1653657 -4.07556 -4.0179443 -4.0188503 -4.0674338 -4.1326232 -4.1858377 -4.22128 -4.2347689 -4.22468 -4.2060208][-4.3607359 -4.3534689 -4.3289952 -4.2873669 -4.2332959 -4.1757932 -4.1414366 -4.1479454 -4.1826377 -4.2177353 -4.2329364 -4.2238669 -4.1923175 -4.1471548 -4.1145768][-4.3527822 -4.3517451 -4.3422432 -4.3231368 -4.2952976 -4.2643805 -4.244976 -4.2485919 -4.2641559 -4.2666464 -4.2376518 -4.1791339 -4.100843 -4.0219545 -3.9813495][-4.3441491 -4.3474865 -4.3488417 -4.3456469 -4.3364968 -4.3235979 -4.3148026 -4.3158164 -4.3149571 -4.288013 -4.22199 -4.1210656 -4.0027161 -3.8992283 -3.8604379][-4.3325067 -4.3382473 -4.3447185 -4.3496952 -4.3503308 -4.3484077 -4.3483849 -4.3510914 -4.3417363 -4.3034554 -4.2228293 -4.1061277 -3.9732537 -3.870842 -3.8465958][-4.3231282 -4.3286562 -4.335628 -4.3427649 -4.3477359 -4.3509579 -4.355536 -4.3578515 -4.3468952 -4.3121533 -4.2430849 -4.1424017 -4.0300293 -3.9527917 -3.9449036][-4.3157368 -4.3206449 -4.3267231 -4.3322439 -4.337328 -4.3419704 -4.3477211 -4.3510733 -4.3437715 -4.3195615 -4.2733116 -4.204349 -4.1264081 -4.0764503 -4.0756106]]...]
INFO - root - 2017-12-07 15:38:07.533151: step 25410, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 59h:47m:17s remains)
INFO - root - 2017-12-07 15:38:14.301202: step 25420, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.714 sec/batch; 60h:52m:23s remains)
INFO - root - 2017-12-07 15:38:21.096442: step 25430, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 58h:13m:08s remains)
INFO - root - 2017-12-07 15:38:27.940323: step 25440, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 57h:21m:05s remains)
INFO - root - 2017-12-07 15:38:34.789403: step 25450, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 61h:16m:38s remains)
INFO - root - 2017-12-07 15:38:41.648072: step 25460, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 61h:34m:45s remains)
INFO - root - 2017-12-07 15:38:48.405732: step 25470, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 59h:30m:45s remains)
INFO - root - 2017-12-07 15:38:55.251555: step 25480, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 55h:22m:33s remains)
INFO - root - 2017-12-07 15:39:02.085970: step 25490, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 57h:39m:38s remains)
INFO - root - 2017-12-07 15:39:08.987880: step 25500, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 60h:00m:53s remains)
2017-12-07 15:39:09.764905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3087578 -4.3143735 -4.3046875 -4.26973 -4.2068558 -4.1387362 -4.0921388 -4.0945144 -4.1426053 -4.1964865 -4.2352524 -4.2588534 -4.2603097 -4.2446036 -4.2162127][-4.3125658 -4.324183 -4.3183055 -4.2786293 -4.2062836 -4.124289 -4.065618 -4.0641966 -4.1190367 -4.1821079 -4.2262545 -4.2480793 -4.2480149 -4.2330532 -4.2095251][-4.3117294 -4.3277979 -4.3245411 -4.2827058 -4.2057209 -4.1142449 -4.0452614 -4.035543 -4.0918794 -4.1681643 -4.2186155 -4.2379642 -4.2322989 -4.2145181 -4.1930361][-4.3070297 -4.323875 -4.3202848 -4.2771897 -4.2034335 -4.1109843 -4.0401344 -4.0278869 -4.0858784 -4.1675477 -4.2183661 -4.2320428 -4.21949 -4.1971822 -4.1796789][-4.2998734 -4.3144279 -4.3092146 -4.2655816 -4.1967192 -4.1063552 -4.0365682 -4.028955 -4.0915537 -4.1747313 -4.2171803 -4.2254505 -4.2103362 -4.1864462 -4.1724648][-4.2938213 -4.3060155 -4.3004751 -4.2561107 -4.1909127 -4.1054397 -4.0375209 -4.0307608 -4.0966744 -4.1750393 -4.2062325 -4.2039213 -4.1868949 -4.1685452 -4.1615624][-4.2897472 -4.30081 -4.2981992 -4.25687 -4.1944394 -4.1120825 -4.0420418 -4.0336962 -4.0957341 -4.1642361 -4.1869235 -4.1762834 -4.1572828 -4.145752 -4.1475163][-4.2857251 -4.2961855 -4.2960362 -4.2567987 -4.1953349 -4.1101313 -4.033637 -4.0211067 -4.0779696 -4.1458726 -4.1735916 -4.1664124 -4.1453352 -4.1349993 -4.1408973][-4.2820392 -4.2942686 -4.2953153 -4.2557735 -4.1964989 -4.1160531 -4.0386119 -4.0178604 -4.0651603 -4.1362405 -4.1746697 -4.1739054 -4.1569581 -4.1487622 -4.1520243][-4.2809486 -4.29546 -4.2983284 -4.2600446 -4.209599 -4.1440129 -4.0813031 -4.0618782 -4.0982862 -4.1592054 -4.19228 -4.1926117 -4.1808453 -4.1711836 -4.1667972][-4.2830458 -4.299108 -4.3044438 -4.2696176 -4.2246256 -4.1709423 -4.1229744 -4.1059842 -4.13398 -4.1824675 -4.2085257 -4.2109261 -4.2040834 -4.1930189 -4.1842146][-4.2846203 -4.3026218 -4.3091412 -4.2768264 -4.2334213 -4.1832361 -4.1402779 -4.12324 -4.1474767 -4.1958337 -4.2234225 -4.2306957 -4.224895 -4.209497 -4.1966629][-4.2813754 -4.2989144 -4.30482 -4.2737269 -4.23155 -4.185153 -4.1435609 -4.1186018 -4.1384463 -4.1901822 -4.230186 -4.2446427 -4.2400475 -4.2246757 -4.2111182][-4.2757421 -4.288383 -4.2915606 -4.2622089 -4.2228127 -4.1840758 -4.1525064 -4.1237483 -4.134161 -4.1821213 -4.2223287 -4.2362533 -4.2336988 -4.2259421 -4.2178121][-4.2701831 -4.2761736 -4.2761683 -4.2483869 -4.216917 -4.1939573 -4.1741447 -4.145843 -4.1445336 -4.1791654 -4.208035 -4.2175756 -4.2182097 -4.2200546 -4.2204418]]...]
INFO - root - 2017-12-07 15:39:16.481929: step 25510, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 54h:56m:07s remains)
INFO - root - 2017-12-07 15:39:23.188421: step 25520, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 57h:14m:15s remains)
INFO - root - 2017-12-07 15:39:30.136387: step 25530, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 62h:21m:19s remains)
INFO - root - 2017-12-07 15:39:37.032359: step 25540, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 61h:57m:04s remains)
INFO - root - 2017-12-07 15:39:43.888203: step 25550, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 55h:34m:20s remains)
INFO - root - 2017-12-07 15:39:50.739551: step 25560, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 56h:58m:04s remains)
INFO - root - 2017-12-07 15:39:57.522897: step 25570, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 58h:45m:00s remains)
INFO - root - 2017-12-07 15:40:04.402042: step 25580, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 61h:40m:31s remains)
INFO - root - 2017-12-07 15:40:11.318550: step 25590, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 57h:56m:58s remains)
INFO - root - 2017-12-07 15:40:18.058658: step 25600, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 54h:21m:33s remains)
2017-12-07 15:40:18.764195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2083826 -4.1867261 -4.1720715 -4.1734772 -4.183013 -4.1976671 -4.1789761 -4.1299691 -4.0903778 -4.0960989 -4.1228337 -4.151691 -4.1703906 -4.1670828 -4.173079][-4.2065196 -4.1854205 -4.1677113 -4.1673245 -4.1748319 -4.1873679 -4.1650124 -4.1137681 -4.0734506 -4.0837169 -4.1101255 -4.1355429 -4.1557641 -4.1566725 -4.1629715][-4.2172642 -4.2027969 -4.1846552 -4.1812363 -4.1840925 -4.1860728 -4.15778 -4.1065078 -4.0745678 -4.0963283 -4.1256351 -4.1457124 -4.1606827 -4.164537 -4.16548][-4.2180238 -4.2137785 -4.2017064 -4.1994829 -4.1971383 -4.186954 -4.1523862 -4.0975761 -4.0747085 -4.1057396 -4.1346531 -4.1493063 -4.16668 -4.1771021 -4.1729531][-4.206553 -4.2133727 -4.2115445 -4.2115016 -4.2049117 -4.1815085 -4.1349792 -4.0699849 -4.0495644 -4.0883241 -4.1258316 -4.1464 -4.1721492 -4.189065 -4.183876][-4.1717992 -4.1878386 -4.2008662 -4.2108812 -4.2040033 -4.1691847 -4.1029997 -4.0174828 -3.9995115 -4.0609479 -4.1225057 -4.1569843 -4.1889024 -4.2012744 -4.1885066][-4.1411262 -4.161551 -4.1845164 -4.2051587 -4.2011437 -4.1545625 -4.0627322 -3.9440112 -3.9235172 -4.0227776 -4.1236148 -4.1811862 -4.2208285 -4.2286458 -4.202529][-4.1470718 -4.1711621 -4.1974282 -4.2194529 -4.2121067 -4.1514173 -4.0361133 -3.8879514 -3.8589635 -3.985723 -4.1166768 -4.1908183 -4.2406492 -4.254076 -4.2270041][-4.1655846 -4.1950636 -4.2250004 -4.2455535 -4.2383142 -4.1758285 -4.0589089 -3.9213943 -3.8911765 -4.0058212 -4.130229 -4.2010913 -4.2479396 -4.2655354 -4.2451296][-4.1852551 -4.2161655 -4.2460933 -4.2670231 -4.2659783 -4.2124295 -4.1086287 -3.9968233 -3.9753242 -4.0689864 -4.1725969 -4.2290182 -4.2635732 -4.2749324 -4.2590604][-4.2142224 -4.23742 -4.2629213 -4.2854404 -4.29116 -4.2483015 -4.1584682 -4.0664496 -4.0471525 -4.1213484 -4.2012777 -4.2452931 -4.2740173 -4.284874 -4.2739844][-4.2500768 -4.2649713 -4.2852845 -4.3049784 -4.3100247 -4.27575 -4.2047563 -4.131875 -4.1120391 -4.1615777 -4.2184124 -4.2541704 -4.2797656 -4.2906208 -4.2856913][-4.2846203 -4.2921352 -4.3029637 -4.3127475 -4.312212 -4.2868567 -4.2386947 -4.1869593 -4.1693659 -4.1970043 -4.23403 -4.2581921 -4.275496 -4.2838197 -4.2832975][-4.3018618 -4.3033972 -4.3055358 -4.3076615 -4.3033423 -4.28762 -4.2589893 -4.2282639 -4.2150545 -4.2272835 -4.246552 -4.2573323 -4.2673359 -4.2771683 -4.2823477][-4.3056622 -4.3025346 -4.2992725 -4.2964821 -4.2904882 -4.2817802 -4.2665486 -4.2510982 -4.2438374 -4.2484655 -4.2556062 -4.2592263 -4.2675076 -4.2800474 -4.2886734]]...]
INFO - root - 2017-12-07 15:40:25.572858: step 25610, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 57h:25m:57s remains)
INFO - root - 2017-12-07 15:40:32.185832: step 25620, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 62h:17m:08s remains)
INFO - root - 2017-12-07 15:40:39.066878: step 25630, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.720 sec/batch; 61h:24m:13s remains)
INFO - root - 2017-12-07 15:40:46.031012: step 25640, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 58h:48m:54s remains)
INFO - root - 2017-12-07 15:40:52.920646: step 25650, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 61h:47m:53s remains)
INFO - root - 2017-12-07 15:40:59.497063: step 25660, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 56h:25m:38s remains)
INFO - root - 2017-12-07 15:41:06.265230: step 25670, loss = 2.10, batch loss = 2.05 (12.6 examples/sec; 0.634 sec/batch; 54h:03m:57s remains)
INFO - root - 2017-12-07 15:41:13.051716: step 25680, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 58h:33m:34s remains)
INFO - root - 2017-12-07 15:41:19.788874: step 25690, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 58h:42m:18s remains)
INFO - root - 2017-12-07 15:41:26.539726: step 25700, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 55h:29m:27s remains)
2017-12-07 15:41:27.188792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3131256 -4.2960367 -4.28181 -4.2525158 -4.2214217 -4.2143526 -4.2183471 -4.2224712 -4.2169752 -4.20732 -4.1959009 -4.1861582 -4.1860142 -4.2003288 -4.2272272][-4.308569 -4.2905917 -4.2770133 -4.2477417 -4.2189512 -4.2141118 -4.219811 -4.2254686 -4.22015 -4.2073016 -4.1931534 -4.1827817 -4.18608 -4.2060704 -4.2354288][-4.3079963 -4.2906723 -4.2777219 -4.2487197 -4.220109 -4.2136922 -4.2191095 -4.22804 -4.22551 -4.212595 -4.2028456 -4.1983132 -4.2075362 -4.229393 -4.2522674][-4.3069329 -4.2897668 -4.2768908 -4.248991 -4.2181273 -4.2070141 -4.2106175 -4.2231174 -4.2274141 -4.221179 -4.22163 -4.2253385 -4.2377191 -4.2540622 -4.2640963][-4.3048859 -4.2869043 -4.2729764 -4.2452264 -4.2111421 -4.1881084 -4.1827235 -4.1963029 -4.2107949 -4.2183328 -4.2329631 -4.2455678 -4.2592731 -4.2676578 -4.2644453][-4.3023739 -4.2830853 -4.2649331 -4.2333741 -4.1911583 -4.1471367 -4.1229157 -4.1373062 -4.1655879 -4.1907825 -4.2212267 -4.2442942 -4.2612729 -4.263926 -4.2514853][-4.3001194 -4.278615 -4.2538261 -4.2151484 -4.1601152 -4.087266 -4.0332608 -4.0506291 -4.1010766 -4.1467385 -4.1938043 -4.227397 -4.2460518 -4.2450771 -4.2255654][-4.3010979 -4.2770553 -4.2459669 -4.2003188 -4.1308169 -4.0248747 -3.9330988 -3.9585307 -4.0400195 -4.1083374 -4.1706133 -4.2117448 -4.2289553 -4.2195339 -4.1887016][-4.3062363 -4.281601 -4.2483706 -4.2011275 -4.1226816 -3.9949343 -3.8769121 -3.9135442 -4.0173078 -4.0990825 -4.1653347 -4.2097464 -4.2220573 -4.2050714 -4.1657991][-4.3108015 -4.286746 -4.2550912 -4.2122235 -4.1405029 -4.0257254 -3.9256835 -3.9616489 -4.0533266 -4.123589 -4.177156 -4.2167435 -4.2222977 -4.2010837 -4.1587982][-4.3129439 -4.2897315 -4.2616496 -4.2258358 -4.17057 -4.0909677 -4.02873 -4.0559959 -4.1196346 -4.1661134 -4.198451 -4.2259569 -4.2245512 -4.202116 -4.1631021][-4.3149056 -4.2936192 -4.2694883 -4.2381392 -4.1986589 -4.149888 -4.1163363 -4.1356397 -4.177176 -4.2036996 -4.2198005 -4.237071 -4.2331491 -4.2093606 -4.1737337][-4.3164587 -4.2998734 -4.2801132 -4.2495208 -4.2162938 -4.1875095 -4.1702466 -4.1806827 -4.2042975 -4.2200632 -4.2323 -4.2481642 -4.2455859 -4.22173 -4.1855531][-4.3167048 -4.3037219 -4.2869487 -4.2558413 -4.2230921 -4.2033987 -4.1929603 -4.1955843 -4.2067332 -4.219275 -4.2366004 -4.255682 -4.2578244 -4.2383242 -4.2052531][-4.3163824 -4.3029275 -4.2854271 -4.2560096 -4.2233973 -4.2057061 -4.1957803 -4.1950817 -4.2000513 -4.2125974 -4.2351646 -4.2560291 -4.2627177 -4.251555 -4.2279892]]...]
INFO - root - 2017-12-07 15:41:33.964708: step 25710, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.690 sec/batch; 58h:48m:42s remains)
INFO - root - 2017-12-07 15:41:40.779169: step 25720, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 62h:03m:22s remains)
INFO - root - 2017-12-07 15:41:47.558090: step 25730, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 59h:28m:33s remains)
INFO - root - 2017-12-07 15:41:54.331723: step 25740, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 54h:48m:13s remains)
INFO - root - 2017-12-07 15:42:01.131509: step 25750, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.632 sec/batch; 53h:53m:14s remains)
INFO - root - 2017-12-07 15:42:07.984003: step 25760, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 58h:22m:57s remains)
INFO - root - 2017-12-07 15:42:14.800666: step 25770, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 61h:24m:55s remains)
INFO - root - 2017-12-07 15:42:21.620852: step 25780, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 57h:27m:14s remains)
INFO - root - 2017-12-07 15:42:28.466895: step 25790, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 55h:59m:02s remains)
INFO - root - 2017-12-07 15:42:35.303696: step 25800, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 61h:01m:03s remains)
2017-12-07 15:42:36.040255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2361727 -4.1984229 -4.164155 -4.1423984 -4.1509027 -4.185061 -4.2131815 -4.2181659 -4.2039137 -4.1844864 -4.174233 -4.1676874 -4.1559916 -4.142426 -4.1313481][-4.2233796 -4.1770754 -4.1341658 -4.1092167 -4.1219082 -4.1570215 -4.1821384 -4.1834884 -4.1704488 -4.1581922 -4.1556497 -4.1540384 -4.13764 -4.1122723 -4.0894647][-4.2109365 -4.15659 -4.1086392 -4.0889854 -4.1102366 -4.1449623 -4.16342 -4.1563082 -4.1409287 -4.1371317 -4.1465569 -4.1533465 -4.1350155 -4.0973086 -4.0605726][-4.1824036 -4.1304235 -4.0930758 -4.0891848 -4.11689 -4.1421485 -4.14236 -4.1183295 -4.0943408 -4.0967746 -4.119803 -4.1396542 -4.1227484 -4.0808544 -4.0414972][-4.1538429 -4.1221085 -4.1021948 -4.1067662 -4.1269164 -4.1347046 -4.1108103 -4.0611491 -4.0249243 -4.0367727 -4.0748391 -4.10789 -4.100368 -4.0656214 -4.0406804][-4.1454396 -4.1351037 -4.127089 -4.128932 -4.136539 -4.1239414 -4.0687647 -3.980895 -3.9269145 -3.9607222 -4.0233469 -4.0726061 -4.0846171 -4.0734739 -4.0744729][-4.1459022 -4.15032 -4.1469579 -4.1423216 -4.1337328 -4.0976276 -4.0116553 -3.8918936 -3.8477573 -3.9263508 -4.0155492 -4.0790477 -4.1098313 -4.1176434 -4.1254849][-4.1338887 -4.1484976 -4.1521816 -4.14823 -4.1306615 -4.0877752 -4.0055647 -3.9129834 -3.9057727 -3.988204 -4.0671334 -4.1235328 -4.1556811 -4.1637859 -4.1610489][-4.1335316 -4.155601 -4.1671886 -4.1679111 -4.1507463 -4.11474 -4.0547085 -4.0037632 -4.0149775 -4.069406 -4.1210222 -4.1582232 -4.17949 -4.1814241 -4.1672115][-4.1578379 -4.1769128 -4.1831627 -4.18008 -4.16311 -4.135272 -4.0942631 -4.0711732 -4.0918074 -4.1288147 -4.1576691 -4.1733351 -4.1845708 -4.1821308 -4.1574721][-4.174767 -4.1839304 -4.1784992 -4.1666164 -4.1497602 -4.1313548 -4.1114807 -4.1147432 -4.1460652 -4.1774445 -4.1926355 -4.1959848 -4.1978569 -4.1870646 -4.1547756][-4.1897068 -4.1902938 -4.1773968 -4.1623287 -4.1486349 -4.1396894 -4.1384597 -4.1592979 -4.1934738 -4.2217717 -4.2310681 -4.231534 -4.2296696 -4.2135816 -4.1834922][-4.2144775 -4.2098279 -4.1979151 -4.1869874 -4.1771135 -4.1719165 -4.1800256 -4.2081118 -4.23993 -4.2600675 -4.264421 -4.2663569 -4.2644138 -4.249752 -4.2251244][-4.2477727 -4.2427664 -4.2357316 -4.2327771 -4.2290215 -4.2239914 -4.2295361 -4.2524128 -4.2760582 -4.2866025 -4.2869711 -4.287353 -4.28435 -4.2732754 -4.2549567][-4.2812338 -4.2785616 -4.2761526 -4.2784324 -4.2812777 -4.2794981 -4.2797713 -4.2895441 -4.2993231 -4.30164 -4.3000689 -4.2985463 -4.2940464 -4.2861819 -4.2754393]]...]
INFO - root - 2017-12-07 15:42:42.844691: step 25810, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 57h:22m:43s remains)
INFO - root - 2017-12-07 15:42:49.439016: step 25820, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 54h:36m:36s remains)
INFO - root - 2017-12-07 15:42:56.279172: step 25830, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 59h:58m:34s remains)
INFO - root - 2017-12-07 15:43:03.179756: step 25840, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 59h:37m:36s remains)
INFO - root - 2017-12-07 15:43:09.933667: step 25850, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.683 sec/batch; 58h:08m:51s remains)
INFO - root - 2017-12-07 15:43:16.715411: step 25860, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 57h:14m:38s remains)
INFO - root - 2017-12-07 15:43:23.497094: step 25870, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 54h:05m:16s remains)
INFO - root - 2017-12-07 15:43:30.226565: step 25880, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 60h:23m:04s remains)
INFO - root - 2017-12-07 15:43:37.014191: step 25890, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 58h:15m:06s remains)
INFO - root - 2017-12-07 15:43:43.764003: step 25900, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 56h:57m:42s remains)
2017-12-07 15:43:44.465905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.335309 -4.3286672 -4.3198862 -4.3154187 -4.3156185 -4.3181934 -4.3200264 -4.3207989 -4.3226385 -4.3243146 -4.3276482 -4.3320866 -4.3377147 -4.3435526 -4.348803][-4.3054981 -4.2916241 -4.2791018 -4.2728033 -4.2729273 -4.2783785 -4.2827415 -4.2848678 -4.2880921 -4.2900138 -4.2963481 -4.3026781 -4.31187 -4.3225312 -4.3321838][-4.2828007 -4.2609057 -4.2424679 -4.2293735 -4.2232795 -4.2276154 -4.2328067 -4.2364969 -4.2409968 -4.2452526 -4.2556925 -4.2624741 -4.2750421 -4.2913 -4.3069248][-4.2738767 -4.2463255 -4.2222943 -4.1989789 -4.1827259 -4.18433 -4.1867151 -4.1882339 -4.1896257 -4.1916647 -4.2028556 -4.209517 -4.2265625 -4.248426 -4.27169][-4.2745347 -4.2490392 -4.2234654 -4.1924319 -4.1699934 -4.1666279 -4.1584463 -4.147645 -4.1358004 -4.1268196 -4.1334453 -4.1379776 -4.1578288 -4.185503 -4.2212505][-4.2784848 -4.2572179 -4.230978 -4.1956263 -4.166194 -4.1487637 -4.1211386 -4.092123 -4.0624614 -4.0435848 -4.0462885 -4.0481567 -4.07308 -4.1121712 -4.164567][-4.2739258 -4.2545128 -4.2262244 -4.1845427 -4.1434646 -4.1072483 -4.0596561 -4.0158644 -3.9707937 -3.9503922 -3.9572589 -3.9639583 -4.0002375 -4.0544329 -4.1233454][-4.24744 -4.2218533 -4.1860194 -4.1343575 -4.0782585 -4.0228348 -3.9615512 -3.9104373 -3.8621056 -3.8479209 -3.8649204 -3.886857 -3.9447324 -4.0200076 -4.1059051][-4.2036295 -4.1652985 -4.1178784 -4.0563121 -3.9901783 -3.9253182 -3.8650784 -3.823679 -3.7916315 -3.7942414 -3.8267989 -3.8676908 -3.9427347 -4.0299716 -4.1210794][-4.1733871 -4.1265812 -4.0757151 -4.0195503 -3.9638271 -3.9140227 -3.8767195 -3.8582151 -3.8480718 -3.8655467 -3.9032702 -3.9497547 -4.0215487 -4.0986328 -4.1755571][-4.200891 -4.16465 -4.1297913 -4.0943079 -4.0612097 -4.034379 -4.0179958 -4.0120134 -4.0106096 -4.0285087 -4.0593224 -4.0978131 -4.149797 -4.2026277 -4.2534423][-4.2646117 -4.2493958 -4.235445 -4.2189097 -4.2026987 -4.1902237 -4.182169 -4.1784525 -4.1769729 -4.1887569 -4.2098951 -4.2351232 -4.2654405 -4.2933526 -4.319592][-4.3163514 -4.3130207 -4.3098292 -4.3034248 -4.2971811 -4.2917924 -4.2874289 -4.2852087 -4.2850285 -4.2927585 -4.305006 -4.3182306 -4.331851 -4.3430014 -4.3536615][-4.350131 -4.349319 -4.3481159 -4.3451982 -4.3429151 -4.3410964 -4.3397112 -4.3390532 -4.339674 -4.3438659 -4.3483543 -4.352334 -4.3571591 -4.36117 -4.3659105][-4.3662157 -4.3656988 -4.3655887 -4.3655229 -4.3663874 -4.36664 -4.3665524 -4.3659172 -4.365293 -4.3660932 -4.3666143 -4.3664384 -4.3676505 -4.3695126 -4.3716712]]...]
INFO - root - 2017-12-07 15:43:51.165221: step 25910, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 58h:45m:58s remains)
INFO - root - 2017-12-07 15:43:57.793413: step 25920, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 60h:30m:30s remains)
INFO - root - 2017-12-07 15:44:04.615332: step 25930, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 56h:13m:51s remains)
INFO - root - 2017-12-07 15:44:11.320860: step 25940, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 54h:43m:08s remains)
INFO - root - 2017-12-07 15:44:18.266358: step 25950, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 60h:29m:30s remains)
INFO - root - 2017-12-07 15:44:25.030277: step 25960, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 57h:16m:52s remains)
INFO - root - 2017-12-07 15:44:31.659658: step 25970, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 58h:08m:26s remains)
INFO - root - 2017-12-07 15:44:38.459348: step 25980, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 56h:20m:09s remains)
INFO - root - 2017-12-07 15:44:45.295680: step 25990, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 55h:45m:20s remains)
INFO - root - 2017-12-07 15:44:52.191378: step 26000, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 60h:10m:00s remains)
2017-12-07 15:44:52.958471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3015676 -4.2837796 -4.2360291 -4.17745 -4.1357975 -4.1048818 -4.0928531 -4.1129947 -4.1523461 -4.1986737 -4.2340884 -4.2519145 -4.2543669 -4.2474132 -4.2366166][-4.295053 -4.2724781 -4.2204442 -4.1589813 -4.1145406 -4.0846062 -4.0784688 -4.1071033 -4.1509705 -4.19868 -4.2352505 -4.2557845 -4.2565303 -4.2430997 -4.2284384][-4.2995381 -4.2813382 -4.233911 -4.1758018 -4.1274548 -4.0919685 -4.0835371 -4.1094313 -4.1539063 -4.2025504 -4.2402682 -4.2607069 -4.2573948 -4.2359672 -4.2144456][-4.3096228 -4.2961831 -4.2559366 -4.2062016 -4.1573534 -4.1181345 -4.11036 -4.1282907 -4.1600251 -4.20147 -4.2358489 -4.2524648 -4.2465396 -4.2207479 -4.1948247][-4.318428 -4.3099356 -4.2766047 -4.2346435 -4.185904 -4.1426387 -4.1282878 -4.1288495 -4.1393666 -4.1718903 -4.2064953 -4.2246747 -4.2242355 -4.2000418 -4.1691909][-4.3197107 -4.3131566 -4.2830567 -4.2420897 -4.1883774 -4.1336317 -4.0995779 -4.0725827 -4.0589671 -4.0954137 -4.1471691 -4.1842017 -4.201592 -4.1823788 -4.1427975][-4.3108926 -4.3003349 -4.2663388 -4.2207146 -4.1594195 -4.0888643 -4.0264945 -3.961627 -3.9247704 -3.9773469 -4.0625205 -4.1346688 -4.1771455 -4.1655154 -4.1184716][-4.2994366 -4.2824368 -4.2458782 -4.1987338 -4.1354795 -4.0580444 -3.9800634 -3.8908281 -3.8374624 -3.90044 -4.0117993 -4.1077361 -4.1647491 -4.162364 -4.1189055][-4.2965479 -4.2802186 -4.250514 -4.2107539 -4.156086 -4.0868583 -4.017755 -3.934921 -3.8799214 -3.9315581 -4.0350075 -4.1282597 -4.1849666 -4.19041 -4.1592464][-4.3008704 -4.2896113 -4.2710547 -4.2398176 -4.1934152 -4.1413927 -4.0963955 -4.0427966 -4.0028114 -4.0322742 -4.106463 -4.1767592 -4.2208586 -4.2288408 -4.2065749][-4.3154736 -4.3136134 -4.3067527 -4.2841139 -4.2453241 -4.2075043 -4.1814556 -4.1523433 -4.1268892 -4.1433077 -4.1908851 -4.2365236 -4.2674928 -4.27824 -4.2624893][-4.3284912 -4.3346357 -4.337399 -4.3245525 -4.2940197 -4.2668304 -4.2521963 -4.2385235 -4.22695 -4.2372475 -4.2673869 -4.2965021 -4.3186421 -4.3296165 -4.3184032][-4.3309188 -4.3397379 -4.3463635 -4.34121 -4.3224144 -4.3072257 -4.3019834 -4.2987037 -4.2962613 -4.3004379 -4.3175654 -4.3346953 -4.347599 -4.3550177 -4.3489437][-4.3274684 -4.3338804 -4.3392773 -4.3377786 -4.327734 -4.3202033 -4.3195448 -4.32128 -4.3229542 -4.3248935 -4.3325734 -4.3404865 -4.3459988 -4.3502054 -4.3479342][-4.32386 -4.326561 -4.3287292 -4.3283415 -4.3233023 -4.3198729 -4.3208389 -4.323854 -4.3267093 -4.3274374 -4.3293333 -4.3313904 -4.3330684 -4.334887 -4.3340917]]...]
INFO - root - 2017-12-07 15:44:59.632198: step 26010, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 56h:07m:01s remains)
INFO - root - 2017-12-07 15:45:06.271251: step 26020, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 56h:15m:51s remains)
INFO - root - 2017-12-07 15:45:13.083107: step 26030, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 58h:29m:47s remains)
INFO - root - 2017-12-07 15:45:19.793250: step 26040, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 57h:19m:39s remains)
INFO - root - 2017-12-07 15:45:26.571003: step 26050, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 57h:16m:35s remains)
INFO - root - 2017-12-07 15:45:33.434572: step 26060, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 56h:45m:52s remains)
INFO - root - 2017-12-07 15:45:40.303709: step 26070, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 60h:09m:28s remains)
INFO - root - 2017-12-07 15:45:47.153694: step 26080, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.720 sec/batch; 61h:19m:31s remains)
INFO - root - 2017-12-07 15:45:53.897828: step 26090, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 58h:36m:47s remains)
INFO - root - 2017-12-07 15:46:00.688585: step 26100, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 56h:10m:02s remains)
2017-12-07 15:46:01.475269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1692953 -4.1832314 -4.1802826 -4.1807132 -4.196394 -4.2211146 -4.2360234 -4.2304692 -4.2040086 -4.1796675 -4.168375 -4.1597829 -4.1596823 -4.1583033 -4.1634064][-4.1308 -4.1491404 -4.1491132 -4.1563849 -4.184783 -4.2187033 -4.2373571 -4.2256675 -4.1918883 -4.162044 -4.14614 -4.135252 -4.1380978 -4.1363549 -4.1327834][-4.1124964 -4.1343241 -4.1368351 -4.1498389 -4.1819706 -4.2143598 -4.2306828 -4.2153778 -4.1816964 -4.1552243 -4.1390395 -4.1244483 -4.12386 -4.1137438 -4.1038074][-4.1234307 -4.1428809 -4.1476455 -4.1638379 -4.1892958 -4.210681 -4.2209287 -4.2042589 -4.1765718 -4.1551318 -4.135807 -4.1238742 -4.12065 -4.1102548 -4.1035023][-4.1514678 -4.1593142 -4.1620564 -4.1832671 -4.2040133 -4.210566 -4.2103715 -4.1901941 -4.1722131 -4.1603446 -4.1415877 -4.1322103 -4.1297278 -4.1226153 -4.1219549][-4.1680217 -4.1618977 -4.1584787 -4.1799903 -4.1928716 -4.1882629 -4.1842227 -4.162951 -4.1525335 -4.15471 -4.1459737 -4.1466208 -4.147119 -4.140059 -4.1394887][-4.1596174 -4.1415658 -4.1282558 -4.1392198 -4.13974 -4.1326585 -4.1350164 -4.1154881 -4.1047273 -4.1208134 -4.1267872 -4.142427 -4.1490421 -4.1444378 -4.1436977][-4.1411881 -4.1148629 -4.0917354 -4.0956955 -4.0968285 -4.0941072 -4.1010866 -4.0798893 -4.0660591 -4.0836253 -4.0925651 -4.1143122 -4.1310825 -4.1335111 -4.1384358][-4.1283908 -4.1058326 -4.0837131 -4.0790806 -4.0812335 -4.0842733 -4.1005459 -4.0910535 -4.0820436 -4.0937595 -4.0933495 -4.1014004 -4.1137986 -4.119452 -4.1257734][-4.079926 -4.0763855 -4.0671129 -4.0644851 -4.07157 -4.0874252 -4.1152821 -4.1190367 -4.1161222 -4.1173849 -4.1023831 -4.0944195 -4.0977612 -4.1015358 -4.1034822][-4.0162516 -4.0288196 -4.0338283 -4.0382419 -4.0488563 -4.0741291 -4.1095185 -4.1198759 -4.1204295 -4.1163816 -4.0907559 -4.0740852 -4.0717955 -4.0722923 -4.0685554][-3.991653 -4.0076208 -4.0192127 -4.0221934 -4.0284405 -4.0521445 -4.0855074 -4.0928173 -4.092576 -4.0865545 -4.0608873 -4.041955 -4.0382023 -4.0369015 -4.0343971][-4.0273161 -4.0405974 -4.0463815 -4.0360832 -4.0284824 -4.0410919 -4.061974 -4.06428 -4.0638771 -4.0612564 -4.0454307 -4.0296087 -4.0221467 -4.0205984 -4.0276446][-4.0910621 -4.0879235 -4.0758305 -4.0563631 -4.0491652 -4.059432 -4.0747485 -4.0798507 -4.0855732 -4.0907154 -4.086781 -4.0777869 -4.0707684 -4.0704412 -4.0832691][-4.1737571 -4.1582828 -4.1392 -4.1265512 -4.1277447 -4.1370363 -4.1501365 -4.1569424 -4.162838 -4.1706944 -4.1721811 -4.1662345 -4.1602612 -4.1608033 -4.1748829]]...]
INFO - root - 2017-12-07 15:46:08.141303: step 26110, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 56h:52m:21s remains)
INFO - root - 2017-12-07 15:46:14.867755: step 26120, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 61h:36m:57s remains)
INFO - root - 2017-12-07 15:46:21.596954: step 26130, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 55h:35m:48s remains)
INFO - root - 2017-12-07 15:46:28.312860: step 26140, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 55h:13m:26s remains)
INFO - root - 2017-12-07 15:46:35.169063: step 26150, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 60h:29m:31s remains)
INFO - root - 2017-12-07 15:46:42.076341: step 26160, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 61h:23m:54s remains)
INFO - root - 2017-12-07 15:46:48.843230: step 26170, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 60h:13m:16s remains)
INFO - root - 2017-12-07 15:46:55.620598: step 26180, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 57h:26m:29s remains)
INFO - root - 2017-12-07 15:47:02.449122: step 26190, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 54h:27m:39s remains)
INFO - root - 2017-12-07 15:47:09.330061: step 26200, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 60h:19m:57s remains)
2017-12-07 15:47:10.052027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1839151 -4.1892257 -4.1852608 -4.1886053 -4.1987686 -4.2177906 -4.2411904 -4.2569966 -4.2597704 -4.255527 -4.248343 -4.2415133 -4.2411962 -4.2456985 -4.247673][-4.2063932 -4.2031431 -4.1982803 -4.2048368 -4.2185264 -4.2373257 -4.2587519 -4.2734666 -4.278626 -4.2758965 -4.2671041 -4.2566 -4.2504439 -4.2456317 -4.2379651][-4.2253575 -4.2120171 -4.2043204 -4.2096944 -4.2249937 -4.2452312 -4.2672262 -4.2844343 -4.2940755 -4.2933321 -4.2830644 -4.26964 -4.2575774 -4.2436218 -4.2276874][-4.2449069 -4.2297482 -4.221765 -4.2245951 -4.2345142 -4.2506838 -4.2692523 -4.2868648 -4.3028717 -4.3075895 -4.2984514 -4.2815595 -4.2624979 -4.2392459 -4.2180142][-4.2467756 -4.2340751 -4.2251663 -4.2227707 -4.2234311 -4.2292042 -4.2396727 -4.2611604 -4.2912831 -4.304564 -4.2954965 -4.2752347 -4.2497249 -4.2191157 -4.1945791][-4.2350917 -4.2244916 -4.2147179 -4.204782 -4.1898561 -4.1776247 -4.1774983 -4.2078366 -4.256846 -4.2810416 -4.27335 -4.25124 -4.2246718 -4.194293 -4.1720333][-4.2006254 -4.1971855 -4.1942267 -4.1796141 -4.1501417 -4.11588 -4.102529 -4.1435189 -4.2136731 -4.2505465 -4.2478404 -4.2259407 -4.1996183 -4.1744704 -4.1576657][-4.1483083 -4.1516385 -4.1540008 -4.1349978 -4.0918469 -4.0331731 -4.0030408 -4.0587897 -4.1546717 -4.2082691 -4.2156653 -4.1993928 -4.1738005 -4.1516881 -4.1404171][-4.1331649 -4.1373281 -4.13255 -4.1024866 -4.0429187 -3.9547236 -3.900295 -3.9714434 -4.094811 -4.167263 -4.1863132 -4.176908 -4.1547828 -4.1361413 -4.1321173][-4.1522603 -4.1524191 -4.1415524 -4.1091537 -4.0518851 -3.9686584 -3.9112027 -3.9663243 -4.0785365 -4.1533041 -4.1763945 -4.1699486 -4.1514306 -4.136333 -4.1367736][-4.18587 -4.1820359 -4.1693683 -4.1416712 -4.0953979 -4.03251 -3.9840269 -4.01098 -4.0916023 -4.1568246 -4.1814075 -4.1769032 -4.1607852 -4.1512079 -4.1552644][-4.2214 -4.2166014 -4.2034597 -4.1804833 -4.1395969 -4.0874147 -4.0485454 -4.0593786 -4.114028 -4.1657329 -4.187602 -4.1857181 -4.1757936 -4.1723008 -4.1761417][-4.2564163 -4.252923 -4.2438188 -4.2249718 -4.1884303 -4.1474195 -4.1202359 -4.1224971 -4.1535616 -4.1871185 -4.2044311 -4.2062244 -4.1968312 -4.1892667 -4.1873779][-4.2815123 -4.2812467 -4.2780004 -4.2648458 -4.2366114 -4.2074304 -4.1874304 -4.1809897 -4.1938047 -4.2147522 -4.2262378 -4.2256732 -4.2126923 -4.1965156 -4.1882858][-4.2951226 -4.2980781 -4.2969894 -4.2875571 -4.2689495 -4.2503896 -4.2312417 -4.2174459 -4.2214537 -4.23377 -4.2392783 -4.2368488 -4.2212415 -4.1983275 -4.1828647]]...]
INFO - root - 2017-12-07 15:47:16.657678: step 26210, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 55h:53m:36s remains)
INFO - root - 2017-12-07 15:47:23.348000: step 26220, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 57h:11m:20s remains)
INFO - root - 2017-12-07 15:47:30.254967: step 26230, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 60h:09m:25s remains)
INFO - root - 2017-12-07 15:47:37.027279: step 26240, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 58h:25m:28s remains)
INFO - root - 2017-12-07 15:47:43.748758: step 26250, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 54h:36m:12s remains)
INFO - root - 2017-12-07 15:47:50.509684: step 26260, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 54h:43m:03s remains)
INFO - root - 2017-12-07 15:47:57.335673: step 26270, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 57h:14m:09s remains)
INFO - root - 2017-12-07 15:48:03.838252: step 26280, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 56h:48m:48s remains)
INFO - root - 2017-12-07 15:48:10.745127: step 26290, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 58h:05m:28s remains)
INFO - root - 2017-12-07 15:48:17.452356: step 26300, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 53h:57m:16s remains)
2017-12-07 15:48:18.204273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1225543 -4.1153584 -4.1246691 -4.164834 -4.2103519 -4.2258377 -4.1948709 -4.1566038 -4.139576 -4.1139402 -4.0895958 -4.0810823 -4.0754027 -4.0959244 -4.1338639][-4.111773 -4.1147494 -4.1293755 -4.1651382 -4.2047443 -4.2163873 -4.1888294 -4.1536098 -4.1230021 -4.0830832 -4.0449109 -4.0213919 -4.0118732 -4.03706 -4.08723][-4.1358743 -4.146358 -4.1617727 -4.1889715 -4.21382 -4.2101417 -4.1737943 -4.1405768 -4.1163063 -4.0818596 -4.0371284 -3.9991279 -3.9849198 -4.0062237 -4.0599265][-4.1505451 -4.1669111 -4.1778717 -4.1915751 -4.2038565 -4.1872578 -4.1489892 -4.1230421 -4.1197162 -4.1082106 -4.0760765 -4.0409632 -4.0239582 -4.0426598 -4.0870237][-4.1640272 -4.1671124 -4.1675391 -4.1684961 -4.1747255 -4.15942 -4.1254468 -4.1058168 -4.1229539 -4.1318827 -4.1202993 -4.1065435 -4.0964317 -4.1159382 -4.1452551][-4.1899381 -4.17222 -4.1503892 -4.131784 -4.1214495 -4.0911989 -4.0449181 -4.0214481 -4.0689487 -4.1115384 -4.1305547 -4.1481752 -4.1603279 -4.178689 -4.193573][-4.2282672 -4.1914945 -4.1458306 -4.0987082 -4.0457368 -3.9667659 -3.8672135 -3.8147473 -3.9021721 -4.0101109 -4.0781226 -4.1260066 -4.1606388 -4.1794195 -4.1862903][-4.26773 -4.2198362 -4.1584439 -4.0861917 -3.9900184 -3.860141 -3.6943877 -3.573617 -3.6977847 -3.8806725 -3.9967582 -4.0666056 -4.1113887 -4.1347675 -4.1455436][-4.2942977 -4.2448349 -4.1788273 -4.0978484 -3.9930747 -3.8632896 -3.7006457 -3.5668709 -3.6659091 -3.8492496 -3.9577355 -4.0241685 -4.0610862 -4.0904441 -4.1067829][-4.3034277 -4.2553039 -4.1955876 -4.123898 -4.0416856 -3.9588287 -3.8649316 -3.7885756 -3.8364694 -3.9405737 -3.9971797 -4.0268226 -4.0440626 -4.0727468 -4.0936928][-4.2974768 -4.2567835 -4.2099338 -4.1606483 -4.1113524 -4.0708156 -4.0285673 -3.9912267 -4.0075474 -4.0488505 -4.060082 -4.0579939 -4.0616159 -4.0879011 -4.1070623][-4.27196 -4.245295 -4.2148542 -4.1882024 -4.1645637 -4.1493034 -4.131999 -4.1108823 -4.110899 -4.1239982 -4.1228428 -4.1141291 -4.1132793 -4.1317625 -4.1448536][-4.2390761 -4.225183 -4.2113724 -4.2052755 -4.2014141 -4.2000732 -4.1944823 -4.1804709 -4.1720719 -4.171524 -4.1669164 -4.1590633 -4.1597042 -4.1732612 -4.1838236][-4.2360983 -4.23334 -4.2326145 -4.237566 -4.2406573 -4.2423396 -4.2378311 -4.2246251 -4.2107177 -4.2014575 -4.19585 -4.1884108 -4.1894994 -4.2003736 -4.2137942][-4.2591114 -4.2647653 -4.2714739 -4.276432 -4.2797413 -4.2816305 -4.2784452 -4.2690563 -4.2569084 -4.2482395 -4.2446146 -4.2366872 -4.233036 -4.2367883 -4.24635]]...]
INFO - root - 2017-12-07 15:48:24.946241: step 26310, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.683 sec/batch; 58h:05m:32s remains)
INFO - root - 2017-12-07 15:48:31.572883: step 26320, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 57h:09m:03s remains)
INFO - root - 2017-12-07 15:48:38.350466: step 26330, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 55h:20m:46s remains)
INFO - root - 2017-12-07 15:48:45.100360: step 26340, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 55h:43m:51s remains)
INFO - root - 2017-12-07 15:48:51.889017: step 26350, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.708 sec/batch; 60h:11m:01s remains)
INFO - root - 2017-12-07 15:48:58.774192: step 26360, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 61h:25m:16s remains)
INFO - root - 2017-12-07 15:49:05.521693: step 26370, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 56h:00m:41s remains)
INFO - root - 2017-12-07 15:49:12.209996: step 26380, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 55h:47m:41s remains)
INFO - root - 2017-12-07 15:49:18.979814: step 26390, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 57h:33m:51s remains)
INFO - root - 2017-12-07 15:49:25.804142: step 26400, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 61h:10m:53s remains)
2017-12-07 15:49:26.464346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3291197 -4.3250089 -4.3173594 -4.3115115 -4.3115163 -4.3197761 -4.3321567 -4.3420444 -4.3474946 -4.3489623 -4.3441234 -4.3351707 -4.3269353 -4.3208218 -4.3189125][-4.3266983 -4.3135309 -4.2933888 -4.2777429 -4.2755551 -4.2882867 -4.3084307 -4.3270311 -4.3410959 -4.3516297 -4.3556185 -4.3528008 -4.34674 -4.33904 -4.3329296][-4.319622 -4.2936177 -4.2568121 -4.2266955 -4.2184739 -4.2342305 -4.2620554 -4.2912 -4.3167748 -4.3395052 -4.3549285 -4.3608961 -4.3594556 -4.3525276 -4.3436656][-4.3121614 -4.2745476 -4.2217593 -4.1757107 -4.1566362 -4.1686649 -4.197649 -4.234179 -4.2724819 -4.3100033 -4.3406944 -4.3595695 -4.3660007 -4.3617687 -4.351018][-4.3069534 -4.2645946 -4.2027769 -4.1424751 -4.1059494 -4.1021476 -4.1197615 -4.1559963 -4.2060423 -4.2598414 -4.3070889 -4.3413634 -4.3599958 -4.3627782 -4.3536706][-4.3032651 -4.2633104 -4.2012286 -4.132731 -4.0751433 -4.0416136 -4.0309963 -4.0559993 -4.1172562 -4.1888518 -4.2523766 -4.3029327 -4.3370228 -4.3514023 -4.3489113][-4.3029532 -4.270124 -4.2154427 -4.144743 -4.0681796 -3.9985774 -3.9459453 -3.9481409 -4.0163708 -4.1043968 -4.1822734 -4.247879 -4.2978392 -4.3273511 -4.3359852][-4.3071175 -4.2839885 -4.2417431 -4.1785936 -4.0965819 -4.0027132 -3.9129176 -3.8837643 -3.9433334 -4.0324807 -4.1141629 -4.1867948 -4.2486858 -4.2930603 -4.3151622][-4.3142037 -4.3006573 -4.2723575 -4.2258072 -4.1562824 -4.0646281 -3.9668217 -3.9145055 -3.9402533 -4.0029445 -4.0700116 -4.1378675 -4.2037854 -4.2587271 -4.2932048][-4.3187323 -4.3125687 -4.2971377 -4.2702584 -4.2236032 -4.1544037 -4.0739412 -4.0158024 -4.0045571 -4.0272455 -4.0670905 -4.1185946 -4.1774926 -4.2341118 -4.2753825][-4.3196268 -4.3183594 -4.31209 -4.3002305 -4.2759161 -4.2351246 -4.1829638 -4.133275 -4.1025286 -4.0941544 -4.1065116 -4.1357322 -4.1782761 -4.2269239 -4.2672305][-4.3186378 -4.3204947 -4.31942 -4.3158097 -4.3061538 -4.2887564 -4.2635431 -4.2314482 -4.2004166 -4.1795888 -4.1731782 -4.1807556 -4.2040324 -4.2384758 -4.2708511][-4.3172245 -4.3222413 -4.3242135 -4.3240676 -4.3212981 -4.3167243 -4.3083019 -4.292985 -4.2730589 -4.2547956 -4.2426848 -4.2381983 -4.2460527 -4.2647328 -4.2852521][-4.3118682 -4.3202271 -4.3242788 -4.3247185 -4.3235855 -4.3231773 -4.3216276 -4.3170271 -4.3092494 -4.3002534 -4.2915306 -4.2854204 -4.286129 -4.2935252 -4.3031859][-4.2980032 -4.3089423 -4.3142118 -4.3143654 -4.3132672 -4.3134346 -4.3139343 -4.3150921 -4.3151064 -4.3139153 -4.3121161 -4.3107514 -4.3110218 -4.3131976 -4.316627]]...]
INFO - root - 2017-12-07 15:49:33.174688: step 26410, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 53h:06m:53s remains)
INFO - root - 2017-12-07 15:49:39.730350: step 26420, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 53h:49m:07s remains)
INFO - root - 2017-12-07 15:49:46.494300: step 26430, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 58h:26m:07s remains)
INFO - root - 2017-12-07 15:49:53.335475: step 26440, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 59h:52m:47s remains)
INFO - root - 2017-12-07 15:50:00.107728: step 26450, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.668 sec/batch; 56h:49m:15s remains)
INFO - root - 2017-12-07 15:50:06.946012: step 26460, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 56h:43m:42s remains)
INFO - root - 2017-12-07 15:50:13.774625: step 26470, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 57h:44m:44s remains)
INFO - root - 2017-12-07 15:50:20.456458: step 26480, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 58h:58m:25s remains)
INFO - root - 2017-12-07 15:50:27.221466: step 26490, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 56h:14m:34s remains)
INFO - root - 2017-12-07 15:50:33.990434: step 26500, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 56h:00m:55s remains)
2017-12-07 15:50:34.735797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31048 -4.309042 -4.2914033 -4.2602997 -4.2296572 -4.2082162 -4.1989536 -4.1904716 -4.1816778 -4.18853 -4.2193365 -4.2573762 -4.28833 -4.3003745 -4.2934346][-4.3360996 -4.3272824 -4.2955561 -4.2482347 -4.2095695 -4.1953077 -4.2042756 -4.2078738 -4.204627 -4.2081828 -4.2312574 -4.2598753 -4.2835207 -4.2930822 -4.2846966][-4.3404384 -4.3222027 -4.2751265 -4.2152724 -4.1783156 -4.1837358 -4.20897 -4.2181721 -4.2088766 -4.2026591 -4.2163076 -4.2406235 -4.2634583 -4.2722206 -4.2539973][-4.3300371 -4.3024235 -4.2411056 -4.1779857 -4.1482568 -4.1657114 -4.1970553 -4.1998153 -4.1780667 -4.1638288 -4.1787834 -4.2113714 -4.2401147 -4.2459974 -4.2194314][-4.3001356 -4.2695351 -4.2086229 -4.1555223 -4.1376133 -4.1547379 -4.1692162 -4.1414151 -4.0965166 -4.0804262 -4.1135163 -4.1637182 -4.1964073 -4.196641 -4.1666451][-4.265161 -4.237968 -4.1905336 -4.1544547 -4.1410131 -4.1355429 -4.1043911 -4.0225868 -3.9537368 -3.963851 -4.0332956 -4.092988 -4.1206212 -4.1158829 -4.0946417][-4.2359471 -4.2177558 -4.1860795 -4.1590552 -4.1278038 -4.078783 -3.984334 -3.8499937 -3.790895 -3.8717937 -3.9827981 -4.0479383 -4.0712633 -4.0709829 -4.071312][-4.2075887 -4.1948576 -4.1736035 -4.1523023 -4.1123681 -4.0405602 -3.9265232 -3.8130171 -3.8244066 -3.9481165 -4.0640712 -4.1238804 -4.1436462 -4.1485419 -4.15744][-4.1927223 -4.1901708 -4.18622 -4.1772509 -4.149313 -4.1006212 -4.0330534 -3.9942462 -4.0309563 -4.1155233 -4.1903973 -4.2307377 -4.2444816 -4.247179 -4.2493863][-4.2167025 -4.2166471 -4.222146 -4.2226181 -4.2069449 -4.1825266 -4.1513557 -4.14285 -4.1693497 -4.213892 -4.2544951 -4.2785029 -4.2910452 -4.2963166 -4.2967048][-4.2367973 -4.235291 -4.2423491 -4.2443995 -4.2330651 -4.2144318 -4.1903653 -4.18392 -4.2019134 -4.2307062 -4.2569327 -4.2764068 -4.294785 -4.3068109 -4.3098335][-4.2259378 -4.2270536 -4.2322831 -4.2330809 -4.2196832 -4.1967039 -4.1695795 -4.1612186 -4.1759157 -4.2018905 -4.2274776 -4.2496338 -4.2736053 -4.2950196 -4.3050418][-4.2088261 -4.2068954 -4.2079897 -4.2093043 -4.1949587 -4.1662054 -4.1391659 -4.1372447 -4.1573071 -4.1855159 -4.2133055 -4.2347169 -4.2593746 -4.2812843 -4.2926989][-4.2159576 -4.2051568 -4.196805 -4.1921492 -4.1766 -4.1489897 -4.1269422 -4.1294956 -4.1531205 -4.1815391 -4.207149 -4.227313 -4.2508693 -4.2694616 -4.2771788][-4.2326579 -4.2128754 -4.1979303 -4.1927905 -4.1844873 -4.164659 -4.1498857 -4.15421 -4.1725035 -4.1924324 -4.2088037 -4.2205973 -4.2353787 -4.2462287 -4.2487483]]...]
INFO - root - 2017-12-07 15:50:41.422109: step 26510, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 59h:35m:41s remains)
INFO - root - 2017-12-07 15:50:47.988418: step 26520, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 54h:15m:06s remains)
INFO - root - 2017-12-07 15:50:54.786053: step 26530, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 56h:42m:28s remains)
INFO - root - 2017-12-07 15:51:01.522507: step 26540, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 56h:12m:22s remains)
INFO - root - 2017-12-07 15:51:08.313240: step 26550, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 58h:49m:11s remains)
INFO - root - 2017-12-07 15:51:15.053910: step 26560, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 62h:06m:28s remains)
INFO - root - 2017-12-07 15:51:21.896997: step 26570, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 59h:01m:38s remains)
INFO - root - 2017-12-07 15:51:28.654290: step 26580, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 55h:56m:39s remains)
INFO - root - 2017-12-07 15:51:35.332550: step 26590, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 56h:31m:23s remains)
INFO - root - 2017-12-07 15:51:42.160728: step 26600, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 59h:33m:56s remains)
2017-12-07 15:51:42.891635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3125372 -4.310266 -4.3048725 -4.29904 -4.2965555 -4.2979856 -4.2993565 -4.295186 -4.2861843 -4.2796206 -4.2792954 -4.2861481 -4.2967682 -4.3058271 -4.3096428][-4.3265209 -4.3215714 -4.3103604 -4.2969794 -4.2876816 -4.2845149 -4.285624 -4.2860327 -4.28141 -4.2761126 -4.2739611 -4.2767477 -4.286355 -4.2994146 -4.3076754][-4.3296261 -4.3202348 -4.3032966 -4.2835307 -4.2674747 -4.255055 -4.2511744 -4.2519779 -4.2533846 -4.2536449 -4.2539325 -4.2540741 -4.2603474 -4.2741847 -4.2892289][-4.3194323 -4.30298 -4.2783113 -4.2511311 -4.2282481 -4.2070565 -4.1925354 -4.188283 -4.1936264 -4.2029629 -4.2118831 -4.2145748 -4.2206855 -4.2328095 -4.2506185][-4.3045692 -4.2789426 -4.2439671 -4.2088604 -4.1803808 -4.1476688 -4.1157269 -4.1018953 -4.1162057 -4.1419568 -4.1658983 -4.1750464 -4.1778789 -4.1867957 -4.2062626][-4.2854757 -4.2476311 -4.2011843 -4.1596918 -4.1263533 -4.0804172 -4.0224853 -3.9923801 -4.0202322 -4.0712633 -4.1126552 -4.1273508 -4.1199093 -4.1191192 -4.1397095][-4.2633729 -4.2118154 -4.153604 -4.1077223 -4.0679731 -4.00636 -3.9205482 -3.8603039 -3.893383 -3.9754679 -4.0381808 -4.0599155 -4.0451531 -4.0348978 -4.054781][-4.2400012 -4.1873841 -4.1290231 -4.0910115 -4.0600071 -4.004631 -3.9193392 -3.8472266 -3.8640742 -3.9504595 -4.024334 -4.0542192 -4.0384893 -4.0140777 -4.0202856][-4.2168417 -4.1852689 -4.1514931 -4.1340461 -4.1192284 -4.0872254 -4.0362239 -3.9921775 -3.9980021 -4.0469003 -4.0996428 -4.1264215 -4.1213684 -4.100059 -4.0910268][-4.1876059 -4.1800823 -4.1770844 -4.1832824 -4.1853714 -4.1703506 -4.14501 -4.1246176 -4.1255007 -4.1443996 -4.1722016 -4.190177 -4.19232 -4.1834154 -4.1747026][-4.152463 -4.1629128 -4.1856208 -4.2130642 -4.2318439 -4.2333369 -4.2241869 -4.2179379 -4.2182097 -4.22238 -4.2339544 -4.2451711 -4.2498569 -4.2473741 -4.2407665][-4.1451817 -4.1610136 -4.1883659 -4.2203383 -4.2453413 -4.2558026 -4.2578664 -4.2587671 -4.2596755 -4.25937 -4.2646523 -4.2753668 -4.2832241 -4.2850609 -4.2825742][-4.1825337 -4.1979456 -4.217329 -4.2392211 -4.25865 -4.2695289 -4.2755752 -4.2780466 -4.2798 -4.2824197 -4.2891226 -4.2992764 -4.307405 -4.3111906 -4.3098965][-4.2252746 -4.2379069 -4.2483172 -4.2572851 -4.2669287 -4.2749996 -4.282795 -4.2882977 -4.2929244 -4.29847 -4.3057323 -4.3135676 -4.3199306 -4.324276 -4.3239059][-4.2596397 -4.2688861 -4.2738729 -4.2742453 -4.2760668 -4.2798605 -4.2848911 -4.2897792 -4.2950029 -4.3017206 -4.3093262 -4.3153968 -4.318821 -4.3204503 -4.3184857]]...]
INFO - root - 2017-12-07 15:51:49.643801: step 26610, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 57h:41m:01s remains)
INFO - root - 2017-12-07 15:51:56.235326: step 26620, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 61h:59m:44s remains)
INFO - root - 2017-12-07 15:52:03.107571: step 26630, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 58h:36m:26s remains)
INFO - root - 2017-12-07 15:52:09.815813: step 26640, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 54h:37m:23s remains)
INFO - root - 2017-12-07 15:52:16.634278: step 26650, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 54h:54m:34s remains)
INFO - root - 2017-12-07 15:52:23.344023: step 26660, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 62h:09m:22s remains)
INFO - root - 2017-12-07 15:52:30.074228: step 26670, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 60h:28m:58s remains)
INFO - root - 2017-12-07 15:52:36.747958: step 26680, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 56h:42m:52s remains)
INFO - root - 2017-12-07 15:52:43.548016: step 26690, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 54h:26m:05s remains)
INFO - root - 2017-12-07 15:52:50.294450: step 26700, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 53h:51m:07s remains)
2017-12-07 15:52:51.076615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009834 -4.30262 -4.2996716 -4.2920074 -4.2857656 -4.2883534 -4.2889028 -4.2865763 -4.2848272 -4.2812405 -4.2768612 -4.2740183 -4.2722316 -4.2638206 -4.2558508][-4.280252 -4.2819881 -4.2790251 -4.2688384 -4.2569566 -4.2570624 -4.2543197 -4.2439537 -4.233644 -4.228066 -4.2288976 -4.2308965 -4.2314992 -4.2241359 -4.22034][-4.2559581 -4.2576923 -4.2535849 -4.2423992 -4.2248831 -4.2177467 -4.2068 -4.1823521 -4.1608009 -4.1566782 -4.1680875 -4.1779828 -4.1823144 -4.1813197 -4.1868749][-4.2365727 -4.2348833 -4.2274065 -4.2138715 -4.1899786 -4.1731539 -4.1501851 -4.1116328 -4.0836411 -4.086246 -4.1164961 -4.1370687 -4.143713 -4.1490192 -4.1647754][-4.2179971 -4.2112632 -4.1969995 -4.1767187 -4.1436934 -4.1106305 -4.0680065 -4.0134559 -3.9894905 -4.0186005 -4.0779891 -4.1160021 -4.1249895 -4.133513 -4.1529112][-4.1968904 -4.187325 -4.1657548 -4.1381149 -4.0965834 -4.0436306 -3.9719543 -3.8889284 -3.87819 -3.953846 -4.0494885 -4.1086464 -4.1269917 -4.1369414 -4.1551495][-4.1695638 -4.157505 -4.1316938 -4.1062956 -4.0640259 -3.9960518 -3.891911 -3.7701521 -3.7776327 -3.9066048 -4.0347962 -4.1119251 -4.1404533 -4.1510911 -4.1655874][-4.1555166 -4.1413431 -4.1164942 -4.1034756 -4.0701876 -4.0016341 -3.8952391 -3.7759931 -3.7982006 -3.9377856 -4.0645647 -4.13746 -4.1615 -4.162869 -4.1671233][-4.1595287 -4.1454387 -4.1254907 -4.1225572 -4.101274 -4.0495358 -3.9752314 -3.9063902 -3.9372206 -4.0431161 -4.1348343 -4.183979 -4.1936135 -4.1794181 -4.1662211][-4.178679 -4.1647573 -4.1445055 -4.1411438 -4.1277561 -4.0910749 -4.0449276 -4.0119925 -4.0486574 -4.1255751 -4.1891975 -4.2238631 -4.2226143 -4.1968026 -4.1716547][-4.2015281 -4.1922116 -4.1743989 -4.1690359 -4.1608334 -4.1356268 -4.098968 -4.0760784 -4.1150088 -4.1803312 -4.2292686 -4.2558975 -4.2496638 -4.2162242 -4.1879749][-4.2184663 -4.2143459 -4.2041464 -4.2044964 -4.200738 -4.1826329 -4.1525931 -4.136477 -4.172719 -4.226275 -4.2612534 -4.2813544 -4.2750821 -4.2404528 -4.2070179][-4.2357664 -4.24184 -4.2419038 -4.2439017 -4.2385907 -4.2210007 -4.2017317 -4.1973987 -4.2262864 -4.2634206 -4.28284 -4.2919188 -4.28412 -4.2540994 -4.2206674][-4.25314 -4.2673283 -4.2711639 -4.2737103 -4.2640114 -4.2450552 -4.23206 -4.2352028 -4.2562132 -4.2775235 -4.2858105 -4.2830033 -4.2706447 -4.2463593 -4.2203164][-4.2698812 -4.2843409 -4.2883205 -4.2889705 -4.2764897 -4.2583938 -4.2478776 -4.2504368 -4.2646036 -4.278142 -4.28098 -4.2743921 -4.2632093 -4.2480907 -4.2341132]]...]
INFO - root - 2017-12-07 15:52:57.785171: step 26710, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 56h:43m:43s remains)
INFO - root - 2017-12-07 15:53:04.329912: step 26720, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 54h:45m:05s remains)
INFO - root - 2017-12-07 15:53:11.155090: step 26730, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 54h:29m:36s remains)
INFO - root - 2017-12-07 15:53:18.076250: step 26740, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 59h:24m:30s remains)
INFO - root - 2017-12-07 15:53:24.818044: step 26750, loss = 2.08, batch loss = 2.03 (10.7 examples/sec; 0.745 sec/batch; 63h:18m:47s remains)
INFO - root - 2017-12-07 15:53:31.562997: step 26760, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.700 sec/batch; 59h:27m:02s remains)
INFO - root - 2017-12-07 15:53:38.313030: step 26770, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 56h:47m:36s remains)
INFO - root - 2017-12-07 15:53:45.203831: step 26780, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 57h:27m:34s remains)
INFO - root - 2017-12-07 15:53:52.086131: step 26790, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.744 sec/batch; 63h:09m:41s remains)
INFO - root - 2017-12-07 15:53:58.913856: step 26800, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 63h:04m:38s remains)
2017-12-07 15:53:59.668850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1066294 -4.1210113 -4.1321979 -4.1367459 -4.1308746 -4.108923 -4.0859189 -4.0957913 -4.1266041 -4.144474 -4.1400352 -4.1318436 -4.1271253 -4.1201491 -4.1107922][-4.0723372 -4.0897188 -4.104506 -4.1107426 -4.1046586 -4.0839324 -4.0606608 -4.06712 -4.094305 -4.111937 -4.1076055 -4.1004472 -4.095768 -4.089325 -4.0843835][-4.0660558 -4.08316 -4.0997782 -4.1100283 -4.1067448 -4.0890036 -4.0683174 -4.0718832 -4.0918074 -4.1020513 -4.0904684 -4.078589 -4.0684137 -4.0575218 -4.056046][-4.085731 -4.0978222 -4.1132765 -4.1243272 -4.1222949 -4.1055989 -4.0867586 -4.0851083 -4.0940475 -4.0910592 -4.06845 -4.0479536 -4.03474 -4.0240097 -4.0265255][-4.0964155 -4.0988445 -4.1125293 -4.1254678 -4.1235533 -4.1050496 -4.0793824 -4.066298 -4.0666323 -4.0616097 -4.0449386 -4.0299087 -4.0217915 -4.0153103 -4.0164552][-4.085712 -4.0784516 -4.0863762 -4.0952239 -4.0934081 -4.0760078 -4.0473251 -4.0267072 -4.0298114 -4.039824 -4.03948 -4.0311627 -4.0267825 -4.0241394 -4.0282736][-4.0630426 -4.0407586 -4.0362053 -4.0372128 -4.0352407 -4.0240498 -4.0002303 -3.9790859 -3.9888458 -4.015254 -4.0334754 -4.0386267 -4.0452909 -4.0532389 -4.0645604][-4.052279 -4.0139027 -3.9938755 -3.9901397 -3.989085 -3.9856813 -3.9708467 -3.9504361 -3.9612873 -3.9994133 -4.037251 -4.0629025 -4.0857897 -4.1018572 -4.1137867][-4.0551004 -4.015759 -3.9936857 -3.9926507 -3.9951861 -3.998528 -3.9886065 -3.9675474 -3.9789152 -4.0247717 -4.0756364 -4.1116695 -4.137516 -4.1529512 -4.1612921][-4.1043792 -4.0755157 -4.0643921 -4.069169 -4.071784 -4.071979 -4.0614495 -4.0424995 -4.0518517 -4.0910778 -4.1339769 -4.1611686 -4.1751561 -4.1815042 -4.1864634][-4.18301 -4.1680045 -4.1676717 -4.172133 -4.1719995 -4.1708379 -4.1613774 -4.1445417 -4.1450143 -4.161932 -4.1795325 -4.1882281 -4.1897378 -4.1888223 -4.1889105][-4.2468677 -4.242744 -4.2432494 -4.2426085 -4.2408767 -4.2425075 -4.236125 -4.2232041 -4.2166791 -4.2168179 -4.2168055 -4.2110643 -4.2013335 -4.1906819 -4.1837792][-4.2915564 -4.2920046 -4.2900581 -4.2859111 -4.2850623 -4.2882595 -4.2831564 -4.2701049 -4.25793 -4.248302 -4.2379594 -4.2216043 -4.2013927 -4.1828141 -4.171773][-4.2951674 -4.29953 -4.2996597 -4.2971125 -4.2980337 -4.3020411 -4.2995219 -4.2898145 -4.2777758 -4.264647 -4.2486749 -4.2262526 -4.2036738 -4.1834059 -4.1695843][-4.284935 -4.2923903 -4.2969818 -4.2971988 -4.2975974 -4.2998857 -4.2985835 -4.29229 -4.2819304 -4.2679892 -4.2501121 -4.2288289 -4.2126956 -4.198153 -4.1863937]]...]
INFO - root - 2017-12-07 15:54:06.512076: step 26810, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 59h:56m:25s remains)
INFO - root - 2017-12-07 15:54:13.242555: step 26820, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 60h:26m:23s remains)
INFO - root - 2017-12-07 15:54:20.151121: step 26830, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 60h:20m:06s remains)
INFO - root - 2017-12-07 15:54:26.960671: step 26840, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.643 sec/batch; 54h:33m:26s remains)
INFO - root - 2017-12-07 15:54:33.767219: step 26850, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 60h:02m:21s remains)
INFO - root - 2017-12-07 15:54:40.598003: step 26860, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 61h:49m:43s remains)
INFO - root - 2017-12-07 15:54:47.359263: step 26870, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 56h:48m:23s remains)
INFO - root - 2017-12-07 15:54:54.214549: step 26880, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.699 sec/batch; 59h:18m:25s remains)
INFO - root - 2017-12-07 15:55:01.092354: step 26890, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 56h:33m:20s remains)
INFO - root - 2017-12-07 15:55:07.737048: step 26900, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 55h:57m:43s remains)
2017-12-07 15:55:08.465210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3380213 -4.3409128 -4.3494215 -4.3560114 -4.3528395 -4.34142 -4.3286462 -4.3161449 -4.3070769 -4.30215 -4.2999191 -4.2935672 -4.2894177 -4.2825541 -4.2637134][-4.3219252 -4.3305135 -4.3398495 -4.3418922 -4.331676 -4.3113227 -4.29228 -4.276906 -4.2690334 -4.2697978 -4.2734256 -4.2723479 -4.2745261 -4.2757478 -4.2689319][-4.2983871 -4.3117161 -4.3195238 -4.3152966 -4.3019581 -4.278656 -4.2525597 -4.2292686 -4.2176809 -4.2254224 -4.2442989 -4.2582846 -4.2689333 -4.2716327 -4.2707314][-4.2972817 -4.3107796 -4.3133006 -4.3006873 -4.2821674 -4.2545595 -4.2136974 -4.1748657 -4.160809 -4.1822329 -4.2237387 -4.2585425 -4.2760558 -4.2724676 -4.2670493][-4.3027091 -4.3132324 -4.3090477 -4.2927489 -4.2693491 -4.2351584 -4.1817126 -4.130085 -4.1126633 -4.1434431 -4.1997418 -4.246521 -4.2692413 -4.2583075 -4.242415][-4.3027968 -4.3053961 -4.298429 -4.2845578 -4.2574258 -4.211844 -4.1471391 -4.0889254 -4.0755415 -4.1165509 -4.1791897 -4.2260728 -4.2489858 -4.235496 -4.2142997][-4.3029137 -4.2941303 -4.2827363 -4.2734013 -4.2491808 -4.1981306 -4.1244383 -4.06724 -4.0755033 -4.1339536 -4.1957788 -4.2321382 -4.2438965 -4.2202854 -4.1894903][-4.3084307 -4.289144 -4.2709775 -4.2552681 -4.2275591 -4.1769552 -4.1059837 -4.0639796 -4.097703 -4.1683526 -4.2245879 -4.2448978 -4.24132 -4.2067184 -4.1680126][-4.2971406 -4.27307 -4.2440376 -4.2105441 -4.1728954 -4.12666 -4.0742807 -4.0580068 -4.1057839 -4.1741719 -4.2300434 -4.2477727 -4.2377663 -4.2060661 -4.171567][-4.2707043 -4.241107 -4.1980252 -4.1419578 -4.0916233 -4.0505719 -4.0261683 -4.0362558 -4.0890636 -4.1573267 -4.2203484 -4.2493472 -4.2456036 -4.2246933 -4.2066841][-4.246892 -4.2028837 -4.1390419 -4.0610604 -3.9976892 -3.9646583 -3.9635372 -3.9946678 -4.0570188 -4.1349268 -4.2104788 -4.2502618 -4.2531948 -4.2402248 -4.2276874][-4.2236366 -4.1655641 -4.0929832 -4.01374 -3.9576042 -3.937916 -3.9494629 -3.98821 -4.0492506 -4.1250873 -4.1969419 -4.23678 -4.2399421 -4.2272811 -4.2159286][-4.2038221 -4.1480789 -4.0922322 -4.0355864 -3.9971583 -3.9873848 -3.9990063 -4.0320115 -4.0777378 -4.1264205 -4.1723604 -4.1975164 -4.1998811 -4.1916513 -4.193831][-4.2023993 -4.1574807 -4.1244116 -4.0923953 -4.0721822 -4.0667281 -4.0708318 -4.0902953 -4.116179 -4.1296735 -4.1378717 -4.1338887 -4.1215992 -4.1161966 -4.1336088][-4.2250595 -4.1912723 -4.1740975 -4.1613889 -4.1544008 -4.1487679 -4.1443429 -4.1466637 -4.1522889 -4.1422234 -4.1165848 -4.0775375 -4.035295 -4.0188093 -4.0432839]]...]
INFO - root - 2017-12-07 15:55:15.254012: step 26910, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.675 sec/batch; 57h:19m:57s remains)
INFO - root - 2017-12-07 15:55:21.881719: step 26920, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 59h:39m:27s remains)
INFO - root - 2017-12-07 15:55:28.859387: step 26930, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.714 sec/batch; 60h:37m:51s remains)
INFO - root - 2017-12-07 15:55:35.712855: step 26940, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 57h:39m:37s remains)
INFO - root - 2017-12-07 15:55:42.449557: step 26950, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 56h:21m:17s remains)
INFO - root - 2017-12-07 15:55:49.367943: step 26960, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 57h:23m:53s remains)
INFO - root - 2017-12-07 15:55:56.156350: step 26970, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 58h:44m:48s remains)
INFO - root - 2017-12-07 15:56:02.912233: step 26980, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 59h:38m:21s remains)
INFO - root - 2017-12-07 15:56:09.802709: step 26990, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 57h:09m:08s remains)
INFO - root - 2017-12-07 15:56:16.588013: step 27000, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 56h:25m:54s remains)
2017-12-07 15:56:17.328150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1455421 -4.1654587 -4.1800036 -4.1770716 -4.1661148 -4.1574316 -4.1413517 -4.1253815 -4.1177497 -4.1342664 -4.1571083 -4.1738238 -4.1921091 -4.2133632 -4.2436733][-4.1524849 -4.17309 -4.1821575 -4.1709352 -4.1523376 -4.1387315 -4.1218514 -4.1015697 -4.0908546 -4.1041503 -4.1249146 -4.1449327 -4.168591 -4.1950746 -4.2299862][-4.160182 -4.1801662 -4.1913009 -4.1823459 -4.1625795 -4.1440663 -4.1291237 -4.1102715 -4.0984569 -4.1101332 -4.1314178 -4.1555247 -4.1813421 -4.2038326 -4.2356634][-4.167305 -4.1874952 -4.2021828 -4.2003832 -4.1857572 -4.1648116 -4.1457191 -4.1269226 -4.1182451 -4.136867 -4.1637182 -4.1888981 -4.2131972 -4.2314787 -4.2545829][-4.1579804 -4.1790528 -4.1945171 -4.1931486 -4.1741018 -4.1449676 -4.1165166 -4.09234 -4.0912108 -4.1234436 -4.1651883 -4.195581 -4.2221832 -4.2433491 -4.2615771][-4.1403322 -4.161253 -4.1734819 -4.1591749 -4.1221857 -4.0764856 -4.0282936 -3.9852335 -3.9824386 -4.0377083 -4.1048717 -4.1503925 -4.1890659 -4.2223563 -4.2452474][-4.1094203 -4.1262879 -4.1268024 -4.0887876 -4.022943 -3.9524994 -3.8759754 -3.7988095 -3.7916234 -3.8842697 -3.99226 -4.0684123 -4.1308737 -4.1831117 -4.2177582][-4.0894251 -4.0966964 -4.0827847 -4.0267811 -3.9454079 -3.8641324 -3.7753417 -3.6774168 -3.6694813 -3.7882133 -3.9203045 -4.0150671 -4.0936356 -4.1573167 -4.2014074][-4.103879 -4.1032224 -4.0868406 -4.0360208 -3.9714818 -3.9152765 -3.8536649 -3.7851665 -3.7781239 -3.8628957 -3.9663525 -4.0436053 -4.1154509 -4.1743584 -4.2151747][-4.1587739 -4.151906 -4.1342297 -4.09349 -4.0539885 -4.0253696 -3.9910421 -3.9556241 -3.9570713 -4.0095897 -4.0720263 -4.121645 -4.1724038 -4.2155056 -4.2447209][-4.2308116 -4.2200756 -4.2048254 -4.1745491 -4.151659 -4.1362 -4.1161933 -4.0976086 -4.1016903 -4.1306849 -4.1647716 -4.1978145 -4.2308578 -4.2570109 -4.2734489][-4.2846227 -4.2790179 -4.2711792 -4.2532568 -4.2395811 -4.2300677 -4.2176833 -4.204936 -4.2061644 -4.2181249 -4.2331772 -4.2520618 -4.2712936 -4.2832537 -4.2891622][-4.314992 -4.3137445 -4.3087153 -4.2990146 -4.2927279 -4.2880549 -4.2812443 -4.2731543 -4.2747035 -4.2780848 -4.2780571 -4.2825584 -4.2896376 -4.2904162 -4.2892275][-4.3262787 -4.3270626 -4.3227658 -4.3178077 -4.3149638 -4.3116736 -4.3069034 -4.3031783 -4.3085833 -4.3087735 -4.3011303 -4.29498 -4.2921929 -4.2876267 -4.2837687][-4.3263454 -4.3278241 -4.3250237 -4.321907 -4.3201685 -4.317441 -4.3142414 -4.3144059 -4.3209457 -4.318686 -4.305541 -4.2910128 -4.2816205 -4.2767239 -4.2748933]]...]
INFO - root - 2017-12-07 15:56:24.103098: step 27010, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 59h:13m:37s remains)
INFO - root - 2017-12-07 15:56:30.696268: step 27020, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 59h:36m:06s remains)
INFO - root - 2017-12-07 15:56:37.362889: step 27030, loss = 2.04, batch loss = 1.98 (13.1 examples/sec; 0.611 sec/batch; 51h:49m:41s remains)
INFO - root - 2017-12-07 15:56:44.235355: step 27040, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 57h:01m:21s remains)
INFO - root - 2017-12-07 15:56:50.986488: step 27050, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.666 sec/batch; 56h:31m:22s remains)
INFO - root - 2017-12-07 15:56:57.777150: step 27060, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:42m:19s remains)
INFO - root - 2017-12-07 15:57:04.569735: step 27070, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 58h:00m:53s remains)
INFO - root - 2017-12-07 15:57:11.279556: step 27080, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 55h:09m:59s remains)
INFO - root - 2017-12-07 15:57:18.022410: step 27090, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 55h:14m:51s remains)
INFO - root - 2017-12-07 15:57:24.878671: step 27100, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 57h:16m:31s remains)
2017-12-07 15:57:25.558599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.308538 -4.3068357 -4.304697 -4.3023338 -4.3022237 -4.3046064 -4.3069634 -4.3077345 -4.3063345 -4.3053455 -4.3061504 -4.3079548 -4.308888 -4.3082614 -4.3065763][-4.2957268 -4.2942257 -4.2909627 -4.2862387 -4.2846956 -4.2889647 -4.2933722 -4.296433 -4.2965288 -4.2961307 -4.2982278 -4.3020835 -4.3034234 -4.3008504 -4.2953258][-4.2812829 -4.280961 -4.2746205 -4.2636948 -4.2581568 -4.2639132 -4.2712908 -4.2760491 -4.2760835 -4.2750568 -4.2793155 -4.2879648 -4.29394 -4.2924662 -4.2830453][-4.2762914 -4.27611 -4.2653279 -4.2451334 -4.2322011 -4.2381358 -4.2462549 -4.2513914 -4.2496228 -4.2476521 -4.2548761 -4.2687931 -4.2814145 -4.2856417 -4.2780342][-4.2779794 -4.275794 -4.2622819 -4.2344871 -4.2121067 -4.2155194 -4.2255578 -4.232296 -4.229023 -4.2230082 -4.2305245 -4.2504754 -4.2727847 -4.2877393 -4.2872968][-4.2783384 -4.2742634 -4.257164 -4.2254958 -4.1954784 -4.1900425 -4.2011909 -4.2103567 -4.2063694 -4.1990519 -4.2062011 -4.2318387 -4.2626548 -4.2880716 -4.2969017][-4.2777386 -4.2732491 -4.2533703 -4.2191796 -4.1855974 -4.1708264 -4.1796985 -4.1912661 -4.1914525 -4.1833773 -4.1889997 -4.2172694 -4.2516332 -4.2813516 -4.2957258][-4.2745051 -4.2708693 -4.2519245 -4.2186818 -4.1839275 -4.1637816 -4.1720176 -4.1900191 -4.195538 -4.1844039 -4.185339 -4.2128916 -4.2457886 -4.2722812 -4.2902713][-4.2608461 -4.2588315 -4.2458043 -4.2176208 -4.1880894 -4.1721272 -4.1847153 -4.2098393 -4.2191362 -4.2048378 -4.1961589 -4.21478 -4.2420712 -4.2631278 -4.2816043][-4.2462473 -4.246635 -4.2409205 -4.2233315 -4.2049704 -4.1966214 -4.2105703 -4.2350097 -4.2497821 -4.242198 -4.2303843 -4.2360387 -4.2504416 -4.2641892 -4.2780972][-4.2402768 -4.2389154 -4.2332568 -4.2217402 -4.2155662 -4.2162232 -4.2281351 -4.2471237 -4.2637997 -4.2681866 -4.261538 -4.2584195 -4.259625 -4.2628613 -4.2681274][-4.242135 -4.2399597 -4.231534 -4.2195807 -4.2163291 -4.2198186 -4.228507 -4.2451954 -4.26107 -4.2723346 -4.2718 -4.2652469 -4.2588048 -4.2535725 -4.2503977][-4.2534719 -4.2460203 -4.2351513 -4.22186 -4.2134004 -4.2095065 -4.2135072 -4.2275209 -4.2425728 -4.2579236 -4.2622256 -4.2583556 -4.2520313 -4.2458749 -4.238091][-4.2628765 -4.2540841 -4.2408862 -4.22782 -4.2157297 -4.2073512 -4.2040186 -4.208693 -4.2175188 -4.2343683 -4.2445908 -4.2478476 -4.2524509 -4.2516232 -4.2388296][-4.2633691 -4.26185 -4.256659 -4.2474504 -4.2319293 -4.2174034 -4.2043486 -4.1981115 -4.1969595 -4.2070103 -4.220787 -4.2349753 -4.2519927 -4.2592936 -4.2443871]]...]
INFO - root - 2017-12-07 15:57:32.257575: step 27110, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.625 sec/batch; 52h:59m:57s remains)
INFO - root - 2017-12-07 15:57:38.995102: step 27120, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 57h:47m:49s remains)
INFO - root - 2017-12-07 15:57:45.988673: step 27130, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 60h:09m:36s remains)
INFO - root - 2017-12-07 15:57:52.693893: step 27140, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 55h:07m:33s remains)
INFO - root - 2017-12-07 15:57:59.598127: step 27150, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 61h:49m:46s remains)
INFO - root - 2017-12-07 15:58:06.370663: step 27160, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 52h:56m:26s remains)
INFO - root - 2017-12-07 15:58:13.162027: step 27170, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 57h:42m:32s remains)
INFO - root - 2017-12-07 15:58:20.021112: step 27180, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.703 sec/batch; 59h:37m:44s remains)
INFO - root - 2017-12-07 15:58:26.858403: step 27190, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 63h:09m:48s remains)
INFO - root - 2017-12-07 15:58:33.665726: step 27200, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 61h:22m:39s remains)
2017-12-07 15:58:34.363730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2681322 -4.2783256 -4.287787 -4.2937393 -4.2993989 -4.3026381 -4.3031135 -4.3027573 -4.3030119 -4.3005729 -4.2979512 -4.2978873 -4.2980261 -4.29549 -4.2897015][-4.2946587 -4.300405 -4.3101 -4.3163385 -4.3194866 -4.3176565 -4.3102012 -4.3018117 -4.2973647 -4.2943535 -4.2921872 -4.29324 -4.2950726 -4.2937717 -4.2870173][-4.3000422 -4.3042755 -4.3158722 -4.32538 -4.3286934 -4.3227816 -4.3081045 -4.2914391 -4.2799177 -4.2745204 -4.272779 -4.277523 -4.2840128 -4.2860074 -4.2796731][-4.2719445 -4.280055 -4.2956638 -4.3096771 -4.3134551 -4.303946 -4.2837396 -4.2607503 -4.2433543 -4.2350917 -4.23453 -4.2439733 -4.2567077 -4.2637539 -4.2622318][-4.2126708 -4.2248855 -4.2426987 -4.2588711 -4.2617345 -4.2498178 -4.2265029 -4.2006421 -4.1814628 -4.1728907 -4.1753755 -4.1919441 -4.2134833 -4.228591 -4.2359962][-4.1481295 -4.1607556 -4.1762729 -4.1906247 -4.1935887 -4.1824346 -4.1587868 -4.1320267 -4.1139369 -4.1077251 -4.114934 -4.1405973 -4.1717176 -4.19548 -4.2123132][-4.1149015 -4.1235189 -4.1330261 -4.1423006 -4.1449142 -4.1361766 -4.1145687 -4.0876684 -4.0694704 -4.0660515 -4.0799875 -4.1145988 -4.1530886 -4.1816077 -4.2020154][-4.1263661 -4.1325393 -4.1380506 -4.1433907 -4.1445241 -4.1355844 -4.1154184 -4.0884728 -4.0685916 -4.0660238 -4.0839939 -4.1208305 -4.15875 -4.185874 -4.2054][-4.1663837 -4.1733489 -4.1796255 -4.1849551 -4.18447 -4.1729331 -4.1522245 -4.12478 -4.1027532 -4.09809 -4.1142507 -4.1455126 -4.1776371 -4.2015057 -4.2187524][-4.21468 -4.2224226 -4.2309031 -4.2374582 -4.235127 -4.220541 -4.1999736 -4.1747589 -4.1528296 -4.1455207 -4.1568336 -4.1801844 -4.2046132 -4.2239876 -4.2375755][-4.2624598 -4.2694287 -4.2789111 -4.2864504 -4.2834792 -4.2680326 -4.249083 -4.228364 -4.2091727 -4.2004142 -4.2061877 -4.2204103 -4.235487 -4.2478933 -4.2573929][-4.3020339 -4.3071179 -4.3155241 -4.3229365 -4.3211365 -4.3083715 -4.2930675 -4.2778764 -4.2626495 -4.2537985 -4.2559628 -4.2627831 -4.2687631 -4.2729564 -4.2787895][-4.327085 -4.3296542 -4.3359432 -4.34226 -4.3424969 -4.3347163 -4.3250237 -4.3149753 -4.3040085 -4.2975407 -4.2978883 -4.2996516 -4.2989526 -4.2981167 -4.303081][-4.3400831 -4.3411088 -4.3448215 -4.3495784 -4.3515992 -4.3484225 -4.3436179 -4.3383036 -4.3323421 -4.3292422 -4.3290343 -4.3276558 -4.32417 -4.3221974 -4.3271637][-4.3452697 -4.3448853 -4.3465075 -4.3496027 -4.3521013 -4.35187 -4.3507032 -4.34883 -4.3465428 -4.3457127 -4.3452687 -4.34309 -4.3401637 -4.3395991 -4.3446951]]...]
INFO - root - 2017-12-07 15:58:40.926327: step 27210, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 58h:54m:10s remains)
INFO - root - 2017-12-07 15:58:47.667716: step 27220, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 58h:55m:40s remains)
INFO - root - 2017-12-07 15:58:54.442685: step 27230, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 53h:10m:12s remains)
INFO - root - 2017-12-07 15:59:01.328740: step 27240, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.670 sec/batch; 56h:48m:47s remains)
INFO - root - 2017-12-07 15:59:08.174495: step 27250, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 60h:43m:06s remains)
INFO - root - 2017-12-07 15:59:14.996546: step 27260, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 60h:53m:02s remains)
INFO - root - 2017-12-07 15:59:21.813086: step 27270, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.670 sec/batch; 56h:49m:15s remains)
INFO - root - 2017-12-07 15:59:28.598622: step 27280, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 52h:42m:56s remains)
INFO - root - 2017-12-07 15:59:35.468534: step 27290, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 59h:43m:15s remains)
INFO - root - 2017-12-07 15:59:42.407666: step 27300, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 62h:47m:41s remains)
2017-12-07 15:59:43.096387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3151217 -4.3080921 -4.2993369 -4.28487 -4.26899 -4.2572269 -4.2469368 -4.2372375 -4.23994 -4.2511468 -4.2652016 -4.2781734 -4.2845187 -4.2788296 -4.2825274][-4.3250756 -4.3253012 -4.32497 -4.3170981 -4.3005614 -4.2871914 -4.2791891 -4.27355 -4.2756 -4.2853055 -4.2961183 -4.307004 -4.3128395 -4.305614 -4.3019433][-4.3283482 -4.3289614 -4.3302927 -4.3253031 -4.3107147 -4.3004031 -4.2987337 -4.3005219 -4.3057003 -4.3126535 -4.3149838 -4.3174973 -4.319488 -4.3082891 -4.2968206][-4.3180146 -4.3172231 -4.3186464 -4.3161054 -4.3049588 -4.2983241 -4.2987962 -4.3019853 -4.3082218 -4.31484 -4.3144283 -4.3126483 -4.3110704 -4.2953405 -4.27732][-4.2839141 -4.2840452 -4.28696 -4.2831793 -4.2723379 -4.2627144 -4.2555704 -4.25248 -4.2609797 -4.2744331 -4.2764244 -4.2755833 -4.2769451 -4.2616982 -4.2354865][-4.2128572 -4.2139149 -4.2189713 -4.2111359 -4.195025 -4.1762004 -4.1507611 -4.1314368 -4.1470766 -4.1797123 -4.19571 -4.1981993 -4.2035766 -4.1932874 -4.1649709][-4.135015 -4.1265397 -4.1181941 -4.0918765 -4.0597572 -4.0229077 -3.9651973 -3.9124722 -3.9416103 -4.0100517 -4.0531 -4.0700779 -4.0919619 -4.1014595 -4.0916729][-4.119103 -4.0920072 -4.0517983 -3.9976413 -3.9429476 -3.8854921 -3.8018208 -3.7203562 -3.7651451 -3.8690529 -3.9409478 -3.9805255 -4.0217352 -4.0522318 -4.0624895][-4.174634 -4.1461143 -4.1032815 -4.0517173 -4.0026178 -3.9566789 -3.8977239 -3.8444724 -3.8707013 -3.94296 -3.9993343 -4.0341415 -4.0672345 -4.0970478 -4.1086688][-4.229073 -4.2138672 -4.188427 -4.160429 -4.1308823 -4.1051183 -4.0751595 -4.0485649 -4.0568523 -4.0887041 -4.1161189 -4.1347036 -4.1504722 -4.1631689 -4.16112][-4.2386489 -4.2330422 -4.2270021 -4.2205353 -4.2074738 -4.1941724 -4.1797647 -4.1677485 -4.166985 -4.1702166 -4.1742096 -4.1771574 -4.17767 -4.1755424 -4.1622005][-4.1948605 -4.2051544 -4.2186279 -4.2299 -4.2317591 -4.2295494 -4.2218704 -4.2159357 -4.2130322 -4.2069988 -4.2027068 -4.20085 -4.1963105 -4.1911793 -4.1789613][-4.1255865 -4.1462331 -4.174808 -4.2050676 -4.2214875 -4.2314172 -4.2340984 -4.2352977 -4.23442 -4.2266579 -4.2251024 -4.2274532 -4.2287035 -4.2275338 -4.2198935][-4.0860276 -4.1054006 -4.1405659 -4.1788163 -4.20272 -4.2250752 -4.239553 -4.2514386 -4.2574592 -4.2562175 -4.2613068 -4.2687435 -4.2749553 -4.2741427 -4.2634096][-4.09475 -4.1046391 -4.1361055 -4.1727834 -4.1960111 -4.2200904 -4.2406597 -4.2598033 -4.2741818 -4.281177 -4.2916574 -4.3044267 -4.3142796 -4.3096857 -4.2942257]]...]
INFO - root - 2017-12-07 15:59:49.863272: step 27310, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 56h:10m:22s remains)
INFO - root - 2017-12-07 15:59:56.608850: step 27320, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 62h:45m:26s remains)
INFO - root - 2017-12-07 16:00:03.461909: step 27330, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:51m:42s remains)
INFO - root - 2017-12-07 16:00:10.257169: step 27340, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 54h:57m:38s remains)
INFO - root - 2017-12-07 16:00:17.022973: step 27350, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 54h:53m:32s remains)
INFO - root - 2017-12-07 16:00:23.822932: step 27360, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 59h:35m:57s remains)
INFO - root - 2017-12-07 16:00:30.610984: step 27370, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 58h:23m:40s remains)
INFO - root - 2017-12-07 16:00:37.502570: step 27380, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 58h:12m:12s remains)
INFO - root - 2017-12-07 16:00:44.320896: step 27390, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 54h:52m:43s remains)
INFO - root - 2017-12-07 16:00:51.189638: step 27400, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.715 sec/batch; 60h:34m:36s remains)
2017-12-07 16:00:51.935534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2603545 -4.2263851 -4.1755447 -4.1203537 -4.070569 -4.0537558 -4.0839539 -4.1453881 -4.1994848 -4.2217746 -4.2081337 -4.1661081 -4.12716 -4.1153154 -4.1255088][-4.269094 -4.2432313 -4.19713 -4.1327362 -4.0728478 -4.0580635 -4.0989347 -4.1649222 -4.2131882 -4.2355509 -4.2330251 -4.2056174 -4.1759157 -4.1587543 -4.1583629][-4.2635942 -4.2468362 -4.2139039 -4.161665 -4.1119928 -4.1023536 -4.1402683 -4.1931534 -4.2264504 -4.24277 -4.2442589 -4.2306595 -4.2112975 -4.1941857 -4.1868324][-4.2450256 -4.2359805 -4.2131658 -4.1792574 -4.1441364 -4.1389008 -4.1633058 -4.1978455 -4.2229705 -4.2403316 -4.2483788 -4.2487116 -4.23961 -4.2265353 -4.215446][-4.2243991 -4.2179289 -4.2010722 -4.1812553 -4.1585908 -4.1529222 -4.1602688 -4.1769071 -4.1997771 -4.2227116 -4.2400393 -4.2560806 -4.2574759 -4.2466292 -4.2350211][-4.2107978 -4.2082725 -4.2012076 -4.1909614 -4.1736631 -4.1589708 -4.1428442 -4.1393194 -4.158699 -4.1856427 -4.2129979 -4.2461576 -4.2600431 -4.2548523 -4.2467017][-4.2085328 -4.2065187 -4.20338 -4.1957669 -4.1821871 -4.1587348 -4.1170697 -4.0945153 -4.1150918 -4.1515441 -4.1873684 -4.2282815 -4.2486167 -4.2488708 -4.2462196][-4.21931 -4.2122746 -4.2021327 -4.1867146 -4.1692977 -4.1348424 -4.0707083 -4.0391159 -4.0742793 -4.129137 -4.1714578 -4.2030325 -4.2201962 -4.2266374 -4.2328339][-4.235486 -4.2226362 -4.2040968 -4.1800346 -4.1521959 -4.1034 -4.0255675 -3.9970891 -4.0545259 -4.1293788 -4.1708407 -4.1870203 -4.1928129 -4.1978087 -4.2091532][-4.2472067 -4.2299271 -4.2102489 -4.1891532 -4.1605277 -4.1069407 -4.0341496 -4.009532 -4.066771 -4.1428785 -4.1765003 -4.1833038 -4.1767373 -4.1724644 -4.18368][-4.2500648 -4.2328119 -4.2203975 -4.213799 -4.1937137 -4.1485276 -4.0893583 -4.0606828 -4.0897946 -4.1420226 -4.1644149 -4.1649938 -4.1548529 -4.1489954 -4.163744][-4.2460079 -4.2356534 -4.231441 -4.2346935 -4.2233019 -4.1884751 -4.1402659 -4.1067276 -4.1072078 -4.1320906 -4.1426105 -4.1349111 -4.1227369 -4.1265144 -4.1513157][-4.2415786 -4.2361612 -4.2357149 -4.2420659 -4.2371907 -4.2084417 -4.1684461 -4.136085 -4.1237187 -4.1311707 -4.1259661 -4.1058707 -4.0926361 -4.1041956 -4.1374984][-4.2389727 -4.2324462 -4.2325277 -4.2386947 -4.2362275 -4.2151213 -4.1841025 -4.1604786 -4.1510892 -4.1531887 -4.1418495 -4.1206737 -4.1088529 -4.1194782 -4.1431155][-4.2358713 -4.2275891 -4.2269363 -4.2320166 -4.2297397 -4.2142706 -4.1916809 -4.1747818 -4.1669731 -4.1666451 -4.1609678 -4.1525078 -4.1508617 -4.1571403 -4.1642532]]...]
INFO - root - 2017-12-07 16:00:58.629246: step 27410, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 55h:47m:35s remains)
INFO - root - 2017-12-07 16:01:05.237900: step 27420, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 55h:05m:37s remains)
INFO - root - 2017-12-07 16:01:12.147173: step 27430, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 61h:40m:08s remains)
INFO - root - 2017-12-07 16:01:19.065498: step 27440, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.759 sec/batch; 64h:20m:03s remains)
INFO - root - 2017-12-07 16:01:25.975365: step 27450, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.702 sec/batch; 59h:29m:23s remains)
INFO - root - 2017-12-07 16:01:32.727823: step 27460, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.653 sec/batch; 55h:17m:36s remains)
INFO - root - 2017-12-07 16:01:39.666282: step 27470, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 60h:07m:10s remains)
INFO - root - 2017-12-07 16:01:46.624614: step 27480, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 59h:19m:01s remains)
INFO - root - 2017-12-07 16:01:53.484322: step 27490, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 59h:00m:40s remains)
INFO - root - 2017-12-07 16:02:00.212423: step 27500, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 53h:10m:23s remains)
2017-12-07 16:02:00.955502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1833334 -4.1731844 -4.1649752 -4.1690784 -4.1863761 -4.2156787 -4.2474065 -4.2727604 -4.2801433 -4.27467 -4.2658744 -4.2502193 -4.2284389 -4.2065721 -4.1990366][-4.183774 -4.1806407 -4.1731248 -4.1675243 -4.1766191 -4.2039156 -4.2348366 -4.2607489 -4.2729216 -4.2722826 -4.2643471 -4.2486157 -4.2268324 -4.210042 -4.2060065][-4.1832042 -4.1892796 -4.1842656 -4.1716747 -4.166223 -4.1820979 -4.2108178 -4.241478 -4.2613983 -4.265801 -4.2585692 -4.2435522 -4.2254357 -4.2198486 -4.2222943][-4.183322 -4.1930513 -4.1890554 -4.1737313 -4.156249 -4.1534419 -4.1671538 -4.1947889 -4.2199264 -4.2300549 -4.226306 -4.2175236 -4.2106061 -4.2218695 -4.2314396][-4.1903729 -4.1983266 -4.1934171 -4.1761684 -4.1513448 -4.1293859 -4.1150556 -4.1269374 -4.1488008 -4.1669455 -4.1758537 -4.1820893 -4.194622 -4.2255182 -4.244318][-4.2131929 -4.2149878 -4.2076378 -4.1935139 -4.1647716 -4.1251054 -4.0831189 -4.0647063 -4.0732517 -4.1022735 -4.1348267 -4.1625566 -4.1934118 -4.2345972 -4.257669][-4.2376986 -4.2292457 -4.2207894 -4.2145143 -4.1899114 -4.1427169 -4.0816317 -4.0274968 -4.0061836 -4.0381241 -4.1003375 -4.1526065 -4.1942124 -4.2337689 -4.2525778][-4.2573676 -4.2399621 -4.2301264 -4.2306 -4.2152824 -4.17095 -4.1052561 -4.0333505 -3.9867456 -4.0202894 -4.1063914 -4.1756206 -4.2143435 -4.2387476 -4.2460265][-4.2800364 -4.2596636 -4.2484393 -4.2535062 -4.2517304 -4.2198215 -4.1656165 -4.1007276 -4.0513859 -4.0765133 -4.16037 -4.2295256 -4.2566223 -4.2597528 -4.2545285][-4.310473 -4.2898231 -4.2781668 -4.2852173 -4.295053 -4.2792797 -4.2396016 -4.1890411 -4.1478047 -4.1576514 -4.2179937 -4.2735848 -4.2895927 -4.2774706 -4.2641087][-4.3264546 -4.3077435 -4.2973342 -4.3058405 -4.3205538 -4.3177009 -4.292614 -4.2561364 -4.2228603 -4.2208681 -4.2577658 -4.2983918 -4.3074646 -4.2901993 -4.2754173][-4.3310485 -4.313693 -4.3041968 -4.3151617 -4.3335848 -4.339015 -4.3250594 -4.2980328 -4.2707534 -4.2627368 -4.2828331 -4.3096585 -4.3174024 -4.3057365 -4.295579][-4.3433375 -4.3270559 -4.3156548 -4.3237638 -4.3406873 -4.3486195 -4.3407135 -4.3199959 -4.2971911 -4.2873344 -4.2966809 -4.3140168 -4.3227444 -4.3194995 -4.314755][-4.3467989 -4.3337922 -4.3220916 -4.3245382 -4.3354993 -4.3420014 -4.3371978 -4.3223486 -4.3054633 -4.2965031 -4.2991629 -4.3096976 -4.3180385 -4.3197632 -4.3189826][-4.3272796 -4.3192492 -4.31177 -4.3123074 -4.3178768 -4.3212795 -4.3192682 -4.311758 -4.3020592 -4.2949495 -4.2937794 -4.2990489 -4.3058829 -4.309989 -4.3114586]]...]
INFO - root - 2017-12-07 16:02:07.692934: step 27510, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 60h:29m:48s remains)
INFO - root - 2017-12-07 16:02:14.337945: step 27520, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 55h:20m:52s remains)
INFO - root - 2017-12-07 16:02:21.122155: step 27530, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 56h:14m:45s remains)
INFO - root - 2017-12-07 16:02:27.932804: step 27540, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 58h:43m:40s remains)
INFO - root - 2017-12-07 16:02:34.730683: step 27550, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 59h:33m:49s remains)
INFO - root - 2017-12-07 16:02:41.555734: step 27560, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 57h:25m:45s remains)
INFO - root - 2017-12-07 16:02:48.422442: step 27570, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 56h:07m:58s remains)
INFO - root - 2017-12-07 16:02:55.182144: step 27580, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 57h:07m:25s remains)
INFO - root - 2017-12-07 16:03:02.011541: step 27590, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 60h:19m:26s remains)
INFO - root - 2017-12-07 16:03:08.926116: step 27600, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 60h:36m:11s remains)
2017-12-07 16:03:09.623347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2304931 -4.2387514 -4.2433043 -4.2422504 -4.2439508 -4.2324786 -4.2163129 -4.2053547 -4.1939945 -4.1780705 -4.16288 -4.1513996 -4.1531277 -4.1686339 -4.1773596][-4.2278609 -4.246686 -4.2604904 -4.2616363 -4.2584848 -4.2339454 -4.2032628 -4.186367 -4.1802888 -4.1677203 -4.1568246 -4.1478033 -4.1511374 -4.1659021 -4.1729193][-4.2193022 -4.2503624 -4.2748332 -4.2766876 -4.2606368 -4.2209005 -4.1763964 -4.1576095 -4.1632676 -4.1662111 -4.162878 -4.1564693 -4.1592846 -4.1730475 -4.1767039][-4.2066593 -4.2466688 -4.2798491 -4.2834063 -4.2572427 -4.1983385 -4.1349463 -4.1097393 -4.1323042 -4.1651134 -4.1775765 -4.170505 -4.1669359 -4.174006 -4.1736608][-4.1931763 -4.2389517 -4.27655 -4.2812657 -4.2484112 -4.1753821 -4.0906291 -4.05233 -4.0894156 -4.1564193 -4.1898437 -4.185842 -4.1720223 -4.1692457 -4.1636534][-4.1891475 -4.2358937 -4.2696376 -4.2712317 -4.2305465 -4.1465282 -4.0410371 -3.9816232 -4.0296164 -4.1279349 -4.18855 -4.1937466 -4.1788354 -4.1707587 -4.1632662][-4.1797628 -4.2259855 -4.25596 -4.2541008 -4.2042608 -4.1062803 -3.9708638 -3.8805981 -3.941308 -4.0748177 -4.1676135 -4.1892276 -4.181129 -4.1748767 -4.1687617][-4.1646047 -4.2033949 -4.23094 -4.2286444 -4.1827221 -4.0821915 -3.9218926 -3.7900167 -3.8560383 -4.021266 -4.1419387 -4.1838875 -4.1856136 -4.1821313 -4.1721539][-4.1658063 -4.1922188 -4.2142806 -4.2129979 -4.1816416 -4.10435 -3.969779 -3.8382635 -3.8676615 -4.0113907 -4.1341977 -4.1881089 -4.1991315 -4.1965046 -4.1835365][-4.1955104 -4.2086406 -4.2199383 -4.2195921 -4.2054062 -4.1616759 -4.0785427 -3.9852962 -3.9776244 -4.0566587 -4.1457324 -4.1928425 -4.2087574 -4.2090511 -4.197166][-4.2323165 -4.2308116 -4.2271504 -4.2250366 -4.2252636 -4.2095261 -4.16692 -4.1098042 -4.086719 -4.1137209 -4.1650615 -4.198029 -4.2129803 -4.2171345 -4.2106142][-4.2578497 -4.2489567 -4.2346783 -4.2240543 -4.2275658 -4.2320366 -4.2156863 -4.1828232 -4.1582637 -4.1584892 -4.1823964 -4.2046733 -4.2150617 -4.2224083 -4.2206945][-4.2737203 -4.2647705 -4.247714 -4.2276783 -4.2244439 -4.2369909 -4.2368522 -4.2204385 -4.1995382 -4.189074 -4.1956825 -4.2085786 -4.2157788 -4.2269692 -4.2338][-4.2721939 -4.2715783 -4.2597022 -4.24036 -4.234468 -4.2469463 -4.2570515 -4.24978 -4.2289395 -4.2099662 -4.2026882 -4.2065415 -4.2136912 -4.2302656 -4.2465382][-4.25278 -4.2579241 -4.2532206 -4.2385154 -4.2319655 -4.241137 -4.2556505 -4.2583637 -4.243866 -4.2208667 -4.2056289 -4.2042532 -4.2105322 -4.2290654 -4.2513814]]...]
INFO - root - 2017-12-07 16:03:16.400239: step 27610, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 60h:04m:44s remains)
INFO - root - 2017-12-07 16:03:22.993529: step 27620, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 58h:54m:49s remains)
INFO - root - 2017-12-07 16:03:29.841510: step 27630, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 57h:05m:02s remains)
INFO - root - 2017-12-07 16:03:36.697714: step 27640, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 55h:06m:17s remains)
INFO - root - 2017-12-07 16:03:43.549259: step 27650, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 54h:40m:23s remains)
INFO - root - 2017-12-07 16:03:50.344034: step 27660, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 60h:43m:24s remains)
INFO - root - 2017-12-07 16:03:57.176381: step 27670, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 57h:24m:19s remains)
INFO - root - 2017-12-07 16:04:03.904106: step 27680, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 56h:34m:43s remains)
INFO - root - 2017-12-07 16:04:10.747551: step 27690, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.653 sec/batch; 55h:19m:51s remains)
INFO - root - 2017-12-07 16:04:17.440824: step 27700, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 55h:48m:40s remains)
2017-12-07 16:04:18.148765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0670485 -4.0798979 -4.1144581 -4.1391044 -4.1555753 -4.1636357 -4.1603417 -4.1438766 -4.1191392 -4.1156478 -4.1272206 -4.1572075 -4.2048516 -4.2467122 -4.2787027][-4.1098304 -4.1246037 -4.1587396 -4.1832833 -4.2004147 -4.2094059 -4.20411 -4.1864405 -4.1607046 -4.1512184 -4.1540737 -4.1737928 -4.2105727 -4.2455587 -4.2755113][-4.1675978 -4.1854558 -4.2162871 -4.2379727 -4.2513213 -4.2575364 -4.2511921 -4.2339654 -4.2108936 -4.19799 -4.194499 -4.2064934 -4.2292123 -4.2535896 -4.2756491][-4.2156267 -4.23127 -4.2548881 -4.2710776 -4.277535 -4.2769284 -4.2665987 -4.2484684 -4.2281089 -4.2177558 -4.2190704 -4.2333803 -4.2513971 -4.2662559 -4.2784667][-4.2355161 -4.2417603 -4.2536879 -4.2591209 -4.2557707 -4.24467 -4.228857 -4.2115765 -4.1959949 -4.1929989 -4.2042656 -4.2293878 -4.2547507 -4.2700658 -4.2790422][-4.2220125 -4.2188492 -4.2209654 -4.2168431 -4.2020788 -4.1773219 -4.15267 -4.1335344 -4.1247106 -4.139061 -4.1695237 -4.2099385 -4.2482862 -4.2690592 -4.2785931][-4.179419 -4.1670327 -4.1598892 -4.1487107 -4.1265736 -4.090127 -4.0566897 -4.0338521 -4.0308146 -4.0637822 -4.1205463 -4.1824846 -4.2366681 -4.265933 -4.2787671][-4.1460462 -4.1281185 -4.1139326 -4.09791 -4.0730047 -4.0334792 -3.997998 -3.97722 -3.9769926 -4.0158806 -4.0840397 -4.1593857 -4.2257447 -4.2652588 -4.2840333][-4.1428428 -4.1298127 -4.1154652 -4.0973191 -4.0720992 -4.037941 -4.0075316 -3.9922237 -3.9948523 -4.0301838 -4.0922031 -4.1628342 -4.2266984 -4.2688031 -4.2902675][-4.1939721 -4.1882834 -4.1797309 -4.16424 -4.1430216 -4.1169772 -4.093236 -4.0764694 -4.0716262 -4.0919065 -4.1360531 -4.187067 -4.2361746 -4.271574 -4.29157][-4.2549868 -4.2532172 -4.2512951 -4.2439556 -4.2281685 -4.2080855 -4.19062 -4.1724257 -4.1575756 -4.1636224 -4.1914654 -4.2228532 -4.2527466 -4.2765975 -4.2908363][-4.2773743 -4.2786288 -4.2821612 -4.2812281 -4.2725172 -4.259706 -4.2469139 -4.2284536 -4.2106619 -4.2096138 -4.2272992 -4.2462025 -4.2635975 -4.277545 -4.2864542][-4.2660475 -4.2683558 -4.2749753 -4.2788048 -4.2777867 -4.2744684 -4.2644606 -4.2400517 -4.2169666 -4.2150888 -4.2281313 -4.2423115 -4.25707 -4.2684526 -4.2785015][-4.2494731 -4.2505531 -4.2557635 -4.2607646 -4.2640133 -4.2628016 -4.2495294 -4.2175851 -4.1887212 -4.1905169 -4.205956 -4.222877 -4.2420287 -4.2577782 -4.2716527][-4.2428551 -4.2432537 -4.2456846 -4.2472868 -4.2492051 -4.24519 -4.2271214 -4.1898251 -4.1591911 -4.1702147 -4.1943517 -4.2168617 -4.2391872 -4.2581539 -4.2763209]]...]
INFO - root - 2017-12-07 16:04:24.952294: step 27710, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 57h:06m:59s remains)
INFO - root - 2017-12-07 16:04:31.634971: step 27720, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 58h:41m:30s remains)
INFO - root - 2017-12-07 16:04:38.411986: step 27730, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 56h:05m:09s remains)
INFO - root - 2017-12-07 16:04:45.381615: step 27740, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.653 sec/batch; 55h:15m:26s remains)
INFO - root - 2017-12-07 16:04:52.304641: step 27750, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 59h:18m:23s remains)
INFO - root - 2017-12-07 16:04:59.116604: step 27760, loss = 2.02, batch loss = 1.97 (11.1 examples/sec; 0.721 sec/batch; 61h:02m:43s remains)
INFO - root - 2017-12-07 16:05:05.915661: step 27770, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 56h:47m:26s remains)
INFO - root - 2017-12-07 16:05:12.696003: step 27780, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 53h:45m:13s remains)
INFO - root - 2017-12-07 16:05:19.529143: step 27790, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 56h:30m:26s remains)
INFO - root - 2017-12-07 16:05:26.326023: step 27800, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 60h:15m:05s remains)
2017-12-07 16:05:27.039666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2670012 -4.2647147 -4.2601957 -4.2528272 -4.2436695 -4.239985 -4.2380805 -4.2279005 -4.2136893 -4.2123322 -4.2306943 -4.2425828 -4.2347612 -4.2218909 -4.2318015][-4.27367 -4.2705307 -4.2623706 -4.2508221 -4.239913 -4.2360878 -4.237184 -4.2332973 -4.2232237 -4.2215829 -4.2367392 -4.2428341 -4.2273588 -4.2076726 -4.2184992][-4.2758064 -4.2706289 -4.2617431 -4.2510986 -4.241426 -4.2360821 -4.2390065 -4.2412205 -4.23594 -4.2313175 -4.2373385 -4.2329454 -4.2102413 -4.1871371 -4.2002769][-4.27059 -4.26355 -4.2572961 -4.2504892 -4.2406845 -4.2295871 -4.23053 -4.2400389 -4.2419438 -4.2372422 -4.2333117 -4.2163424 -4.1857662 -4.1632 -4.1816974][-4.2542911 -4.2489152 -4.2496862 -4.2477956 -4.2350416 -4.2117448 -4.2022338 -4.215704 -4.2290354 -4.2322392 -4.2247968 -4.2006359 -4.1657195 -4.1466665 -4.171392][-4.2362914 -4.2358952 -4.2460032 -4.2496748 -4.230834 -4.1895151 -4.1588373 -4.1712232 -4.2034287 -4.2242112 -4.223815 -4.2000031 -4.1633186 -4.1483736 -4.1775489][-4.2195673 -4.2206306 -4.2384906 -4.2477 -4.2236023 -4.1622806 -4.1008039 -4.1038332 -4.1616821 -4.2130313 -4.2296476 -4.2118397 -4.17472 -4.1591148 -4.1874824][-4.2041583 -4.2002225 -4.2207294 -4.2333345 -4.2033486 -4.1216621 -4.0247264 -4.0087838 -4.0896893 -4.1801376 -4.2235317 -4.2185392 -4.1831489 -4.1636329 -4.1872931][-4.1942453 -4.18202 -4.1992388 -4.2141924 -4.1867805 -4.1047573 -3.9932518 -3.9551182 -4.0353384 -4.1428351 -4.2057753 -4.2151217 -4.1856356 -4.1627145 -4.1784558][-4.1965 -4.1779671 -4.1890807 -4.2062945 -4.1929488 -4.1377592 -4.0543461 -4.0100579 -4.0552626 -4.1371264 -4.1949382 -4.2099581 -4.1886787 -4.1651678 -4.1708164][-4.2062283 -4.1836505 -4.186698 -4.2020283 -4.2008772 -4.1758232 -4.128624 -4.0917916 -4.10237 -4.1476469 -4.191411 -4.208106 -4.19286 -4.1689129 -4.16374][-4.2125378 -4.1903577 -4.1894889 -4.2022328 -4.2080774 -4.2023764 -4.1810741 -4.1535892 -4.1414156 -4.1543059 -4.1806526 -4.1977067 -4.1880093 -4.1667271 -4.1561174][-4.2118635 -4.1932273 -4.1940074 -4.2070179 -4.2181473 -4.2198505 -4.2090154 -4.1847391 -4.1608934 -4.1525712 -4.1639714 -4.1788464 -4.1711097 -4.15193 -4.1385264][-4.2096519 -4.1907363 -4.1904411 -4.2043014 -4.2185369 -4.2216997 -4.2118473 -4.1861935 -4.1567225 -4.1387057 -4.1416745 -4.1535277 -4.1492052 -4.1336775 -4.1205454][-4.2083988 -4.1846123 -4.1773672 -4.1896715 -4.2076983 -4.2154202 -4.2103291 -4.1889467 -4.1612024 -4.1405129 -4.13899 -4.1487274 -4.1484823 -4.1399345 -4.1318974]]...]
INFO - root - 2017-12-07 16:05:33.749857: step 27810, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:56m:47s remains)
INFO - root - 2017-12-07 16:05:40.266715: step 27820, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 56h:10m:49s remains)
INFO - root - 2017-12-07 16:05:46.877003: step 27830, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:36m:50s remains)
INFO - root - 2017-12-07 16:05:53.755168: step 27840, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 60h:32m:53s remains)
INFO - root - 2017-12-07 16:06:00.606285: step 27850, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 54h:04m:55s remains)
INFO - root - 2017-12-07 16:06:07.418364: step 27860, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 59h:04m:06s remains)
INFO - root - 2017-12-07 16:06:14.251170: step 27870, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 60h:29m:47s remains)
INFO - root - 2017-12-07 16:06:20.936072: step 27880, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 54h:46m:20s remains)
INFO - root - 2017-12-07 16:06:27.769005: step 27890, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 58h:37m:40s remains)
INFO - root - 2017-12-07 16:06:34.564408: step 27900, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 54h:59m:27s remains)
2017-12-07 16:06:35.274754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14335 -4.1502347 -4.1410837 -4.1277571 -4.1216955 -4.1304312 -4.1528912 -4.1848273 -4.2022586 -4.2005353 -4.1813316 -4.1536541 -4.1247759 -4.0916939 -4.0539513][-4.146647 -4.1639137 -4.1683364 -4.1678352 -4.16611 -4.1681113 -4.1825213 -4.2146277 -4.2394781 -4.2414331 -4.2204828 -4.1871009 -4.154356 -4.1225572 -4.0885515][-4.1350646 -4.1621137 -4.1832066 -4.19664 -4.1974163 -4.1915092 -4.1958919 -4.2231588 -4.2528214 -4.2611794 -4.2455974 -4.2143593 -4.1828594 -4.1527014 -4.1230226][-4.1074028 -4.1391134 -4.1723342 -4.1940236 -4.1946936 -4.1834993 -4.1792135 -4.2006741 -4.2342844 -4.2527318 -4.2483959 -4.2250223 -4.1970935 -4.1719055 -4.1505127][-4.0861163 -4.1113348 -4.1426625 -4.16047 -4.154212 -4.1354189 -4.1226697 -4.1405635 -4.1811485 -4.2123213 -4.2230673 -4.2137494 -4.1951041 -4.1811047 -4.1734886][-4.08563 -4.1002522 -4.1166043 -4.1184721 -4.095788 -4.0594668 -4.0305538 -4.0461078 -4.09862 -4.1463408 -4.1749349 -4.1793413 -4.1711826 -4.171627 -4.1816244][-4.0894403 -4.0976481 -4.0996256 -4.0848746 -4.0445566 -3.9872758 -3.9364786 -3.9495246 -4.0161762 -4.0810728 -4.1229744 -4.1340556 -4.1327553 -4.1429415 -4.1672378][-4.0881128 -4.0940466 -4.0907445 -4.0720921 -4.0322881 -3.9749432 -3.9208231 -3.9301198 -3.9938984 -4.054708 -4.0913229 -4.0971284 -4.0974441 -4.115603 -4.1494107][-4.0924478 -4.096499 -4.0935817 -4.0860591 -4.0682521 -4.0349712 -3.9934874 -3.9895442 -4.0234833 -4.0559654 -4.0726757 -4.0645437 -4.0642776 -4.0900531 -4.1312337][-4.1021576 -4.1010652 -4.0963607 -4.0968027 -4.0984721 -4.0866442 -4.0567722 -4.0392203 -4.0422058 -4.0484071 -4.0521894 -4.0392933 -4.0438089 -4.0787864 -4.1230159][-4.0994072 -4.091352 -4.0862508 -4.0948596 -4.11003 -4.1151686 -4.0939813 -4.06823 -4.052247 -4.0414863 -4.0413175 -4.0329041 -4.041903 -4.0809455 -4.1278458][-4.0951977 -4.0817084 -4.0806279 -4.0977845 -4.1246166 -4.1433649 -4.131453 -4.1050692 -4.0832105 -4.0646124 -4.0639329 -4.0586767 -4.0638742 -4.0996761 -4.1485338][-4.1092324 -4.0915556 -4.091393 -4.1120839 -4.1443386 -4.17023 -4.1662488 -4.1430912 -4.1178784 -4.0970068 -4.0997458 -4.1003284 -4.1041026 -4.13646 -4.1838317][-4.1289577 -4.1039934 -4.09834 -4.1154013 -4.1502209 -4.1826444 -4.1860342 -4.166667 -4.1409893 -4.120677 -4.1248231 -4.132751 -4.1401982 -4.1710491 -4.2129245][-4.1378522 -4.1043234 -4.0887837 -4.0995612 -4.1383786 -4.1785078 -4.1908684 -4.1808796 -4.1634383 -4.1487689 -4.1508846 -4.1605115 -4.1702433 -4.1969051 -4.2323442]]...]
INFO - root - 2017-12-07 16:06:42.118503: step 27910, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 55h:49m:42s remains)
INFO - root - 2017-12-07 16:06:48.692798: step 27920, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 56h:15m:17s remains)
INFO - root - 2017-12-07 16:06:55.575015: step 27930, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:22m:17s remains)
INFO - root - 2017-12-07 16:07:02.472617: step 27940, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 59h:21m:22s remains)
INFO - root - 2017-12-07 16:07:09.350866: step 27950, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 59h:11m:34s remains)
INFO - root - 2017-12-07 16:07:16.169751: step 27960, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 56h:41m:07s remains)
INFO - root - 2017-12-07 16:07:23.025204: step 27970, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 56h:21m:23s remains)
INFO - root - 2017-12-07 16:07:29.912679: step 27980, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 57h:36m:24s remains)
INFO - root - 2017-12-07 16:07:36.644993: step 27990, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 58h:46m:14s remains)
INFO - root - 2017-12-07 16:07:43.365833: step 28000, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 56h:28m:32s remains)
2017-12-07 16:07:44.047441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1774068 -4.1704793 -4.1631165 -4.1646571 -4.1776876 -4.1807647 -4.1777229 -4.1612735 -4.12815 -4.0996656 -4.1064415 -4.1395903 -4.1713629 -4.1968279 -4.21671][-4.185009 -4.1838651 -4.1776271 -4.1754894 -4.1801362 -4.1692748 -4.1503348 -4.1167355 -4.074966 -4.0512943 -4.0745897 -4.1192632 -4.1619315 -4.1975813 -4.2197638][-4.1998744 -4.2031689 -4.1992378 -4.1951466 -4.1918678 -4.1690216 -4.13592 -4.0893722 -4.0456505 -4.0396047 -4.0791764 -4.1352096 -4.1793032 -4.2132945 -4.2273073][-4.2264981 -4.2293391 -4.223495 -4.2153382 -4.2054744 -4.171042 -4.12506 -4.0709991 -4.0343552 -4.04834 -4.099391 -4.1616616 -4.2072325 -4.2379289 -4.2437849][-4.2445426 -4.2398667 -4.2309079 -4.2212796 -4.2071986 -4.1632929 -4.1026821 -4.0402207 -4.0179687 -4.05237 -4.1141977 -4.1811042 -4.2315831 -4.2580843 -4.2530642][-4.23752 -4.22634 -4.2157011 -4.2082057 -4.1895418 -4.135303 -4.0452132 -3.9597173 -3.9564111 -4.0260053 -4.1068077 -4.1805339 -4.2385745 -4.2646875 -4.2497706][-4.2069712 -4.1911798 -4.1796317 -4.1770587 -4.1538391 -4.0891843 -3.97042 -3.8529174 -3.8719327 -3.9836912 -4.0852051 -4.1671224 -4.232161 -4.2650175 -4.2501268][-4.1733732 -4.1594615 -4.1512785 -4.1558704 -4.1378803 -4.079546 -3.9577148 -3.8238835 -3.8502569 -3.981318 -4.0892096 -4.1720467 -4.2368031 -4.2700481 -4.258369][-4.1519437 -4.1505957 -4.15282 -4.1612382 -4.1517606 -4.1137242 -4.016747 -3.8960204 -3.9121511 -4.0284276 -4.1287513 -4.2022791 -4.2547131 -4.2769361 -4.2614794][-4.1408744 -4.1539578 -4.1685572 -4.1772823 -4.174305 -4.1516762 -4.0798388 -3.9819415 -3.9870131 -4.0848708 -4.1741037 -4.230125 -4.2606063 -4.268569 -4.2438221][-4.1318121 -4.15226 -4.1741738 -4.1792393 -4.1755953 -4.1610274 -4.1101847 -4.0318995 -4.0321555 -4.1183581 -4.1955485 -4.23649 -4.2521596 -4.2478738 -4.2179608][-4.1176467 -4.1370969 -4.1597323 -4.1626744 -4.1617765 -4.15277 -4.1191297 -4.0567422 -4.0477486 -4.1190147 -4.1844082 -4.2184725 -4.2314482 -4.22645 -4.1938214][-4.093627 -4.1113629 -4.132031 -4.137877 -4.1457181 -4.1419997 -4.1205864 -4.0710912 -4.0494609 -4.1027036 -4.1605124 -4.1926584 -4.2072473 -4.2051625 -4.1737194][-4.0889478 -4.1001167 -4.1192656 -4.1287875 -4.1392951 -4.1413813 -4.1336889 -4.1011357 -4.0754671 -4.1089268 -4.1544738 -4.1799107 -4.1928525 -4.1942215 -4.1741595][-4.1177921 -4.1079364 -4.1131516 -4.1189528 -4.1293421 -4.1395378 -4.1502981 -4.1375217 -4.11659 -4.131278 -4.1584315 -4.1763077 -4.1888518 -4.1959658 -4.1932836]]...]
INFO - root - 2017-12-07 16:07:50.770819: step 28010, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 57h:59m:12s remains)
INFO - root - 2017-12-07 16:07:57.453224: step 28020, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 59h:18m:25s remains)
INFO - root - 2017-12-07 16:08:04.393613: step 28030, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 56h:36m:24s remains)
INFO - root - 2017-12-07 16:08:11.212155: step 28040, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 54h:13m:20s remains)
INFO - root - 2017-12-07 16:08:18.143283: step 28050, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 61h:26m:52s remains)
INFO - root - 2017-12-07 16:08:25.026979: step 28060, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 57h:37m:36s remains)
INFO - root - 2017-12-07 16:08:31.831654: step 28070, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 62h:12m:16s remains)
INFO - root - 2017-12-07 16:08:38.584615: step 28080, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 55h:41m:35s remains)
INFO - root - 2017-12-07 16:08:45.372914: step 28090, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 55h:32m:47s remains)
INFO - root - 2017-12-07 16:08:52.152872: step 28100, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 58h:16m:27s remains)
2017-12-07 16:08:52.998500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2052612 -4.1947079 -4.1795578 -4.1509213 -4.1362119 -4.1464791 -4.1713624 -4.1936936 -4.2078037 -4.2205067 -4.2228308 -4.2099233 -4.182869 -4.154634 -4.1346455][-4.1921859 -4.1680951 -4.142632 -4.1143489 -4.1116586 -4.1364141 -4.171936 -4.2041864 -4.2268758 -4.2438321 -4.2434015 -4.2260842 -4.1940269 -4.1618743 -4.1380243][-4.1771784 -4.1379771 -4.0997162 -4.0726666 -4.0876966 -4.1315341 -4.1794066 -4.2210755 -4.2518096 -4.2719765 -4.2725229 -4.25677 -4.2289124 -4.2020788 -4.1806612][-4.1721535 -4.1220675 -4.0689321 -4.0406885 -4.0659795 -4.1220098 -4.1800814 -4.2294927 -4.2685471 -4.2969627 -4.3047471 -4.295877 -4.274807 -4.2524505 -4.2324953][-4.1792483 -4.1254787 -4.064784 -4.0293126 -4.0485291 -4.1019611 -4.1601896 -4.2112322 -4.2570639 -4.2968388 -4.3179951 -4.3192048 -4.3061137 -4.2883925 -4.2715321][-4.1862259 -4.1375618 -4.0790262 -4.0342784 -4.0318427 -4.0634084 -4.1065135 -4.1538777 -4.2052159 -4.2553658 -4.2921748 -4.3066559 -4.3044796 -4.2957711 -4.2870669][-4.1835427 -4.1450424 -4.0948095 -4.044436 -4.0150661 -4.0113645 -4.02773 -4.0663195 -4.11791 -4.1725974 -4.2215433 -4.2505617 -4.2608471 -4.2630019 -4.2647176][-4.182014 -4.1525455 -4.1117859 -4.0621734 -4.0151916 -3.9825144 -3.9704165 -3.9929655 -4.0348845 -4.0828218 -4.1328688 -4.1690927 -4.1882396 -4.1984625 -4.2079539][-4.1881013 -4.1662006 -4.1354647 -4.0931339 -4.0446992 -4.0023246 -3.9744949 -3.9799449 -4.0052166 -4.0379763 -4.076128 -4.1064582 -4.1257815 -4.1380868 -4.15058][-4.2025208 -4.1891375 -4.1687293 -4.1373973 -4.0983324 -4.0613317 -4.0321918 -4.0271444 -4.03725 -4.0544314 -4.0753512 -4.0919 -4.1050739 -4.1142917 -4.12672][-4.2249694 -4.2221045 -4.2130527 -4.1938977 -4.1666708 -4.1400585 -4.1166167 -4.108202 -4.1104326 -4.1154213 -4.1201668 -4.1245008 -4.1306939 -4.137208 -4.1473427][-4.2490067 -4.2541385 -4.254324 -4.2451372 -4.2301393 -4.2150903 -4.1987214 -4.1905165 -4.1889462 -4.188024 -4.1854229 -4.1829891 -4.1843657 -4.1891489 -4.1968331][-4.2740211 -4.28419 -4.2892132 -4.2863779 -4.2799764 -4.2747121 -4.2674923 -4.2609854 -4.2583261 -4.2553158 -4.2502317 -4.2454729 -4.2442012 -4.2461267 -4.2504659][-4.2952247 -4.3070216 -4.3132973 -4.313879 -4.3116212 -4.3113503 -4.3100266 -4.3074431 -4.3053789 -4.302547 -4.2974443 -4.29292 -4.291141 -4.2912908 -4.2929707][-4.3089623 -4.3193994 -4.3256016 -4.3275313 -4.3273897 -4.3284483 -4.3289318 -4.3285685 -4.3275886 -4.3249464 -4.3210006 -4.3178544 -4.3166919 -4.31696 -4.3180404]]...]
INFO - root - 2017-12-07 16:08:59.749109: step 28110, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 53h:35m:05s remains)
INFO - root - 2017-12-07 16:09:06.323046: step 28120, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 55h:07m:12s remains)
INFO - root - 2017-12-07 16:09:13.144083: step 28130, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 55h:46m:54s remains)
INFO - root - 2017-12-07 16:09:19.787477: step 28140, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 57h:58m:57s remains)
INFO - root - 2017-12-07 16:09:26.622920: step 28150, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 55h:04m:58s remains)
INFO - root - 2017-12-07 16:09:33.362787: step 28160, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 56h:13m:08s remains)
INFO - root - 2017-12-07 16:09:40.155694: step 28170, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 58h:29m:18s remains)
INFO - root - 2017-12-07 16:09:46.880560: step 28180, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 56h:27m:52s remains)
INFO - root - 2017-12-07 16:09:53.568160: step 28190, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 57h:47m:18s remains)
INFO - root - 2017-12-07 16:10:00.255534: step 28200, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 55h:48m:45s remains)
2017-12-07 16:10:00.955875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2663507 -4.2762332 -4.2837958 -4.2861028 -4.2823715 -4.25923 -4.2323494 -4.2106953 -4.2055869 -4.227808 -4.2553015 -4.2675138 -4.2522268 -4.2274408 -4.2121716][-4.2631617 -4.277575 -4.2922082 -4.2983961 -4.2855473 -4.2509179 -4.2121053 -4.176652 -4.1630778 -4.1858659 -4.2243218 -4.2566948 -4.2559247 -4.2391467 -4.2300282][-4.2888203 -4.307117 -4.3258266 -4.3277245 -4.2961645 -4.2417512 -4.1854 -4.1365709 -4.1197724 -4.1455669 -4.1962109 -4.2503986 -4.2665796 -4.2618318 -4.2595534][-4.3244152 -4.3445048 -4.362298 -4.3544545 -4.3075709 -4.2400718 -4.1674333 -4.1026773 -4.0783963 -4.1099911 -4.17637 -4.2466669 -4.277308 -4.2835073 -4.2863564][-4.3613086 -4.3789811 -4.3845096 -4.3588963 -4.2989488 -4.2201681 -4.12698 -4.04508 -4.0258808 -4.0842619 -4.169065 -4.2460179 -4.2845044 -4.2995434 -4.3076844][-4.3899417 -4.4005084 -4.3862753 -4.3375258 -4.259481 -4.1575251 -4.0365405 -3.9417238 -3.9473066 -4.047235 -4.1503253 -4.2321653 -4.279717 -4.3029408 -4.31775][-4.3974452 -4.3941479 -4.3576922 -4.2853408 -4.1873121 -4.0586767 -3.9172442 -3.8332224 -3.8822961 -4.0192003 -4.1319332 -4.2136407 -4.2684212 -4.3000307 -4.3193145][-4.3802876 -4.36224 -4.3101749 -4.2229128 -4.1158142 -3.9788713 -3.8416271 -3.7933693 -3.8807855 -4.0238342 -4.1278486 -4.200912 -4.2553906 -4.2912683 -4.3127909][-4.3506317 -4.32486 -4.2716212 -4.1892829 -4.0959954 -3.9808373 -3.8733492 -3.8589473 -3.952311 -4.0742364 -4.1576843 -4.2153158 -4.2593436 -4.2876573 -4.3048363][-4.3257389 -4.302072 -4.2595887 -4.1946354 -4.1290078 -4.0489206 -3.9745028 -3.9727581 -4.0479236 -4.1415281 -4.2059255 -4.250041 -4.2812495 -4.2954731 -4.3005657][-4.306663 -4.2872896 -4.2581205 -4.2131114 -4.1729174 -4.1232686 -4.0737338 -4.0692286 -4.1191511 -4.1846242 -4.2342143 -4.2702746 -4.2915516 -4.2937078 -4.28758][-4.2808614 -4.2655511 -4.2472057 -4.2219696 -4.203618 -4.1780324 -4.1454754 -4.1348796 -4.1603007 -4.2024732 -4.23808 -4.2665925 -4.2789688 -4.2750707 -4.2657719][-4.2597227 -4.2482228 -4.2363954 -4.225431 -4.2240939 -4.2187128 -4.19835 -4.1840191 -4.191453 -4.2146654 -4.2372236 -4.2577782 -4.2644081 -4.2598324 -4.2530909][-4.2608776 -4.2536721 -4.243937 -4.2369447 -4.2425737 -4.2506123 -4.2429662 -4.2310658 -4.2285957 -4.2367873 -4.2499156 -4.265213 -4.2710109 -4.2683287 -4.2639689][-4.2756624 -4.2717152 -4.2649379 -4.2585883 -4.2629638 -4.2729974 -4.2733407 -4.2657228 -4.2613168 -4.2635355 -4.271759 -4.282495 -4.2890162 -4.2901587 -4.2885761]]...]
INFO - root - 2017-12-07 16:10:07.628844: step 28210, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 57h:43m:40s remains)
INFO - root - 2017-12-07 16:10:14.262045: step 28220, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:16m:07s remains)
INFO - root - 2017-12-07 16:10:21.002867: step 28230, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 55h:47m:30s remains)
INFO - root - 2017-12-07 16:10:27.872569: step 28240, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 55h:16m:32s remains)
INFO - root - 2017-12-07 16:10:34.586295: step 28250, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 57h:48m:14s remains)
INFO - root - 2017-12-07 16:10:41.523297: step 28260, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 61h:11m:31s remains)
INFO - root - 2017-12-07 16:10:48.333810: step 28270, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 58h:14m:44s remains)
INFO - root - 2017-12-07 16:10:55.021944: step 28280, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 52h:50m:13s remains)
INFO - root - 2017-12-07 16:11:02.059927: step 28290, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 59h:36m:28s remains)
INFO - root - 2017-12-07 16:11:08.896275: step 28300, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 57h:37m:53s remains)
2017-12-07 16:11:09.660130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3151894 -4.31639 -4.3107762 -4.2961841 -4.2702579 -4.2362576 -4.2021542 -4.1846771 -4.1780543 -4.19004 -4.2105112 -4.2244153 -4.2251811 -4.2141094 -4.1978512][-4.3193469 -4.3163853 -4.3026977 -4.2797861 -4.2485132 -4.2096953 -4.1709247 -4.1456513 -4.1417851 -4.1682329 -4.2017531 -4.2189827 -4.2192855 -4.21305 -4.1961975][-4.3185863 -4.3094788 -4.2880077 -4.25557 -4.214653 -4.1691384 -4.1238327 -4.0933194 -4.1031051 -4.1462426 -4.1852884 -4.2041416 -4.2050581 -4.204031 -4.1901979][-4.31312 -4.3008494 -4.274004 -4.2350397 -4.1860375 -4.131062 -4.0752883 -4.0426636 -4.0747509 -4.1344261 -4.1773109 -4.20056 -4.2040296 -4.2041578 -4.1918221][-4.3127527 -4.3004651 -4.2722697 -4.2299428 -4.1728997 -4.1047935 -4.0320582 -3.9949207 -4.0499396 -4.1273036 -4.179966 -4.2165775 -4.2272072 -4.2255716 -4.2132034][-4.3125134 -4.30027 -4.2698984 -4.2214217 -4.1519322 -4.0652318 -3.9635811 -3.9134984 -3.9982214 -4.1074605 -4.1807337 -4.2350035 -4.2548285 -4.2545495 -4.2445965][-4.3068018 -4.2944145 -4.2599869 -4.2029862 -4.1212149 -4.0156174 -3.8906968 -3.8399346 -3.9602878 -4.0964661 -4.1840453 -4.2454858 -4.2752271 -4.2805085 -4.277442][-4.3054132 -4.29079 -4.2511368 -4.1869445 -4.1042728 -4.0010118 -3.8938498 -3.8753552 -3.9913397 -4.1093493 -4.1867733 -4.2428479 -4.2760406 -4.29226 -4.300529][-4.3050165 -4.2917027 -4.2508917 -4.18874 -4.1173415 -4.031363 -3.952929 -3.9555874 -4.0445266 -4.1319289 -4.1947641 -4.2400537 -4.2687354 -4.2896485 -4.3114295][-4.3099031 -4.3000517 -4.2638431 -4.2087927 -4.1499133 -4.0818963 -4.024128 -4.0371785 -4.1045871 -4.1653037 -4.2139897 -4.2497749 -4.2727656 -4.2900586 -4.3126688][-4.3201218 -4.314868 -4.2864838 -4.2420874 -4.198772 -4.1517138 -4.1169848 -4.1329942 -4.1776328 -4.2121453 -4.2420235 -4.2676053 -4.28621 -4.2982683 -4.3118334][-4.3236303 -4.3203931 -4.2995343 -4.2678943 -4.2396278 -4.2137671 -4.1984911 -4.2123671 -4.23588 -4.2488713 -4.2613993 -4.27872 -4.2950754 -4.3046865 -4.3104277][-4.3183494 -4.3147144 -4.3000035 -4.2798305 -4.2631335 -4.2533355 -4.2517819 -4.2610674 -4.2689495 -4.268261 -4.2726345 -4.2838459 -4.2955737 -4.3048406 -4.30853][-4.3180046 -4.3133788 -4.3024697 -4.2878385 -4.2775412 -4.27519 -4.2781549 -4.2821016 -4.283669 -4.2827315 -4.2863259 -4.29347 -4.3007274 -4.3084064 -4.3126445][-4.3236933 -4.3176956 -4.3084273 -4.2973633 -4.291327 -4.2913127 -4.2939191 -4.2946091 -4.2955832 -4.2981706 -4.3028 -4.3069153 -4.3105369 -4.3154383 -4.3194227]]...]
INFO - root - 2017-12-07 16:11:16.335464: step 28310, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 52h:48m:22s remains)
INFO - root - 2017-12-07 16:11:23.013141: step 28320, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 60h:00m:02s remains)
INFO - root - 2017-12-07 16:11:29.905900: step 28330, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 60h:28m:42s remains)
INFO - root - 2017-12-07 16:11:36.691105: step 28340, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 59h:34m:52s remains)
INFO - root - 2017-12-07 16:11:43.552970: step 28350, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 55h:00m:42s remains)
INFO - root - 2017-12-07 16:11:50.376321: step 28360, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 55h:59m:35s remains)
INFO - root - 2017-12-07 16:11:57.223682: step 28370, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 59h:09m:49s remains)
INFO - root - 2017-12-07 16:12:03.999429: step 28380, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:29m:25s remains)
INFO - root - 2017-12-07 16:12:10.810943: step 28390, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 62h:34m:50s remains)
INFO - root - 2017-12-07 16:12:17.643693: step 28400, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 57h:54m:05s remains)
2017-12-07 16:12:18.396256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3389325 -4.3403983 -4.3351583 -4.3248858 -4.3102064 -4.29974 -4.3000441 -4.3034625 -4.3026152 -4.2966661 -4.2844982 -4.2695537 -4.2594147 -4.2577338 -4.2625523][-4.3343325 -4.3381257 -4.3332376 -4.3195076 -4.2994051 -4.2861047 -4.2885723 -4.2981911 -4.3053746 -4.3077455 -4.3016725 -4.2875719 -4.273489 -4.2635202 -4.2582369][-4.3224058 -4.3247514 -4.3160915 -4.294981 -4.2656364 -4.2456365 -4.2464304 -4.2594461 -4.275908 -4.2891221 -4.2942963 -4.2890377 -4.2813115 -4.2739844 -4.267715][-4.3084555 -4.3037615 -4.2844691 -4.2496538 -4.2071915 -4.176795 -4.1722441 -4.1863117 -4.2121425 -4.2380304 -4.2563162 -4.2643204 -4.270113 -4.2755003 -4.2798305][-4.29542 -4.2843995 -4.2540894 -4.207315 -4.1534157 -4.1105223 -4.0930705 -4.1002879 -4.1286798 -4.1606069 -4.1888547 -4.2127094 -4.2351446 -4.2567873 -4.2768488][-4.2883329 -4.2779937 -4.24511 -4.1954889 -4.1359329 -4.0794668 -4.03924 -4.0262618 -4.0423689 -4.0686769 -4.0997005 -4.1369104 -4.1762967 -4.2129483 -4.2483382][-4.2791796 -4.2760978 -4.2481441 -4.2030535 -4.144722 -4.0812483 -4.023634 -3.99136 -3.9924245 -4.0089941 -4.0370479 -4.0804958 -4.129303 -4.172719 -4.2156763][-4.2671256 -4.276031 -4.2594357 -4.2234826 -4.1749492 -4.1207924 -4.0675154 -4.0355196 -4.0315018 -4.0428519 -4.0642462 -4.1014967 -4.14219 -4.1757483 -4.2113047][-4.264492 -4.2852669 -4.2830687 -4.2605309 -4.2295666 -4.1962271 -4.15994 -4.1362028 -4.130548 -4.1341014 -4.1407018 -4.1613035 -4.1859283 -4.2060714 -4.2294703][-4.2741351 -4.3015223 -4.311491 -4.3039942 -4.2909069 -4.2757192 -4.251225 -4.2279873 -4.2151012 -4.2073851 -4.1991525 -4.2043796 -4.2173743 -4.2301149 -4.2462173][-4.2917123 -4.315537 -4.3314791 -4.336081 -4.3369346 -4.3348031 -4.3203959 -4.29786 -4.2777147 -4.2583051 -4.2372751 -4.2274966 -4.230947 -4.241425 -4.254261][-4.3075061 -4.3225527 -4.3369775 -4.346138 -4.3537631 -4.3588023 -4.3531733 -4.3357406 -4.3132291 -4.2850909 -4.25443 -4.2311888 -4.2236843 -4.2324271 -4.2448735][-4.318923 -4.3245311 -4.3335185 -4.3411756 -4.3485317 -4.3541937 -4.3521924 -4.3418808 -4.3220139 -4.2918286 -4.2570295 -4.2240624 -4.2047186 -4.2078443 -4.2181196][-4.3234954 -4.3202333 -4.3206625 -4.3227186 -4.3254423 -4.3266883 -4.3242621 -4.3193526 -4.3053412 -4.279007 -4.2468739 -4.2145662 -4.1917167 -4.1903338 -4.1988583][-4.3195052 -4.3081951 -4.298737 -4.291801 -4.286325 -4.2798572 -4.2742109 -4.2710166 -4.2637472 -4.24677 -4.2273116 -4.2082057 -4.194119 -4.1942787 -4.20208]]...]
INFO - root - 2017-12-07 16:12:25.117077: step 28410, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 60h:42m:15s remains)
INFO - root - 2017-12-07 16:12:31.666859: step 28420, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 59h:14m:55s remains)
INFO - root - 2017-12-07 16:12:38.395040: step 28430, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:48m:31s remains)
INFO - root - 2017-12-07 16:12:45.338298: step 28440, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 55h:43m:39s remains)
INFO - root - 2017-12-07 16:12:52.078599: step 28450, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 57h:04m:44s remains)
INFO - root - 2017-12-07 16:12:58.836345: step 28460, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 55h:42m:25s remains)
INFO - root - 2017-12-07 16:13:05.686808: step 28470, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 55h:58m:46s remains)
INFO - root - 2017-12-07 16:13:12.451838: step 28480, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 56h:47m:16s remains)
INFO - root - 2017-12-07 16:13:19.197309: step 28490, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 59h:22m:46s remains)
INFO - root - 2017-12-07 16:13:25.904794: step 28500, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 55h:43m:06s remains)
2017-12-07 16:13:26.629515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2933655 -4.2845349 -4.28798 -4.2995157 -4.3076096 -4.3085194 -4.3060641 -4.3023295 -4.2938361 -4.28276 -4.2819672 -4.2858162 -4.2817035 -4.281383 -4.2953138][-4.2897983 -4.2765722 -4.2773733 -4.28704 -4.2929907 -4.2903509 -4.2815313 -4.27343 -4.2621646 -4.2481766 -4.2508464 -4.2626228 -4.2636657 -4.2657413 -4.2816005][-4.2797008 -4.2638345 -4.2624397 -4.2709856 -4.2760143 -4.2699995 -4.2536192 -4.2404532 -4.2263875 -4.2102461 -4.2212305 -4.2457128 -4.2563205 -4.2628117 -4.2795792][-4.2648568 -4.2441754 -4.2378721 -4.2440553 -4.2454977 -4.2335291 -4.21244 -4.20148 -4.1885729 -4.1745105 -4.188457 -4.2179794 -4.238905 -4.2564549 -4.2784982][-4.243331 -4.2161093 -4.2042866 -4.2090993 -4.2060084 -4.1812983 -4.1522169 -4.1470685 -4.1402745 -4.1310625 -4.1437941 -4.1720362 -4.2003531 -4.2321768 -4.2638836][-4.21093 -4.1786222 -4.161056 -4.161819 -4.1491709 -4.10304 -4.0619979 -4.0714273 -4.0766988 -4.0735617 -4.0869336 -4.1119313 -4.1452141 -4.1902723 -4.2342186][-4.1743593 -4.13355 -4.104085 -4.0886054 -4.054987 -3.9829929 -3.9312491 -3.9578407 -3.9776888 -3.9899368 -4.0163636 -4.05782 -4.1063175 -4.1651034 -4.2156553][-4.1530213 -4.0981569 -4.0515618 -4.0128264 -3.9501054 -3.8539743 -3.8039505 -3.8515429 -3.8982859 -3.941117 -3.9939084 -4.0593834 -4.1254525 -4.1898427 -4.2374425][-4.1561575 -4.0959311 -4.0444832 -4.000299 -3.936074 -3.8523045 -3.8194108 -3.8796613 -3.9468191 -4.0076294 -4.0682163 -4.1314406 -4.1920776 -4.2473264 -4.2814126][-4.1935916 -4.14591 -4.1106873 -4.08146 -4.0376592 -3.9838431 -3.9669247 -4.0154285 -4.0735216 -4.1276975 -4.1779408 -4.2244687 -4.2689805 -4.3061008 -4.3245597][-4.2348394 -4.2110076 -4.1993861 -4.182785 -4.1538405 -4.1227384 -4.1136255 -4.1468196 -4.1890407 -4.228272 -4.2632561 -4.2939386 -4.3204155 -4.3395844 -4.3472743][-4.2556224 -4.2478051 -4.2548132 -4.2519226 -4.2363734 -4.2233214 -4.2207227 -4.2404413 -4.265542 -4.2872152 -4.3065572 -4.3226194 -4.33811 -4.3474522 -4.3496757][-4.2682271 -4.2622437 -4.274333 -4.2827306 -4.2791781 -4.2765961 -4.2796836 -4.2898979 -4.3029823 -4.3158708 -4.326973 -4.3352833 -4.3424387 -4.3473577 -4.3484359][-4.2892833 -4.2844071 -4.2943344 -4.3052669 -4.3107953 -4.3145685 -4.3196263 -4.3244414 -4.3294821 -4.3341784 -4.3378787 -4.3399034 -4.3421755 -4.3454471 -4.347734][-4.3153386 -4.3113456 -4.3162322 -4.3243217 -4.3306289 -4.3351712 -4.339 -4.3412952 -4.3417621 -4.3422718 -4.3433881 -4.3438282 -4.3448496 -4.3478312 -4.3510561]]...]
INFO - root - 2017-12-07 16:13:33.393119: step 28510, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 59h:15m:37s remains)
INFO - root - 2017-12-07 16:13:40.065887: step 28520, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 59h:02m:36s remains)
INFO - root - 2017-12-07 16:13:46.822932: step 28530, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 57h:14m:23s remains)
INFO - root - 2017-12-07 16:13:53.711461: step 28540, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 58h:27m:43s remains)
INFO - root - 2017-12-07 16:14:00.366389: step 28550, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 55h:20m:13s remains)
INFO - root - 2017-12-07 16:14:07.164252: step 28560, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 55h:28m:48s remains)
INFO - root - 2017-12-07 16:14:13.950977: step 28570, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 59h:01m:12s remains)
INFO - root - 2017-12-07 16:14:20.685103: step 28580, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 56h:56m:44s remains)
INFO - root - 2017-12-07 16:14:27.543067: step 28590, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 57h:23m:16s remains)
INFO - root - 2017-12-07 16:14:34.339122: step 28600, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 58h:00m:25s remains)
2017-12-07 16:14:35.031123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2339349 -4.2086492 -4.1776838 -4.1613393 -4.1683221 -4.1793633 -4.1835108 -4.1756082 -4.1652355 -4.1553435 -4.1463261 -4.1492381 -4.1612163 -4.1702337 -4.1705408][-4.200531 -4.1770234 -4.1370096 -4.1149507 -4.1235132 -4.1375484 -4.1479421 -4.1447387 -4.1262407 -4.1084452 -4.1012034 -4.1077113 -4.1243625 -4.1346593 -4.1308784][-4.1862864 -4.1737061 -4.1348944 -4.1133118 -4.1223822 -4.1354022 -4.1473064 -4.1424627 -4.1137662 -4.0889792 -4.0874338 -4.1002874 -4.1178355 -4.124352 -4.113553][-4.1943479 -4.1906195 -4.1636081 -4.1467228 -4.1512146 -4.153584 -4.1573949 -4.1443491 -4.1091752 -4.0876718 -4.1012893 -4.1284089 -4.1500635 -4.151897 -4.12914][-4.1773524 -4.1773987 -4.1607938 -4.15367 -4.1668038 -4.1629386 -4.1510434 -4.117496 -4.0708575 -4.0476117 -4.0818515 -4.1320028 -4.1628294 -4.1660514 -4.1398745][-4.1554446 -4.1558371 -4.1436014 -4.14874 -4.1701241 -4.1637306 -4.1328044 -4.0688577 -3.9871233 -3.9503403 -4.0147805 -4.1026654 -4.1550508 -4.1688628 -4.1461754][-4.1251359 -4.12832 -4.1225557 -4.1321526 -4.1543326 -4.1506004 -4.1118507 -4.0259709 -3.9067755 -3.8483877 -3.9344063 -4.0485649 -4.1182637 -4.1480308 -4.1386919][-4.1157284 -4.1269608 -4.1241226 -4.1312585 -4.1500845 -4.148416 -4.1210966 -4.0575805 -3.9575963 -3.9068496 -3.9712362 -4.0597134 -4.1186876 -4.1467767 -4.1444421][-4.1310854 -4.1493773 -4.152988 -4.1570468 -4.1728477 -4.1731167 -4.1551361 -4.1186237 -4.0610118 -4.0362349 -4.0750623 -4.1280851 -4.1665492 -4.1838441 -4.179997][-4.1405468 -4.1616788 -4.1711636 -4.1797981 -4.19616 -4.1992407 -4.1858082 -4.158515 -4.1240172 -4.1099648 -4.1322217 -4.1668181 -4.1949654 -4.2067871 -4.2052488][-4.1411939 -4.1550822 -4.1629534 -4.1701674 -4.1902461 -4.2050433 -4.2017984 -4.1788797 -4.1468358 -4.1310496 -4.1429038 -4.1742258 -4.2025828 -4.2151937 -4.2181778][-4.1407795 -4.1434417 -4.1531124 -4.1633892 -4.1811304 -4.1948752 -4.1946034 -4.1748457 -4.1418347 -4.1258364 -4.1430526 -4.1807642 -4.213829 -4.2248206 -4.2261477][-4.1321268 -4.127739 -4.1473374 -4.16512 -4.1770816 -4.1864157 -4.1868181 -4.1619225 -4.1215777 -4.1062655 -4.129457 -4.1747975 -4.2125 -4.225316 -4.2226505][-4.115633 -4.1008754 -4.1212649 -4.1447105 -4.1616311 -4.1726117 -4.17472 -4.14996 -4.1099019 -4.0961218 -4.114677 -4.1586266 -4.2012019 -4.2178307 -4.2125769][-4.091073 -4.072 -4.0926633 -4.1254039 -4.1552258 -4.1733871 -4.1722536 -4.1453929 -4.1093011 -4.0965648 -4.1080623 -4.141932 -4.18279 -4.20154 -4.199255]]...]
INFO - root - 2017-12-07 16:14:41.833056: step 28610, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 57h:34m:20s remains)
INFO - root - 2017-12-07 16:14:48.498989: step 28620, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.689 sec/batch; 58h:09m:47s remains)
INFO - root - 2017-12-07 16:14:55.221738: step 28630, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 53h:34m:48s remains)
INFO - root - 2017-12-07 16:15:01.986441: step 28640, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 57h:12m:45s remains)
INFO - root - 2017-12-07 16:15:08.802277: step 28650, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 59h:37m:23s remains)
INFO - root - 2017-12-07 16:15:15.595986: step 28660, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 57h:04m:49s remains)
INFO - root - 2017-12-07 16:15:22.291201: step 28670, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 52h:27m:22s remains)
INFO - root - 2017-12-07 16:15:29.020834: step 28680, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.632 sec/batch; 53h:22m:18s remains)
INFO - root - 2017-12-07 16:15:35.816523: step 28690, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 55h:55m:06s remains)
INFO - root - 2017-12-07 16:15:42.686745: step 28700, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 60h:58m:42s remains)
2017-12-07 16:15:43.480589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0448585 -4.0929017 -4.1295033 -4.1479297 -4.1543913 -4.1682858 -4.1786485 -4.189157 -4.2113132 -4.23092 -4.22028 -4.1858053 -4.1686249 -4.20005 -4.2449532][-4.0700274 -4.1238165 -4.1521883 -4.1615887 -4.1581254 -4.1574163 -4.1670418 -4.1878085 -4.2205319 -4.2450104 -4.240212 -4.213378 -4.1974955 -4.2167268 -4.2462645][-4.0868559 -4.1474724 -4.1675987 -4.1678996 -4.1569185 -4.1432476 -4.1463137 -4.1701131 -4.205514 -4.2340713 -4.2422223 -4.231214 -4.2263651 -4.2430096 -4.2643309][-4.0916305 -4.1543117 -4.1776114 -4.1759043 -4.1611452 -4.1323032 -4.1169696 -4.1288371 -4.163744 -4.20479 -4.23184 -4.2372789 -4.24474 -4.2645178 -4.2852054][-4.1075168 -4.1620255 -4.1924934 -4.197958 -4.1796021 -4.1320319 -4.0873489 -4.070971 -4.098897 -4.1539111 -4.1996932 -4.2223082 -4.2417393 -4.2645597 -4.2829552][-4.1303797 -4.181128 -4.2219248 -4.2355561 -4.2095647 -4.1392407 -4.0564456 -4.0028768 -4.0180259 -4.0844512 -4.1539655 -4.1987267 -4.23087 -4.2549982 -4.265636][-4.148695 -4.1990361 -4.24673 -4.2617407 -4.2265177 -4.1360388 -4.0169358 -3.9250362 -3.9221783 -3.9988458 -4.0950036 -4.1642628 -4.2070193 -4.230454 -4.23517][-4.1685042 -4.2093387 -4.2524648 -4.2583885 -4.2110744 -4.1028175 -3.9556613 -3.8367198 -3.8318951 -3.9263604 -4.0493236 -4.1362319 -4.1805854 -4.1968818 -4.1892228][-4.1866312 -4.21371 -4.2439861 -4.23815 -4.1814222 -4.0645514 -3.9066825 -3.7841771 -3.7907887 -3.9062865 -4.0443053 -4.1308241 -4.1661491 -4.1693606 -4.1478128][-4.1996069 -4.2159619 -4.2308717 -4.2175322 -4.1670609 -4.0629773 -3.9289036 -3.8269339 -3.8376598 -3.9417465 -4.0611563 -4.1338434 -4.1545296 -4.1429882 -4.1106529][-4.2037811 -4.2061539 -4.2064881 -4.1971045 -4.1676116 -4.1019588 -4.0165949 -3.9398978 -3.9367428 -4.0024834 -4.0861754 -4.1305423 -4.1368046 -4.1218071 -4.0905933][-4.1951327 -4.17928 -4.1749487 -4.1802197 -4.1792741 -4.1520429 -4.1058044 -4.0538716 -4.0321484 -4.0570192 -4.1060357 -4.1251469 -4.1268516 -4.1234541 -4.10696][-4.1709704 -4.1382356 -4.1291523 -4.1481347 -4.1720304 -4.1758022 -4.1611543 -4.1292577 -4.0943518 -4.0870814 -4.10847 -4.1166506 -4.1242595 -4.1410103 -4.1468892][-4.1316533 -4.0842171 -4.074141 -4.1083379 -4.1531162 -4.1768508 -4.1776223 -4.1553431 -4.1152205 -4.0906677 -4.0996194 -4.1041508 -4.12283 -4.1570206 -4.1838121][-4.0896807 -4.0411587 -4.0380912 -4.08597 -4.144722 -4.1769948 -4.1822486 -4.1625686 -4.1209393 -4.082027 -4.0829349 -4.0842071 -4.1107903 -4.1562076 -4.1977844]]...]
INFO - root - 2017-12-07 16:15:50.220580: step 28710, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 54h:20m:03s remains)
INFO - root - 2017-12-07 16:15:56.875826: step 28720, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 59h:52m:15s remains)
INFO - root - 2017-12-07 16:16:03.638471: step 28730, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 61h:10m:11s remains)
INFO - root - 2017-12-07 16:16:10.457912: step 28740, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 59h:43m:44s remains)
INFO - root - 2017-12-07 16:16:17.289189: step 28750, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 58h:55m:20s remains)
INFO - root - 2017-12-07 16:16:24.125093: step 28760, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 51h:46m:49s remains)
INFO - root - 2017-12-07 16:16:30.912528: step 28770, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 61h:15m:10s remains)
INFO - root - 2017-12-07 16:16:37.771871: step 28780, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 59h:12m:26s remains)
INFO - root - 2017-12-07 16:16:44.487841: step 28790, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 52h:38m:54s remains)
INFO - root - 2017-12-07 16:16:51.274625: step 28800, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 54h:50m:18s remains)
2017-12-07 16:16:51.953723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2372842 -4.2290416 -4.2293758 -4.2455616 -4.2654572 -4.2790422 -4.28407 -4.2816129 -4.2657223 -4.2475967 -4.2357988 -4.2342606 -4.2330737 -4.2179132 -4.2079773][-4.23761 -4.2312317 -4.2358146 -4.2545123 -4.2740345 -4.2816944 -4.2799149 -4.2750888 -4.2585759 -4.2393975 -4.2280488 -4.2291174 -4.2328324 -4.2181673 -4.2051778][-4.2363286 -4.2323322 -4.2375765 -4.2501955 -4.2616653 -4.2617946 -4.2533026 -4.2468672 -4.2314129 -4.2144003 -4.2098169 -4.2187538 -4.2296529 -4.213439 -4.1925383][-4.236412 -4.2332959 -4.2350626 -4.2404814 -4.2404938 -4.2274613 -4.2080021 -4.1958342 -4.1817946 -4.1704454 -4.1753154 -4.1985741 -4.2203488 -4.2067504 -4.1824508][-4.238452 -4.2345357 -4.2270937 -4.2207894 -4.2090569 -4.1858959 -4.1551695 -4.1363873 -4.125402 -4.1200743 -4.1351838 -4.1710644 -4.2079906 -4.2053323 -4.1806169][-4.2383018 -4.2329583 -4.2214947 -4.2093458 -4.1877184 -4.1568713 -4.1166992 -4.0914135 -4.0788341 -4.0765009 -4.0993705 -4.1463628 -4.1965132 -4.2053871 -4.1853995][-4.2298932 -4.2222462 -4.2164941 -4.2051244 -4.1834111 -4.1501665 -4.1042938 -4.0741744 -4.0611849 -4.0582323 -4.0779829 -4.1282821 -4.1843185 -4.2029967 -4.189568][-4.2103858 -4.2021937 -4.2045226 -4.202641 -4.1933513 -4.1682577 -4.1320791 -4.10274 -4.09168 -4.0878468 -4.096251 -4.1339784 -4.1809988 -4.197875 -4.18366][-4.197742 -4.1857672 -4.1919203 -4.2014794 -4.2087374 -4.1965623 -4.1745329 -4.1527953 -4.1446481 -4.1390457 -4.1339731 -4.1536293 -4.1841288 -4.1900597 -4.1675358][-4.199038 -4.1793242 -4.1837621 -4.1980395 -4.2158403 -4.212606 -4.1964512 -4.1808434 -4.1787372 -4.1734219 -4.162899 -4.1719742 -4.1895509 -4.1840315 -4.1539283][-4.2076669 -4.181633 -4.1818767 -4.1931262 -4.20875 -4.20674 -4.1946216 -4.1862779 -4.1881971 -4.1838665 -4.1727567 -4.17704 -4.1910191 -4.1828756 -4.1559806][-4.2228441 -4.1966596 -4.1941991 -4.196353 -4.1979513 -4.1933193 -4.1849661 -4.1820464 -4.1890283 -4.1871376 -4.1779757 -4.1776881 -4.1871052 -4.1794968 -4.1620755][-4.250164 -4.2283349 -4.2270479 -4.2206964 -4.210402 -4.1994634 -4.1903963 -4.1890535 -4.1948752 -4.1930461 -4.1885419 -4.1893663 -4.1959691 -4.1879535 -4.1778703][-4.2741737 -4.2609062 -4.2633896 -4.2562189 -4.24333 -4.2349787 -4.2261848 -4.2229095 -4.2253246 -4.2222486 -4.2203574 -4.2209659 -4.2222977 -4.2115054 -4.2027044][-4.2880359 -4.2843666 -4.2904792 -4.2883067 -4.2779622 -4.2707615 -4.2629704 -4.2599759 -4.2609057 -4.2557158 -4.2533922 -4.2547755 -4.2550731 -4.2442937 -4.2353921]]...]
INFO - root - 2017-12-07 16:16:58.744547: step 28810, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 57h:47m:06s remains)
INFO - root - 2017-12-07 16:17:05.402603: step 28820, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 54h:37m:01s remains)
INFO - root - 2017-12-07 16:17:12.347006: step 28830, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 55h:13m:02s remains)
INFO - root - 2017-12-07 16:17:19.275541: step 28840, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 58h:11m:34s remains)
INFO - root - 2017-12-07 16:17:26.055277: step 28850, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:30m:59s remains)
INFO - root - 2017-12-07 16:17:32.855795: step 28860, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 58h:36m:27s remains)
INFO - root - 2017-12-07 16:17:39.564698: step 28870, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 53h:05m:56s remains)
INFO - root - 2017-12-07 16:17:46.336625: step 28880, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 53h:50m:30s remains)
INFO - root - 2017-12-07 16:17:53.100402: step 28890, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 58h:59m:06s remains)
INFO - root - 2017-12-07 16:17:59.921662: step 28900, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 59h:47m:54s remains)
2017-12-07 16:18:00.604812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1832786 -4.1734209 -4.2035036 -4.2308474 -4.2376494 -4.213922 -4.1868587 -4.1890321 -4.2110829 -4.2227526 -4.2307653 -4.2269592 -4.2128792 -4.210341 -4.2102456][-4.2174869 -4.2018824 -4.2194309 -4.237721 -4.2379365 -4.2105131 -4.1820688 -4.1800814 -4.1996517 -4.206553 -4.2115641 -4.2138147 -4.2077179 -4.2072196 -4.20703][-4.2430525 -4.2205839 -4.2204447 -4.2257133 -4.2185812 -4.194653 -4.17303 -4.1723194 -4.1844258 -4.1866641 -4.1903224 -4.1955819 -4.19611 -4.2010474 -4.2021456][-4.25477 -4.2310324 -4.21765 -4.2093391 -4.195446 -4.1758771 -4.1611509 -4.1599655 -4.1655583 -4.1643558 -4.1668205 -4.170692 -4.175458 -4.1891241 -4.1940265][-4.2445703 -4.2256613 -4.2071662 -4.1885157 -4.168118 -4.150373 -4.1422606 -4.1415939 -4.1428308 -4.139328 -4.1405916 -4.14567 -4.1549306 -4.1743455 -4.1795535][-4.2122712 -4.2029972 -4.1872668 -4.1615791 -4.1345997 -4.1194425 -4.116375 -4.115335 -4.1158438 -4.1107168 -4.10822 -4.1116691 -4.1237607 -4.1491609 -4.1562586][-4.1822381 -4.1867571 -4.1808729 -4.15241 -4.112154 -4.0885925 -4.0824265 -4.0853705 -4.0898395 -4.0858154 -4.0800772 -4.0816107 -4.0969014 -4.1316061 -4.1466918][-4.1891828 -4.2009535 -4.2021976 -4.1700554 -4.1104789 -4.0674582 -4.0510316 -4.0617895 -4.0818992 -4.0874066 -4.0844216 -4.0888147 -4.1091018 -4.1503677 -4.1693344][-4.2230897 -4.2345638 -4.2364383 -4.2002006 -4.1315255 -4.0808249 -4.0629992 -4.0830431 -4.1158214 -4.1284008 -4.125968 -4.1269283 -4.1449261 -4.1865163 -4.2062716][-4.2547522 -4.261138 -4.2614965 -4.2271266 -4.1665292 -4.12469 -4.1164412 -4.140039 -4.1727233 -4.186552 -4.1798549 -4.172224 -4.1841993 -4.2211428 -4.2401109][-4.2740541 -4.2774768 -4.2778587 -4.2485 -4.2036324 -4.174952 -4.1734529 -4.1927409 -4.2197061 -4.2356853 -4.2320542 -4.2195444 -4.2268977 -4.2552266 -4.2684793][-4.2979689 -4.3032784 -4.30291 -4.2761588 -4.2395244 -4.2159667 -4.2137156 -4.227241 -4.2508659 -4.2706308 -4.2733388 -4.2627816 -4.2680006 -4.2876949 -4.2951517][-4.3199654 -4.3255024 -4.3224669 -4.2969246 -4.262938 -4.2413092 -4.2362456 -4.2470722 -4.2678757 -4.2876854 -4.29414 -4.2900648 -4.295176 -4.30951 -4.314281][-4.32703 -4.3296251 -4.3258829 -4.3048167 -4.2769089 -4.2611446 -4.2572207 -4.2647305 -4.28099 -4.297101 -4.3051233 -4.304852 -4.3099856 -4.3201828 -4.3258352][-4.3212943 -4.3209605 -4.3170485 -4.303153 -4.285893 -4.2782145 -4.2776365 -4.2825384 -4.2941122 -4.3059344 -4.3126478 -4.3125415 -4.315764 -4.3234563 -4.3297215]]...]
INFO - root - 2017-12-07 16:18:07.338505: step 28910, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 56h:05m:30s remains)
INFO - root - 2017-12-07 16:18:13.928051: step 28920, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 57h:34m:55s remains)
INFO - root - 2017-12-07 16:18:20.850516: step 28930, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 58h:00m:12s remains)
INFO - root - 2017-12-07 16:18:27.747545: step 28940, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 59h:10m:07s remains)
INFO - root - 2017-12-07 16:18:34.537179: step 28950, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:40m:53s remains)
INFO - root - 2017-12-07 16:18:41.342586: step 28960, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 58h:51m:38s remains)
INFO - root - 2017-12-07 16:18:48.230659: step 28970, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 59h:56m:42s remains)
INFO - root - 2017-12-07 16:18:55.148727: step 28980, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 57h:15m:05s remains)
INFO - root - 2017-12-07 16:19:01.935919: step 28990, loss = 2.02, batch loss = 1.97 (12.4 examples/sec; 0.648 sec/batch; 54h:36m:11s remains)
INFO - root - 2017-12-07 16:19:08.696463: step 29000, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 57h:15m:39s remains)
2017-12-07 16:19:09.472290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3069367 -4.2924252 -4.2680798 -4.2344689 -4.2151127 -4.2295876 -4.2563667 -4.2759109 -4.2864461 -4.2915969 -4.2946806 -4.2918458 -4.2894282 -4.2865338 -4.2794819][-4.2755246 -4.267983 -4.2522507 -4.2233543 -4.1995816 -4.2103863 -4.2435722 -4.267838 -4.2768335 -4.2757711 -4.2739315 -4.2693663 -4.2693357 -4.2718778 -4.2700233][-4.2758808 -4.2765918 -4.2686825 -4.2457867 -4.2158217 -4.2170477 -4.2424183 -4.2602925 -4.2628183 -4.2534294 -4.2482982 -4.2483664 -4.2543917 -4.258306 -4.2612929][-4.2845249 -4.2912097 -4.2891726 -4.2701411 -4.2380838 -4.2223144 -4.2332492 -4.2454376 -4.2433839 -4.2279348 -4.2225442 -4.2326241 -4.2476053 -4.2567158 -4.2650604][-4.2790141 -4.2905807 -4.2926579 -4.2758164 -4.24434 -4.2182016 -4.2154264 -4.2209268 -4.217154 -4.2018156 -4.1984324 -4.2168946 -4.2426929 -4.2605605 -4.2741947][-4.261538 -4.2705526 -4.2720466 -4.2549586 -4.2321033 -4.2079391 -4.1950994 -4.1896558 -4.1801763 -4.1668243 -4.1707449 -4.1987677 -4.2306862 -4.2526455 -4.2699137][-4.2336221 -4.2338476 -4.2322311 -4.2195654 -4.2089443 -4.1941452 -4.1806469 -4.1762462 -4.1669855 -4.1575146 -4.1666274 -4.1959944 -4.223341 -4.2430487 -4.2589779][-4.2234793 -4.2155719 -4.2089458 -4.202322 -4.1993914 -4.1900792 -4.1810579 -4.1787605 -4.1720886 -4.1649318 -4.1760788 -4.2033458 -4.2277842 -4.2416306 -4.2523689][-4.2222924 -4.2109413 -4.198904 -4.19609 -4.2015748 -4.2017956 -4.1970282 -4.1916175 -4.18619 -4.1839795 -4.2004366 -4.2309518 -4.2555437 -4.2631516 -4.2617984][-4.2085524 -4.2091184 -4.2023897 -4.2030969 -4.2111225 -4.2164168 -4.2130404 -4.2075939 -4.2031164 -4.2069197 -4.2287297 -4.259573 -4.2842393 -4.2886229 -4.2774668][-4.2111411 -4.2294574 -4.2298026 -4.2289662 -4.2285223 -4.2296424 -4.2240224 -4.216176 -4.2151031 -4.2241559 -4.2452006 -4.2727094 -4.2948241 -4.3008661 -4.29015][-4.2245979 -4.2460222 -4.249104 -4.2412982 -4.234127 -4.2316055 -4.2228789 -4.2123928 -4.2105603 -4.2244329 -4.2486906 -4.2765207 -4.2959661 -4.3003421 -4.2920775][-4.2378993 -4.2486124 -4.2434464 -4.2293429 -4.220921 -4.22225 -4.2112937 -4.1990485 -4.1957068 -4.2075195 -4.2324562 -4.2598767 -4.2795916 -4.287766 -4.2849703][-4.23197 -4.2378798 -4.230813 -4.21861 -4.210444 -4.2082019 -4.1962156 -4.18324 -4.1784849 -4.1869531 -4.2144675 -4.2439032 -4.2637758 -4.2728119 -4.2752743][-4.2194128 -4.2268844 -4.2232494 -4.2167568 -4.2113204 -4.20422 -4.1912966 -4.1805854 -4.173583 -4.1759152 -4.2038183 -4.2351303 -4.25315 -4.2618418 -4.2638493]]...]
INFO - root - 2017-12-07 16:19:16.131140: step 29010, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 55h:53m:01s remains)
INFO - root - 2017-12-07 16:19:22.778717: step 29020, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 53h:22m:20s remains)
INFO - root - 2017-12-07 16:19:29.662812: step 29030, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 55h:59m:04s remains)
INFO - root - 2017-12-07 16:19:36.459523: step 29040, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 59h:31m:02s remains)
INFO - root - 2017-12-07 16:19:43.268607: step 29050, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 57h:25m:40s remains)
INFO - root - 2017-12-07 16:19:50.162874: step 29060, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 58h:05m:47s remains)
INFO - root - 2017-12-07 16:19:56.805998: step 29070, loss = 2.06, batch loss = 2.00 (13.9 examples/sec; 0.577 sec/batch; 48h:37m:58s remains)
INFO - root - 2017-12-07 16:20:03.556400: step 29080, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 60h:54m:58s remains)
INFO - root - 2017-12-07 16:20:10.398104: step 29090, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:22m:15s remains)
INFO - root - 2017-12-07 16:20:17.189123: step 29100, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 56h:01m:37s remains)
2017-12-07 16:20:17.908991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.20505 -4.1758022 -4.1628447 -4.1660542 -4.181736 -4.1993628 -4.2154179 -4.227304 -4.2427816 -4.2548485 -4.2617316 -4.2599478 -4.2534657 -4.2433333 -4.2234879][-4.2024817 -4.1818686 -4.1773067 -4.1831775 -4.1925449 -4.19583 -4.1949229 -4.191062 -4.204813 -4.22468 -4.243875 -4.2488456 -4.2391562 -4.2215438 -4.193809][-4.1988482 -4.1795745 -4.181284 -4.1935034 -4.2020197 -4.1978817 -4.1865921 -4.1735845 -4.1852336 -4.2106113 -4.2368178 -4.2436409 -4.2259574 -4.1939468 -4.1534863][-4.1924458 -4.1707916 -4.1729369 -4.1865425 -4.1919613 -4.1843281 -4.1696582 -4.1528058 -4.1650419 -4.1973224 -4.2307549 -4.2373304 -4.2143884 -4.1723104 -4.1196632][-4.1883326 -4.1642437 -4.1602578 -4.16626 -4.1639071 -4.1495976 -4.1309338 -4.115304 -4.1334734 -4.1764569 -4.2185254 -4.2306666 -4.2136745 -4.174161 -4.1157665][-4.1813879 -4.1568332 -4.1445041 -4.136023 -4.1160994 -4.0866218 -4.0573711 -4.042398 -4.0749974 -4.134172 -4.1912742 -4.2233195 -4.227529 -4.2061691 -4.1561227][-4.1737719 -4.1483574 -4.1313353 -4.1103654 -4.0704527 -4.0154076 -3.9618089 -3.9360981 -3.9801097 -4.0604444 -4.1395512 -4.1969237 -4.2290916 -4.2290959 -4.1937][-4.1856771 -4.1590247 -4.1446381 -4.1265368 -4.090322 -4.0336089 -3.9714789 -3.9307094 -3.9550204 -4.027297 -4.1094122 -4.1766205 -4.2212396 -4.2381625 -4.2222443][-4.2223787 -4.1977706 -4.1928449 -4.1889486 -4.1740041 -4.1406889 -4.1008158 -4.0685854 -4.0706248 -4.1074338 -4.1579194 -4.2061496 -4.2415495 -4.2576241 -4.2487211][-4.2529988 -4.2376871 -4.2429676 -4.2489047 -4.24402 -4.2252631 -4.204596 -4.1899157 -4.1921868 -4.2102561 -4.2329016 -4.2570038 -4.2722521 -4.2770066 -4.2630506][-4.253686 -4.2470603 -4.2578115 -4.2691469 -4.271265 -4.260747 -4.2487493 -4.24029 -4.2405338 -4.2489414 -4.2605648 -4.2723861 -4.2770534 -4.2736397 -4.2550211][-4.2143197 -4.208231 -4.2187734 -4.2332706 -4.2438226 -4.2417054 -4.2335544 -4.2252655 -4.220274 -4.2217255 -4.2286587 -4.2377954 -4.2441792 -4.2434072 -4.2252235][-4.160706 -4.1573329 -4.1679511 -4.1840682 -4.1971688 -4.1981406 -4.1932597 -4.1838374 -4.1731634 -4.1696811 -4.175199 -4.1844678 -4.1930938 -4.1936994 -4.1745586][-4.1323361 -4.1325703 -4.1427464 -4.1581912 -4.16914 -4.1705885 -4.1683135 -4.1616468 -4.1512136 -4.14535 -4.1488204 -4.1589231 -4.1684031 -4.1681352 -4.1496239][-4.1587968 -4.1632161 -4.1748233 -4.1873603 -4.1943793 -4.1953459 -4.1951766 -4.1908946 -4.1809235 -4.1740437 -4.1743703 -4.1816511 -4.189847 -4.1906734 -4.1775637]]...]
INFO - root - 2017-12-07 16:20:24.612970: step 29110, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 58h:33m:18s remains)
INFO - root - 2017-12-07 16:20:31.245249: step 29120, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 57h:19m:47s remains)
INFO - root - 2017-12-07 16:20:38.044176: step 29130, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 55h:43m:42s remains)
INFO - root - 2017-12-07 16:20:44.773427: step 29140, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 53h:28m:23s remains)
INFO - root - 2017-12-07 16:20:51.565201: step 29150, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 57h:54m:31s remains)
INFO - root - 2017-12-07 16:20:58.365857: step 29160, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 61h:06m:50s remains)
INFO - root - 2017-12-07 16:21:05.210460: step 29170, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 59h:37m:26s remains)
INFO - root - 2017-12-07 16:21:12.014449: step 29180, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 54h:51m:06s remains)
INFO - root - 2017-12-07 16:21:18.947318: step 29190, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 55h:26m:51s remains)
INFO - root - 2017-12-07 16:21:25.824053: step 29200, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 61h:13m:26s remains)
2017-12-07 16:21:26.540426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2897167 -4.2827926 -4.2721696 -4.2594919 -4.2486839 -4.2396445 -4.2348933 -4.2356348 -4.2352457 -4.2406583 -4.2427497 -4.23129 -4.213438 -4.1975474 -4.1969757][-4.2915745 -4.2784977 -4.2560215 -4.2336268 -4.220953 -4.2131743 -4.2134509 -4.2178349 -4.2215047 -4.2324777 -4.2399631 -4.2370629 -4.2248807 -4.2146258 -4.2237482][-4.2902765 -4.2674141 -4.2296128 -4.1973615 -4.1848907 -4.176796 -4.1791096 -4.1861744 -4.1997571 -4.2197609 -4.2321768 -4.2386584 -4.2370815 -4.2372222 -4.2557235][-4.2812281 -4.2496243 -4.2010221 -4.1650209 -4.1560931 -4.1451569 -4.133564 -4.1259456 -4.1437125 -4.1740985 -4.1957259 -4.212194 -4.2258697 -4.2371612 -4.265378][-4.2715864 -4.2380133 -4.1868567 -4.1518188 -4.1444826 -4.1213636 -4.07795 -4.0396309 -4.0602875 -4.1040554 -4.1400228 -4.1689034 -4.1941586 -4.2125025 -4.2496986][-4.2610722 -4.2284369 -4.18189 -4.1452608 -4.1229076 -4.0764823 -3.9938772 -3.9172664 -3.9497166 -4.0226588 -4.0731244 -4.1137557 -4.1494932 -4.179543 -4.2270913][-4.2431455 -4.2110381 -4.1716685 -4.1319833 -4.0906653 -4.0172353 -3.8868346 -3.7566607 -3.8138776 -3.9332221 -4.0062394 -4.063664 -4.1122832 -4.1552644 -4.2117987][-4.2164364 -4.1854634 -4.1572437 -4.1209126 -4.0697055 -3.9781625 -3.8115816 -3.6352961 -3.7189059 -3.8807855 -3.9740999 -4.04354 -4.1038961 -4.1539168 -4.2117181][-4.2003965 -4.1706843 -4.1503782 -4.1227612 -4.0833521 -4.0069513 -3.8696263 -3.731715 -3.8014615 -3.935297 -4.0109587 -4.0682516 -4.123837 -4.17224 -4.2232513][-4.206183 -4.1763158 -4.153841 -4.1324735 -4.1092095 -4.0604019 -3.9773374 -3.9051013 -3.9540937 -4.0398278 -4.0885425 -4.1219063 -4.1600242 -4.2004805 -4.2411709][-4.2323523 -4.2046251 -4.1800752 -4.1617823 -4.149097 -4.1217971 -4.0791497 -4.0498919 -4.0871615 -4.1400504 -4.1692963 -4.1840162 -4.2008281 -4.226161 -4.2526054][-4.265718 -4.2454963 -4.2227116 -4.2120929 -4.2116952 -4.2000084 -4.179812 -4.1677041 -4.1898584 -4.2154503 -4.2283578 -4.2292047 -4.2292089 -4.238203 -4.25087][-4.2844934 -4.2773943 -4.2645779 -4.2628212 -4.2717552 -4.2706656 -4.2603612 -4.2504611 -4.2560954 -4.2607946 -4.2580113 -4.2434673 -4.2302513 -4.2264333 -4.2285008][-4.2899227 -4.2900515 -4.2853475 -4.2879386 -4.2995539 -4.3008785 -4.2922692 -4.28536 -4.283216 -4.2776489 -4.2635212 -4.24015 -4.2180886 -4.2042108 -4.1971726][-4.2935019 -4.2973413 -4.2971077 -4.299027 -4.3052344 -4.3051486 -4.297802 -4.291831 -4.2879539 -4.2807274 -4.2622452 -4.232985 -4.2016892 -4.1806254 -4.1649852]]...]
INFO - root - 2017-12-07 16:21:33.323352: step 29210, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.679 sec/batch; 57h:14m:21s remains)
INFO - root - 2017-12-07 16:21:39.986322: step 29220, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 56h:46m:45s remains)
INFO - root - 2017-12-07 16:21:46.968565: step 29230, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 59h:35m:26s remains)
INFO - root - 2017-12-07 16:21:53.901936: step 29240, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 62h:44m:04s remains)
INFO - root - 2017-12-07 16:22:00.673336: step 29250, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 57h:04m:44s remains)
INFO - root - 2017-12-07 16:22:07.432698: step 29260, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 53h:47m:11s remains)
INFO - root - 2017-12-07 16:22:14.253367: step 29270, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 55h:37m:37s remains)
INFO - root - 2017-12-07 16:22:21.078899: step 29280, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 56h:47m:10s remains)
INFO - root - 2017-12-07 16:22:27.910462: step 29290, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 55h:11m:24s remains)
INFO - root - 2017-12-07 16:22:34.674414: step 29300, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 56h:07m:38s remains)
2017-12-07 16:22:35.368596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33562 -4.3306866 -4.3226571 -4.3131642 -4.3057013 -4.2975802 -4.2920847 -4.2900362 -4.2925253 -4.289463 -4.28553 -4.288466 -4.2957635 -4.2998576 -4.3031325][-4.3302917 -4.320076 -4.3050365 -4.2899327 -4.2808518 -4.2743044 -4.2693677 -4.2664537 -4.2656689 -4.2563376 -4.2471576 -4.2522106 -4.2659721 -4.275723 -4.2820482][-4.3233466 -4.3077416 -4.2880793 -4.2705183 -4.2610874 -4.25504 -4.2458963 -4.2366304 -4.2366405 -4.2292323 -4.2212505 -4.2256184 -4.2419109 -4.2561212 -4.2659497][-4.3131042 -4.2950115 -4.2729893 -4.2509608 -4.2346826 -4.2225852 -4.20809 -4.1922116 -4.1887326 -4.1907086 -4.197937 -4.2073164 -4.2231655 -4.2392845 -4.2510333][-4.3022442 -4.2809992 -4.2553263 -4.2255969 -4.1998329 -4.1737723 -4.1499453 -4.1287384 -4.119657 -4.1299329 -4.156836 -4.1799297 -4.1971722 -4.2143092 -4.2299337][-4.290205 -4.2630277 -4.2295804 -4.1864142 -4.1439757 -4.0989456 -4.0540133 -4.0173087 -4.0046926 -4.0271416 -4.0752254 -4.1214085 -4.1500931 -4.1717939 -4.1924005][-4.2780261 -4.2418156 -4.1866207 -4.1206942 -4.0579453 -3.9864798 -3.8985047 -3.8285086 -3.8386497 -3.9057517 -3.9853806 -4.0560069 -4.1002684 -4.1242809 -4.1487446][-4.2667451 -4.21947 -4.1420374 -4.0542173 -3.970813 -3.8671994 -3.7200756 -3.6072912 -3.6650236 -3.8023355 -3.9124668 -3.9967661 -4.0532217 -4.0787725 -4.1069031][-4.2543812 -4.1985664 -4.1128259 -4.0204649 -3.9250257 -3.8002591 -3.6289792 -3.5120668 -3.6235845 -3.7972994 -3.9064951 -3.9759388 -4.0249138 -4.0525408 -4.0851974][-4.2466984 -4.1910348 -4.1164551 -4.043364 -3.9618382 -3.86056 -3.7433245 -3.6898513 -3.7915506 -3.915251 -3.9730082 -4.007791 -4.0397291 -4.0682492 -4.1100039][-4.2537 -4.2104735 -4.159018 -4.1177664 -4.0704036 -4.0123448 -3.9634233 -3.9576058 -4.0110846 -4.0606737 -4.0727153 -4.0809174 -4.1021833 -4.1294909 -4.1749411][-4.2773113 -4.2489572 -4.2217774 -4.2078118 -4.189724 -4.1587567 -4.1453009 -4.1538639 -4.170577 -4.1802821 -4.1784091 -4.1807876 -4.1965876 -4.220305 -4.2556748][-4.3053017 -4.2885447 -4.2777929 -4.2766962 -4.2700806 -4.2554584 -4.256412 -4.2649593 -4.2681227 -4.2685061 -4.266305 -4.2679443 -4.2812419 -4.3021646 -4.3256712][-4.3270254 -4.3174553 -4.3129606 -4.3157706 -4.3149204 -4.3116827 -4.3166757 -4.3239307 -4.325808 -4.3270736 -4.3243427 -4.3282595 -4.3401709 -4.3528748 -4.3639817][-4.33922 -4.333962 -4.3323016 -4.3355732 -4.3361173 -4.3371081 -4.342679 -4.3480105 -4.3502026 -4.3523774 -4.3513222 -4.3546925 -4.3612432 -4.3655734 -4.3681951]]...]
INFO - root - 2017-12-07 16:22:42.100743: step 29310, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 54h:21m:15s remains)
INFO - root - 2017-12-07 16:22:48.766884: step 29320, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 60h:35m:03s remains)
INFO - root - 2017-12-07 16:22:55.636045: step 29330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 55h:54m:13s remains)
INFO - root - 2017-12-07 16:23:02.452054: step 29340, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 54h:54m:31s remains)
INFO - root - 2017-12-07 16:23:09.290306: step 29350, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 60h:14m:48s remains)
INFO - root - 2017-12-07 16:23:16.224544: step 29360, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.739 sec/batch; 62h:11m:41s remains)
INFO - root - 2017-12-07 16:23:22.957405: step 29370, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 57h:05m:13s remains)
INFO - root - 2017-12-07 16:23:29.592044: step 29380, loss = 2.05, batch loss = 1.99 (16.6 examples/sec; 0.482 sec/batch; 40h:37m:21s remains)
INFO - root - 2017-12-07 16:23:36.367836: step 29390, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 57h:08m:57s remains)
INFO - root - 2017-12-07 16:23:43.252321: step 29400, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 60h:54m:01s remains)
2017-12-07 16:23:43.890412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2498145 -4.2524939 -4.2576756 -4.2634535 -4.2577724 -4.2484827 -4.2403131 -4.23516 -4.2307496 -4.21705 -4.19428 -4.1583166 -4.1322932 -4.1413994 -4.1692886][-4.2273774 -4.2328954 -4.2415862 -4.2469873 -4.2354178 -4.2203469 -4.2116623 -4.21081 -4.214293 -4.2095461 -4.1989913 -4.1746283 -4.1529207 -4.161808 -4.19003][-4.2181134 -4.2272143 -4.2369823 -4.2364426 -4.2151289 -4.1891117 -4.1793952 -4.1863446 -4.1987953 -4.2053289 -4.2057323 -4.1942015 -4.1785812 -4.1846147 -4.2052145][-4.2062607 -4.2212653 -4.2292747 -4.2193642 -4.1813679 -4.1393394 -4.1325231 -4.1563616 -4.1836481 -4.2010818 -4.2062116 -4.2039108 -4.1953044 -4.2019033 -4.2153492][-4.2050858 -4.2205925 -4.219028 -4.1888943 -4.1260033 -4.060842 -4.0614486 -4.1105537 -4.1636152 -4.1975451 -4.2101417 -4.2162194 -4.2156663 -4.2225471 -4.2307715][-4.2229357 -4.231338 -4.2141075 -4.1556311 -4.0587621 -3.9565635 -3.9598062 -4.0408969 -4.1296105 -4.188705 -4.2194257 -4.2360721 -4.2415776 -4.2463746 -4.25014][-4.2497125 -4.2442293 -4.2050982 -4.1148896 -3.9848223 -3.8496954 -3.8569973 -3.9752288 -4.0993557 -4.1852794 -4.2348824 -4.2599487 -4.2672696 -4.2689815 -4.2699533][-4.2729559 -4.2470474 -4.1792164 -4.0599413 -3.9072461 -3.7597935 -3.7816772 -3.9353755 -4.0891109 -4.19881 -4.2615972 -4.2924275 -4.2988715 -4.296268 -4.2934127][-4.2962227 -4.2563076 -4.1688437 -4.0342784 -3.8771188 -3.7386715 -3.7783751 -3.9479766 -4.1113057 -4.2287679 -4.2924657 -4.32247 -4.326417 -4.3198657 -4.3147736][-4.3146095 -4.2724581 -4.1838021 -4.054904 -3.9084733 -3.7894709 -3.8331425 -3.9917905 -4.1490393 -4.2606616 -4.3181014 -4.3439112 -4.3456264 -4.3387222 -4.3349881][-4.32724 -4.2911057 -4.2134171 -4.0983791 -3.9665895 -3.8658361 -3.9032674 -4.0407572 -4.1830397 -4.2858853 -4.3365555 -4.3578444 -4.35849 -4.3521581 -4.3501325][-4.3347697 -4.3037591 -4.240828 -4.1449351 -4.031858 -3.9500418 -3.9822433 -4.0960245 -4.2164836 -4.305089 -4.3496537 -4.3687639 -4.3697882 -4.3641891 -4.3633757][-4.3379674 -4.3114343 -4.2618866 -4.1878366 -4.1026258 -4.0485668 -4.0788074 -4.1681952 -4.2604465 -4.3281655 -4.36153 -4.3753963 -4.3754759 -4.3700433 -4.369483][-4.3374071 -4.3178725 -4.2840371 -4.2332153 -4.1779103 -4.1495781 -4.1759005 -4.2400336 -4.3027987 -4.346756 -4.3671169 -4.374805 -4.3735557 -4.3693404 -4.368938][-4.3393478 -4.3259029 -4.3061342 -4.2766242 -4.2464223 -4.2348719 -4.2546206 -4.2946682 -4.33134 -4.3551259 -4.3666139 -4.3713207 -4.3705974 -4.3687935 -4.369369]]...]
INFO - root - 2017-12-07 16:23:50.626997: step 29410, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 55h:23m:34s remains)
INFO - root - 2017-12-07 16:23:57.249375: step 29420, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 60h:07m:43s remains)
INFO - root - 2017-12-07 16:24:04.046392: step 29430, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 59h:44m:29s remains)
INFO - root - 2017-12-07 16:24:10.913954: step 29440, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 55h:09m:30s remains)
INFO - root - 2017-12-07 16:24:17.710482: step 29450, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 55h:55m:26s remains)
INFO - root - 2017-12-07 16:24:24.650431: step 29460, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 58h:36m:12s remains)
INFO - root - 2017-12-07 16:24:31.485387: step 29470, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 59h:38m:55s remains)
INFO - root - 2017-12-07 16:24:38.322895: step 29480, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 54h:35m:52s remains)
INFO - root - 2017-12-07 16:24:45.128092: step 29490, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 53h:11m:43s remains)
INFO - root - 2017-12-07 16:24:51.923831: step 29500, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 59h:29m:38s remains)
2017-12-07 16:24:52.705212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228857 -4.2157011 -4.2038913 -4.1855917 -4.1896276 -4.2046208 -4.202301 -4.1894946 -4.1706405 -4.1689019 -4.1834583 -4.1935673 -4.1842194 -4.1615872 -4.1353364][-4.1908746 -4.1775508 -4.1669979 -4.1475015 -4.1546583 -4.1749177 -4.17182 -4.152493 -4.1261907 -4.1281624 -4.1506796 -4.1648593 -4.1567235 -4.1306543 -4.0978265][-4.1453428 -4.1327496 -4.1168389 -4.0886307 -4.0895886 -4.1143274 -4.113493 -4.0961971 -4.0721679 -4.0838776 -4.1113286 -4.1241403 -4.1181855 -4.1030688 -4.0767527][-4.1041207 -4.0874052 -4.0634432 -4.024621 -4.0170932 -4.0476661 -4.0601697 -4.0522537 -4.0388231 -4.0643253 -4.0950704 -4.0993924 -4.0977683 -4.0999932 -4.092083][-4.0681167 -4.0483 -4.0217214 -3.9724267 -3.9539793 -3.9904075 -4.0255113 -4.0362105 -4.0358076 -4.06915 -4.1025739 -4.1039443 -4.1044073 -4.1182718 -4.1256008][-4.0466437 -4.03831 -4.0180087 -3.9634669 -3.9352298 -3.9573956 -3.9911561 -4.0052156 -4.0134578 -4.0596752 -4.1020007 -4.1089234 -4.1044855 -4.1182413 -4.135489][-4.0405478 -4.0452027 -4.0312848 -3.9774971 -3.9358902 -3.9191766 -3.9272623 -3.9264452 -3.9446323 -4.0129571 -4.0687881 -4.0781217 -4.0698051 -4.0819755 -4.1046724][-4.0269346 -4.0320663 -4.0210013 -3.9765353 -3.9295206 -3.8786302 -3.8517718 -3.8341737 -3.859515 -3.9455743 -4.0059447 -4.0111361 -3.9975884 -4.0071869 -4.0379257][-4.0153937 -4.01127 -3.996515 -3.9610355 -3.9128432 -3.8485894 -3.8078067 -3.792923 -3.82376 -3.8988581 -3.9451153 -3.94307 -3.9290736 -3.9355443 -3.9735596][-4.0258956 -4.0136 -3.9940839 -3.964889 -3.9275622 -3.8803408 -3.8547661 -3.8524311 -3.8761654 -3.9169335 -3.9351099 -3.9273047 -3.9161105 -3.9241989 -3.970124][-4.0698147 -4.0557618 -4.0377808 -4.0156841 -3.9909995 -3.9677014 -3.9597509 -3.9678848 -3.979605 -3.9897199 -3.9863083 -3.9773505 -3.9710889 -3.9842172 -4.0303206][-4.1446872 -4.1367464 -4.12437 -4.1020474 -4.0800934 -4.0664673 -4.06709 -4.0785284 -4.083056 -4.081841 -4.0739121 -4.0680327 -4.0648851 -4.0812378 -4.1154184][-4.2204895 -4.2215443 -4.2159672 -4.19317 -4.1740789 -4.168458 -4.1705203 -4.1809015 -4.1854596 -4.1837792 -4.1765032 -4.1714964 -4.1685805 -4.1810927 -4.1959753][-4.2687349 -4.2761607 -4.2765226 -4.2608428 -4.24852 -4.2477036 -4.2503972 -4.2571788 -4.2615037 -4.2596145 -4.2541442 -4.2479291 -4.244976 -4.2503915 -4.2554364][-4.2865787 -4.2962279 -4.2994652 -4.2919059 -4.2854486 -4.2846551 -4.2865224 -4.2930155 -4.2980576 -4.2969522 -4.2927294 -4.2871327 -4.28373 -4.2863111 -4.2897053]]...]
INFO - root - 2017-12-07 16:24:59.400061: step 29510, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.639 sec/batch; 53h:48m:30s remains)
INFO - root - 2017-12-07 16:25:05.946597: step 29520, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 54h:17m:44s remains)
INFO - root - 2017-12-07 16:25:12.688647: step 29530, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 58h:32m:15s remains)
INFO - root - 2017-12-07 16:25:19.509505: step 29540, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 62h:08m:59s remains)
INFO - root - 2017-12-07 16:25:26.297428: step 29550, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 57h:42m:29s remains)
INFO - root - 2017-12-07 16:25:32.936279: step 29560, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 54h:13m:27s remains)
INFO - root - 2017-12-07 16:25:39.776395: step 29570, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.680 sec/batch; 57h:11m:48s remains)
INFO - root - 2017-12-07 16:25:46.652361: step 29580, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 56h:51m:32s remains)
INFO - root - 2017-12-07 16:25:53.510444: step 29590, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 62h:38m:44s remains)
INFO - root - 2017-12-07 16:26:00.321729: step 29600, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 55h:13m:58s remains)
2017-12-07 16:26:00.995861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2031326 -4.2161546 -4.2272291 -4.2408533 -4.2533092 -4.2578778 -4.2625527 -4.2740545 -4.2837334 -4.2822766 -4.2702637 -4.2455578 -4.2152767 -4.1958451 -4.1981411][-4.2288976 -4.2443275 -4.2522926 -4.2555857 -4.254344 -4.2500315 -4.2533975 -4.2687082 -4.2833543 -4.2857847 -4.2759771 -4.2506347 -4.2152333 -4.187881 -4.1875706][-4.2678518 -4.2777886 -4.2773581 -4.2665353 -4.2503338 -4.236464 -4.2376318 -4.2543 -4.2731996 -4.2815981 -4.2778664 -4.258882 -4.2281742 -4.2041092 -4.2056408][-4.2873464 -4.2926035 -4.28583 -4.2613406 -4.2296367 -4.2061453 -4.2066326 -4.2259769 -4.2492952 -4.2676959 -4.2762852 -4.2718334 -4.2534628 -4.2342749 -4.2362142][-4.2907109 -4.298811 -4.2848105 -4.2427273 -4.193028 -4.15928 -4.1554542 -4.1779213 -4.2102542 -4.2454033 -4.273766 -4.2883472 -4.2850218 -4.2722921 -4.27007][-4.2831011 -4.2905083 -4.2671833 -4.2052946 -4.1356688 -4.0818129 -4.0628152 -4.0922112 -4.1482387 -4.2094603 -4.2593379 -4.2911944 -4.3029289 -4.2977233 -4.2904949][-4.2489576 -4.2519779 -4.2165823 -4.1348104 -4.0363655 -3.9434905 -3.8939359 -3.9366047 -4.0404468 -4.1421065 -4.2135296 -4.2615948 -4.2909651 -4.2967315 -4.29112][-4.2159042 -4.197 -4.1311135 -4.0136447 -3.8690112 -3.7182338 -3.6233215 -3.6828003 -3.8505008 -4.0042 -4.1053305 -4.1741061 -4.2276988 -4.2543082 -4.2626934][-4.2153916 -4.1741209 -4.0871458 -3.9481256 -3.7764273 -3.5874043 -3.4510219 -3.507375 -3.7016137 -3.8805637 -4.0032763 -4.0945387 -4.1694345 -4.2153111 -4.2381549][-4.2494383 -4.2135615 -4.1480165 -4.0464587 -3.9244146 -3.7915907 -3.6960478 -3.72779 -3.8533716 -3.9730179 -4.0564933 -4.1179991 -4.1729555 -4.2077394 -4.2243385][-4.2843475 -4.2597303 -4.2204189 -4.1606507 -4.0909195 -4.0198774 -3.9744353 -3.9953215 -4.0644655 -4.1296439 -4.1695633 -4.1942596 -4.2168937 -4.2297525 -4.2275591][-4.2908726 -4.2719126 -4.24902 -4.2137451 -4.170855 -4.1327419 -4.1166229 -4.1396065 -4.1870456 -4.2218709 -4.2302475 -4.2303057 -4.2331944 -4.2295632 -4.2131872][-4.2551012 -4.2389197 -4.2264781 -4.2064381 -4.179184 -4.1592288 -4.1612306 -4.1871543 -4.2242503 -4.2438431 -4.2397761 -4.2322264 -4.2292366 -4.21734 -4.1950679][-4.2211971 -4.206811 -4.2004757 -4.1890664 -4.17331 -4.1690364 -4.1835995 -4.2107873 -4.2390809 -4.2493749 -4.2404037 -4.2300467 -4.2262449 -4.218184 -4.2014852][-4.2148709 -4.2032971 -4.1993141 -4.1892095 -4.1776743 -4.1816311 -4.2028537 -4.2314014 -4.2561226 -4.2629242 -4.252049 -4.2379689 -4.2289577 -4.2218857 -4.2137461]]...]
INFO - root - 2017-12-07 16:26:07.712505: step 29610, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 58h:29m:32s remains)
INFO - root - 2017-12-07 16:26:14.501030: step 29620, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 59h:39m:18s remains)
INFO - root - 2017-12-07 16:26:21.344868: step 29630, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 58h:11m:59s remains)
INFO - root - 2017-12-07 16:26:28.187037: step 29640, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 53h:23m:00s remains)
INFO - root - 2017-12-07 16:26:35.020001: step 29650, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 60h:14m:16s remains)
INFO - root - 2017-12-07 16:26:41.851606: step 29660, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 59h:38m:59s remains)
INFO - root - 2017-12-07 16:26:48.598354: step 29670, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 57h:25m:01s remains)
INFO - root - 2017-12-07 16:26:55.446914: step 29680, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:32m:34s remains)
INFO - root - 2017-12-07 16:27:02.065549: step 29690, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 56h:51m:28s remains)
INFO - root - 2017-12-07 16:27:08.902701: step 29700, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 61h:39m:55s remains)
2017-12-07 16:27:09.592541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3499041 -4.3527803 -4.3549447 -4.3414726 -4.3099103 -4.2638292 -4.2105346 -4.1742311 -4.156723 -4.1547418 -4.1510453 -4.1310048 -4.13265 -4.1700597 -4.2192564][-4.3498497 -4.3542266 -4.3588867 -4.347178 -4.3141727 -4.2593355 -4.1917429 -4.1455679 -4.1372156 -4.1517968 -4.1645541 -4.1510115 -4.1486187 -4.1833315 -4.2314906][-4.349154 -4.3544617 -4.3599668 -4.3497462 -4.3160295 -4.2520633 -4.1677527 -4.1090417 -4.1124988 -4.1504035 -4.1815114 -4.1790113 -4.1780448 -4.2081466 -4.2483678][-4.3488364 -4.3539939 -4.3590355 -4.3483691 -4.3145814 -4.2430935 -4.1435919 -4.0756207 -4.0926871 -4.1566453 -4.2059984 -4.2140675 -4.2167339 -4.2424116 -4.2709408][-4.3491378 -4.3534231 -4.3569417 -4.3453846 -4.3104978 -4.2335873 -4.1228819 -4.04637 -4.0739746 -4.159132 -4.2248087 -4.2401924 -4.24581 -4.2674522 -4.2875643][-4.3500009 -4.3535104 -4.3553443 -4.3433976 -4.3076863 -4.2238808 -4.1023593 -4.0166945 -4.048871 -4.1553197 -4.2365465 -4.2567668 -4.2649879 -4.2848816 -4.3007607][-4.3508339 -4.3540173 -4.3546524 -4.3425064 -4.3052 -4.2149892 -4.082293 -3.9845405 -4.0158887 -4.1399574 -4.2396593 -4.2702 -4.2805042 -4.2941813 -4.3059616][-4.3512797 -4.3537416 -4.353374 -4.3407941 -4.3030186 -4.2118058 -4.0747347 -3.9637527 -3.9819255 -4.1117854 -4.2274771 -4.2726803 -4.288765 -4.2941623 -4.299861][-4.3508825 -4.3521295 -4.3511038 -4.3385921 -4.3034554 -4.2186489 -4.0908661 -3.9761033 -3.9685581 -4.0847988 -4.2042007 -4.26266 -4.287982 -4.291492 -4.2912936][-4.3495111 -4.34922 -4.3479238 -4.3373556 -4.3067236 -4.2317414 -4.1217461 -4.018744 -3.9884796 -4.0717864 -4.1753883 -4.2388759 -4.277904 -4.2885246 -4.2879267][-4.3479319 -4.3448105 -4.3419604 -4.3338327 -4.3097467 -4.2462287 -4.1562119 -4.0758462 -4.04032 -4.0903869 -4.1622596 -4.216506 -4.2668695 -4.2875428 -4.2880774][-4.3455544 -4.33891 -4.3333254 -4.325798 -4.3071508 -4.2565231 -4.1867118 -4.1314864 -4.1055117 -4.1331167 -4.17254 -4.2077341 -4.2612982 -4.2922692 -4.29353][-4.3433642 -4.3330789 -4.3235788 -4.3149385 -4.2996836 -4.2603989 -4.2064538 -4.1717095 -4.161046 -4.1810455 -4.2005486 -4.2144737 -4.2596064 -4.2930923 -4.2938938][-4.3413925 -4.3266864 -4.3132372 -4.3037663 -4.2896619 -4.2570071 -4.2140608 -4.1918097 -4.1943583 -4.2171979 -4.230711 -4.2306871 -4.2612238 -4.2927294 -4.2922707][-4.340745 -4.3220425 -4.3039336 -4.2926974 -4.2784758 -4.2494006 -4.2135186 -4.1966481 -4.2064795 -4.2318182 -4.2487855 -4.2495914 -4.2669339 -4.2905941 -4.2920008]]...]
INFO - root - 2017-12-07 16:27:16.292890: step 29710, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 55h:32m:23s remains)
INFO - root - 2017-12-07 16:27:22.937082: step 29720, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 59h:20m:56s remains)
INFO - root - 2017-12-07 16:27:29.876647: step 29730, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 61h:39m:49s remains)
INFO - root - 2017-12-07 16:27:36.802201: step 29740, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 57h:04m:25s remains)
INFO - root - 2017-12-07 16:27:43.594774: step 29750, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 55h:15m:30s remains)
INFO - root - 2017-12-07 16:27:50.417659: step 29760, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 58h:59m:44s remains)
INFO - root - 2017-12-07 16:27:57.280815: step 29770, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 59h:00m:23s remains)
INFO - root - 2017-12-07 16:28:04.051796: step 29780, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 56h:46m:46s remains)
INFO - root - 2017-12-07 16:28:10.940026: step 29790, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 57h:38m:18s remains)
INFO - root - 2017-12-07 16:28:17.663256: step 29800, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 53h:43m:11s remains)
2017-12-07 16:28:18.418853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3167443 -4.3128595 -4.3094292 -4.3063512 -4.3059769 -4.3061409 -4.3073764 -4.3090949 -4.3066792 -4.298728 -4.2873335 -4.2746634 -4.2635393 -4.2561131 -4.2521977][-4.292563 -4.2862072 -4.2805395 -4.2752581 -4.2745175 -4.2769871 -4.2824192 -4.2892284 -4.2914395 -4.2876348 -4.2783008 -4.2656846 -4.2529445 -4.2423363 -4.2337823][-4.2659092 -4.2575431 -4.2490711 -4.2403259 -4.2386928 -4.2416573 -4.2491517 -4.2599831 -4.2668848 -4.2679377 -4.263586 -4.2553949 -4.2431927 -4.2293806 -4.2159233][-4.2433062 -4.2306767 -4.2198973 -4.2093363 -4.2059975 -4.206389 -4.2112231 -4.2217278 -4.2329841 -4.2406707 -4.2424731 -4.242456 -4.2360368 -4.22701 -4.2168489][-4.2350845 -4.2120395 -4.1959672 -4.1824846 -4.1758285 -4.1717911 -4.1707292 -4.1797137 -4.195961 -4.2110982 -4.2186956 -4.2267895 -4.231246 -4.2335253 -4.2345886][-4.2403145 -4.2020779 -4.1713462 -4.1501136 -4.1330976 -4.1107006 -4.0968428 -4.109591 -4.1383653 -4.1644554 -4.1797109 -4.1955385 -4.2120781 -4.2306538 -4.2464118][-4.2371206 -4.1813955 -4.1262383 -4.0826092 -4.0368924 -3.9823985 -3.9564495 -3.9827402 -4.0329704 -4.0779595 -4.1087713 -4.1362295 -4.1683726 -4.204145 -4.2314568][-4.2261319 -4.1653075 -4.0904179 -4.0199203 -3.9406121 -3.8572288 -3.8269613 -3.8676038 -3.9360876 -3.9922016 -4.0364375 -4.0800347 -4.1296396 -4.1769791 -4.2094016][-4.2220373 -4.1806135 -4.1149421 -4.0441442 -3.9575474 -3.8700562 -3.8448875 -3.8840342 -3.9398093 -3.9799786 -4.0199232 -4.0709453 -4.1288557 -4.1781096 -4.2074194][-4.2135377 -4.2052021 -4.1691031 -4.1229076 -4.0610037 -4.0004673 -3.9862192 -4.0098186 -4.0376606 -4.056499 -4.0853019 -4.1280737 -4.1755066 -4.2122083 -4.2300963][-4.1935816 -4.2174606 -4.2138023 -4.2001705 -4.172749 -4.1427989 -4.1362743 -4.1467719 -4.1535492 -4.1609797 -4.1796689 -4.2094727 -4.2389851 -4.2581644 -4.2658091][-4.1862531 -4.2248282 -4.240767 -4.2490788 -4.2488542 -4.2443795 -4.2450833 -4.2484493 -4.2511148 -4.2609172 -4.2766037 -4.2932138 -4.3069925 -4.3104997 -4.3103385][-4.2169609 -4.2526178 -4.272306 -4.2896619 -4.3008757 -4.306602 -4.3088408 -4.3100615 -4.3127322 -4.3213172 -4.3339968 -4.3435469 -4.3475614 -4.3453522 -4.3442297][-4.2689557 -4.2905178 -4.3049879 -4.3228 -4.3336926 -4.3395939 -4.34185 -4.3426375 -4.3439183 -4.3488407 -4.3560758 -4.3599982 -4.3601375 -4.3603425 -4.3624563][-4.314445 -4.3211284 -4.3291235 -4.3433008 -4.3528152 -4.3564091 -4.3569107 -4.3572044 -4.3582625 -4.3605385 -4.3630867 -4.3640985 -4.3647604 -4.3671246 -4.3706636]]...]
INFO - root - 2017-12-07 16:28:25.193074: step 29810, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 57h:21m:18s remains)
INFO - root - 2017-12-07 16:28:31.854662: step 29820, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 54h:17m:43s remains)
INFO - root - 2017-12-07 16:28:38.640214: step 29830, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 53h:58m:35s remains)
INFO - root - 2017-12-07 16:28:45.401695: step 29840, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 58h:08m:55s remains)
INFO - root - 2017-12-07 16:28:52.200245: step 29850, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 59h:19m:25s remains)
INFO - root - 2017-12-07 16:28:58.984349: step 29860, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 56h:49m:21s remains)
INFO - root - 2017-12-07 16:29:05.832604: step 29870, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 56h:16m:06s remains)
INFO - root - 2017-12-07 16:29:12.613605: step 29880, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 54h:47m:11s remains)
INFO - root - 2017-12-07 16:29:19.424063: step 29890, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 58h:53m:44s remains)
INFO - root - 2017-12-07 16:29:26.311982: step 29900, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 58h:45m:47s remains)
2017-12-07 16:29:27.014429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2552466 -4.240665 -4.2474971 -4.2575727 -4.2621584 -4.2475915 -4.2262158 -4.2085495 -4.2105513 -4.2313337 -4.2504587 -4.269598 -4.2785172 -4.2905331 -4.3026981][-4.21685 -4.2057967 -4.2129188 -4.2278852 -4.24513 -4.2456818 -4.23855 -4.2261972 -4.2219787 -4.2376943 -4.2529039 -4.2596865 -4.2575779 -4.2601614 -4.2615638][-4.1774917 -4.1694155 -4.176703 -4.1955032 -4.2180848 -4.2259593 -4.2324114 -4.2356529 -4.2374368 -4.249392 -4.257616 -4.2515965 -4.235352 -4.223805 -4.2142477][-4.1416163 -4.1385269 -4.1511564 -4.1734481 -4.1905389 -4.1932478 -4.1949182 -4.208621 -4.2283669 -4.241343 -4.2461767 -4.2330966 -4.2101226 -4.1854587 -4.1640115][-4.1117744 -4.1123605 -4.1348052 -4.1593528 -4.1604457 -4.1443863 -4.1231627 -4.1350837 -4.1766391 -4.2045927 -4.2165484 -4.2059264 -4.1780972 -4.141583 -4.10953][-4.0964689 -4.0831618 -4.1018209 -4.1244469 -4.1163845 -4.0744452 -4.0090361 -4.0063181 -4.0783944 -4.1297026 -4.1528435 -4.1574955 -4.1402564 -4.1037145 -4.0740085][-4.0823913 -4.032732 -4.0303078 -4.042707 -4.0195928 -3.9438787 -3.8172328 -3.7818844 -3.9037392 -3.9929397 -4.0323853 -4.0635495 -4.0780373 -4.0612206 -4.0455971][-4.0777841 -3.9949172 -3.9668517 -3.9615655 -3.9194417 -3.8236747 -3.6608377 -3.5922954 -3.7585249 -3.8817036 -3.9346201 -3.9884267 -4.0349193 -4.0434852 -4.0493779][-4.119803 -4.0392132 -4.006011 -3.9912698 -3.9485178 -3.8702159 -3.753855 -3.7028618 -3.8234212 -3.9247313 -3.9679468 -4.0147314 -4.0661817 -4.0873466 -4.1036429][-4.1873384 -4.1352153 -4.1061692 -4.0862374 -4.0500298 -4.0005116 -3.9392853 -3.913281 -3.9770443 -4.041328 -4.0679865 -4.1008658 -4.1396513 -4.1580744 -4.1710205][-4.2353678 -4.218483 -4.2063684 -4.1853371 -4.1556716 -4.1245756 -4.0952439 -4.0877442 -4.11832 -4.1526818 -4.16978 -4.1891632 -4.2092156 -4.2171211 -4.223999][-4.2361345 -4.2457604 -4.2511849 -4.2429805 -4.221169 -4.2000122 -4.1869135 -4.1881571 -4.2051296 -4.2205567 -4.2257848 -4.2364016 -4.2426667 -4.2416439 -4.2435169][-4.2192516 -4.2392569 -4.2539396 -4.2556705 -4.2419128 -4.2303958 -4.2253165 -4.2273388 -4.2368488 -4.2410121 -4.2377124 -4.2400393 -4.2385197 -4.2315259 -4.2283864][-4.2062278 -4.2260547 -4.2409167 -4.247869 -4.2404518 -4.2316685 -4.2269163 -4.2255497 -4.2278161 -4.2259583 -4.221056 -4.2204151 -4.2194819 -4.2123 -4.2069044][-4.2223487 -4.2341752 -4.2416139 -4.2449908 -4.2399712 -4.2311988 -4.2258639 -4.223722 -4.2222614 -4.2206373 -4.2194057 -4.2209411 -4.22361 -4.2215772 -4.2189736]]...]
INFO - root - 2017-12-07 16:29:33.813304: step 29910, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.664 sec/batch; 55h:50m:04s remains)
INFO - root - 2017-12-07 16:29:40.445065: step 29920, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 58h:41m:40s remains)
INFO - root - 2017-12-07 16:29:47.249036: step 29930, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 61h:09m:13s remains)
INFO - root - 2017-12-07 16:29:54.029994: step 29940, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 55h:04m:00s remains)
INFO - root - 2017-12-07 16:30:00.814777: step 29950, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 52h:51m:11s remains)
INFO - root - 2017-12-07 16:30:07.623427: step 29960, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 55h:19m:28s remains)
INFO - root - 2017-12-07 16:30:14.397621: step 29970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 56h:36m:11s remains)
INFO - root - 2017-12-07 16:30:21.275905: step 29980, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 61h:05m:09s remains)
INFO - root - 2017-12-07 16:30:28.161998: step 29990, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 58h:22m:50s remains)
INFO - root - 2017-12-07 16:30:34.908676: step 30000, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 52h:25m:07s remains)
2017-12-07 16:30:35.699005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2727308 -4.2500234 -4.2194314 -4.1873765 -4.1640797 -4.1556659 -4.169714 -4.2005286 -4.2296257 -4.2487764 -4.2609792 -4.2652049 -4.2655935 -4.2645254 -4.2639303][-4.2488265 -4.2131691 -4.1677628 -4.1224089 -4.0882816 -4.072475 -4.09224 -4.1425233 -4.1879082 -4.2167315 -4.2352347 -4.2447891 -4.2487669 -4.2491207 -4.2485051][-4.2322955 -4.1809654 -4.1162605 -4.052094 -3.9980516 -3.9636564 -3.9867167 -4.0662136 -4.140029 -4.182003 -4.2065783 -4.2213573 -4.2314677 -4.23783 -4.2422037][-4.2214804 -4.1577907 -4.0763488 -3.9931197 -3.9116967 -3.8388858 -3.8557405 -3.9712403 -4.0843048 -4.147017 -4.1808295 -4.1998782 -4.2152114 -4.2291875 -4.2413383][-4.2207189 -4.1515675 -4.058404 -3.9510205 -3.8338315 -3.7264497 -3.7378564 -3.8802462 -4.0268779 -4.1102057 -4.154737 -4.1814609 -4.2018924 -4.22052 -4.2363253][-4.2173219 -4.1456604 -4.0484462 -3.9283204 -3.7917926 -3.6709206 -3.6757693 -3.8186631 -3.9811025 -4.0770421 -4.1299324 -4.1643929 -4.18945 -4.2087464 -4.2234564][-4.2102537 -4.1393423 -4.0473919 -3.9424386 -3.8255255 -3.7220087 -3.7202501 -3.8322344 -3.9738524 -4.0638914 -4.1192513 -4.1572018 -4.1827641 -4.2015123 -4.2138457][-4.2112494 -4.149734 -4.0716705 -3.9915445 -3.9144397 -3.8497305 -3.8481286 -3.9178989 -4.02021 -4.0920544 -4.1366343 -4.1654835 -4.1880441 -4.2075911 -4.2195663][-4.2118487 -4.1691365 -4.1110668 -4.051446 -4.0030775 -3.9699774 -3.9696364 -4.01095 -4.0896115 -4.1483755 -4.1764731 -4.1937633 -4.2110887 -4.2285666 -4.2356787][-4.2071409 -4.1824985 -4.1454134 -4.1038508 -4.0659113 -4.0421562 -4.03168 -4.0573564 -4.1236596 -4.177072 -4.1999493 -4.2140145 -4.2300138 -4.2433772 -4.2449989][-4.2111506 -4.1899605 -4.1535249 -4.1191845 -4.08194 -4.0471234 -4.0234747 -4.0392489 -4.0974116 -4.1587095 -4.1968079 -4.2231069 -4.2424445 -4.2560916 -4.2550511][-4.2383089 -4.2155056 -4.1717491 -4.1295543 -4.0792389 -4.0205641 -3.9830284 -3.9964674 -4.0545893 -4.129334 -4.189774 -4.2292075 -4.2545924 -4.271296 -4.2673373][-4.2703276 -4.2466278 -4.2009954 -4.150074 -4.0837493 -4.0077333 -3.9646285 -3.9867847 -4.0522447 -4.1324248 -4.2021341 -4.24586 -4.2738781 -4.2875981 -4.2766409][-4.28621 -4.2647958 -4.2200055 -4.1600437 -4.0826378 -4.0041919 -3.9743619 -4.0138383 -4.0827136 -4.1572227 -4.2218475 -4.2627611 -4.2841234 -4.2855582 -4.2629695][-4.2795281 -4.2595167 -4.2214894 -4.1655722 -4.0929317 -4.026927 -4.01736 -4.0661883 -4.1308208 -4.1937885 -4.246985 -4.2790651 -4.2856445 -4.2692442 -4.235589]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 16:30:42.992102: step 30010, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 57h:48m:26s remains)
INFO - root - 2017-12-07 16:30:49.545980: step 30020, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 57h:27m:29s remains)
INFO - root - 2017-12-07 16:30:56.273743: step 30030, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.668 sec/batch; 56h:05m:56s remains)
INFO - root - 2017-12-07 16:31:03.006313: step 30040, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 55h:24m:34s remains)
INFO - root - 2017-12-07 16:31:09.792670: step 30050, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 56h:29m:51s remains)
INFO - root - 2017-12-07 16:31:16.492909: step 30060, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.697 sec/batch; 58h:31m:22s remains)
INFO - root - 2017-12-07 16:31:23.346433: step 30070, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 58h:47m:11s remains)
INFO - root - 2017-12-07 16:31:30.182542: step 30080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 57h:03m:19s remains)
INFO - root - 2017-12-07 16:31:37.084469: step 30090, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 55h:52m:32s remains)
INFO - root - 2017-12-07 16:31:43.823354: step 30100, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 53h:46m:22s remains)
2017-12-07 16:31:44.598473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181778 -4.194067 -4.183609 -4.19833 -4.2180076 -4.2315235 -4.2324405 -4.2117348 -4.1695976 -4.1379867 -4.1559753 -4.2121134 -4.2630825 -4.2889624 -4.2897878][-4.20045 -4.1680794 -4.15182 -4.1625204 -4.1810846 -4.1928577 -4.1874442 -4.156991 -4.1023445 -4.0676694 -4.0969491 -4.1692262 -4.2345366 -4.2709222 -4.2770061][-4.2118945 -4.1773548 -4.1505456 -4.1447258 -4.1578684 -4.1707244 -4.1598864 -4.118434 -4.0595632 -4.0383968 -4.081171 -4.1563311 -4.2210631 -4.2593083 -4.2682691][-4.2364354 -4.2091327 -4.1774693 -4.1596608 -4.1641684 -4.177556 -4.1646123 -4.1193419 -4.068162 -4.0641823 -4.1084871 -4.1724372 -4.2240882 -4.2531128 -4.2604909][-4.2582326 -4.2401857 -4.2158527 -4.1971569 -4.1933789 -4.1963868 -4.1776609 -4.1362896 -4.1057367 -4.1138325 -4.1501455 -4.1980844 -4.235239 -4.251255 -4.2502017][-4.2717443 -4.2602849 -4.2446933 -4.2262726 -4.2070808 -4.1856437 -4.1387405 -4.0867486 -4.0806284 -4.1227913 -4.1714172 -4.214941 -4.2459292 -4.2548776 -4.2469172][-4.2783971 -4.269228 -4.2542768 -4.2284913 -4.1858892 -4.1247373 -4.0265622 -3.9440041 -3.9747488 -4.0794964 -4.1628327 -4.2167006 -4.2514334 -4.2631221 -4.2554393][-4.2785892 -4.27267 -4.2506943 -4.2067561 -4.1354017 -4.0331445 -3.8883753 -3.7788262 -3.8563578 -4.0286474 -4.1511006 -4.2187657 -4.2594633 -4.2733912 -4.2636762][-4.272697 -4.2732387 -4.2438397 -4.1824584 -4.0948668 -3.9831381 -3.8464429 -3.7646151 -3.8640802 -4.0438604 -4.166822 -4.229785 -4.264503 -4.2705131 -4.2545867][-4.25892 -4.262394 -4.2339644 -4.1777062 -4.1043477 -4.0232854 -3.941715 -3.9102323 -3.9860845 -4.1079607 -4.1902823 -4.2301159 -4.2500749 -4.2493534 -4.2285304][-4.2313023 -4.23373 -4.2217927 -4.1974897 -4.1628709 -4.1184936 -4.0756354 -4.0614839 -4.0969462 -4.1520357 -4.1870532 -4.2036667 -4.2159457 -4.216074 -4.1992397][-4.2004328 -4.2025518 -4.2135525 -4.2262893 -4.2247448 -4.19916 -4.1663232 -4.1453376 -4.1421638 -4.1400547 -4.1365204 -4.1416168 -4.15702 -4.1675186 -4.1634512][-4.1744833 -4.1780038 -4.20521 -4.2365065 -4.2499175 -4.2305412 -4.1962981 -4.1616297 -4.1312222 -4.0991707 -4.0754747 -4.079309 -4.1029782 -4.1303234 -4.1435914][-4.1520882 -4.1532531 -4.1847653 -4.2174506 -4.2319207 -4.2194538 -4.1928196 -4.1621513 -4.1342068 -4.0994735 -4.0723372 -4.0733166 -4.0957956 -4.1274824 -4.14966][-4.1316338 -4.1269379 -4.1471286 -4.1687059 -4.1820145 -4.1839643 -4.1767216 -4.1719661 -4.1731725 -4.1579537 -4.1398582 -4.13578 -4.1476264 -4.1674232 -4.1841021]]...]
INFO - root - 2017-12-07 16:31:51.388158: step 30110, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 58h:23m:51s remains)
INFO - root - 2017-12-07 16:31:58.062064: step 30120, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 54h:46m:25s remains)
INFO - root - 2017-12-07 16:32:04.808093: step 30130, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 53h:49m:28s remains)
INFO - root - 2017-12-07 16:32:11.637009: step 30140, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 60h:09m:12s remains)
INFO - root - 2017-12-07 16:32:18.470362: step 30150, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 61h:59m:46s remains)
INFO - root - 2017-12-07 16:32:25.242534: step 30160, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 55h:38m:44s remains)
INFO - root - 2017-12-07 16:32:32.019188: step 30170, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:29m:20s remains)
INFO - root - 2017-12-07 16:32:38.876129: step 30180, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.682 sec/batch; 57h:16m:45s remains)
INFO - root - 2017-12-07 16:32:45.721965: step 30190, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 56h:58m:12s remains)
INFO - root - 2017-12-07 16:32:52.469574: step 30200, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 59h:47m:00s remains)
2017-12-07 16:32:53.204285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2919731 -4.2652531 -4.2418466 -4.2233591 -4.2044873 -4.1998882 -4.2161727 -4.2203937 -4.1981673 -4.1788588 -4.1788745 -4.172718 -4.1593814 -4.16174 -4.1629972][-4.3011775 -4.2715025 -4.2482491 -4.2299294 -4.2109723 -4.2055931 -4.2231388 -4.2315784 -4.2113309 -4.1937556 -4.196722 -4.1944532 -4.1841664 -4.186573 -4.1945486][-4.3136277 -4.2845087 -4.2617936 -4.2445397 -4.2209764 -4.2099905 -4.226769 -4.2392564 -4.2215023 -4.2040191 -4.206789 -4.207355 -4.203424 -4.2115879 -4.2276769][-4.3232965 -4.2979579 -4.2759509 -4.2579 -4.2253819 -4.2019682 -4.2115164 -4.2314124 -4.2267652 -4.2139392 -4.2155752 -4.2156463 -4.2122636 -4.2219491 -4.240036][-4.3278542 -4.3060746 -4.2850423 -4.2605052 -4.2113 -4.1633329 -4.1631632 -4.1929407 -4.2086554 -4.2142954 -4.2291226 -4.2320614 -4.2284417 -4.2336092 -4.246325][-4.325892 -4.3054194 -4.2798567 -4.2433887 -4.1681905 -4.0816255 -4.0754986 -4.1324782 -4.1755886 -4.2067857 -4.2406478 -4.2476635 -4.2379756 -4.2398233 -4.2497673][-4.3226109 -4.3017888 -4.2671356 -4.2145834 -4.1089807 -3.9796996 -3.9686837 -4.0684075 -4.1452627 -4.1908584 -4.2324295 -4.234489 -4.2162886 -4.2188959 -4.2392778][-4.318224 -4.2988062 -4.2594466 -4.1970668 -4.0818605 -3.9441404 -3.9351921 -4.0545149 -4.14071 -4.18392 -4.213738 -4.2037783 -4.1762867 -4.1821961 -4.2180471][-4.3105955 -4.2951164 -4.2596383 -4.1990452 -4.1028614 -4.0044813 -4.012516 -4.1091566 -4.1724935 -4.1980219 -4.2094474 -4.1860523 -4.1479764 -4.1540942 -4.1993361][-4.3055778 -4.2969823 -4.2740192 -4.2259822 -4.1600537 -4.1066885 -4.1259847 -4.1927118 -4.2300453 -4.2396359 -4.2395768 -4.2093039 -4.1649857 -4.1644335 -4.2029128][-4.3070292 -4.3037367 -4.2922511 -4.2630043 -4.2264838 -4.2000637 -4.2143054 -4.25587 -4.2787108 -4.2845569 -4.2842345 -4.2612491 -4.2203789 -4.2140636 -4.2402353][-4.315217 -4.3157415 -4.3114176 -4.2969694 -4.2778592 -4.2652559 -4.2728324 -4.296526 -4.3125253 -4.3178005 -4.3183408 -4.3066778 -4.2810616 -4.27389 -4.2883053][-4.3218393 -4.324882 -4.3243661 -4.317142 -4.3070054 -4.2983017 -4.303215 -4.3169894 -4.3289294 -4.3380165 -4.3409009 -4.3379226 -4.3244681 -4.3191094 -4.3245864][-4.3264804 -4.3297768 -4.3291445 -4.32339 -4.316781 -4.3103237 -4.3137369 -4.3215213 -4.3313341 -4.3419509 -4.3463225 -4.3482852 -4.34365 -4.3397374 -4.3394728][-4.3335743 -4.3355927 -4.3342209 -4.3286648 -4.324254 -4.3213391 -4.3212214 -4.3250713 -4.33318 -4.3412533 -4.3450193 -4.3477268 -4.3466673 -4.3441677 -4.3415465]]...]
INFO - root - 2017-12-07 16:32:59.974898: step 30210, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 55h:50m:32s remains)
INFO - root - 2017-12-07 16:33:06.597071: step 30220, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 58h:36m:07s remains)
INFO - root - 2017-12-07 16:33:13.481807: step 30230, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 58h:18m:23s remains)
INFO - root - 2017-12-07 16:33:20.340417: step 30240, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 55h:27m:12s remains)
INFO - root - 2017-12-07 16:33:27.163956: step 30250, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 55h:27m:36s remains)
INFO - root - 2017-12-07 16:33:33.936932: step 30260, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 57h:37m:15s remains)
INFO - root - 2017-12-07 16:33:40.865935: step 30270, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:17m:58s remains)
INFO - root - 2017-12-07 16:33:47.706879: step 30280, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 56h:55m:22s remains)
INFO - root - 2017-12-07 16:33:54.481736: step 30290, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 55h:37m:51s remains)
INFO - root - 2017-12-07 16:34:01.256683: step 30300, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 56h:34m:32s remains)
2017-12-07 16:34:02.023726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2931657 -4.2639608 -4.2299929 -4.2173285 -4.22233 -4.2229719 -4.2227879 -4.2359996 -4.2510524 -4.2546306 -4.2480617 -4.2336545 -4.2047205 -4.1729894 -4.1512713][-4.2998576 -4.2706113 -4.2348037 -4.2162576 -4.2125831 -4.2072015 -4.2045736 -4.2160096 -4.2298937 -4.2327108 -4.2282677 -4.2224212 -4.2040915 -4.1828184 -4.1654019][-4.2914653 -4.2591262 -4.2234445 -4.2047696 -4.1979127 -4.1910295 -4.1904678 -4.201427 -4.2111769 -4.2088523 -4.2074203 -4.2132859 -4.213522 -4.2061462 -4.1902256][-4.2862453 -4.2523184 -4.21759 -4.2004809 -4.18761 -4.1774807 -4.1788311 -4.1875486 -4.1898575 -4.18251 -4.1834354 -4.2010026 -4.2189226 -4.2217383 -4.2073822][-4.2828455 -4.2556953 -4.2274103 -4.2107606 -4.1859961 -4.1596537 -4.1510839 -4.1512957 -4.1475019 -4.143074 -4.1539984 -4.1838403 -4.2142653 -4.2231035 -4.211997][-4.2650852 -4.2428393 -4.2205625 -4.2056928 -4.1751037 -4.1403742 -4.1199102 -4.1045837 -4.0936279 -4.1041651 -4.1386237 -4.182229 -4.2179623 -4.2269435 -4.2156491][-4.2375922 -4.21973 -4.2021275 -4.1835427 -4.1482611 -4.1107607 -4.0820193 -4.04062 -4.0149417 -4.0466285 -4.1144366 -4.1761365 -4.2132015 -4.2246428 -4.2152677][-4.2058535 -4.1962085 -4.1843152 -4.1646204 -4.1234779 -4.0800834 -4.0355215 -3.9584856 -3.9084587 -3.9561267 -4.0534167 -4.1347046 -4.1861033 -4.2087007 -4.2035875][-4.1917663 -4.1905961 -4.1830831 -4.1653166 -4.1257787 -4.0802803 -4.0269604 -3.9369793 -3.8799944 -3.9284868 -4.0265 -4.1077862 -4.16348 -4.1935263 -4.1923432][-4.2173247 -4.2226338 -4.2208486 -4.2094946 -4.1783595 -4.142693 -4.0997181 -4.0360012 -3.9979746 -4.0227456 -4.0780187 -4.1287942 -4.1700587 -4.1963735 -4.195735][-4.2462296 -4.2514729 -4.2534881 -4.246213 -4.2211046 -4.194325 -4.1642241 -4.1240926 -4.1004548 -4.107059 -4.1305556 -4.1574287 -4.1818366 -4.1983814 -4.198051][-4.2528577 -4.255703 -4.2566228 -4.2500887 -4.2292805 -4.2112508 -4.1928859 -4.1682863 -4.1541262 -4.1495671 -4.1537433 -4.1671925 -4.1817183 -4.1917529 -4.1919761][-4.2540321 -4.2540164 -4.2494512 -4.238246 -4.2204604 -4.2116218 -4.2058873 -4.1932244 -4.1818042 -4.1686339 -4.160274 -4.1681614 -4.1805835 -4.1883268 -4.1892204][-4.2593451 -4.2573462 -4.2455974 -4.2279615 -4.2130866 -4.2120161 -4.2192683 -4.217051 -4.2042284 -4.1828671 -4.1680808 -4.1723914 -4.179872 -4.1841779 -4.1892786][-4.2546387 -4.2503934 -4.2343531 -4.2128229 -4.2002125 -4.2043695 -4.2199244 -4.2258878 -4.215416 -4.1939521 -4.1740608 -4.1690164 -4.1695452 -4.1734548 -4.1881533]]...]
INFO - root - 2017-12-07 16:34:08.619147: step 30310, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 54h:30m:49s remains)
INFO - root - 2017-12-07 16:34:14.954594: step 30320, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 51h:35m:49s remains)
INFO - root - 2017-12-07 16:34:21.693607: step 30330, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 58h:29m:39s remains)
INFO - root - 2017-12-07 16:34:28.398097: step 30340, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 60h:41m:17s remains)
INFO - root - 2017-12-07 16:34:35.168339: step 30350, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 57h:56m:07s remains)
INFO - root - 2017-12-07 16:34:42.128151: step 30360, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 55h:58m:22s remains)
INFO - root - 2017-12-07 16:34:48.966566: step 30370, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 55h:37m:45s remains)
INFO - root - 2017-12-07 16:34:55.850739: step 30380, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 59h:25m:32s remains)
INFO - root - 2017-12-07 16:35:02.760885: step 30390, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 61h:39m:36s remains)
INFO - root - 2017-12-07 16:35:09.498406: step 30400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 58h:41m:55s remains)
2017-12-07 16:35:10.227267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2349329 -4.24182 -4.2465653 -4.2485743 -4.2498693 -4.2511315 -4.2512436 -4.2500033 -4.2478313 -4.2452393 -4.2430696 -4.2415714 -4.2399445 -4.2381029 -4.2364659][-4.2585673 -4.2617559 -4.2641411 -4.2644982 -4.2637939 -4.2630372 -4.2617373 -4.2600656 -4.2580142 -4.2554359 -4.2528629 -4.2510839 -4.2494922 -4.2481303 -4.246623][-4.2774177 -4.2774878 -4.2769494 -4.2733469 -4.2695642 -4.2680368 -4.2687354 -4.2704825 -4.271935 -4.2718863 -4.2707024 -4.2701054 -4.2697859 -4.2699242 -4.2695732][-4.2764406 -4.2734942 -4.2667279 -4.25451 -4.244482 -4.2417173 -4.2467413 -4.2558336 -4.2644677 -4.2709169 -4.27504 -4.277832 -4.2799687 -4.2821336 -4.2837567][-4.2485604 -4.2425728 -4.229528 -4.2076821 -4.1900506 -4.1843376 -4.1918182 -4.2081418 -4.2260523 -4.2426991 -4.2551384 -4.2624044 -4.2663321 -4.2694306 -4.2723637][-4.2078533 -4.1983585 -4.1814561 -4.1558752 -4.1350923 -4.1255922 -4.1296239 -4.1481428 -4.173583 -4.1984553 -4.2168827 -4.2276416 -4.2331042 -4.2370028 -4.2413983][-4.1843948 -4.16982 -4.1496711 -4.1235232 -4.1001964 -4.0824242 -4.0768 -4.0936561 -4.1255493 -4.1558185 -4.1756654 -4.1862755 -4.1910706 -4.1955128 -4.2029343][-4.19316 -4.1753397 -4.1551576 -4.1307158 -4.1025581 -4.0696597 -4.0465436 -4.0536304 -4.0860624 -4.1177058 -4.1356249 -4.1419029 -4.1433077 -4.1493111 -4.16242][-4.2225866 -4.2034206 -4.1837578 -4.1631355 -4.136188 -4.0980158 -4.0659657 -4.0632849 -4.0860505 -4.1098609 -4.1212406 -4.1209197 -4.1155677 -4.1193085 -4.1351733][-4.2532849 -4.2350326 -4.2156372 -4.1974554 -4.17456 -4.1412048 -4.1135635 -4.1085362 -4.1221123 -4.1360135 -4.140954 -4.1371455 -4.128685 -4.1299491 -4.1430936][-4.2768965 -4.2633934 -4.2475662 -4.2344351 -4.2179089 -4.1946111 -4.1758189 -4.1712613 -4.177412 -4.1815119 -4.1794553 -4.1725616 -4.1627893 -4.1618948 -4.1710405][-4.2880135 -4.2811837 -4.2713628 -4.2643013 -4.254663 -4.241426 -4.2311778 -4.2283626 -4.2306857 -4.2309251 -4.2268767 -4.2186337 -4.207356 -4.2036443 -4.2088418][-4.2803111 -4.2794647 -4.2766232 -4.2758913 -4.273335 -4.2684708 -4.2633724 -4.260479 -4.2599363 -4.2595015 -4.257443 -4.2501807 -4.23831 -4.2328486 -4.2361126][-4.2612338 -4.2633381 -4.2641249 -4.2650971 -4.26549 -4.2649889 -4.2608671 -4.257134 -4.25655 -4.2582483 -4.2587991 -4.2526097 -4.2421389 -4.2377081 -4.2426295][-4.2459631 -4.2485118 -4.250452 -4.2508135 -4.2494006 -4.24612 -4.2369876 -4.2287769 -4.2273355 -4.2311316 -4.2347765 -4.2313819 -4.2238212 -4.2222629 -4.2307572]]...]
INFO - root - 2017-12-07 16:35:16.889178: step 30410, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 57h:57m:31s remains)
INFO - root - 2017-12-07 16:35:23.552432: step 30420, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.745 sec/batch; 62h:32m:02s remains)
INFO - root - 2017-12-07 16:35:30.325124: step 30430, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 57h:49m:52s remains)
INFO - root - 2017-12-07 16:35:37.156721: step 30440, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 54h:01m:51s remains)
INFO - root - 2017-12-07 16:35:43.938440: step 30450, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 57h:12m:27s remains)
INFO - root - 2017-12-07 16:35:50.795072: step 30460, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 59h:41m:45s remains)
INFO - root - 2017-12-07 16:35:57.616708: step 30470, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 56h:03m:07s remains)
INFO - root - 2017-12-07 16:36:04.469400: step 30480, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 56h:32m:33s remains)
INFO - root - 2017-12-07 16:36:11.320413: step 30490, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 54h:18m:01s remains)
INFO - root - 2017-12-07 16:36:18.168827: step 30500, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 59h:05m:56s remains)
2017-12-07 16:36:18.966216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3319468 -4.333241 -4.336719 -4.34065 -4.3430886 -4.3422656 -4.3393445 -4.3373051 -4.3366981 -4.337276 -4.3383183 -4.3388171 -4.3373246 -4.334147 -4.3310723][-4.3274145 -4.3300252 -4.335433 -4.3400664 -4.3414083 -4.3374462 -4.3305044 -4.3252559 -4.3241978 -4.3274679 -4.3330641 -4.336977 -4.3361893 -4.3315463 -4.3256702][-4.3212314 -4.3254824 -4.3311844 -4.3347149 -4.3329906 -4.3228712 -4.3081446 -4.2943063 -4.2864089 -4.2892 -4.3027678 -4.31847 -4.3277354 -4.3277864 -4.322063][-4.3207417 -4.3251348 -4.3275781 -4.325408 -4.3153644 -4.2953324 -4.2688713 -4.2406344 -4.217895 -4.2149296 -4.2393675 -4.2770457 -4.3092017 -4.3252139 -4.3260226][-4.3249941 -4.3270483 -4.3226309 -4.3107705 -4.2863708 -4.2489882 -4.2042103 -4.1556511 -4.1082416 -4.0951653 -4.1369262 -4.2095146 -4.277041 -4.318265 -4.3313446][-4.3268261 -4.3256526 -4.3143697 -4.291913 -4.2515087 -4.1932464 -4.1211257 -4.036921 -3.9476876 -3.9173744 -3.9839182 -4.1037049 -4.2172 -4.2918582 -4.32424][-4.3258171 -4.32453 -4.3123341 -4.2848549 -4.2345815 -4.1620703 -4.0649853 -3.9381347 -3.7954614 -3.7370222 -3.8235781 -3.9855063 -4.1395683 -4.2462411 -4.3017387][-4.3248367 -4.32603 -4.316844 -4.290071 -4.2420769 -4.1725526 -4.0725813 -3.9328911 -3.7677274 -3.6861873 -3.7639382 -3.9289513 -4.0919819 -4.2114286 -4.2805848][-4.3284926 -4.33045 -4.3228922 -4.2989545 -4.2604318 -4.2077761 -4.1291871 -4.0195284 -3.8891184 -3.8171296 -3.8620563 -3.980298 -4.1101818 -4.2132163 -4.2769866][-4.3367867 -4.33929 -4.3346524 -4.317173 -4.2926087 -4.2611132 -4.2115607 -4.1416807 -4.0584874 -4.008379 -4.0261865 -4.0929313 -4.1766071 -4.2483811 -4.292809][-4.3438191 -4.3470445 -4.3469429 -4.3400035 -4.3306794 -4.3179965 -4.2916365 -4.2519283 -4.2030549 -4.1706328 -4.17291 -4.2051015 -4.2518129 -4.2935157 -4.3173356][-4.3462038 -4.350162 -4.3534908 -4.3538 -4.3541675 -4.3530278 -4.3418007 -4.3217158 -4.2953038 -4.2747583 -4.2699366 -4.2820377 -4.3041673 -4.3243928 -4.3343496][-4.3389053 -4.3440261 -4.3511562 -4.3578048 -4.3642411 -4.3697362 -4.368907 -4.3612957 -4.3483768 -4.3351874 -4.3261971 -4.3248591 -4.3310423 -4.3380046 -4.3404403][-4.3231087 -4.3270683 -4.3358088 -4.3460951 -4.3553276 -4.3622241 -4.3652854 -4.3638043 -4.35914 -4.3528476 -4.3467975 -4.3426833 -4.3423872 -4.34318 -4.342185][-4.3041096 -4.30512 -4.3135576 -4.3256035 -4.3367963 -4.3446789 -4.3490248 -4.3501129 -4.3490896 -4.3468485 -4.3446879 -4.3433228 -4.342989 -4.3421483 -4.340641]]...]
INFO - root - 2017-12-07 16:36:25.778164: step 30510, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 54h:38m:03s remains)
INFO - root - 2017-12-07 16:36:32.485644: step 30520, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 57h:56m:05s remains)
INFO - root - 2017-12-07 16:36:39.258915: step 30530, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 59h:24m:53s remains)
INFO - root - 2017-12-07 16:36:46.029761: step 30540, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 58h:45m:10s remains)
INFO - root - 2017-12-07 16:36:52.855132: step 30550, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 55h:03m:58s remains)
INFO - root - 2017-12-07 16:36:59.618284: step 30560, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 56h:26m:37s remains)
INFO - root - 2017-12-07 16:37:06.578838: step 30570, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 61h:52m:05s remains)
INFO - root - 2017-12-07 16:37:13.376828: step 30580, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 58h:18m:27s remains)
INFO - root - 2017-12-07 16:37:20.153749: step 30590, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:11m:37s remains)
INFO - root - 2017-12-07 16:37:26.917813: step 30600, loss = 2.08, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 52h:42m:51s remains)
2017-12-07 16:37:27.703559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.281847 -4.2831883 -4.2812095 -4.2756677 -4.2709551 -4.26798 -4.2648883 -4.2640572 -4.2602696 -4.2493196 -4.2340393 -4.2235627 -4.2249994 -4.2356062 -4.2487965][-4.3013678 -4.3061876 -4.3059273 -4.299139 -4.2873054 -4.2734332 -4.2629251 -4.2628703 -4.264647 -4.2592473 -4.2442832 -4.2304835 -4.2272658 -4.2376423 -4.2545018][-4.3150306 -4.3219848 -4.3234158 -4.3116517 -4.2842522 -4.2500296 -4.2235909 -4.2227635 -4.235527 -4.2434478 -4.2381186 -4.2295637 -4.2256479 -4.2376761 -4.2597637][-4.3066149 -4.310504 -4.3085837 -4.2856827 -4.2403603 -4.1868739 -4.1470361 -4.1525621 -4.1834817 -4.20762 -4.2160087 -4.2153425 -4.2130065 -4.2259521 -4.2532468][-4.2796693 -4.2792277 -4.2737269 -4.2443943 -4.1902089 -4.1229987 -4.0643873 -4.0760832 -4.1348524 -4.1752253 -4.1917734 -4.1954956 -4.1922383 -4.2028756 -4.2327614][-4.2565017 -4.246572 -4.2335806 -4.1975861 -4.1317286 -4.0400386 -3.9510894 -3.9651165 -4.0750556 -4.150445 -4.1790009 -4.1912737 -4.1834011 -4.1835518 -4.2105165][-4.2354722 -4.2137947 -4.185184 -4.1326146 -4.0417652 -3.9094825 -3.77602 -3.7941618 -3.9748259 -4.1092453 -4.1604223 -4.1820512 -4.1770878 -4.1713028 -4.1955528][-4.2157369 -4.187891 -4.1468692 -4.078186 -3.964994 -3.7904224 -3.6056523 -3.6153178 -3.850003 -4.0405388 -4.1218948 -4.1556687 -4.1606431 -4.155323 -4.1773844][-4.2216263 -4.20734 -4.172142 -4.1077728 -4.00635 -3.8508792 -3.6805773 -3.6666334 -3.8574023 -4.035985 -4.1206355 -4.1554761 -4.1636095 -4.1495728 -4.1603847][-4.2559648 -4.2695951 -4.2495284 -4.1961403 -4.1193824 -4.014677 -3.9026155 -3.8704596 -3.9739482 -4.1027451 -4.1778278 -4.2067447 -4.2088232 -4.1880269 -4.1851153][-4.256722 -4.2982478 -4.2989163 -4.2621822 -4.2081623 -4.1402755 -4.0729718 -4.0397377 -4.08657 -4.1690774 -4.2340446 -4.2613144 -4.2643681 -4.2516236 -4.2499018][-4.227562 -4.2831683 -4.2998157 -4.2778378 -4.2404127 -4.194612 -4.1484585 -4.1218505 -4.148581 -4.2092562 -4.264905 -4.289927 -4.2985497 -4.3005791 -4.3075318][-4.1995792 -4.2478404 -4.2746153 -4.2700844 -4.2519345 -4.2249732 -4.1913738 -4.1667485 -4.1842713 -4.2359395 -4.2879748 -4.3121119 -4.322341 -4.3327918 -4.3442407][-4.2247577 -4.2412148 -4.2562966 -4.2621694 -4.2566938 -4.2391453 -4.2134962 -4.1916957 -4.2005529 -4.2438745 -4.2925286 -4.3196712 -4.3350863 -4.3495393 -4.3570747][-4.2636786 -4.251421 -4.2442107 -4.2494588 -4.2457509 -4.2322645 -4.214478 -4.1963348 -4.1973171 -4.2334261 -4.2796025 -4.3112311 -4.3333912 -4.3450313 -4.3415608]]...]
INFO - root - 2017-12-07 16:37:34.436426: step 30610, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 59h:06m:15s remains)
INFO - root - 2017-12-07 16:37:40.788113: step 30620, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 55h:08m:21s remains)
INFO - root - 2017-12-07 16:37:47.528493: step 30630, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 54h:06m:50s remains)
INFO - root - 2017-12-07 16:37:54.220725: step 30640, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 55h:00m:52s remains)
INFO - root - 2017-12-07 16:38:01.061081: step 30650, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.736 sec/batch; 61h:44m:01s remains)
INFO - root - 2017-12-07 16:38:07.827072: step 30660, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 56h:10m:04s remains)
INFO - root - 2017-12-07 16:38:14.725715: step 30670, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 55h:18m:31s remains)
INFO - root - 2017-12-07 16:38:21.521931: step 30680, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 56h:24m:23s remains)
INFO - root - 2017-12-07 16:38:28.397943: step 30690, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 58h:20m:41s remains)
INFO - root - 2017-12-07 16:38:35.231877: step 30700, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 58h:45m:55s remains)
2017-12-07 16:38:35.894181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3088274 -4.2965412 -4.2896929 -4.2878265 -4.2833514 -4.2812419 -4.282083 -4.2846866 -4.2859545 -4.2857771 -4.2834377 -4.2825527 -4.2835631 -4.2852583 -4.2868161][-4.2872715 -4.2730513 -4.2689285 -4.2669415 -4.2569609 -4.2502832 -4.2507582 -4.2559118 -4.2611551 -4.2654147 -4.2681327 -4.2712216 -4.2723217 -4.2733054 -4.2768087][-4.2704988 -4.2567124 -4.2548995 -4.247704 -4.2255359 -4.2094331 -4.2047753 -4.2094316 -4.2213473 -4.233861 -4.2465587 -4.2579741 -4.2616005 -4.2619934 -4.2684984][-4.2431984 -4.2275934 -4.22282 -4.2086635 -4.1776404 -4.1544375 -4.1441007 -4.1515985 -4.1728725 -4.1969738 -4.220408 -4.2390952 -4.2456083 -4.2445583 -4.2513642][-4.1993966 -4.1815853 -4.1751952 -4.1644373 -4.1400347 -4.1178904 -4.1024532 -4.1070127 -4.1354804 -4.1690621 -4.1927605 -4.2046528 -4.2097683 -4.2071114 -4.2151856][-4.1685619 -4.1526847 -4.150794 -4.150116 -4.1317291 -4.1029763 -4.0669103 -4.0515113 -4.0827603 -4.1267629 -4.149509 -4.1544027 -4.1567359 -4.1594744 -4.1767564][-4.1647644 -4.1496086 -4.1465392 -4.1426172 -4.1167197 -4.0695171 -4.0013151 -3.9521654 -3.9889376 -4.0569615 -4.0920439 -4.0988503 -4.104497 -4.1198606 -4.1490011][-4.1741443 -4.155077 -4.1419182 -4.1228948 -4.0862541 -4.0268769 -3.9398835 -3.8768625 -3.9325745 -4.0275974 -4.07886 -4.0859666 -4.0909376 -4.1136694 -4.1487494][-4.1760297 -4.1475339 -4.1262293 -4.105391 -4.0780716 -4.03927 -3.9870429 -3.9617035 -4.014894 -4.0914321 -4.1311173 -4.1269937 -4.1238413 -4.1406703 -4.1699204][-4.1728067 -4.1440043 -4.133626 -4.1302719 -4.1202712 -4.0990405 -4.0722647 -4.0649614 -4.1011019 -4.1518087 -4.1762171 -4.164856 -4.1548276 -4.1660795 -4.1925845][-4.1866446 -4.1655016 -4.1682291 -4.1774817 -4.1754365 -4.1592336 -4.1408777 -4.1360579 -4.1598449 -4.1946063 -4.2109385 -4.1975722 -4.1849322 -4.1940846 -4.2193713][-4.2190785 -4.2054348 -4.2102461 -4.221725 -4.2246871 -4.2148981 -4.2003098 -4.1954269 -4.2094297 -4.2313094 -4.2430191 -4.23572 -4.2262168 -4.2313151 -4.2520285][-4.2580585 -4.2521982 -4.2572684 -4.2675319 -4.2715015 -4.2653918 -4.2559566 -4.2536526 -4.2595854 -4.2694736 -4.2750096 -4.2720089 -4.2666812 -4.2708116 -4.2855506][-4.2870622 -4.2854881 -4.2918162 -4.3009157 -4.3056178 -4.3027062 -4.2982321 -4.2990818 -4.3015623 -4.30355 -4.3026891 -4.2986894 -4.2957745 -4.2985878 -4.3070416][-4.3087792 -4.3056941 -4.3093996 -4.3160558 -4.320148 -4.3204269 -4.3202047 -4.3226442 -4.3251657 -4.3249097 -4.32245 -4.3182306 -4.3144794 -4.313695 -4.3176789]]...]
INFO - root - 2017-12-07 16:38:42.607110: step 30710, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 56h:18m:15s remains)
INFO - root - 2017-12-07 16:38:49.318260: step 30720, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 62h:18m:56s remains)
INFO - root - 2017-12-07 16:38:56.125150: step 30730, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 57h:47m:20s remains)
INFO - root - 2017-12-07 16:39:02.926970: step 30740, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 57h:22m:46s remains)
INFO - root - 2017-12-07 16:39:09.777447: step 30750, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 55h:24m:40s remains)
INFO - root - 2017-12-07 16:39:16.601626: step 30760, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.698 sec/batch; 58h:27m:59s remains)
INFO - root - 2017-12-07 16:39:23.474543: step 30770, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 58h:16m:13s remains)
INFO - root - 2017-12-07 16:39:30.269583: step 30780, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 57h:30m:19s remains)
INFO - root - 2017-12-07 16:39:37.015177: step 30790, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 54h:06m:47s remains)
INFO - root - 2017-12-07 16:39:43.865336: step 30800, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 56h:43m:00s remains)
2017-12-07 16:39:44.623312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2922955 -4.2757249 -4.2671366 -4.2626505 -4.2594824 -4.2568073 -4.2487168 -4.2313147 -4.218842 -4.2175379 -4.2210727 -4.2188721 -4.213377 -4.2148018 -4.2267694][-4.275403 -4.2560992 -4.2488275 -4.2442126 -4.23564 -4.2268639 -4.2117209 -4.1844234 -4.1693277 -4.1739311 -4.1871462 -4.1926684 -4.1886339 -4.1885543 -4.2001209][-4.2671013 -4.251276 -4.2475605 -4.2404609 -4.2213035 -4.2006593 -4.1745582 -4.1386652 -4.1244226 -4.1369023 -4.1612992 -4.1751561 -4.1711092 -4.1663966 -4.1709189][-4.2643394 -4.2527642 -4.2488565 -4.2393708 -4.2117147 -4.1815419 -4.1457534 -4.0992918 -4.0864086 -4.1084437 -4.1403923 -4.1603107 -4.1565018 -4.14829 -4.142632][-4.2657094 -4.257174 -4.2547975 -4.2429657 -4.2103362 -4.1735711 -4.1265168 -4.0625825 -4.0470142 -4.0852375 -4.1294222 -4.1544566 -4.1514583 -4.1404357 -4.1244178][-4.2675786 -4.2612944 -4.2576246 -4.2434797 -4.2033539 -4.1513472 -4.0764446 -3.9766867 -3.9507508 -4.0222554 -4.0967751 -4.1326804 -4.1354713 -4.1210332 -4.09917][-4.2625489 -4.2562003 -4.2485938 -4.2300086 -4.1850214 -4.1164904 -4.0143461 -3.8722672 -3.8249493 -3.9361594 -4.0463567 -4.0996437 -4.1115222 -4.0975866 -4.0745845][-4.2482934 -4.2407703 -4.2341785 -4.2170448 -4.176043 -4.1160345 -4.0226493 -3.8894386 -3.8334639 -3.9454007 -4.0516424 -4.1049066 -4.1197462 -4.1089005 -4.085237][-4.2341104 -4.225852 -4.2227731 -4.2102318 -4.1793485 -4.1394606 -4.0811419 -4.000422 -3.9677749 -4.0394626 -4.1081972 -4.1461315 -4.1583066 -4.1474695 -4.1259174][-4.2371087 -4.230094 -4.2289276 -4.2172732 -4.1940627 -4.1681638 -4.1328058 -4.0894008 -4.0730424 -4.1160188 -4.1590462 -4.185945 -4.195313 -4.1858306 -4.1696019][-4.2553968 -4.2505755 -4.2502861 -4.2405128 -4.2246418 -4.2069616 -4.1831574 -4.156817 -4.1470413 -4.1754484 -4.2031407 -4.22004 -4.2264957 -4.2192588 -4.2080793][-4.2781305 -4.2751226 -4.274785 -4.2658067 -4.2536192 -4.2410145 -4.2242074 -4.2033153 -4.1987405 -4.2216439 -4.2407932 -4.2505512 -4.2532196 -4.2455244 -4.2351952][-4.2915931 -4.2908444 -4.2929082 -4.2875 -4.2777858 -4.2695961 -4.2597842 -4.2477808 -4.2465439 -4.2630081 -4.276835 -4.2833953 -4.2845788 -4.278307 -4.2678003][-4.2998624 -4.301311 -4.306571 -4.3056149 -4.2988811 -4.295218 -4.2933283 -4.2891059 -4.28883 -4.2974854 -4.3068881 -4.3121247 -4.3133006 -4.310185 -4.3007121][-4.3048506 -4.3038316 -4.3080873 -4.310801 -4.3087978 -4.3083186 -4.3105283 -4.309896 -4.3087473 -4.310822 -4.3157043 -4.32069 -4.3220797 -4.3197832 -4.3134193]]...]
INFO - root - 2017-12-07 16:39:51.361315: step 30810, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 59h:35m:04s remains)
INFO - root - 2017-12-07 16:39:57.931622: step 30820, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 52h:59m:31s remains)
INFO - root - 2017-12-07 16:40:04.901437: step 30830, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.683 sec/batch; 57h:12m:45s remains)
INFO - root - 2017-12-07 16:40:11.796738: step 30840, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:11m:29s remains)
INFO - root - 2017-12-07 16:40:18.601147: step 30850, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 61h:21m:01s remains)
INFO - root - 2017-12-07 16:40:25.396206: step 30860, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 55h:50m:24s remains)
INFO - root - 2017-12-07 16:40:32.195023: step 30870, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 53h:26m:39s remains)
INFO - root - 2017-12-07 16:40:39.050926: step 30880, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 57h:35m:50s remains)
INFO - root - 2017-12-07 16:40:45.898196: step 30890, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 58h:44m:09s remains)
INFO - root - 2017-12-07 16:40:52.767296: step 30900, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 61h:33m:26s remains)
2017-12-07 16:40:53.448525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2365785 -4.2328653 -4.2440205 -4.2521806 -4.2535968 -4.2483854 -4.2401748 -4.2342587 -4.2378211 -4.2440572 -4.2574515 -4.27391 -4.2727156 -4.2554526 -4.2513666][-4.2414145 -4.2303505 -4.2336416 -4.2356586 -4.2333837 -4.22854 -4.2186213 -4.2080193 -4.2180758 -4.2404461 -4.265234 -4.2833467 -4.2753253 -4.2523003 -4.2477484][-4.2318459 -4.21955 -4.2205181 -4.2209167 -4.2161908 -4.2046828 -4.1857853 -4.1712284 -4.1921425 -4.2287674 -4.2605209 -4.2795753 -4.2686105 -4.2497888 -4.2498474][-4.2170596 -4.2029195 -4.2036638 -4.2052755 -4.1990609 -4.1794987 -4.1477656 -4.1271477 -4.1532936 -4.1990032 -4.23426 -4.2526703 -4.2424197 -4.2310305 -4.2422061][-4.2114029 -4.1990094 -4.2015524 -4.2039061 -4.19406 -4.1659946 -4.1176462 -4.0826707 -4.1039376 -4.156805 -4.1993446 -4.2185807 -4.2092514 -4.2034216 -4.2247925][-4.2371111 -4.2285972 -4.2317767 -4.2312021 -4.2131286 -4.1734781 -4.107172 -4.0530906 -4.0616722 -4.1173439 -4.1669407 -4.185781 -4.1713119 -4.1662335 -4.1945443][-4.2699466 -4.2650819 -4.2688146 -4.2630291 -4.2381244 -4.1921282 -4.1225023 -4.0632644 -4.062767 -4.1110811 -4.1541262 -4.1658173 -4.1413355 -4.1279011 -4.1598587][-4.3046637 -4.3015552 -4.3042378 -4.2939677 -4.2663865 -4.2203717 -4.1573143 -4.1027641 -4.0968518 -4.1317811 -4.1592469 -4.1633329 -4.1343751 -4.1099634 -4.1346073][-4.330884 -4.3269858 -4.3278503 -4.3157372 -4.2888584 -4.2460322 -4.1909094 -4.1390276 -4.1259565 -4.146719 -4.1633868 -4.1678867 -4.1461592 -4.1185589 -4.1287632][-4.3441448 -4.3397155 -4.3383288 -4.3261847 -4.3023815 -4.263339 -4.2129812 -4.1614466 -4.1393771 -4.1473274 -4.1589017 -4.1706672 -4.1608181 -4.1365705 -4.1343951][-4.35073 -4.3475313 -4.34519 -4.3348503 -4.3149323 -4.2798533 -4.2323895 -4.1840358 -4.1564064 -4.1548057 -4.1647253 -4.1832314 -4.1831584 -4.1647067 -4.1548815][-4.3524947 -4.3508725 -4.3489442 -4.3428745 -4.3275752 -4.3008347 -4.2616258 -4.2211351 -4.1957684 -4.191277 -4.1978297 -4.21109 -4.210566 -4.1948862 -4.1821671][-4.352994 -4.352016 -4.3500409 -4.3483171 -4.3403206 -4.3239126 -4.2976532 -4.2717943 -4.256032 -4.2523479 -4.2548871 -4.2560792 -4.2477446 -4.2301478 -4.2169628][-4.3537369 -4.3537459 -4.3523664 -4.3539462 -4.3515606 -4.3427758 -4.3284521 -4.3148527 -4.3093834 -4.3094015 -4.3093762 -4.3036904 -4.2905922 -4.2730789 -4.2613716][-4.3542433 -4.3557229 -4.35574 -4.3581066 -4.3589096 -4.3537035 -4.3459826 -4.3398767 -4.3390293 -4.341466 -4.342299 -4.3368669 -4.324996 -4.3115926 -4.3032794]]...]
INFO - root - 2017-12-07 16:41:00.262747: step 30910, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 56h:48m:36s remains)
INFO - root - 2017-12-07 16:41:07.033583: step 30920, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 61h:50m:35s remains)
INFO - root - 2017-12-07 16:41:13.641938: step 30930, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 56h:19m:09s remains)
INFO - root - 2017-12-07 16:41:20.377162: step 30940, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 52h:55m:58s remains)
INFO - root - 2017-12-07 16:41:27.221300: step 30950, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.697 sec/batch; 58h:20m:52s remains)
INFO - root - 2017-12-07 16:41:33.976769: step 30960, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 58h:57m:39s remains)
INFO - root - 2017-12-07 16:41:40.749181: step 30970, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 58h:46m:10s remains)
INFO - root - 2017-12-07 16:41:47.489622: step 30980, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 54h:54m:48s remains)
INFO - root - 2017-12-07 16:41:54.152421: step 30990, loss = 2.04, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 53h:02m:48s remains)
INFO - root - 2017-12-07 16:42:00.895428: step 31000, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 58h:43m:53s remains)
2017-12-07 16:42:01.590880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3300848 -4.3319244 -4.3325653 -4.3329697 -4.3315883 -4.330843 -4.3296156 -4.329133 -4.3287387 -4.3258572 -4.3220072 -4.3190975 -4.317142 -4.31665 -4.316493][-4.3306384 -4.330039 -4.3315225 -4.3315091 -4.3295636 -4.3290148 -4.327394 -4.3266978 -4.3243351 -4.3170033 -4.3091683 -4.3040175 -4.3016152 -4.3033848 -4.3059511][-4.3102531 -4.3106837 -4.3118224 -4.3064756 -4.3014431 -4.297276 -4.2903428 -4.2890878 -4.2889013 -4.282208 -4.2732024 -4.2677369 -4.2682719 -4.2767215 -4.2862072][-4.2729855 -4.2812476 -4.2839 -4.2681208 -4.2524581 -4.2336802 -4.2116737 -4.2072916 -4.21496 -4.2165761 -4.2116685 -4.2110605 -4.2204013 -4.24154 -4.2627125][-4.2252693 -4.2496433 -4.2567177 -4.2326322 -4.1966319 -4.146759 -4.0951848 -4.0856724 -4.1113911 -4.1302166 -4.1366668 -4.1496639 -4.174974 -4.2111635 -4.2437668][-4.1627684 -4.206409 -4.2219458 -4.1938787 -4.1333623 -4.0403538 -3.9480321 -3.9393768 -3.9988897 -4.0469427 -4.0734329 -4.1085982 -4.1539984 -4.2013135 -4.2375717][-4.110013 -4.1748695 -4.2004437 -4.1696911 -4.0840616 -3.9472373 -3.8188155 -3.8211174 -3.9212599 -4.0052509 -4.0598183 -4.1176562 -4.175601 -4.2221141 -4.2510333][-4.0971189 -4.1725559 -4.2054114 -4.1767507 -4.0839911 -3.9420056 -3.8221314 -3.8367167 -3.9408984 -4.0336957 -4.1004419 -4.166172 -4.2229428 -4.2589235 -4.2741494][-4.1301136 -4.1997714 -4.233398 -4.2122436 -4.1328731 -4.0208893 -3.9432311 -3.9638605 -4.0375853 -4.1057782 -4.160615 -4.2166839 -4.2622261 -4.2841573 -4.28795][-4.1814075 -4.2386 -4.268208 -4.2556992 -4.1974473 -4.1235518 -4.0830941 -4.0998664 -4.1375275 -4.1751881 -4.2120628 -4.252594 -4.2833576 -4.2946467 -4.2928348][-4.2283721 -4.273284 -4.2946725 -4.2849226 -4.2457857 -4.2032452 -4.18483 -4.1946306 -4.2089214 -4.2268782 -4.2501678 -4.2765646 -4.2943797 -4.2984033 -4.2951841][-4.2644858 -4.2942815 -4.3050575 -4.2965326 -4.2726722 -4.251 -4.2465448 -4.253243 -4.2576203 -4.2646327 -4.2778106 -4.2931366 -4.302062 -4.3020358 -4.2984462][-4.28803 -4.3040442 -4.3071027 -4.3013105 -4.2892151 -4.2804041 -4.2814293 -4.2851591 -4.2856669 -4.2875485 -4.2938104 -4.3019614 -4.3057356 -4.3038745 -4.3003535][-4.3016281 -4.3082366 -4.3063173 -4.3028288 -4.297832 -4.2949286 -4.2968307 -4.2982574 -4.2970691 -4.2960949 -4.2978182 -4.3015575 -4.3030457 -4.3010616 -4.2978039][-4.3054714 -4.3075786 -4.303268 -4.3003068 -4.298099 -4.297308 -4.2988739 -4.2993712 -4.2976847 -4.2956281 -4.29515 -4.2965536 -4.297133 -4.295598 -4.29328]]...]
INFO - root - 2017-12-07 16:42:08.400749: step 31010, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 54h:54m:32s remains)
INFO - root - 2017-12-07 16:42:14.951768: step 31020, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 54h:55m:18s remains)
INFO - root - 2017-12-07 16:42:21.816435: step 31030, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 57h:10m:56s remains)
INFO - root - 2017-12-07 16:42:28.695930: step 31040, loss = 2.03, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 60h:49m:30s remains)
INFO - root - 2017-12-07 16:42:35.501424: step 31050, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 56h:14m:10s remains)
INFO - root - 2017-12-07 16:42:42.333870: step 31060, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.648 sec/batch; 54h:13m:28s remains)
INFO - root - 2017-12-07 16:42:49.149804: step 31070, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 55h:06m:21s remains)
INFO - root - 2017-12-07 16:42:56.100595: step 31080, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 60h:39m:05s remains)
INFO - root - 2017-12-07 16:43:02.906890: step 31090, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 55h:54m:08s remains)
INFO - root - 2017-12-07 16:43:09.685557: step 31100, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 53h:01m:29s remains)
2017-12-07 16:43:10.395408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2169619 -4.1873074 -4.1705475 -4.1506104 -4.1302567 -4.1397858 -4.1745152 -4.2115045 -4.2437315 -4.26105 -4.2607145 -4.2387738 -4.2008758 -4.1591654 -4.1524854][-4.196866 -4.1627607 -4.1481423 -4.1380925 -4.1309304 -4.1534977 -4.1923246 -4.2262249 -4.2509756 -4.2703094 -4.2802114 -4.2644806 -4.2286878 -4.1883297 -4.1763172][-4.1954489 -4.1586795 -4.1473761 -4.14448 -4.1483822 -4.1790466 -4.217905 -4.2424221 -4.2583413 -4.27347 -4.2854457 -4.2764688 -4.2452283 -4.2076855 -4.1902151][-4.2123828 -4.1793385 -4.1723037 -4.1713281 -4.1796541 -4.2032971 -4.2278881 -4.2441168 -4.2599297 -4.2754426 -4.28905 -4.2865191 -4.2580643 -4.2201338 -4.1985593][-4.2233305 -4.1932216 -4.1859946 -4.178915 -4.1756916 -4.1768031 -4.1796694 -4.1933274 -4.2207937 -4.2512441 -4.2789078 -4.2883525 -4.2625308 -4.2239084 -4.1989021][-4.2161479 -4.1864023 -4.1762938 -4.162756 -4.1376257 -4.1065726 -4.0822759 -4.0885181 -4.1383066 -4.2027164 -4.2607551 -4.2870092 -4.26738 -4.2316666 -4.208859][-4.2015471 -4.1688313 -4.1518936 -4.1240196 -4.0739861 -4.0118589 -3.9611485 -3.9655159 -4.0458241 -4.1545014 -4.2426467 -4.2838106 -4.2755871 -4.2420053 -4.2192888][-4.185957 -4.150187 -4.1338873 -4.0958991 -4.0277905 -3.9394507 -3.8719559 -3.886888 -3.9966304 -4.1338935 -4.2361383 -4.2826385 -4.2808981 -4.2504163 -4.2272758][-4.1768942 -4.1391521 -4.130127 -4.1012936 -4.0379295 -3.9491329 -3.8855259 -3.9110346 -4.0242219 -4.1596193 -4.2536783 -4.2897391 -4.2847657 -4.2548323 -4.2300072][-4.1787968 -4.1454678 -4.1450162 -4.1319065 -4.0876093 -4.0245867 -3.9866748 -4.0154109 -4.1060753 -4.212513 -4.281085 -4.2993431 -4.2866492 -4.2542596 -4.2270274][-4.190763 -4.1648979 -4.1717134 -4.176096 -4.158772 -4.1316948 -4.1196723 -4.142169 -4.2040238 -4.2724695 -4.3097897 -4.3070297 -4.2845073 -4.2516327 -4.2241459][-4.1965761 -4.17299 -4.1824617 -4.2019286 -4.2109036 -4.2138219 -4.2211466 -4.2392163 -4.2780809 -4.3121662 -4.3209252 -4.2993174 -4.2719755 -4.2450237 -4.2259655][-4.2148566 -4.1853328 -4.1928606 -4.22315 -4.2489343 -4.2660007 -4.2844028 -4.3015542 -4.321682 -4.3295779 -4.318913 -4.2886434 -4.2588387 -4.2378764 -4.2192984][-4.2273946 -4.198761 -4.2096443 -4.2434869 -4.2736993 -4.2943158 -4.3119955 -4.3259807 -4.3353605 -4.3286891 -4.309474 -4.2804675 -4.2520981 -4.2275891 -4.200954][-4.2308841 -4.2150249 -4.2345009 -4.2673326 -4.2931428 -4.3096948 -4.3214493 -4.330255 -4.3329768 -4.3234706 -4.30428 -4.2813935 -4.2565012 -4.2271395 -4.1951704]]...]
INFO - root - 2017-12-07 16:43:17.158601: step 31110, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 57h:11m:58s remains)
INFO - root - 2017-12-07 16:43:23.929701: step 31120, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 59h:09m:31s remains)
INFO - root - 2017-12-07 16:43:30.615069: step 31130, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.612 sec/batch; 51h:15m:53s remains)
INFO - root - 2017-12-07 16:43:37.448844: step 31140, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 55h:43m:27s remains)
INFO - root - 2017-12-07 16:43:44.400844: step 31150, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 59h:45m:18s remains)
INFO - root - 2017-12-07 16:43:51.328934: step 31160, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 56h:36m:03s remains)
INFO - root - 2017-12-07 16:43:58.229211: step 31170, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 56h:41m:11s remains)
INFO - root - 2017-12-07 16:44:04.899698: step 31180, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 55h:16m:05s remains)
INFO - root - 2017-12-07 16:44:11.714663: step 31190, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 56h:31m:13s remains)
INFO - root - 2017-12-07 16:44:18.561615: step 31200, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 60h:41m:02s remains)
2017-12-07 16:44:19.316803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3133016 -4.3109379 -4.3106275 -4.3107076 -4.3102994 -4.3062348 -4.2996459 -4.2967825 -4.2983627 -4.2959876 -4.2767859 -4.2512746 -4.2340331 -4.2219162 -4.2148829][-4.2977557 -4.2925615 -4.2846274 -4.2751436 -4.2686472 -4.2579927 -4.2457705 -4.2429085 -4.253109 -4.2615752 -4.2489328 -4.2255311 -4.2087436 -4.1971722 -4.1880231][-4.2798181 -4.269434 -4.2539196 -4.2328696 -4.2164516 -4.1980982 -4.181808 -4.1836495 -4.2108688 -4.2388463 -4.2401037 -4.2226577 -4.206388 -4.19276 -4.1763597][-4.2668242 -4.2503109 -4.2287688 -4.1988473 -4.1699896 -4.1384063 -4.1155834 -4.1255684 -4.1766791 -4.2322879 -4.2523022 -4.2410874 -4.2214427 -4.2015009 -4.1723251][-4.2682581 -4.249959 -4.2217722 -4.1801238 -4.1324005 -4.0778794 -4.0413332 -4.0642548 -4.1476521 -4.2377896 -4.2793608 -4.2715497 -4.2460465 -4.2176495 -4.1764894][-4.2738786 -4.2593808 -4.2282739 -4.1691194 -4.0873413 -3.9905512 -3.9290817 -3.9727826 -4.0984807 -4.2295685 -4.2973747 -4.3009753 -4.2717676 -4.2361445 -4.1851463][-4.2771211 -4.2708273 -4.2397394 -4.160481 -4.030468 -3.8665276 -3.7591209 -3.8326149 -4.0186534 -4.1956196 -4.2912602 -4.3138642 -4.2896848 -4.24957 -4.1908908][-4.2771716 -4.2790337 -4.2482839 -4.1510577 -3.9735436 -3.7388718 -3.5791779 -3.688983 -3.9372163 -4.1517148 -4.270555 -4.3131762 -4.2990613 -4.2531042 -4.1869445][-4.2744112 -4.2884398 -4.2672563 -4.1715107 -3.9849348 -3.733536 -3.5517616 -3.6651831 -3.928668 -4.1458931 -4.2640381 -4.3088779 -4.2969303 -4.2431073 -4.1712475][-4.2655354 -4.2940521 -4.2904992 -4.2180762 -4.0683732 -3.8670082 -3.7196174 -3.7960243 -4.0061145 -4.1837807 -4.2746229 -4.3047619 -4.2866845 -4.2270889 -4.1560078][-4.2433157 -4.2801781 -4.291563 -4.2505403 -4.150516 -4.0150204 -3.9172783 -3.9609339 -4.1008472 -4.22683 -4.2842312 -4.2940049 -4.2670956 -4.2090087 -4.1472254][-4.2342143 -4.2694983 -4.2863383 -4.2682776 -4.2062011 -4.1211557 -4.0621881 -4.0902076 -4.1796031 -4.2614756 -4.2951293 -4.29351 -4.2637177 -4.2136178 -4.1671586][-4.2505207 -4.2766695 -4.2915373 -4.2858725 -4.2497959 -4.19705 -4.1616759 -4.1810107 -4.2405853 -4.2940969 -4.3156056 -4.3118086 -4.2836447 -4.2431388 -4.2119365][-4.2834563 -4.301774 -4.3128209 -4.311244 -4.2915711 -4.26003 -4.23815 -4.2502613 -4.2874279 -4.3211708 -4.3347464 -4.3311496 -4.30906 -4.2800159 -4.2613864][-4.3135381 -4.3272061 -4.3350921 -4.3361912 -4.327301 -4.3123674 -4.2998667 -4.3040113 -4.3246865 -4.34334 -4.3484111 -4.3436165 -4.32736 -4.3094764 -4.2989082]]...]
INFO - root - 2017-12-07 16:44:26.090734: step 31210, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 55h:21m:00s remains)
INFO - root - 2017-12-07 16:44:32.750492: step 31220, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 58h:57m:50s remains)
INFO - root - 2017-12-07 16:44:39.567294: step 31230, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 59h:24m:30s remains)
INFO - root - 2017-12-07 16:44:46.002239: step 31240, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 57h:18m:42s remains)
INFO - root - 2017-12-07 16:44:52.779176: step 31250, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 55h:05m:40s remains)
INFO - root - 2017-12-07 16:44:59.562909: step 31260, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.648 sec/batch; 54h:14m:03s remains)
INFO - root - 2017-12-07 16:45:06.426729: step 31270, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 58h:17m:22s remains)
INFO - root - 2017-12-07 16:45:13.365560: step 31280, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 60h:53m:18s remains)
INFO - root - 2017-12-07 16:45:20.158424: step 31290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 57h:39m:17s remains)
INFO - root - 2017-12-07 16:45:26.952055: step 31300, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 55h:31m:10s remains)
2017-12-07 16:45:27.677356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1328444 -4.1296978 -4.11472 -4.1109128 -4.1307111 -4.1528497 -4.1665344 -4.166049 -4.1569619 -4.1430984 -4.149889 -4.1672134 -4.1715155 -4.1573105 -4.1266723][-4.1070833 -4.118094 -4.1209416 -4.1284451 -4.1489792 -4.1627707 -4.1674485 -4.1636615 -4.1544738 -4.143321 -4.1518679 -4.1654716 -4.1668205 -4.1525121 -4.1280403][-4.103065 -4.1239619 -4.1419954 -4.1597824 -4.1811261 -4.1864853 -4.1780529 -4.1649761 -4.1541376 -4.1470375 -4.1601295 -4.1722555 -4.1732016 -4.1638412 -4.1508212][-4.1094761 -4.135118 -4.1613455 -4.1803536 -4.196466 -4.1943045 -4.1720667 -4.1496034 -4.1385159 -4.1413088 -4.1678615 -4.1918 -4.201931 -4.1989312 -4.1951289][-4.1361017 -4.1611996 -4.1866 -4.2010975 -4.2040749 -4.1885781 -4.1508131 -4.1187153 -4.1097407 -4.124373 -4.1657305 -4.206749 -4.2305503 -4.2343326 -4.23384][-4.1710992 -4.1924996 -4.2117987 -4.2180772 -4.2060275 -4.1697817 -4.1134176 -4.0739536 -4.0684872 -4.0964823 -4.1523643 -4.2086625 -4.2417564 -4.2455363 -4.2396507][-4.1930752 -4.2114811 -4.2263217 -4.225132 -4.1966348 -4.1416278 -4.07181 -4.0282779 -4.0266738 -4.0663652 -4.1354465 -4.2033958 -4.2408857 -4.2385383 -4.2200975][-4.2172933 -4.2282357 -4.23852 -4.22844 -4.18504 -4.1163216 -4.0429668 -4.0027528 -4.0070925 -4.0559435 -4.1293349 -4.2011971 -4.2415266 -4.2388773 -4.2175012][-4.2432451 -4.2455339 -4.2501807 -4.2365346 -4.1915488 -4.1222925 -4.0587358 -4.0336185 -4.0462618 -4.0898538 -4.1492114 -4.2082939 -4.2424169 -4.2375526 -4.2173243][-4.2520947 -4.2446432 -4.2460051 -4.2405963 -4.2103696 -4.1601443 -4.1166596 -4.1065807 -4.1207652 -4.1480875 -4.1840892 -4.2211018 -4.2422047 -4.2308483 -4.2130923][-4.2452488 -4.2287521 -4.2266769 -4.2290559 -4.2162051 -4.1924272 -4.1726451 -4.1695571 -4.1778436 -4.1912251 -4.2105365 -4.2288213 -4.2350717 -4.2184892 -4.204349][-4.2282538 -4.2082348 -4.206038 -4.2106824 -4.2076793 -4.2011442 -4.1947575 -4.1928024 -4.19547 -4.2011628 -4.213531 -4.220603 -4.215982 -4.1989231 -4.1865339][-4.201849 -4.1812987 -4.1779795 -4.1809855 -4.1795478 -4.1805844 -4.1816359 -4.179389 -4.1759982 -4.1761017 -4.1850863 -4.1872053 -4.1765981 -4.1584616 -4.1480451][-4.1818948 -4.1641436 -4.1626964 -4.1658721 -4.1663146 -4.170249 -4.1721773 -4.1671958 -4.160264 -4.1586809 -4.1644716 -4.1623468 -4.1492805 -4.132977 -4.1248174][-4.1978474 -4.1859932 -4.1868486 -4.19115 -4.192853 -4.1966195 -4.1945848 -4.1862235 -4.17944 -4.1790643 -4.182415 -4.1767445 -4.1619158 -4.148725 -4.1455956]]...]
INFO - root - 2017-12-07 16:45:34.413803: step 31310, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.675 sec/batch; 56h:26m:16s remains)
INFO - root - 2017-12-07 16:45:41.114584: step 31320, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 57h:02m:20s remains)
INFO - root - 2017-12-07 16:45:47.800925: step 31330, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 53h:35m:55s remains)
INFO - root - 2017-12-07 16:45:54.629976: step 31340, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.673 sec/batch; 56h:17m:51s remains)
INFO - root - 2017-12-07 16:46:01.454871: step 31350, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 58h:43m:04s remains)
INFO - root - 2017-12-07 16:46:08.260299: step 31360, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 57h:48m:15s remains)
INFO - root - 2017-12-07 16:46:15.023089: step 31370, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 56h:37m:54s remains)
INFO - root - 2017-12-07 16:46:21.881210: step 31380, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 55h:14m:31s remains)
INFO - root - 2017-12-07 16:46:28.706422: step 31390, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 57h:33m:07s remains)
INFO - root - 2017-12-07 16:46:35.484988: step 31400, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 57h:31m:43s remains)
2017-12-07 16:46:36.179173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3217311 -4.3149691 -4.3099895 -4.30396 -4.290421 -4.2716517 -4.2513604 -4.2372026 -4.2355475 -4.2403913 -4.2497663 -4.2689457 -4.289742 -4.2932162 -4.2734756][-4.3318863 -4.3277564 -4.3209395 -4.310308 -4.2913523 -4.267108 -4.241991 -4.2266459 -4.2277126 -4.2334638 -4.2442632 -4.2664509 -4.2867155 -4.2838812 -4.2602816][-4.3454218 -4.3425069 -4.3346009 -4.3190722 -4.2925549 -4.2663941 -4.2411928 -4.2275586 -4.2324557 -4.2385435 -4.2484651 -4.2690229 -4.28638 -4.2751732 -4.2458596][-4.3493075 -4.3424382 -4.3296847 -4.3075805 -4.2726817 -4.2434149 -4.2182732 -4.20557 -4.2116237 -4.2201667 -4.2317538 -4.254077 -4.2703128 -4.2530532 -4.219058][-4.3453689 -4.3298249 -4.3061771 -4.2769375 -4.2351623 -4.2017961 -4.1703243 -4.1533852 -4.1595473 -4.1737976 -4.1923184 -4.2207823 -4.2398381 -4.2214789 -4.1914525][-4.3390479 -4.31439 -4.2806969 -4.2433114 -4.1914973 -4.1475716 -4.1034894 -4.0743117 -4.0838017 -4.1159396 -4.1515455 -4.1919856 -4.2195358 -4.2081847 -4.1862926][-4.3290086 -4.2983155 -4.2595739 -4.2168636 -4.1550274 -4.0960722 -4.0394759 -4.0043902 -4.0197325 -4.0736971 -4.1313386 -4.1826448 -4.2120104 -4.2040315 -4.1849594][-4.3164215 -4.2833309 -4.2446423 -4.2010589 -4.1383371 -4.0793052 -4.0256467 -3.9960432 -4.0156083 -4.0758657 -4.1389527 -4.1815968 -4.2000723 -4.1860046 -4.1644616][-4.3076992 -4.2743649 -4.2400203 -4.2054381 -4.1588359 -4.1151123 -4.0744996 -4.0570779 -4.0779343 -4.1279187 -4.171946 -4.1906371 -4.1876888 -4.1587934 -4.1304035][-4.3002353 -4.2675691 -4.2399879 -4.2177076 -4.190093 -4.1623359 -4.130868 -4.1177306 -4.1337109 -4.1649384 -4.1846547 -4.1806612 -4.1611896 -4.1202393 -4.0887361][-4.2912283 -4.257144 -4.233191 -4.2179542 -4.1998887 -4.1820755 -4.1549544 -4.1393337 -4.1469936 -4.164413 -4.1679029 -4.1511 -4.1254907 -4.0827537 -4.0524235][-4.2860312 -4.2534189 -4.2332468 -4.2236581 -4.2130337 -4.2031269 -4.1815124 -4.1621232 -4.1607018 -4.1645122 -4.1550274 -4.13085 -4.1041832 -4.0676904 -4.0402932][-4.2876563 -4.2585382 -4.2434978 -4.2402029 -4.2350068 -4.2284026 -4.2105646 -4.191535 -4.18586 -4.1825633 -4.1657023 -4.1397495 -4.1176333 -4.09587 -4.0760202][-4.2938995 -4.2684116 -4.2570791 -4.2578998 -4.2542682 -4.246007 -4.229948 -4.2143884 -4.2078032 -4.2022796 -4.1869912 -4.166903 -4.1546125 -4.1480317 -4.1415424][-4.3060575 -4.2844329 -4.2730889 -4.2736659 -4.26786 -4.2548079 -4.2378993 -4.2282939 -4.2258596 -4.2230659 -4.2130532 -4.2027435 -4.1992617 -4.2027631 -4.2079492]]...]
INFO - root - 2017-12-07 16:46:42.897460: step 31410, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.632 sec/batch; 52h:53m:40s remains)
INFO - root - 2017-12-07 16:46:49.614003: step 31420, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 59h:14m:56s remains)
INFO - root - 2017-12-07 16:46:56.453754: step 31430, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.699 sec/batch; 58h:25m:44s remains)
INFO - root - 2017-12-07 16:47:03.411156: step 31440, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 58h:55m:54s remains)
INFO - root - 2017-12-07 16:47:10.245465: step 31450, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 54h:12m:05s remains)
INFO - root - 2017-12-07 16:47:17.022576: step 31460, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 56h:00m:32s remains)
INFO - root - 2017-12-07 16:47:23.836993: step 31470, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 59h:08m:21s remains)
INFO - root - 2017-12-07 16:47:30.754094: step 31480, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 58h:59m:29s remains)
INFO - root - 2017-12-07 16:47:37.630909: step 31490, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 60h:03m:06s remains)
INFO - root - 2017-12-07 16:47:44.529638: step 31500, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 56h:41m:52s remains)
2017-12-07 16:47:45.230891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2553124 -4.2701654 -4.2747283 -4.2598805 -4.2257218 -4.1835256 -4.1505919 -4.1407285 -4.1586475 -4.1967235 -4.2493849 -4.2864413 -4.3051014 -4.3140235 -4.3128133][-4.291719 -4.2972779 -4.2940826 -4.2675543 -4.2171035 -4.1572886 -4.1046076 -4.0816612 -4.1017041 -4.1586556 -4.2363906 -4.2925115 -4.3226819 -4.3363423 -4.3365979][-4.327239 -4.324935 -4.3137851 -4.2752833 -4.2097936 -4.1355324 -4.0623093 -4.0199471 -4.0360646 -4.1060624 -4.2033057 -4.2753444 -4.3182817 -4.3398647 -4.3435373][-4.3494515 -4.3398633 -4.3220429 -4.2754579 -4.2013593 -4.1204047 -4.0317345 -3.9693806 -3.9776592 -4.0513463 -4.1579103 -4.2409863 -4.2954397 -4.3244176 -4.3313704][-4.3504629 -4.3369045 -4.316546 -4.2680759 -4.1892686 -4.10182 -3.9980857 -3.9171839 -3.9188259 -3.9973738 -4.112874 -4.2060404 -4.2698226 -4.3053422 -4.3163137][-4.3417325 -4.3267884 -4.30792 -4.2627244 -4.1832147 -4.0885453 -3.9726443 -3.8764172 -3.8704176 -3.9547224 -4.0812078 -4.184958 -4.2555885 -4.2965136 -4.3099732][-4.3332734 -4.3169737 -4.2995677 -4.2590795 -4.1818056 -4.0843973 -3.9639909 -3.8650658 -3.8530815 -3.9370513 -4.066927 -4.1799607 -4.25631 -4.2994833 -4.3134761][-4.3245649 -4.3069973 -4.2869992 -4.2490311 -4.1809006 -4.0885005 -3.9729569 -3.8830285 -3.8713117 -3.9469774 -4.0674267 -4.1803141 -4.2607312 -4.3041129 -4.3178506][-4.3122864 -4.2950735 -4.2718849 -4.2348585 -4.1767011 -4.0940852 -3.9929025 -3.919661 -3.9154491 -3.9839072 -4.08896 -4.1923108 -4.269875 -4.3097978 -4.3222132][-4.3025084 -4.2880678 -4.2626028 -4.224144 -4.1724157 -4.1016946 -4.0208807 -3.9689822 -3.9755683 -4.038425 -4.1298447 -4.2196469 -4.2866321 -4.3196535 -4.3282609][-4.3047109 -4.2943163 -4.26899 -4.2322 -4.1870046 -4.1307392 -4.0731087 -4.0435848 -4.0581951 -4.113915 -4.190053 -4.263628 -4.3145838 -4.3356419 -4.338212][-4.3158441 -4.310267 -4.2905111 -4.2618775 -4.2291751 -4.1894045 -4.1502409 -4.1348658 -4.14986 -4.1919675 -4.2500849 -4.305346 -4.3406253 -4.3502097 -4.34757][-4.3281388 -4.32726 -4.315556 -4.2979226 -4.2802444 -4.257585 -4.2336788 -4.2255826 -4.2385297 -4.2665658 -4.3037424 -4.337564 -4.3582377 -4.3597965 -4.3541827][-4.337666 -4.3393483 -4.3346148 -4.3258643 -4.3189111 -4.3092012 -4.2980332 -4.2940631 -4.3030481 -4.3195338 -4.3399706 -4.3569589 -4.3665361 -4.3637619 -4.357502][-4.3432641 -4.344955 -4.3445597 -4.3423858 -4.342268 -4.3414297 -4.3396091 -4.3381939 -4.3416815 -4.3488135 -4.3574238 -4.3638964 -4.366044 -4.3624344 -4.3576922]]...]
INFO - root - 2017-12-07 16:47:51.973808: step 31510, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 58h:59m:45s remains)
INFO - root - 2017-12-07 16:47:58.677468: step 31520, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 59h:03m:00s remains)
INFO - root - 2017-12-07 16:48:05.619856: step 31530, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 53h:16m:26s remains)
INFO - root - 2017-12-07 16:48:12.404266: step 31540, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 57h:43m:00s remains)
INFO - root - 2017-12-07 16:48:19.058745: step 31550, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 57h:40m:03s remains)
INFO - root - 2017-12-07 16:48:25.726224: step 31560, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 57h:08m:27s remains)
INFO - root - 2017-12-07 16:48:32.464286: step 31570, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 54h:50m:01s remains)
INFO - root - 2017-12-07 16:48:39.265215: step 31580, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 58h:05m:56s remains)
INFO - root - 2017-12-07 16:48:46.065012: step 31590, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 56h:45m:41s remains)
INFO - root - 2017-12-07 16:48:52.886460: step 31600, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 57h:28m:21s remains)
2017-12-07 16:48:53.681679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1924057 -4.1666374 -4.1408563 -4.1194563 -4.1285233 -4.1682296 -4.20791 -4.2238398 -4.2202582 -4.2114644 -4.2062287 -4.1956449 -4.187211 -4.1776714 -4.1689444][-4.1880527 -4.1611547 -4.1354008 -4.1128335 -4.1198153 -4.1549392 -4.1940022 -4.2172985 -4.2177982 -4.2129321 -4.2075377 -4.1969104 -4.1862059 -4.1707406 -4.1532316][-4.1908917 -4.1648569 -4.1371312 -4.1114516 -4.10601 -4.1325622 -4.1727581 -4.2037387 -4.2135005 -4.2150288 -4.212594 -4.2044864 -4.1935153 -4.1715622 -4.1447906][-4.20115 -4.1683941 -4.1285629 -4.096283 -4.0775728 -4.0968146 -4.1414738 -4.1856523 -4.2104468 -4.2177448 -4.2167315 -4.2105289 -4.199852 -4.1764078 -4.1440177][-4.2132249 -4.1722646 -4.1182027 -4.0789332 -4.0530291 -4.0623312 -4.1057882 -4.1600547 -4.198472 -4.2168937 -4.220201 -4.2162333 -4.2100544 -4.1933317 -4.1621456][-4.2190742 -4.17269 -4.11097 -4.0650382 -4.0332561 -4.026907 -4.0608435 -4.1223555 -4.1731644 -4.1999097 -4.2099767 -4.2156596 -4.2219639 -4.217494 -4.1938558][-4.2109709 -4.1681576 -4.1133919 -4.06758 -4.0270581 -4.0019212 -4.0193291 -4.0778561 -4.1324492 -4.1650786 -4.1857619 -4.2097363 -4.2318187 -4.2409463 -4.2276535][-4.1919546 -4.1592717 -4.11936 -4.0820036 -4.0361185 -3.9893661 -3.9840486 -4.0307021 -4.0863056 -4.128314 -4.162353 -4.2017221 -4.234405 -4.2512546 -4.2500958][-4.1922259 -4.1763339 -4.1501184 -4.120379 -4.0745859 -4.0129528 -3.9803891 -4.008471 -4.0597463 -4.1117649 -4.1535549 -4.1956658 -4.2336831 -4.2542315 -4.26][-4.2133684 -4.2121296 -4.1983213 -4.1754532 -4.1373873 -4.0787477 -4.0331979 -4.0364218 -4.0708156 -4.121007 -4.1632175 -4.201211 -4.2371116 -4.257627 -4.2622681][-4.2419462 -4.248096 -4.24248 -4.2282147 -4.2046609 -4.1621361 -4.120234 -4.1052976 -4.118432 -4.1536841 -4.1885438 -4.2210088 -4.2533031 -4.2714205 -4.2718496][-4.2778511 -4.2844672 -4.2832518 -4.2760353 -4.2631884 -4.2350392 -4.2045135 -4.1828609 -4.1819696 -4.2007394 -4.223227 -4.2483916 -4.2744474 -4.2873392 -4.2897024][-4.3115058 -4.3158221 -4.3164682 -4.3126006 -4.3079987 -4.2905293 -4.26847 -4.2477961 -4.2365656 -4.24178 -4.2507329 -4.2667079 -4.2879338 -4.2952275 -4.3000259][-4.3261437 -4.3302717 -4.3296781 -4.3251338 -4.3235173 -4.3138552 -4.3011637 -4.2871327 -4.272162 -4.2644796 -4.2610917 -4.2721934 -4.2923489 -4.2984343 -4.3040423][-4.3301897 -4.3335156 -4.3297052 -4.3216615 -4.3198638 -4.3161283 -4.310329 -4.3019905 -4.2891083 -4.2751436 -4.2627506 -4.2714653 -4.2926307 -4.3004036 -4.3067985]]...]
INFO - root - 2017-12-07 16:49:00.522043: step 31610, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 58h:41m:20s remains)
INFO - root - 2017-12-07 16:49:07.137741: step 31620, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 60h:48m:41s remains)
INFO - root - 2017-12-07 16:49:13.901616: step 31630, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 57h:03m:09s remains)
INFO - root - 2017-12-07 16:49:20.613221: step 31640, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 53h:02m:38s remains)
INFO - root - 2017-12-07 16:49:27.384583: step 31650, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.658 sec/batch; 55h:00m:02s remains)
INFO - root - 2017-12-07 16:49:34.273959: step 31660, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 57h:57m:28s remains)
INFO - root - 2017-12-07 16:49:41.104413: step 31670, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 57h:37m:31s remains)
INFO - root - 2017-12-07 16:49:47.913648: step 31680, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 54h:52m:35s remains)
INFO - root - 2017-12-07 16:49:54.765160: step 31690, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.669 sec/batch; 55h:53m:14s remains)
INFO - root - 2017-12-07 16:50:01.578243: step 31700, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.700 sec/batch; 58h:31m:20s remains)
2017-12-07 16:50:02.314416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2595115 -4.2475667 -4.2271371 -4.1991005 -4.1643634 -4.117784 -4.0922608 -4.1173844 -4.1449771 -4.1666694 -4.1907 -4.2361827 -4.271492 -4.2845011 -4.279521][-4.2722306 -4.2661605 -4.2409964 -4.200263 -4.1482658 -4.0789356 -4.026196 -4.0520663 -4.1003304 -4.1488142 -4.1913834 -4.237195 -4.2735691 -4.2847443 -4.2776742][-4.2709846 -4.2733879 -4.2506104 -4.2044024 -4.1421933 -4.05852 -3.9877822 -4.0074329 -4.0654254 -4.1333842 -4.1940756 -4.2396865 -4.2745171 -4.2838354 -4.2724276][-4.2566171 -4.2729845 -4.2612748 -4.2201214 -4.1610093 -4.0805435 -4.0088611 -4.0187697 -4.070478 -4.1396937 -4.2033567 -4.2433743 -4.2749605 -4.2807832 -4.265614][-4.2322183 -4.2596035 -4.2621546 -4.2290483 -4.1777844 -4.1143489 -4.0609584 -4.0708251 -4.1144938 -4.1725583 -4.2248869 -4.2554741 -4.2800951 -4.2818789 -4.2659435][-4.2117658 -4.2409825 -4.2506361 -4.2284312 -4.1882954 -4.1397004 -4.1084337 -4.1244845 -4.1624084 -4.2103662 -4.249516 -4.268858 -4.2851391 -4.2866888 -4.2750411][-4.2030358 -4.2274632 -4.2371945 -4.2253819 -4.1946082 -4.1583672 -4.1426959 -4.1618419 -4.197659 -4.2350526 -4.2625613 -4.2744908 -4.2824745 -4.2869687 -4.2826557][-4.1968751 -4.2178121 -4.230042 -4.2243953 -4.1975179 -4.1673985 -4.1564908 -4.1778355 -4.21496 -4.2441583 -4.2628217 -4.2748141 -4.2779531 -4.2856421 -4.2887659][-4.1897211 -4.2104874 -4.2260962 -4.2244425 -4.1977367 -4.1666527 -4.1506786 -4.170774 -4.2133093 -4.2422633 -4.2622948 -4.2778654 -4.2786932 -4.2844443 -4.2897415][-4.1896744 -4.2145548 -4.2332368 -4.2335215 -4.2078018 -4.1717939 -4.1436486 -4.1571507 -4.2045636 -4.2379441 -4.2630544 -4.2824192 -4.2831173 -4.2854156 -4.2876229][-4.1971707 -4.2282057 -4.2495584 -4.2494183 -4.2256188 -4.1863127 -4.1514311 -4.1606441 -4.2092133 -4.2456436 -4.2726369 -4.2931738 -4.295558 -4.2923512 -4.2870407][-4.2168345 -4.2477226 -4.2672796 -4.2689452 -4.2480993 -4.2113781 -4.1756721 -4.1777458 -4.2198529 -4.2559361 -4.2819276 -4.3029118 -4.3070564 -4.2990179 -4.2880721][-4.2373261 -4.2632375 -4.2810955 -4.284791 -4.2690382 -4.2385826 -4.208065 -4.2042751 -4.23233 -4.262795 -4.2838182 -4.3023753 -4.3055964 -4.2963319 -4.2847786][-4.2531538 -4.2711143 -4.2873073 -4.29415 -4.2832956 -4.260077 -4.2374749 -4.2323604 -4.2492924 -4.2722063 -4.2856522 -4.2976217 -4.2974014 -4.2873421 -4.2778769][-4.2728162 -4.2837415 -4.2953043 -4.3022604 -4.2970409 -4.282475 -4.2680421 -4.2646661 -4.2741213 -4.2890873 -4.2955241 -4.3000612 -4.2976027 -4.28828 -4.28213]]...]
INFO - root - 2017-12-07 16:50:09.074411: step 31710, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 56h:43m:17s remains)
INFO - root - 2017-12-07 16:50:15.647726: step 31720, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 53h:43m:33s remains)
INFO - root - 2017-12-07 16:50:22.403671: step 31730, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 57h:41m:40s remains)
INFO - root - 2017-12-07 16:50:29.312591: step 31740, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 58h:23m:09s remains)
INFO - root - 2017-12-07 16:50:35.995882: step 31750, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 57h:09m:16s remains)
INFO - root - 2017-12-07 16:50:42.767530: step 31760, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 53h:19m:45s remains)
INFO - root - 2017-12-07 16:50:49.492106: step 31770, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 55h:38m:06s remains)
INFO - root - 2017-12-07 16:50:56.331901: step 31780, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.754 sec/batch; 63h:01m:15s remains)
INFO - root - 2017-12-07 16:51:03.124666: step 31790, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.710 sec/batch; 59h:18m:34s remains)
INFO - root - 2017-12-07 16:51:09.858755: step 31800, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 57h:22m:10s remains)
2017-12-07 16:51:10.548089: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.320323 -4.3107038 -4.2928138 -4.2635632 -4.2297769 -4.2087855 -4.203649 -4.2126808 -4.229167 -4.2524204 -4.2583857 -4.2574396 -4.244175 -4.2078667 -4.1758866][-4.3085351 -4.3024836 -4.2877297 -4.255065 -4.2163677 -4.188869 -4.1724362 -4.1761465 -4.2036028 -4.2445412 -4.2647867 -4.2645264 -4.2407312 -4.1904407 -4.1484542][-4.2903347 -4.293117 -4.2880573 -4.2564273 -4.2135954 -4.1759896 -4.138185 -4.1307316 -4.1731839 -4.22825 -4.2578945 -4.2569418 -4.2248135 -4.1654992 -4.1157389][-4.2701449 -4.2757096 -4.2754025 -4.2478189 -4.2063017 -4.160531 -4.1048865 -4.0842319 -4.1386251 -4.2065783 -4.2432952 -4.2490754 -4.2217035 -4.1672878 -4.118824][-4.2537827 -4.2580962 -4.2598147 -4.235301 -4.1951709 -4.1431441 -4.0752683 -4.0412111 -4.0990477 -4.1785049 -4.2277489 -4.2494736 -4.2378 -4.1975141 -4.1544933][-4.2477236 -4.2521238 -4.2525058 -4.2278552 -4.1840882 -4.1138763 -4.0188231 -3.9657469 -4.0318809 -4.1306257 -4.1998086 -4.2417841 -4.2467804 -4.2234206 -4.1904006][-4.2398367 -4.2468953 -4.2474709 -4.2214117 -4.1703534 -4.0771332 -3.9467351 -3.871336 -3.9400976 -4.0524549 -4.1449494 -4.2127895 -4.2362685 -4.2254443 -4.2009072][-4.2179313 -4.2314596 -4.2378187 -4.2155643 -4.1646814 -4.0677423 -3.9322555 -3.8599024 -3.9191918 -4.0137281 -4.1029654 -4.18014 -4.2142429 -4.2112737 -4.1931458][-4.2046013 -4.2272921 -4.24209 -4.2279954 -4.1908603 -4.1119914 -3.9990065 -3.9452746 -3.9866447 -4.045682 -4.107347 -4.1755137 -4.2091246 -4.2102113 -4.1958175][-4.1975045 -4.2296243 -4.2521653 -4.2497215 -4.2299118 -4.1770496 -4.0888634 -4.0496254 -4.0736666 -4.0966687 -4.1242161 -4.173749 -4.2085814 -4.2163296 -4.2094111][-4.197391 -4.2311835 -4.2560873 -4.2591643 -4.2494 -4.2124205 -4.1441813 -4.1149693 -4.1266961 -4.1299505 -4.1371822 -4.1736655 -4.2087007 -4.2255926 -4.2266726][-4.2174811 -4.2375331 -4.2571988 -4.2604218 -4.2552671 -4.2313194 -4.1864953 -4.1710587 -4.1822171 -4.1803885 -4.1804552 -4.2037873 -4.2321982 -4.250031 -4.2545862][-4.2535176 -4.2561665 -4.2646856 -4.2607722 -4.2556019 -4.2420254 -4.2191043 -4.2202287 -4.2348 -4.2350516 -4.2321138 -4.2449455 -4.2633252 -4.2770929 -4.2814155][-4.2865181 -4.2794757 -4.2771478 -4.2663574 -4.2583704 -4.2490206 -4.2373815 -4.2443714 -4.259613 -4.2636762 -4.2642226 -4.2711611 -4.2801781 -4.2877388 -4.2886543][-4.3117709 -4.3049722 -4.299396 -4.2843857 -4.2753487 -4.2710333 -4.2671819 -4.2755194 -4.2901282 -4.2951989 -4.2934756 -4.2912621 -4.28896 -4.2876129 -4.2835937]]...]
INFO - root - 2017-12-07 16:51:17.510963: step 31810, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 59h:51m:32s remains)
INFO - root - 2017-12-07 16:51:24.076199: step 31820, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 58h:20m:06s remains)
INFO - root - 2017-12-07 16:51:30.853123: step 31830, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 56h:59m:03s remains)
INFO - root - 2017-12-07 16:51:37.611719: step 31840, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 52h:38m:41s remains)
INFO - root - 2017-12-07 16:51:44.392943: step 31850, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 54h:23m:22s remains)
INFO - root - 2017-12-07 16:51:50.991282: step 31860, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 55h:40m:36s remains)
INFO - root - 2017-12-07 16:51:57.661450: step 31870, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.676 sec/batch; 56h:28m:01s remains)
INFO - root - 2017-12-07 16:52:04.372695: step 31880, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 56h:14m:01s remains)
INFO - root - 2017-12-07 16:52:11.075852: step 31890, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 55h:10m:00s remains)
INFO - root - 2017-12-07 16:52:17.838151: step 31900, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 56h:16m:52s remains)
2017-12-07 16:52:18.672443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2975621 -4.2927046 -4.2778268 -4.250123 -4.2260332 -4.2235537 -4.23324 -4.2550364 -4.2682981 -4.2721338 -4.2706137 -4.2570114 -4.2388859 -4.2321634 -4.2345071][-4.3154664 -4.3091164 -4.2909303 -4.2611475 -4.2324519 -4.2234817 -4.2288752 -4.2478075 -4.2559166 -4.2532306 -4.2475181 -4.2329993 -4.2152085 -4.2119489 -4.2234082][-4.3278666 -4.3255405 -4.3097534 -4.2818189 -4.2527556 -4.236546 -4.2325392 -4.242382 -4.2395082 -4.2281814 -4.2206016 -4.2078247 -4.1911435 -4.1920886 -4.2120886][-4.3281727 -4.3308115 -4.3208194 -4.2956228 -4.2656994 -4.2409935 -4.2243776 -4.21929 -4.2028012 -4.1882043 -4.1904745 -4.1897416 -4.1812553 -4.1878195 -4.2120781][-4.3188128 -4.3244715 -4.3164649 -4.2910676 -4.259151 -4.2286768 -4.2048855 -4.18879 -4.160903 -4.1482372 -4.1694703 -4.1905522 -4.1970878 -4.209229 -4.230051][-4.2995019 -4.3028731 -4.2931495 -4.267323 -4.2372327 -4.2057223 -4.1834378 -4.1692433 -4.14307 -4.1405225 -4.1765389 -4.2090182 -4.2264376 -4.240056 -4.2487621][-4.2819805 -4.2780304 -4.2653265 -4.2411995 -4.2155542 -4.1863632 -4.1721082 -4.1703658 -4.1573033 -4.1671195 -4.2085061 -4.2427211 -4.2622652 -4.273561 -4.2730327][-4.2911191 -4.2808118 -4.2638488 -4.2391529 -4.2152886 -4.1939015 -4.1894073 -4.1981263 -4.1962447 -4.211266 -4.2482071 -4.2766304 -4.2916436 -4.298974 -4.2947311][-4.3122768 -4.2988825 -4.2792172 -4.2579989 -4.2395349 -4.2268543 -4.2298393 -4.2415962 -4.2432313 -4.2575221 -4.285109 -4.30432 -4.3114662 -4.3127208 -4.3074903][-4.3173933 -4.30356 -4.2874279 -4.2784023 -4.2725377 -4.2711062 -4.2783494 -4.2866259 -4.286705 -4.2933917 -4.3092232 -4.316937 -4.3175526 -4.3157816 -4.310895][-4.2945561 -4.2813187 -4.2742219 -4.2799811 -4.2856941 -4.291223 -4.2962542 -4.2977037 -4.2940445 -4.2938733 -4.2989659 -4.2965589 -4.2936039 -4.2919817 -4.2920294][-4.2507114 -4.2388067 -4.2440839 -4.2606573 -4.2705216 -4.2746172 -4.2719688 -4.2648435 -4.2589641 -4.2561879 -4.2585611 -4.2531514 -4.2506242 -4.2516232 -4.2574625][-4.2110424 -4.19942 -4.2107925 -4.2320533 -4.2412891 -4.2402854 -4.2267342 -4.2126489 -4.2102432 -4.2122107 -4.2192826 -4.216464 -4.2178817 -4.223918 -4.2338519][-4.1929426 -4.1817384 -4.19493 -4.2154794 -4.2200007 -4.21421 -4.1955523 -4.18111 -4.187398 -4.1983223 -4.2095208 -4.2063503 -4.2077527 -4.2154536 -4.2253032][-4.189817 -4.1811819 -4.1967783 -4.2188687 -4.2226753 -4.2159753 -4.2008042 -4.1950803 -4.2075529 -4.2215896 -4.2302365 -4.2222385 -4.2200508 -4.2253914 -4.2324724]]...]
INFO - root - 2017-12-07 16:52:25.432977: step 31910, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.637 sec/batch; 53h:13m:37s remains)
INFO - root - 2017-12-07 16:52:32.065284: step 31920, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 54h:15m:18s remains)
INFO - root - 2017-12-07 16:52:38.936200: step 31930, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 59h:00m:56s remains)
INFO - root - 2017-12-07 16:52:45.829977: step 31940, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 57h:29m:23s remains)
INFO - root - 2017-12-07 16:52:52.819856: step 31950, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 59h:49m:32s remains)
INFO - root - 2017-12-07 16:52:59.578776: step 31960, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 53h:37m:12s remains)
INFO - root - 2017-12-07 16:53:06.354094: step 31970, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 57h:35m:33s remains)
INFO - root - 2017-12-07 16:53:13.204159: step 31980, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 57h:13m:43s remains)
INFO - root - 2017-12-07 16:53:20.051698: step 31990, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 58h:08m:57s remains)
INFO - root - 2017-12-07 16:53:26.765022: step 32000, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 53h:41m:45s remains)
2017-12-07 16:53:27.517157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.322505 -4.3197837 -4.3160791 -4.3041234 -4.2852364 -4.2668481 -4.2505484 -4.2352138 -4.2348452 -4.2400761 -4.2388182 -4.2306504 -4.2339239 -4.2434497 -4.2534838][-4.3124609 -4.308495 -4.3040214 -4.2863469 -4.25882 -4.2341542 -4.2170768 -4.2041893 -4.2075286 -4.2116714 -4.2066627 -4.1955733 -4.2011423 -4.2155194 -4.2281318][-4.301239 -4.2958426 -4.2900815 -4.2691989 -4.238163 -4.211925 -4.1968937 -4.19232 -4.1990943 -4.1946321 -4.17943 -4.1625938 -4.1686258 -4.1851625 -4.2012944][-4.2915907 -4.2811842 -4.2667208 -4.2386789 -4.2077131 -4.1829934 -4.1723781 -4.1774182 -4.188921 -4.1775384 -4.1550918 -4.1406927 -4.149724 -4.1703987 -4.1911464][-4.2878542 -4.270781 -4.2429314 -4.2062864 -4.171207 -4.14596 -4.1398616 -4.1543994 -4.1676207 -4.1512985 -4.1288171 -4.12295 -4.137526 -4.1639261 -4.1910505][-4.2926049 -4.2731638 -4.238306 -4.1957989 -4.1530747 -4.124836 -4.1175604 -4.1329565 -4.1455526 -4.1284156 -4.1089668 -4.1105433 -4.1312046 -4.1617627 -4.1928673][-4.3035173 -4.2858028 -4.2536154 -4.2147946 -4.1742029 -4.1437306 -4.1254387 -4.1253767 -4.1345773 -4.1226897 -4.10766 -4.1133423 -4.138701 -4.1697254 -4.197504][-4.3083143 -4.2912259 -4.266902 -4.2380772 -4.2077332 -4.17928 -4.1490159 -4.1292777 -4.1279125 -4.1237364 -4.1193309 -4.1274638 -4.1533251 -4.182188 -4.2041097][-4.3044086 -4.2867312 -4.2677388 -4.246695 -4.2229738 -4.1913972 -4.1504974 -4.1180468 -4.113482 -4.1194715 -4.125824 -4.1374435 -4.1622863 -4.1917677 -4.211246][-4.2995095 -4.282228 -4.2663007 -4.2477241 -4.2228055 -4.1824389 -4.1269765 -4.084569 -4.0846176 -4.1057615 -4.12255 -4.136714 -4.1605587 -4.1915693 -4.2122][-4.2968221 -4.27881 -4.2593536 -4.2367373 -4.2091293 -4.1682973 -4.1091566 -4.0669661 -4.0783772 -4.113718 -4.1370068 -4.1459546 -4.158833 -4.1828017 -4.2040906][-4.2938542 -4.2723289 -4.24485 -4.2136092 -4.1861396 -4.1558261 -4.1113009 -4.0898604 -4.1126094 -4.14966 -4.1704016 -4.1729994 -4.1701455 -4.17673 -4.190248][-4.2912865 -4.2691407 -4.23562 -4.2022943 -4.1814866 -4.1645451 -4.13762 -4.1303034 -4.1557975 -4.1853442 -4.200366 -4.197464 -4.18525 -4.178112 -4.1815987][-4.2920613 -4.2703562 -4.2322459 -4.1996737 -4.1830215 -4.1702175 -4.1516924 -4.1499724 -4.1718783 -4.1953135 -4.2088161 -4.2039242 -4.192194 -4.1833811 -4.1853886][-4.2937689 -4.271759 -4.231564 -4.19738 -4.1766229 -4.1633182 -4.1501122 -4.1497545 -4.1652317 -4.1815062 -4.1967826 -4.1987262 -4.1972361 -4.1955419 -4.1980639]]...]
INFO - root - 2017-12-07 16:53:34.217396: step 32010, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 57h:15m:35s remains)
INFO - root - 2017-12-07 16:53:40.890097: step 32020, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 57h:12m:50s remains)
INFO - root - 2017-12-07 16:53:47.693990: step 32030, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:56m:51s remains)
INFO - root - 2017-12-07 16:53:54.513448: step 32040, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 55h:39m:27s remains)
INFO - root - 2017-12-07 16:54:01.436570: step 32050, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 60h:28m:22s remains)
INFO - root - 2017-12-07 16:54:08.247803: step 32060, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 55h:54m:01s remains)
INFO - root - 2017-12-07 16:54:14.999728: step 32070, loss = 2.09, batch loss = 2.04 (12.1 examples/sec; 0.661 sec/batch; 55h:11m:14s remains)
INFO - root - 2017-12-07 16:54:21.774647: step 32080, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 53h:20m:05s remains)
INFO - root - 2017-12-07 16:54:28.485748: step 32090, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 54h:50m:46s remains)
INFO - root - 2017-12-07 16:54:35.286530: step 32100, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 60h:39m:02s remains)
2017-12-07 16:54:36.001627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2805986 -4.2700739 -4.2546773 -4.2463741 -4.2475133 -4.2503071 -4.2506814 -4.2539015 -4.262104 -4.2742257 -4.290863 -4.3052306 -4.3131909 -4.3177695 -4.3165631][-4.2817845 -4.2635722 -4.2417068 -4.2310896 -4.2328868 -4.2367125 -4.2387514 -4.2486649 -4.2616391 -4.278451 -4.2984943 -4.3114586 -4.31459 -4.3139076 -4.3102317][-4.2733064 -4.2529984 -4.228559 -4.2122602 -4.2082572 -4.2105403 -4.2171688 -4.2361989 -4.2557635 -4.2790384 -4.3004379 -4.309906 -4.3063111 -4.2978549 -4.2919073][-4.2564921 -4.2396722 -4.2137012 -4.1899309 -4.1778688 -4.17698 -4.1899042 -4.2165432 -4.2395115 -4.2654452 -4.2861133 -4.2911758 -4.2822094 -4.268043 -4.2612796][-4.2269 -4.2195344 -4.1953917 -4.166441 -4.1473379 -4.1453147 -4.1624894 -4.1916647 -4.2143388 -4.23832 -4.2556891 -4.2571015 -4.2467241 -4.2316809 -4.2239332][-4.1885505 -4.1926508 -4.1762762 -4.1481647 -4.1250648 -4.1233339 -4.1430979 -4.1700997 -4.190166 -4.2095861 -4.2226992 -4.2237487 -4.2145147 -4.1994781 -4.1887326][-4.1476293 -4.1585879 -4.1530271 -4.1326113 -4.11251 -4.110312 -4.127789 -4.1503954 -4.168469 -4.1876945 -4.2010288 -4.20272 -4.1944633 -4.1796913 -4.1652565][-4.1216946 -4.1318746 -4.1343937 -4.1240268 -4.1086178 -4.0994482 -4.1052976 -4.1206765 -4.1373005 -4.1611919 -4.1867261 -4.1996775 -4.1960187 -4.1831274 -4.167285][-4.1254411 -4.12998 -4.1351624 -4.1320677 -4.1175261 -4.0944705 -4.0789304 -4.0784268 -4.0874743 -4.114481 -4.1560378 -4.1884232 -4.198802 -4.1926746 -4.1820931][-4.1537418 -4.1556668 -4.1618319 -4.1603484 -4.1446161 -4.1133003 -4.0787449 -4.0572982 -4.0516996 -4.0734639 -4.1175632 -4.1577992 -4.1772695 -4.1781178 -4.1780481][-4.1832814 -4.1883349 -4.19504 -4.1952844 -4.1816578 -4.1523333 -4.11441 -4.082974 -4.0637064 -4.0675187 -4.0914512 -4.11757 -4.1330633 -4.1376915 -4.1484194][-4.1925163 -4.2074833 -4.2195692 -4.2245183 -4.2152953 -4.1921268 -4.1638126 -4.1372361 -4.1145706 -4.1019244 -4.097147 -4.0957632 -4.0975676 -4.1031094 -4.1188164][-4.179656 -4.2013717 -4.2217617 -4.23343 -4.2312288 -4.2177925 -4.202178 -4.1873317 -4.1724181 -4.1579657 -4.1396093 -4.1179037 -4.1047344 -4.1042614 -4.114471][-4.1585345 -4.1825666 -4.2061853 -4.2200861 -4.2212257 -4.2159891 -4.2128973 -4.2126937 -4.2118082 -4.2058043 -4.1890459 -4.1627874 -4.1436071 -4.1376724 -4.1377058][-4.1464334 -4.1689529 -4.188766 -4.19818 -4.1960664 -4.1900024 -4.1931705 -4.2081137 -4.223629 -4.22889 -4.2177773 -4.1964316 -4.178906 -4.1680808 -4.1605916]]...]
INFO - root - 2017-12-07 16:54:42.604486: step 32110, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 53h:34m:28s remains)
INFO - root - 2017-12-07 16:54:49.324346: step 32120, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 54h:04m:39s remains)
INFO - root - 2017-12-07 16:54:56.076849: step 32130, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 60h:04m:28s remains)
INFO - root - 2017-12-07 16:55:02.846725: step 32140, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 58h:19m:21s remains)
INFO - root - 2017-12-07 16:55:09.728720: step 32150, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 57h:44m:16s remains)
INFO - root - 2017-12-07 16:55:16.506838: step 32160, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 55h:22m:16s remains)
INFO - root - 2017-12-07 16:55:23.311941: step 32170, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 56h:18m:52s remains)
INFO - root - 2017-12-07 16:55:30.112318: step 32180, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.683 sec/batch; 56h:57m:20s remains)
INFO - root - 2017-12-07 16:55:36.910445: step 32190, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.716 sec/batch; 59h:44m:34s remains)
INFO - root - 2017-12-07 16:55:43.835557: step 32200, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 54h:21m:00s remains)
2017-12-07 16:55:44.589980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3097596 -4.3119273 -4.312253 -4.3105812 -4.3081012 -4.3052964 -4.3022742 -4.2996807 -4.2985168 -4.3002334 -4.3042336 -4.3092318 -4.3147521 -4.3199716 -4.3236232][-4.3105154 -4.3132524 -4.3134761 -4.3109741 -4.3073177 -4.3034821 -4.2998509 -4.2974072 -4.2976055 -4.3017035 -4.3088121 -4.3162432 -4.3224254 -4.3264828 -4.3281956][-4.3113632 -4.3132629 -4.3109703 -4.3037548 -4.2941804 -4.2851052 -4.2788262 -4.2772956 -4.2826867 -4.293982 -4.30843 -4.3211956 -4.3296695 -4.3322544 -4.3304896][-4.3107095 -4.3098793 -4.3014197 -4.2834187 -4.2597489 -4.2376661 -4.2240911 -4.2224274 -4.235682 -4.2597966 -4.2881594 -4.3122973 -4.3281994 -4.3331914 -4.3297272][-4.3054996 -4.299356 -4.2813325 -4.2466564 -4.1990113 -4.1527057 -4.1230168 -4.1184673 -4.1445718 -4.1891 -4.2392368 -4.2818556 -4.3113437 -4.3243351 -4.32285][-4.2950716 -4.283608 -4.2570219 -4.205586 -4.1320381 -4.055594 -4.0016737 -3.9896512 -4.0300956 -4.0991826 -4.1744814 -4.2380915 -4.2824664 -4.3049779 -4.3078041][-4.28649 -4.272511 -4.2435093 -4.1858754 -4.0985236 -4.0001822 -3.9239416 -3.9032202 -3.9538748 -4.040689 -4.1321373 -4.2084 -4.2597823 -4.28535 -4.2897496][-4.2868681 -4.2750812 -4.25115 -4.2031484 -4.1265392 -4.0331979 -3.9556253 -3.9326591 -3.9792571 -4.0599489 -4.1426992 -4.2115564 -4.2564692 -4.2763467 -4.2783804][-4.2939081 -4.2870679 -4.2731328 -4.2436295 -4.1934633 -4.1261187 -4.0655 -4.0440264 -4.0733161 -4.1290751 -4.1865559 -4.2338958 -4.2627544 -4.2724929 -4.270071][-4.3035951 -4.3018746 -4.2969203 -4.2837811 -4.2591381 -4.2217579 -4.1839886 -4.1679697 -4.1809945 -4.2099853 -4.2400641 -4.2621484 -4.2712765 -4.2691197 -4.2612228][-4.3116403 -4.312211 -4.3098955 -4.3029003 -4.2914906 -4.2757459 -4.2601519 -4.2544651 -4.2614722 -4.2749958 -4.2875891 -4.2914405 -4.284862 -4.2711544 -4.25653][-4.3148427 -4.3131733 -4.3053851 -4.2907658 -4.2758708 -4.267416 -4.268106 -4.2766681 -4.2901483 -4.301446 -4.3084683 -4.3061991 -4.2943659 -4.2768192 -4.2592092][-4.3120275 -4.3070049 -4.2870011 -4.2528934 -4.2193217 -4.2048006 -4.2151413 -4.23883 -4.2678475 -4.2898364 -4.3038898 -4.3067126 -4.2995458 -4.2870932 -4.2729912][-4.2950225 -4.2911711 -4.2622871 -4.2090826 -4.1527748 -4.1266522 -4.1434069 -4.18287 -4.2290707 -4.2659125 -4.2917728 -4.3043828 -4.3061118 -4.3018017 -4.2945337][-4.2669659 -4.2742085 -4.2492962 -4.1918039 -4.1266012 -4.0960016 -4.1170726 -4.1646404 -4.2179475 -4.2593541 -4.2888942 -4.3061223 -4.3135691 -4.3152394 -4.3130631]]...]
INFO - root - 2017-12-07 16:55:51.371955: step 32210, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 56h:40m:57s remains)
INFO - root - 2017-12-07 16:55:58.002205: step 32220, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 52h:52m:08s remains)
INFO - root - 2017-12-07 16:56:04.737105: step 32230, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 53h:37m:32s remains)
INFO - root - 2017-12-07 16:56:11.532612: step 32240, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 55h:31m:30s remains)
INFO - root - 2017-12-07 16:56:18.372962: step 32250, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 60h:50m:33s remains)
INFO - root - 2017-12-07 16:56:25.238688: step 32260, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 58h:59m:23s remains)
INFO - root - 2017-12-07 16:56:32.023188: step 32270, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 55h:50m:45s remains)
INFO - root - 2017-12-07 16:56:38.739456: step 32280, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 55h:32m:23s remains)
INFO - root - 2017-12-07 16:56:45.559011: step 32290, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 55h:57m:46s remains)
INFO - root - 2017-12-07 16:56:52.396325: step 32300, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 57h:40m:48s remains)
2017-12-07 16:56:53.168743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2444692 -4.2610092 -4.273396 -4.2782497 -4.273725 -4.2550564 -4.232235 -4.2162294 -4.2106414 -4.2163229 -4.2337203 -4.255794 -4.2773533 -4.2915196 -4.2966084][-4.2334647 -4.2474651 -4.25348 -4.2472525 -4.2320743 -4.2129116 -4.1977091 -4.189168 -4.1887431 -4.1995564 -4.2214174 -4.24759 -4.272593 -4.2901192 -4.2981577][-4.2254024 -4.2428408 -4.2439532 -4.2234545 -4.1937938 -4.1742668 -4.1683121 -4.1704512 -4.1759052 -4.189312 -4.2108607 -4.2366614 -4.2620826 -4.28192 -4.2927003][-4.2139592 -4.2332864 -4.2289314 -4.194768 -4.1529517 -4.1359754 -4.1381836 -4.147058 -4.1573095 -4.1708646 -4.1922517 -4.2221074 -4.2501378 -4.2732558 -4.2881141][-4.2091513 -4.2197824 -4.2069807 -4.1609554 -4.1089678 -4.096405 -4.1065044 -4.119019 -4.1340189 -4.1480207 -4.1741228 -4.209187 -4.2400723 -4.2659435 -4.2836227][-4.21456 -4.2042518 -4.1746197 -4.1174755 -4.0617661 -4.054637 -4.0720448 -4.0891018 -4.1100879 -4.1287804 -4.157989 -4.1941791 -4.2278976 -4.2544174 -4.2726631][-4.2279291 -4.1927176 -4.1435671 -4.074419 -4.0141544 -4.0046425 -4.0230594 -4.0422192 -4.0704365 -4.0996571 -4.1350956 -4.1752882 -4.2153707 -4.2467227 -4.264708][-4.2401471 -4.186141 -4.1243529 -4.0506573 -3.9884925 -3.9648013 -3.9700768 -3.984684 -4.0202789 -4.0618105 -4.10723 -4.1589241 -4.2106419 -4.2451582 -4.258728][-4.2455263 -4.1869917 -4.1248765 -4.059412 -4.0014887 -3.9654417 -3.9530358 -3.9643359 -4.0040951 -4.0512781 -4.102407 -4.1612744 -4.2166843 -4.2479296 -4.2485681][-4.2490015 -4.1969552 -4.1440916 -4.090251 -4.04289 -4.0067067 -3.9928195 -4.0137315 -4.0555096 -4.0999546 -4.1469393 -4.1991315 -4.2415204 -4.2570186 -4.2374039][-4.2514181 -4.2124181 -4.1755476 -4.1381593 -4.1073012 -4.08672 -4.0867033 -4.1157565 -4.1568594 -4.1960654 -4.2310009 -4.2623343 -4.2777252 -4.2659965 -4.2213488][-4.2510519 -4.2306929 -4.2122159 -4.1926131 -4.1783266 -4.1732492 -4.1819577 -4.2082481 -4.2429862 -4.2733731 -4.2924461 -4.302485 -4.2970896 -4.2647052 -4.2022357][-4.2610869 -4.2587447 -4.2549591 -4.2471228 -4.2398763 -4.2374749 -4.2424226 -4.25727 -4.2802491 -4.2986875 -4.3073597 -4.3072329 -4.2949104 -4.2602496 -4.1994467][-4.2709155 -4.2784986 -4.2802181 -4.2748923 -4.2659225 -4.258739 -4.2549629 -4.2579765 -4.2700796 -4.2807093 -4.2876849 -4.2899303 -4.28334 -4.2579684 -4.2123036][-4.2819262 -4.2890949 -4.2897925 -4.281343 -4.2665887 -4.2507133 -4.238348 -4.2321496 -4.2369981 -4.2451491 -4.2548647 -4.2625 -4.263669 -4.2480173 -4.219099]]...]
INFO - root - 2017-12-07 16:56:59.872065: step 32310, loss = 2.08, batch loss = 2.03 (13.1 examples/sec; 0.610 sec/batch; 50h:53m:33s remains)
INFO - root - 2017-12-07 16:57:06.599199: step 32320, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 58h:04m:49s remains)
INFO - root - 2017-12-07 16:57:13.348635: step 32330, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 52h:49m:10s remains)
INFO - root - 2017-12-07 16:57:20.227334: step 32340, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 58h:53m:29s remains)
INFO - root - 2017-12-07 16:57:27.029117: step 32350, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 56h:35m:18s remains)
INFO - root - 2017-12-07 16:57:33.798042: step 32360, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 52h:47m:46s remains)
INFO - root - 2017-12-07 16:57:40.588566: step 32370, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 55h:34m:27s remains)
INFO - root - 2017-12-07 16:57:47.494026: step 32380, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 59h:31m:09s remains)
INFO - root - 2017-12-07 16:57:54.346167: step 32390, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 58h:17m:04s remains)
INFO - root - 2017-12-07 16:58:01.234247: step 32400, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 56h:19m:20s remains)
2017-12-07 16:58:02.003273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2018213 -4.2022948 -4.2086511 -4.2267146 -4.243372 -4.2533331 -4.2658391 -4.2830687 -4.2959428 -4.2990861 -4.2988868 -4.3003626 -4.30182 -4.3057828 -4.3107729][-4.1506696 -4.1475577 -4.1511974 -4.1706285 -4.1899409 -4.1994176 -4.2115507 -4.2329512 -4.2491684 -4.2542043 -4.260869 -4.269989 -4.2790928 -4.2883177 -4.2959342][-4.1368337 -4.1307387 -4.13047 -4.1409912 -4.150702 -4.1499348 -4.1544733 -4.1699157 -4.1813631 -4.1863904 -4.2022009 -4.2228246 -4.2429757 -4.2611609 -4.2724195][-4.1519241 -4.1463127 -4.145853 -4.142055 -4.1302652 -4.1108131 -4.1004043 -4.1049614 -4.1112509 -4.1238737 -4.153861 -4.18595 -4.2128611 -4.2374372 -4.25137][-4.1621013 -4.16152 -4.165462 -4.1508055 -4.1149306 -4.0688848 -4.033329 -4.0162044 -4.0263996 -4.065784 -4.1169209 -4.1574216 -4.1861749 -4.2141681 -4.2301245][-4.1623039 -4.1686568 -4.1781821 -4.1626644 -4.11274 -4.0386415 -3.9560742 -3.900038 -3.9272907 -4.01396 -4.0876708 -4.1305332 -4.1600289 -4.1877055 -4.2053213][-4.1552691 -4.1695018 -4.1856766 -4.1735082 -4.1179538 -4.0172482 -3.8817639 -3.7831612 -3.848407 -3.9801183 -4.0665426 -4.1094112 -4.1398039 -4.1642008 -4.18177][-4.1541758 -4.1752872 -4.1989431 -4.191896 -4.1347013 -4.0297256 -3.8971522 -3.8192828 -3.8938036 -4.0030465 -4.0741673 -4.10954 -4.1328306 -4.1488681 -4.1663485][-4.1781178 -4.2029085 -4.2278619 -4.2206368 -4.1672688 -4.0854926 -4.0054655 -3.9763317 -4.0185809 -4.0710206 -4.102046 -4.1202025 -4.1344829 -4.1415291 -4.1645041][-4.2169189 -4.2391787 -4.2599573 -4.2532163 -4.2075973 -4.1523438 -4.114881 -4.1085882 -4.1260929 -4.1436048 -4.1433725 -4.1409945 -4.1413116 -4.1443315 -4.175169][-4.2596426 -4.2700806 -4.28492 -4.2837372 -4.2552323 -4.2219934 -4.2026911 -4.2052259 -4.2130413 -4.2151332 -4.1988287 -4.1768546 -4.1625385 -4.1654916 -4.1996117][-4.2826042 -4.2815914 -4.2945652 -4.3004284 -4.2871456 -4.2666206 -4.2568259 -4.2650394 -4.2729936 -4.2683024 -4.2435493 -4.2106409 -4.1832881 -4.1880736 -4.2253022][-4.2967572 -4.28967 -4.29723 -4.3056536 -4.3017435 -4.2903256 -4.2856345 -4.29956 -4.3105321 -4.3024569 -4.2759562 -4.2357321 -4.204453 -4.2117262 -4.2489586][-4.3081417 -4.3013592 -4.3033261 -4.3053255 -4.3013225 -4.293437 -4.2910495 -4.3049135 -4.3171172 -4.3154917 -4.2923779 -4.254396 -4.2269392 -4.2353792 -4.2672706][-4.3053083 -4.3004456 -4.2997761 -4.2968678 -4.2895436 -4.2816687 -4.2814 -4.2937326 -4.3045735 -4.3037348 -4.2874861 -4.2643542 -4.2491689 -4.2569237 -4.2814875]]...]
INFO - root - 2017-12-07 16:58:08.693631: step 32410, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 54h:36m:07s remains)
INFO - root - 2017-12-07 16:58:15.401596: step 32420, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 59h:53m:51s remains)
INFO - root - 2017-12-07 16:58:22.241723: step 32430, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 55h:21m:06s remains)
INFO - root - 2017-12-07 16:58:29.055831: step 32440, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 53h:34m:47s remains)
INFO - root - 2017-12-07 16:58:35.861271: step 32450, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 56h:46m:51s remains)
INFO - root - 2017-12-07 16:58:42.674243: step 32460, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 56h:29m:20s remains)
INFO - root - 2017-12-07 16:58:49.511077: step 32470, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 57h:10m:31s remains)
INFO - root - 2017-12-07 16:58:56.140123: step 32480, loss = 2.08, batch loss = 2.03 (16.1 examples/sec; 0.496 sec/batch; 41h:19m:09s remains)
INFO - root - 2017-12-07 16:59:02.962696: step 32490, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 60h:24m:48s remains)
INFO - root - 2017-12-07 16:59:09.922616: step 32500, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 60h:32m:07s remains)
2017-12-07 16:59:10.611639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2082658 -4.133327 -4.0601654 -4.0084443 -3.9890537 -4.0103602 -4.0302062 -4.0289874 -4.0198545 -4.002367 -3.9700494 -3.9656463 -3.9945431 -4.0542827 -4.0923028][-4.2256246 -4.1613431 -4.0982852 -4.0582447 -4.0408535 -4.0558953 -4.07382 -4.0678759 -4.046381 -4.0197039 -3.9907384 -3.9868174 -4.0085421 -4.0585012 -4.0909138][-4.2425308 -4.1897597 -4.1403027 -4.1098318 -4.0913153 -4.0949588 -4.1114264 -4.111217 -4.090064 -4.064281 -4.0455947 -4.0434051 -4.0576506 -4.0853696 -4.1002569][-4.2571592 -4.2111917 -4.1671157 -4.1348171 -4.1075945 -4.0956264 -4.1045485 -4.1143613 -4.1016026 -4.0853581 -4.0801458 -4.0805168 -4.0852861 -4.092793 -4.0982833][-4.2684069 -4.2214613 -4.1696477 -4.1238389 -4.0847325 -4.0668945 -4.0710373 -4.0826993 -4.0779676 -4.0748096 -4.0805459 -4.0802541 -4.0736279 -4.0734882 -4.0857158][-4.2718039 -4.2174058 -4.14751 -4.0753746 -4.0194855 -3.9962072 -3.9987047 -4.0225167 -4.0399623 -4.0587358 -4.0761294 -4.0818563 -4.0738788 -4.0737486 -4.0873489][-4.2571654 -4.184063 -4.0800452 -3.9730773 -3.9047322 -3.888196 -3.8973415 -3.9340115 -3.9685669 -3.9961085 -4.0204539 -4.0386119 -4.0433726 -4.0544548 -4.0696378][-4.2294621 -4.1351776 -3.9991693 -3.8662195 -3.8075688 -3.8102612 -3.83204 -3.8785005 -3.9186532 -3.9430404 -3.9662607 -3.9910631 -4.006712 -4.0339413 -4.0636773][-4.2207341 -4.1299057 -4.0127521 -3.9080846 -3.8794689 -3.897006 -3.9243264 -3.9722853 -4.0131688 -4.0257139 -4.0301228 -4.0398159 -4.0430923 -4.0609941 -4.0892253][-4.2326217 -4.1599073 -4.0785007 -4.0168548 -4.00477 -4.01849 -4.0376749 -4.077302 -4.1117625 -4.1124167 -4.1009779 -4.0965724 -4.0907416 -4.1001306 -4.1233497][-4.2516637 -4.1930766 -4.1311984 -4.0911894 -4.079668 -4.0812249 -4.0918012 -4.1222358 -4.143455 -4.1385317 -4.1211119 -4.1096935 -4.1063023 -4.1209488 -4.1465993][-4.2649527 -4.2152529 -4.1617823 -4.1277156 -4.1064615 -4.0917358 -4.0940514 -4.1179667 -4.1329756 -4.1301913 -4.1158342 -4.0999751 -4.0987825 -4.1207943 -4.141901][-4.2539115 -4.1984844 -4.1406465 -4.099638 -4.0689511 -4.0454187 -4.0477991 -4.0745983 -4.0943112 -4.1066895 -4.1022243 -4.0906186 -4.0922537 -4.1130314 -4.1293788][-4.2322507 -4.1646252 -4.0982237 -4.0506091 -4.0180469 -3.9952869 -4.0014682 -4.0339189 -4.0631657 -4.0843482 -4.085146 -4.0740004 -4.077177 -4.0975475 -4.1132622][-4.2163596 -4.1424527 -4.0759983 -4.0299263 -3.9985132 -3.9803698 -3.9880321 -4.0249782 -4.0611906 -4.0725222 -4.0607209 -4.0450344 -4.0469894 -4.0704031 -4.0885572]]...]
INFO - root - 2017-12-07 16:59:17.458463: step 32510, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 59h:49m:24s remains)
INFO - root - 2017-12-07 16:59:24.165855: step 32520, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 58h:38m:00s remains)
INFO - root - 2017-12-07 16:59:31.001495: step 32530, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 57h:16m:58s remains)
INFO - root - 2017-12-07 16:59:37.823564: step 32540, loss = 2.03, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 52h:34m:53s remains)
INFO - root - 2017-12-07 16:59:44.623418: step 32550, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 55h:36m:56s remains)
INFO - root - 2017-12-07 16:59:51.435613: step 32560, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 57h:14m:10s remains)
INFO - root - 2017-12-07 16:59:58.339480: step 32570, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 57h:45m:40s remains)
INFO - root - 2017-12-07 17:00:05.143691: step 32580, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 60h:29m:56s remains)
INFO - root - 2017-12-07 17:00:11.903868: step 32590, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 57h:21m:40s remains)
INFO - root - 2017-12-07 17:00:18.764265: step 32600, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 56h:16m:29s remains)
2017-12-07 17:00:19.597604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.210844 -4.2285671 -4.259438 -4.28288 -4.3016367 -4.3023539 -4.2727208 -4.2162957 -4.1777 -4.1866241 -4.2093244 -4.2273803 -4.2317472 -4.226728 -4.2266517][-4.2103949 -4.2321754 -4.2634811 -4.2799945 -4.2851009 -4.2786322 -4.2488217 -4.1954827 -4.165185 -4.1763444 -4.1965628 -4.2133989 -4.2231488 -4.2258339 -4.2327967][-4.1998372 -4.2193203 -4.2482605 -4.2625866 -4.265326 -4.255178 -4.2285357 -4.1850085 -4.1660366 -4.1704822 -4.1829929 -4.198101 -4.209228 -4.2206607 -4.2398815][-4.1815395 -4.1975055 -4.2193594 -4.2336364 -4.24174 -4.2345524 -4.2156773 -4.1847973 -4.1678419 -4.1631169 -4.1699977 -4.190228 -4.2047219 -4.2267 -4.2527971][-4.1642041 -4.1733232 -4.190753 -4.2092447 -4.2245474 -4.2222457 -4.2097821 -4.182076 -4.1594753 -4.1404924 -4.1404805 -4.1738224 -4.2079868 -4.2448354 -4.2718663][-4.1536841 -4.1504836 -4.1608639 -4.1839733 -4.2052422 -4.2051363 -4.1941805 -4.1696277 -4.14653 -4.12357 -4.1272731 -4.1785307 -4.2328091 -4.27363 -4.2912][-4.1614504 -4.1458387 -4.1419816 -4.1557121 -4.1715178 -4.1674213 -4.156487 -4.1432409 -4.1356497 -4.1256814 -4.1402488 -4.1904049 -4.2400045 -4.2758384 -4.2880692][-4.1859212 -4.1620607 -4.145205 -4.1389928 -4.1372867 -4.1204429 -4.1017804 -4.0974908 -4.1081471 -4.1112623 -4.1317983 -4.1756458 -4.2190776 -4.2532115 -4.2695885][-4.2125125 -4.1867723 -4.164073 -4.1462903 -4.1269822 -4.0941715 -4.0659266 -4.0707808 -4.0950909 -4.1119938 -4.1354 -4.1757517 -4.2123351 -4.2389135 -4.2497473][-4.2023478 -4.1827655 -4.1675081 -4.1533523 -4.1315255 -4.0913129 -4.0570917 -4.0733943 -4.1138997 -4.1418672 -4.166101 -4.1984067 -4.2225022 -4.2325354 -4.2313666][-4.1865072 -4.17675 -4.1733937 -4.1660614 -4.1442027 -4.1064911 -4.0781341 -4.0991721 -4.1418271 -4.1677675 -4.1840591 -4.2042522 -4.216897 -4.2160449 -4.20882][-4.1911521 -4.1908345 -4.1995258 -4.1993179 -4.1780529 -4.1515703 -4.1389766 -4.1526933 -4.1830225 -4.2027454 -4.213788 -4.2184896 -4.2186852 -4.2112908 -4.2013783][-4.2073927 -4.2103367 -4.2215629 -4.2235923 -4.2053542 -4.1913795 -4.1949854 -4.2032571 -4.2143631 -4.2189679 -4.2167268 -4.2131858 -4.213614 -4.2137966 -4.2068329][-4.2171092 -4.2196517 -4.2285867 -4.2285376 -4.2142758 -4.2112236 -4.2214327 -4.224051 -4.2231536 -4.2168126 -4.20641 -4.201827 -4.2074003 -4.2155266 -4.2147126][-4.21572 -4.2160754 -4.225512 -4.2266989 -4.2168026 -4.2159152 -4.2242336 -4.2281656 -4.2274013 -4.2187419 -4.2106795 -4.2076616 -4.2119436 -4.2224 -4.2270074]]...]
INFO - root - 2017-12-07 17:00:26.278314: step 32610, loss = 2.09, batch loss = 2.03 (14.6 examples/sec; 0.546 sec/batch; 45h:30m:38s remains)
INFO - root - 2017-12-07 17:00:32.927823: step 32620, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.643 sec/batch; 53h:31m:21s remains)
INFO - root - 2017-12-07 17:00:39.711948: step 32630, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 55h:03m:44s remains)
INFO - root - 2017-12-07 17:00:46.489907: step 32640, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 56h:53m:00s remains)
INFO - root - 2017-12-07 17:00:53.310115: step 32650, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 59h:35m:33s remains)
INFO - root - 2017-12-07 17:01:00.000062: step 32660, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 56h:07m:40s remains)
INFO - root - 2017-12-07 17:01:06.723958: step 32670, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 54h:55m:07s remains)
INFO - root - 2017-12-07 17:01:13.505086: step 32680, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 54h:00m:18s remains)
INFO - root - 2017-12-07 17:01:20.270159: step 32690, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 58h:23m:21s remains)
INFO - root - 2017-12-07 17:01:26.995669: step 32700, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 57h:01m:01s remains)
2017-12-07 17:01:27.819451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2953472 -4.2886324 -4.2907047 -4.2931461 -4.287035 -4.2647386 -4.2381058 -4.2182345 -4.2083597 -4.2020774 -4.193994 -4.197361 -4.1893616 -4.1733885 -4.16828][-4.2860818 -4.2781496 -4.2779727 -4.2763896 -4.2683034 -4.2431726 -4.2098808 -4.1867948 -4.1739335 -4.1606469 -4.14719 -4.1454816 -4.1285915 -4.1061506 -4.0996232][-4.28349 -4.2783432 -4.2777691 -4.2714672 -4.2603641 -4.234302 -4.1929784 -4.1613708 -4.1437397 -4.128015 -4.1132984 -4.1092286 -4.0885782 -4.0658083 -4.0607939][-4.2923341 -4.2908888 -4.289784 -4.2766132 -4.2532182 -4.2143507 -4.1616073 -4.12194 -4.1065497 -4.1009569 -4.0960317 -4.0940123 -4.0764751 -4.0625067 -4.0633259][-4.306479 -4.3051839 -4.3014779 -4.2783289 -4.2335925 -4.1688356 -4.0940251 -4.0455675 -4.0491529 -4.0774503 -4.0916557 -4.0934982 -4.0844259 -4.0822468 -4.0961409][-4.3151841 -4.3137865 -4.3100758 -4.2771859 -4.2100053 -4.1137123 -4.003253 -3.9268415 -3.9608552 -4.0467715 -4.0955291 -4.11282 -4.1166987 -4.1270595 -4.1506143][-4.3156853 -4.3136373 -4.3123045 -4.274116 -4.1946492 -4.0777154 -3.9366286 -3.8273973 -3.8867495 -4.0272703 -4.1102138 -4.14816 -4.1695161 -4.1879077 -4.2133245][-4.3106904 -4.3065996 -4.3051543 -4.2692561 -4.1918278 -4.0802922 -3.9495447 -3.856194 -3.9195871 -4.0618882 -4.1496582 -4.1920376 -4.2177248 -4.2378025 -4.2565761][-4.3085771 -4.3020725 -4.2962732 -4.2649055 -4.1975288 -4.1018014 -4.00825 -3.9603176 -4.0181732 -4.1278658 -4.191895 -4.2246246 -4.246871 -4.2647853 -4.2770419][-4.3068814 -4.2979517 -4.2875996 -4.2604842 -4.2021356 -4.1279297 -4.0703039 -4.0552382 -4.108561 -4.1879482 -4.229785 -4.249403 -4.267323 -4.2815065 -4.2873821][-4.3049407 -4.2927756 -4.2796926 -4.2557945 -4.2040591 -4.1464639 -4.1155519 -4.1194177 -4.1676478 -4.22578 -4.252912 -4.264327 -4.2790833 -4.2912064 -4.291913][-4.3042517 -4.2894154 -4.2747269 -4.2548833 -4.2107115 -4.1627378 -4.1487412 -4.1623425 -4.2039909 -4.2477026 -4.2658796 -4.27185 -4.2823944 -4.2938557 -4.29519][-4.30447 -4.2892132 -4.2736454 -4.2586989 -4.2239065 -4.1834397 -4.1781125 -4.1924582 -4.219285 -4.2489972 -4.2572775 -4.2595787 -4.2703872 -4.2864852 -4.2943296][-4.3050942 -4.2915182 -4.277329 -4.2660532 -4.2395353 -4.206543 -4.2041335 -4.2124686 -4.2213316 -4.2328639 -4.2282233 -4.2271333 -4.2436895 -4.2693872 -4.2851982][-4.3043194 -4.2923427 -4.2804637 -4.2722521 -4.2518163 -4.2258968 -4.2210793 -4.222086 -4.2113037 -4.1996074 -4.1733055 -4.1657758 -4.1969013 -4.2414289 -4.2715459]]...]
INFO - root - 2017-12-07 17:01:34.499661: step 32710, loss = 2.06, batch loss = 2.01 (15.1 examples/sec; 0.529 sec/batch; 44h:01m:54s remains)
INFO - root - 2017-12-07 17:01:41.333125: step 32720, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 58h:31m:47s remains)
INFO - root - 2017-12-07 17:01:48.164365: step 32730, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 55h:23m:16s remains)
INFO - root - 2017-12-07 17:01:55.111302: step 32740, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 57h:31m:02s remains)
INFO - root - 2017-12-07 17:02:01.900314: step 32750, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 53h:25m:05s remains)
INFO - root - 2017-12-07 17:02:08.744441: step 32760, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 58h:58m:01s remains)
INFO - root - 2017-12-07 17:02:15.530832: step 32770, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 59h:49m:25s remains)
INFO - root - 2017-12-07 17:02:22.308985: step 32780, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 58h:56m:08s remains)
INFO - root - 2017-12-07 17:02:29.030060: step 32790, loss = 2.06, batch loss = 2.00 (15.5 examples/sec; 0.515 sec/batch; 42h:51m:04s remains)
INFO - root - 2017-12-07 17:02:35.727750: step 32800, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 55h:16m:58s remains)
2017-12-07 17:02:36.524619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2563205 -4.257452 -4.2364621 -4.2099261 -4.19308 -4.190414 -4.2050147 -4.2312846 -4.2539482 -4.269794 -4.2772875 -4.2803388 -4.2805352 -4.27575 -4.26398][-4.2339396 -4.2334652 -4.2049341 -4.1619053 -4.1328354 -4.1289473 -4.1484008 -4.1849675 -4.22371 -4.252008 -4.2701588 -4.2800636 -4.2872496 -4.2899718 -4.2867117][-4.182971 -4.1816564 -4.1547546 -4.1029987 -4.0648246 -4.0642796 -4.0999784 -4.1531272 -4.2027454 -4.2310176 -4.2511239 -4.2655149 -4.2759171 -4.2822046 -4.2855787][-4.1225991 -4.125329 -4.1107736 -4.0645409 -4.0234494 -4.0272117 -4.0753145 -4.1359692 -4.184525 -4.206264 -4.2220097 -4.2394204 -4.2537155 -4.2650037 -4.2759562][-4.0983496 -4.1085687 -4.11573 -4.0919943 -4.0542884 -4.0511646 -4.0835147 -4.1250105 -4.1599622 -4.174859 -4.1847119 -4.2015114 -4.2178578 -4.2329845 -4.2503614][-4.0980506 -4.1122174 -4.1399531 -4.136251 -4.0986395 -4.0686278 -4.0547605 -4.0541091 -4.0727687 -4.08871 -4.1034279 -4.1296339 -4.1596665 -4.1870012 -4.2141752][-4.0862832 -4.0934792 -4.1232638 -4.1198559 -4.069993 -4.0062642 -3.9418669 -3.8978038 -3.9175673 -3.9621859 -4.0001349 -4.041223 -4.0851989 -4.1287489 -4.1697259][-4.076817 -4.0748792 -4.08966 -4.07537 -4.0101624 -3.9199188 -3.8220749 -3.7523656 -3.7806363 -3.8594921 -3.9226182 -3.9764774 -4.0284958 -4.0852027 -4.1391554][-4.1137133 -4.1087332 -4.10996 -4.0901489 -4.0291343 -3.9462013 -3.8552461 -3.7971535 -3.8240798 -3.8934214 -3.951359 -4.0011282 -4.0500021 -4.1040034 -4.1585712][-4.1674891 -4.1634145 -4.16159 -4.152627 -4.1101413 -4.0479856 -3.9853258 -3.951386 -3.9638724 -4.0014472 -4.0411844 -4.0805283 -4.1193304 -4.1618066 -4.2033482][-4.2193465 -4.219841 -4.2199225 -4.2185726 -4.1932435 -4.1529274 -4.1166077 -4.0972667 -4.0981474 -4.1124654 -4.1387758 -4.1684651 -4.1930571 -4.2179518 -4.2416067][-4.2510223 -4.2599092 -4.2631903 -4.2662454 -4.2561989 -4.2384992 -4.2240486 -4.2155209 -4.2114282 -4.2148852 -4.230679 -4.2494555 -4.2596273 -4.2661796 -4.2722526][-4.2604308 -4.2772841 -4.2852607 -4.289907 -4.2876759 -4.2841587 -4.2804308 -4.2793384 -4.276588 -4.2779641 -4.2855248 -4.2942882 -4.2966638 -4.2938762 -4.290751][-4.2615523 -4.2817845 -4.2930965 -4.2995162 -4.3019142 -4.3006945 -4.2975607 -4.2992306 -4.3000774 -4.3012276 -4.3022919 -4.3030272 -4.303081 -4.2997537 -4.2953587][-4.2627845 -4.2786288 -4.2893467 -4.2968073 -4.3016424 -4.30116 -4.2987394 -4.3005409 -4.3022051 -4.3026838 -4.2997308 -4.2963486 -4.2952938 -4.2953897 -4.2950635]]...]
INFO - root - 2017-12-07 17:02:43.133422: step 32810, loss = 2.07, batch loss = 2.01 (13.9 examples/sec; 0.574 sec/batch; 47h:49m:13s remains)
INFO - root - 2017-12-07 17:02:49.771532: step 32820, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 52h:44m:33s remains)
INFO - root - 2017-12-07 17:02:56.537644: step 32830, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.662 sec/batch; 55h:08m:43s remains)
INFO - root - 2017-12-07 17:03:03.474717: step 32840, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 59h:17m:00s remains)
INFO - root - 2017-12-07 17:03:10.236775: step 32850, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 57h:48m:28s remains)
INFO - root - 2017-12-07 17:03:16.925208: step 32860, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 57h:24m:18s remains)
INFO - root - 2017-12-07 17:03:23.688021: step 32870, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 55h:40m:44s remains)
INFO - root - 2017-12-07 17:03:30.437542: step 32880, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:52m:17s remains)
INFO - root - 2017-12-07 17:03:37.311522: step 32890, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:50m:52s remains)
INFO - root - 2017-12-07 17:03:44.123623: step 32900, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 57h:59m:45s remains)
2017-12-07 17:03:44.843972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2109518 -4.2120085 -4.2134962 -4.2239494 -4.2390924 -4.2564697 -4.2724295 -4.2764082 -4.266747 -4.2485867 -4.2400775 -4.2505689 -4.259254 -4.2599339 -4.2573104][-4.1861191 -4.1899681 -4.1901684 -4.1998448 -4.2182012 -4.2394576 -4.2581711 -4.2610979 -4.2499919 -4.2309852 -4.2211251 -4.2297149 -4.2403212 -4.2463064 -4.2476454][-4.1619487 -4.1676693 -4.1697168 -4.1839485 -4.2061892 -4.2253122 -4.2425427 -4.2488956 -4.244246 -4.2298503 -4.2195973 -4.2240486 -4.2315841 -4.239006 -4.2406845][-4.1428456 -4.1505 -4.1565666 -4.175056 -4.1960282 -4.207675 -4.2206206 -4.2334189 -4.239295 -4.2358732 -4.2319641 -4.2372317 -4.2428732 -4.247788 -4.2428875][-4.1187544 -4.1254668 -4.1343226 -4.1533294 -4.1709471 -4.1748915 -4.1758528 -4.1856775 -4.1994219 -4.2103314 -4.2156591 -4.2259116 -4.2357273 -4.2404046 -4.233439][-4.1034522 -4.1031237 -4.1059284 -4.1153932 -4.1226296 -4.1165714 -4.0996614 -4.0917926 -4.1105809 -4.1436753 -4.1663737 -4.1841149 -4.1980824 -4.2104545 -4.210578][-4.0913196 -4.0760717 -4.065732 -4.0639172 -4.0642428 -4.0459228 -3.9991426 -3.9570861 -3.9811742 -4.04975 -4.0970182 -4.1249228 -4.1437731 -4.1692104 -4.1848383][-4.0806146 -4.0522246 -4.0296144 -4.0203619 -4.017478 -3.9849586 -3.899425 -3.8040781 -3.8281138 -3.9398596 -4.0165591 -4.059545 -4.0893393 -4.1302543 -4.1673851][-4.098701 -4.0679197 -4.0474911 -4.0431151 -4.0398779 -4.0014224 -3.901315 -3.7810626 -3.7895713 -3.900907 -3.980859 -4.0258856 -4.061233 -4.1106539 -4.1592941][-4.1359835 -4.1098275 -4.1000552 -4.1046762 -4.108501 -4.0869031 -4.0217052 -3.9358516 -3.9225841 -3.979239 -4.030479 -4.0614543 -4.0857887 -4.1227617 -4.1636882][-4.1556187 -4.1354208 -4.1308961 -4.1377568 -4.147747 -4.1415854 -4.1105108 -4.0619545 -4.041564 -4.0621309 -4.0924287 -4.1105752 -4.1207471 -4.139761 -4.1686187][-4.1732812 -4.1596479 -4.1569977 -4.1619449 -4.17382 -4.1762319 -4.1644454 -4.1399441 -4.12098 -4.1231718 -4.1382861 -4.1458869 -4.1478519 -4.1565647 -4.1772585][-4.2102041 -4.2025356 -4.2012191 -4.2050962 -4.2136021 -4.2167945 -4.2116017 -4.1979022 -4.1820288 -4.1744928 -4.1794581 -4.1818585 -4.1843266 -4.1919994 -4.2061338][-4.2573414 -4.2525544 -4.2512116 -4.2528 -4.2570925 -4.2594323 -4.2577243 -4.2508283 -4.2384748 -4.2274466 -4.2276883 -4.2315607 -4.2376785 -4.2459235 -4.2538705][-4.2936659 -4.2912469 -4.2900648 -4.2907953 -4.2927313 -4.2938976 -4.2940297 -4.2915258 -4.28326 -4.2724433 -4.2725263 -4.2781477 -4.2842803 -4.2897086 -4.2929235]]...]
INFO - root - 2017-12-07 17:03:51.413853: step 32910, loss = 2.07, batch loss = 2.01 (14.6 examples/sec; 0.548 sec/batch; 45h:34m:57s remains)
INFO - root - 2017-12-07 17:03:58.290904: step 32920, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 56h:04m:17s remains)
INFO - root - 2017-12-07 17:04:05.082410: step 32930, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 55h:57m:58s remains)
INFO - root - 2017-12-07 17:04:11.862884: step 32940, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 54h:53m:07s remains)
INFO - root - 2017-12-07 17:04:18.589201: step 32950, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 52h:59m:06s remains)
INFO - root - 2017-12-07 17:04:25.532499: step 32960, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 59h:58m:19s remains)
INFO - root - 2017-12-07 17:04:32.315159: step 32970, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 57h:54m:11s remains)
INFO - root - 2017-12-07 17:04:39.033123: step 32980, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 55h:48m:54s remains)
INFO - root - 2017-12-07 17:04:45.708228: step 32990, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 55h:06m:18s remains)
INFO - root - 2017-12-07 17:04:52.508524: step 33000, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 57h:32m:42s remains)
2017-12-07 17:04:53.251296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2555304 -4.2235193 -4.2159462 -4.2196636 -4.2302594 -4.2367115 -4.2410965 -4.2434568 -4.2400122 -4.2513309 -4.26891 -4.2710428 -4.2622991 -4.2615967 -4.2733679][-4.2263355 -4.1909447 -4.182776 -4.1824241 -4.1924033 -4.2029362 -4.2026634 -4.1954331 -4.1901731 -4.2143545 -4.2482691 -4.2557225 -4.2477188 -4.2468781 -4.260179][-4.2036476 -4.1694241 -4.1611624 -4.152606 -4.1511569 -4.1566758 -4.1417427 -4.1224651 -4.1175718 -4.1611066 -4.2126765 -4.2320476 -4.2316289 -4.2336187 -4.2469721][-4.1858325 -4.1532531 -4.1435595 -4.1262131 -4.1089673 -4.1038313 -4.0688128 -4.0290465 -4.0319557 -4.0996566 -4.1675439 -4.2019143 -4.2115169 -4.2186885 -4.2336779][-4.1855407 -4.1580267 -4.1468024 -4.1244841 -4.0940814 -4.061305 -3.9792705 -3.9034295 -3.9321232 -4.0386543 -4.1267734 -4.1761909 -4.1975751 -4.2108936 -4.2250876][-4.2119212 -4.1873908 -4.1698503 -4.1422224 -4.0959177 -4.0172076 -3.8613796 -3.7346578 -3.810348 -3.9793792 -4.0985641 -4.166151 -4.1992664 -4.2133832 -4.2234368][-4.2470665 -4.2204442 -4.1901221 -4.1520758 -4.0908885 -3.9649649 -3.736901 -3.5667667 -3.7168241 -3.94684 -4.0896921 -4.1714182 -4.2117543 -4.2200518 -4.2231822][-4.2751918 -4.244453 -4.2037568 -4.16021 -4.0936203 -3.95356 -3.7223618 -3.5711346 -3.7425191 -3.9693789 -4.1046925 -4.18127 -4.2195058 -4.2211313 -4.2196689][-4.3030381 -4.2752109 -4.2401619 -4.2038517 -4.1457067 -4.0278606 -3.8481998 -3.7446766 -3.8722014 -4.0403728 -4.1391387 -4.1952925 -4.2229414 -4.22299 -4.2221308][-4.32735 -4.3108077 -4.2906995 -4.2642813 -4.2120595 -4.1164703 -3.9817524 -3.9183264 -4.0067611 -4.1183524 -4.1798925 -4.2124343 -4.2296615 -4.2312036 -4.2319708][-4.3406849 -4.3348579 -4.3250303 -4.3078394 -4.2624292 -4.1854382 -4.0825257 -4.0463037 -4.1105151 -4.179554 -4.217855 -4.2360992 -4.2469864 -4.24834 -4.2486591][-4.3403358 -4.340445 -4.3341317 -4.3151054 -4.2732897 -4.2131605 -4.1335583 -4.1177692 -4.1729527 -4.2222919 -4.2478542 -4.2589874 -4.2645555 -4.2634339 -4.2619724][-4.3272662 -4.3284545 -4.3240476 -4.3048573 -4.2707925 -4.2314463 -4.1809506 -4.1760769 -4.2176228 -4.2479243 -4.2639623 -4.2695966 -4.2711329 -4.2688212 -4.2672205][-4.3087664 -4.3084021 -4.3040471 -4.2882409 -4.2659631 -4.246233 -4.2204366 -4.2208 -4.2472649 -4.2649136 -4.2721858 -4.2702632 -4.2699485 -4.2705712 -4.2738762][-4.3022361 -4.2987523 -4.2948341 -4.2841434 -4.2723594 -4.2640791 -4.2555771 -4.2590737 -4.2749777 -4.2840061 -4.2848287 -4.2790451 -4.2788358 -4.2840385 -4.293469]]...]
INFO - root - 2017-12-07 17:04:59.858561: step 33010, loss = 2.08, batch loss = 2.02 (14.5 examples/sec; 0.552 sec/batch; 45h:54m:00s remains)
INFO - root - 2017-12-07 17:05:06.589482: step 33020, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 55h:03m:08s remains)
INFO - root - 2017-12-07 17:05:13.372668: step 33030, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 55h:22m:14s remains)
INFO - root - 2017-12-07 17:05:20.062319: step 33040, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 55h:43m:21s remains)
INFO - root - 2017-12-07 17:05:26.895263: step 33050, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.733 sec/batch; 60h:57m:10s remains)
INFO - root - 2017-12-07 17:05:33.831898: step 33060, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 56h:06m:49s remains)
INFO - root - 2017-12-07 17:05:40.589690: step 33070, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 54h:26m:00s remains)
INFO - root - 2017-12-07 17:05:47.370177: step 33080, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 55h:48m:18s remains)
INFO - root - 2017-12-07 17:05:54.147006: step 33090, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.675 sec/batch; 56h:09m:32s remains)
INFO - root - 2017-12-07 17:06:00.995774: step 33100, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 53h:52m:57s remains)
2017-12-07 17:06:01.667242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2863064 -4.2763758 -4.2731338 -4.2736154 -4.2761054 -4.2754412 -4.2654791 -4.2427726 -4.2094617 -4.1756144 -4.1536741 -4.161283 -4.1967649 -4.2295389 -4.249331][-4.2853551 -4.2717428 -4.266871 -4.2689023 -4.271626 -4.2684307 -4.251317 -4.2177825 -4.1777997 -4.1467304 -4.1350713 -4.1539431 -4.1964073 -4.2286186 -4.2453437][-4.2689362 -4.2496305 -4.2443271 -4.2489333 -4.2562294 -4.2539568 -4.2276993 -4.1796002 -4.1370444 -4.1177564 -4.1245246 -4.1583309 -4.19949 -4.2206535 -4.2294][-4.2615614 -4.2363324 -4.2273779 -4.2352204 -4.2476654 -4.2425523 -4.2039204 -4.1415319 -4.1002703 -4.1018782 -4.1334529 -4.1796846 -4.2126536 -4.2175932 -4.2208505][-4.2662334 -4.230855 -4.2153673 -4.2228136 -4.2375994 -4.2273021 -4.180285 -4.1118908 -4.0794258 -4.1033945 -4.1529059 -4.1995039 -4.2232594 -4.2253442 -4.2311296][-4.2760568 -4.2275295 -4.2039118 -4.2079306 -4.2189164 -4.2036443 -4.1579304 -4.0979056 -4.0802855 -4.1200733 -4.1724658 -4.2118769 -4.2282553 -4.2310939 -4.2369494][-4.2742162 -4.2213936 -4.1986232 -4.2000942 -4.2050409 -4.1879053 -4.1478276 -4.1014132 -4.0941472 -4.1383085 -4.1905785 -4.2257338 -4.23975 -4.243752 -4.2442884][-4.265831 -4.2141509 -4.1973343 -4.1963453 -4.2009721 -4.1880727 -4.1565857 -4.1200404 -4.1098723 -4.1405888 -4.1858721 -4.2193737 -4.2366786 -4.2442579 -4.2414742][-4.2615347 -4.2167444 -4.203804 -4.2023826 -4.2073779 -4.1959009 -4.1702414 -4.133183 -4.1107607 -4.1266246 -4.1663265 -4.2019129 -4.2249131 -4.2315702 -4.2277927][-4.2490621 -4.2087073 -4.1975446 -4.200079 -4.2101345 -4.2051148 -4.1766014 -4.1335912 -4.1034455 -4.1157894 -4.1526427 -4.189219 -4.208035 -4.2093515 -4.2039022][-4.2375937 -4.1984973 -4.1830044 -4.1897755 -4.2070775 -4.2051373 -4.1728339 -4.12826 -4.1040363 -4.1205668 -4.1513362 -4.1842794 -4.1954861 -4.1926937 -4.1878948][-4.2312679 -4.18697 -4.1645546 -4.1614575 -4.1730661 -4.1768556 -4.1529627 -4.1226568 -4.1155663 -4.1418324 -4.1664596 -4.1887403 -4.1919217 -4.185976 -4.1835208][-4.2200346 -4.1727834 -4.1483345 -4.1315556 -4.1243191 -4.1276994 -4.1209383 -4.1160841 -4.1289034 -4.1640892 -4.1808085 -4.1933026 -4.199048 -4.1978006 -4.196701][-4.2291694 -4.1911554 -4.1748033 -4.1503553 -4.116344 -4.0989375 -4.101697 -4.1245909 -4.1498113 -4.1813107 -4.18657 -4.1949773 -4.21323 -4.222887 -4.2202606][-4.2458906 -4.22124 -4.2142711 -4.1945004 -4.150815 -4.1205692 -4.122478 -4.1524973 -4.177249 -4.1946378 -4.1892581 -4.1933656 -4.2164354 -4.2323251 -4.2343054]]...]
INFO - root - 2017-12-07 17:06:08.304666: step 33110, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 51h:45m:08s remains)
INFO - root - 2017-12-07 17:06:15.008572: step 33120, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 55h:55m:56s remains)
INFO - root - 2017-12-07 17:06:21.729435: step 33130, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 58h:15m:14s remains)
INFO - root - 2017-12-07 17:06:28.439374: step 33140, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.641 sec/batch; 53h:18m:35s remains)
INFO - root - 2017-12-07 17:06:35.158356: step 33150, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 54h:24m:51s remains)
INFO - root - 2017-12-07 17:06:41.947329: step 33160, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 57h:54m:42s remains)
INFO - root - 2017-12-07 17:06:48.741844: step 33170, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.699 sec/batch; 58h:09m:11s remains)
INFO - root - 2017-12-07 17:06:55.515025: step 33180, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 51h:42m:09s remains)
INFO - root - 2017-12-07 17:07:02.254454: step 33190, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 54h:15m:57s remains)
INFO - root - 2017-12-07 17:07:09.074765: step 33200, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 55h:10m:58s remains)
2017-12-07 17:07:09.859357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.289361 -4.2902513 -4.2845254 -4.2615995 -4.2291903 -4.1799221 -4.1293378 -4.1064138 -4.1104517 -4.14119 -4.1711292 -4.1861763 -4.1853456 -4.1823039 -4.1998096][-4.2738304 -4.2774181 -4.2757564 -4.2508183 -4.2030988 -4.1363463 -4.0818663 -4.0677538 -4.0874634 -4.1338863 -4.1670594 -4.1815386 -4.1842203 -4.1852212 -4.2037721][-4.2559881 -4.2613444 -4.26143 -4.2350063 -4.1752176 -4.0976639 -4.0443435 -4.04373 -4.079936 -4.1356535 -4.1656847 -4.1752453 -4.1803379 -4.186 -4.2029662][-4.2366638 -4.2503386 -4.2527728 -4.2245927 -4.1557932 -4.0670676 -4.0086727 -4.0224657 -4.0800142 -4.14398 -4.1692419 -4.1740746 -4.1759319 -4.1825929 -4.1984534][-4.215044 -4.2400546 -4.2468109 -4.215775 -4.1359529 -4.02908 -3.9547172 -3.9777315 -4.0638037 -4.1448364 -4.1707091 -4.1741314 -4.1720953 -4.1798329 -4.1982126][-4.206131 -4.2359104 -4.2372441 -4.1953311 -4.0948482 -3.958565 -3.852129 -3.8859651 -4.0170655 -4.1248269 -4.1577568 -4.1603365 -4.16225 -4.1767459 -4.2027864][-4.1862741 -4.2181549 -4.2124619 -4.1561732 -4.0328755 -3.8683331 -3.7365921 -3.7809124 -3.9578242 -4.0898547 -4.13105 -4.14106 -4.1557474 -4.1811337 -4.2134085][-4.1369009 -4.1777954 -4.1834631 -4.1291008 -4.0010195 -3.84271 -3.7320447 -3.7819526 -3.9568396 -4.0846586 -4.1218228 -4.1336441 -4.1568232 -4.1884952 -4.2222629][-4.117166 -4.1625147 -4.1823292 -4.1413217 -4.0314426 -3.9136868 -3.8537297 -3.9024091 -4.0277052 -4.11893 -4.1352873 -4.1333981 -4.1544189 -4.1879063 -4.2211413][-4.1499352 -4.1857567 -4.2062607 -4.1760192 -4.0903664 -4.0111227 -3.9835014 -4.0218554 -4.1021652 -4.1572895 -4.158217 -4.1454878 -4.161622 -4.192831 -4.2216225][-4.2008471 -4.2249575 -4.2402153 -4.21686 -4.1472692 -4.0858135 -4.0715685 -4.0984755 -4.1510248 -4.1848326 -4.183609 -4.1729317 -4.186645 -4.2119713 -4.2336273][-4.2345257 -4.2490072 -4.2595005 -4.2383966 -4.1812444 -4.131649 -4.1206751 -4.1409802 -4.1809359 -4.2061839 -4.2059989 -4.1989985 -4.2124782 -4.2346354 -4.2533431][-4.2420635 -4.2486467 -4.254302 -4.2344556 -4.1944861 -4.1592274 -4.1539955 -4.1740894 -4.2097464 -4.2274466 -4.2239861 -4.2190118 -4.232935 -4.2535067 -4.2728333][-4.2518191 -4.2571187 -4.259716 -4.2434969 -4.2150908 -4.1915855 -4.1888623 -4.2052288 -4.2349758 -4.2494979 -4.2467489 -4.24509 -4.2606678 -4.2795367 -4.2973351][-4.2737708 -4.2762461 -4.2751808 -4.2636461 -4.2454233 -4.23212 -4.2277479 -4.2388558 -4.2616277 -4.2740755 -4.274147 -4.276166 -4.2904119 -4.3063459 -4.3232012]]...]
INFO - root - 2017-12-07 17:07:16.481974: step 33210, loss = 2.04, batch loss = 1.98 (14.3 examples/sec; 0.558 sec/batch; 46h:23m:42s remains)
INFO - root - 2017-12-07 17:07:23.221369: step 33220, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 56h:47m:33s remains)
INFO - root - 2017-12-07 17:07:30.020759: step 33230, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 54h:10m:15s remains)
INFO - root - 2017-12-07 17:07:36.689761: step 33240, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 57h:57m:33s remains)
INFO - root - 2017-12-07 17:07:43.659635: step 33250, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.750 sec/batch; 62h:22m:36s remains)
INFO - root - 2017-12-07 17:07:50.412904: step 33260, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.677 sec/batch; 56h:17m:38s remains)
INFO - root - 2017-12-07 17:07:57.152834: step 33270, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 53h:27m:16s remains)
INFO - root - 2017-12-07 17:08:03.938717: step 33280, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 55h:45m:24s remains)
INFO - root - 2017-12-07 17:08:10.814137: step 33290, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 58h:55m:39s remains)
INFO - root - 2017-12-07 17:08:17.580197: step 33300, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 54h:28m:32s remains)
2017-12-07 17:08:18.294515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2737122 -4.2815537 -4.2673454 -4.233511 -4.2014546 -4.1685438 -4.1333828 -4.1077065 -4.1182709 -4.163672 -4.2158604 -4.2647324 -4.3011727 -4.3213482 -4.3324933][-4.2924647 -4.3010316 -4.2821097 -4.2403951 -4.2036409 -4.16317 -4.1144972 -4.0795488 -4.0963955 -4.1490641 -4.2056446 -4.2600513 -4.3006425 -4.3210969 -4.3329592][-4.2971725 -4.3073807 -4.2874379 -4.242013 -4.1997485 -4.1459126 -4.0825419 -4.0445957 -4.0748024 -4.1355877 -4.19637 -4.2574434 -4.30027 -4.3200631 -4.3317266][-4.2809215 -4.2949133 -4.2770882 -4.2305403 -4.1801758 -4.1105719 -4.04021 -4.0082865 -4.0530534 -4.1217408 -4.1882472 -4.2567139 -4.3002319 -4.3189745 -4.3301759][-4.2558889 -4.27111 -4.2531543 -4.2051444 -4.1450081 -4.0623283 -3.9948139 -3.9797356 -4.03945 -4.1165519 -4.1928816 -4.265192 -4.3068185 -4.3221235 -4.3318315][-4.2446351 -4.2523522 -4.2296343 -4.1769109 -4.1032581 -4.0135651 -3.962709 -3.97131 -4.0413189 -4.1232691 -4.2068963 -4.2790623 -4.3168249 -4.3296795 -4.3360319][-4.2308183 -4.2264218 -4.1946864 -4.1283507 -4.0335097 -3.9421062 -3.9304681 -3.973736 -4.0543046 -4.137661 -4.2227054 -4.2916036 -4.3250489 -4.3371 -4.3394804][-4.2089572 -4.1947508 -4.1532922 -4.0681953 -3.9451306 -3.850749 -3.8863091 -3.9697492 -4.0652294 -4.1513543 -4.2341681 -4.2991276 -4.3307085 -4.341866 -4.3419075][-4.2042189 -4.1847472 -4.1414456 -4.0561514 -3.9276357 -3.8389406 -3.8887141 -3.9784229 -4.0701714 -4.1528244 -4.2318792 -4.2960124 -4.3305988 -4.3438063 -4.3436842][-4.2128649 -4.1950507 -4.1570573 -4.0857582 -3.9781408 -3.9090428 -3.9457545 -4.0066018 -4.0762377 -4.1482038 -4.2231507 -4.2893958 -4.327683 -4.3427491 -4.3436][-4.2194514 -4.2058716 -4.1768889 -4.1222348 -4.0395966 -3.9900546 -4.0113883 -4.0444236 -4.092514 -4.1527433 -4.2193146 -4.2845359 -4.3243141 -4.3404622 -4.3424878][-4.2257242 -4.2109165 -4.1868014 -4.1479545 -4.0874915 -4.0509806 -4.0586915 -4.0762925 -4.1152678 -4.1682091 -4.22534 -4.2851481 -4.3235788 -4.3397994 -4.3420391][-4.2366228 -4.2194266 -4.1996717 -4.1733527 -4.128849 -4.0992408 -4.0955606 -4.1035204 -4.1390224 -4.1892724 -4.2391047 -4.2903252 -4.3245716 -4.3402295 -4.3426709][-4.25419 -4.2392693 -4.2231693 -4.2045774 -4.1715446 -4.1456747 -4.1353316 -4.1358371 -4.1643238 -4.2083869 -4.251524 -4.295013 -4.3254442 -4.3406062 -4.3429132][-4.2709985 -4.2598186 -4.2471237 -4.2301774 -4.2046866 -4.1850262 -4.175015 -4.1721091 -4.1908545 -4.2249608 -4.2625561 -4.3000174 -4.3270326 -4.3405986 -4.3422537]]...]
INFO - root - 2017-12-07 17:08:24.824076: step 33310, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 0.538 sec/batch; 44h:42m:32s remains)
INFO - root - 2017-12-07 17:08:31.793409: step 33320, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 61h:01m:23s remains)
INFO - root - 2017-12-07 17:08:38.707538: step 33330, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 59h:06m:33s remains)
INFO - root - 2017-12-07 17:08:45.552291: step 33340, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 54h:45m:48s remains)
INFO - root - 2017-12-07 17:08:52.292985: step 33350, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 53h:54m:45s remains)
INFO - root - 2017-12-07 17:08:59.100344: step 33360, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 58h:40m:08s remains)
INFO - root - 2017-12-07 17:09:05.899009: step 33370, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 58h:57m:52s remains)
INFO - root - 2017-12-07 17:09:12.691973: step 33380, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 57h:28m:54s remains)
INFO - root - 2017-12-07 17:09:19.419891: step 33390, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 52h:58m:56s remains)
INFO - root - 2017-12-07 17:09:26.239321: step 33400, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 54h:20m:10s remains)
2017-12-07 17:09:26.995132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2214212 -4.220685 -4.2219691 -4.2149696 -4.2027245 -4.2034307 -4.2045107 -4.208744 -4.2241168 -4.2394066 -4.2453275 -4.2414217 -4.233726 -4.2279825 -4.2180543][-4.167356 -4.1684394 -4.1782308 -4.180933 -4.1772928 -4.1847115 -4.1946697 -4.2032571 -4.2177572 -4.230186 -4.2375774 -4.2333694 -4.222909 -4.2127051 -4.1950264][-4.1344132 -4.1409531 -4.1613579 -4.1813221 -4.1954265 -4.2080789 -4.21866 -4.2266 -4.236721 -4.2432384 -4.25103 -4.2491183 -4.23684 -4.2231913 -4.2008681][-4.1369438 -4.1572394 -4.1873302 -4.2172661 -4.2391214 -4.2501411 -4.2567325 -4.2611332 -4.2600842 -4.2550778 -4.2625761 -4.2678871 -4.2606964 -4.247663 -4.2265544][-4.1833591 -4.2104926 -4.2405453 -4.2611217 -4.2753696 -4.2803135 -4.2761807 -4.2686744 -4.251905 -4.2356768 -4.2434416 -4.2580538 -4.2620974 -4.2570753 -4.2382617][-4.2477336 -4.267417 -4.2839508 -4.283534 -4.2772756 -4.2655635 -4.2433572 -4.2191825 -4.1986489 -4.1888204 -4.2021995 -4.2244406 -4.2399983 -4.244072 -4.2333026][-4.2852139 -4.286056 -4.2800255 -4.26147 -4.2367964 -4.2075858 -4.16 -4.1186881 -4.107152 -4.1188989 -4.1412406 -4.1753607 -4.2068319 -4.2252536 -4.2368131][-4.297998 -4.2831225 -4.2574425 -4.221282 -4.1789694 -4.131505 -4.0522323 -3.9800045 -3.9818659 -4.03026 -4.0721765 -4.1229682 -4.1733952 -4.2134886 -4.2496557][-4.303134 -4.2853565 -4.2496357 -4.1968279 -4.1366005 -4.0721445 -3.9707255 -3.8815544 -3.8993816 -3.983892 -4.0487146 -4.1126375 -4.1703172 -4.2182636 -4.261374][-4.3041816 -4.2846022 -4.2472348 -4.1907268 -4.1277823 -4.0671444 -3.9912879 -3.9399652 -3.9718227 -4.0487118 -4.112639 -4.1742535 -4.224369 -4.260386 -4.2886233][-4.2990942 -4.2749825 -4.2407403 -4.1937828 -4.1459794 -4.1069264 -4.0750961 -4.0736 -4.1137252 -4.1663637 -4.2121034 -4.2600541 -4.2930903 -4.31069 -4.3204207][-4.2775488 -4.2395539 -4.2004437 -4.1650019 -4.1399446 -4.1281209 -4.1358619 -4.1687059 -4.2104425 -4.2408161 -4.268384 -4.3006234 -4.3188066 -4.3272667 -4.3292823][-4.2396655 -4.1858659 -4.1383157 -4.1106324 -4.1064229 -4.12288 -4.1589384 -4.2140284 -4.25548 -4.272161 -4.284102 -4.3007646 -4.3109546 -4.315052 -4.3129988][-4.197298 -4.1333776 -4.0781617 -4.0570726 -4.0755215 -4.1194506 -4.1738749 -4.2323546 -4.265718 -4.2715406 -4.271101 -4.275794 -4.2819195 -4.2858768 -4.2807617][-4.1755714 -4.116972 -4.0698724 -4.0577216 -4.087647 -4.138546 -4.1895461 -4.2344971 -4.2540326 -4.2508802 -4.2424111 -4.2375779 -4.2390003 -4.2434978 -4.2356358]]...]
INFO - root - 2017-12-07 17:09:33.575429: step 33410, loss = 2.06, batch loss = 2.00 (16.0 examples/sec; 0.501 sec/batch; 41h:38m:11s remains)
INFO - root - 2017-12-07 17:09:40.211526: step 33420, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 54h:24m:20s remains)
INFO - root - 2017-12-07 17:09:46.992794: step 33430, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 58h:27m:39s remains)
INFO - root - 2017-12-07 17:09:53.929522: step 33440, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 59h:21m:06s remains)
INFO - root - 2017-12-07 17:10:00.709479: step 33450, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 57h:00m:46s remains)
INFO - root - 2017-12-07 17:10:07.460206: step 33460, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 55h:29m:12s remains)
INFO - root - 2017-12-07 17:10:14.158994: step 33470, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 52h:34m:42s remains)
INFO - root - 2017-12-07 17:10:20.915708: step 33480, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 58h:53m:45s remains)
INFO - root - 2017-12-07 17:10:27.778322: step 33490, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.731 sec/batch; 60h:41m:00s remains)
INFO - root - 2017-12-07 17:10:34.592217: step 33500, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 56h:46m:23s remains)
2017-12-07 17:10:35.415296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33728 -4.3280053 -4.3112626 -4.2884355 -4.2598233 -4.2275128 -4.2032442 -4.2030616 -4.2239947 -4.2469711 -4.259953 -4.262578 -4.2566791 -4.2485447 -4.2392945][-4.3478074 -4.3354516 -4.310863 -4.2757063 -4.2333856 -4.1887765 -4.1520281 -4.1435342 -4.1666 -4.1996942 -4.2245445 -4.2352023 -4.2375245 -4.2394457 -4.2365122][-4.3531628 -4.3375082 -4.3064876 -4.2605519 -4.2075505 -4.1537428 -4.1064878 -4.0923133 -4.1203127 -4.1630216 -4.1966763 -4.2091255 -4.2134809 -4.220654 -4.2220497][-4.3536158 -4.3356028 -4.2989497 -4.2441254 -4.1820574 -4.1210628 -4.0664558 -4.0521317 -4.0900264 -4.1469269 -4.1874323 -4.1951213 -4.1937623 -4.1989555 -4.201685][-4.3490529 -4.3269911 -4.2838688 -4.22098 -4.1498647 -4.0825047 -4.0242062 -4.0174408 -4.0699444 -4.1427507 -4.1914239 -4.1975422 -4.1915269 -4.1922846 -4.196311][-4.3405652 -4.3119507 -4.2604895 -4.1882477 -4.1091771 -4.0404935 -3.9877653 -3.9937544 -4.0590549 -4.1469011 -4.20427 -4.2128115 -4.2054873 -4.2047391 -4.2133274][-4.3307662 -4.2953439 -4.2401934 -4.1634736 -4.08367 -4.0252428 -3.9881675 -4.0101862 -4.0830512 -4.1731658 -4.2287512 -4.2382226 -4.2321105 -4.2325954 -4.2451553][-4.3241825 -4.2858543 -4.2327905 -4.16199 -4.09373 -4.0536842 -4.0366378 -4.0694 -4.14029 -4.2184219 -4.2634139 -4.2713594 -4.2667904 -4.2668691 -4.2779822][-4.3218474 -4.2839417 -4.2371116 -4.1796765 -4.1296263 -4.1046829 -4.1033435 -4.1419096 -4.2031679 -4.2637467 -4.2952256 -4.301548 -4.2974858 -4.2953849 -4.3023014][-4.3188691 -4.283535 -4.2454987 -4.2021284 -4.1703105 -4.1581459 -4.164979 -4.2032824 -4.253973 -4.2962394 -4.3148079 -4.3180761 -4.3145185 -4.314096 -4.3180861][-4.3135195 -4.2809377 -4.2491579 -4.2182965 -4.2026982 -4.2047954 -4.2203794 -4.2564983 -4.2959223 -4.321919 -4.3276858 -4.3246288 -4.319766 -4.3196597 -4.3221483][-4.3091841 -4.2818966 -4.2563572 -4.2379718 -4.2379608 -4.252079 -4.2715273 -4.3015223 -4.3267055 -4.3373861 -4.3318825 -4.3232031 -4.3159442 -4.3143415 -4.3130994][-4.3062148 -4.2853217 -4.2691197 -4.2642775 -4.2757835 -4.2933364 -4.3092818 -4.32668 -4.33724 -4.3375974 -4.3267689 -4.316772 -4.3080063 -4.3014402 -4.2903566][-4.305953 -4.2913971 -4.2836957 -4.2881308 -4.3048668 -4.3194265 -4.326242 -4.3308134 -4.3312783 -4.3260326 -4.3148613 -4.3059731 -4.2954187 -4.2819819 -4.2599134][-4.3087492 -4.2999511 -4.2978582 -4.30528 -4.3205528 -4.3308282 -4.330565 -4.3256879 -4.3193712 -4.3121581 -4.3035035 -4.2964206 -4.2837934 -4.265378 -4.2336984]]...]
INFO - root - 2017-12-07 17:10:41.984655: step 33510, loss = 2.06, batch loss = 2.00 (14.5 examples/sec; 0.552 sec/batch; 45h:52m:59s remains)
INFO - root - 2017-12-07 17:10:48.707294: step 33520, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 58h:18m:45s remains)
INFO - root - 2017-12-07 17:10:55.457952: step 33530, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 57h:11m:47s remains)
INFO - root - 2017-12-07 17:11:02.249908: step 33540, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 53h:40m:29s remains)
INFO - root - 2017-12-07 17:11:09.119819: step 33550, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 58h:14m:37s remains)
INFO - root - 2017-12-07 17:11:15.927748: step 33560, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 55h:16m:14s remains)
INFO - root - 2017-12-07 17:11:22.775931: step 33570, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 59h:15m:31s remains)
INFO - root - 2017-12-07 17:11:29.573514: step 33580, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 55h:21m:49s remains)
INFO - root - 2017-12-07 17:11:36.267603: step 33590, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 52h:39m:53s remains)
INFO - root - 2017-12-07 17:11:43.136576: step 33600, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 60h:40m:03s remains)
2017-12-07 17:11:43.821837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.292428 -4.2858768 -4.2775135 -4.2708206 -4.2672572 -4.2633734 -4.2517204 -4.244668 -4.246531 -4.256568 -4.262888 -4.2634692 -4.2639856 -4.2707624 -4.2788157][-4.2691884 -4.2616844 -4.2593231 -4.2558484 -4.24717 -4.2354369 -4.2193828 -4.2148113 -4.2220488 -4.238 -4.2480655 -4.2463794 -4.2427959 -4.2449446 -4.2507553][-4.241323 -4.2348833 -4.2391934 -4.2370687 -4.2209625 -4.1977181 -4.1697 -4.1621761 -4.174222 -4.2022738 -4.2235961 -4.2273192 -4.2229342 -4.2222142 -4.2250366][-4.2074809 -4.2087264 -4.2196064 -4.2175632 -4.197298 -4.1634197 -4.1163182 -4.0893579 -4.101295 -4.1507058 -4.1927147 -4.2063389 -4.2049837 -4.203824 -4.2058415][-4.1747031 -4.1893392 -4.2074862 -4.2064815 -4.1850586 -4.1407208 -4.0694366 -4.0065761 -4.0135736 -4.0919986 -4.1600738 -4.1865759 -4.1871686 -4.1884723 -4.1950908][-4.145082 -4.1700296 -4.1939259 -4.195456 -4.1744833 -4.1219058 -4.0219045 -3.9142315 -3.9133029 -4.0221786 -4.1162033 -4.1592345 -4.1691508 -4.1796813 -4.1893568][-4.1239972 -4.1542039 -4.1799264 -4.1832328 -4.164269 -4.108532 -3.9875927 -3.8366506 -3.8198495 -3.9528894 -4.0700946 -4.1323686 -4.1600056 -4.1769433 -4.184947][-4.1161747 -4.1479597 -4.1725397 -4.1799059 -4.1661248 -4.1138916 -3.9923108 -3.8276944 -3.7946274 -3.9246297 -4.0516992 -4.129065 -4.1680984 -4.184329 -4.1855249][-4.1235533 -4.1570325 -4.1821885 -4.1947675 -4.1887074 -4.1468477 -4.0494313 -3.9222007 -3.899924 -3.9923594 -4.0928292 -4.161447 -4.194427 -4.2034345 -4.1983776][-4.1437855 -4.1784039 -4.2046547 -4.21573 -4.2101159 -4.1775475 -4.1081977 -4.0267682 -4.0215249 -4.0837846 -4.14759 -4.1930466 -4.2162113 -4.2227669 -4.2173653][-4.1674609 -4.201736 -4.2272687 -4.2313232 -4.2205405 -4.197618 -4.153276 -4.1032991 -4.1071644 -4.1508121 -4.186028 -4.2067614 -4.2213674 -4.2291574 -4.2231154][-4.1836252 -4.2135787 -4.2367287 -4.2353249 -4.2237663 -4.2119975 -4.1861539 -4.1555271 -4.1628175 -4.1919918 -4.2072244 -4.2103038 -4.2190824 -4.227829 -4.2268996][-4.1999688 -4.2199492 -4.2384787 -4.2359242 -4.2261839 -4.2185745 -4.1994805 -4.1787133 -4.1875772 -4.2073865 -4.2143049 -4.21403 -4.2215142 -4.22984 -4.2346005][-4.2334819 -4.2439442 -4.25144 -4.2457304 -4.2368135 -4.2316389 -4.2188287 -4.2058859 -4.2150407 -4.2269907 -4.2299795 -4.2326279 -4.2391949 -4.2461376 -4.2542105][-4.2721963 -4.2754316 -4.27429 -4.2655544 -4.2587237 -4.2561922 -4.2510381 -4.2461967 -4.2544751 -4.2617846 -4.26308 -4.2651095 -4.2685165 -4.2735305 -4.2829823]]...]
INFO - root - 2017-12-07 17:11:50.374218: step 33610, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.484 sec/batch; 40h:11m:56s remains)
INFO - root - 2017-12-07 17:11:57.084030: step 33620, loss = 2.03, batch loss = 1.98 (13.1 examples/sec; 0.612 sec/batch; 50h:46m:34s remains)
INFO - root - 2017-12-07 17:12:03.853479: step 33630, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 59h:22m:58s remains)
INFO - root - 2017-12-07 17:12:10.661689: step 33640, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.720 sec/batch; 59h:44m:59s remains)
INFO - root - 2017-12-07 17:12:17.544632: step 33650, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 56h:52m:32s remains)
INFO - root - 2017-12-07 17:12:24.436521: step 33660, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 53h:49m:06s remains)
INFO - root - 2017-12-07 17:12:31.239546: step 33670, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 56h:43m:43s remains)
INFO - root - 2017-12-07 17:12:37.941685: step 33680, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 58h:32m:09s remains)
INFO - root - 2017-12-07 17:12:44.766655: step 33690, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 59h:44m:46s remains)
INFO - root - 2017-12-07 17:12:51.550417: step 33700, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 54h:46m:56s remains)
2017-12-07 17:12:52.287797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2833581 -4.282444 -4.2979565 -4.3185048 -4.3280244 -4.3165751 -4.31197 -4.3169885 -4.3131113 -4.3023477 -4.2969184 -4.3033953 -4.3149242 -4.3284292 -4.3405838][-4.240499 -4.23883 -4.2600632 -4.2861676 -4.2915893 -4.2746983 -4.2702904 -4.2808356 -4.2763891 -4.2637391 -4.2607346 -4.2691278 -4.2826018 -4.3003659 -4.319644][-4.1829815 -4.1868711 -4.2183428 -4.2487783 -4.2466884 -4.2233391 -4.2197742 -4.2381482 -4.2379384 -4.2287297 -4.234746 -4.2489324 -4.263144 -4.2802196 -4.30129][-4.1333952 -4.1451974 -4.1841345 -4.2138262 -4.199676 -4.1697912 -4.161582 -4.1827126 -4.1903062 -4.19082 -4.2087255 -4.2332916 -4.2593908 -4.2788219 -4.2949572][-4.1212564 -4.131433 -4.163065 -4.1805048 -4.1477575 -4.106678 -4.0897455 -4.1083121 -4.1209135 -4.1357541 -4.1715407 -4.2101398 -4.2510085 -4.2785687 -4.2919941][-4.1320176 -4.1330709 -4.1507916 -4.1472521 -4.09595 -4.0351033 -4.0008316 -4.0201354 -4.0443411 -4.0680757 -4.1142249 -4.1660972 -4.223269 -4.263546 -4.2773409][-4.1521096 -4.142911 -4.1439776 -4.1190491 -4.0515494 -3.9705057 -3.9159164 -3.9333398 -3.9602945 -3.9887025 -4.046886 -4.1125054 -4.1834106 -4.2312441 -4.249887][-4.1799021 -4.1575294 -4.139442 -4.1024113 -4.0330358 -3.9488449 -3.8849773 -3.8834653 -3.887032 -3.9058619 -3.9759469 -4.0561295 -4.1338291 -4.1849837 -4.2114754][-4.2026997 -4.1729541 -4.1474447 -4.1102743 -4.0536661 -3.9893558 -3.9353151 -3.9227171 -3.9060335 -3.9040811 -3.9687057 -4.0489359 -4.1203756 -4.1663094 -4.1938748][-4.2225628 -4.1967874 -4.1756606 -4.1473737 -4.1077042 -4.0684509 -4.0374279 -4.0288625 -4.0115986 -3.9978552 -4.0403519 -4.1005025 -4.151228 -4.1864691 -4.2085462][-4.24274 -4.2244344 -4.2128 -4.1977973 -4.1748114 -4.1542139 -4.1404052 -4.1353569 -4.1198812 -4.1047254 -4.1262259 -4.1619244 -4.1937571 -4.2206798 -4.2419987][-4.2625537 -4.2508011 -4.2460876 -4.2410321 -4.2321649 -4.2261009 -4.2229767 -4.2200994 -4.2061224 -4.1927624 -4.2002277 -4.2181187 -4.2384992 -4.2598758 -4.2784305][-4.2853575 -4.276722 -4.2743726 -4.2747197 -4.2729049 -4.2729464 -4.2744422 -4.2730594 -4.2620645 -4.2515831 -4.2526059 -4.2622256 -4.2761526 -4.2910957 -4.306293][-4.3079519 -4.3036766 -4.3027725 -4.3040175 -4.3032866 -4.3032842 -4.3037724 -4.3023944 -4.2963829 -4.290904 -4.2902241 -4.295989 -4.3065939 -4.3178291 -4.3299227][-4.3267889 -4.3262811 -4.3266511 -4.32735 -4.3269749 -4.3267059 -4.3270669 -4.3259268 -4.3242369 -4.3236904 -4.3251424 -4.3287358 -4.3351355 -4.3420358 -4.3486109]]...]
INFO - root - 2017-12-07 17:12:58.898705: step 33710, loss = 2.09, batch loss = 2.04 (13.7 examples/sec; 0.585 sec/batch; 48h:31m:17s remains)
INFO - root - 2017-12-07 17:13:05.676569: step 33720, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 53h:51m:17s remains)
INFO - root - 2017-12-07 17:13:12.258647: step 33730, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 54h:28m:19s remains)
INFO - root - 2017-12-07 17:13:19.068936: step 33740, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 57h:03m:41s remains)
INFO - root - 2017-12-07 17:13:25.987932: step 33750, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 60h:17m:29s remains)
INFO - root - 2017-12-07 17:13:32.744976: step 33760, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 55h:11m:22s remains)
INFO - root - 2017-12-07 17:13:39.496401: step 33770, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:40m:22s remains)
INFO - root - 2017-12-07 17:13:46.306837: step 33780, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 52h:48m:27s remains)
INFO - root - 2017-12-07 17:13:53.181331: step 33790, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 57h:48m:35s remains)
INFO - root - 2017-12-07 17:13:59.986451: step 33800, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 59h:49m:24s remains)
2017-12-07 17:14:00.648331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2771645 -4.2535324 -4.227387 -4.1983275 -4.1604991 -4.1237445 -4.1175652 -4.1496735 -4.1820498 -4.212183 -4.2425184 -4.261281 -4.2601981 -4.2552357 -4.2531385][-4.2861872 -4.2681246 -4.2422981 -4.2092223 -4.1675925 -4.1212916 -4.1007671 -4.1273451 -4.1692171 -4.2058725 -4.2360029 -4.25664 -4.2571778 -4.2539515 -4.2514005][-4.2879038 -4.270762 -4.2449207 -4.2134767 -4.1728249 -4.1207509 -4.0913234 -4.1101794 -4.15847 -4.2035141 -4.2377925 -4.2590985 -4.2643218 -4.2641907 -4.2613091][-4.2778473 -4.2602119 -4.233428 -4.2052393 -4.1734672 -4.1276755 -4.0985413 -4.1102867 -4.1510153 -4.1969404 -4.2361741 -4.2577157 -4.2632422 -4.2662716 -4.2703538][-4.2676997 -4.2531686 -4.2267981 -4.1979604 -4.1693835 -4.1290717 -4.0993633 -4.0990047 -4.1245122 -4.1736145 -4.222558 -4.2462234 -4.2487049 -4.2518506 -4.2636123][-4.2638903 -4.2538786 -4.2269645 -4.1910105 -4.1526217 -4.1031957 -4.0571828 -4.0220194 -4.0295238 -4.105227 -4.1814451 -4.2215476 -4.232192 -4.2394905 -4.2569809][-4.2648149 -4.2628112 -4.2361817 -4.1925726 -4.1388884 -4.0698929 -3.9898686 -3.9024785 -3.8953645 -4.0164495 -4.1299772 -4.1904068 -4.2159724 -4.2329426 -4.2552319][-4.2726817 -4.2800469 -4.2570271 -4.2088456 -4.1473823 -4.0671878 -3.9680841 -3.8638654 -3.8596344 -3.9975746 -4.1164341 -4.1775832 -4.2036343 -4.2230062 -4.2489429][-4.2835317 -4.2943459 -4.2730808 -4.219274 -4.1577125 -4.086771 -4.0039582 -3.93323 -3.9532931 -4.0663648 -4.1485076 -4.1846876 -4.198596 -4.2159238 -4.2416611][-4.2932577 -4.3066225 -4.2883906 -4.2367487 -4.1786628 -4.1212268 -4.05696 -4.0138884 -4.052299 -4.1377087 -4.1862741 -4.2055693 -4.2097435 -4.218647 -4.239181][-4.299088 -4.3153424 -4.3035536 -4.260457 -4.2076035 -4.1648197 -4.1114278 -4.0737929 -4.1117764 -4.1781688 -4.2116332 -4.2267041 -4.2315559 -4.2349973 -4.2444029][-4.3042569 -4.3188419 -4.3121285 -4.2807646 -4.2373648 -4.210237 -4.1701679 -4.1357684 -4.1626368 -4.2101121 -4.2339616 -4.2496967 -4.2563071 -4.2589064 -4.2604532][-4.3124881 -4.3217616 -4.3174448 -4.296412 -4.2691321 -4.2567272 -4.235477 -4.2116256 -4.2263041 -4.2511649 -4.2638917 -4.2756772 -4.2806344 -4.2818089 -4.2793546][-4.320127 -4.3227592 -4.3203249 -4.3079429 -4.291038 -4.2835817 -4.2725706 -4.2578082 -4.2655997 -4.2793369 -4.287128 -4.2964864 -4.3023434 -4.3022947 -4.2980366][-4.3239565 -4.3237786 -4.3231082 -4.316195 -4.3042684 -4.2978916 -4.2910943 -4.2823148 -4.2840819 -4.2914639 -4.2996192 -4.3080873 -4.3136067 -4.3145995 -4.3122811]]...]
INFO - root - 2017-12-07 17:14:07.257994: step 33810, loss = 2.05, batch loss = 2.00 (14.9 examples/sec; 0.537 sec/batch; 44h:35m:08s remains)
INFO - root - 2017-12-07 17:14:14.130792: step 33820, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 59h:34m:32s remains)
INFO - root - 2017-12-07 17:14:20.883461: step 33830, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 58h:02m:21s remains)
INFO - root - 2017-12-07 17:14:27.720717: step 33840, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 54h:30m:06s remains)
INFO - root - 2017-12-07 17:14:34.448015: step 33850, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 55h:16m:18s remains)
INFO - root - 2017-12-07 17:14:41.205534: step 33860, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 57h:26m:02s remains)
INFO - root - 2017-12-07 17:14:48.072076: step 33870, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 59h:57m:56s remains)
INFO - root - 2017-12-07 17:14:54.772585: step 33880, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 56h:26m:21s remains)
INFO - root - 2017-12-07 17:15:01.462458: step 33890, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 53h:10m:41s remains)
INFO - root - 2017-12-07 17:15:08.227996: step 33900, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.650 sec/batch; 53h:52m:25s remains)
2017-12-07 17:15:08.972305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1709127 -4.1844969 -4.2033067 -4.2060533 -4.1948009 -4.1827011 -4.177773 -4.16883 -4.1609817 -4.1590552 -4.1656246 -4.1855636 -4.1906409 -4.1783576 -4.1500654][-4.1259937 -4.1400161 -4.1634207 -4.1669884 -4.1540217 -4.1393523 -4.1321216 -4.1190014 -4.1146088 -4.1199923 -4.131804 -4.1521921 -4.1672 -4.163693 -4.134192][-4.10098 -4.1142526 -4.1407213 -4.1439023 -4.1287794 -4.1116495 -4.0982809 -4.0779305 -4.0754285 -4.0896544 -4.1081934 -4.1287785 -4.1513424 -4.15883 -4.1341958][-4.0961819 -4.1085534 -4.137238 -4.1426282 -4.1257348 -4.1046104 -4.0839691 -4.0573406 -4.05741 -4.0811 -4.1052022 -4.126318 -4.150991 -4.1619205 -4.1398869][-4.1063609 -4.1138716 -4.1398215 -4.145225 -4.1261773 -4.0995464 -4.0668907 -4.03183 -4.0433178 -4.085824 -4.1162009 -4.1331773 -4.1517816 -4.1591783 -4.1351995][-4.1148777 -4.1149278 -4.1285844 -4.1258831 -4.1021714 -4.0681109 -4.0211186 -3.9748476 -4.003273 -4.0718355 -4.113585 -4.12921 -4.1387405 -4.1395597 -4.1154804][-4.1250362 -4.1186738 -4.1189132 -4.1057072 -4.0740538 -4.0264606 -3.9568315 -3.8915231 -3.9323735 -4.031002 -4.0965686 -4.1220975 -4.1307263 -4.1289043 -4.1074595][-4.1275363 -4.1180615 -4.1124449 -4.0927696 -4.0564938 -4.0015965 -3.9186692 -3.8396802 -3.8817344 -3.995276 -4.075983 -4.1142812 -4.1309524 -4.1362505 -4.1200237][-4.1402364 -4.1371083 -4.1331668 -4.1178627 -4.0956006 -4.0563412 -3.9879003 -3.9184213 -3.9385409 -4.0230827 -4.0865889 -4.1224623 -4.1431727 -4.1555624 -4.1445823][-4.1525354 -4.1530857 -4.1549406 -4.1527443 -4.1502962 -4.1318731 -4.0842071 -4.0300026 -4.0248322 -4.0693288 -4.1075387 -4.1352777 -4.1555386 -4.1702452 -4.1648889][-4.18175 -4.1829495 -4.1881146 -4.1940908 -4.2018943 -4.1971216 -4.1658812 -4.122448 -4.1033764 -4.1191015 -4.1367965 -4.1579881 -4.176929 -4.1931119 -4.1909518][-4.2176442 -4.2204037 -4.2252707 -4.2308245 -4.2413249 -4.2445903 -4.2274694 -4.1963992 -4.1747007 -4.1746874 -4.1768641 -4.188036 -4.2004766 -4.2131543 -4.2115359][-4.2425361 -4.2470722 -4.2549505 -4.2606997 -4.27086 -4.2781234 -4.2704887 -4.2517176 -4.2359252 -4.2316208 -4.227314 -4.2282381 -4.2281556 -4.2320042 -4.2288804][-4.2735381 -4.2758145 -4.2825613 -4.2885132 -4.2968564 -4.3038359 -4.301053 -4.2907286 -4.2818279 -4.27575 -4.26819 -4.2608175 -4.2516184 -4.2420382 -4.23095][-4.2933455 -4.2926149 -4.2973938 -4.3009562 -4.3043356 -4.3058696 -4.3006611 -4.2918668 -4.2852612 -4.2789702 -4.270751 -4.2619467 -4.2508216 -4.2300458 -4.2087379]]...]
INFO - root - 2017-12-07 17:15:15.574362: step 33910, loss = 2.09, batch loss = 2.03 (13.2 examples/sec; 0.607 sec/batch; 50h:19m:33s remains)
INFO - root - 2017-12-07 17:15:22.158766: step 33920, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 54h:00m:01s remains)
INFO - root - 2017-12-07 17:15:28.884301: step 33930, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 53h:40m:00s remains)
INFO - root - 2017-12-07 17:15:35.598253: step 33940, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 59h:29m:05s remains)
INFO - root - 2017-12-07 17:15:42.305415: step 33950, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 55h:57m:28s remains)
INFO - root - 2017-12-07 17:15:48.946633: step 33960, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 55h:10m:36s remains)
INFO - root - 2017-12-07 17:15:55.682364: step 33970, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 54h:13m:34s remains)
INFO - root - 2017-12-07 17:16:02.397566: step 33980, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 53h:10m:37s remains)
INFO - root - 2017-12-07 17:16:09.137694: step 33990, loss = 2.09, batch loss = 2.04 (11.4 examples/sec; 0.705 sec/batch; 58h:25m:02s remains)
INFO - root - 2017-12-07 17:16:15.982722: step 34000, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 56h:23m:34s remains)
2017-12-07 17:16:16.670614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2379704 -4.2278237 -4.2242455 -4.2281866 -4.2342219 -4.2396159 -4.2372746 -4.2280612 -4.2173185 -4.2108092 -4.2073307 -4.2027259 -4.1887183 -4.1644745 -4.1554446][-4.2190104 -4.2035279 -4.1962414 -4.1975241 -4.2023015 -4.2079697 -4.2053628 -4.1924834 -4.1795678 -4.1746564 -4.1744533 -4.1709 -4.1542606 -4.1263223 -4.116715][-4.2035775 -4.1809249 -4.1672196 -4.1621 -4.1623821 -4.167398 -4.166841 -4.153317 -4.1433268 -4.1471615 -4.1550808 -4.1543336 -4.1421537 -4.1192861 -4.1124053][-4.1941214 -4.1631851 -4.1402993 -4.1263871 -4.1198492 -4.1208682 -4.1183205 -4.1025405 -4.0988975 -4.1187005 -4.1388206 -4.1446095 -4.14071 -4.1262059 -4.1224246][-4.1894827 -4.1513782 -4.1176209 -4.0913787 -4.0742388 -4.0658364 -4.0523653 -4.0291271 -4.0386028 -4.0814486 -4.1179752 -4.1343665 -4.1397748 -4.1284914 -4.12481][-4.1886578 -4.1446247 -4.0996766 -4.0615277 -4.0301661 -4.0034232 -3.966502 -3.9300218 -3.9618618 -4.0392604 -4.0954437 -4.1223416 -4.1373754 -4.1298213 -4.1245446][-4.1855674 -4.1382384 -4.0852814 -4.037107 -3.9917338 -3.9412315 -3.8732462 -3.8196447 -3.879874 -3.9939287 -4.0699382 -4.1071267 -4.1289978 -4.1225567 -4.1176777][-4.1810756 -4.1323419 -4.0794878 -4.0334682 -3.9863522 -3.9253345 -3.8435135 -3.7874694 -3.8629551 -3.98626 -4.0620928 -4.0965128 -4.113142 -4.1057315 -4.1037207][-4.1834807 -4.1393862 -4.0960803 -4.0616527 -4.0261545 -3.9785514 -3.9192834 -3.8887854 -3.9537344 -4.0454993 -4.0940752 -4.1106248 -4.1168375 -4.1106467 -4.1123981][-4.2044296 -4.17123 -4.1416106 -4.1199713 -4.0956492 -4.06434 -4.0328221 -4.0264482 -4.0744681 -4.1308136 -4.1531367 -4.1537108 -4.1490221 -4.1397829 -4.1404839][-4.23462 -4.2134423 -4.1948833 -4.1813445 -4.1652479 -4.1465545 -4.1353569 -4.1432862 -4.1762304 -4.208765 -4.2169981 -4.210288 -4.1981707 -4.1841187 -4.1818919][-4.2631979 -4.2517357 -4.2405419 -4.2318239 -4.2222819 -4.21361 -4.2141786 -4.2268581 -4.2481451 -4.2676163 -4.2704954 -4.2624788 -4.2466493 -4.2304893 -4.2258306][-4.2858396 -4.2826581 -4.2763963 -4.270997 -4.2676635 -4.2668114 -4.2714515 -4.2833767 -4.2961154 -4.3071856 -4.3074436 -4.3012218 -4.2852116 -4.2694287 -4.26133][-4.3021693 -4.3038225 -4.3004193 -4.2964787 -4.2961774 -4.2986465 -4.3044782 -4.3144989 -4.3214865 -4.3271241 -4.3264318 -4.3224835 -4.311698 -4.3012142 -4.2928143][-4.3065233 -4.3095307 -4.3075833 -4.3041868 -4.3042703 -4.3066587 -4.3110104 -4.3182111 -4.3228602 -4.3260188 -4.32545 -4.3236265 -4.3182425 -4.3138433 -4.3092747]]...]
INFO - root - 2017-12-07 17:16:23.208126: step 34010, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 0.514 sec/batch; 42h:36m:15s remains)
INFO - root - 2017-12-07 17:16:29.980173: step 34020, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 58h:33m:06s remains)
INFO - root - 2017-12-07 17:16:36.792910: step 34030, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 57h:16m:21s remains)
INFO - root - 2017-12-07 17:16:43.457101: step 34040, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 52h:25m:11s remains)
INFO - root - 2017-12-07 17:16:50.306695: step 34050, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 54h:30m:49s remains)
INFO - root - 2017-12-07 17:16:57.091091: step 34060, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 57h:35m:38s remains)
INFO - root - 2017-12-07 17:17:03.924317: step 34070, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 56h:52m:59s remains)
INFO - root - 2017-12-07 17:17:10.757716: step 34080, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 57h:25m:38s remains)
INFO - root - 2017-12-07 17:17:17.558888: step 34090, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 52h:54m:20s remains)
INFO - root - 2017-12-07 17:17:24.335177: step 34100, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 54h:13m:23s remains)
2017-12-07 17:17:25.048512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3317809 -4.3316884 -4.3299537 -4.3245993 -4.3162026 -4.3068686 -4.3023376 -4.3024778 -4.3067665 -4.3138475 -4.3193169 -4.3213391 -4.3206921 -4.3187666 -4.3147745][-4.3389969 -4.33864 -4.336412 -4.3309879 -4.3235588 -4.3168468 -4.3143935 -4.3153944 -4.3209944 -4.330162 -4.3374963 -4.3394051 -4.33668 -4.3313136 -4.3215551][-4.3382864 -4.336606 -4.3320808 -4.3225293 -4.3108773 -4.3024268 -4.2973638 -4.2957768 -4.3026862 -4.3173909 -4.3312912 -4.3381844 -4.3375921 -4.3300653 -4.3116765][-4.32711 -4.3228707 -4.3134418 -4.2953773 -4.2734847 -4.2569828 -4.244102 -4.2387185 -4.2486005 -4.2721515 -4.2963572 -4.3119226 -4.3157125 -4.3047938 -4.2729888][-4.3055873 -4.2967505 -4.2759867 -4.2390132 -4.1953993 -4.1594219 -4.1335645 -4.1288433 -4.1517305 -4.1958375 -4.2391152 -4.2695608 -4.2807531 -4.2664881 -4.2190261][-4.277554 -4.2583017 -4.2173891 -4.1509013 -4.0743303 -4.007606 -3.9635372 -3.96622 -4.0162158 -4.0924726 -4.162344 -4.2125778 -4.2371631 -4.2271776 -4.1774182][-4.2570271 -4.2284241 -4.1729116 -4.0833664 -3.9792726 -3.8821249 -3.8169916 -3.8273432 -3.9058785 -4.0124612 -4.1028323 -4.169054 -4.2068672 -4.2084994 -4.1721735][-4.2500491 -4.221477 -4.168541 -4.08273 -3.9828346 -3.8872783 -3.8252339 -3.8402781 -3.9209409 -4.0251884 -4.1083355 -4.1697521 -4.2079558 -4.2153234 -4.1908889][-4.2508574 -4.23016 -4.1930933 -4.1340542 -4.067204 -4.0047369 -3.9714139 -3.9917378 -4.0520706 -4.1236043 -4.1767287 -4.2150741 -4.2382307 -4.2388253 -4.2148395][-4.2633576 -4.2548928 -4.2360129 -4.2047648 -4.1701937 -4.1381888 -4.1255727 -4.144949 -4.1838264 -4.225256 -4.2533369 -4.2697086 -4.2758493 -4.2654943 -4.2373843][-4.2823424 -4.2826667 -4.2781148 -4.2667007 -4.2529221 -4.2384338 -4.233211 -4.2441587 -4.265029 -4.2875528 -4.3038664 -4.3124127 -4.312798 -4.3005786 -4.2756438][-4.3012147 -4.3059115 -4.3105068 -4.311676 -4.3095107 -4.3046317 -4.30193 -4.3059025 -4.3152742 -4.3267918 -4.3361273 -4.3404508 -4.3392305 -4.3288312 -4.3106956][-4.3101692 -4.3146434 -4.3229537 -4.3307991 -4.3353338 -4.3365946 -4.3364043 -4.3371844 -4.3399763 -4.3440967 -4.3470335 -4.3474526 -4.345777 -4.338326 -4.3279915][-4.3074384 -4.3090582 -4.3171082 -4.3261495 -4.332406 -4.3352442 -4.3359265 -4.3355904 -4.3358822 -4.3368592 -4.3371563 -4.3363218 -4.335361 -4.3322644 -4.3309407][-4.2938414 -4.2899466 -4.2966394 -4.307229 -4.3168674 -4.3232689 -4.3265853 -4.3274851 -4.3276615 -4.32769 -4.3263674 -4.3234553 -4.321106 -4.3200502 -4.3237853]]...]
INFO - root - 2017-12-07 17:17:31.658790: step 34110, loss = 2.07, batch loss = 2.01 (14.8 examples/sec; 0.539 sec/batch; 44h:40m:22s remains)
INFO - root - 2017-12-07 17:17:38.485521: step 34120, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 53h:06m:40s remains)
INFO - root - 2017-12-07 17:17:45.295468: step 34130, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 55h:56m:27s remains)
INFO - root - 2017-12-07 17:17:52.077170: step 34140, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 57h:46m:14s remains)
INFO - root - 2017-12-07 17:17:58.949123: step 34150, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 59h:33m:09s remains)
INFO - root - 2017-12-07 17:18:05.732712: step 34160, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.666 sec/batch; 55h:10m:38s remains)
INFO - root - 2017-12-07 17:18:12.423026: step 34170, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 53h:18m:39s remains)
INFO - root - 2017-12-07 17:18:19.215657: step 34180, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 55h:46m:23s remains)
INFO - root - 2017-12-07 17:18:26.032385: step 34190, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 55h:34m:55s remains)
INFO - root - 2017-12-07 17:18:32.844311: step 34200, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 57h:32m:04s remains)
2017-12-07 17:18:33.510374: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3459721 -4.3532071 -4.3570948 -4.3560395 -4.3522859 -4.3484545 -4.3488965 -4.3526654 -4.3553538 -4.3549104 -4.35159 -4.3465014 -4.3436136 -4.3432288 -4.3447256][-4.3271818 -4.3325734 -4.3361597 -4.335978 -4.3318453 -4.3260674 -4.327064 -4.3334284 -4.33933 -4.3413973 -4.3366423 -4.328074 -4.3203692 -4.3170176 -4.3196645][-4.2916059 -4.2911735 -4.2936745 -4.2936788 -4.2890792 -4.2829237 -4.2862759 -4.2967548 -4.305563 -4.3083663 -4.3022013 -4.2904243 -4.2776418 -4.2714415 -4.2767529][-4.2448611 -4.2398233 -4.2420387 -4.2405596 -4.23411 -4.2278843 -4.2325287 -4.2494707 -4.2636108 -4.2684088 -4.2605968 -4.2444572 -4.2282724 -4.2210021 -4.2288461][-4.2072439 -4.2038412 -4.2044792 -4.1986966 -4.1895862 -4.1844292 -4.1903324 -4.2080779 -4.22168 -4.2245836 -4.2141533 -4.1966629 -4.1846328 -4.1818986 -4.1943583][-4.1885457 -4.1855884 -4.1819062 -4.1667104 -4.1505327 -4.1436586 -4.150094 -4.1683927 -4.178484 -4.1775146 -4.166862 -4.1520038 -4.1470752 -4.1520262 -4.1729155][-4.1769733 -4.1674533 -4.1547327 -4.1283345 -4.0972805 -4.0791359 -4.0799665 -4.1010404 -4.1167579 -4.1231647 -4.1204457 -4.1097016 -4.1081219 -4.1207151 -4.1524029][-4.1556754 -4.1352539 -4.10922 -4.0631647 -4.0079961 -3.9680572 -3.9562984 -3.9809117 -4.009964 -4.0335956 -4.0518947 -4.0578794 -4.0681462 -4.0931878 -4.1368036][-4.1269407 -4.0971661 -4.0616269 -4.0014353 -3.9225841 -3.8544357 -3.8229368 -3.8497634 -3.9019308 -3.9578292 -4.0071883 -4.0354886 -4.0569534 -4.0866661 -4.1327643][-4.0841975 -4.0619459 -4.0404596 -3.9958391 -3.9236696 -3.8545475 -3.8173168 -3.8403888 -3.9007883 -3.9652288 -4.014648 -4.0358 -4.0450945 -4.0641794 -4.1068091][-4.026639 -4.0298977 -4.0415416 -4.0373635 -4.0024924 -3.9609733 -3.9359415 -3.9477 -3.98389 -4.0205288 -4.038434 -4.0293865 -4.013072 -4.0146637 -4.0523915][-3.9542308 -3.9848475 -4.0339627 -4.0749755 -4.0859451 -4.0805645 -4.0725808 -4.07434 -4.0823946 -4.0828505 -4.062489 -4.0162768 -3.9645879 -3.9452834 -3.9812605][-3.9220161 -3.9706397 -4.0471349 -4.1182895 -4.1588855 -4.1772494 -4.1803746 -4.1771111 -4.1660042 -4.1385851 -4.0885563 -4.0148869 -3.9379299 -3.9062698 -3.9444981][-3.9784327 -4.029716 -4.1076841 -4.1822219 -4.2296591 -4.25352 -4.2564077 -4.2461147 -4.2242346 -4.1839213 -4.1253796 -4.0528269 -3.9836407 -3.9601121 -3.9987936][-4.1113276 -4.1489282 -4.2040615 -4.2563143 -4.2879529 -4.301549 -4.29625 -4.2829185 -4.2626591 -4.2311997 -4.1889787 -4.1426792 -4.1028862 -4.095295 -4.1278944]]...]
INFO - root - 2017-12-07 17:18:39.991620: step 34210, loss = 2.07, batch loss = 2.01 (15.9 examples/sec; 0.502 sec/batch; 41h:37m:54s remains)
INFO - root - 2017-12-07 17:18:46.840042: step 34220, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 59h:04m:27s remains)
INFO - root - 2017-12-07 17:18:53.654864: step 34230, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 60h:40m:04s remains)
INFO - root - 2017-12-07 17:19:00.554025: step 34240, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 55h:12m:58s remains)
INFO - root - 2017-12-07 17:19:07.301509: step 34250, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 52h:28m:22s remains)
INFO - root - 2017-12-07 17:19:14.097061: step 34260, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 55h:16m:04s remains)
INFO - root - 2017-12-07 17:19:20.870061: step 34270, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 60h:48m:50s remains)
INFO - root - 2017-12-07 17:19:27.666646: step 34280, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 60h:17m:07s remains)
INFO - root - 2017-12-07 17:19:34.427123: step 34290, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 57h:16m:01s remains)
INFO - root - 2017-12-07 17:19:41.178541: step 34300, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 52h:23m:36s remains)
2017-12-07 17:19:41.899126: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2920885 -4.297328 -4.2908139 -4.2820959 -4.2768359 -4.2810307 -4.29312 -4.3017831 -4.2992339 -4.2879648 -4.2784333 -4.276938 -4.2785983 -4.2810354 -4.2820058][-4.2911935 -4.2851772 -4.2657404 -4.2492356 -4.2417965 -4.2482195 -4.2673993 -4.2848206 -4.2927637 -4.2880406 -4.2797422 -4.2783666 -4.28209 -4.2852445 -4.2876563][-4.2690306 -4.248301 -4.2135973 -4.1866422 -4.1749549 -4.182375 -4.2092781 -4.2415123 -4.2684865 -4.2780881 -4.2760234 -4.2753282 -4.2795095 -4.283061 -4.2877951][-4.224936 -4.1939669 -4.151866 -4.1207213 -4.1113682 -4.1207256 -4.1497936 -4.1914206 -4.2316551 -4.2551737 -4.2623768 -4.2653275 -4.2722034 -4.279552 -4.2882619][-4.1806765 -4.1537862 -4.1236415 -4.1049156 -4.1029191 -4.1111026 -4.1285658 -4.1603551 -4.196393 -4.2235446 -4.2375956 -4.2484546 -4.2628303 -4.2766161 -4.2879844][-4.1575885 -4.142971 -4.1322651 -4.1267052 -4.1252117 -4.1225595 -4.1208305 -4.134439 -4.1589308 -4.1810603 -4.2015686 -4.2265663 -4.2522588 -4.2714944 -4.2816057][-4.1536989 -4.1469727 -4.1475816 -4.1461816 -4.1363678 -4.1201839 -4.1054745 -4.1089425 -4.1249 -4.1391478 -4.1614666 -4.1954651 -4.2289839 -4.2495632 -4.2553353][-4.1683788 -4.1605372 -4.1617427 -4.159708 -4.141829 -4.1154962 -4.0934606 -4.0910783 -4.0979261 -4.1058 -4.1260614 -4.1577024 -4.1866927 -4.2011671 -4.19865][-4.1935883 -4.1762562 -4.1737118 -4.1723328 -4.1537433 -4.1248531 -4.1012454 -4.0929112 -4.0897403 -4.0886536 -4.1021113 -4.1205392 -4.1301055 -4.13004 -4.1179285][-4.2189126 -4.1926312 -4.1877503 -4.1882319 -4.1764889 -4.1524067 -4.1329546 -4.1206288 -4.1062908 -4.0917015 -4.0881534 -4.0836124 -4.0695758 -4.0564213 -4.0452862][-4.2359776 -4.2034087 -4.1977706 -4.202239 -4.1991639 -4.1846232 -4.1673374 -4.149981 -4.1245613 -4.0964837 -4.0746593 -4.0479827 -4.0202489 -4.0096097 -4.0153575][-4.2460203 -4.21011 -4.2023664 -4.2098637 -4.2167044 -4.2129006 -4.1983871 -4.1786408 -4.1478295 -4.1146722 -4.0871162 -4.0597582 -4.0408192 -4.0424247 -4.0575976][-4.2565503 -4.22412 -4.2169156 -4.2272921 -4.2401347 -4.2431211 -4.2319574 -4.2138529 -4.1870422 -4.1573792 -4.133379 -4.1159444 -4.1089091 -4.1163306 -4.1274905][-4.266777 -4.2411423 -4.2353349 -4.2439737 -4.2542753 -4.2563729 -4.2471185 -4.2367935 -4.2222948 -4.2017837 -4.1823654 -4.1694789 -4.1685343 -4.1781764 -4.1884713][-4.2698054 -4.2507896 -4.2449546 -4.2467942 -4.2482476 -4.2457466 -4.2402077 -4.240406 -4.2430968 -4.2383986 -4.228652 -4.220397 -4.2221756 -4.2324939 -4.243597]]...]
INFO - root - 2017-12-07 17:19:48.424350: step 34310, loss = 2.03, batch loss = 1.98 (15.7 examples/sec; 0.511 sec/batch; 42h:18m:08s remains)
INFO - root - 2017-12-07 17:19:55.219776: step 34320, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 57h:34m:54s remains)
INFO - root - 2017-12-07 17:20:02.103037: step 34330, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 55h:22m:24s remains)
INFO - root - 2017-12-07 17:20:08.916781: step 34340, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 53h:16m:47s remains)
INFO - root - 2017-12-07 17:20:15.554843: step 34350, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 56h:27m:20s remains)
INFO - root - 2017-12-07 17:20:22.403224: step 34360, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 60h:02m:20s remains)
INFO - root - 2017-12-07 17:20:29.318814: step 34370, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 59h:59m:26s remains)
INFO - root - 2017-12-07 17:20:36.140036: step 34380, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 53h:11m:17s remains)
INFO - root - 2017-12-07 17:20:42.956590: step 34390, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.708 sec/batch; 58h:39m:01s remains)
INFO - root - 2017-12-07 17:20:49.855824: step 34400, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.750 sec/batch; 62h:07m:14s remains)
2017-12-07 17:20:50.603423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2355022 -4.2299747 -4.2292652 -4.2353787 -4.2437205 -4.2478547 -4.2509665 -4.2570243 -4.2617507 -4.2624445 -4.2552609 -4.2424712 -4.2265682 -4.2039614 -4.1793928][-4.2411427 -4.2389288 -4.2363353 -4.2349653 -4.2378941 -4.2397852 -4.2396245 -4.2471743 -4.2565265 -4.2658172 -4.2694068 -4.2658248 -4.2569246 -4.2378125 -4.2097492][-4.2401271 -4.2400651 -4.236239 -4.2320347 -4.2333345 -4.233521 -4.2273703 -4.2276754 -4.2358265 -4.2501531 -4.2586541 -4.261549 -4.2597828 -4.2477007 -4.2234869][-4.2218943 -4.2308106 -4.2318883 -4.2272129 -4.2251072 -4.2189875 -4.2029443 -4.1923828 -4.1960869 -4.2120347 -4.2274966 -4.2387953 -4.2438807 -4.2367105 -4.220058][-4.2056241 -4.221036 -4.2279038 -4.2231646 -4.2122693 -4.1940861 -4.1647844 -4.1434727 -4.1430011 -4.1600375 -4.1836519 -4.2049556 -4.2171545 -4.2135677 -4.2007442][-4.188293 -4.2005973 -4.2071891 -4.1974521 -4.17292 -4.1420174 -4.1071506 -4.0875692 -4.0957837 -4.1187758 -4.1506524 -4.1824803 -4.2013283 -4.2005877 -4.1866941][-4.1733828 -4.1755886 -4.1723852 -4.14882 -4.1074324 -4.0672293 -4.0385137 -4.0380983 -4.0675964 -4.1010122 -4.1359129 -4.16902 -4.1904559 -4.1903996 -4.1751552][-4.1687975 -4.1586151 -4.1432824 -4.1083131 -4.0588436 -4.0189528 -4.0039725 -4.0271158 -4.0779972 -4.1169939 -4.141129 -4.156467 -4.1636696 -4.159205 -4.1476188][-4.17817 -4.1617279 -4.1385469 -4.0974536 -4.0457778 -4.0090351 -4.0039058 -4.0455 -4.1095867 -4.1488833 -4.1571813 -4.1466851 -4.1293316 -4.1100917 -4.101368][-4.1943049 -4.1759391 -4.1489162 -4.1076045 -4.0625305 -4.0322728 -4.0299416 -4.0703244 -4.1292877 -4.1605916 -4.1549888 -4.1262684 -4.0911427 -4.0614285 -4.0538268][-4.208674 -4.1906476 -4.1629543 -4.1267595 -4.092628 -4.0688343 -4.0628886 -4.0879183 -4.1259956 -4.1422744 -4.1266632 -4.0934114 -4.0595608 -4.0301957 -4.0222006][-4.204196 -4.188755 -4.1668367 -4.1450114 -4.1288309 -4.1166506 -4.1103606 -4.1196728 -4.1356168 -4.1354485 -4.1143723 -4.0858917 -4.0657434 -4.0474982 -4.0355573][-4.196559 -4.1823297 -4.167985 -4.16023 -4.1597061 -4.1608753 -4.1610765 -4.1637983 -4.1661057 -4.1560726 -4.1335554 -4.1098986 -4.1010771 -4.0934563 -4.0813808][-4.2059665 -4.1937728 -4.1837139 -4.1821656 -4.1893268 -4.1971774 -4.2001863 -4.1991234 -4.1945963 -4.1850858 -4.1692648 -4.1524487 -4.150054 -4.1489749 -4.1374865][-4.2240496 -4.2155151 -4.2097816 -4.2102556 -4.2179551 -4.22439 -4.2238269 -4.2168384 -4.2104435 -4.2080417 -4.202363 -4.1916084 -4.1887231 -4.1858878 -4.1688008]]...]
INFO - root - 2017-12-07 17:20:57.118795: step 34410, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.478 sec/batch; 39h:33m:29s remains)
INFO - root - 2017-12-07 17:21:04.028219: step 34420, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 60h:22m:16s remains)
INFO - root - 2017-12-07 17:21:10.779973: step 34430, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 55h:34m:14s remains)
INFO - root - 2017-12-07 17:21:17.724452: step 34440, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 57h:35m:54s remains)
INFO - root - 2017-12-07 17:21:24.495871: step 34450, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 51h:50m:01s remains)
INFO - root - 2017-12-07 17:21:31.339505: step 34460, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.681 sec/batch; 56h:23m:42s remains)
INFO - root - 2017-12-07 17:21:38.241670: step 34470, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 57h:36m:16s remains)
INFO - root - 2017-12-07 17:21:45.084857: step 34480, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 55h:58m:01s remains)
INFO - root - 2017-12-07 17:21:51.815630: step 34490, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 53h:34m:33s remains)
INFO - root - 2017-12-07 17:21:58.645518: step 34500, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 56h:40m:22s remains)
2017-12-07 17:21:59.384026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2304783 -4.2176743 -4.2065978 -4.1982136 -4.1915965 -4.1931653 -4.1988668 -4.2058034 -4.2007055 -4.1770906 -4.1477704 -4.1265507 -4.1310973 -4.1649122 -4.20457][-4.2493968 -4.2364936 -4.2275186 -4.2231879 -4.2210116 -4.22559 -4.2317386 -4.2365088 -4.2286649 -4.2007461 -4.166358 -4.1420341 -4.1487703 -4.1879 -4.2314844][-4.2699184 -4.2612519 -4.2555418 -4.25441 -4.2548094 -4.26022 -4.2656794 -4.2687726 -4.2618642 -4.2384486 -4.2093964 -4.1874609 -4.1921825 -4.22243 -4.25643][-4.2813621 -4.2774396 -4.2754593 -4.2775636 -4.2790694 -4.2828293 -4.2857828 -4.2880592 -4.2868786 -4.2743769 -4.2576404 -4.2428966 -4.2427611 -4.2571006 -4.2754331][-4.285224 -4.2841015 -4.2840543 -4.286768 -4.2853942 -4.2815495 -4.2759557 -4.2755752 -4.2826319 -4.2864351 -4.2883639 -4.2861733 -4.2847972 -4.2873125 -4.2910056][-4.28052 -4.2790728 -4.2773695 -4.2729859 -4.2585683 -4.2366023 -4.2141204 -4.2088528 -4.2286978 -4.25759 -4.2865739 -4.3031273 -4.3063049 -4.3025255 -4.296113][-4.2650418 -4.2588773 -4.2493806 -4.2298465 -4.1931834 -4.1460233 -4.1036825 -4.0961637 -4.1335111 -4.1925607 -4.2515731 -4.2882137 -4.2992935 -4.294487 -4.2848015][-4.23975 -4.22446 -4.2020841 -4.162581 -4.10194 -4.0320435 -3.975728 -3.9734037 -4.035543 -4.1259246 -4.2095289 -4.2606473 -4.277895 -4.2759848 -4.2688355][-4.2090893 -4.1862226 -4.1541176 -4.1030097 -4.0330167 -3.9592254 -3.906359 -3.9178262 -3.9984431 -4.1033659 -4.1946912 -4.2484641 -4.2667546 -4.2668529 -4.2623653][-4.1853342 -4.1647763 -4.137959 -4.0976663 -4.0454612 -3.9942482 -3.9618549 -3.9795258 -4.0498281 -4.1386623 -4.2141747 -4.2568378 -4.2696805 -4.2682848 -4.2644291][-4.181242 -4.1749754 -4.1653409 -4.1460266 -4.1163836 -4.0883765 -4.0731654 -4.0890088 -4.1377759 -4.1985035 -4.249186 -4.2744083 -4.2784834 -4.2744918 -4.2707171][-4.1949635 -4.2016177 -4.206162 -4.2024555 -4.1890388 -4.1780424 -4.1743059 -4.1877594 -4.2173815 -4.2516713 -4.2778454 -4.28556 -4.2804804 -4.2742376 -4.2715592][-4.2183208 -4.228579 -4.2387657 -4.2431059 -4.2394762 -4.2391582 -4.2439237 -4.255229 -4.2699637 -4.2820892 -4.2866025 -4.2792196 -4.268795 -4.2633729 -4.2635913][-4.2434273 -4.2495189 -4.2564931 -4.2618461 -4.2626252 -4.2684317 -4.2775264 -4.2867217 -4.2917967 -4.2897892 -4.2812614 -4.2664967 -4.2551312 -4.2521033 -4.2553716][-4.2646775 -4.2639713 -4.2649364 -4.268249 -4.2713304 -4.2792926 -4.2877479 -4.2935534 -4.293602 -4.2864 -4.2751889 -4.2614369 -4.2527685 -4.2524071 -4.2576747]]...]
INFO - root - 2017-12-07 17:22:05.831729: step 34510, loss = 2.07, batch loss = 2.01 (16.4 examples/sec; 0.489 sec/batch; 40h:28m:52s remains)
INFO - root - 2017-12-07 17:22:12.641833: step 34520, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.640 sec/batch; 52h:58m:55s remains)
INFO - root - 2017-12-07 17:22:19.427509: step 34530, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 53h:23m:13s remains)
INFO - root - 2017-12-07 17:22:26.354393: step 34540, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 57h:33m:43s remains)
INFO - root - 2017-12-07 17:22:33.108742: step 34550, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 52h:58m:44s remains)
INFO - root - 2017-12-07 17:22:39.970309: step 34560, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 53h:17m:28s remains)
INFO - root - 2017-12-07 17:22:46.761268: step 34570, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 54h:02m:53s remains)
INFO - root - 2017-12-07 17:22:53.574745: step 34580, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 55h:10m:05s remains)
INFO - root - 2017-12-07 17:23:00.464685: step 34590, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 59h:03m:10s remains)
INFO - root - 2017-12-07 17:23:07.231991: step 34600, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 58h:03m:04s remains)
2017-12-07 17:23:07.982164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516503 -4.2649741 -4.2797403 -4.2861314 -4.2842503 -4.286191 -4.2939081 -4.3064175 -4.3248596 -4.3353086 -4.3283162 -4.3108239 -4.2934608 -4.2848582 -4.2835832][-4.2658672 -4.27435 -4.2855291 -4.2891541 -4.2869282 -4.2868752 -4.2895088 -4.2969809 -4.3114753 -4.3177071 -4.309206 -4.2963662 -4.2874432 -4.2802548 -4.2733431][-4.285912 -4.2956905 -4.30606 -4.306962 -4.3029423 -4.29471 -4.2867141 -4.2856917 -4.294035 -4.2960162 -4.2899556 -4.2849388 -4.2817564 -4.2682285 -4.2456765][-4.3124056 -4.3197494 -4.3230052 -4.3179708 -4.3084922 -4.2904291 -4.268116 -4.257288 -4.261796 -4.2639155 -4.2585244 -4.259788 -4.2598262 -4.2370768 -4.1918564][-4.3347592 -4.3371115 -4.3328118 -4.3207264 -4.3002977 -4.2656646 -4.2231641 -4.1951218 -4.1955175 -4.2022352 -4.2032051 -4.21635 -4.2224927 -4.1946106 -4.1304564][-4.3465524 -4.3459415 -4.3339128 -4.3060594 -4.2610865 -4.19562 -4.1228409 -4.0724492 -4.075727 -4.1060367 -4.1330166 -4.174386 -4.1977587 -4.1783395 -4.1165752][-4.3482294 -4.3413568 -4.3159194 -4.2636509 -4.1884847 -4.08751 -3.9757187 -3.9035983 -3.9307384 -4.0101275 -4.0809855 -4.1522236 -4.1931853 -4.1924949 -4.1582022][-4.3347282 -4.3216648 -4.2851686 -4.2180924 -4.1216455 -3.9951105 -3.8611512 -3.7942472 -3.8653259 -3.9914608 -4.09302 -4.1718774 -4.2187762 -4.2307172 -4.2201648][-4.3084435 -4.29766 -4.2633963 -4.1986132 -4.1080847 -4.0012507 -3.8977876 -3.862534 -3.9401488 -4.0563745 -4.149478 -4.2119803 -4.2448225 -4.2520804 -4.2489572][-4.2638197 -4.2579846 -4.2416425 -4.2071257 -4.1589274 -4.102325 -4.0434408 -4.0248995 -4.0715213 -4.1409726 -4.19975 -4.2364931 -4.2512207 -4.2486014 -4.2504444][-4.21508 -4.2149363 -4.2178779 -4.2168446 -4.2094288 -4.1938605 -4.16309 -4.1437731 -4.1580219 -4.1910563 -4.2251563 -4.2449207 -4.2511368 -4.2491283 -4.2570672][-4.1818147 -4.1790614 -4.1880388 -4.2023172 -4.2181439 -4.2289033 -4.2183571 -4.2047462 -4.2045135 -4.2217817 -4.2450728 -4.2593937 -4.2652216 -4.26636 -4.2769408][-4.1738248 -4.1616888 -4.1673822 -4.1860428 -4.2134662 -4.2447429 -4.2569346 -4.2555752 -4.2545543 -4.2648091 -4.2791376 -4.2885418 -4.2924023 -4.2943754 -4.3013935][-4.1922979 -4.1751909 -4.1752615 -4.1964011 -4.2323456 -4.2740264 -4.2984724 -4.305377 -4.3037348 -4.3042822 -4.307251 -4.3089719 -4.3079267 -4.3068185 -4.3105631][-4.2358294 -4.222033 -4.2193456 -4.2342796 -4.2628312 -4.2978473 -4.3234315 -4.3332815 -4.328918 -4.3199897 -4.3122282 -4.3078547 -4.3034916 -4.2988529 -4.2990189]]...]
INFO - root - 2017-12-07 17:23:14.497834: step 34610, loss = 2.05, batch loss = 1.99 (14.5 examples/sec; 0.551 sec/batch; 45h:34m:14s remains)
INFO - root - 2017-12-07 17:23:21.332578: step 34620, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 60h:07m:22s remains)
INFO - root - 2017-12-07 17:23:28.057626: step 34630, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 58h:03m:54s remains)
INFO - root - 2017-12-07 17:23:34.846228: step 34640, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 53h:50m:28s remains)
INFO - root - 2017-12-07 17:23:41.639661: step 34650, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 54h:10m:43s remains)
INFO - root - 2017-12-07 17:23:48.228812: step 34660, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 56h:42m:23s remains)
INFO - root - 2017-12-07 17:23:54.998827: step 34670, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 57h:35m:06s remains)
INFO - root - 2017-12-07 17:24:01.825134: step 34680, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.692 sec/batch; 57h:14m:29s remains)
INFO - root - 2017-12-07 17:24:08.516270: step 34690, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:24m:57s remains)
INFO - root - 2017-12-07 17:24:15.276689: step 34700, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 55h:07m:46s remains)
2017-12-07 17:24:16.037176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3333144 -4.3320045 -4.3301654 -4.328496 -4.3277497 -4.3283386 -4.3299527 -4.3314214 -4.3314514 -4.330111 -4.32915 -4.3294339 -4.3299289 -4.3299432 -4.3298268][-4.3354878 -4.3338037 -4.33112 -4.3278894 -4.3250237 -4.3230438 -4.323029 -4.3246908 -4.3258939 -4.3259549 -4.3258238 -4.3265715 -4.3276591 -4.3288321 -4.330049][-4.3363695 -4.3343382 -4.3311963 -4.3269305 -4.3219028 -4.3166122 -4.3140321 -4.3141007 -4.3147159 -4.3159013 -4.3170562 -4.3184366 -4.31976 -4.3217311 -4.3245473][-4.32937 -4.3235588 -4.3173084 -4.3115907 -4.306025 -4.2988825 -4.2933249 -4.2913508 -4.2919703 -4.2943621 -4.2975316 -4.3007555 -4.30438 -4.3083959 -4.3133459][-4.3106093 -4.2985373 -4.2864537 -4.2768316 -4.2692037 -4.2600918 -4.2505445 -4.2468014 -4.2489085 -4.25305 -4.2585444 -4.2649746 -4.2727222 -4.2814689 -4.2911263][-4.2797523 -4.25979 -4.2408948 -4.2263536 -4.2136145 -4.1964493 -4.1776881 -4.1722183 -4.1819639 -4.1935306 -4.2035289 -4.2130132 -4.2255821 -4.2412477 -4.2581749][-4.2423058 -4.2168689 -4.195158 -4.1782775 -4.1589804 -4.1260414 -4.087688 -4.0743361 -4.0941133 -4.1198335 -4.1370406 -4.1494584 -4.1646023 -4.1863256 -4.2119846][-4.2061391 -4.1775894 -4.15737 -4.1426105 -4.1201859 -4.0713816 -4.0094237 -3.9803853 -4.0065517 -4.0500021 -4.0797534 -4.0988693 -4.1179409 -4.1433063 -4.173275][-4.1921983 -4.1648221 -4.1467557 -4.1351795 -4.1139512 -4.0632486 -3.9963229 -3.9591708 -3.98157 -4.0314703 -4.071723 -4.0991688 -4.120461 -4.1426735 -4.166832][-4.2038779 -4.1845665 -4.1691017 -4.1584182 -4.142808 -4.1104026 -4.0723448 -4.0516124 -4.0634255 -4.0945597 -4.1248522 -4.1496615 -4.1664748 -4.1803737 -4.1937461][-4.2272763 -4.2187939 -4.2102871 -4.2050633 -4.19835 -4.1847825 -4.1721892 -4.1677794 -4.1737046 -4.1848111 -4.1975379 -4.2086978 -4.2139192 -4.2180047 -4.2220473][-4.2386384 -4.2392359 -4.2385144 -4.24037 -4.2403154 -4.237308 -4.2355857 -4.2350478 -4.2341051 -4.2325063 -4.2330561 -4.233849 -4.2305994 -4.2263246 -4.2240591][-4.2262368 -4.231626 -4.2358193 -4.2402859 -4.2419133 -4.2431684 -4.2440915 -4.2408309 -4.2336659 -4.2270441 -4.2252111 -4.2248197 -4.2206459 -4.215044 -4.2121062][-4.2079077 -4.2128992 -4.2171597 -4.2199717 -4.2199521 -4.2211537 -4.2238636 -4.2193751 -4.20811 -4.2006683 -4.2004361 -4.2023 -4.2033134 -4.2038183 -4.2055311][-4.2053657 -4.2060733 -4.207643 -4.207005 -4.2047229 -4.2048306 -4.2082586 -4.2037473 -4.1922469 -4.185957 -4.1863041 -4.1890888 -4.1933637 -4.2012706 -4.2096381]]...]
INFO - root - 2017-12-07 17:24:22.622709: step 34710, loss = 2.09, batch loss = 2.03 (15.5 examples/sec; 0.515 sec/batch; 42h:37m:07s remains)
INFO - root - 2017-12-07 17:24:29.406859: step 34720, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 52h:22m:59s remains)
INFO - root - 2017-12-07 17:24:36.173999: step 34730, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 57h:33m:22s remains)
INFO - root - 2017-12-07 17:24:43.024884: step 34740, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.691 sec/batch; 57h:10m:29s remains)
INFO - root - 2017-12-07 17:24:49.837559: step 34750, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 56h:29m:56s remains)
INFO - root - 2017-12-07 17:24:56.627168: step 34760, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 54h:33m:49s remains)
INFO - root - 2017-12-07 17:25:03.409199: step 34770, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 53h:51m:43s remains)
INFO - root - 2017-12-07 17:25:10.277811: step 34780, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 55h:43m:15s remains)
INFO - root - 2017-12-07 17:25:17.202574: step 34790, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 58h:38m:48s remains)
INFO - root - 2017-12-07 17:25:23.937727: step 34800, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 56h:31m:23s remains)
2017-12-07 17:25:24.627938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1491661 -4.1509743 -4.1355686 -4.1169543 -4.0987582 -4.1007547 -4.1122389 -4.1102638 -4.09941 -4.1184516 -4.1516471 -4.1739688 -4.1894469 -4.1834488 -4.1643329][-4.1164303 -4.1325788 -4.1289573 -4.1071177 -4.0803976 -4.0814052 -4.1029706 -4.1176028 -4.12256 -4.142889 -4.1673737 -4.180243 -4.1846242 -4.1647263 -4.1324492][-4.1343675 -4.1526027 -4.1523108 -4.1291733 -4.1002121 -4.0961223 -4.1144218 -4.1391811 -4.1578474 -4.182941 -4.2037129 -4.2079444 -4.1993289 -4.1696186 -4.1354036][-4.1837435 -4.1978688 -4.1936212 -4.1701975 -4.1389251 -4.12327 -4.1235962 -4.13913 -4.1627865 -4.2011566 -4.2295256 -4.2344623 -4.2212424 -4.1930609 -4.1651182][-4.2138824 -4.2254763 -4.2182527 -4.19179 -4.1521955 -4.1198664 -4.0999837 -4.103951 -4.1383495 -4.1912384 -4.2329617 -4.2475047 -4.2404947 -4.2202506 -4.19804][-4.2131205 -4.2191434 -4.2085 -4.178618 -4.1301665 -4.0745506 -4.0269322 -4.0243006 -4.0803571 -4.1602249 -4.218029 -4.2415471 -4.2433829 -4.2325487 -4.2139592][-4.188261 -4.1825671 -4.1655231 -4.1309934 -4.0715208 -3.9803438 -3.8885489 -3.8885505 -3.9884858 -4.1034589 -4.17868 -4.2154512 -4.2306666 -4.2324729 -4.2188244][-4.1645455 -4.1436491 -4.1224694 -4.0892925 -4.0199857 -3.8934693 -3.77071 -3.7878733 -3.9240403 -4.0512304 -4.1309819 -4.1805472 -4.2081118 -4.2203212 -4.2118025][-4.1690922 -4.13728 -4.1135292 -4.0929465 -4.0381165 -3.9271562 -3.8319118 -3.854867 -3.9614036 -4.057785 -4.1226892 -4.1706715 -4.2030292 -4.2179003 -4.2119145][-4.1738749 -4.13795 -4.1147585 -4.109652 -4.0798554 -4.0099368 -3.9595265 -3.9744267 -4.0355654 -4.0944262 -4.1429682 -4.1853013 -4.2142119 -4.2244077 -4.2132192][-4.1632128 -4.1239862 -4.1039882 -4.1079645 -4.0986967 -4.06367 -4.0440407 -4.054944 -4.0891967 -4.1274996 -4.1678324 -4.2041612 -4.2241483 -4.2269945 -4.2123203][-4.1666012 -4.1299915 -4.111949 -4.1151524 -4.1133904 -4.0974 -4.0929093 -4.104764 -4.1291776 -4.1579771 -4.1905146 -4.2194438 -4.2319674 -4.2293978 -4.2158976][-4.1851692 -4.1535578 -4.1352415 -4.13249 -4.1319251 -4.1264815 -4.1286626 -4.14131 -4.1606064 -4.1814151 -4.2053857 -4.2271996 -4.2344465 -4.2322478 -4.2240248][-4.2143593 -4.1899652 -4.17061 -4.1598849 -4.1571274 -4.1587343 -4.16757 -4.18175 -4.1975851 -4.2126584 -4.2273889 -4.23745 -4.2384348 -4.2377524 -4.2357693][-4.2443604 -4.2294588 -4.2147164 -4.203229 -4.2003284 -4.20403 -4.2134113 -4.2246909 -4.233562 -4.2413778 -4.2481217 -4.2511787 -4.250401 -4.2508488 -4.2519321]]...]
INFO - root - 2017-12-07 17:25:31.239226: step 34810, loss = 2.07, batch loss = 2.01 (13.6 examples/sec; 0.589 sec/batch; 48h:43m:06s remains)
INFO - root - 2017-12-07 17:25:38.036216: step 34820, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 57h:22m:59s remains)
INFO - root - 2017-12-07 17:25:44.758942: step 34830, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 55h:01m:45s remains)
INFO - root - 2017-12-07 17:25:51.495992: step 34840, loss = 2.03, batch loss = 1.97 (12.4 examples/sec; 0.644 sec/batch; 53h:16m:36s remains)
INFO - root - 2017-12-07 17:25:58.274093: step 34850, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 57h:35m:28s remains)
INFO - root - 2017-12-07 17:26:05.031826: step 34860, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 57h:59m:49s remains)
INFO - root - 2017-12-07 17:26:11.804396: step 34870, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 57h:03m:14s remains)
INFO - root - 2017-12-07 17:26:18.614204: step 34880, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 55h:53m:31s remains)
INFO - root - 2017-12-07 17:26:25.230632: step 34890, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 52h:14m:21s remains)
INFO - root - 2017-12-07 17:26:32.062199: step 34900, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.644 sec/batch; 53h:12m:37s remains)
2017-12-07 17:26:32.852367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1720209 -4.1292477 -4.0918684 -4.0840578 -4.1180358 -4.1669431 -4.1906834 -4.1811152 -4.1380844 -4.0929136 -4.0888548 -4.1290932 -4.1697917 -4.207006 -4.2493372][-4.1981964 -4.155818 -4.1100168 -4.0953016 -4.1265368 -4.1715422 -4.1934023 -4.17735 -4.1174145 -4.0563488 -4.0495825 -4.094183 -4.1437135 -4.1929541 -4.2457347][-4.2177148 -4.1747913 -4.1238627 -4.1051297 -4.13449 -4.1775637 -4.2032928 -4.1898203 -4.1279974 -4.0655041 -4.0542951 -4.0952353 -4.1507034 -4.2081161 -4.2653995][-4.2170396 -4.1717644 -4.1191297 -4.09464 -4.1157861 -4.1583815 -4.1944113 -4.1964622 -4.1515894 -4.1046028 -4.0925775 -4.124053 -4.178061 -4.2361307 -4.2913942][-4.2006636 -4.1498146 -4.0983796 -4.0680876 -4.0780706 -4.1205812 -4.1703978 -4.1953821 -4.1781425 -4.1498237 -4.1357327 -4.1520643 -4.19439 -4.2450581 -4.2952113][-4.1911511 -4.1368 -4.0843592 -4.04477 -4.0416026 -4.079917 -4.1390448 -4.1835232 -4.1895332 -4.1741624 -4.1549878 -4.1558418 -4.1822085 -4.2246771 -4.2715011][-4.1933393 -4.1380749 -4.0826721 -4.0329323 -4.01494 -4.0472112 -4.1087141 -4.1631804 -4.1841173 -4.1763163 -4.1537538 -4.144197 -4.1570754 -4.18935 -4.2336521][-4.200459 -4.1529646 -4.1022053 -4.0503736 -4.0193214 -4.0372143 -4.0920105 -4.1467438 -4.1722722 -4.1658525 -4.1399665 -4.1217241 -4.1210837 -4.1451993 -4.1915631][-4.2088389 -4.1743836 -4.1387625 -4.0972285 -4.0620866 -4.0614805 -4.0948544 -4.1370635 -4.1596036 -4.1525326 -4.1246915 -4.1021047 -4.0924487 -4.1112437 -4.1604028][-4.2101312 -4.1894875 -4.1707311 -4.1481957 -4.1215029 -4.1064014 -4.112772 -4.1338749 -4.1475406 -4.1389041 -4.1138458 -4.0930181 -4.0813208 -4.100913 -4.149621][-4.190906 -4.1832719 -4.1812711 -4.1828079 -4.1714358 -4.1489229 -4.1334453 -4.1374083 -4.1456437 -4.1389155 -4.1229339 -4.1116719 -4.10592 -4.1253443 -4.1633053][-4.1537471 -4.1579013 -4.1711793 -4.1906552 -4.194313 -4.1716518 -4.1472387 -4.1465569 -4.1586881 -4.1626806 -4.1616178 -4.1624618 -4.162159 -4.1750665 -4.1940312][-4.1178784 -4.1316772 -4.1585855 -4.1897349 -4.2030249 -4.1848927 -4.1595879 -4.1577849 -4.1763306 -4.1915612 -4.2051463 -4.2162881 -4.2191753 -4.2235703 -4.2234287][-4.1003366 -4.1196795 -4.1536303 -4.1896482 -4.2047534 -4.18836 -4.1654167 -4.1667967 -4.1927452 -4.2170911 -4.2387533 -4.2553887 -4.2593966 -4.2558689 -4.2419734][-4.1094222 -4.1265693 -4.1574731 -4.1888423 -4.1971493 -4.1781068 -4.1596651 -4.1693 -4.2041278 -4.2332745 -4.2557778 -4.2720294 -4.2734385 -4.2633891 -4.2427335]]...]
INFO - root - 2017-12-07 17:26:39.489122: step 34910, loss = 2.07, batch loss = 2.01 (14.7 examples/sec; 0.545 sec/batch; 45h:01m:45s remains)
INFO - root - 2017-12-07 17:26:46.224796: step 34920, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 52h:15m:28s remains)
INFO - root - 2017-12-07 17:26:52.969935: step 34930, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 52h:58m:23s remains)
INFO - root - 2017-12-07 17:26:59.753873: step 34940, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.732 sec/batch; 60h:32m:34s remains)
INFO - root - 2017-12-07 17:27:06.657372: step 34950, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.711 sec/batch; 58h:47m:29s remains)
INFO - root - 2017-12-07 17:27:13.524849: step 34960, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.756 sec/batch; 62h:30m:36s remains)
INFO - root - 2017-12-07 17:27:20.166301: step 34970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 55h:46m:03s remains)
INFO - root - 2017-12-07 17:27:27.018842: step 34980, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 56h:07m:49s remains)
INFO - root - 2017-12-07 17:27:33.834521: step 34990, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 58h:14m:44s remains)
INFO - root - 2017-12-07 17:27:40.664236: step 35000, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 54h:13m:42s remains)
2017-12-07 17:27:41.401571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3559828 -4.3601465 -4.3628955 -4.3628106 -4.3599892 -4.3564987 -4.3529739 -4.3470883 -4.3401656 -4.3365378 -4.3417153 -4.35068 -4.3571506 -4.3552165 -4.3425822][-4.3557916 -4.3595734 -4.3609805 -4.3586931 -4.3535509 -4.3485994 -4.3418751 -4.3307462 -4.3186264 -4.3111529 -4.3175564 -4.3310857 -4.34249 -4.343224 -4.3289361][-4.3564968 -4.3580103 -4.3573618 -4.3535419 -4.345612 -4.3360448 -4.3199034 -4.2984877 -4.279655 -4.270381 -4.2790828 -4.2984576 -4.3161254 -4.320015 -4.3032489][-4.3575664 -4.3575258 -4.3556023 -4.3485546 -4.3337946 -4.3140769 -4.2827911 -4.2464528 -4.2209263 -4.2123876 -4.2263741 -4.2540622 -4.2795048 -4.2884359 -4.2736549][-4.3609567 -4.3609657 -4.355804 -4.3404331 -4.3131666 -4.2783518 -4.230166 -4.1804042 -4.149806 -4.1515574 -4.1810703 -4.2200847 -4.2519813 -4.2687507 -4.2611985][-4.3571138 -4.35767 -4.349534 -4.3273377 -4.285996 -4.2312641 -4.1600952 -4.0908475 -4.0562339 -4.0786548 -4.1342053 -4.1915693 -4.235909 -4.26269 -4.2662239][-4.3447876 -4.3463435 -4.3379722 -4.3091269 -4.2523503 -4.1742997 -4.0754657 -3.9818361 -3.953089 -4.0088444 -4.094552 -4.1742177 -4.2311769 -4.2639909 -4.272584][-4.3214245 -4.3259954 -4.3173709 -4.2801661 -4.2078104 -4.1087608 -3.9825191 -3.8699188 -3.8649566 -3.9613633 -4.073236 -4.1683474 -4.2287097 -4.2572393 -4.2638788][-4.2866254 -4.2958884 -4.2885752 -4.2467966 -4.1643643 -4.053936 -3.9176135 -3.813643 -3.8478093 -3.9750931 -4.0980458 -4.1839027 -4.2243032 -4.2381849 -4.2423382][-4.2560821 -4.2712216 -4.2697716 -4.2296753 -4.1476755 -4.0486164 -3.9414916 -3.8757093 -3.9318657 -4.0570827 -4.1606579 -4.2137628 -4.2204661 -4.2170215 -4.2218065][-4.2348452 -4.2610531 -4.2705207 -4.2388787 -4.1694932 -4.0991268 -4.0408921 -4.0150323 -4.0662169 -4.1537032 -4.2134695 -4.2303362 -4.2150927 -4.2061224 -4.219358][-4.2261381 -4.26426 -4.283371 -4.2626147 -4.2107773 -4.1692491 -4.1476531 -4.145081 -4.1807122 -4.22274 -4.2387609 -4.2292504 -4.2131824 -4.2142072 -4.2396073][-4.2243204 -4.26917 -4.288455 -4.2758512 -4.2436786 -4.2273126 -4.23024 -4.2370014 -4.2505903 -4.2532568 -4.23725 -4.2188225 -4.2158384 -4.2348742 -4.2714839][-4.22546 -4.2606258 -4.2754755 -4.2707725 -4.258225 -4.2635708 -4.2783928 -4.2799153 -4.2694817 -4.2458072 -4.2190442 -4.21007 -4.2261086 -4.2610345 -4.3013954][-4.2347965 -4.2467937 -4.2548294 -4.2573695 -4.2605796 -4.2765713 -4.2895679 -4.2775846 -4.2471318 -4.21663 -4.2020311 -4.2138982 -4.2475328 -4.2888556 -4.3232656]]...]
INFO - root - 2017-12-07 17:27:47.970228: step 35010, loss = 2.07, batch loss = 2.01 (15.4 examples/sec; 0.519 sec/batch; 42h:52m:49s remains)
INFO - root - 2017-12-07 17:27:54.864343: step 35020, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 54h:14m:38s remains)
INFO - root - 2017-12-07 17:28:01.589750: step 35030, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.628 sec/batch; 51h:55m:52s remains)
INFO - root - 2017-12-07 17:28:08.411280: step 35040, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 56h:28m:02s remains)
INFO - root - 2017-12-07 17:28:15.218123: step 35050, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 58h:19m:11s remains)
INFO - root - 2017-12-07 17:28:21.969399: step 35060, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 57h:25m:29s remains)
INFO - root - 2017-12-07 17:28:28.754554: step 35070, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.668 sec/batch; 55h:12m:20s remains)
INFO - root - 2017-12-07 17:28:35.533460: step 35080, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 57h:27m:05s remains)
INFO - root - 2017-12-07 17:28:42.396061: step 35090, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 56h:57m:52s remains)
INFO - root - 2017-12-07 17:28:49.214288: step 35100, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 57h:32m:49s remains)
2017-12-07 17:28:49.895104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2731915 -4.2514009 -4.2289891 -4.2167878 -4.2127285 -4.2103949 -4.2116666 -4.2172394 -4.22491 -4.2372766 -4.25678 -4.2760987 -4.2830281 -4.2806549 -4.2758021][-4.2781849 -4.2603087 -4.2393022 -4.2287469 -4.2299509 -4.2356009 -4.2404928 -4.2433882 -4.2468967 -4.2576027 -4.2779341 -4.2984991 -4.3032455 -4.2974291 -4.2894912][-4.2698188 -4.2541041 -4.2366352 -4.2305694 -4.2381516 -4.2490931 -4.2553368 -4.2554369 -4.2527652 -4.2604074 -4.2823024 -4.3039217 -4.3078456 -4.3001842 -4.2914362][-4.2477579 -4.2323184 -4.2173829 -4.2147579 -4.2229018 -4.2348924 -4.2450747 -4.2481484 -4.2421889 -4.2486033 -4.274075 -4.2991829 -4.3046312 -4.2972531 -4.2907267][-4.2194142 -4.201076 -4.1829348 -4.1770906 -4.1799803 -4.1901093 -4.206912 -4.2168393 -4.2122164 -4.2175803 -4.247613 -4.2792916 -4.2898374 -4.2862425 -4.2844524][-4.1913705 -4.1711125 -4.1490078 -4.1376 -4.1336107 -4.1413198 -4.163703 -4.1776743 -4.1714258 -4.1733685 -4.2057643 -4.2454491 -4.2629209 -4.2657437 -4.2707996][-4.1765442 -4.1579466 -4.1348009 -4.1204033 -4.1086197 -4.108634 -4.1297541 -4.1441431 -4.1358857 -4.13216 -4.1645484 -4.2104912 -4.2375846 -4.247818 -4.2573056][-4.1841974 -4.1701174 -4.1482644 -4.1337929 -4.1176076 -4.1111193 -4.1283979 -4.139751 -4.1310544 -4.1218367 -4.1509008 -4.1997604 -4.2328377 -4.2478819 -4.2575512][-4.2082791 -4.2024302 -4.1875181 -4.177495 -4.1625795 -4.1529603 -4.1642704 -4.1722136 -4.1662164 -4.1585774 -4.1858134 -4.2296114 -4.2565064 -4.266283 -4.2702384][-4.23062 -4.2330027 -4.2304344 -4.2302866 -4.2229819 -4.2145991 -4.2194414 -4.2216415 -4.2183051 -4.2158513 -4.2405167 -4.2749248 -4.2923608 -4.2944012 -4.2921767][-4.2478542 -4.2562046 -4.2612567 -4.2665153 -4.2659044 -4.2616282 -4.2623892 -4.2612748 -4.2604914 -4.2648726 -4.28751 -4.3111982 -4.3188834 -4.3155112 -4.3113985][-4.2567768 -4.268332 -4.2757859 -4.2815714 -4.2823763 -4.2798996 -4.2773418 -4.2728963 -4.2735462 -4.2807679 -4.2997212 -4.3159041 -4.320785 -4.3198981 -4.3184896][-4.2486143 -4.2621288 -4.2711778 -4.2768154 -4.2766461 -4.2732563 -4.2664843 -4.2600884 -4.2639642 -4.2745013 -4.2912836 -4.3042426 -4.3097734 -4.3132572 -4.3155947][-4.2402949 -4.2510881 -4.2577095 -4.2606058 -4.2584848 -4.2521105 -4.2394886 -4.2273521 -4.2321439 -4.2465105 -4.2674646 -4.282536 -4.2908211 -4.2990618 -4.3041515][-4.2293553 -4.2362 -4.2401433 -4.2404208 -4.2350221 -4.2251835 -4.2053504 -4.183157 -4.1832261 -4.2026424 -4.2309351 -4.2495408 -4.2598033 -4.2729135 -4.2837992]]...]
INFO - root - 2017-12-07 17:28:56.416300: step 35110, loss = 2.08, batch loss = 2.02 (16.2 examples/sec; 0.493 sec/batch; 40h:42m:04s remains)
INFO - root - 2017-12-07 17:29:03.205528: step 35120, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 55h:46m:24s remains)
INFO - root - 2017-12-07 17:29:10.194244: step 35130, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 61h:12m:52s remains)
INFO - root - 2017-12-07 17:29:17.055205: step 35140, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 55h:15m:25s remains)
INFO - root - 2017-12-07 17:29:23.811677: step 35150, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 54h:30m:42s remains)
INFO - root - 2017-12-07 17:29:30.594048: step 35160, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 53h:50m:06s remains)
INFO - root - 2017-12-07 17:29:37.335620: step 35170, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 56h:47m:38s remains)
INFO - root - 2017-12-07 17:29:44.040722: step 35180, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 56h:04m:20s remains)
INFO - root - 2017-12-07 17:29:50.830144: step 35190, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 55h:52m:03s remains)
INFO - root - 2017-12-07 17:29:57.604316: step 35200, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.678 sec/batch; 55h:58m:09s remains)
2017-12-07 17:29:58.337315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2727795 -4.2820339 -4.2690434 -4.2484131 -4.2221675 -4.2015872 -4.1804614 -4.1766591 -4.1707988 -4.1414738 -4.1211724 -4.1106553 -4.10511 -4.1043863 -4.1104269][-4.2756433 -4.2882371 -4.2742486 -4.2511787 -4.2190566 -4.1927481 -4.1700535 -4.1616 -4.1583776 -4.1455541 -4.133945 -4.1186881 -4.102304 -4.0899692 -4.0907731][-4.2734828 -4.2865572 -4.2715387 -4.2448664 -4.2072506 -4.1760783 -4.1500525 -4.1386929 -4.1459994 -4.15118 -4.1504307 -4.1354957 -4.1120687 -4.0912986 -4.0867319][-4.2708154 -4.2847447 -4.2714229 -4.2449827 -4.2038851 -4.167521 -4.1326146 -4.115325 -4.1297154 -4.1487074 -4.1599407 -4.1530657 -4.1319318 -4.1073303 -4.0973096][-4.2695 -4.2849388 -4.2762613 -4.2531977 -4.2114305 -4.16563 -4.11444 -4.0817366 -4.0972676 -4.1326318 -4.1601815 -4.1683245 -4.1584487 -4.1339025 -4.1165776][-4.2628427 -4.2787743 -4.2765555 -4.259871 -4.2208881 -4.1627836 -4.0836225 -4.0189514 -4.0426569 -4.1109142 -4.1632805 -4.1864538 -4.186408 -4.1670685 -4.1469884][-4.2530279 -4.2709723 -4.2760282 -4.2620411 -4.2183375 -4.146245 -4.0337653 -3.9310586 -3.9765649 -4.0856419 -4.1668396 -4.2068148 -4.2148261 -4.2009935 -4.184824][-4.24629 -4.2651243 -4.270967 -4.2506909 -4.200356 -4.1252885 -4.0098233 -3.9126949 -3.9769197 -4.0922837 -4.1732159 -4.2168212 -4.2333179 -4.230648 -4.2241268][-4.2366996 -4.2518163 -4.2522411 -4.2289481 -4.183259 -4.1254945 -4.0470967 -3.9973035 -4.0477567 -4.1243448 -4.176446 -4.2113237 -4.2345524 -4.2472863 -4.2546806][-4.2234869 -4.2333636 -4.2272553 -4.2062645 -4.1752939 -4.1410475 -4.0988674 -4.0791903 -4.1095228 -4.1520023 -4.1795068 -4.2033854 -4.22853 -4.2513676 -4.2679744][-4.2141562 -4.2184319 -4.2059951 -4.1871309 -4.1698761 -4.1537828 -4.1338663 -4.1260843 -4.1444755 -4.173317 -4.1927629 -4.2106175 -4.2334204 -4.2554817 -4.2680559][-4.2116041 -4.212975 -4.1986294 -4.1794314 -4.16555 -4.1597915 -4.1502795 -4.1448154 -4.1586151 -4.18358 -4.2031379 -4.2178683 -4.23607 -4.2513609 -4.2555995][-4.2091923 -4.2113996 -4.1982994 -4.1773658 -4.164175 -4.1642265 -4.159513 -4.1548848 -4.165328 -4.1860023 -4.2059488 -4.2206521 -4.2357726 -4.2445307 -4.2429285][-4.2001362 -4.2035141 -4.1914287 -4.1726804 -4.1635184 -4.1696634 -4.1690993 -4.1668839 -4.1733122 -4.187252 -4.2043777 -4.2166076 -4.229629 -4.2369561 -4.2336483][-4.1961179 -4.2063375 -4.2007322 -4.1817365 -4.1706572 -4.1747761 -4.1762161 -4.1736212 -4.1745868 -4.1838117 -4.1985707 -4.2124915 -4.2276654 -4.2385182 -4.2399416]]...]
INFO - root - 2017-12-07 17:30:04.954335: step 35210, loss = 2.06, batch loss = 2.00 (14.0 examples/sec; 0.571 sec/batch; 47h:11m:02s remains)
INFO - root - 2017-12-07 17:30:11.784114: step 35220, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 56h:01m:08s remains)
INFO - root - 2017-12-07 17:30:18.592598: step 35230, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 53h:52m:09s remains)
INFO - root - 2017-12-07 17:30:25.410321: step 35240, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 55h:32m:58s remains)
INFO - root - 2017-12-07 17:30:32.226662: step 35250, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 57h:17m:40s remains)
INFO - root - 2017-12-07 17:30:39.046624: step 35260, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 60h:09m:19s remains)
INFO - root - 2017-12-07 17:30:45.767806: step 35270, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.681 sec/batch; 56h:11m:11s remains)
INFO - root - 2017-12-07 17:30:52.370158: step 35280, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 54h:01m:45s remains)
INFO - root - 2017-12-07 17:30:59.136024: step 35290, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 57h:03m:41s remains)
INFO - root - 2017-12-07 17:31:06.012667: step 35300, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 56h:54m:38s remains)
2017-12-07 17:31:06.760061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2388611 -4.2259517 -4.2188468 -4.2155623 -4.2150025 -4.2083731 -4.1922531 -4.1679125 -4.1627831 -4.1870675 -4.223762 -4.25435 -4.2851443 -4.3192391 -4.3411703][-4.2354393 -4.2151942 -4.2036052 -4.1964569 -4.1928988 -4.1811991 -4.1596718 -4.1333985 -4.1305518 -4.1633644 -4.2101936 -4.2479124 -4.2812958 -4.3167543 -4.3403645][-4.231513 -4.2103724 -4.1985006 -4.1913381 -4.1860561 -4.1763744 -4.1589355 -4.1340208 -4.1290359 -4.1592889 -4.20642 -4.2451119 -4.2769041 -4.3112006 -4.3329234][-4.2224874 -4.2047491 -4.1947732 -4.1911159 -4.1916156 -4.1891251 -4.1752205 -4.1488781 -4.1355052 -4.1586814 -4.2036881 -4.2414303 -4.2718163 -4.3032951 -4.3222122][-4.1991472 -4.1847672 -4.1759248 -4.1769724 -4.1867013 -4.1945138 -4.1855826 -4.1573381 -4.1353221 -4.1506577 -4.1933327 -4.2309647 -4.2622156 -4.2905354 -4.3085408][-4.1690059 -4.1502266 -4.13443 -4.1333046 -4.1480093 -4.1615915 -4.1545477 -4.1249862 -4.0987263 -4.1090345 -4.1517205 -4.1971164 -4.2353706 -4.2684755 -4.2897129][-4.1489854 -4.11792 -4.0858569 -4.0719385 -4.0813336 -4.0944052 -4.086978 -4.0574045 -4.0298052 -4.040123 -4.0892348 -4.1491933 -4.2019057 -4.244956 -4.2711067][-4.14246 -4.0970082 -4.0492463 -4.0217304 -4.023519 -4.03637 -4.0321741 -4.0082273 -3.9865391 -4.0015879 -4.0549474 -4.121995 -4.1813936 -4.2285309 -4.2552872][-4.1590481 -4.1154289 -4.0697179 -4.0401678 -4.0369258 -4.0479503 -4.0462022 -4.0278344 -4.0114622 -4.0279694 -4.076262 -4.1370859 -4.1923571 -4.2342243 -4.2573934][-4.1871009 -4.156024 -4.126905 -4.1106467 -4.1108479 -4.1172724 -4.1155634 -4.102335 -4.0894361 -4.102859 -4.13958 -4.18718 -4.23226 -4.26405 -4.2774496][-4.2078161 -4.1891747 -4.1736875 -4.1661482 -4.1675639 -4.1705852 -4.16716 -4.1575069 -4.1492395 -4.1611195 -4.1924591 -4.233541 -4.2699823 -4.2913995 -4.2965837][-4.2203503 -4.2126818 -4.2066503 -4.2050185 -4.2068748 -4.2047362 -4.1959095 -4.1849022 -4.1788125 -4.1912661 -4.2206993 -4.2581439 -4.2897496 -4.3056688 -4.3095884][-4.2091351 -4.2097859 -4.2137733 -4.2212968 -4.2275648 -4.2241483 -4.209795 -4.1940665 -4.1868076 -4.19965 -4.2278605 -4.2632213 -4.2933617 -4.3103962 -4.3192759][-4.1931744 -4.1981778 -4.2116952 -4.2260613 -4.2343159 -4.2292361 -4.2096968 -4.1901 -4.1817932 -4.1942873 -4.2224936 -4.2563004 -4.2848144 -4.3054805 -4.3193097][-4.1851339 -4.1918993 -4.2098131 -4.226356 -4.2338085 -4.2265792 -4.203835 -4.1824203 -4.1740742 -4.1873946 -4.2169442 -4.2486949 -4.2745876 -4.2948961 -4.3105478]]...]
INFO - root - 2017-12-07 17:31:13.329170: step 35310, loss = 2.07, batch loss = 2.01 (15.9 examples/sec; 0.502 sec/batch; 41h:26m:33s remains)
INFO - root - 2017-12-07 17:31:20.144391: step 35320, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 58h:51m:07s remains)
INFO - root - 2017-12-07 17:31:27.012106: step 35330, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 58h:19m:13s remains)
INFO - root - 2017-12-07 17:31:33.782651: step 35340, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 56h:17m:42s remains)
INFO - root - 2017-12-07 17:31:40.540232: step 35350, loss = 2.06, batch loss = 2.00 (13.1 examples/sec; 0.610 sec/batch; 50h:19m:12s remains)
INFO - root - 2017-12-07 17:31:47.293298: step 35360, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 53h:48m:31s remains)
INFO - root - 2017-12-07 17:31:54.100379: step 35370, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 57h:57m:12s remains)
INFO - root - 2017-12-07 17:32:01.043417: step 35380, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 59h:56m:42s remains)
INFO - root - 2017-12-07 17:32:07.780603: step 35390, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 56h:19m:21s remains)
INFO - root - 2017-12-07 17:32:14.514605: step 35400, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 54h:30m:19s remains)
2017-12-07 17:32:15.269041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1584954 -4.1257267 -4.1312838 -4.1716185 -4.215384 -4.2432742 -4.2513623 -4.258749 -4.2673044 -4.2674971 -4.2615662 -4.2570438 -4.2560987 -4.2574563 -4.2587442][-4.1960387 -4.16314 -4.162755 -4.1972528 -4.2315192 -4.2478557 -4.2521753 -4.2661462 -4.284708 -4.2876205 -4.27622 -4.26429 -4.2600384 -4.2622657 -4.2671652][-4.2418766 -4.2159548 -4.2124677 -4.231987 -4.2446475 -4.2410316 -4.2380447 -4.258822 -4.2913756 -4.3039742 -4.2942014 -4.278008 -4.270721 -4.271534 -4.2767439][-4.2765574 -4.2628984 -4.2570109 -4.2595482 -4.246417 -4.2177062 -4.198431 -4.2183981 -4.2680931 -4.2981224 -4.2967343 -4.2822013 -4.2780318 -4.2816563 -4.28733][-4.2754173 -4.2678661 -4.2587943 -4.2489109 -4.2146206 -4.1608987 -4.1154265 -4.1270943 -4.2005472 -4.2572727 -4.2706022 -4.262979 -4.2668042 -4.2791595 -4.288517][-4.247333 -4.2365627 -4.2278075 -4.2135463 -4.1665339 -4.0899878 -4.0161548 -4.0129228 -4.1131177 -4.1993866 -4.2282124 -4.2288527 -4.2373552 -4.2548914 -4.2684197][-4.2121224 -4.1915092 -4.1798706 -4.166759 -4.1166134 -4.0216808 -3.9120207 -3.887274 -4.0170865 -4.1363859 -4.1841741 -4.1938329 -4.2011271 -4.216363 -4.2328715][-4.2018085 -4.1768422 -4.1626916 -4.1531954 -4.1116867 -4.0242209 -3.9088421 -3.8683748 -3.9915848 -4.1151085 -4.1722217 -4.1880145 -4.1894431 -4.1983953 -4.2101355][-4.2201161 -4.2011418 -4.1891851 -4.1914406 -4.1771908 -4.1221433 -4.0422492 -4.0139561 -4.0897527 -4.1693716 -4.2101631 -4.2269835 -4.2271214 -4.2312927 -4.240675][-4.2350016 -4.220829 -4.2140069 -4.2277613 -4.2355795 -4.211266 -4.1670465 -4.1499562 -4.1851172 -4.226872 -4.2509341 -4.2684035 -4.2742472 -4.2794142 -4.2907896][-4.2425032 -4.2239151 -4.2222776 -4.2459707 -4.2662778 -4.2615304 -4.2407222 -4.2310581 -4.2414231 -4.2625203 -4.2787395 -4.2922196 -4.3000441 -4.3074317 -4.3208661][-4.2387323 -4.2091122 -4.2084308 -4.2405519 -4.2691274 -4.2752848 -4.2693882 -4.2668376 -4.2642941 -4.2688165 -4.27952 -4.291759 -4.2989159 -4.3062134 -4.3182788][-4.2272892 -4.1857219 -4.1796303 -4.2112994 -4.2420974 -4.2542028 -4.2587819 -4.2640381 -4.2581191 -4.2524314 -4.2585082 -4.2705922 -4.2796516 -4.2880626 -4.2980943][-4.2189388 -4.1684237 -4.1545353 -4.1809783 -4.2088256 -4.2194333 -4.2266035 -4.2383356 -4.2345285 -4.22238 -4.225039 -4.2379723 -4.2500339 -4.2583289 -4.2684183][-4.2209978 -4.1740003 -4.1610985 -4.1842928 -4.2049847 -4.2066035 -4.2092433 -4.2224865 -4.2223616 -4.2111635 -4.2150211 -4.2258897 -4.2335906 -4.2398019 -4.2489071]]...]
INFO - root - 2017-12-07 17:32:21.784394: step 35410, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 0.537 sec/batch; 44h:19m:04s remains)
INFO - root - 2017-12-07 17:32:28.652710: step 35420, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 57h:19m:52s remains)
INFO - root - 2017-12-07 17:32:35.385648: step 35430, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 52h:08m:33s remains)
INFO - root - 2017-12-07 17:32:42.156620: step 35440, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 56h:43m:02s remains)
INFO - root - 2017-12-07 17:32:48.990141: step 35450, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 58h:53m:52s remains)
INFO - root - 2017-12-07 17:32:55.714770: step 35460, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 56h:39m:59s remains)
INFO - root - 2017-12-07 17:33:02.467791: step 35470, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 56h:27m:29s remains)
INFO - root - 2017-12-07 17:33:09.310475: step 35480, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 57h:47m:43s remains)
INFO - root - 2017-12-07 17:33:16.111861: step 35490, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:17m:11s remains)
INFO - root - 2017-12-07 17:33:22.889945: step 35500, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 57h:56m:28s remains)
2017-12-07 17:33:23.670239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3524847 -4.3395715 -4.3224168 -4.3077478 -4.2921209 -4.2723341 -4.2503824 -4.2422132 -4.2553072 -4.2699256 -4.28011 -4.2846675 -4.2885842 -4.2949643 -4.301507][-4.3472886 -4.329277 -4.3071527 -4.2904115 -4.2710161 -4.240828 -4.2069254 -4.1870208 -4.2027335 -4.2247429 -4.2373977 -4.2387309 -4.240612 -4.2499733 -4.2620254][-4.3376627 -4.316381 -4.2920165 -4.2750511 -4.2532043 -4.2158442 -4.173955 -4.1434145 -4.157032 -4.1772695 -4.187088 -4.1855254 -4.1860852 -4.2017527 -4.2236867][-4.3247371 -4.2990012 -4.2687321 -4.2453084 -4.217196 -4.1741982 -4.1280313 -4.0895009 -4.0987272 -4.108788 -4.1113806 -4.1122227 -4.1183844 -4.1418357 -4.1737113][-4.31608 -4.2860136 -4.2476177 -4.2126889 -4.1764054 -4.1300087 -4.0769377 -4.0265484 -4.0252924 -4.0266042 -4.0239229 -4.0260606 -4.0396423 -4.0741377 -4.1191683][-4.3078718 -4.2699203 -4.2215686 -4.1788573 -4.1386056 -4.0894318 -4.0245757 -3.9533637 -3.9426825 -3.9487536 -3.9507544 -3.9582057 -3.9764748 -4.0201244 -4.0752673][-4.3054504 -4.2603149 -4.2057877 -4.1609192 -4.1126823 -4.0515356 -3.967226 -3.8767817 -3.8651085 -3.8853192 -3.9017587 -3.9199295 -3.9432821 -3.9886453 -4.048944][-4.3037996 -4.255074 -4.1992311 -4.1538363 -4.101419 -4.0364156 -3.9549046 -3.8760397 -3.8732333 -3.8999562 -3.9183688 -3.9380357 -3.958446 -3.99193 -4.0468917][-4.3096571 -4.26474 -4.2134781 -4.1742258 -4.13166 -4.0839009 -4.0290728 -3.9787693 -3.9786065 -3.9951982 -4.002532 -4.0170932 -4.0339332 -4.055336 -4.0981016][-4.31891 -4.2828846 -4.2429867 -4.2144003 -4.1832781 -4.1509624 -4.114871 -4.0854497 -4.0865993 -4.0938816 -4.0973163 -4.1143875 -4.1348062 -4.1466002 -4.1732616][-4.3332868 -4.3081918 -4.2810721 -4.2613788 -4.237834 -4.2157507 -4.1938939 -4.1783381 -4.1836433 -4.1916566 -4.1961184 -4.2129245 -4.2303672 -4.2328129 -4.2447433][-4.3499136 -4.3343697 -4.318707 -4.3063359 -4.2904892 -4.2768054 -4.2658515 -4.2579145 -4.2654462 -4.2749696 -4.2815042 -4.2951779 -4.3028822 -4.2993979 -4.3031082][-4.3623495 -4.3517714 -4.3412871 -4.3331208 -4.3237295 -4.3166084 -4.3132396 -4.3125973 -4.3228035 -4.3345246 -4.3422084 -4.3494906 -4.3528385 -4.3495779 -4.3507228][-4.3701997 -4.36329 -4.3555865 -4.3483858 -4.3412962 -4.33614 -4.3354788 -4.3381863 -4.3479424 -4.3590789 -4.3657517 -4.3699641 -4.3717952 -4.3699312 -4.3708334][-4.376894 -4.3724351 -4.3664074 -4.3598313 -4.3532405 -4.3478155 -4.3462124 -4.3474569 -4.3540912 -4.3622952 -4.3675518 -4.3703542 -4.371942 -4.3721004 -4.373631]]...]
INFO - root - 2017-12-07 17:33:30.219796: step 35510, loss = 2.06, batch loss = 2.01 (15.0 examples/sec; 0.533 sec/batch; 43h:59m:16s remains)
INFO - root - 2017-12-07 17:33:36.986687: step 35520, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 54h:09m:35s remains)
INFO - root - 2017-12-07 17:33:43.718940: step 35530, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 57h:11m:49s remains)
INFO - root - 2017-12-07 17:33:50.636572: step 35540, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 59h:28m:22s remains)
INFO - root - 2017-12-07 17:33:57.483853: step 35550, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 56h:50m:11s remains)
INFO - root - 2017-12-07 17:34:04.200002: step 35560, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 51h:21m:47s remains)
INFO - root - 2017-12-07 17:34:10.998799: step 35570, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.669 sec/batch; 55h:10m:11s remains)
INFO - root - 2017-12-07 17:34:17.773123: step 35580, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 57h:02m:53s remains)
INFO - root - 2017-12-07 17:34:24.529045: step 35590, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 58h:26m:08s remains)
INFO - root - 2017-12-07 17:34:31.344068: step 35600, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 55h:15m:03s remains)
2017-12-07 17:34:32.085426: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3445349 -4.3461738 -4.3318524 -4.291183 -4.2232761 -4.1391096 -4.0890584 -4.1080027 -4.1646967 -4.2170329 -4.2548442 -4.2871337 -4.3170657 -4.3293958 -4.3315487][-4.3469806 -4.3473587 -4.3277445 -4.2798958 -4.2015686 -4.1034236 -4.0505872 -4.0868764 -4.1633387 -4.223042 -4.25971 -4.2900581 -4.3198838 -4.3319278 -4.3334541][-4.3466206 -4.3451405 -4.3192706 -4.2662797 -4.1787262 -4.0667405 -4.0095739 -4.0587673 -4.1561446 -4.2252154 -4.2621727 -4.2931924 -4.3226929 -4.3339639 -4.3338938][-4.3434706 -4.339273 -4.3079195 -4.2506003 -4.1576939 -4.0367103 -3.9716594 -4.0220094 -4.1324911 -4.2156653 -4.2584586 -4.29254 -4.3225517 -4.33342 -4.3302855][-4.339201 -4.3352861 -4.3038807 -4.2433457 -4.1483941 -4.0253892 -3.9475007 -3.9895012 -4.1078339 -4.2040811 -4.2559972 -4.2929521 -4.3220129 -4.3333721 -4.3263288][-4.3351803 -4.3341432 -4.3067012 -4.2422047 -4.1375875 -4.007205 -3.9100347 -3.9410846 -4.0723238 -4.1835232 -4.2460079 -4.2871947 -4.31599 -4.3284259 -4.320405][-4.329483 -4.3313951 -4.3083806 -4.240881 -4.1225815 -3.9743161 -3.8550522 -3.8772545 -4.0224395 -4.149796 -4.22603 -4.2732711 -4.3034749 -4.3173676 -4.3085656][-4.3224692 -4.325861 -4.3072071 -4.2416868 -4.1184988 -3.9575162 -3.820245 -3.8264451 -3.9718542 -4.1131339 -4.2023659 -4.2555161 -4.286067 -4.3010445 -4.292007][-4.3176203 -4.3213263 -4.3051505 -4.2488585 -4.1388946 -3.9895911 -3.8573346 -3.8394485 -3.9583969 -4.0972385 -4.1928368 -4.25002 -4.2796583 -4.2923355 -4.2823215][-4.3148155 -4.3179593 -4.3045607 -4.2602286 -4.1727362 -4.0522223 -3.941829 -3.9063821 -3.988133 -4.1073785 -4.198926 -4.2547054 -4.2813811 -4.2909141 -4.2800994][-4.3128428 -4.314652 -4.3029251 -4.2668462 -4.2003813 -4.1096463 -4.0262761 -3.9922802 -4.0473452 -4.1431437 -4.2241893 -4.2719121 -4.2920189 -4.2969923 -4.2857742][-4.3115454 -4.31416 -4.3071918 -4.2829823 -4.2356968 -4.17103 -4.113133 -4.0891447 -4.1270041 -4.1984472 -4.2627549 -4.2971454 -4.308496 -4.3071103 -4.2945433][-4.3091679 -4.3136883 -4.31284 -4.3014627 -4.2728081 -4.2305522 -4.1922045 -4.1773839 -4.2031379 -4.2529469 -4.2978754 -4.3190584 -4.3220978 -4.315043 -4.3013549][-4.30635 -4.3118539 -4.3140564 -4.3110323 -4.2971716 -4.2728109 -4.2480831 -4.239625 -4.2564058 -4.2882638 -4.3156028 -4.3280492 -4.32718 -4.3184876 -4.3059711][-4.3042307 -4.3095975 -4.31281 -4.3146634 -4.30965 -4.297215 -4.2819271 -4.2762718 -4.2867117 -4.3059731 -4.3214574 -4.3274403 -4.3251657 -4.3173447 -4.3072782]]...]
INFO - root - 2017-12-07 17:34:38.685338: step 35610, loss = 2.06, batch loss = 2.00 (15.7 examples/sec; 0.510 sec/batch; 42h:01m:52s remains)
INFO - root - 2017-12-07 17:34:45.500405: step 35620, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 56h:03m:42s remains)
INFO - root - 2017-12-07 17:34:52.226743: step 35630, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 51h:41m:56s remains)
INFO - root - 2017-12-07 17:34:59.048709: step 35640, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 53h:41m:19s remains)
INFO - root - 2017-12-07 17:35:05.913296: step 35650, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 60h:20m:34s remains)
INFO - root - 2017-12-07 17:35:12.759032: step 35660, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 55h:26m:46s remains)
INFO - root - 2017-12-07 17:35:19.578093: step 35670, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 55h:54m:38s remains)
INFO - root - 2017-12-07 17:35:26.361638: step 35680, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 56h:39m:35s remains)
INFO - root - 2017-12-07 17:35:33.183987: step 35690, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 52h:50m:37s remains)
INFO - root - 2017-12-07 17:35:40.112190: step 35700, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 57h:46m:00s remains)
2017-12-07 17:35:40.805534: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3264203 -4.327816 -4.3295283 -4.3300328 -4.3316712 -4.3326268 -4.3324137 -4.3313856 -4.3319206 -4.335238 -4.3398824 -4.3437495 -4.3451271 -4.34536 -4.3462362][-4.303299 -4.3075738 -4.3139482 -4.316916 -4.3206391 -4.3221083 -4.3212781 -4.3182812 -4.317688 -4.3202753 -4.32476 -4.3292527 -4.3312635 -4.3307176 -4.3298087][-4.2704043 -4.2759552 -4.2848935 -4.2888503 -4.29219 -4.2896605 -4.2836046 -4.2738724 -4.2693419 -4.2716231 -4.2799187 -4.288465 -4.2954321 -4.2982483 -4.2982683][-4.2430167 -4.24819 -4.2578163 -4.2612271 -4.2627411 -4.2578239 -4.2492671 -4.2337866 -4.2261872 -4.2331352 -4.2479291 -4.26051 -4.2725468 -4.2766027 -4.2754235][-4.2201676 -4.22774 -4.2383242 -4.2403617 -4.2351179 -4.2251258 -4.2111936 -4.1875868 -4.1770668 -4.1913285 -4.2162371 -4.2368989 -4.2551589 -4.2636352 -4.2635136][-4.2100596 -4.2172642 -4.2244039 -4.216248 -4.1956758 -4.171864 -4.1437845 -4.1040506 -4.0899782 -4.1137371 -4.1497688 -4.183784 -4.2170534 -4.2376018 -4.2454443][-4.2132225 -4.2108765 -4.2018437 -4.1699185 -4.12076 -4.065865 -4.0064898 -3.9400887 -3.9221978 -3.9659007 -4.0279245 -4.0903683 -4.1543436 -4.2004189 -4.2246647][-4.2194972 -4.2051759 -4.1773825 -4.1216993 -4.0454721 -3.9595547 -3.8681858 -3.778724 -3.7640991 -3.8378613 -3.9348545 -4.0318618 -4.1269536 -4.1933241 -4.2271733][-4.2228093 -4.206111 -4.1780305 -4.1287103 -4.0646224 -3.9961476 -3.9290106 -3.8699939 -3.8772125 -3.9522395 -4.0361996 -4.114624 -4.188303 -4.2374291 -4.2611723][-4.2272625 -4.2198129 -4.2061439 -4.1794481 -4.1415272 -4.1006436 -4.063447 -4.0309396 -4.0416636 -4.0969553 -4.14976 -4.1939135 -4.2357378 -4.2643647 -4.2809973][-4.2350526 -4.237535 -4.2340665 -4.2208595 -4.1945486 -4.16171 -4.1323981 -4.1063275 -4.109199 -4.1453781 -4.1798606 -4.2043519 -4.2318678 -4.2552419 -4.2751307][-4.2327075 -4.2382441 -4.2377148 -4.2268453 -4.2057657 -4.1773186 -4.1512885 -4.1262646 -4.1210074 -4.1430097 -4.1656251 -4.1824927 -4.2044024 -4.2290926 -4.2550011][-4.2227368 -4.2293935 -4.2313175 -4.2227249 -4.2058077 -4.1822715 -4.1567774 -4.1282897 -4.1157441 -4.1271858 -4.143096 -4.1560469 -4.1740808 -4.2021742 -4.2340126][-4.2127118 -4.220778 -4.2234845 -4.2187109 -4.2084026 -4.1939559 -4.1729059 -4.1474872 -4.1332178 -4.136764 -4.1444111 -4.14908 -4.1610994 -4.1883025 -4.2221417][-4.216907 -4.2209959 -4.2170134 -4.2082577 -4.1967778 -4.1861172 -4.170938 -4.1540751 -4.145102 -4.1505489 -4.1582174 -4.1596518 -4.166986 -4.1901059 -4.2218351]]...]
INFO - root - 2017-12-07 17:35:47.461272: step 35710, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 0.516 sec/batch; 42h:30m:32s remains)
INFO - root - 2017-12-07 17:35:54.361666: step 35720, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.670 sec/batch; 55h:11m:59s remains)
INFO - root - 2017-12-07 17:36:01.161594: step 35730, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 58h:33m:26s remains)
INFO - root - 2017-12-07 17:36:08.013593: step 35740, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 57h:39m:04s remains)
INFO - root - 2017-12-07 17:36:14.821700: step 35750, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 56h:04m:14s remains)
INFO - root - 2017-12-07 17:36:21.642121: step 35760, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 54h:08m:59s remains)
INFO - root - 2017-12-07 17:36:28.465517: step 35770, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 52h:15m:29s remains)
INFO - root - 2017-12-07 17:36:35.298879: step 35780, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 57h:48m:45s remains)
INFO - root - 2017-12-07 17:36:42.150586: step 35790, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 55h:31m:08s remains)
INFO - root - 2017-12-07 17:36:48.950398: step 35800, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 56h:51m:15s remains)
2017-12-07 17:36:49.768059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2717524 -4.2681384 -4.2507391 -4.2291965 -4.1800365 -4.1003137 -4.03744 -4.0454936 -4.104763 -4.1516781 -4.165863 -4.1357889 -4.0996666 -4.1086698 -4.15971][-4.3001523 -4.2937984 -4.2727466 -4.2451692 -4.1864958 -4.0966654 -4.022768 -4.0262756 -4.0926166 -4.1395187 -4.1567831 -4.1345448 -4.1027088 -4.1128979 -4.1685429][-4.3215575 -4.3151608 -4.2939372 -4.2585788 -4.1871572 -4.0853653 -4.0012116 -4.0062227 -4.0788531 -4.1229963 -4.140564 -4.1281629 -4.1019492 -4.1135759 -4.1720166][-4.3341813 -4.3238835 -4.2996087 -4.2544265 -4.1741076 -4.0623479 -3.970181 -3.9862726 -4.0666995 -4.1078529 -4.1231503 -4.110188 -4.0792246 -4.0881734 -4.1553726][-4.3390241 -4.3244944 -4.2941866 -4.2407637 -4.1545262 -4.0329661 -3.930577 -3.957787 -4.05364 -4.0963655 -4.1095104 -4.0897508 -4.0415978 -4.0433087 -4.1239491][-4.3291912 -4.3139029 -4.28363 -4.2285075 -4.1355529 -3.9965677 -3.8739297 -3.9102156 -4.0322437 -4.086225 -4.1055775 -4.0811906 -4.0204334 -4.018939 -4.1114774][-4.3107243 -4.2960234 -4.2676735 -4.2085457 -4.1034861 -3.9422817 -3.7921481 -3.8422308 -3.9946914 -4.0735955 -4.1081085 -4.0882859 -4.0255461 -4.0254006 -4.1227341][-4.2905631 -4.2777843 -4.249105 -4.1885309 -4.08421 -3.9190412 -3.764977 -3.8260598 -3.9923277 -4.0855532 -4.1280017 -4.1207361 -4.0684867 -4.0716133 -4.1534319][-4.2903538 -4.2777286 -4.247107 -4.1938362 -4.1041083 -3.960753 -3.8343563 -3.8943419 -4.0481153 -4.1378112 -4.17115 -4.1701026 -4.1350026 -4.1357327 -4.1940274][-4.2956829 -4.2832918 -4.2560573 -4.2137814 -4.1354036 -4.0154266 -3.91604 -3.9690192 -4.1034069 -4.1885014 -4.2140183 -4.2149553 -4.1958289 -4.1936908 -4.2325878][-4.3016143 -4.2891936 -4.2691441 -4.2383585 -4.1730723 -4.0764866 -4.0021811 -4.0449967 -4.1547251 -4.2314439 -4.2506094 -4.2493544 -4.2380977 -4.2331376 -4.2616124][-4.3217287 -4.3082438 -4.2920475 -4.2682004 -4.2173514 -4.1455779 -4.0947237 -4.1292906 -4.2166591 -4.2807283 -4.2967277 -4.2918196 -4.2833476 -4.2777152 -4.2960258][-4.3414268 -4.3301992 -4.3188777 -4.2993116 -4.2608752 -4.2101455 -4.1751022 -4.2005262 -4.2653675 -4.3183293 -4.336216 -4.3301792 -4.3228908 -4.3184505 -4.3310862][-4.3466721 -4.3367648 -4.3277659 -4.3127556 -4.2827559 -4.2481046 -4.2297649 -4.2556238 -4.3049212 -4.345623 -4.3597817 -4.354044 -4.3472414 -4.3444948 -4.3551273][-4.3421431 -4.3349509 -4.3288765 -4.319418 -4.2989655 -4.2776241 -4.271594 -4.2946029 -4.3300343 -4.3571138 -4.3658128 -4.3597665 -4.35304 -4.3514223 -4.3593516]]...]
INFO - root - 2017-12-07 17:36:56.469994: step 35810, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 53h:00m:59s remains)
INFO - root - 2017-12-07 17:37:03.335767: step 35820, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 55h:00m:04s remains)
INFO - root - 2017-12-07 17:37:10.172691: step 35830, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 51h:40m:45s remains)
INFO - root - 2017-12-07 17:37:17.084024: step 35840, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.657 sec/batch; 54h:06m:33s remains)
INFO - root - 2017-12-07 17:37:23.932868: step 35850, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 58h:12m:32s remains)
INFO - root - 2017-12-07 17:37:30.777127: step 35860, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 60h:15m:50s remains)
INFO - root - 2017-12-07 17:37:37.439344: step 35870, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 54h:18m:37s remains)
INFO - root - 2017-12-07 17:37:44.182962: step 35880, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.620 sec/batch; 51h:04m:38s remains)
INFO - root - 2017-12-07 17:37:51.051796: step 35890, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 55h:28m:13s remains)
INFO - root - 2017-12-07 17:37:57.807263: step 35900, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 56h:36m:55s remains)
2017-12-07 17:37:58.501665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1772985 -4.1877346 -4.1847854 -4.1852 -4.198998 -4.206502 -4.2033048 -4.1840591 -4.1741652 -4.1995978 -4.2270436 -4.2438574 -4.255188 -4.2505903 -4.224731][-4.1768746 -4.1842937 -4.1790247 -4.1722813 -4.1766005 -4.1808448 -4.1845131 -4.1719332 -4.1667676 -4.1926069 -4.2272024 -4.2553058 -4.2719746 -4.2656093 -4.23514][-4.1861644 -4.1821165 -4.1757679 -4.1647062 -4.159905 -4.1587906 -4.1637321 -4.1635609 -4.1649103 -4.1858344 -4.2224379 -4.2622976 -4.2872748 -4.2854791 -4.2562962][-4.203464 -4.1817331 -4.1666074 -4.1513748 -4.1364808 -4.128335 -4.1340222 -4.1455607 -4.156384 -4.1789093 -4.216774 -4.2628098 -4.2925196 -4.2961 -4.2724872][-4.2226706 -4.1839495 -4.1574583 -4.1385136 -4.1140218 -4.094285 -4.09534 -4.1082015 -4.1295242 -4.1637411 -4.2065759 -4.2513175 -4.2840128 -4.2946639 -4.2807145][-4.2428632 -4.1949978 -4.1620669 -4.137311 -4.103364 -4.0646348 -4.0402007 -4.0349751 -4.0669503 -4.12585 -4.1839552 -4.2331915 -4.267467 -4.2823648 -4.2767825][-4.2610164 -4.216516 -4.1831236 -4.1486769 -4.0985022 -4.0343771 -3.9686685 -3.9314141 -3.971457 -4.0700378 -4.1547217 -4.2118864 -4.2498579 -4.2681332 -4.2700982][-4.2693481 -4.2374997 -4.2075725 -4.1600156 -4.0937004 -4.0125189 -3.9187241 -3.8595023 -3.9109044 -4.0386395 -4.1359973 -4.197679 -4.2350974 -4.2494597 -4.2539215][-4.2594032 -4.2413011 -4.2139926 -4.1602497 -4.0845861 -4.0062265 -3.9308302 -3.8944843 -3.945281 -4.0479236 -4.1225948 -4.1769156 -4.212121 -4.2229528 -4.229867][-4.2328339 -4.2254686 -4.2012167 -4.1547461 -4.0879087 -4.029489 -3.9859197 -3.9712734 -4.0026741 -4.0543213 -4.0968695 -4.1474929 -4.1867752 -4.1993032 -4.2106376][-4.1977291 -4.1981578 -4.18207 -4.1553373 -4.1133952 -4.078774 -4.0530295 -4.0393863 -4.0411191 -4.0528073 -4.0767188 -4.1321168 -4.1757421 -4.1896567 -4.1975155][-4.1700163 -4.1706409 -4.160954 -4.1458282 -4.1255779 -4.1104803 -4.0966468 -4.08336 -4.066741 -4.0581236 -4.0787888 -4.1342816 -4.1743097 -4.17989 -4.1756372][-4.1579442 -4.1543427 -4.1425166 -4.1282234 -4.1154027 -4.115519 -4.1146679 -4.1063743 -4.091897 -4.0875311 -4.1094961 -4.1460242 -4.1608305 -4.1509323 -4.1365318][-4.165112 -4.1586847 -4.1385508 -4.1140122 -4.1000452 -4.1108718 -4.1263204 -4.1302328 -4.1264977 -4.1286473 -4.1453643 -4.1537843 -4.1423407 -4.1227956 -4.10649][-4.1823483 -4.1740818 -4.1490068 -4.1189656 -4.1074629 -4.1257143 -4.1531448 -4.1673889 -4.1738067 -4.1759653 -4.17745 -4.1606364 -4.12945 -4.1103783 -4.1016855]]...]
INFO - root - 2017-12-07 17:38:05.030368: step 35910, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 55h:59m:06s remains)
INFO - root - 2017-12-07 17:38:11.840327: step 35920, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 58h:24m:06s remains)
INFO - root - 2017-12-07 17:38:18.804412: step 35930, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 55h:43m:07s remains)
INFO - root - 2017-12-07 17:38:25.736119: step 35940, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 53h:03m:47s remains)
INFO - root - 2017-12-07 17:38:32.513610: step 35950, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 54h:33m:33s remains)
INFO - root - 2017-12-07 17:38:39.429194: step 35960, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.750 sec/batch; 61h:48m:11s remains)
INFO - root - 2017-12-07 17:38:46.276707: step 35970, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 52h:58m:07s remains)
INFO - root - 2017-12-07 17:38:53.105323: step 35980, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 55h:22m:21s remains)
INFO - root - 2017-12-07 17:38:59.901205: step 35990, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 51h:13m:27s remains)
INFO - root - 2017-12-07 17:39:06.840653: step 36000, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 58h:14m:14s remains)
2017-12-07 17:39:07.611765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3655376 -4.3683872 -4.3655963 -4.350853 -4.3211732 -4.2922711 -4.2716246 -4.2653203 -4.2650337 -4.2643385 -4.264729 -4.2532539 -4.237339 -4.233058 -4.2363043][-4.366333 -4.3703127 -4.3675456 -4.3508487 -4.3156619 -4.2704716 -4.2398014 -4.2352319 -4.2433863 -4.2516723 -4.2505641 -4.2311893 -4.2121339 -4.207294 -4.2129993][-4.3640847 -4.3671994 -4.3636827 -4.3472438 -4.3105173 -4.253881 -4.2151752 -4.20877 -4.2248363 -4.2432642 -4.2394133 -4.2119164 -4.1951323 -4.1949444 -4.2045097][-4.3627696 -4.363132 -4.360014 -4.3442273 -4.3064008 -4.2433629 -4.1981907 -4.1884804 -4.2090163 -4.2349472 -4.2276878 -4.19417 -4.18303 -4.1917129 -4.2093306][-4.3634233 -4.3624797 -4.359786 -4.3441615 -4.3062482 -4.2419572 -4.1905217 -4.1750784 -4.1980181 -4.2297306 -4.2247477 -4.1953354 -4.1906934 -4.2074795 -4.230032][-4.3650436 -4.3637323 -4.3615761 -4.3477097 -4.3109436 -4.250258 -4.1968436 -4.1734762 -4.1893449 -4.215538 -4.210639 -4.1929579 -4.2004528 -4.2262168 -4.2499366][-4.36611 -4.3654308 -4.3639741 -4.3497753 -4.3112822 -4.2528286 -4.1990218 -4.1703053 -4.17409 -4.1842279 -4.1780667 -4.1736584 -4.1960182 -4.2321734 -4.257463][-4.3667917 -4.3679595 -4.3673768 -4.35173 -4.3087277 -4.2485385 -4.1960945 -4.1667094 -4.1671047 -4.1665173 -4.1608949 -4.16619 -4.1973248 -4.2358041 -4.2586513][-4.3671136 -4.3703508 -4.3712025 -4.3554211 -4.3090472 -4.2468891 -4.1984162 -4.1766849 -4.1821589 -4.1823983 -4.1806889 -4.1871738 -4.2160692 -4.245316 -4.261178][-4.3663311 -4.370079 -4.3698697 -4.3533807 -4.3033538 -4.2398295 -4.1989055 -4.1923156 -4.2076778 -4.2171855 -4.2203755 -4.2229261 -4.2401247 -4.2573075 -4.2645526][-4.3637476 -4.3657594 -4.3620634 -4.3432708 -4.2907853 -4.2264881 -4.1913366 -4.19794 -4.2271109 -4.2479711 -4.2551756 -4.2550645 -4.264257 -4.2715712 -4.2685537][-4.3597369 -4.3584161 -4.3494697 -4.3280663 -4.2761602 -4.2118015 -4.1762466 -4.1863379 -4.2239466 -4.2552066 -4.268137 -4.2694063 -4.2765417 -4.2740059 -4.2592688][-4.3558068 -4.3503356 -4.3353543 -4.3113194 -4.2627912 -4.2006545 -4.1647968 -4.1727285 -4.2105193 -4.2453604 -4.256691 -4.2582803 -4.2669573 -4.2595305 -4.2373366][-4.3546958 -4.3466921 -4.3283772 -4.3018618 -4.2543488 -4.1943297 -4.1591558 -4.1637235 -4.1995392 -4.2316484 -4.2373123 -4.2418113 -4.2566333 -4.2483835 -4.2231264][-4.3558335 -4.3475561 -4.3290768 -4.3011346 -4.2550473 -4.2015333 -4.1705618 -4.1742058 -4.2032337 -4.2258868 -4.2222552 -4.228385 -4.2484813 -4.2433486 -4.2181644]]...]
INFO - root - 2017-12-07 17:39:14.152868: step 36010, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 54h:53m:43s remains)
INFO - root - 2017-12-07 17:39:20.976558: step 36020, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 56h:28m:28s remains)
INFO - root - 2017-12-07 17:39:27.844734: step 36030, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 58h:11m:08s remains)
INFO - root - 2017-12-07 17:39:34.720300: step 36040, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:56m:29s remains)
INFO - root - 2017-12-07 17:39:41.509865: step 36050, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 57h:04m:11s remains)
INFO - root - 2017-12-07 17:39:48.328200: step 36060, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 56h:34m:03s remains)
INFO - root - 2017-12-07 17:39:55.191994: step 36070, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 54h:42m:53s remains)
INFO - root - 2017-12-07 17:40:01.936652: step 36080, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 57h:40m:13s remains)
INFO - root - 2017-12-07 17:40:08.685749: step 36090, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 56h:30m:38s remains)
INFO - root - 2017-12-07 17:40:15.523954: step 36100, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 56h:52m:49s remains)
2017-12-07 17:40:16.274392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2195878 -4.2276316 -4.2432909 -4.2523041 -4.2522068 -4.2486463 -4.2510958 -4.2622781 -4.2727237 -4.2808638 -4.2931237 -4.3034639 -4.3092275 -4.3085046 -4.2992659][-4.1535745 -4.1628079 -4.1867375 -4.2013507 -4.2011571 -4.1954618 -4.1988673 -4.2156086 -4.2345953 -4.2494454 -4.2688346 -4.2869453 -4.298923 -4.3008828 -4.2900543][-4.1187162 -4.1278324 -4.1517711 -4.1660113 -4.1640406 -4.1555886 -4.1582093 -4.1760159 -4.20225 -4.2250638 -4.2476077 -4.2692595 -4.2861929 -4.2933078 -4.2857375][-4.1186066 -4.1224184 -4.1395802 -4.1482825 -4.1408749 -4.130156 -4.1306577 -4.1440392 -4.1725483 -4.2040458 -4.2296267 -4.2535286 -4.2733526 -4.2849274 -4.2827129][-4.1313682 -4.1315207 -4.1437697 -4.1479926 -4.13607 -4.1219635 -4.11619 -4.1187124 -4.1445417 -4.1819482 -4.2121305 -4.2368188 -4.2582846 -4.276031 -4.281219][-4.1645975 -4.1628041 -4.1698828 -4.1716394 -4.1556983 -4.1334114 -4.1152649 -4.1053185 -4.1266646 -4.168901 -4.2018218 -4.2262273 -4.250361 -4.2731514 -4.2841244][-4.2112379 -4.2044024 -4.2013888 -4.196876 -4.1793962 -4.1502833 -4.1175022 -4.0931458 -4.1067514 -4.1506028 -4.1941423 -4.2228203 -4.2507877 -4.2760911 -4.2891283][-4.2595987 -4.246417 -4.2307086 -4.2181888 -4.2029371 -4.1720295 -4.12616 -4.0845075 -4.0848689 -4.1257315 -4.180419 -4.2169275 -4.2484527 -4.2757835 -4.2902474][-4.288578 -4.2720079 -4.2529178 -4.2396293 -4.2284207 -4.1970973 -4.1454897 -4.0933232 -4.0815516 -4.115304 -4.172657 -4.2117591 -4.2431073 -4.270534 -4.2860532][-4.28897 -4.2711606 -4.2547736 -4.2467151 -4.241662 -4.2150011 -4.165391 -4.1133566 -4.0951715 -4.1210332 -4.1711392 -4.2031589 -4.2290845 -4.2528248 -4.268558][-4.2765651 -4.2607732 -4.2502761 -4.24559 -4.2443933 -4.2272081 -4.1896925 -4.1477637 -4.1298232 -4.1460447 -4.1802816 -4.1969175 -4.2106328 -4.2276411 -4.2416735][-4.2551904 -4.2442369 -4.24098 -4.2399712 -4.2417812 -4.2347107 -4.2118158 -4.179903 -4.1616788 -4.1674681 -4.1870441 -4.1900506 -4.1934571 -4.2048736 -4.2191539][-4.233459 -4.2261739 -4.2281694 -4.2304029 -4.2301645 -4.226584 -4.2133365 -4.1875138 -4.1692104 -4.1716032 -4.1820531 -4.1774421 -4.1779976 -4.1904116 -4.2078233][-4.2124448 -4.2099838 -4.217906 -4.2238712 -4.2207551 -4.2153096 -4.2062092 -4.1856527 -4.1675582 -4.1654649 -4.1709504 -4.1661296 -4.1672015 -4.1839728 -4.207757][-4.2018247 -4.2069788 -4.2257404 -4.2350569 -4.2282324 -4.2172041 -4.2061906 -4.18702 -4.1658335 -4.1561351 -4.1606112 -4.1630445 -4.1669812 -4.1895528 -4.2172351]]...]
INFO - root - 2017-12-07 17:40:22.858979: step 36110, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 59h:25m:54s remains)
INFO - root - 2017-12-07 17:40:29.744431: step 36120, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 57h:44m:19s remains)
INFO - root - 2017-12-07 17:40:36.577480: step 36130, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 54h:06m:12s remains)
INFO - root - 2017-12-07 17:40:43.452607: step 36140, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 55h:37m:35s remains)
INFO - root - 2017-12-07 17:40:50.419262: step 36150, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 58h:35m:57s remains)
INFO - root - 2017-12-07 17:40:57.251099: step 36160, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.775 sec/batch; 63h:49m:25s remains)
INFO - root - 2017-12-07 17:41:04.107289: step 36170, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 56h:32m:32s remains)
INFO - root - 2017-12-07 17:41:10.917088: step 36180, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 56h:22m:51s remains)
INFO - root - 2017-12-07 17:41:17.750323: step 36190, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 52h:06m:34s remains)
INFO - root - 2017-12-07 17:41:24.573548: step 36200, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 58h:14m:54s remains)
2017-12-07 17:41:25.294414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.268239 -4.2777534 -4.2744236 -4.2509551 -4.1997356 -4.1004729 -3.9707499 -3.8844826 -3.919003 -4.02147 -4.1290174 -4.2198358 -4.2884583 -4.3277264 -4.3448319][-4.2741313 -4.2776742 -4.2685528 -4.2395277 -4.1853828 -4.0799475 -3.9396122 -3.8442075 -3.8824186 -3.9938943 -4.1108866 -4.2094846 -4.2837648 -4.3270764 -4.3460927][-4.2712784 -4.266398 -4.250545 -4.2207065 -4.1747446 -4.0797777 -3.9560573 -3.8749948 -3.9141965 -4.0172024 -4.1258435 -4.2184157 -4.28852 -4.3310261 -4.3481359][-4.2688084 -4.2538991 -4.2312675 -4.2026143 -4.1686 -4.0911317 -3.9916551 -3.9331119 -3.9730232 -4.0651908 -4.1602149 -4.2383823 -4.2980886 -4.3351617 -4.3496122][-4.2686415 -4.2497559 -4.224103 -4.197669 -4.17187 -4.1068473 -4.0226374 -3.9742129 -4.0141659 -4.1001787 -4.1871824 -4.2561736 -4.3070526 -4.3379011 -4.3504763][-4.2687426 -4.253118 -4.2301774 -4.2051024 -4.1777658 -4.1137967 -4.0297804 -3.9809408 -4.0236173 -4.1114182 -4.1981015 -4.2640414 -4.3117275 -4.3393607 -4.3505707][-4.2643414 -4.2546592 -4.2387104 -4.215312 -4.1819148 -4.112134 -4.0211043 -3.9677668 -4.0141335 -4.1054463 -4.1948576 -4.2630739 -4.312571 -4.3402252 -4.3502855][-4.2561264 -4.2526274 -4.2458124 -4.22647 -4.1879239 -4.1107063 -4.0106544 -3.9487326 -3.9946876 -4.0892987 -4.18363 -4.2565746 -4.3097796 -4.3393555 -4.3499804][-4.250495 -4.2510891 -4.2500048 -4.23464 -4.1940713 -4.111486 -4.0031123 -3.9332991 -3.9764361 -4.0724478 -4.1715755 -4.24976 -4.3062811 -4.3381977 -4.3502507][-4.2508221 -4.2516842 -4.2525458 -4.2399259 -4.2003212 -4.1178212 -4.0089211 -3.9382565 -3.9773235 -4.0695605 -4.1683965 -4.2482414 -4.3054066 -4.3374586 -4.3505316][-4.2533464 -4.2540526 -4.2541289 -4.2435923 -4.2080626 -4.1310086 -4.0302291 -3.9656599 -4.0008078 -4.08318 -4.175272 -4.2523561 -4.3075786 -4.3374438 -4.3503962][-4.2515769 -4.2529016 -4.2530155 -4.2456045 -4.2182403 -4.1489425 -4.0561032 -3.9955332 -4.0268641 -4.1004286 -4.1854491 -4.2588625 -4.3113604 -4.3386197 -4.3503261][-4.2404785 -4.2424593 -4.2414012 -4.2352705 -4.2156715 -4.1544213 -4.0654054 -4.0042334 -4.0322409 -4.1040945 -4.18835 -4.2624664 -4.3141413 -4.3397846 -4.3503137][-4.2238259 -4.2261372 -4.221252 -4.2140756 -4.1983476 -4.1417727 -4.0527973 -3.992101 -4.02145 -4.0962291 -4.1853542 -4.2643619 -4.3150196 -4.3398314 -4.3498659][-4.2134657 -4.2166343 -4.2108555 -4.2045159 -4.191371 -4.1378379 -4.0497608 -3.9939885 -4.0273728 -4.1023378 -4.1912394 -4.2698565 -4.3171029 -4.3389735 -4.3482461]]...]
INFO - root - 2017-12-07 17:41:31.686985: step 36210, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.677 sec/batch; 55h:44m:06s remains)
INFO - root - 2017-12-07 17:41:38.599065: step 36220, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 60h:20m:09s remains)
INFO - root - 2017-12-07 17:41:45.469973: step 36230, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 58h:02m:52s remains)
INFO - root - 2017-12-07 17:41:52.442584: step 36240, loss = 2.11, batch loss = 2.05 (11.1 examples/sec; 0.720 sec/batch; 59h:16m:13s remains)
INFO - root - 2017-12-07 17:41:59.198463: step 36250, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 52h:24m:32s remains)
INFO - root - 2017-12-07 17:42:06.006143: step 36260, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 53h:28m:07s remains)
INFO - root - 2017-12-07 17:42:12.828840: step 36270, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 56h:25m:25s remains)
INFO - root - 2017-12-07 17:42:19.726077: step 36280, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 58h:45m:26s remains)
INFO - root - 2017-12-07 17:42:26.600075: step 36290, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 56h:27m:27s remains)
INFO - root - 2017-12-07 17:42:33.286143: step 36300, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 52h:56m:10s remains)
2017-12-07 17:42:34.023655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3313222 -4.3250031 -4.31949 -4.3144169 -4.3095589 -4.3055878 -4.3031445 -4.301446 -4.3027239 -4.3038383 -4.304338 -4.3049574 -4.3064141 -4.3080511 -4.3147268][-4.3198104 -4.3065443 -4.2957864 -4.2868042 -4.2788568 -4.2708707 -4.2646637 -4.2601757 -4.2606421 -4.258954 -4.260726 -4.2646055 -4.2711926 -4.2784348 -4.2934446][-4.301434 -4.2793393 -4.2620335 -4.2491856 -4.2384119 -4.2269373 -4.2134442 -4.203177 -4.2017069 -4.1984668 -4.2032776 -4.2142019 -4.2305646 -4.2457433 -4.2699151][-4.2812481 -4.2511082 -4.2265387 -4.2092957 -4.1981606 -4.1849279 -4.161128 -4.1409574 -4.1404438 -4.1447721 -4.1582012 -4.1784658 -4.2074552 -4.2315264 -4.2597818][-4.2594943 -4.222806 -4.1922979 -4.1734953 -4.1639438 -4.1474977 -4.1110606 -4.0855827 -4.0993896 -4.1205482 -4.1394815 -4.1603613 -4.1974468 -4.2310519 -4.2583032][-4.2373657 -4.1958957 -4.1612082 -4.14143 -4.1289883 -4.1036415 -4.0511427 -4.0227966 -4.0610404 -4.1077685 -4.1288743 -4.1354218 -4.1675477 -4.2104521 -4.2382164][-4.2127271 -4.1644812 -4.1241369 -4.1018982 -4.0830221 -4.04219 -3.9664779 -3.933774 -4.00069 -4.0775862 -4.1099482 -4.1053686 -4.1298127 -4.1812892 -4.214108][-4.1877594 -4.13193 -4.0853252 -4.0597496 -4.0331593 -3.9738264 -3.8690877 -3.8273077 -3.9287584 -4.0399876 -4.0912485 -4.0824842 -4.1006231 -4.1566005 -4.197433][-4.1772795 -4.1211247 -4.0749569 -4.0496469 -4.0235891 -3.9636388 -3.8577094 -3.8213835 -3.9351161 -4.0609188 -4.1171708 -4.1068268 -4.1174479 -4.1685548 -4.209373][-4.1878934 -4.1402683 -4.1032867 -4.0822 -4.0646086 -4.0282021 -3.9648368 -3.9464836 -4.0309296 -4.1340714 -4.1828184 -4.17855 -4.1869564 -4.2267041 -4.2567177][-4.2106705 -4.1744885 -4.1474991 -4.1307096 -4.1201077 -4.1061616 -4.0803614 -4.08143 -4.1388974 -4.2130985 -4.250989 -4.2487235 -4.2603145 -4.2909851 -4.3089528][-4.2385988 -4.2138128 -4.1945276 -4.1809163 -4.176353 -4.1751018 -4.1708813 -4.18021 -4.2195644 -4.2694154 -4.2953897 -4.2940793 -4.3085256 -4.3344078 -4.3442006][-4.2658677 -4.2489471 -4.232842 -4.2207236 -4.2207265 -4.2289772 -4.2363343 -4.2461591 -4.2669606 -4.2915711 -4.3067632 -4.3113542 -4.3292761 -4.3511267 -4.3560081][-4.2914386 -4.2798357 -4.2665405 -4.256372 -4.2570014 -4.2671633 -4.2773156 -4.2839365 -4.2903357 -4.2966309 -4.29989 -4.3052745 -4.3240218 -4.3437815 -4.3498855][-4.3129869 -4.3060732 -4.297761 -4.2896996 -4.2872477 -4.2919741 -4.2971144 -4.299264 -4.2985473 -4.2974834 -4.2963285 -4.2999496 -4.3132534 -4.3274727 -4.3349433]]...]
INFO - root - 2017-12-07 17:42:40.582217: step 36310, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 54h:54m:23s remains)
INFO - root - 2017-12-07 17:42:47.367590: step 36320, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 54h:22m:59s remains)
INFO - root - 2017-12-07 17:42:54.218858: step 36330, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 55h:26m:39s remains)
INFO - root - 2017-12-07 17:43:01.213774: step 36340, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 59h:23m:45s remains)
INFO - root - 2017-12-07 17:43:08.066117: step 36350, loss = 2.11, batch loss = 2.05 (10.9 examples/sec; 0.732 sec/batch; 60h:10m:43s remains)
INFO - root - 2017-12-07 17:43:14.910146: step 36360, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 55h:12m:59s remains)
INFO - root - 2017-12-07 17:43:21.624251: step 36370, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 52h:01m:07s remains)
INFO - root - 2017-12-07 17:43:28.509301: step 36380, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 55h:27m:00s remains)
INFO - root - 2017-12-07 17:43:35.399785: step 36390, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 57h:58m:31s remains)
INFO - root - 2017-12-07 17:43:42.216328: step 36400, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 60h:02m:56s remains)
2017-12-07 17:43:42.912861: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1985612 -4.15399 -4.1363211 -4.1515222 -4.1799307 -4.2013454 -4.2161565 -4.2152119 -4.1905928 -4.1534052 -4.1157565 -4.0960932 -4.1074572 -4.1489491 -4.1904116][-4.2076936 -4.1667242 -4.1482358 -4.1584077 -4.1802611 -4.1960239 -4.2070422 -4.2130437 -4.2055006 -4.1837869 -4.1522856 -4.1221013 -4.1176643 -4.1492863 -4.1883125][-4.2237587 -4.1924176 -4.1767535 -4.1792789 -4.1851821 -4.1865926 -4.1905394 -4.2038918 -4.2183371 -4.2216215 -4.2095151 -4.1885066 -4.1801829 -4.1982183 -4.223835][-4.2386312 -4.2198057 -4.2099867 -4.2038383 -4.1866374 -4.16312 -4.1540422 -4.1719618 -4.2048488 -4.2306376 -4.2394972 -4.2377768 -4.2370505 -4.2492247 -4.2685251][-4.2473807 -4.2397342 -4.2365975 -4.2227688 -4.1832471 -4.1294312 -4.0959077 -4.1057005 -4.1504345 -4.1957192 -4.2198162 -4.2312875 -4.2379026 -4.250463 -4.2722154][-4.2475953 -4.2453723 -4.24766 -4.2291708 -4.174099 -4.0948744 -4.0323563 -4.0218644 -4.0713487 -4.1311212 -4.1615119 -4.1721034 -4.1810141 -4.1986456 -4.2290025][-4.2420712 -4.2395253 -4.2444797 -4.2274666 -4.1668386 -4.0773983 -3.9939756 -3.9569042 -3.9920902 -4.0498343 -4.077116 -4.076602 -4.0762477 -4.0951977 -4.1396155][-4.2404242 -4.233345 -4.2404413 -4.2317648 -4.1816463 -4.1041174 -4.0267377 -3.9781239 -3.986042 -4.0239239 -4.0393381 -4.03078 -4.0214882 -4.0332646 -4.0786085][-4.2482319 -4.2366395 -4.2448545 -4.2460136 -4.2167373 -4.1663752 -4.1160784 -4.079349 -4.07315 -4.0893464 -4.0946565 -4.0854659 -4.0771756 -4.0843472 -4.11368][-4.2634411 -4.2497239 -4.256494 -4.2648487 -4.255403 -4.2315083 -4.2069182 -4.1858206 -4.176939 -4.1835623 -4.1881781 -4.1844578 -4.1833534 -4.1882615 -4.2020788][-4.2687097 -4.2535524 -4.2596169 -4.274694 -4.2789392 -4.2715034 -4.2635851 -4.2517319 -4.2390633 -4.2395382 -4.2465076 -4.2493935 -4.2528019 -4.2569656 -4.2624588][-4.2566319 -4.2397671 -4.2468958 -4.2675538 -4.2807879 -4.2801347 -4.27732 -4.2667217 -4.2475872 -4.2415185 -4.2477403 -4.2551246 -4.2615895 -4.2634459 -4.2643342][-4.2332582 -4.2110715 -4.2119846 -4.2334218 -4.2535338 -4.2603312 -4.2593179 -4.2484245 -4.2266488 -4.2180209 -4.2222195 -4.2294021 -4.2355957 -4.2368803 -4.2357445][-4.2160463 -4.1821766 -4.1725445 -4.189321 -4.2117352 -4.2254663 -4.2273126 -4.2169714 -4.1968684 -4.187448 -4.1876359 -4.1934867 -4.2012224 -4.2031937 -4.200562][-4.2176108 -4.1737785 -4.1554928 -4.1669436 -4.189374 -4.205719 -4.2106948 -4.2062716 -4.1933675 -4.184268 -4.1785774 -4.1791639 -4.1860743 -4.188724 -4.18607]]...]
INFO - root - 2017-12-07 17:43:49.548259: step 36410, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 53h:28m:57s remains)
INFO - root - 2017-12-07 17:43:56.358484: step 36420, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 59h:44m:33s remains)
INFO - root - 2017-12-07 17:44:03.190437: step 36430, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 59h:59m:25s remains)
INFO - root - 2017-12-07 17:44:09.940627: step 36440, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 54h:56m:21s remains)
INFO - root - 2017-12-07 17:44:16.720377: step 36450, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 54h:04m:54s remains)
INFO - root - 2017-12-07 17:44:23.560391: step 36460, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 54h:24m:18s remains)
INFO - root - 2017-12-07 17:44:30.270463: step 36470, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 57h:49m:38s remains)
INFO - root - 2017-12-07 17:44:37.022432: step 36480, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 57h:35m:03s remains)
INFO - root - 2017-12-07 17:44:43.837239: step 36490, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 53h:40m:22s remains)
INFO - root - 2017-12-07 17:44:50.537668: step 36500, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 53h:34m:21s remains)
2017-12-07 17:44:51.249807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24016 -4.25559 -4.279974 -4.3007827 -4.3120146 -4.3073072 -4.2819624 -4.2516971 -4.2429113 -4.2534347 -4.2783895 -4.30095 -4.3199277 -4.3323631 -4.3377461][-4.2274876 -4.2480493 -4.276917 -4.30053 -4.3160563 -4.3141451 -4.2909102 -4.2617688 -4.2529178 -4.2613635 -4.283205 -4.3034072 -4.3204832 -4.3315749 -4.3360205][-4.2095776 -4.2313485 -4.2619071 -4.2865663 -4.3036275 -4.3033967 -4.2824068 -4.256238 -4.2510214 -4.2626371 -4.2847733 -4.3038769 -4.3196535 -4.3302503 -4.3340259][-4.1778131 -4.1965775 -4.2273116 -4.2507057 -4.2660065 -4.2654891 -4.2434497 -4.2210841 -4.2256513 -4.2479095 -4.2746735 -4.2952089 -4.3128009 -4.3260322 -4.3301663][-4.136157 -4.1498418 -4.179503 -4.2042022 -4.2176957 -4.2103457 -4.1788707 -4.1567993 -4.1761656 -4.2134352 -4.2461205 -4.2700748 -4.2948947 -4.3141103 -4.321352][-4.095305 -4.0988922 -4.1236539 -4.1483364 -4.1606579 -4.1440363 -4.0983214 -4.0740342 -4.1103144 -4.16379 -4.2041974 -4.2357621 -4.2710176 -4.2977915 -4.30797][-4.0613918 -4.0529037 -4.0681181 -4.0908823 -4.1019874 -4.0753293 -4.0155878 -3.9878032 -4.0423427 -4.1165705 -4.1701322 -4.2114778 -4.25605 -4.2865949 -4.2967272][-4.0491848 -4.0320048 -4.0378947 -4.0541434 -4.0558777 -4.0189791 -3.955591 -3.9332547 -4.0004306 -4.0878162 -4.15483 -4.2083406 -4.2591934 -4.2899494 -4.2972803][-4.0885487 -4.072587 -4.0725307 -4.078537 -4.0687461 -4.0317488 -3.9853241 -3.9754717 -4.0329003 -4.1088758 -4.1722255 -4.2271752 -4.2760377 -4.3032465 -4.3063154][-4.1499043 -4.139905 -4.1389256 -4.1344843 -4.1168942 -4.0915627 -4.0705905 -4.0677886 -4.1011543 -4.147511 -4.1930971 -4.2398558 -4.2827387 -4.3072929 -4.3104239][-4.18059 -4.1740112 -4.1717672 -4.1590905 -4.14033 -4.1280189 -4.1245737 -4.1231413 -4.1353388 -4.1591716 -4.1923985 -4.2336268 -4.2733331 -4.2983832 -4.3058167][-4.1756277 -4.169127 -4.1634274 -4.1484065 -4.1329889 -4.1300831 -4.1318989 -4.1259346 -4.1280031 -4.1434069 -4.1739264 -4.2146215 -4.254549 -4.2809649 -4.2948141][-4.1417017 -4.1347141 -4.1284728 -4.1143365 -4.1003265 -4.0987873 -4.0997376 -4.0928473 -4.0945473 -4.1122494 -4.146625 -4.1915417 -4.2355967 -4.2663684 -4.2860246][-4.101696 -4.0967178 -4.0954356 -4.084404 -4.0697064 -4.0658412 -4.0663538 -4.0614462 -4.0676479 -4.0919285 -4.1335559 -4.1841488 -4.2301855 -4.2632446 -4.2846551][-4.0980773 -4.0965161 -4.1004181 -4.0953374 -4.0854173 -4.0832462 -4.084794 -4.0805693 -4.0896635 -4.1161757 -4.1584339 -4.2077465 -4.2475367 -4.2745352 -4.2917323]]...]
INFO - root - 2017-12-07 17:44:57.841079: step 36510, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.689 sec/batch; 56h:37m:23s remains)
INFO - root - 2017-12-07 17:45:04.356865: step 36520, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 53h:15m:23s remains)
INFO - root - 2017-12-07 17:45:11.164606: step 36530, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 54h:32m:07s remains)
INFO - root - 2017-12-07 17:45:18.003774: step 36540, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 56h:55m:44s remains)
INFO - root - 2017-12-07 17:45:24.786629: step 36550, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 58h:44m:47s remains)
INFO - root - 2017-12-07 17:45:31.614238: step 36560, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 53h:46m:12s remains)
INFO - root - 2017-12-07 17:45:38.493140: step 36570, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 53h:44m:13s remains)
INFO - root - 2017-12-07 17:45:45.318214: step 36580, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 60h:35m:16s remains)
INFO - root - 2017-12-07 17:45:52.082724: step 36590, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 56h:59m:40s remains)
INFO - root - 2017-12-07 17:45:58.845769: step 36600, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 55h:17m:13s remains)
2017-12-07 17:45:59.545098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2897835 -4.2766919 -4.2746878 -4.2811527 -4.2912521 -4.2996311 -4.3044305 -4.3077888 -4.3115516 -4.3154297 -4.3181095 -4.3194275 -4.3190794 -4.3183804 -4.3196425][-4.2557664 -4.23574 -4.233604 -4.2445579 -4.2615085 -4.2741485 -4.2790084 -4.2828856 -4.2913518 -4.3024654 -4.3108807 -4.3153262 -4.3149457 -4.312644 -4.3134279][-4.2151804 -4.1894093 -4.1878171 -4.2046742 -4.2319293 -4.2506852 -4.2531962 -4.2525015 -4.2638073 -4.282618 -4.2991147 -4.3098726 -4.3121009 -4.3088908 -4.3069282][-4.1833072 -4.1569428 -4.1571741 -4.1792784 -4.2137032 -4.234457 -4.2298594 -4.2190495 -4.2293916 -4.2548456 -4.2776446 -4.29436 -4.3004375 -4.29688 -4.2928576][-4.1700191 -4.1437454 -4.1436291 -4.1657143 -4.1965017 -4.2056413 -4.1837497 -4.159956 -4.1701555 -4.2059727 -4.2403617 -4.26734 -4.2808356 -4.2792892 -4.2743406][-4.1768403 -4.1513753 -4.1455641 -4.1556015 -4.1681032 -4.1515489 -4.1020088 -4.06106 -4.0759621 -4.1284089 -4.1808023 -4.2209382 -4.2439842 -4.2491693 -4.2483072][-4.1927176 -4.1685762 -4.1571274 -4.1522846 -4.1389103 -4.0887527 -4.0024548 -3.9393313 -3.9653873 -4.0433702 -4.1161804 -4.1713223 -4.2055945 -4.21829 -4.223166][-4.2075186 -4.1869769 -4.1764355 -4.165534 -4.1326175 -4.0524764 -3.9272275 -3.8405652 -3.8822112 -3.9819331 -4.06562 -4.1300182 -4.1767039 -4.1987805 -4.2089472][-4.2141433 -4.1990151 -4.196074 -4.1924682 -4.1607018 -4.08103 -3.9638991 -3.8920271 -3.9297342 -4.0078425 -4.06695 -4.1209707 -4.1696677 -4.1969786 -4.2099929][-4.2245107 -4.2153487 -4.21751 -4.2199206 -4.1992917 -4.14187 -4.0614605 -4.0188017 -4.045054 -4.0917654 -4.1252551 -4.1593003 -4.1943483 -4.2163324 -4.2278152][-4.2487888 -4.24379 -4.2464123 -4.2488155 -4.2381148 -4.2014422 -4.1513524 -4.1295919 -4.1497025 -4.1765518 -4.1943417 -4.2138472 -4.2357893 -4.2484574 -4.2547655][-4.2807417 -4.2787514 -4.280057 -4.2784081 -4.2684011 -4.2422338 -4.2088451 -4.1982946 -4.2145705 -4.2308869 -4.2426081 -4.2570434 -4.27049 -4.2772236 -4.2778482][-4.3088388 -4.3092833 -4.30983 -4.3058271 -4.2942305 -4.2730875 -4.2482586 -4.2388678 -4.2469034 -4.2561579 -4.2657495 -4.2800536 -4.29052 -4.2944212 -4.2936926][-4.3270955 -4.32769 -4.3277612 -4.3248472 -4.3163137 -4.3024964 -4.2871776 -4.2785077 -4.2787013 -4.2807441 -4.2852974 -4.2951779 -4.3039589 -4.3078442 -4.3087163][-4.3362207 -4.3362174 -4.3364687 -4.3354797 -4.3313808 -4.3249478 -4.3184209 -4.3135624 -4.3112421 -4.3092217 -4.3090811 -4.3118277 -4.3141809 -4.3148112 -4.3161416]]...]
INFO - root - 2017-12-07 17:46:06.130985: step 36610, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 59h:48m:12s remains)
INFO - root - 2017-12-07 17:46:12.807682: step 36620, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 56h:20m:23s remains)
INFO - root - 2017-12-07 17:46:19.647489: step 36630, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.671 sec/batch; 55h:09m:40s remains)
INFO - root - 2017-12-07 17:46:26.479087: step 36640, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 52h:58m:32s remains)
INFO - root - 2017-12-07 17:46:33.337379: step 36650, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 57h:13m:03s remains)
INFO - root - 2017-12-07 17:46:40.190137: step 36660, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 57h:58m:04s remains)
INFO - root - 2017-12-07 17:46:46.977240: step 36670, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 57h:30m:30s remains)
INFO - root - 2017-12-07 17:46:53.653083: step 36680, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.623 sec/batch; 51h:13m:56s remains)
INFO - root - 2017-12-07 17:47:00.445220: step 36690, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 55h:26m:49s remains)
INFO - root - 2017-12-07 17:47:07.151715: step 36700, loss = 2.02, batch loss = 1.96 (11.3 examples/sec; 0.705 sec/batch; 57h:56m:33s remains)
2017-12-07 17:47:07.895786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2817769 -4.2629833 -4.2548141 -4.2521281 -4.2570891 -4.2662764 -4.27143 -4.2744403 -4.2763219 -4.2749758 -4.2710919 -4.2705727 -4.2762084 -4.2768736 -4.2736588][-4.2567482 -4.23215 -4.2210922 -4.2187653 -4.2259378 -4.2379436 -4.2437191 -4.2482209 -4.253655 -4.2552438 -4.2527466 -4.2522354 -4.2588673 -4.2574315 -4.2496114][-4.2326088 -4.20454 -4.1894526 -4.1883125 -4.1979027 -4.2087059 -4.2073507 -4.2058043 -4.2126703 -4.2198353 -4.2220116 -4.2243676 -4.2330894 -4.23139 -4.2216253][-4.2108531 -4.1802173 -4.1649623 -4.1661987 -4.1789522 -4.1822419 -4.1641884 -4.1505833 -4.1544118 -4.1658692 -4.17299 -4.1833076 -4.2000737 -4.2025251 -4.1936846][-4.1826334 -4.1556654 -4.1417208 -4.1457047 -4.1604333 -4.1493406 -4.1108046 -4.082377 -4.0817862 -4.1021528 -4.1197143 -4.1423378 -4.1688123 -4.17603 -4.1714053][-4.1651039 -4.1442609 -4.1311831 -4.1308746 -4.1327324 -4.0984135 -4.0413818 -4.0036569 -4.007762 -4.0425525 -4.0740538 -4.1060863 -4.1408858 -4.1532178 -4.154048][-4.1658964 -4.1494503 -4.131371 -4.1154656 -4.08759 -4.022192 -3.9541121 -3.9211676 -3.9398561 -3.9977365 -4.047389 -4.0885396 -4.1282773 -4.147017 -4.1532354][-4.19127 -4.1774817 -4.1479297 -4.1050968 -4.03969 -3.9501181 -3.8836589 -3.8666165 -3.9017391 -3.9781041 -4.0421672 -4.089447 -4.1318979 -4.1528358 -4.162168][-4.23883 -4.2268519 -4.1860566 -4.1226282 -4.0368352 -3.9397087 -3.8811338 -3.8828385 -3.9322295 -4.0098815 -4.0691047 -4.1093988 -4.1490569 -4.1729026 -4.1866794][-4.2895536 -4.2828236 -4.239481 -4.1706386 -4.0855036 -3.9993858 -3.9589951 -3.9815836 -4.0366082 -4.0983138 -4.1367278 -4.1657534 -4.199543 -4.2206297 -4.2325096][-4.3137193 -4.3108969 -4.2749596 -4.2137151 -4.1408124 -4.0711727 -4.0462332 -4.0761166 -4.125906 -4.1726656 -4.1974111 -4.2174859 -4.2420206 -4.2599564 -4.26837][-4.3124418 -4.3092694 -4.2828484 -4.236371 -4.1830969 -4.133604 -4.1204572 -4.1513457 -4.19522 -4.2291579 -4.240911 -4.249495 -4.2636857 -4.2773 -4.2816854][-4.302352 -4.296567 -4.2753553 -4.2433939 -4.2102833 -4.1842914 -4.18198 -4.2117376 -4.2467747 -4.2687483 -4.2705703 -4.266871 -4.2683764 -4.2760882 -4.2763667][-4.2889495 -4.2824984 -4.2680168 -4.2521467 -4.2406054 -4.2330089 -4.2357326 -4.2584367 -4.2801423 -4.2901487 -4.28454 -4.2746143 -4.2693257 -4.2721162 -4.2716374][-4.2812166 -4.2770414 -4.2703424 -4.268342 -4.2715573 -4.2708631 -4.2702541 -4.2825661 -4.2946243 -4.2984934 -4.2927213 -4.2838058 -4.2763624 -4.2763581 -4.276588]]...]
INFO - root - 2017-12-07 17:47:14.393029: step 36710, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 54h:09m:11s remains)
INFO - root - 2017-12-07 17:47:21.183576: step 36720, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 55h:05m:29s remains)
INFO - root - 2017-12-07 17:47:27.932015: step 36730, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 55h:31m:03s remains)
INFO - root - 2017-12-07 17:47:34.775776: step 36740, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 59h:02m:58s remains)
INFO - root - 2017-12-07 17:47:41.527927: step 36750, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 55h:58m:26s remains)
INFO - root - 2017-12-07 17:47:48.346358: step 36760, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 53h:48m:54s remains)
INFO - root - 2017-12-07 17:47:55.129046: step 36770, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.613 sec/batch; 50h:20m:00s remains)
INFO - root - 2017-12-07 17:48:01.878517: step 36780, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 58h:34m:27s remains)
INFO - root - 2017-12-07 17:48:08.660486: step 36790, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 56h:32m:01s remains)
INFO - root - 2017-12-07 17:48:15.440576: step 36800, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 58h:32m:36s remains)
2017-12-07 17:48:16.118145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702084 -4.2494617 -4.2293997 -4.22771 -4.2474027 -4.276639 -4.2957778 -4.2959614 -4.2843246 -4.2686939 -4.2454 -4.2193704 -4.1886859 -4.1657739 -4.1664114][-4.241456 -4.212079 -4.1890483 -4.1956377 -4.2255015 -4.2587471 -4.2769012 -4.2730956 -4.2599053 -4.2445388 -4.2190804 -4.1860757 -4.1407094 -4.1060405 -4.1144018][-4.2030482 -4.1693058 -4.1506572 -4.1696005 -4.208374 -4.2397647 -4.2473354 -4.2339773 -4.2196822 -4.2071018 -4.1832614 -4.1460996 -4.0922494 -4.0522027 -4.070776][-4.1831317 -4.1497293 -4.1360559 -4.158988 -4.1941404 -4.2094207 -4.1912889 -4.1608243 -4.1515656 -4.1559072 -4.1487327 -4.1229672 -4.0739865 -4.0344229 -4.0573192][-4.199306 -4.1726089 -4.163126 -4.1739192 -4.1796942 -4.1542873 -4.0934963 -4.0383687 -4.04315 -4.0837231 -4.1166053 -4.1248775 -4.1045775 -4.0785 -4.09937][-4.2159796 -4.1918163 -4.1802416 -4.1688647 -4.1341128 -4.0540361 -3.9283874 -3.8311758 -3.8688636 -3.9788013 -4.0718131 -4.1254449 -4.1419277 -4.1347303 -4.1511631][-4.2191668 -4.1946 -4.1793089 -4.1505761 -4.089736 -3.9681127 -3.7764816 -3.6242743 -3.7030559 -3.8904839 -4.0361848 -4.1235981 -4.1616788 -4.1667342 -4.182797][-4.2141061 -4.195025 -4.185195 -4.1559215 -4.096601 -3.9849031 -3.7995181 -3.6491089 -3.7298906 -3.9137754 -4.0525088 -4.1377754 -4.1756783 -4.1847935 -4.198895][-4.2050028 -4.1942687 -4.1975355 -4.1826792 -4.1456265 -4.0782084 -3.9568856 -3.8562326 -3.9054377 -4.027781 -4.1134515 -4.1649218 -4.184072 -4.1881847 -4.2000213][-4.1728535 -4.1725125 -4.1920447 -4.1976576 -4.1901374 -4.1638379 -4.0963774 -4.0395751 -4.0711246 -4.1418471 -4.1759486 -4.185514 -4.1736517 -4.1602569 -4.1672511][-4.1508803 -4.155612 -4.1841855 -4.2056131 -4.2208862 -4.2234597 -4.1926932 -4.1677127 -4.1852989 -4.2118282 -4.2077026 -4.1846833 -4.1465049 -4.1160345 -4.1201491][-4.161407 -4.1702313 -4.2006407 -4.2298875 -4.2546749 -4.2701173 -4.2598786 -4.250205 -4.255218 -4.2477264 -4.215404 -4.1743088 -4.1277113 -4.0912771 -4.0957975][-4.1888046 -4.1959572 -4.2207813 -4.2463541 -4.2709942 -4.2916565 -4.2939305 -4.2916355 -4.2891383 -4.2645645 -4.2213845 -4.1775384 -4.1383843 -4.1092844 -4.1156635][-4.2184038 -4.2206478 -4.2369714 -4.2538729 -4.2719345 -4.2920713 -4.3026371 -4.3055558 -4.2989583 -4.2725544 -4.2363906 -4.203939 -4.1783037 -4.1594062 -4.1624365][-4.2545781 -4.25294 -4.2604084 -4.2677717 -4.2768393 -4.2904773 -4.3015909 -4.3077927 -4.3040533 -4.2855968 -4.2622929 -4.2421546 -4.2263942 -4.2131433 -4.2087984]]...]
INFO - root - 2017-12-07 17:48:22.721593: step 36810, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.675 sec/batch; 55h:26m:08s remains)
INFO - root - 2017-12-07 17:48:29.494832: step 36820, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 58h:39m:48s remains)
INFO - root - 2017-12-07 17:48:36.027010: step 36830, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 56h:54m:42s remains)
INFO - root - 2017-12-07 17:48:42.877234: step 36840, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 54h:24m:00s remains)
INFO - root - 2017-12-07 17:48:49.711125: step 36850, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 58h:26m:30s remains)
INFO - root - 2017-12-07 17:48:56.585074: step 36860, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.744 sec/batch; 61h:07m:47s remains)
INFO - root - 2017-12-07 17:49:03.300060: step 36870, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 59h:11m:06s remains)
INFO - root - 2017-12-07 17:49:10.070198: step 36880, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 53h:22m:40s remains)
INFO - root - 2017-12-07 17:49:16.828155: step 36890, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 51h:37m:10s remains)
INFO - root - 2017-12-07 17:49:23.676358: step 36900, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 56h:43m:26s remains)
2017-12-07 17:49:24.512630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3084121 -4.3016114 -4.2901168 -4.2823915 -4.2853112 -4.289144 -4.2868342 -4.2788315 -4.2654371 -4.2405281 -4.2186065 -4.2098327 -4.2201395 -4.2352772 -4.2565227][-4.3082647 -4.3054109 -4.2948413 -4.2839146 -4.2813597 -4.2813072 -4.2749543 -4.2659559 -4.2583947 -4.2480116 -4.2452617 -4.2517171 -4.2624593 -4.2691269 -4.2800069][-4.2925553 -4.297883 -4.2930317 -4.2811494 -4.2714915 -4.2641096 -4.2531905 -4.2456746 -4.2498775 -4.2526207 -4.2620335 -4.2749109 -4.2879739 -4.288 -4.28719][-4.2718039 -4.2808008 -4.2789268 -4.2670679 -4.2529588 -4.2409358 -4.2284827 -4.2251072 -4.2401028 -4.2523675 -4.2630224 -4.2735491 -4.2838645 -4.2787304 -4.2749662][-4.2347116 -4.2455325 -4.2491875 -4.2401404 -4.2271533 -4.2139812 -4.2008724 -4.2006536 -4.2199745 -4.2329249 -4.2381592 -4.2443042 -4.2506704 -4.2395096 -4.2384868][-4.1823115 -4.1830969 -4.1844273 -4.1760588 -4.1650624 -4.1529832 -4.1394005 -4.1461258 -4.1687803 -4.1836352 -4.1936293 -4.2070041 -4.2123442 -4.1940947 -4.1910434][-4.1485467 -4.1247811 -4.1085734 -4.0907531 -4.0814266 -4.0739193 -4.0634003 -4.07839 -4.111733 -4.1405 -4.1647034 -4.1865487 -4.1907649 -4.1696467 -4.1638122][-4.1658535 -4.1182618 -4.0799112 -4.0494981 -4.0352373 -4.0284395 -4.02004 -4.0417671 -4.0895867 -4.1357803 -4.1739106 -4.2007308 -4.2043915 -4.18259 -4.1702914][-4.2050014 -4.1492939 -4.1021075 -4.0716596 -4.0643334 -4.0671296 -4.0651751 -4.0879445 -4.1356611 -4.1837192 -4.2187123 -4.2353492 -4.2328472 -4.2124958 -4.1960683][-4.2286782 -4.1752229 -4.1245265 -4.0973077 -4.0996628 -4.1137896 -4.1232295 -4.144269 -4.1853871 -4.2239928 -4.2445884 -4.242301 -4.2311144 -4.2157564 -4.2060885][-4.2399726 -4.1953936 -4.1430192 -4.1083932 -4.1027555 -4.1123238 -4.1231503 -4.1418457 -4.1786489 -4.2108631 -4.2193022 -4.2030282 -4.1843529 -4.1746097 -4.1776376][-4.2464972 -4.2097325 -4.1558 -4.1094136 -4.0848031 -4.0789881 -4.0848346 -4.1014695 -4.1341429 -4.1624551 -4.1656804 -4.1453452 -4.1232038 -4.1172957 -4.1324205][-4.2522559 -4.2237997 -4.1752262 -4.1258674 -4.0901151 -4.0721617 -4.0725055 -4.0858378 -4.1151323 -4.138608 -4.1384382 -4.1178303 -4.0956173 -4.09041 -4.1077228][-4.253799 -4.2377534 -4.2045064 -4.1691012 -4.1412148 -4.125473 -4.1249027 -4.1306958 -4.1494446 -4.1676116 -4.1671629 -4.1520963 -4.137929 -4.1347175 -4.1444707][-4.2518916 -4.2523813 -4.2396159 -4.2214088 -4.2064285 -4.1973634 -4.1938219 -4.1918941 -4.2013593 -4.2155132 -4.2180028 -4.2135596 -4.2136774 -4.2164054 -4.2195387]]...]
INFO - root - 2017-12-07 17:49:31.054790: step 36910, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 55h:40m:04s remains)
INFO - root - 2017-12-07 17:49:37.782608: step 36920, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:47m:59s remains)
INFO - root - 2017-12-07 17:49:44.555477: step 36930, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 56h:16m:12s remains)
INFO - root - 2017-12-07 17:49:51.435125: step 36940, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 57h:42m:16s remains)
INFO - root - 2017-12-07 17:49:58.285435: step 36950, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 57h:43m:17s remains)
INFO - root - 2017-12-07 17:50:05.111564: step 36960, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 54h:15m:26s remains)
INFO - root - 2017-12-07 17:50:11.800396: step 36970, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 53h:43m:20s remains)
INFO - root - 2017-12-07 17:50:18.508940: step 36980, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 53h:53m:31s remains)
INFO - root - 2017-12-07 17:50:25.312653: step 36990, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 58h:42m:34s remains)
INFO - root - 2017-12-07 17:50:32.167804: step 37000, loss = 2.03, batch loss = 1.97 (10.8 examples/sec; 0.744 sec/batch; 61h:02m:18s remains)
2017-12-07 17:50:32.867732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.279202 -4.2191224 -4.1691332 -4.1439862 -4.1312041 -4.136622 -4.158318 -4.1701488 -4.1705446 -4.174232 -4.1850929 -4.2044859 -4.2304311 -4.2457709 -4.2400637][-4.2697496 -4.2233896 -4.1913009 -4.1853642 -4.1840434 -4.1842752 -4.1907253 -4.1836414 -4.1626339 -4.146008 -4.1442275 -4.1656213 -4.2052226 -4.2347918 -4.242671][-4.2575793 -4.2198439 -4.1999369 -4.2070203 -4.2117214 -4.2061162 -4.1986041 -4.17741 -4.1429467 -4.1144838 -4.1041141 -4.1252065 -4.1718082 -4.207242 -4.225234][-4.2323322 -4.1915317 -4.1781855 -4.1970396 -4.2117157 -4.2078938 -4.1914692 -4.16497 -4.1259656 -4.0865917 -4.0577049 -4.0643778 -4.11292 -4.1623211 -4.1954622][-4.2147937 -4.1598005 -4.1365142 -4.156044 -4.1808333 -4.1854792 -4.1696849 -4.14328 -4.0988016 -4.0403624 -3.9853809 -3.9739914 -4.030714 -4.1108651 -4.1755118][-4.2122116 -4.1452546 -4.1012263 -4.1036038 -4.1222081 -4.1286545 -4.1180668 -4.0960679 -4.0582027 -3.9984739 -3.9322875 -3.9115989 -3.9715037 -4.0741968 -4.1628995][-4.2201381 -4.1471848 -4.0835571 -4.0591693 -4.0564127 -4.0527506 -4.045825 -4.0423088 -4.0284991 -3.9882035 -3.9323146 -3.9071822 -3.9560134 -4.0571227 -4.1514606][-4.2320065 -4.1558266 -4.0810823 -4.0389171 -4.0255451 -4.0238891 -4.0276589 -4.046979 -4.0576248 -4.0379577 -3.9979522 -3.9737306 -4.0003204 -4.0719647 -4.148469][-4.2556381 -4.1815934 -4.1070571 -4.0687218 -4.0626764 -4.06719 -4.0820789 -4.1144233 -4.1378202 -4.1349063 -4.1091146 -4.0856833 -4.0901709 -4.1262689 -4.169805][-4.2726188 -4.2131677 -4.1539121 -4.1301084 -4.1318545 -4.1399922 -4.1572151 -4.1870136 -4.2114639 -4.2187004 -4.2016096 -4.1778059 -4.16788 -4.1759305 -4.1927328][-4.2846613 -4.2442446 -4.2051797 -4.1923947 -4.1971259 -4.2058163 -4.2186189 -4.2412848 -4.2636614 -4.2730007 -4.2620625 -4.2407389 -4.2235193 -4.2178121 -4.2221117][-4.2949896 -4.2677364 -4.2435751 -4.2378759 -4.2434554 -4.252439 -4.2624674 -4.2786431 -4.2952938 -4.3018069 -4.29679 -4.28269 -4.2652116 -4.2531729 -4.2512922][-4.30446 -4.285028 -4.2723351 -4.2716346 -4.2781477 -4.2843642 -4.2898912 -4.2983966 -4.3080459 -4.3132968 -4.3147864 -4.3092937 -4.29695 -4.2870431 -4.2834187][-4.3089285 -4.2944574 -4.2893715 -4.2921429 -4.2979689 -4.3000927 -4.299902 -4.3027105 -4.3083038 -4.3124433 -4.315434 -4.3146777 -4.3087969 -4.30415 -4.3021426][-4.3091969 -4.300096 -4.2999568 -4.3035207 -4.3059807 -4.3059225 -4.3048515 -4.3060188 -4.3086095 -4.31044 -4.3110681 -4.310246 -4.307435 -4.3046918 -4.3020191]]...]
INFO - root - 2017-12-07 17:50:39.299757: step 37010, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.628 sec/batch; 51h:30m:47s remains)
INFO - root - 2017-12-07 17:50:46.220519: step 37020, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 60h:04m:20s remains)
INFO - root - 2017-12-07 17:50:53.058109: step 37030, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 55h:41m:53s remains)
INFO - root - 2017-12-07 17:50:59.933442: step 37040, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 54h:00m:13s remains)
INFO - root - 2017-12-07 17:51:06.754370: step 37050, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 54h:00m:40s remains)
INFO - root - 2017-12-07 17:51:13.526797: step 37060, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 55h:47m:27s remains)
INFO - root - 2017-12-07 17:51:20.406769: step 37070, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.711 sec/batch; 58h:22m:35s remains)
INFO - root - 2017-12-07 17:51:27.232824: step 37080, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 54h:13m:16s remains)
INFO - root - 2017-12-07 17:51:33.972168: step 37090, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 52h:47m:54s remains)
INFO - root - 2017-12-07 17:51:40.760591: step 37100, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 55h:19m:56s remains)
2017-12-07 17:51:41.482902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.10053 -4.1063633 -4.1518555 -4.2085752 -4.2458539 -4.2509704 -4.2401438 -4.213563 -4.1687169 -4.1213636 -4.1055288 -4.1332703 -4.1930704 -4.2415204 -4.2630615][-4.1828456 -4.1802058 -4.1990018 -4.2194138 -4.2221808 -4.2053852 -4.191184 -4.1849823 -4.1743488 -4.1598425 -4.1612296 -4.1880803 -4.2379556 -4.2769027 -4.2902937][-4.2345986 -4.226459 -4.2210188 -4.2057962 -4.1730132 -4.1345787 -4.1222072 -4.1412635 -4.1711683 -4.1939096 -4.2127476 -4.238369 -4.275032 -4.30294 -4.3117065][-4.2604461 -4.2431655 -4.2151151 -4.1733813 -4.1140928 -4.05612 -4.043901 -4.083065 -4.1475787 -4.2024908 -4.235106 -4.2561669 -4.2774858 -4.2917833 -4.2972655][-4.27691 -4.24933 -4.2054634 -4.1460552 -4.0704436 -3.998708 -3.9801042 -4.0281582 -4.115202 -4.1952019 -4.2376823 -4.2501574 -4.2528152 -4.2494926 -4.2476683][-4.2894459 -4.2590237 -4.2099133 -4.143661 -4.0636053 -3.986042 -3.9520245 -3.9931915 -4.088037 -4.18589 -4.2408624 -4.2493963 -4.2343836 -4.2110682 -4.1983743][-4.2993402 -4.2725682 -4.2270346 -4.1618371 -4.0826669 -4.0031681 -3.9546356 -3.9773955 -4.0647535 -4.1692467 -4.2373705 -4.253067 -4.2332225 -4.1988182 -4.1789865][-4.2970881 -4.2760372 -4.2372923 -4.1777735 -4.1036382 -4.0303617 -3.9787445 -3.9816294 -4.0501575 -4.1480932 -4.2238007 -4.2497144 -4.2344742 -4.2028394 -4.1848497][-4.2905555 -4.2755275 -4.2472353 -4.1997313 -4.1380506 -4.0757251 -4.0242777 -4.0051427 -4.0414915 -4.1177888 -4.1916652 -4.228179 -4.225358 -4.206708 -4.1983433][-4.2817054 -4.26869 -4.2526956 -4.2253127 -4.1833687 -4.1338816 -4.0826764 -4.0421553 -4.0445814 -4.0887742 -4.1477847 -4.1883392 -4.1997337 -4.2013645 -4.20896][-4.2676764 -4.2568979 -4.2514806 -4.2446823 -4.2251496 -4.191649 -4.1449637 -4.0923843 -4.0649934 -4.0779715 -4.1179419 -4.156486 -4.1795316 -4.2003193 -4.2230959][-4.2474542 -4.2381454 -4.2399426 -4.2459664 -4.2441964 -4.226264 -4.1896296 -4.1388659 -4.1001983 -4.0947156 -4.1191692 -4.1526766 -4.1809182 -4.2133584 -4.2453837][-4.2308044 -4.2232513 -4.2260108 -4.2358031 -4.2431283 -4.2358642 -4.2105675 -4.17317 -4.141016 -4.1339 -4.1505008 -4.1750584 -4.20115 -4.2349916 -4.2683883][-4.2347097 -4.22985 -4.2308526 -4.2366037 -4.2436633 -4.2410579 -4.2236295 -4.1983976 -4.177897 -4.1758714 -4.1901441 -4.2088971 -4.2282314 -4.2544713 -4.2828093][-4.2547746 -4.2539306 -4.2537322 -4.2551451 -4.258204 -4.255805 -4.2433596 -4.2264352 -4.2141519 -4.2148352 -4.226707 -4.240828 -4.2537422 -4.2703905 -4.2920232]]...]
INFO - root - 2017-12-07 17:51:48.084423: step 37110, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 54h:53m:25s remains)
INFO - root - 2017-12-07 17:51:54.976664: step 37120, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 53h:00m:43s remains)
INFO - root - 2017-12-07 17:52:01.803091: step 37130, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 54h:02m:06s remains)
INFO - root - 2017-12-07 17:52:08.409418: step 37140, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 58h:38m:03s remains)
INFO - root - 2017-12-07 17:52:15.277971: step 37150, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 57h:28m:56s remains)
INFO - root - 2017-12-07 17:52:21.963204: step 37160, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 51h:01m:50s remains)
INFO - root - 2017-12-07 17:52:28.754834: step 37170, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 54h:00m:45s remains)
INFO - root - 2017-12-07 17:52:35.556862: step 37180, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 57h:17m:21s remains)
INFO - root - 2017-12-07 17:52:42.318470: step 37190, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 58h:14m:40s remains)
INFO - root - 2017-12-07 17:52:49.102692: step 37200, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.687 sec/batch; 56h:19m:22s remains)
2017-12-07 17:52:49.805412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28055 -4.2705917 -4.2632742 -4.26168 -4.2724648 -4.288569 -4.292202 -4.2854996 -4.2783847 -4.2749734 -4.2769942 -4.2805238 -4.2820492 -4.2805104 -4.2818036][-4.235692 -4.2170186 -4.2042232 -4.2024307 -4.2207408 -4.2468729 -4.2529244 -4.24563 -4.2380157 -4.233407 -4.2343974 -4.2378273 -4.2377834 -4.2361331 -4.241797][-4.2017088 -4.1765614 -4.160913 -4.1617813 -4.1873283 -4.2212095 -4.2280211 -4.2191358 -4.2098269 -4.2033877 -4.2033482 -4.2054524 -4.2032213 -4.20038 -4.2091856][-4.19105 -4.1616573 -4.1469822 -4.1523151 -4.1811304 -4.2144637 -4.2176437 -4.2063117 -4.1963396 -4.1909213 -4.1918635 -4.1931572 -4.1894097 -4.1851425 -4.1946893][-4.1959987 -4.1682825 -4.159143 -4.1681256 -4.1913309 -4.21485 -4.2133331 -4.2020135 -4.1942425 -4.19089 -4.1945825 -4.1959834 -4.1909575 -4.1857419 -4.194283][-4.1967263 -4.1748767 -4.1728292 -4.185874 -4.2028165 -4.2159929 -4.2112083 -4.2024927 -4.1984816 -4.1982694 -4.2042603 -4.204771 -4.1966662 -4.1908069 -4.1979566][-4.184947 -4.1658535 -4.1650624 -4.1795492 -4.1958394 -4.2065911 -4.2007961 -4.1933165 -4.19464 -4.2012067 -4.2087398 -4.2090368 -4.2013197 -4.1948185 -4.2000923][-4.1685166 -4.1513858 -4.1470785 -4.1582794 -4.1761866 -4.1914272 -4.187356 -4.180531 -4.1856818 -4.1987567 -4.2071753 -4.206378 -4.1994877 -4.1927204 -4.1967382][-4.1680565 -4.1544785 -4.1477437 -4.1540384 -4.1715579 -4.1909051 -4.1899881 -4.1837716 -4.1885457 -4.1998219 -4.2060533 -4.2040553 -4.1974945 -4.1920624 -4.1955824][-4.1923342 -4.1833124 -4.1770115 -4.1800504 -4.1940742 -4.2111893 -4.211628 -4.2062945 -4.2079158 -4.2132797 -4.2159886 -4.2125707 -4.2060213 -4.2001171 -4.2039137][-4.2317395 -4.2260079 -4.2221136 -4.2223334 -4.2304425 -4.2415128 -4.2415819 -4.2353954 -4.2330875 -4.2353849 -4.2360048 -4.2326655 -4.2254543 -4.2184024 -4.2221937][-4.2750311 -4.2714148 -4.2685518 -4.2666879 -4.2701139 -4.275526 -4.274456 -4.2675328 -4.2630229 -4.2641191 -4.264842 -4.2626591 -4.2552614 -4.2483139 -4.2505856][-4.3093987 -4.3074808 -4.304924 -4.3015633 -4.3008413 -4.3013821 -4.3003669 -4.2960567 -4.2938662 -4.2957964 -4.2986336 -4.2980003 -4.2918992 -4.2859674 -4.2856951][-4.329505 -4.328516 -4.3258142 -4.3214388 -4.318172 -4.3167171 -4.3164392 -4.3155203 -4.315897 -4.3190613 -4.3226075 -4.3224859 -4.3183312 -4.3136158 -4.3115773][-4.3356075 -4.3352008 -4.3326635 -4.3290997 -4.3259826 -4.32467 -4.3248143 -4.3257904 -4.3275304 -4.3304353 -4.33336 -4.3336711 -4.3314781 -4.3283834 -4.3264232]]...]
INFO - root - 2017-12-07 17:52:56.423092: step 37210, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 58h:17m:51s remains)
INFO - root - 2017-12-07 17:53:03.290835: step 37220, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 57h:28m:00s remains)
INFO - root - 2017-12-07 17:53:10.139769: step 37230, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 56h:14m:15s remains)
INFO - root - 2017-12-07 17:53:16.973723: step 37240, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 54h:50m:17s remains)
INFO - root - 2017-12-07 17:53:23.799860: step 37250, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 57h:39m:55s remains)
INFO - root - 2017-12-07 17:53:30.618893: step 37260, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 59h:39m:36s remains)
INFO - root - 2017-12-07 17:53:37.429390: step 37270, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 54h:08m:18s remains)
INFO - root - 2017-12-07 17:53:44.204702: step 37280, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 53h:02m:48s remains)
INFO - root - 2017-12-07 17:53:51.042571: step 37290, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 55h:33m:19s remains)
INFO - root - 2017-12-07 17:53:57.788071: step 37300, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 55h:20m:13s remains)
2017-12-07 17:53:58.483646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3415174 -4.3391933 -4.3390317 -4.3379622 -4.3391151 -4.3437328 -4.3469934 -4.3483553 -4.3456964 -4.342134 -4.3386593 -4.3352013 -4.3336835 -4.3338785 -4.3352809][-4.3306837 -4.3203559 -4.3166313 -4.3144193 -4.3195786 -4.3280673 -4.3337564 -4.3342118 -4.3281636 -4.321486 -4.3169155 -4.3126478 -4.3131075 -4.3164849 -4.3200297][-4.2951965 -4.2760177 -4.2686524 -4.2653065 -4.2723804 -4.2814407 -4.2871675 -4.2847071 -4.2753615 -4.2679305 -4.2630978 -4.2613177 -4.2691026 -4.2811384 -4.2905154][-4.2331347 -4.2068667 -4.2025337 -4.2026992 -4.2028651 -4.1993651 -4.1945357 -4.1843209 -4.16983 -4.1635289 -4.1681714 -4.1834483 -4.2083855 -4.2299848 -4.2456493][-4.1651425 -4.13477 -4.1367126 -4.1378608 -4.1218185 -4.08916 -4.0624313 -4.0441217 -4.0248957 -4.0275187 -4.0558672 -4.1021457 -4.1495366 -4.1801968 -4.2017493][-4.1086531 -4.0816565 -4.0967259 -4.1016407 -4.0604205 -3.9895189 -3.9404683 -3.9202385 -3.9102812 -3.9266756 -3.9712586 -4.0390511 -4.1044927 -4.1446996 -4.17684][-4.087029 -4.0691504 -4.0911779 -4.0875864 -4.0207081 -3.9271283 -3.871022 -3.8574791 -3.8641586 -3.8961458 -3.9427624 -4.0179086 -4.0948796 -4.1439519 -4.18242][-4.1142497 -4.1019464 -4.1177859 -4.1067133 -4.0381036 -3.9573977 -3.9138911 -3.9029875 -3.9115863 -3.9400187 -3.9805248 -4.0517011 -4.1286931 -4.181334 -4.2190261][-4.1783552 -4.1673441 -4.1750588 -4.1640563 -4.1183281 -4.0677748 -4.0424285 -4.0252542 -4.0182023 -4.0334744 -4.0672607 -4.1293397 -4.1975 -4.2350836 -4.2642369][-4.2447705 -4.2378693 -4.2414627 -4.2345643 -4.2049689 -4.1708117 -4.15306 -4.1337404 -4.1204643 -4.1321197 -4.158216 -4.2106171 -4.2647958 -4.2862892 -4.3034768][-4.29126 -4.28914 -4.2890263 -4.2814894 -4.2541962 -4.2299 -4.2199068 -4.2097931 -4.2072535 -4.2206049 -4.2409625 -4.2780404 -4.3136845 -4.3260255 -4.3333082][-4.3155017 -4.3165736 -4.3152146 -4.3033571 -4.2767849 -4.2573061 -4.2562294 -4.2560983 -4.2647934 -4.2839532 -4.3024077 -4.3224893 -4.3441329 -4.3515429 -4.3530545][-4.3311186 -4.3365297 -4.3336549 -4.3184576 -4.29186 -4.2740226 -4.2761726 -4.2833061 -4.2990303 -4.3203545 -4.3393173 -4.353828 -4.366478 -4.37003 -4.36729][-4.346724 -4.3532667 -4.346045 -4.3266449 -4.2975035 -4.279521 -4.281013 -4.291482 -4.3093495 -4.3301816 -4.3522725 -4.3677559 -4.3737054 -4.3733687 -4.3691068][-4.3552375 -4.3598776 -4.3496885 -4.3285 -4.3021021 -4.2819242 -4.2765126 -4.2860675 -4.3031416 -4.3234386 -4.3480086 -4.3653879 -4.3698053 -4.3657608 -4.3603396]]...]
INFO - root - 2017-12-07 17:54:04.924985: step 37310, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 51h:21m:12s remains)
INFO - root - 2017-12-07 17:54:11.856343: step 37320, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 55h:26m:47s remains)
INFO - root - 2017-12-07 17:54:18.639512: step 37330, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 55h:13m:38s remains)
INFO - root - 2017-12-07 17:54:25.476851: step 37340, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 58h:25m:52s remains)
INFO - root - 2017-12-07 17:54:32.305991: step 37350, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 52h:19m:35s remains)
INFO - root - 2017-12-07 17:54:39.123022: step 37360, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 53h:23m:06s remains)
INFO - root - 2017-12-07 17:54:45.896713: step 37370, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 56h:00m:21s remains)
INFO - root - 2017-12-07 17:54:52.766512: step 37380, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 56h:22m:37s remains)
INFO - root - 2017-12-07 17:54:59.568174: step 37390, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:42m:26s remains)
INFO - root - 2017-12-07 17:55:06.264684: step 37400, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 51h:19m:13s remains)
2017-12-07 17:55:07.031759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2798533 -4.2765226 -4.2654891 -4.2568631 -4.2524991 -4.24766 -4.2444849 -4.2509813 -4.2694459 -4.2822542 -4.28268 -4.274087 -4.2605834 -4.2571073 -4.2717967][-4.2790508 -4.2801962 -4.26588 -4.2468357 -4.2311563 -4.2184529 -4.2112131 -4.2177591 -4.2448788 -4.2684503 -4.2758207 -4.2740564 -4.2632675 -4.2569027 -4.26891][-4.2800827 -4.2876368 -4.2710028 -4.2378812 -4.2048154 -4.1810293 -4.1652431 -4.1658492 -4.2019167 -4.2407012 -4.2585082 -4.262125 -4.2547293 -4.2415028 -4.2417555][-4.2808242 -4.2996922 -4.2910476 -4.2515512 -4.2027874 -4.1606326 -4.1252728 -4.1115551 -4.1563849 -4.21701 -4.2474461 -4.2561316 -4.249011 -4.225718 -4.2067008][-4.2705178 -4.2963171 -4.3013124 -4.2693868 -4.2139525 -4.1485028 -4.0791082 -4.0413132 -4.0926414 -4.1835804 -4.2390919 -4.2578483 -4.2513766 -4.219451 -4.1828728][-4.2458882 -4.275516 -4.2931204 -4.27412 -4.2203212 -4.1332517 -4.0164552 -3.9308186 -3.985512 -4.1222119 -4.2168822 -4.2549291 -4.2548962 -4.2232184 -4.18027][-4.2027431 -4.2412109 -4.2726049 -4.268364 -4.2204528 -4.1170797 -3.952177 -3.7990823 -3.8468425 -4.0370007 -4.1742091 -4.2347426 -4.2489071 -4.2260728 -4.1875024][-4.1587725 -4.20196 -4.2486153 -4.2589688 -4.2246313 -4.1322508 -3.9615836 -3.7702219 -3.7936788 -3.9991868 -4.1478853 -4.2186332 -4.2430258 -4.2302752 -4.1999111][-4.1351981 -4.1721616 -4.2272372 -4.2549558 -4.2468338 -4.193934 -4.075336 -3.9254627 -3.9165483 -4.0532146 -4.1598811 -4.2133312 -4.2422104 -4.2428064 -4.224905][-4.1219692 -4.1462674 -4.2072911 -4.2529411 -4.2712345 -4.2576351 -4.1970572 -4.1019731 -4.0767245 -4.1349683 -4.1778746 -4.2019005 -4.2281313 -4.2428918 -4.2430363][-4.123209 -4.1331263 -4.192194 -4.2495341 -4.2830319 -4.2898459 -4.2633224 -4.2032628 -4.1757679 -4.1859403 -4.1772985 -4.1704855 -4.1884856 -4.2081919 -4.2253313][-4.1559153 -4.1541824 -4.197392 -4.2490439 -4.2829857 -4.2957182 -4.2820697 -4.2424769 -4.21519 -4.2003036 -4.1640892 -4.1364264 -4.1433787 -4.159193 -4.1807818][-4.2138348 -4.2048383 -4.2214346 -4.2482233 -4.270627 -4.2832108 -4.2780466 -4.2533474 -4.2272038 -4.2015371 -4.1572275 -4.1227908 -4.1232405 -4.1308446 -4.1443205][-4.2483926 -4.2363462 -4.23073 -4.23319 -4.2440386 -4.2598476 -4.2653623 -4.2561874 -4.2352057 -4.2068577 -4.1668434 -4.1407685 -4.1397805 -4.1415834 -4.1362548][-4.2437224 -4.2283559 -4.2123318 -4.2070012 -4.2156062 -4.2394133 -4.2558956 -4.2572608 -4.2430105 -4.2211337 -4.191155 -4.1776962 -4.1797614 -4.1788406 -4.1610708]]...]
INFO - root - 2017-12-07 17:55:13.639410: step 37410, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 59h:47m:09s remains)
INFO - root - 2017-12-07 17:55:20.516898: step 37420, loss = 2.09, batch loss = 2.04 (11.4 examples/sec; 0.699 sec/batch; 57h:17m:44s remains)
INFO - root - 2017-12-07 17:55:27.381243: step 37430, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 54h:48m:55s remains)
INFO - root - 2017-12-07 17:55:34.117564: step 37440, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 56h:38m:54s remains)
INFO - root - 2017-12-07 17:55:40.735720: step 37450, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 54h:24m:50s remains)
INFO - root - 2017-12-07 17:55:47.408507: step 37460, loss = 2.04, batch loss = 1.99 (12.9 examples/sec; 0.621 sec/batch; 50h:53m:07s remains)
INFO - root - 2017-12-07 17:55:54.114756: step 37470, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 54h:17m:52s remains)
INFO - root - 2017-12-07 17:56:00.969143: step 37480, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.697 sec/batch; 57h:05m:56s remains)
INFO - root - 2017-12-07 17:56:07.839041: step 37490, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.745 sec/batch; 61h:02m:50s remains)
INFO - root - 2017-12-07 17:56:14.662554: step 37500, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 52h:48m:18s remains)
2017-12-07 17:56:15.415116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.129775 -4.1127911 -4.1042447 -4.1165686 -4.1370444 -4.1552734 -4.169045 -4.1783924 -4.1836424 -4.1820407 -4.1837525 -4.1879735 -4.1820955 -4.1699018 -4.1646366][-4.1237831 -4.10576 -4.1010361 -4.1171026 -4.1462946 -4.171978 -4.1856923 -4.1878085 -4.1887078 -4.1830277 -4.182106 -4.1883135 -4.1799879 -4.1694317 -4.1669064][-4.1341515 -4.114696 -4.1055784 -4.1194677 -4.1515923 -4.1806874 -4.1939707 -4.1902633 -4.1843624 -4.1784019 -4.1791668 -4.1888471 -4.1848612 -4.1765294 -4.1703534][-4.1328735 -4.1113586 -4.1020808 -4.1201234 -4.1561718 -4.180006 -4.1825576 -4.1675482 -4.1583028 -4.1585608 -4.1726251 -4.1963582 -4.205277 -4.2028894 -4.193759][-4.1182513 -4.1004157 -4.1052365 -4.1324754 -4.1627789 -4.1672244 -4.1474056 -4.1191387 -4.1090903 -4.1221089 -4.1583872 -4.2066464 -4.2319403 -4.23607 -4.224546][-4.1108637 -4.1013594 -4.1202774 -4.1437278 -4.1545858 -4.1270952 -4.0748386 -4.0265689 -4.0202646 -4.062604 -4.1330009 -4.20382 -4.23897 -4.24422 -4.2319527][-4.1153917 -4.1157417 -4.1398082 -4.1535516 -4.1385794 -4.0807557 -4.001657 -3.9393883 -3.9467278 -4.0217328 -4.113265 -4.1869974 -4.2236915 -4.2310462 -4.2210383][-4.1218886 -4.1376991 -4.1653862 -4.1742878 -4.1455092 -4.0781059 -3.9948978 -3.9360781 -3.9549327 -4.037262 -4.1218634 -4.1848831 -4.2177262 -4.2256274 -4.2167759][-4.1425004 -4.1713076 -4.2004447 -4.2079234 -4.1810503 -4.1257219 -4.0595956 -4.0131865 -4.0247703 -4.0881214 -4.1548953 -4.2017703 -4.2248096 -4.2279587 -4.2185521][-4.1838632 -4.2134833 -4.2341733 -4.2361379 -4.212944 -4.1728253 -4.1275425 -4.0949173 -4.101378 -4.1483436 -4.1970286 -4.2253108 -4.234436 -4.2299089 -4.2176008][-4.2100167 -4.2278514 -4.2372613 -4.2355051 -4.2184253 -4.1938782 -4.1683235 -4.1544237 -4.1657887 -4.2008362 -4.2309537 -4.2400928 -4.2375216 -4.2277803 -4.2145581][-4.2216816 -4.2246146 -4.22324 -4.2196884 -4.2113023 -4.2025514 -4.1940503 -4.1943278 -4.2083573 -4.2302179 -4.2423134 -4.2365789 -4.2273622 -4.2207823 -4.215508][-4.2201152 -4.2121186 -4.2061343 -4.2069516 -4.2106972 -4.2175884 -4.2222252 -4.2258177 -4.231319 -4.2388458 -4.2400613 -4.2330022 -4.2285037 -4.2307472 -4.2338924][-4.193038 -4.1727409 -4.1644392 -4.1772709 -4.2016792 -4.2287416 -4.2474031 -4.2508588 -4.2439418 -4.2371988 -4.2329755 -4.2305942 -4.2349715 -4.245132 -4.2526078][-4.1432414 -4.111289 -4.1036448 -4.1348505 -4.1867952 -4.2377777 -4.2697082 -4.2740541 -4.259563 -4.2450972 -4.2366524 -4.2338104 -4.2419906 -4.2558131 -4.2658753]]...]
INFO - root - 2017-12-07 17:56:22.006993: step 37510, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 57h:12m:42s remains)
INFO - root - 2017-12-07 17:56:28.788243: step 37520, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 58h:04m:48s remains)
INFO - root - 2017-12-07 17:56:35.529977: step 37530, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 55h:39m:56s remains)
INFO - root - 2017-12-07 17:56:42.357070: step 37540, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 54h:32m:23s remains)
INFO - root - 2017-12-07 17:56:49.058758: step 37550, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 52h:06m:21s remains)
INFO - root - 2017-12-07 17:56:55.848262: step 37560, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 56h:00m:41s remains)
INFO - root - 2017-12-07 17:57:02.576001: step 37570, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 54h:39m:30s remains)
INFO - root - 2017-12-07 17:57:09.365205: step 37580, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 58h:27m:13s remains)
INFO - root - 2017-12-07 17:57:16.076478: step 37590, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 56h:32m:57s remains)
INFO - root - 2017-12-07 17:57:22.843357: step 37600, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.623 sec/batch; 51h:01m:26s remains)
2017-12-07 17:57:23.634831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2105932 -4.2377787 -4.2491512 -4.2319708 -4.1955023 -4.1557703 -4.1338634 -4.1457372 -4.179184 -4.2276778 -4.2675824 -4.2890687 -4.3018446 -4.3122606 -4.3255215][-4.1964989 -4.2192869 -4.2220106 -4.1911407 -4.1496096 -4.1166167 -4.1096373 -4.133996 -4.1754713 -4.2253556 -4.2619228 -4.2812028 -4.2948694 -4.3084579 -4.3247361][-4.2288713 -4.2395115 -4.2307792 -4.185462 -4.1338949 -4.09493 -4.0879197 -4.114078 -4.15742 -4.2064981 -4.2445455 -4.2675605 -4.2843184 -4.3023548 -4.3225965][-4.282517 -4.285903 -4.2702961 -4.2167959 -4.1566548 -4.0998192 -4.0664392 -4.0777526 -4.1186714 -4.1709952 -4.21726 -4.24982 -4.2763457 -4.3006597 -4.3234978][-4.3160558 -4.3187585 -4.2999959 -4.2510505 -4.193574 -4.1220741 -4.0561104 -4.0452552 -4.0794883 -4.1366706 -4.1963053 -4.2393479 -4.2760868 -4.3057256 -4.3272262][-4.3236723 -4.3258462 -4.3076167 -4.2706485 -4.227993 -4.1545224 -4.0702381 -4.0428057 -4.0675616 -4.1267958 -4.1939034 -4.2426672 -4.2839556 -4.3130174 -4.33102][-4.3124609 -4.3138776 -4.2946482 -4.265058 -4.2347717 -4.1683459 -4.090807 -4.0636725 -4.0849295 -4.14277 -4.208693 -4.2556005 -4.2941504 -4.3186712 -4.3335352][-4.29937 -4.2985468 -4.2777328 -4.2534943 -4.2302494 -4.1730351 -4.1106853 -4.0934796 -4.114255 -4.1682096 -4.2273054 -4.26726 -4.2999825 -4.3202462 -4.333498][-4.2853131 -4.2792282 -4.25794 -4.239594 -4.2244515 -4.176825 -4.1287937 -4.1209316 -4.1408935 -4.19178 -4.242866 -4.2752361 -4.301558 -4.3182349 -4.3314495][-4.2598281 -4.2435632 -4.2193208 -4.2028465 -4.1918516 -4.1545796 -4.123354 -4.1274509 -4.15127 -4.2020111 -4.2494483 -4.2792974 -4.3015752 -4.3162827 -4.3296194][-4.2282052 -4.1982021 -4.162837 -4.1361513 -4.1171579 -4.0883493 -4.0816588 -4.1057496 -4.1438379 -4.2027254 -4.2512279 -4.2822671 -4.303266 -4.3164067 -4.3294182][-4.1857238 -4.1444468 -4.0983391 -4.0579925 -4.0278664 -4.00997 -4.0335779 -4.0837674 -4.1402197 -4.2056484 -4.2555394 -4.2872858 -4.3077331 -4.31898 -4.3308682][-4.1494474 -4.1125264 -4.0710435 -4.0263171 -3.9896069 -3.9800694 -4.0236821 -4.0904717 -4.1547771 -4.218061 -4.2650056 -4.293468 -4.3119769 -4.321393 -4.33169][-4.1420245 -4.1214066 -4.1011386 -4.0641718 -4.0281754 -4.0216327 -4.0631838 -4.1250606 -4.1804709 -4.2339926 -4.27364 -4.2972088 -4.313971 -4.3224373 -4.3321772][-4.1659374 -4.1632257 -4.1625972 -4.1340294 -4.099586 -4.0888906 -4.1115432 -4.1563153 -4.199398 -4.2441177 -4.2774291 -4.2965803 -4.3121557 -4.3206491 -4.3316903]]...]
INFO - root - 2017-12-07 17:57:30.238460: step 37610, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.732 sec/batch; 59h:55m:52s remains)
INFO - root - 2017-12-07 17:57:36.960009: step 37620, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 55h:59m:10s remains)
INFO - root - 2017-12-07 17:57:43.781979: step 37630, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 56h:23m:27s remains)
INFO - root - 2017-12-07 17:57:50.658978: step 37640, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 55h:48m:32s remains)
INFO - root - 2017-12-07 17:57:57.582587: step 37650, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 59h:29m:17s remains)
INFO - root - 2017-12-07 17:58:04.413132: step 37660, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 58h:01m:54s remains)
INFO - root - 2017-12-07 17:58:11.235459: step 37670, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 53h:03m:56s remains)
INFO - root - 2017-12-07 17:58:17.940830: step 37680, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 53h:32m:10s remains)
INFO - root - 2017-12-07 17:58:24.712672: step 37690, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 54h:48m:06s remains)
INFO - root - 2017-12-07 17:58:31.532973: step 37700, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 57h:49m:26s remains)
2017-12-07 17:58:32.233898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2339373 -4.2324815 -4.2396564 -4.2487407 -4.2521057 -4.2550812 -4.2594433 -4.2630262 -4.2660785 -4.2666678 -4.2632537 -4.2632084 -4.2718186 -4.2776642 -4.2731109][-4.2398357 -4.2450795 -4.2522888 -4.2556553 -4.2524457 -4.2491174 -4.2484574 -4.2499061 -4.2530251 -4.2561574 -4.2559333 -4.2564931 -4.2660122 -4.2737031 -4.2712545][-4.2424717 -4.2490511 -4.2525249 -4.246418 -4.2323847 -4.2193422 -4.2118254 -4.2076378 -4.2092805 -4.2159123 -4.2232528 -4.2271519 -4.236702 -4.2442036 -4.240715][-4.2237196 -4.2242756 -4.2223721 -4.2076163 -4.1841736 -4.1626158 -4.1513205 -4.1441236 -4.1458092 -4.1550789 -4.1666188 -4.1740828 -4.1832089 -4.1900077 -4.1852274][-4.1844535 -4.1743689 -4.1677217 -4.1486754 -4.12221 -4.0953693 -4.0817895 -4.0723186 -4.0742035 -4.0865245 -4.1063933 -4.1191888 -4.1295485 -4.1342039 -4.1289454][-4.13702 -4.1166887 -4.1066236 -4.0881624 -4.0626473 -4.0305882 -4.0092025 -3.9940889 -3.9929512 -4.011878 -4.0433445 -4.0683923 -4.0888968 -4.09235 -4.0815172][-4.1012688 -4.0779409 -4.0692968 -4.0544267 -4.0286784 -3.9891903 -3.95018 -3.9204907 -3.9176068 -3.9506 -3.9908504 -4.0302553 -4.0672321 -4.0704975 -4.054728][-4.0876603 -4.0674539 -4.0653062 -4.0547347 -4.02976 -3.9854331 -3.9287982 -3.8824079 -3.8766727 -3.9198771 -3.9656038 -4.0127387 -4.0591111 -4.06101 -4.0408545][-4.1014109 -4.0860152 -4.0894761 -4.0840211 -4.0650768 -4.0260048 -3.9651108 -3.9105003 -3.8999128 -3.9436007 -3.9892144 -4.0330849 -4.0732756 -4.0695219 -4.0435534][-4.1315823 -4.1218438 -4.1285877 -4.1289682 -4.1208677 -4.094471 -4.0410142 -3.9847629 -3.9674339 -4.0054984 -4.0492511 -4.0857596 -4.1047182 -4.085186 -4.0535035][-4.1598315 -4.1543269 -4.165442 -4.1744227 -4.179996 -4.1674838 -4.1262646 -4.0716019 -4.0435109 -4.0704103 -4.1060085 -4.1249814 -4.1174297 -4.0871449 -4.05483][-4.1748605 -4.1717992 -4.186893 -4.2045007 -4.2182035 -4.2185555 -4.1906419 -4.1404266 -4.1007776 -4.1021194 -4.1204753 -4.1274228 -4.1097527 -4.0765691 -4.0426865][-4.1851573 -4.1816053 -4.1976881 -4.2214775 -4.2403436 -4.24603 -4.226851 -4.1840825 -4.1391978 -4.1175518 -4.1156769 -4.113636 -4.0941439 -4.0631928 -4.0308013][-4.1971235 -4.1910009 -4.2025881 -4.225152 -4.2449508 -4.251915 -4.2413359 -4.2137508 -4.1751795 -4.142837 -4.127614 -4.1193075 -4.1033435 -4.08025 -4.0531244][-4.2147818 -4.2050214 -4.2079434 -4.2213569 -4.2366581 -4.2440481 -4.2408476 -4.226655 -4.2004681 -4.171999 -4.15897 -4.1527448 -4.1458473 -4.1351929 -4.1161938]]...]
INFO - root - 2017-12-07 17:58:38.811001: step 37710, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 55h:16m:48s remains)
INFO - root - 2017-12-07 17:58:45.703886: step 37720, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 54h:02m:46s remains)
INFO - root - 2017-12-07 17:58:52.566085: step 37730, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 58h:31m:39s remains)
INFO - root - 2017-12-07 17:58:59.401710: step 37740, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 56h:32m:38s remains)
INFO - root - 2017-12-07 17:59:06.229723: step 37750, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 56h:52m:51s remains)
INFO - root - 2017-12-07 17:59:12.831327: step 37760, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 52h:46m:53s remains)
INFO - root - 2017-12-07 17:59:19.622561: step 37770, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 56h:50m:27s remains)
INFO - root - 2017-12-07 17:59:26.420492: step 37780, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 55h:40m:06s remains)
INFO - root - 2017-12-07 17:59:33.173478: step 37790, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 53h:39m:40s remains)
INFO - root - 2017-12-07 17:59:39.837426: step 37800, loss = 2.05, batch loss = 2.00 (13.1 examples/sec; 0.611 sec/batch; 49h:58m:56s remains)
2017-12-07 17:59:40.539154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1920466 -4.1914558 -4.186152 -4.1569915 -4.1114688 -4.069694 -4.0527272 -4.0704222 -4.120717 -4.1715093 -4.19552 -4.194943 -4.1926866 -4.1879759 -4.1863103][-4.1767154 -4.1805382 -4.1826539 -4.1631403 -4.1263142 -4.0877552 -4.07155 -4.0900097 -4.1367407 -4.1789808 -4.1932693 -4.185256 -4.1778073 -4.1719351 -4.17414][-4.172718 -4.18054 -4.1895823 -4.1852717 -4.1647453 -4.1355615 -4.1203232 -4.1318126 -4.1631761 -4.1891761 -4.1916342 -4.1776366 -4.167984 -4.1617274 -4.1624842][-4.1692958 -4.1828823 -4.1997414 -4.2093434 -4.2055254 -4.1868386 -4.1681089 -4.1638651 -4.1753416 -4.1868324 -4.1819415 -4.1678038 -4.1602631 -4.1547089 -4.1527877][-4.1659241 -4.184063 -4.2055693 -4.22377 -4.2276206 -4.2087045 -4.1781955 -4.1587315 -4.1599026 -4.1695237 -4.1663432 -4.1590691 -4.1571674 -4.1514668 -4.1459284][-4.1770358 -4.1931915 -4.2089534 -4.2252393 -4.2268257 -4.2006712 -4.1573024 -4.1264672 -4.1271653 -4.1470923 -4.1548157 -4.1544647 -4.1574988 -4.1505866 -4.1384254][-4.2057714 -4.2101493 -4.2133064 -4.2218986 -4.2196708 -4.1848445 -4.1263471 -4.07835 -4.075666 -4.1111941 -4.1355038 -4.14396 -4.1502886 -4.1423264 -4.1266904][-4.2245407 -4.2121663 -4.2026353 -4.2044435 -4.1994405 -4.1607 -4.0878243 -4.0186133 -4.0097189 -4.0645905 -4.1122894 -4.1364784 -4.1466908 -4.1344943 -4.1161389][-4.2233672 -4.19548 -4.1747441 -4.1707711 -4.1644592 -4.1297779 -4.0564332 -3.974237 -3.9617715 -4.0367122 -4.1059208 -4.1447353 -4.158133 -4.144227 -4.1270752][-4.2118206 -4.1743541 -4.1449838 -4.1374173 -4.1353426 -4.1160121 -4.0667944 -4.0066962 -4.0042381 -4.0748048 -4.1375852 -4.1724453 -4.1804233 -4.1585951 -4.1365266][-4.1959395 -4.1609468 -4.1327662 -4.1262016 -4.13085 -4.1288738 -4.1090188 -4.082221 -4.0870805 -4.1343403 -4.1751447 -4.1966052 -4.1948662 -4.1650424 -4.1363683][-4.1870942 -4.168438 -4.1510053 -4.14628 -4.1515822 -4.1553569 -4.148983 -4.139535 -4.1431909 -4.1653748 -4.1850519 -4.1984081 -4.1944513 -4.1663942 -4.1378517][-4.1898432 -4.1868429 -4.1808691 -4.1815124 -4.1855936 -4.1870503 -4.1789246 -4.1679459 -4.16065 -4.1623988 -4.1723242 -4.1871233 -4.1864176 -4.1669793 -4.1442456][-4.199893 -4.2054915 -4.2072468 -4.2126756 -4.2154722 -4.2112217 -4.1966176 -4.1784616 -4.1604319 -4.1489291 -4.1546712 -4.1721907 -4.1751032 -4.1654472 -4.1515837][-4.2106042 -4.2208533 -4.226069 -4.2315779 -4.2330451 -4.2283516 -4.2137828 -4.1931047 -4.1683841 -4.1465716 -4.1430845 -4.1538563 -4.1591215 -4.1566496 -4.1512361]]...]
INFO - root - 2017-12-07 17:59:47.178290: step 37810, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 59h:00m:53s remains)
INFO - root - 2017-12-07 17:59:54.066641: step 37820, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 55h:08m:13s remains)
INFO - root - 2017-12-07 18:00:00.812777: step 37830, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 52h:19m:38s remains)
INFO - root - 2017-12-07 18:00:07.590406: step 37840, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 52h:11m:54s remains)
INFO - root - 2017-12-07 18:00:14.489504: step 37850, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 58h:49m:40s remains)
INFO - root - 2017-12-07 18:00:21.387521: step 37860, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 58h:18m:11s remains)
INFO - root - 2017-12-07 18:00:28.129225: step 37870, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 57h:24m:34s remains)
INFO - root - 2017-12-07 18:00:34.906763: step 37880, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 51h:37m:03s remains)
INFO - root - 2017-12-07 18:00:41.644612: step 37890, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 52h:14m:40s remains)
INFO - root - 2017-12-07 18:00:48.431431: step 37900, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 57h:50m:26s remains)
2017-12-07 18:00:49.187846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2809238 -4.2812386 -4.27625 -4.27263 -4.2595091 -4.2431865 -4.2270365 -4.20427 -4.1704054 -4.1681032 -4.1934953 -4.2068906 -4.2263355 -4.2622347 -4.2946134][-4.3078823 -4.3079486 -4.2996168 -4.2910223 -4.2674804 -4.2423286 -4.2279625 -4.2087841 -4.1795697 -4.1775851 -4.2026653 -4.2200723 -4.2385631 -4.2695794 -4.2950225][-4.3109322 -4.31061 -4.3009377 -4.288249 -4.2614474 -4.2373524 -4.2274141 -4.2032642 -4.170423 -4.17211 -4.2005506 -4.2246456 -4.2448287 -4.27299 -4.2949905][-4.288516 -4.2809424 -4.2732935 -4.2680693 -4.245913 -4.221734 -4.2075677 -4.1708989 -4.1309485 -4.1429181 -4.1850362 -4.2187657 -4.2426414 -4.2705922 -4.2940903][-4.2472405 -4.2338834 -4.2283297 -4.230104 -4.2129965 -4.1865873 -4.1563573 -4.0935745 -4.042871 -4.0786428 -4.1451406 -4.1933289 -4.2287588 -4.2639551 -4.291368][-4.2196293 -4.201726 -4.1937828 -4.1909475 -4.166647 -4.1282239 -4.0659618 -3.9561472 -3.8897715 -3.9727597 -4.0839844 -4.1547761 -4.2053261 -4.2527413 -4.2867613][-4.2178068 -4.2015433 -4.19332 -4.1849041 -4.1518173 -4.0974526 -4.0051327 -3.84356 -3.749836 -3.87353 -4.0220861 -4.1134362 -4.1768007 -4.2358842 -4.2768264][-4.2263179 -4.212863 -4.2075434 -4.20198 -4.1754475 -4.1291828 -4.0465922 -3.9023104 -3.8146968 -3.9037452 -4.0232391 -4.1022043 -4.1655893 -4.2272973 -4.2685547][-4.2342281 -4.226006 -4.2272425 -4.2281671 -4.2186713 -4.1945496 -4.1372738 -4.0320659 -3.9582272 -3.9992731 -4.0682087 -4.1217647 -4.1722913 -4.2251873 -4.2636323][-4.2335405 -4.2326226 -4.24107 -4.2456079 -4.2448368 -4.2323194 -4.193099 -4.1121206 -4.0433941 -4.0560794 -4.0967827 -4.1353021 -4.1805468 -4.2322545 -4.2697253][-4.2417226 -4.24485 -4.2562361 -4.2608075 -4.2592473 -4.2499585 -4.2219315 -4.1580467 -4.0972466 -4.0962834 -4.1237211 -4.1527052 -4.1980743 -4.2495146 -4.284379][-4.2513576 -4.2519197 -4.2620606 -4.2653918 -4.2619853 -4.2529249 -4.2310147 -4.1843543 -4.1386156 -4.137094 -4.1579823 -4.17961 -4.2185864 -4.2629995 -4.2919078][-4.2627473 -4.2586308 -4.2637277 -4.2648673 -4.2617588 -4.2539277 -4.23786 -4.2104974 -4.1865191 -4.1914139 -4.2085075 -4.2195392 -4.2443786 -4.2769876 -4.3008909][-4.2739182 -4.26282 -4.2587276 -4.2558837 -4.2565823 -4.2544284 -4.2458119 -4.2318521 -4.2252188 -4.2376041 -4.2548838 -4.260891 -4.2737594 -4.2950382 -4.314126][-4.2971759 -4.2797375 -4.2658243 -4.2560396 -4.2569323 -4.2615809 -4.2615638 -4.2570696 -4.2580891 -4.2692218 -4.28373 -4.2919044 -4.302341 -4.3164148 -4.3307543]]...]
INFO - root - 2017-12-07 18:00:55.712737: step 37910, loss = 2.09, batch loss = 2.04 (12.9 examples/sec; 0.621 sec/batch; 50h:48m:13s remains)
INFO - root - 2017-12-07 18:01:02.504388: step 37920, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 56h:34m:42s remains)
INFO - root - 2017-12-07 18:01:09.265912: step 37930, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 55h:57m:47s remains)
INFO - root - 2017-12-07 18:01:16.161232: step 37940, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 57h:50m:04s remains)
INFO - root - 2017-12-07 18:01:22.913418: step 37950, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 55h:54m:09s remains)
INFO - root - 2017-12-07 18:01:29.699736: step 37960, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.658 sec/batch; 53h:52m:17s remains)
INFO - root - 2017-12-07 18:01:36.416493: step 37970, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 55h:19m:44s remains)
INFO - root - 2017-12-07 18:01:43.253562: step 37980, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 57h:00m:03s remains)
INFO - root - 2017-12-07 18:01:50.080615: step 37990, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 60h:59m:56s remains)
INFO - root - 2017-12-07 18:01:56.865159: step 38000, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 56h:15m:50s remains)
2017-12-07 18:01:57.683080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2967305 -4.2953963 -4.2732186 -4.2390084 -4.2073307 -4.1951389 -4.2119389 -4.2460594 -4.2821231 -4.3133 -4.3344207 -4.3377237 -4.3246841 -4.3066554 -4.2836733][-4.281579 -4.2758617 -4.2479134 -4.204483 -4.1553683 -4.1278253 -4.1491003 -4.2019262 -4.2518 -4.2920876 -4.3224869 -4.3375826 -4.3340445 -4.3215275 -4.3044667][-4.2819905 -4.2656503 -4.2289643 -4.1756253 -4.1092763 -4.0642004 -4.0903511 -4.1574097 -4.2130971 -4.2580886 -4.2952695 -4.3237262 -4.330442 -4.32511 -4.3148389][-4.2994428 -4.2701139 -4.223249 -4.1650057 -4.0957236 -4.0433683 -4.0655947 -4.1324711 -4.1835961 -4.2270994 -4.2665153 -4.3044653 -4.3219614 -4.321403 -4.312459][-4.3208289 -4.2808867 -4.22175 -4.1586642 -4.0975981 -4.0548759 -4.0654478 -4.1174312 -4.1638 -4.2024155 -4.2407994 -4.280273 -4.3050642 -4.3068948 -4.2990294][-4.3358617 -4.2892585 -4.2190089 -4.1439633 -4.0775847 -4.0317135 -4.0252481 -4.0670543 -4.1239562 -4.169929 -4.2094173 -4.2475171 -4.2766 -4.2808604 -4.2780533][-4.34591 -4.2983437 -4.2246103 -4.1405025 -4.055131 -3.979053 -3.9360909 -3.9674919 -4.0442762 -4.1100187 -4.1591549 -4.2000418 -4.235043 -4.2490025 -4.2549987][-4.3504782 -4.3184056 -4.2599063 -4.1847181 -4.0914197 -3.9831753 -3.9036503 -3.9201581 -3.9954944 -4.0674629 -4.1224284 -4.1643062 -4.200387 -4.2241006 -4.2385921][-4.3384781 -4.3300204 -4.3030691 -4.2552505 -4.1779609 -4.0789409 -3.9999404 -3.9963217 -4.0397191 -4.0861487 -4.128943 -4.1620846 -4.19086 -4.2171159 -4.2319427][-4.311635 -4.320282 -4.3182 -4.2980881 -4.2525716 -4.1878796 -4.1297417 -4.1113772 -4.12166 -4.1393571 -4.1630735 -4.1809506 -4.199244 -4.2243538 -4.2377124][-4.2783713 -4.2984858 -4.3089805 -4.3055778 -4.2857804 -4.2547612 -4.2206964 -4.1996956 -4.1917605 -4.1901722 -4.1955023 -4.1989365 -4.2102323 -4.2333565 -4.2435122][-4.2312193 -4.2572656 -4.2767406 -4.2862391 -4.2845759 -4.27729 -4.2649736 -4.2534781 -4.2402291 -4.2297173 -4.2247491 -4.2225966 -4.2292604 -4.2454214 -4.2474251][-4.1679015 -4.2021141 -4.2337594 -4.2570519 -4.2704716 -4.2764826 -4.2783656 -4.2771883 -4.2700438 -4.2619653 -4.2558093 -4.2514248 -4.2519364 -4.2580357 -4.254005][-4.113317 -4.1534872 -4.1998906 -4.235847 -4.2579021 -4.2703838 -4.2786326 -4.28272 -4.2835832 -4.2800107 -4.2740355 -4.2660217 -4.260159 -4.2601342 -4.2554965][-4.083405 -4.12673 -4.180624 -4.2262917 -4.25523 -4.2712884 -4.2794814 -4.2841053 -4.2868166 -4.2834163 -4.2753921 -4.262948 -4.253706 -4.2516313 -4.2487135]]...]
INFO - root - 2017-12-07 18:02:04.396848: step 38010, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 57h:47m:45s remains)
INFO - root - 2017-12-07 18:02:11.227269: step 38020, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 54h:04m:47s remains)
INFO - root - 2017-12-07 18:02:17.968156: step 38030, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 55h:50m:56s remains)
INFO - root - 2017-12-07 18:02:24.749880: step 38040, loss = 2.11, batch loss = 2.05 (12.2 examples/sec; 0.655 sec/batch; 53h:33m:25s remains)
INFO - root - 2017-12-07 18:02:31.443370: step 38050, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 55h:28m:51s remains)
INFO - root - 2017-12-07 18:02:38.142999: step 38060, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 56h:38m:12s remains)
INFO - root - 2017-12-07 18:02:44.773861: step 38070, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 54h:45m:25s remains)
INFO - root - 2017-12-07 18:02:51.521519: step 38080, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 53h:45m:59s remains)
INFO - root - 2017-12-07 18:02:58.309471: step 38090, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 56h:24m:40s remains)
INFO - root - 2017-12-07 18:03:05.124496: step 38100, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 56h:20m:18s remains)
2017-12-07 18:03:05.838607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1058264 -4.1233997 -4.1701245 -4.2185392 -4.2288671 -4.2226648 -4.2116461 -4.1881704 -4.1469293 -4.0921783 -4.0532727 -4.0488462 -4.06478 -4.0834265 -4.1162167][-4.1274781 -4.1466875 -4.1921616 -4.2284451 -4.2228823 -4.2044735 -4.1863952 -4.1619148 -4.1193266 -4.0603361 -4.0127964 -4.0076718 -4.0334744 -4.0648637 -4.107728][-4.1641173 -4.1780925 -4.2088451 -4.2254968 -4.2072783 -4.1808772 -4.1582522 -4.1328082 -4.0973516 -4.0429831 -3.9936113 -3.992589 -4.0260162 -4.0719037 -4.1293812][-4.2044063 -4.2136769 -4.2249451 -4.2199841 -4.186038 -4.1486168 -4.1221123 -4.1049814 -4.0911722 -4.0535893 -4.0109658 -4.0115118 -4.0467391 -4.0993552 -4.1657767][-4.2428031 -4.242229 -4.23358 -4.2109089 -4.1606627 -4.1081867 -4.0719614 -4.0689816 -4.0879335 -4.078249 -4.0558329 -4.0628881 -4.0973592 -4.1451859 -4.2065372][-4.2617464 -4.2429171 -4.2155313 -4.1768165 -4.1122937 -4.0435109 -3.9991207 -4.0147076 -4.0612707 -4.0810757 -4.0855579 -4.1051927 -4.1415763 -4.1825981 -4.233861][-4.2484374 -4.2135854 -4.1683636 -4.120286 -4.0452123 -3.9525688 -3.8980689 -3.9356651 -4.0108557 -4.0544729 -4.0796032 -4.1138549 -4.1556582 -4.1976652 -4.2437887][-4.2340159 -4.1894031 -4.1309781 -4.0768685 -3.9965219 -3.8831151 -3.8113894 -3.8587384 -3.9543505 -4.0194807 -4.0636039 -4.1095562 -4.1603379 -4.2088976 -4.2525086][-4.2242522 -4.1828356 -4.1298308 -4.0878596 -4.0256896 -3.9251335 -3.8524711 -3.8791852 -3.9564636 -4.02002 -4.072022 -4.1205215 -4.1705208 -4.218318 -4.2587976][-4.22332 -4.1897893 -4.1494493 -4.1211729 -4.082561 -4.0164051 -3.9679465 -3.9787195 -4.0270162 -4.0719447 -4.11779 -4.1598206 -4.1985769 -4.2360468 -4.2708135][-4.2394328 -4.2084923 -4.1757154 -4.1548176 -4.1316772 -4.0946727 -4.0715442 -4.0796642 -4.1163292 -4.1499381 -4.1869564 -4.2225485 -4.2474952 -4.2695265 -4.2956867][-4.2710857 -4.2424679 -4.2155056 -4.1979742 -4.1838861 -4.1681652 -4.1641555 -4.1741562 -4.2029958 -4.227829 -4.2555957 -4.2817841 -4.2943821 -4.3042755 -4.3204508][-4.3053861 -4.2852068 -4.2659693 -4.2513437 -4.2401576 -4.2324905 -4.234705 -4.2407837 -4.2603431 -4.2775941 -4.2976818 -4.3161697 -4.3227234 -4.3253207 -4.3331347][-4.3271708 -4.3162284 -4.3051085 -4.2948141 -4.2850051 -4.2784677 -4.2783484 -4.2809858 -4.292594 -4.3038235 -4.3178287 -4.3297668 -4.3333874 -4.3333645 -4.3362174][-4.3371263 -4.3316875 -4.3257465 -4.319694 -4.3124413 -4.3059554 -4.304276 -4.3055606 -4.3122587 -4.3193836 -4.3284173 -4.3351026 -4.3371906 -4.3365455 -4.3371806]]...]
INFO - root - 2017-12-07 18:03:12.504948: step 38110, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.611 sec/batch; 49h:58m:03s remains)
INFO - root - 2017-12-07 18:03:19.201518: step 38120, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 53h:37m:10s remains)
INFO - root - 2017-12-07 18:03:25.962400: step 38130, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 59h:07m:50s remains)
INFO - root - 2017-12-07 18:03:32.872197: step 38140, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 57h:31m:45s remains)
INFO - root - 2017-12-07 18:03:39.675885: step 38150, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.684 sec/batch; 55h:53m:19s remains)
INFO - root - 2017-12-07 18:03:46.443496: step 38160, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 54h:16m:29s remains)
INFO - root - 2017-12-07 18:03:53.247184: step 38170, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 52h:30m:30s remains)
INFO - root - 2017-12-07 18:04:00.062109: step 38180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 56h:47m:56s remains)
INFO - root - 2017-12-07 18:04:06.950008: step 38190, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 58h:50m:21s remains)
INFO - root - 2017-12-07 18:04:13.834939: step 38200, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 56h:43m:10s remains)
2017-12-07 18:04:14.614594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3218541 -4.321919 -4.3239269 -4.3250604 -4.3225589 -4.3187432 -4.3153815 -4.3147345 -4.312109 -4.3079453 -4.305233 -4.3017006 -4.296782 -4.2944007 -4.2988667][-4.3081203 -4.3072371 -4.3092046 -4.3099151 -4.306025 -4.3003922 -4.2955213 -4.2956777 -4.2953277 -4.2910061 -4.2872486 -4.2818637 -4.2733874 -4.2689924 -4.2753491][-4.2886081 -4.2856445 -4.2863708 -4.2837267 -4.2746248 -4.2650976 -4.2581172 -4.2568312 -4.2585163 -4.257585 -4.2562017 -4.2515464 -4.242013 -4.23754 -4.2473345][-4.2624416 -4.2539573 -4.2507424 -4.24404 -4.2277541 -4.2088456 -4.19252 -4.184267 -4.1874161 -4.1973019 -4.2039633 -4.2056355 -4.1987753 -4.1969562 -4.2139273][-4.2319231 -4.2147131 -4.2065291 -4.19594 -4.1731563 -4.1409397 -4.10702 -4.0824962 -4.0854859 -4.1143246 -4.1361713 -4.1468878 -4.1438212 -4.1451688 -4.1723032][-4.2029638 -4.1772094 -4.1612768 -4.1436658 -4.1116362 -4.0656776 -4.0078297 -3.9557037 -3.9516373 -4.0071573 -4.0611277 -4.0867538 -4.0886602 -4.0950418 -4.131331][-4.1892467 -4.1566148 -4.1308079 -4.1026897 -4.0595756 -3.9958594 -3.9082723 -3.80878 -3.7684989 -3.845892 -3.9416788 -3.9934654 -4.0123405 -4.0368953 -4.0925913][-4.1949706 -4.16127 -4.1306744 -4.096323 -4.0464129 -3.9761493 -3.8757813 -3.7430997 -3.6541147 -3.7192876 -3.8296313 -3.8985357 -3.9352486 -3.9789176 -4.05424][-4.2204866 -4.1957016 -4.1746154 -4.149529 -4.1098866 -4.0545225 -3.9748428 -3.8660614 -3.7734821 -3.7842517 -3.8461185 -3.8948042 -3.9264946 -3.9704 -4.0455084][-4.2448816 -4.2309604 -4.2236662 -4.214838 -4.1948118 -4.1607995 -4.1087246 -4.0401611 -3.9722507 -3.9518301 -3.967031 -3.9879751 -4.0102596 -4.0418854 -4.0937638][-4.2701206 -4.2644958 -4.2650261 -4.2654681 -4.2599077 -4.2449203 -4.2162881 -4.1794515 -4.1372237 -4.1119547 -4.1060863 -4.1089792 -4.1223454 -4.1434937 -4.177896][-4.3003864 -4.29979 -4.3028183 -4.3049812 -4.3042283 -4.2999945 -4.2870951 -4.27065 -4.2499962 -4.2321053 -4.2218809 -4.2161212 -4.2200279 -4.2347579 -4.2575283][-4.320682 -4.3209152 -4.3239465 -4.3269715 -4.3295941 -4.3301663 -4.32609 -4.3196173 -4.309999 -4.2977443 -4.2887149 -4.28229 -4.281806 -4.2912836 -4.3060026][-4.3341055 -4.3341341 -4.3362327 -4.338943 -4.3427534 -4.3459325 -4.3464513 -4.3452797 -4.3413858 -4.3330822 -4.3248429 -4.3188376 -4.3142037 -4.3158059 -4.3234243][-4.3416109 -4.3406668 -4.3405895 -4.3410506 -4.3425083 -4.3446445 -4.3468561 -4.3484888 -4.3487053 -4.3456068 -4.340632 -4.3360286 -4.3319306 -4.3304877 -4.3337936]]...]
INFO - root - 2017-12-07 18:04:21.155633: step 38210, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:36m:10s remains)
INFO - root - 2017-12-07 18:04:28.024301: step 38220, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.692 sec/batch; 56h:34m:17s remains)
INFO - root - 2017-12-07 18:04:35.030659: step 38230, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 57h:04m:04s remains)
INFO - root - 2017-12-07 18:04:41.764402: step 38240, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 51h:58m:54s remains)
INFO - root - 2017-12-07 18:04:48.610566: step 38250, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 58h:45m:26s remains)
INFO - root - 2017-12-07 18:04:55.531659: step 38260, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 56h:43m:59s remains)
INFO - root - 2017-12-07 18:05:02.321746: step 38270, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 58h:19m:01s remains)
INFO - root - 2017-12-07 18:05:09.081932: step 38280, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 53h:18m:48s remains)
INFO - root - 2017-12-07 18:05:15.875194: step 38290, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.641 sec/batch; 52h:21m:52s remains)
INFO - root - 2017-12-07 18:05:22.690580: step 38300, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 59h:17m:20s remains)
2017-12-07 18:05:23.490418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1660748 -4.1702652 -4.1893063 -4.2110858 -4.2102847 -4.1869879 -4.16973 -4.1579757 -4.1397567 -4.1182103 -4.1200228 -4.1396718 -4.1710863 -4.2079325 -4.2354431][-4.1244154 -4.1249847 -4.1436911 -4.1677761 -4.1688371 -4.1448517 -4.1284308 -4.1156607 -4.0957403 -4.0707917 -4.0751929 -4.1009407 -4.1354337 -4.17447 -4.2077546][-4.0893021 -4.0822978 -4.0933862 -4.1143217 -4.1154294 -4.0951481 -4.0847707 -4.0771327 -4.0576439 -4.0262 -4.034246 -4.0692587 -4.1069016 -4.1458955 -4.1808815][-4.0916133 -4.0779605 -4.073473 -4.0796981 -4.0747552 -4.0590434 -4.0493746 -4.0421124 -4.0246296 -3.9925952 -4.0065527 -4.054606 -4.101038 -4.1411972 -4.1740885][-4.1125555 -4.1019068 -4.0917525 -4.0871487 -4.0751204 -4.055532 -4.0349765 -4.0185661 -4.004642 -3.976903 -3.996165 -4.0554905 -4.1116791 -4.1531696 -4.1860552][-4.1194592 -4.1149845 -4.1085949 -4.1020932 -4.0848265 -4.0534844 -4.012 -3.979779 -3.9649794 -3.9411836 -3.9690523 -4.0413294 -4.1002674 -4.145319 -4.1852336][-4.1194415 -4.1200161 -4.1212759 -4.1215882 -4.105052 -4.0602069 -3.9956043 -3.9432054 -3.9225912 -3.9035258 -3.9409058 -4.0255308 -4.0822091 -4.123 -4.1669044][-4.1131606 -4.1203542 -4.1224923 -4.1251469 -4.1109362 -4.0680752 -4.0062895 -3.9546432 -3.9275012 -3.907517 -3.9494286 -4.0384984 -4.0969939 -4.13211 -4.1685944][-4.0887752 -4.1028543 -4.1026983 -4.1055264 -4.098012 -4.0735092 -4.0402474 -4.0078473 -3.9784403 -3.9562688 -3.9918551 -4.0757046 -4.1347618 -4.1627493 -4.1901317][-4.0540562 -4.0734324 -4.0746336 -4.0768776 -4.0785818 -4.0736041 -4.0634503 -4.0456119 -4.016758 -3.9972095 -4.0285473 -4.1050272 -4.1632991 -4.1896143 -4.2159915][-4.0410352 -4.0574226 -4.0572524 -4.0600548 -4.0671053 -4.0702291 -4.0707874 -4.0589256 -4.0294766 -4.0159187 -4.0481496 -4.1136889 -4.1679287 -4.1979833 -4.2298951][-4.0746975 -4.0790892 -4.0713186 -4.0741076 -4.0803266 -4.079628 -4.0766115 -4.0608463 -4.0325851 -4.0262012 -4.0597796 -4.1122808 -4.1563611 -4.1879444 -4.226552][-4.1326714 -4.127152 -4.1138525 -4.1168838 -4.1222014 -4.1170444 -4.1080637 -4.0886006 -4.0635471 -4.0601463 -4.0895076 -4.1286645 -4.1599913 -4.1890488 -4.2284455][-4.1855211 -4.1772175 -4.1639657 -4.1670871 -4.1736741 -4.1702013 -4.1611018 -4.1442261 -4.1249013 -4.1221275 -4.1429672 -4.1714392 -4.1952367 -4.2183766 -4.2498717][-4.2344084 -4.227767 -4.2161751 -4.2170634 -4.2230906 -4.2228522 -4.2180619 -4.2073913 -4.1963906 -4.1953087 -4.2085114 -4.2276545 -4.2444348 -4.260087 -4.2822671]]...]
INFO - root - 2017-12-07 18:05:29.978316: step 38310, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.621 sec/batch; 50h:43m:23s remains)
INFO - root - 2017-12-07 18:05:36.899987: step 38320, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 55h:44m:55s remains)
INFO - root - 2017-12-07 18:05:43.772772: step 38330, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 58h:16m:10s remains)
INFO - root - 2017-12-07 18:05:50.540620: step 38340, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 56h:13m:03s remains)
INFO - root - 2017-12-07 18:05:57.361712: step 38350, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 53h:24m:15s remains)
INFO - root - 2017-12-07 18:06:04.261946: step 38360, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 57h:11m:10s remains)
INFO - root - 2017-12-07 18:06:11.195245: step 38370, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.734 sec/batch; 59h:57m:12s remains)
INFO - root - 2017-12-07 18:06:17.984641: step 38380, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 55h:32m:22s remains)
INFO - root - 2017-12-07 18:06:24.758992: step 38390, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 53h:08m:52s remains)
INFO - root - 2017-12-07 18:06:31.585538: step 38400, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 55h:38m:43s remains)
2017-12-07 18:06:32.397435: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1529121 -4.1717253 -4.2182665 -4.2624912 -4.2840257 -4.2870603 -4.27069 -4.2475648 -4.2268472 -4.1987176 -4.1730652 -4.1686425 -4.180068 -4.2105708 -4.2479148][-4.1589856 -4.1777859 -4.2244487 -4.2645068 -4.28194 -4.2843304 -4.2705908 -4.2501669 -4.2271624 -4.1987591 -4.1788459 -4.178412 -4.190414 -4.2215133 -4.2596583][-4.1797686 -4.2031136 -4.240809 -4.2640209 -4.2676735 -4.2617917 -4.2507529 -4.2371268 -4.2187271 -4.1957288 -4.1824832 -4.1835737 -4.1954217 -4.228478 -4.2679629][-4.2136388 -4.2392263 -4.260078 -4.2571154 -4.2370396 -4.2181149 -4.2099257 -4.2111278 -4.2056274 -4.1919565 -4.1871886 -4.1924696 -4.2047276 -4.2341647 -4.2709928][-4.2494926 -4.2667942 -4.265996 -4.2356687 -4.1882792 -4.1532316 -4.1489258 -4.1681018 -4.1877337 -4.1913762 -4.1954527 -4.2046108 -4.219183 -4.2419548 -4.2697873][-4.2760649 -4.2758751 -4.2547345 -4.2024384 -4.1299624 -4.0722919 -4.0645828 -4.1067753 -4.1594124 -4.1872082 -4.1970105 -4.2085156 -4.2254133 -4.2464437 -4.2689528][-4.2896109 -4.2734585 -4.2328873 -4.1633072 -4.0695076 -3.9753518 -3.9420476 -4.0144997 -4.1118112 -4.1678762 -4.1872573 -4.2025528 -4.2188292 -4.241478 -4.2640243][-4.2847023 -4.2594895 -4.2134833 -4.1366577 -4.0324078 -3.8976779 -3.8110325 -3.9049931 -4.04839 -4.1314216 -4.1652441 -4.1889324 -4.2103753 -4.2331715 -4.2542281][-4.266582 -4.2434592 -4.2026963 -4.137115 -4.0443583 -3.9091637 -3.7976797 -3.8675561 -4.0078406 -4.094564 -4.1339211 -4.1643314 -4.1970282 -4.2249279 -4.2459536][-4.2515383 -4.2392206 -4.2123508 -4.1664782 -4.103436 -4.0080118 -3.9225104 -3.9461553 -4.0282841 -4.0867147 -4.1178889 -4.1456442 -4.1804991 -4.213706 -4.2356768][-4.2456913 -4.2469354 -4.2351 -4.2102747 -4.1741066 -4.119441 -4.0669284 -4.0622997 -4.0939589 -4.1172881 -4.131135 -4.1497946 -4.1762877 -4.2052631 -4.2257314][-4.2440529 -4.2546525 -4.2561693 -4.2508759 -4.2338667 -4.2064309 -4.18044 -4.1643505 -4.1586251 -4.1466284 -4.1449628 -4.1601982 -4.1847973 -4.20795 -4.225224][-4.2404804 -4.2577829 -4.2693505 -4.2761679 -4.2720366 -4.2634563 -4.2545929 -4.2371907 -4.2102919 -4.174181 -4.155849 -4.1654463 -4.1918507 -4.2142482 -4.2275043][-4.2496514 -4.2672725 -4.2829533 -4.294908 -4.2965903 -4.2958145 -4.2943029 -4.2795434 -4.2520318 -4.2088623 -4.1761684 -4.1746387 -4.1992288 -4.2234645 -4.2358894][-4.27476 -4.287344 -4.2983537 -4.308641 -4.3144116 -4.3167953 -4.3183079 -4.3091221 -4.290072 -4.2530608 -4.2136817 -4.1998091 -4.2147827 -4.2362323 -4.2468214]]...]
INFO - root - 2017-12-07 18:06:39.005851: step 38410, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 56h:57m:44s remains)
INFO - root - 2017-12-07 18:06:45.693439: step 38420, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 51h:34m:46s remains)
INFO - root - 2017-12-07 18:06:52.481385: step 38430, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 53h:27m:56s remains)
INFO - root - 2017-12-07 18:06:59.251193: step 38440, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 55h:48m:59s remains)
INFO - root - 2017-12-07 18:07:06.021991: step 38450, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 57h:27m:17s remains)
INFO - root - 2017-12-07 18:07:12.824461: step 38460, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.696 sec/batch; 56h:49m:25s remains)
INFO - root - 2017-12-07 18:07:19.520154: step 38470, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 52h:33m:20s remains)
INFO - root - 2017-12-07 18:07:26.316603: step 38480, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 53h:33m:24s remains)
INFO - root - 2017-12-07 18:07:33.138664: step 38490, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 56h:45m:21s remains)
INFO - root - 2017-12-07 18:07:39.977249: step 38500, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 55h:09m:54s remains)
2017-12-07 18:07:40.694721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2886996 -4.2936573 -4.30132 -4.3089595 -4.311 -4.3066359 -4.300107 -4.29905 -4.3023477 -4.30723 -4.3121672 -4.3185463 -4.3266325 -4.3348379 -4.3398705][-4.2372117 -4.2403493 -4.24918 -4.2611952 -4.2663264 -4.2625885 -4.2588162 -4.262713 -4.2699676 -4.27595 -4.2813649 -4.2914171 -4.3067493 -4.3231211 -4.3339777][-4.1747031 -4.1754141 -4.1869354 -4.20562 -4.213376 -4.2108355 -4.2065673 -4.213088 -4.2223392 -4.2266049 -4.232141 -4.2482033 -4.2720585 -4.2980237 -4.3175483][-4.1295447 -4.1219931 -4.1350012 -4.1568227 -4.1666203 -4.1628442 -4.1546025 -4.1639252 -4.176764 -4.180378 -4.1847525 -4.2017379 -4.2282896 -4.2622867 -4.2912774][-4.1245461 -4.1062503 -4.1109729 -4.124157 -4.1260176 -4.1087646 -4.084259 -4.0904737 -4.1100368 -4.1184435 -4.12523 -4.1436663 -4.17326 -4.2163162 -4.2570834][-4.1493735 -4.12537 -4.1177058 -4.1122055 -4.089983 -4.03828 -3.9776654 -3.9745111 -4.0065517 -4.0291305 -4.0422196 -4.0657868 -4.1040487 -4.1624165 -4.2218857][-4.185483 -4.162652 -4.1481628 -4.1254253 -4.0732851 -3.9872303 -3.8916974 -3.8778875 -3.9225941 -3.9621956 -3.9850888 -4.0128651 -4.0603905 -4.128212 -4.1983762][-4.21963 -4.199461 -4.1837487 -4.1535311 -4.0827346 -3.9841292 -3.8839858 -3.8705738 -3.917794 -3.9634466 -3.9965575 -4.0277166 -4.0744743 -4.1354575 -4.1976204][-4.2397923 -4.22052 -4.204845 -4.176249 -4.11055 -4.03348 -3.9576771 -3.946166 -3.9825873 -4.0240154 -4.0619764 -4.0952306 -4.1348867 -4.1798654 -4.2246184][-4.2507606 -4.2376637 -4.2274008 -4.2086897 -4.1616082 -4.1122026 -4.0625048 -4.0521874 -4.0701876 -4.1013827 -4.1329756 -4.1582766 -4.1884542 -4.22276 -4.2555346][-4.2576032 -4.2523532 -4.2497106 -4.2396045 -4.2087812 -4.1751533 -4.1383181 -4.1316953 -4.1381178 -4.1582708 -4.1784434 -4.1936178 -4.21857 -4.2499232 -4.2761269][-4.2624369 -4.2614055 -4.2633114 -4.2567077 -4.234694 -4.2091837 -4.1825671 -4.1847625 -4.1914263 -4.200706 -4.2078962 -4.210525 -4.2316556 -4.2628469 -4.2863336][-4.2650037 -4.2612281 -4.2595234 -4.2564678 -4.2404504 -4.2197933 -4.1995773 -4.2060218 -4.2148237 -4.2179732 -4.2174397 -4.2162733 -4.2361584 -4.2672229 -4.2913423][-4.2614145 -4.2525282 -4.2468662 -4.2448239 -4.2351885 -4.2200933 -4.2074795 -4.2148204 -4.22458 -4.2288337 -4.2281127 -4.2274294 -4.244935 -4.272644 -4.29638][-4.2664752 -4.2569323 -4.2500467 -4.248476 -4.2435541 -4.2333717 -4.2270403 -4.2320552 -4.2378697 -4.244545 -4.2485175 -4.2521834 -4.2658544 -4.28682 -4.3058949]]...]
INFO - root - 2017-12-07 18:07:47.287821: step 38510, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 55h:43m:17s remains)
INFO - root - 2017-12-07 18:07:54.125605: step 38520, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 58h:01m:53s remains)
INFO - root - 2017-12-07 18:08:00.950237: step 38530, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 54h:10m:44s remains)
INFO - root - 2017-12-07 18:08:07.714955: step 38540, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 54h:42m:49s remains)
INFO - root - 2017-12-07 18:08:14.574031: step 38550, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 54h:39m:30s remains)
INFO - root - 2017-12-07 18:08:21.444262: step 38560, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 55h:29m:11s remains)
INFO - root - 2017-12-07 18:08:28.206275: step 38570, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 55h:23m:52s remains)
INFO - root - 2017-12-07 18:08:35.109669: step 38580, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.668 sec/batch; 54h:33m:02s remains)
INFO - root - 2017-12-07 18:08:41.867166: step 38590, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 52h:35m:47s remains)
INFO - root - 2017-12-07 18:08:48.716283: step 38600, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 55h:21m:17s remains)
2017-12-07 18:08:49.451323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227968 -4.3060279 -4.2689519 -4.2127337 -4.1483026 -4.077868 -4.023385 -4.0483723 -4.1288786 -4.19943 -4.2299905 -4.2397819 -4.2386975 -4.2331944 -4.2300596][-4.32794 -4.3145256 -4.2826352 -4.2352185 -4.1879153 -4.138433 -4.1009517 -4.1204677 -4.1767955 -4.2285719 -4.254427 -4.2654047 -4.2647643 -4.2552338 -4.2453127][-4.331264 -4.3243585 -4.300518 -4.2638307 -4.2334518 -4.2082434 -4.1927853 -4.2035227 -4.2285037 -4.2566447 -4.27474 -4.2893147 -4.2958674 -4.2901435 -4.2761455][-4.3255887 -4.3218608 -4.3034925 -4.2717342 -4.2491765 -4.2372375 -4.2370367 -4.24517 -4.2528472 -4.2651339 -4.27204 -4.2857723 -4.2987771 -4.3029404 -4.2906489][-4.3131585 -4.3075023 -4.2871151 -4.2550268 -4.2336793 -4.221065 -4.2236104 -4.2317634 -4.2408094 -4.2509027 -4.251545 -4.2591424 -4.2693715 -4.2792339 -4.271163][-4.3024216 -4.296669 -4.2668662 -4.2245274 -4.1934204 -4.1637664 -4.1547589 -4.1611047 -4.1853471 -4.2125006 -4.2206783 -4.2235208 -4.2220845 -4.228322 -4.22635][-4.2894354 -4.2873383 -4.248867 -4.1906576 -4.1354375 -4.0750623 -4.0356612 -4.033082 -4.0945868 -4.1627364 -4.1906614 -4.1916556 -4.1806941 -4.1793213 -4.1759768][-4.2726836 -4.2767758 -4.2380018 -4.1701984 -4.0957127 -4.0069008 -3.9296319 -3.9180746 -4.0192795 -4.1205883 -4.1656389 -4.17077 -4.1614113 -4.1597629 -4.15272][-4.2622528 -4.2724743 -4.2398314 -4.1762347 -4.1023636 -4.0118818 -3.9244211 -3.9068041 -4.0039005 -4.0998464 -4.1488357 -4.1658473 -4.1726055 -4.1800675 -4.1728377][-4.2637939 -4.2751722 -4.2507548 -4.2003379 -4.1410546 -4.0691924 -3.9969175 -3.9769595 -4.0394812 -4.1097364 -4.1536584 -4.1782193 -4.1988273 -4.2166061 -4.2135363][-4.2718797 -4.2792416 -4.2616072 -4.220993 -4.1727023 -4.1219568 -4.0720544 -4.0589495 -4.09766 -4.1481347 -4.1865988 -4.2099977 -4.2336373 -4.25657 -4.2612643][-4.2791667 -4.28651 -4.2740331 -4.239336 -4.1986508 -4.1625543 -4.1331997 -4.1321282 -4.1662035 -4.2064667 -4.2380662 -4.2576146 -4.2764912 -4.2961707 -4.3071971][-4.2870817 -4.2958961 -4.2896886 -4.2602987 -4.227973 -4.2050133 -4.1936464 -4.2037544 -4.2372656 -4.271894 -4.2958307 -4.3088393 -4.3179169 -4.32811 -4.3387942][-4.3015313 -4.3086696 -4.3044739 -4.282093 -4.2597404 -4.2463942 -4.2436771 -4.25562 -4.2838335 -4.313127 -4.3316107 -4.3378329 -4.3365788 -4.3366919 -4.3391795][-4.3142076 -4.3181624 -4.3141518 -4.29733 -4.2811484 -4.2714233 -4.270174 -4.278656 -4.2996411 -4.3234816 -4.3393073 -4.3433132 -4.3363171 -4.3279352 -4.3199143]]...]
INFO - root - 2017-12-07 18:08:55.960864: step 38610, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 57h:42m:18s remains)
INFO - root - 2017-12-07 18:09:02.710688: step 38620, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 53h:36m:54s remains)
INFO - root - 2017-12-07 18:09:09.487659: step 38630, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 54h:13m:27s remains)
INFO - root - 2017-12-07 18:09:16.398322: step 38640, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 58h:23m:38s remains)
INFO - root - 2017-12-07 18:09:23.139306: step 38650, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 55h:27m:44s remains)
INFO - root - 2017-12-07 18:09:29.915962: step 38660, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 52h:19m:54s remains)
INFO - root - 2017-12-07 18:09:36.740254: step 38670, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 53h:37m:00s remains)
INFO - root - 2017-12-07 18:09:43.612249: step 38680, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 57h:30m:05s remains)
INFO - root - 2017-12-07 18:09:50.235459: step 38690, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 56h:37m:03s remains)
INFO - root - 2017-12-07 18:09:57.037007: step 38700, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 53h:46m:33s remains)
2017-12-07 18:09:57.795434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1857419 -4.1705089 -4.1504292 -4.1444182 -4.1617837 -4.1956439 -4.2155104 -4.2056646 -4.18348 -4.171782 -4.17051 -4.171 -4.1816406 -4.2037354 -4.2222867][-4.2015219 -4.1824884 -4.16422 -4.1624637 -4.1816134 -4.2070303 -4.2144246 -4.1908188 -4.1621423 -4.1608458 -4.173841 -4.1769624 -4.1838388 -4.2023768 -4.2205915][-4.2170076 -4.19678 -4.1834846 -4.1844573 -4.1995111 -4.2125535 -4.2067995 -4.1751456 -4.1432633 -4.150238 -4.173039 -4.1807027 -4.1893954 -4.2096205 -4.2323227][-4.2372842 -4.2232695 -4.21595 -4.2154932 -4.2206397 -4.2196264 -4.201004 -4.1623788 -4.1288357 -4.1364059 -4.1620088 -4.1769567 -4.1950331 -4.2242455 -4.2546659][-4.2588425 -4.2537203 -4.2488194 -4.242722 -4.2350626 -4.2189288 -4.1882386 -4.1432567 -4.1121335 -4.121181 -4.1468563 -4.1718779 -4.2010727 -4.2392483 -4.2771549][-4.2756248 -4.2748585 -4.2673979 -4.2528095 -4.2323523 -4.2054687 -4.1618347 -4.1085172 -4.0818934 -4.0989656 -4.1327524 -4.1710358 -4.2086563 -4.25225 -4.2943592][-4.277144 -4.2753325 -4.2629237 -4.2405324 -4.2136569 -4.1842475 -4.1326976 -4.0749702 -4.0520964 -4.0798426 -4.1289988 -4.1801467 -4.2229085 -4.2683606 -4.3107595][-4.2641411 -4.2581434 -4.2406731 -4.2147775 -4.1901932 -4.1667762 -4.1180649 -4.0664854 -4.04853 -4.0795865 -4.1359591 -4.1913018 -4.2381077 -4.284163 -4.3247032][-4.2524657 -4.2432523 -4.2259045 -4.2004762 -4.176806 -4.154479 -4.1145754 -4.0736394 -4.0607648 -4.0894895 -4.1432977 -4.1975374 -4.248085 -4.2950249 -4.3315449][-4.2559471 -4.244349 -4.228673 -4.2051129 -4.1776586 -4.1494522 -4.1116185 -4.0799766 -4.0748005 -4.102025 -4.1494746 -4.2026186 -4.2568789 -4.3039975 -4.3354154][-4.2690082 -4.2583127 -4.2427497 -4.2204742 -4.1914096 -4.1581168 -4.1208358 -4.0940895 -4.09543 -4.1226826 -4.1639585 -4.2140994 -4.2695508 -4.3151336 -4.3395109][-4.2872415 -4.2821169 -4.2683125 -4.2460814 -4.2160277 -4.1818519 -4.1485295 -4.1266346 -4.131022 -4.1552196 -4.191412 -4.2362461 -4.2883487 -4.3279867 -4.3442507][-4.2998514 -4.3019357 -4.2963734 -4.2798986 -4.2520752 -4.2195969 -4.1920438 -4.1740179 -4.17591 -4.1946044 -4.2255583 -4.2643385 -4.3084764 -4.3371763 -4.3457165][-4.309062 -4.317018 -4.3187089 -4.3096437 -4.2872343 -4.258606 -4.2341933 -4.2178693 -4.2179966 -4.2333097 -4.26032 -4.2931767 -4.3252835 -4.3422875 -4.3454924][-4.3165679 -4.3270645 -4.3334837 -4.330523 -4.3157439 -4.2931976 -4.272686 -4.2588687 -4.2575116 -4.2702489 -4.2936115 -4.3190374 -4.339066 -4.3474154 -4.3467765]]...]
INFO - root - 2017-12-07 18:10:04.421747: step 38710, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 57h:02m:45s remains)
INFO - root - 2017-12-07 18:10:11.153856: step 38720, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.682 sec/batch; 55h:38m:29s remains)
INFO - root - 2017-12-07 18:10:18.000840: step 38730, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 50h:45m:40s remains)
INFO - root - 2017-12-07 18:10:24.825878: step 38740, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 52h:38m:56s remains)
INFO - root - 2017-12-07 18:10:31.650973: step 38750, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 58h:47m:01s remains)
INFO - root - 2017-12-07 18:10:38.520049: step 38760, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 57h:57m:18s remains)
INFO - root - 2017-12-07 18:10:45.367790: step 38770, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 57h:26m:50s remains)
INFO - root - 2017-12-07 18:10:52.103707: step 38780, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 51h:38m:36s remains)
INFO - root - 2017-12-07 18:10:59.017566: step 38790, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 56h:06m:58s remains)
INFO - root - 2017-12-07 18:11:05.738662: step 38800, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 56h:24m:24s remains)
2017-12-07 18:11:06.480589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0845327 -4.1147037 -4.1451311 -4.1647129 -4.1667929 -4.1514444 -4.1307926 -4.1258678 -4.1380568 -4.15728 -4.1868539 -4.2178431 -4.2343836 -4.2415652 -4.2486558][-4.1064014 -4.1474438 -4.1873879 -4.2087469 -4.2067432 -4.1911817 -4.1748924 -4.1720695 -4.1861615 -4.2042222 -4.2267833 -4.2448874 -4.2527494 -4.25378 -4.2586393][-4.1632204 -4.2045169 -4.2450466 -4.2636642 -4.2532592 -4.233531 -4.214849 -4.2064495 -4.2144518 -4.2293997 -4.2471714 -4.2555876 -4.2589364 -4.2586031 -4.2663393][-4.2202177 -4.2509122 -4.2847018 -4.3002396 -4.2836862 -4.2557878 -4.2279153 -4.2049761 -4.2039371 -4.2219591 -4.2420335 -4.251009 -4.2589059 -4.2635942 -4.276186][-4.2575378 -4.2792773 -4.3051434 -4.3082676 -4.2819209 -4.2409954 -4.1964622 -4.1574335 -4.1513495 -4.1789036 -4.2084742 -4.2275462 -4.2476711 -4.2634244 -4.282258][-4.2857122 -4.2982888 -4.3094583 -4.2927227 -4.2511187 -4.190012 -4.1208763 -4.0639796 -4.0563412 -4.1034565 -4.1524277 -4.190012 -4.2281194 -4.2578006 -4.2808156][-4.306489 -4.3069005 -4.3012872 -4.2669291 -4.210906 -4.13515 -4.0427608 -3.9698532 -3.9641011 -4.0283728 -4.0981083 -4.1564236 -4.2104754 -4.2503967 -4.27404][-4.311832 -4.3049197 -4.288054 -4.2469797 -4.1878486 -4.111546 -4.0111089 -3.9301698 -3.9254894 -3.9936852 -4.0786738 -4.1546397 -4.2175231 -4.2569327 -4.2734642][-4.3126984 -4.30704 -4.2836871 -4.2375612 -4.1823926 -4.1190968 -4.0315056 -3.9602051 -3.9544485 -4.0119114 -4.0944347 -4.1752315 -4.2394776 -4.2769661 -4.288022][-4.3187165 -4.3190937 -4.2928257 -4.2429867 -4.1887751 -4.1398859 -4.0721292 -4.0129924 -4.0078363 -4.0535879 -4.122879 -4.1960692 -4.2599421 -4.3010092 -4.3119664][-4.3205695 -4.3243833 -4.2999659 -4.2521353 -4.1967912 -4.1539869 -4.1020184 -4.0514259 -4.0468564 -4.0868244 -4.1432633 -4.2071724 -4.2692895 -4.3139863 -4.3298664][-4.3197179 -4.3227592 -4.3036313 -4.2610531 -4.2048512 -4.1622219 -4.1181602 -4.0717621 -4.0656276 -4.102613 -4.1502366 -4.2056823 -4.2620668 -4.3074422 -4.3278823][-4.3197203 -4.3209047 -4.3080482 -4.27253 -4.2191515 -4.1701932 -4.1238909 -4.0785437 -4.0714879 -4.1069326 -4.1438828 -4.1888423 -4.2395906 -4.2826848 -4.3054495][-4.3192182 -4.3189816 -4.3106766 -4.2829027 -4.2361937 -4.1858997 -4.1393576 -4.0965571 -4.0847316 -4.1111426 -4.1366949 -4.169517 -4.2082753 -4.2428474 -4.2630825][-4.3164415 -4.3152671 -4.3075967 -4.2872119 -4.2520442 -4.2118025 -4.173017 -4.1364951 -4.1193128 -4.1307206 -4.1427088 -4.15862 -4.1811113 -4.2033849 -4.2188654]]...]
INFO - root - 2017-12-07 18:11:13.067044: step 38810, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 53h:45m:26s remains)
INFO - root - 2017-12-07 18:11:19.831555: step 38820, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 56h:22m:33s remains)
INFO - root - 2017-12-07 18:11:26.636659: step 38830, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 58h:54m:00s remains)
INFO - root - 2017-12-07 18:11:33.415491: step 38840, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 54h:08m:12s remains)
INFO - root - 2017-12-07 18:11:40.164276: step 38850, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 54h:37m:47s remains)
INFO - root - 2017-12-07 18:11:46.996770: step 38860, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 53h:45m:08s remains)
INFO - root - 2017-12-07 18:11:53.782585: step 38870, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 56h:55m:33s remains)
INFO - root - 2017-12-07 18:12:00.714346: step 38880, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 56h:50m:51s remains)
INFO - root - 2017-12-07 18:12:07.532235: step 38890, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 56h:28m:00s remains)
INFO - root - 2017-12-07 18:12:14.266475: step 38900, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 52h:49m:47s remains)
2017-12-07 18:12:14.985241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1309247 -4.1150832 -4.10277 -4.105638 -4.1186504 -4.1256986 -4.1173711 -4.0983806 -4.0990467 -4.123785 -4.1402888 -4.1281295 -4.0941544 -4.0520229 -4.0412908][-4.1021628 -4.089201 -4.0821323 -4.0869107 -4.10177 -4.116703 -4.1180162 -4.1079173 -4.114058 -4.1387615 -4.151022 -4.1393318 -4.1166019 -4.08756 -4.0809517][-4.1185513 -4.102603 -4.0876122 -4.0845733 -4.0974045 -4.12 -4.1313391 -4.131783 -4.143856 -4.1667833 -4.1776733 -4.1750445 -4.170548 -4.1581726 -4.1542487][-4.1518378 -4.1330643 -4.1081095 -4.0955949 -4.1044827 -4.1306381 -4.1486621 -4.1540947 -4.1692266 -4.1930542 -4.2112255 -4.2243023 -4.2353015 -4.2357349 -4.2307692][-4.1685658 -4.1475148 -4.1214557 -4.1039352 -4.1083384 -4.129283 -4.143806 -4.146 -4.1606879 -4.1895661 -4.220036 -4.247858 -4.2688642 -4.2762427 -4.2657914][-4.1647506 -4.1423297 -4.1220517 -4.106842 -4.1061354 -4.1106882 -4.1051779 -4.0914292 -4.1012659 -4.1419983 -4.1912632 -4.2312884 -4.2532277 -4.2579317 -4.2419872][-4.1514778 -4.1280622 -4.1185832 -4.110755 -4.1021171 -4.0818534 -4.0389595 -3.9927914 -3.9977341 -4.0641012 -4.1394639 -4.18728 -4.2025428 -4.198802 -4.1804767][-4.1328421 -4.1100287 -4.11071 -4.1107435 -4.0917659 -4.0418687 -3.9595208 -3.8775113 -3.8871131 -3.9914422 -4.0906887 -4.1368332 -4.1382976 -4.1207256 -4.105093][-4.1177144 -4.0980358 -4.1069784 -4.1119633 -4.089848 -4.0307722 -3.9416788 -3.8590608 -3.8820381 -3.9919751 -4.0788465 -4.1034279 -4.082222 -4.0540838 -4.0474181][-4.1203179 -4.1045117 -4.1189132 -4.1291203 -4.1172295 -4.0774589 -4.0149713 -3.9606552 -3.980021 -4.0515189 -4.0959792 -4.0842276 -4.0403323 -4.0109754 -4.0187683][-4.1240072 -4.1163263 -4.1341209 -4.150681 -4.1529312 -4.1339025 -4.1002097 -4.0724139 -4.0833979 -4.1142783 -4.1172175 -4.076355 -4.0205994 -3.9969006 -4.0177984][-4.1233435 -4.1306152 -4.1515074 -4.1707311 -4.1791024 -4.1693578 -4.1530223 -4.1430387 -4.1514163 -4.1608639 -4.1457672 -4.0989246 -4.0457411 -4.0285468 -4.0518403][-4.1260624 -4.1489182 -4.1739349 -4.1949625 -4.2067461 -4.2050133 -4.2006249 -4.2023787 -4.2118983 -4.2135425 -4.1969285 -4.1593032 -4.1195278 -4.1068153 -4.121933][-4.1296639 -4.1630135 -4.1956868 -4.2222414 -4.2391787 -4.2456851 -4.2485256 -4.2530932 -4.261023 -4.2626648 -4.2520752 -4.2299504 -4.2056336 -4.1962137 -4.2009344][-4.1526217 -4.1851387 -4.21859 -4.2458797 -4.264957 -4.2739344 -4.2785711 -4.2832384 -4.2895989 -4.2916679 -4.2880859 -4.2791352 -4.266047 -4.2576823 -4.2558842]]...]
INFO - root - 2017-12-07 18:12:21.645222: step 38910, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 58h:34m:52s remains)
INFO - root - 2017-12-07 18:12:28.419442: step 38920, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 56h:42m:26s remains)
INFO - root - 2017-12-07 18:12:35.202196: step 38930, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.632 sec/batch; 51h:34m:25s remains)
INFO - root - 2017-12-07 18:12:42.009009: step 38940, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 55h:14m:30s remains)
INFO - root - 2017-12-07 18:12:48.736710: step 38950, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 55h:50m:00s remains)
INFO - root - 2017-12-07 18:12:55.527615: step 38960, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 58h:44m:39s remains)
INFO - root - 2017-12-07 18:13:02.374638: step 38970, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 54h:34m:43s remains)
INFO - root - 2017-12-07 18:13:09.158415: step 38980, loss = 2.07, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 51h:04m:42s remains)
INFO - root - 2017-12-07 18:13:16.174361: step 38990, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 56h:43m:05s remains)
INFO - root - 2017-12-07 18:13:22.864437: step 39000, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 53h:28m:45s remains)
2017-12-07 18:13:23.680978: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2049108 -4.2083635 -4.208642 -4.2074118 -4.2091494 -4.21693 -4.2242994 -4.2328596 -4.246913 -4.2635345 -4.2733374 -4.2787056 -4.2774878 -4.27846 -4.278183][-4.2225819 -4.2313242 -4.236516 -4.2377992 -4.24457 -4.256918 -4.2672238 -4.2755938 -4.2834229 -4.289288 -4.2878208 -4.2862468 -4.2822642 -4.2810097 -4.2774539][-4.2404437 -4.251368 -4.2576032 -4.2599931 -4.2682967 -4.2801065 -4.2890821 -4.2947907 -4.2996798 -4.3009176 -4.294982 -4.2896566 -4.28345 -4.2798781 -4.2739244][-4.2496562 -4.2581697 -4.2625761 -4.2632723 -4.2676015 -4.2743139 -4.2790346 -4.2823992 -4.2875381 -4.2902203 -4.2863255 -4.2804952 -4.2755051 -4.2720151 -4.2680278][-4.2248936 -4.22848 -4.2322264 -4.2351017 -4.2385306 -4.2413354 -4.2412415 -4.2415566 -4.2480025 -4.25625 -4.2580318 -4.2544689 -4.2524824 -4.2535048 -4.255384][-4.1811185 -4.1757827 -4.1762152 -4.1787591 -4.1807556 -4.178813 -4.1696763 -4.1632047 -4.17059 -4.1869845 -4.1968942 -4.2001023 -4.2047663 -4.2135544 -4.2247577][-4.12176 -4.105412 -4.0999002 -4.0995264 -4.0996308 -4.0897055 -4.0654459 -4.0452309 -4.0528555 -4.0828433 -4.1083221 -4.1225934 -4.1350374 -4.1497021 -4.1659818][-4.0636182 -4.0397143 -4.0293026 -4.0251513 -4.0195708 -3.9963145 -3.9496729 -3.9077182 -3.9143484 -3.9654422 -4.0132513 -4.0411253 -4.0596495 -4.0772929 -4.0956063][-4.0357466 -4.0148468 -4.0099955 -4.01048 -4.0094581 -3.9942222 -3.9585567 -3.9277315 -3.9433529 -3.9971757 -4.0410442 -4.0618644 -4.0718756 -4.0791469 -4.0853319][-4.0807796 -4.0721173 -4.0780497 -4.08668 -4.0964093 -4.0971537 -4.0859656 -4.0757856 -4.0906515 -4.1246247 -4.1474495 -4.1537251 -4.1519461 -4.1444883 -4.1391473][-4.1492019 -4.1503415 -4.1618018 -4.1729507 -4.1823683 -4.187428 -4.1857347 -4.1827602 -4.1900868 -4.2037807 -4.2094655 -4.2079587 -4.2046666 -4.1950817 -4.1847868][-4.2151027 -4.217452 -4.2235861 -4.2250071 -4.2228956 -4.2199631 -4.2148466 -4.2095962 -4.2091 -4.2089953 -4.2064295 -4.2037644 -4.20649 -4.2035913 -4.19557][-4.2464356 -4.2486 -4.249835 -4.242219 -4.2286835 -4.2153335 -4.2020068 -4.1895494 -4.1793814 -4.1688566 -4.16118 -4.1619344 -4.1720324 -4.17589 -4.1731877][-4.2490783 -4.24884 -4.2492738 -4.2403884 -4.2242961 -4.2071214 -4.1906252 -4.1746612 -4.1582761 -4.1417146 -4.1337886 -4.1412978 -4.1596718 -4.1652136 -4.1592488][-4.2416716 -4.2394605 -4.2424984 -4.2372408 -4.2233524 -4.2075291 -4.1935825 -4.1795278 -4.162107 -4.14312 -4.137248 -4.1509333 -4.1758471 -4.1805906 -4.1665711]]...]
INFO - root - 2017-12-07 18:13:30.316302: step 39010, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 56h:12m:55s remains)
INFO - root - 2017-12-07 18:13:37.101047: step 39020, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 58h:52m:36s remains)
INFO - root - 2017-12-07 18:13:43.774863: step 39030, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 54h:36m:58s remains)
INFO - root - 2017-12-07 18:13:50.598173: step 39040, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 53h:26m:32s remains)
INFO - root - 2017-12-07 18:13:57.360143: step 39050, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.658 sec/batch; 53h:39m:04s remains)
INFO - root - 2017-12-07 18:14:04.156202: step 39060, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 55h:47m:04s remains)
INFO - root - 2017-12-07 18:14:11.011615: step 39070, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 59h:31m:52s remains)
INFO - root - 2017-12-07 18:14:17.799440: step 39080, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 56h:57m:48s remains)
INFO - root - 2017-12-07 18:14:24.677156: step 39090, loss = 2.03, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 53h:58m:02s remains)
INFO - root - 2017-12-07 18:14:31.511159: step 39100, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 52h:53m:16s remains)
2017-12-07 18:14:32.192313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2412348 -4.232625 -4.2135029 -4.1977167 -4.1919088 -4.1656804 -4.13083 -4.1659961 -4.2184677 -4.2254672 -4.2190118 -4.1929 -4.1748934 -4.1936965 -4.1927109][-4.2443275 -4.2291803 -4.206943 -4.192235 -4.1824059 -4.1526756 -4.1191654 -4.1577239 -4.2148242 -4.2247295 -4.222012 -4.199944 -4.1828394 -4.2005782 -4.1987195][-4.2470565 -4.227303 -4.2022905 -4.1858273 -4.1738868 -4.1453128 -4.1089249 -4.141902 -4.2041383 -4.2245574 -4.2273183 -4.2099333 -4.19684 -4.2122884 -4.2153025][-4.2548971 -4.2345009 -4.20959 -4.193367 -4.1783609 -4.1485286 -4.1083465 -4.1317325 -4.1956964 -4.2259216 -4.2336836 -4.2209506 -4.2099257 -4.2209716 -4.2264867][-4.2597027 -4.2361226 -4.2094035 -4.1914592 -4.1703658 -4.1348505 -4.0871534 -4.1015835 -4.1683335 -4.2065082 -4.2172351 -4.2051969 -4.192584 -4.1978574 -4.2053251][-4.256918 -4.228981 -4.1955876 -4.1655059 -4.1335893 -4.0846267 -4.0215082 -4.023108 -4.09003 -4.1427 -4.1669164 -4.1598406 -4.1457868 -4.1518817 -4.1670408][-4.2428746 -4.2102313 -4.1692491 -4.1271405 -4.083385 -4.0154734 -3.9308124 -3.9167175 -3.973237 -4.0398021 -4.0904417 -4.1056013 -4.1027093 -4.1160841 -4.1411266][-4.2355413 -4.2064209 -4.1665506 -4.1250076 -4.0765562 -3.9928691 -3.8921556 -3.8634186 -3.8972383 -3.9639931 -4.0434046 -4.089251 -4.102488 -4.1152658 -4.1353097][-4.2471261 -4.2290463 -4.2026463 -4.1752982 -4.1353807 -4.0562649 -3.9650097 -3.9441922 -3.967247 -4.0103011 -4.0747733 -4.1197214 -4.1379328 -4.1465893 -4.1524282][-4.2623377 -4.2545986 -4.2420845 -4.2274542 -4.1994438 -4.1394043 -4.0719619 -4.0706382 -4.0939212 -4.1127772 -4.1476917 -4.1759958 -4.1874757 -4.1900649 -4.1861558][-4.275939 -4.2766733 -4.2717261 -4.2643023 -4.2450018 -4.2018628 -4.1543303 -4.1657705 -4.1897206 -4.1915197 -4.2027149 -4.2150435 -4.2170224 -4.2152038 -4.2076797][-4.2839079 -4.2878284 -4.2823992 -4.2739711 -4.260046 -4.229579 -4.19445 -4.2104869 -4.2318249 -4.22573 -4.2218885 -4.2232847 -4.219243 -4.2162595 -4.20863][-4.2823772 -4.28755 -4.2822695 -4.2723475 -4.2656107 -4.2455626 -4.2184424 -4.2311153 -4.2493176 -4.2432084 -4.2329764 -4.2254677 -4.2182641 -4.2121553 -4.2058387][-4.2805467 -4.2802773 -4.27363 -4.2614727 -4.2593751 -4.246398 -4.2223406 -4.2294135 -4.2454448 -4.2430639 -4.2341027 -4.2243471 -4.2179642 -4.2101536 -4.2070289][-4.2804747 -4.274838 -4.2645006 -4.249054 -4.2462287 -4.2393861 -4.2195115 -4.2245197 -4.2447128 -4.2508593 -4.2466855 -4.2379575 -4.2320309 -4.2224522 -4.2199669]]...]
INFO - root - 2017-12-07 18:14:38.794226: step 39110, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 56h:04m:35s remains)
INFO - root - 2017-12-07 18:14:45.626419: step 39120, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 53h:14m:40s remains)
INFO - root - 2017-12-07 18:14:52.461416: step 39130, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 52h:47m:18s remains)
INFO - root - 2017-12-07 18:14:59.343514: step 39140, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 56h:14m:17s remains)
INFO - root - 2017-12-07 18:15:06.283579: step 39150, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:21m:58s remains)
INFO - root - 2017-12-07 18:15:13.160200: step 39160, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 53h:27m:42s remains)
INFO - root - 2017-12-07 18:15:19.897343: step 39170, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 53h:58m:48s remains)
INFO - root - 2017-12-07 18:15:26.798115: step 39180, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 57h:40m:28s remains)
INFO - root - 2017-12-07 18:15:33.680327: step 39190, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.695 sec/batch; 56h:38m:16s remains)
INFO - root - 2017-12-07 18:15:40.478328: step 39200, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 53h:47m:02s remains)
2017-12-07 18:15:41.261582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.335988 -4.33776 -4.3386712 -4.3399396 -4.3410006 -4.3416109 -4.3417568 -4.3418117 -4.3415761 -4.3405271 -4.339 -4.3376031 -4.3361521 -4.3345408 -4.333456][-4.3201475 -4.3216605 -4.3230743 -4.3241611 -4.324729 -4.324914 -4.324892 -4.3250279 -4.3245711 -4.323122 -4.3213153 -4.3198318 -4.3184705 -4.31718 -4.3166308][-4.3099861 -4.3129969 -4.3137288 -4.3127222 -4.3108788 -4.3089037 -4.3063474 -4.3038378 -4.3011456 -4.2985077 -4.2968407 -4.2965841 -4.2970281 -4.2973967 -4.2975559][-4.3085251 -4.3126721 -4.31213 -4.3079028 -4.30279 -4.2990346 -4.2952228 -4.2907958 -4.2856231 -4.2818351 -4.2799358 -4.280715 -4.282886 -4.2842627 -4.2839684][-4.3014021 -4.3024492 -4.2975793 -4.2884912 -4.2803555 -4.2777009 -4.2782707 -4.2782574 -4.2767034 -4.27631 -4.2758732 -4.276484 -4.2782774 -4.2787981 -4.2765837][-4.2791834 -4.2731447 -4.2619367 -4.2474012 -4.2369142 -4.2371292 -4.2455416 -4.2551146 -4.2639871 -4.2712407 -4.2742434 -4.2740097 -4.2732415 -4.2712512 -4.2671356][-4.2286606 -4.2118711 -4.1910963 -4.1682334 -4.1528106 -4.1575603 -4.1773286 -4.2021265 -4.2275648 -4.2468262 -4.2557716 -4.2542572 -4.2490168 -4.2439542 -4.2394366][-4.1553984 -4.1295266 -4.1002727 -4.0686836 -4.0464616 -4.0527363 -4.0817766 -4.1245995 -4.1713548 -4.2058 -4.2236066 -4.2231994 -4.2162242 -4.2109418 -4.2081609][-4.1098447 -4.0859523 -4.056294 -4.0233655 -3.9953856 -3.9956398 -4.0217638 -4.0696096 -4.1294937 -4.1757965 -4.2025428 -4.2071662 -4.2033606 -4.2006764 -4.198678][-4.1254287 -4.1137547 -4.0955882 -4.0727534 -4.0482869 -4.0426774 -4.0572529 -4.0941081 -4.149734 -4.1963177 -4.2252121 -4.233429 -4.2322955 -4.2300262 -4.2271137][-4.1862159 -4.1869817 -4.1800427 -4.1665621 -4.1467853 -4.1378207 -4.1424913 -4.1638036 -4.2036572 -4.2395964 -4.2628651 -4.2699623 -4.2693896 -4.2683792 -4.2664714][-4.2416792 -4.248702 -4.2464504 -4.2374063 -4.22238 -4.2130394 -4.2126942 -4.223567 -4.2487664 -4.2730374 -4.2868347 -4.2876048 -4.2840657 -4.2823009 -4.2805815][-4.2638149 -4.2733293 -4.2734318 -4.26529 -4.2523174 -4.2420392 -4.2359967 -4.2399035 -4.2571731 -4.2753987 -4.2811127 -4.2738471 -4.2648048 -4.2600784 -4.2571964][-4.2647128 -4.2751904 -4.2761965 -4.2670727 -4.2518282 -4.2376766 -4.2259355 -4.2235661 -4.2358813 -4.250361 -4.2530117 -4.2405939 -4.2261238 -4.2188144 -4.215857][-4.2588086 -4.26537 -4.2614694 -4.2477059 -4.228579 -4.210588 -4.1932254 -4.1852331 -4.1934719 -4.204968 -4.2048564 -4.1872988 -4.1636534 -4.1504474 -4.1447687]]...]
INFO - root - 2017-12-07 18:15:47.902529: step 39210, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 56h:26m:35s remains)
INFO - root - 2017-12-07 18:15:54.686357: step 39220, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 54h:07m:56s remains)
INFO - root - 2017-12-07 18:16:01.486273: step 39230, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.632 sec/batch; 51h:27m:19s remains)
INFO - root - 2017-12-07 18:16:08.374743: step 39240, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 54h:23m:22s remains)
INFO - root - 2017-12-07 18:16:15.164716: step 39250, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 55h:02m:58s remains)
INFO - root - 2017-12-07 18:16:22.070459: step 39260, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.740 sec/batch; 60h:16m:22s remains)
INFO - root - 2017-12-07 18:16:28.925963: step 39270, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 53h:56m:10s remains)
INFO - root - 2017-12-07 18:16:35.652639: step 39280, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 52h:37m:17s remains)
INFO - root - 2017-12-07 18:16:42.475503: step 39290, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 55h:58m:15s remains)
INFO - root - 2017-12-07 18:16:49.453339: step 39300, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 59h:26m:22s remains)
2017-12-07 18:16:50.153294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2235813 -4.2002969 -4.1910753 -4.191515 -4.1956592 -4.2042065 -4.2099919 -4.2117639 -4.2210817 -4.2394252 -4.2499619 -4.2437725 -4.2328324 -4.2173462 -4.2081265][-4.2202497 -4.2061248 -4.209825 -4.2165565 -4.2196984 -4.2224569 -4.22261 -4.2185259 -4.2230144 -4.2384825 -4.248436 -4.2393947 -4.2257428 -4.2104859 -4.203618][-4.2309418 -4.2267537 -4.2393909 -4.2488375 -4.247694 -4.2452612 -4.2404604 -4.2285852 -4.2260971 -4.2365317 -4.2434764 -4.2346039 -4.220612 -4.2076912 -4.2017012][-4.2366672 -4.2354674 -4.248991 -4.2561173 -4.2505178 -4.2438383 -4.2351618 -4.2184091 -4.2111921 -4.2182941 -4.2250972 -4.2154288 -4.2060132 -4.1983485 -4.1923389][-4.2411861 -4.2317939 -4.2346253 -4.2358265 -4.2261648 -4.2175579 -4.2048554 -4.18634 -4.1796231 -4.1887512 -4.1937118 -4.1820455 -4.1763425 -4.1752582 -4.175909][-4.2364955 -4.212019 -4.2029705 -4.1960616 -4.1835718 -4.1724858 -4.1563816 -4.136034 -4.1320219 -4.1452994 -4.1513538 -4.1397686 -4.1368566 -4.1438284 -4.1559734][-4.2182226 -4.1792574 -4.1613812 -4.1467242 -4.126143 -4.102849 -4.0720863 -4.0463982 -4.0539541 -4.0861349 -4.103652 -4.1005259 -4.1026726 -4.1211443 -4.1492023][-4.2031317 -4.1524811 -4.1227341 -4.0971165 -4.0635252 -4.0215731 -3.9713411 -3.9427359 -3.9749973 -4.0376868 -4.0797434 -4.0919604 -4.1080308 -4.1363435 -4.1692786][-4.1864305 -4.1345797 -4.0960736 -4.0602832 -4.0219269 -3.9852452 -3.9503934 -3.940932 -3.9892161 -4.0564594 -4.1017947 -4.1252623 -4.152133 -4.1758823 -4.1983471][-4.17156 -4.1309032 -4.0995927 -4.0685091 -4.0425916 -4.0341744 -4.0348692 -4.0404611 -4.0687957 -4.1123176 -4.1524844 -4.1807852 -4.2088275 -4.2218633 -4.2275829][-4.1716685 -4.1463318 -4.1293745 -4.1127048 -4.1044836 -4.1144929 -4.1290569 -4.1348338 -4.1454706 -4.1696415 -4.20159 -4.2257476 -4.245141 -4.247211 -4.2420993][-4.1745253 -4.1616178 -4.157598 -4.1555014 -4.1604171 -4.1728821 -4.1817751 -4.1824512 -4.1864948 -4.1996932 -4.2172003 -4.2283392 -4.2345152 -4.2334552 -4.2348113][-4.1810489 -4.1754322 -4.17739 -4.180604 -4.1876678 -4.1980872 -4.2032976 -4.2059951 -4.2102537 -4.2158666 -4.2181969 -4.2169924 -4.2177696 -4.2219143 -4.2337456][-4.2204218 -4.2154889 -4.2161379 -4.2191329 -4.2252579 -4.2316637 -4.2341447 -4.2360144 -4.2399411 -4.2429409 -4.237555 -4.229907 -4.2306871 -4.2379212 -4.2539597][-4.2733765 -4.2678094 -4.2649727 -4.2666817 -4.2725544 -4.2755709 -4.2742143 -4.2735882 -4.2762375 -4.2768259 -4.2682781 -4.256083 -4.2544808 -4.2602382 -4.2730722]]...]
INFO - root - 2017-12-07 18:16:56.518617: step 39310, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 54h:11m:37s remains)
INFO - root - 2017-12-07 18:17:03.393068: step 39320, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 57h:43m:52s remains)
INFO - root - 2017-12-07 18:17:10.262056: step 39330, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 53h:46m:13s remains)
INFO - root - 2017-12-07 18:17:17.068376: step 39340, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 53h:17m:17s remains)
INFO - root - 2017-12-07 18:17:23.904157: step 39350, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.681 sec/batch; 55h:27m:47s remains)
INFO - root - 2017-12-07 18:17:30.750107: step 39360, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 56h:42m:22s remains)
INFO - root - 2017-12-07 18:17:37.598878: step 39370, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 56h:24m:35s remains)
INFO - root - 2017-12-07 18:17:44.363349: step 39380, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 54h:12m:39s remains)
INFO - root - 2017-12-07 18:17:51.111587: step 39390, loss = 2.04, batch loss = 1.98 (13.0 examples/sec; 0.616 sec/batch; 50h:08m:47s remains)
INFO - root - 2017-12-07 18:17:58.073357: step 39400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 56h:53m:07s remains)
2017-12-07 18:17:58.861077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3013959 -4.298502 -4.2930803 -4.2858787 -4.2785459 -4.2726483 -4.2695985 -4.2676244 -4.2704482 -4.27379 -4.2736411 -4.2749505 -4.2736974 -4.2724414 -4.2724943][-4.2874174 -4.2829056 -4.2771621 -4.2707286 -4.2655025 -4.2591877 -4.2526684 -4.2457285 -4.2502828 -4.2591009 -4.2619743 -4.2637467 -4.2600174 -4.2546921 -4.2527161][-4.2720065 -4.2663937 -4.260253 -4.2552395 -4.2519188 -4.2422862 -4.2269869 -4.2113972 -4.2173414 -4.2339854 -4.2423034 -4.2456789 -4.2434797 -4.23763 -4.2314453][-4.260622 -4.251595 -4.2415409 -4.2340426 -4.2287693 -4.211956 -4.1831074 -4.1572642 -4.1652136 -4.1912332 -4.2059312 -4.2155972 -4.2242484 -4.2246404 -4.2194958][-4.2503405 -4.236495 -4.2208066 -4.2072768 -4.1941051 -4.1666 -4.1190042 -4.07558 -4.0867896 -4.1328897 -4.1576672 -4.1724367 -4.1934 -4.2047629 -4.2061582][-4.2390075 -4.2187638 -4.1934814 -4.1676278 -4.1413441 -4.0999761 -4.0300093 -3.9611156 -3.9789326 -4.0563688 -4.0970883 -4.1154861 -4.1443691 -4.1656771 -4.1766815][-4.2317858 -4.20136 -4.1652293 -4.1271949 -4.0901318 -4.0396743 -3.9491014 -3.8492093 -3.8677735 -3.9770074 -4.0466285 -4.0818915 -4.1179576 -4.1414237 -4.1562519][-4.2296109 -4.1907291 -4.1477885 -4.1033125 -4.0611448 -4.0093503 -3.9168413 -3.8071878 -3.8179717 -3.9404619 -4.037487 -4.0942669 -4.134737 -4.1543713 -4.1624918][-4.235528 -4.1963592 -4.1589556 -4.1223536 -4.0912905 -4.0563073 -3.9890654 -3.9025192 -3.8983178 -3.9886019 -4.0802464 -4.1403151 -4.175951 -4.190825 -4.192997][-4.2492056 -4.2170324 -4.1915278 -4.1707344 -4.1556635 -4.1385951 -4.1006951 -4.0441823 -4.0287986 -4.08012 -4.1462855 -4.196517 -4.2240567 -4.2330394 -4.2319155][-4.2658596 -4.2414408 -4.2259226 -4.2179923 -4.21529 -4.2105546 -4.1918712 -4.1601129 -4.1464114 -4.1746211 -4.2166667 -4.2522488 -4.2694077 -4.2753429 -4.2737408][-4.2844148 -4.2679777 -4.2604542 -4.2627182 -4.2692494 -4.2716141 -4.2652187 -4.2516947 -4.244359 -4.2589817 -4.2797904 -4.2976713 -4.3063974 -4.3099146 -4.3059626][-4.3025208 -4.2941236 -4.2927709 -4.3015294 -4.3146772 -4.3226838 -4.3218794 -4.3144536 -4.3042603 -4.3029466 -4.3052859 -4.3098097 -4.3142977 -4.3193951 -4.3184428][-4.3136992 -4.3104334 -4.3108277 -4.3191004 -4.3325791 -4.3437009 -4.3461018 -4.3388004 -4.3253565 -4.3137889 -4.306829 -4.3062253 -4.3100691 -4.3161845 -4.3180695][-4.3197713 -4.3183451 -4.3177171 -4.3213725 -4.329062 -4.3367825 -4.3386192 -4.3318219 -4.3189969 -4.3066859 -4.3004994 -4.3008637 -4.305006 -4.3104281 -4.3134732]]...]
INFO - root - 2017-12-07 18:18:05.487732: step 39410, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 55h:21m:45s remains)
INFO - root - 2017-12-07 18:18:12.194184: step 39420, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 54h:11m:53s remains)
INFO - root - 2017-12-07 18:18:18.991482: step 39430, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 59h:32m:49s remains)
INFO - root - 2017-12-07 18:18:25.899059: step 39440, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 60h:20m:03s remains)
INFO - root - 2017-12-07 18:18:32.749741: step 39450, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 54h:03m:50s remains)
INFO - root - 2017-12-07 18:18:39.597727: step 39460, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.625 sec/batch; 50h:50m:03s remains)
INFO - root - 2017-12-07 18:18:46.495480: step 39470, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.673 sec/batch; 54h:44m:46s remains)
INFO - root - 2017-12-07 18:18:53.269687: step 39480, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 56h:56m:28s remains)
INFO - root - 2017-12-07 18:19:00.101972: step 39490, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:19m:26s remains)
INFO - root - 2017-12-07 18:19:06.810377: step 39500, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 53h:32m:09s remains)
2017-12-07 18:19:07.506023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1521516 -4.1607223 -4.1754527 -4.1935925 -4.204535 -4.2096572 -4.2115803 -4.2127357 -4.2222881 -4.2310557 -4.2153869 -4.1772161 -4.1385169 -4.1090565 -4.0868282][-4.0931745 -4.1311078 -4.1678629 -4.1992321 -4.2142091 -4.2190585 -4.2136488 -4.2072725 -4.212183 -4.2167511 -4.1941843 -4.1419888 -4.0873613 -4.0528193 -4.04027][-4.0266495 -4.1010885 -4.1643987 -4.2014637 -4.2115693 -4.201653 -4.17935 -4.1620026 -4.1613188 -4.1666613 -4.1518779 -4.1073561 -4.0630107 -4.0415592 -4.0445795][-4.0493693 -4.1348977 -4.1928205 -4.2123418 -4.2006311 -4.1596451 -4.111702 -4.08238 -4.0817733 -4.1024661 -4.1128554 -4.1017303 -4.0887861 -4.086031 -4.0964251][-4.1483564 -4.2078009 -4.2288933 -4.2107892 -4.1688485 -4.0969667 -4.0246267 -3.9963181 -4.0175414 -4.0644965 -4.1052446 -4.1292739 -4.143199 -4.1505928 -4.1550312][-4.2181363 -4.2391505 -4.2184334 -4.1582174 -4.0846796 -3.9862416 -3.8946557 -3.8933382 -3.9645658 -4.0492358 -4.1181645 -4.1631136 -4.1857882 -4.1854539 -4.1713519][-4.2385945 -4.2292051 -4.1754332 -4.0840278 -3.9872172 -3.8770025 -3.7827296 -3.8263731 -3.9430742 -4.0486631 -4.1286993 -4.1795096 -4.1999407 -4.1852508 -4.152854][-4.2421889 -4.2156391 -4.1493859 -4.0560532 -3.9684856 -3.8868864 -3.8273768 -3.8845167 -3.9934518 -4.0840549 -4.1501355 -4.1937189 -4.2076807 -4.1842575 -4.1467896][-4.2296996 -4.2002864 -4.138588 -4.0657454 -4.0088339 -3.9693062 -3.9494398 -3.9943006 -4.0718136 -4.1337662 -4.1762218 -4.207562 -4.2151933 -4.1930108 -4.1632342][-4.2155957 -4.186636 -4.1337652 -4.0815749 -4.04914 -4.0370984 -4.0437894 -4.0792923 -4.1309123 -4.1710906 -4.1947203 -4.2130103 -4.2178011 -4.2027006 -4.1856251][-4.2266297 -4.2025509 -4.1667919 -4.1371193 -4.1222291 -4.1224718 -4.1387215 -4.1621146 -4.1925654 -4.2139025 -4.2237496 -4.2318316 -4.2362022 -4.2286696 -4.2216082][-4.2664185 -4.254612 -4.23838 -4.2272491 -4.2202797 -4.2222219 -4.2349453 -4.2444248 -4.2568707 -4.2672482 -4.2709265 -4.2750397 -4.280767 -4.2792063 -4.2782621][-4.3105674 -4.3068333 -4.301414 -4.2992411 -4.2964096 -4.2993011 -4.3082237 -4.3111839 -4.3155127 -4.3214188 -4.3239479 -4.3264437 -4.3303857 -4.3288717 -4.3279428][-4.3336649 -4.3342013 -4.3346953 -4.3361011 -4.3355041 -4.3377228 -4.3429232 -4.3436365 -4.3449016 -4.3478193 -4.34891 -4.3492336 -4.3499594 -4.3473878 -4.34587][-4.3419652 -4.344965 -4.3470392 -4.34862 -4.3480411 -4.3477125 -4.3493085 -4.3491712 -4.3489156 -4.3493257 -4.3496213 -4.3492908 -4.3494291 -4.3486838 -4.3476276]]...]
INFO - root - 2017-12-07 18:19:14.104876: step 39510, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 57h:34m:39s remains)
INFO - root - 2017-12-07 18:19:21.062688: step 39520, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 56h:42m:03s remains)
INFO - root - 2017-12-07 18:19:27.961667: step 39530, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.648 sec/batch; 52h:42m:36s remains)
INFO - root - 2017-12-07 18:19:34.790454: step 39540, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 55h:36m:48s remains)
INFO - root - 2017-12-07 18:19:41.574176: step 39550, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 56h:37m:33s remains)
INFO - root - 2017-12-07 18:19:48.390674: step 39560, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 57h:06m:33s remains)
INFO - root - 2017-12-07 18:19:55.209377: step 39570, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 53h:32m:27s remains)
INFO - root - 2017-12-07 18:20:01.942787: step 39580, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.624 sec/batch; 50h:46m:41s remains)
INFO - root - 2017-12-07 18:20:08.718852: step 39590, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 55h:07m:32s remains)
INFO - root - 2017-12-07 18:20:15.557924: step 39600, loss = 2.10, batch loss = 2.05 (11.6 examples/sec; 0.692 sec/batch; 56h:16m:14s remains)
2017-12-07 18:20:16.314348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2927575 -4.2810731 -4.2731104 -4.2682476 -4.2713194 -4.2730265 -4.27292 -4.2723365 -4.2715807 -4.2659111 -4.2613029 -4.261848 -4.2651863 -4.2695508 -4.2740221][-4.2924423 -4.2737837 -4.2559729 -4.2433844 -4.2447939 -4.247963 -4.24831 -4.2492504 -4.2529316 -4.2536764 -4.252954 -4.2576694 -4.2646294 -4.2673745 -4.2674837][-4.2754154 -4.2510428 -4.2250357 -4.2033467 -4.2032371 -4.2083869 -4.2109971 -4.2141814 -4.223496 -4.2332153 -4.2414885 -4.2497644 -4.2553444 -4.2525415 -4.2439451][-4.2650905 -4.2383842 -4.2084827 -4.1826096 -4.1807504 -4.1802006 -4.1778326 -4.1820712 -4.1997995 -4.21722 -4.2305913 -4.2400079 -4.2442579 -4.2403078 -4.2262645][-4.2674227 -4.2398252 -4.2065997 -4.1799626 -4.173213 -4.1600385 -4.1427355 -4.1465354 -4.1757627 -4.207377 -4.2271433 -4.2403255 -4.2505031 -4.2483492 -4.2320037][-4.2547812 -4.2210512 -4.1806808 -4.1491256 -4.1296291 -4.0944233 -4.0559521 -4.05781 -4.1046977 -4.1627245 -4.202147 -4.2303352 -4.2549119 -4.2592349 -4.2452936][-4.1943893 -4.1446595 -4.0912457 -4.0494943 -4.0136304 -3.9579039 -3.8944392 -3.8876717 -3.9628608 -4.05851 -4.123929 -4.169261 -4.2085013 -4.2229285 -4.2136631][-4.1157312 -4.04437 -3.9707379 -3.9120555 -3.8618007 -3.7910981 -3.699312 -3.6705825 -3.7724929 -3.9038291 -3.9900665 -4.0482817 -4.0981688 -4.1235189 -4.1238165][-4.0816951 -3.996135 -3.9138851 -3.8489919 -3.7931902 -3.7200108 -3.6184626 -3.5681293 -3.6641288 -3.7935202 -3.8717077 -3.9217782 -3.9674659 -3.9967525 -4.0051155][-4.1034384 -4.0263963 -3.9571674 -3.9064431 -3.8642673 -3.8135202 -3.7448528 -3.7040856 -3.7518973 -3.8296506 -3.8711243 -3.8888807 -3.9062562 -3.9204421 -3.9250109][-4.1413651 -4.0799541 -4.0280166 -3.9971189 -3.9769895 -3.9526212 -3.9182708 -3.8932395 -3.908298 -3.9437523 -3.9590302 -3.9546332 -3.9497232 -3.9523411 -3.9566624][-4.1622777 -4.1200752 -4.0885687 -4.0752912 -4.0743475 -4.0717669 -4.0590672 -4.0442724 -4.0440464 -4.0592647 -4.0666718 -4.0626016 -4.0568867 -4.06123 -4.0707121][-4.161221 -4.1379852 -4.12661 -4.12995 -4.1463356 -4.1604609 -4.1615086 -4.1532412 -4.1460752 -4.1502438 -4.1556249 -4.1581845 -4.1579819 -4.1632771 -4.1730404][-4.1438746 -4.1342831 -4.1356659 -4.1480193 -4.1753936 -4.2013288 -4.2098188 -4.2027473 -4.1939912 -4.1922784 -4.1947 -4.1993542 -4.2016344 -4.2066007 -4.2138124][-4.1364155 -4.1336279 -4.1388364 -4.151248 -4.1812592 -4.2122607 -4.2245913 -4.2199521 -4.2138028 -4.2105088 -4.2091956 -4.2108016 -4.2114682 -4.2135425 -4.2171879]]...]
INFO - root - 2017-12-07 18:20:22.827768: step 39610, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 52h:30m:17s remains)
INFO - root - 2017-12-07 18:20:29.435179: step 39620, loss = 2.05, batch loss = 2.00 (10.8 examples/sec; 0.741 sec/batch; 60h:15m:05s remains)
INFO - root - 2017-12-07 18:20:36.220884: step 39630, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 56h:09m:11s remains)
INFO - root - 2017-12-07 18:20:42.999420: step 39640, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 51h:39m:23s remains)
INFO - root - 2017-12-07 18:20:49.835496: step 39650, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 52h:20m:29s remains)
INFO - root - 2017-12-07 18:20:56.716586: step 39660, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 56h:33m:19s remains)
INFO - root - 2017-12-07 18:21:03.571797: step 39670, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 58h:29m:05s remains)
INFO - root - 2017-12-07 18:21:10.390770: step 39680, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 57h:01m:51s remains)
INFO - root - 2017-12-07 18:21:17.207868: step 39690, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 54h:00m:09s remains)
INFO - root - 2017-12-07 18:21:23.982787: step 39700, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 53h:55m:08s remains)
2017-12-07 18:21:24.674760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2248383 -4.192183 -4.1615434 -4.1581531 -4.1724353 -4.1876054 -4.1949868 -4.1838312 -4.1669364 -4.1677632 -4.1933923 -4.2256775 -4.2367043 -4.2382269 -4.2525253][-4.2072883 -4.1785674 -4.1492162 -4.1427341 -4.1531658 -4.1619959 -4.1572828 -4.13728 -4.1180549 -4.1216383 -4.1559062 -4.1953592 -4.2072458 -4.2125521 -4.2267208][-4.1775393 -4.1459994 -4.1214924 -4.1229916 -4.1358662 -4.1400232 -4.1234 -4.0977082 -4.0805392 -4.0858459 -4.1189337 -4.1489048 -4.1493111 -4.1609082 -4.1873722][-4.155364 -4.1282825 -4.1085939 -4.1141849 -4.1325808 -4.1331091 -4.1067238 -4.0784216 -4.0694923 -4.0895824 -4.1227956 -4.13681 -4.1239491 -4.1344776 -4.1659884][-4.1359472 -4.1307335 -4.1213212 -4.128788 -4.1461778 -4.142561 -4.1107168 -4.0772085 -4.0733676 -4.1138082 -4.16248 -4.178349 -4.1647267 -4.1688023 -4.1919818][-4.1387424 -4.1497717 -4.1469288 -4.1504736 -4.1543708 -4.1379004 -4.0916839 -4.0235257 -3.9959681 -4.0595603 -4.1492391 -4.2028155 -4.2159219 -4.2250214 -4.2421656][-4.1504049 -4.1666961 -4.16129 -4.1507964 -4.132019 -4.0880704 -4.0049114 -3.8666222 -3.7933822 -3.9058816 -4.06598 -4.1718822 -4.2237811 -4.24868 -4.2581954][-4.1719313 -4.1815419 -4.1669259 -4.1391659 -4.1002035 -4.0335279 -3.9150834 -3.7128258 -3.596174 -3.7647567 -3.9851177 -4.1246853 -4.2010994 -4.236412 -4.2386193][-4.2035332 -4.2038546 -4.1797948 -4.1425362 -4.1043267 -4.0555534 -3.9650898 -3.8077097 -3.7201047 -3.8589683 -4.0392308 -4.1516418 -4.21446 -4.2318277 -4.2113743][-4.2407346 -4.2352362 -4.2083192 -4.177824 -4.1673651 -4.1585784 -4.117413 -4.0255265 -3.9750237 -4.0550566 -4.1569538 -4.2228236 -4.2547774 -4.2458305 -4.201539][-4.2711248 -4.2615762 -4.240314 -4.2242055 -4.2364445 -4.251133 -4.2388358 -4.1852441 -4.1535983 -4.1953216 -4.2442017 -4.2703 -4.2708912 -4.2451625 -4.1962943][-4.27897 -4.2729959 -4.2623882 -4.2566743 -4.270524 -4.2875681 -4.2859988 -4.2534695 -4.2340665 -4.2550983 -4.2715244 -4.2731247 -4.254498 -4.2219267 -4.1835084][-4.269609 -4.2722526 -4.2684655 -4.2607484 -4.2612958 -4.2688494 -4.2704244 -4.2528868 -4.2387209 -4.2541814 -4.264009 -4.2549548 -4.2255206 -4.1993451 -4.1831975][-4.251163 -4.2598715 -4.2583351 -4.2430849 -4.226799 -4.221489 -4.2193546 -4.2057753 -4.1948905 -4.2152224 -4.2319684 -4.226625 -4.2008142 -4.1845255 -4.1872692][-4.226562 -4.23107 -4.2185125 -4.1942444 -4.1674485 -4.1561041 -4.1554961 -4.1500869 -4.1562705 -4.18974 -4.2140651 -4.2150927 -4.2004142 -4.19479 -4.2065353]]...]
INFO - root - 2017-12-07 18:21:31.204603: step 39710, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 54h:53m:10s remains)
INFO - root - 2017-12-07 18:21:38.026162: step 39720, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 53h:29m:07s remains)
INFO - root - 2017-12-07 18:21:44.838840: step 39730, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 53h:38m:07s remains)
INFO - root - 2017-12-07 18:21:51.745868: step 39740, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 57h:27m:41s remains)
INFO - root - 2017-12-07 18:21:58.565582: step 39750, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 57h:10m:03s remains)
INFO - root - 2017-12-07 18:22:05.396318: step 39760, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 56h:21m:41s remains)
INFO - root - 2017-12-07 18:22:12.130050: step 39770, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 51h:33m:22s remains)
INFO - root - 2017-12-07 18:22:18.996733: step 39780, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.697 sec/batch; 56h:39m:27s remains)
INFO - root - 2017-12-07 18:22:25.768724: step 39790, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 60h:01m:43s remains)
INFO - root - 2017-12-07 18:22:32.548067: step 39800, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.686 sec/batch; 55h:47m:49s remains)
2017-12-07 18:22:33.275709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.282249 -4.2674594 -4.2607379 -4.2745457 -4.3044553 -4.3331184 -4.3378038 -4.3098803 -4.2598 -4.2167039 -4.1967111 -4.1971307 -4.2072043 -4.2239957 -4.2399755][-4.2791777 -4.2745481 -4.2746668 -4.2942615 -4.32519 -4.3462257 -4.3373427 -4.2944822 -4.2335439 -4.1915703 -4.1796622 -4.1863942 -4.2018418 -4.21947 -4.235374][-4.2784314 -4.2852407 -4.2964096 -4.3210855 -4.3454614 -4.3511662 -4.3263316 -4.270236 -4.2045527 -4.1738505 -4.1766758 -4.1915908 -4.2095294 -4.2241278 -4.2369995][-4.2961769 -4.3159842 -4.3382416 -4.3597236 -4.3669724 -4.3516479 -4.3085132 -4.2348666 -4.1633663 -4.1481128 -4.1705551 -4.1988239 -4.2261524 -4.2417226 -4.2504277][-4.3197966 -4.3488469 -4.3744521 -4.3824773 -4.365119 -4.3257442 -4.2588935 -4.162334 -4.0812206 -4.086144 -4.1397667 -4.1932673 -4.2384963 -4.2615685 -4.2659378][-4.3352995 -4.366128 -4.3847504 -4.374054 -4.3352604 -4.2757363 -4.1833935 -4.0574169 -3.9616508 -3.9927962 -4.0897179 -4.1782761 -4.2440839 -4.2709131 -4.2697544][-4.33815 -4.3652005 -4.3707714 -4.3420353 -4.2873755 -4.2142148 -4.1021 -3.9467235 -3.8386722 -3.9055917 -4.0474024 -4.1667871 -4.2479043 -4.2724953 -4.267807][-4.3264756 -4.3433008 -4.3308167 -4.290627 -4.2304287 -4.1572 -4.0381122 -3.8642457 -3.7698092 -3.8800387 -4.049777 -4.1747503 -4.2539854 -4.2736292 -4.2699165][-4.3103447 -4.3152628 -4.29311 -4.2526345 -4.1981397 -4.1362867 -4.0251718 -3.8658023 -3.8078151 -3.9398558 -4.0987039 -4.2018976 -4.2650862 -4.28029 -4.2769566][-4.3048582 -4.302669 -4.2792883 -4.2444563 -4.2010031 -4.1532469 -4.0694151 -3.955909 -3.9338729 -4.0462427 -4.1596293 -4.2344918 -4.2795343 -4.2873616 -4.281466][-4.3024945 -4.3004575 -4.2795143 -4.2489147 -4.2141428 -4.1823878 -4.1292443 -4.0606852 -4.05973 -4.1409664 -4.21756 -4.2695537 -4.2960362 -4.2923946 -4.2817521][-4.2974949 -4.2981462 -4.2799482 -4.2546024 -4.23121 -4.2171335 -4.1876163 -4.1462317 -4.1493807 -4.2047038 -4.2570024 -4.2904553 -4.3011918 -4.291173 -4.2801189][-4.2901125 -4.293467 -4.2801828 -4.2642345 -4.2556438 -4.2565522 -4.2434077 -4.2173252 -4.2181344 -4.2506657 -4.2818155 -4.299273 -4.2977276 -4.2847953 -4.2736549][-4.2818742 -4.2810235 -4.2717347 -4.2636819 -4.2688594 -4.28224 -4.2835121 -4.2719011 -4.2716422 -4.286068 -4.298326 -4.3032389 -4.2964911 -4.28425 -4.2718611][-4.2808924 -4.2721295 -4.2577953 -4.2514915 -4.2660828 -4.2908645 -4.3049774 -4.3038964 -4.3029528 -4.3060408 -4.3048887 -4.3019214 -4.2966146 -4.2877164 -4.2758627]]...]
INFO - root - 2017-12-07 18:22:39.824070: step 39810, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.699 sec/batch; 56h:49m:20s remains)
INFO - root - 2017-12-07 18:22:46.634520: step 39820, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 56h:56m:48s remains)
INFO - root - 2017-12-07 18:22:53.451465: step 39830, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 54h:53m:00s remains)
INFO - root - 2017-12-07 18:23:00.286444: step 39840, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 54h:40m:43s remains)
INFO - root - 2017-12-07 18:23:07.195910: step 39850, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 52h:26m:29s remains)
INFO - root - 2017-12-07 18:23:14.152605: step 39860, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 56h:17m:07s remains)
INFO - root - 2017-12-07 18:23:21.030138: step 39870, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 54h:49m:27s remains)
INFO - root - 2017-12-07 18:23:27.775137: step 39880, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 53h:09m:37s remains)
INFO - root - 2017-12-07 18:23:34.520016: step 39890, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 52h:18m:49s remains)
INFO - root - 2017-12-07 18:23:41.293737: step 39900, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 53h:16m:57s remains)
2017-12-07 18:23:42.025777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2758389 -4.293654 -4.3041205 -4.3036842 -4.2947907 -4.2846909 -4.2744923 -4.2640114 -4.23522 -4.1824441 -4.1317587 -4.1207976 -4.1449227 -4.1814737 -4.2281213][-4.2544813 -4.274354 -4.286262 -4.2848053 -4.2719903 -4.2600803 -4.2518458 -4.246316 -4.2279816 -4.1875825 -4.1441917 -4.1315246 -4.1508412 -4.1860943 -4.2285814][-4.2174816 -4.2423062 -4.2581124 -4.2560024 -4.2412934 -4.22999 -4.2209916 -4.217093 -4.2125015 -4.1952376 -4.169982 -4.1620026 -4.1772981 -4.2086344 -4.2438049][-4.1681094 -4.1957316 -4.21393 -4.2150722 -4.2022419 -4.1920276 -4.1786923 -4.1667185 -4.1722779 -4.1857815 -4.1862 -4.189425 -4.2061768 -4.2337155 -4.2631111][-4.1359963 -4.1657505 -4.1852031 -4.1888828 -4.1779971 -4.1603589 -4.1217189 -4.076334 -4.0811806 -4.1332655 -4.17118 -4.1962852 -4.2214842 -4.2520618 -4.2812519][-4.156929 -4.1846638 -4.1965804 -4.188612 -4.1669869 -4.1267962 -4.0388064 -3.9336004 -3.9275141 -4.0277653 -4.112215 -4.1692934 -4.2121944 -4.2535753 -4.2883906][-4.2011981 -4.2217736 -4.2239342 -4.2010217 -4.1589336 -4.0880547 -3.9452331 -3.7693515 -3.7420225 -3.8918624 -4.0250521 -4.1138711 -4.1800919 -4.2372832 -4.2802844][-4.244132 -4.2573576 -4.2533045 -4.2249904 -4.1681314 -4.07675 -3.9142113 -3.7190459 -3.6752207 -3.8318086 -3.9784007 -4.0750356 -4.1498265 -4.2162848 -4.265367][-4.2648177 -4.2756567 -4.27296 -4.2476377 -4.1890454 -4.1041951 -3.9708524 -3.8280809 -3.7892795 -3.8903744 -3.995872 -4.072351 -4.1375241 -4.2004933 -4.2491574][-4.2683749 -4.2811127 -4.2841797 -4.2689538 -4.2203894 -4.1543655 -4.0659742 -3.9819283 -3.9500365 -3.986738 -4.0349231 -4.0791569 -4.1298461 -4.1863189 -4.2346592][-4.2684116 -4.2848396 -4.2971148 -4.291419 -4.260798 -4.2169437 -4.1645651 -4.1160293 -4.0839243 -4.0740266 -4.0756841 -4.0908422 -4.1275821 -4.1793017 -4.2268472][-4.2712984 -4.29142 -4.3106775 -4.3127074 -4.2966156 -4.2705994 -4.2387452 -4.2080307 -4.1752262 -4.1420193 -4.1194797 -4.116158 -4.1416626 -4.1876903 -4.2309046][-4.2781405 -4.2991819 -4.3216424 -4.327189 -4.3184123 -4.3034582 -4.2829804 -4.26623 -4.2406297 -4.204567 -4.1739392 -4.1609845 -4.1779718 -4.2102222 -4.2432256][-4.2903686 -4.3075929 -4.3249011 -4.3297691 -4.3267817 -4.3192773 -4.3069267 -4.2985392 -4.2830453 -4.2563663 -4.2287116 -4.212687 -4.2199793 -4.2373157 -4.25566][-4.3051548 -4.3181806 -4.3278036 -4.3294692 -4.3284578 -4.3254619 -4.3188562 -4.3148174 -4.3076 -4.2953348 -4.2798142 -4.26654 -4.2629132 -4.2668109 -4.272964]]...]
INFO - root - 2017-12-07 18:23:48.565877: step 39910, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 53h:05m:48s remains)
INFO - root - 2017-12-07 18:23:55.356505: step 39920, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 52h:52m:20s remains)
INFO - root - 2017-12-07 18:24:02.031092: step 39930, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 56h:10m:27s remains)
INFO - root - 2017-12-07 18:24:08.843879: step 39940, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 55h:15m:43s remains)
INFO - root - 2017-12-07 18:24:15.581998: step 39950, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 51h:57m:13s remains)
INFO - root - 2017-12-07 18:24:22.338617: step 39960, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 53h:00m:15s remains)
INFO - root - 2017-12-07 18:24:29.201439: step 39970, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 55h:13m:37s remains)
INFO - root - 2017-12-07 18:24:36.050111: step 39980, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 58h:25m:49s remains)
INFO - root - 2017-12-07 18:24:42.846847: step 39990, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 55h:22m:15s remains)
INFO - root - 2017-12-07 18:24:49.585343: step 40000, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 52h:58m:13s remains)
2017-12-07 18:24:50.310788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2329865 -4.1940165 -4.1239161 -4.0769262 -4.105052 -4.1609888 -4.1937294 -4.1996579 -4.1998544 -4.2077465 -4.2143068 -4.2129507 -4.20874 -4.2015262 -4.19519][-4.2170339 -4.1955733 -4.1383853 -4.074482 -4.0809155 -4.1378441 -4.1847072 -4.1987762 -4.1987953 -4.2004156 -4.2039294 -4.2043638 -4.2041626 -4.2018304 -4.20094][-4.2014279 -4.1994948 -4.1681089 -4.1067576 -4.0801535 -4.1207867 -4.1755691 -4.1992707 -4.1996708 -4.1937227 -4.1929426 -4.1971431 -4.203084 -4.2028847 -4.2027369][-4.1954079 -4.2068028 -4.1983509 -4.148139 -4.0915384 -4.0980358 -4.1530561 -4.1934137 -4.2010694 -4.1910348 -4.18154 -4.1872087 -4.1963015 -4.1975975 -4.1969509][-4.2044754 -4.2184663 -4.2226853 -4.1825657 -4.1050835 -4.0630198 -4.1053452 -4.1669788 -4.19689 -4.192276 -4.1780558 -4.1811509 -4.1897569 -4.1906309 -4.1902289][-4.2201543 -4.2297974 -4.2346396 -4.2018156 -4.1143136 -4.0262551 -4.0343804 -4.1153765 -4.1810641 -4.200376 -4.1964884 -4.1965446 -4.2001295 -4.1974306 -4.1932788][-4.2289486 -4.2322893 -4.23105 -4.2014108 -4.1151257 -3.9978285 -3.9583106 -4.04516 -4.1482606 -4.2046256 -4.2209759 -4.2227764 -4.2210126 -4.2144485 -4.20658][-4.2288628 -4.2286887 -4.2226787 -4.1961517 -4.1249266 -4.0088439 -3.9338768 -3.9929612 -4.111937 -4.1982303 -4.2362947 -4.2434368 -4.2381964 -4.228961 -4.2171106][-4.2282205 -4.2225361 -4.2151437 -4.1980686 -4.1581664 -4.0789909 -4.0009031 -4.0030665 -4.0944281 -4.1859131 -4.2394552 -4.2545795 -4.2509656 -4.2399716 -4.2223167][-4.2329874 -4.2194352 -4.204371 -4.19442 -4.1826835 -4.1501746 -4.093369 -4.05699 -4.0939012 -4.1685376 -4.2291327 -4.2548957 -4.2570624 -4.2471972 -4.2256527][-4.2440367 -4.2219105 -4.1932206 -4.17892 -4.1839838 -4.1897388 -4.1640677 -4.1214919 -4.1080294 -4.1486316 -4.205512 -4.2417846 -4.251924 -4.2477622 -4.228117][-4.2580614 -4.23369 -4.1954355 -4.1705947 -4.1799226 -4.2071266 -4.207674 -4.1735377 -4.1314869 -4.1298246 -4.1696281 -4.2124362 -4.2369895 -4.24163 -4.2277966][-4.2640467 -4.2462435 -4.2130418 -4.1860075 -4.191041 -4.2206011 -4.23239 -4.2099652 -4.1603093 -4.1239929 -4.1372228 -4.1786704 -4.2141829 -4.2263803 -4.2188921][-4.2611041 -4.2534304 -4.2335453 -4.2116442 -4.2113094 -4.2340288 -4.2459407 -4.2331133 -4.1903925 -4.1403027 -4.1276035 -4.1591287 -4.1968455 -4.2128797 -4.2076793][-4.2519817 -4.2531033 -4.2441759 -4.2291 -4.22351 -4.232316 -4.2357683 -4.2263374 -4.197947 -4.1565766 -4.1362844 -4.1578732 -4.192378 -4.2081556 -4.2031665]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 18:24:57.762420: step 40010, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 57h:31m:16s remains)
INFO - root - 2017-12-07 18:25:04.683810: step 40020, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 58h:18m:35s remains)
INFO - root - 2017-12-07 18:25:11.673874: step 40030, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.759 sec/batch; 61h:38m:33s remains)
INFO - root - 2017-12-07 18:25:18.613898: step 40040, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 56h:09m:51s remains)
INFO - root - 2017-12-07 18:25:25.411874: step 40050, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 51h:50m:09s remains)
INFO - root - 2017-12-07 18:25:32.366944: step 40060, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 53h:55m:54s remains)
INFO - root - 2017-12-07 18:25:39.104628: step 40070, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 60h:20m:36s remains)
INFO - root - 2017-12-07 18:25:45.881527: step 40080, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 57h:00m:13s remains)
INFO - root - 2017-12-07 18:25:52.715520: step 40090, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 56h:00m:09s remains)
INFO - root - 2017-12-07 18:25:59.506525: step 40100, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 55h:03m:23s remains)
2017-12-07 18:26:00.268712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.006475 -4.0803022 -4.1422353 -4.1831279 -4.1779971 -4.1388407 -4.1201982 -4.142745 -4.1878915 -4.2113962 -4.2150688 -4.2326913 -4.2529726 -4.2478924 -4.2188239][-3.9945438 -4.059803 -4.1189923 -4.1702161 -4.1821747 -4.1573954 -4.1441107 -4.1678143 -4.2069807 -4.2194262 -4.2141833 -4.2207618 -4.233849 -4.230906 -4.2045302][-4.0397706 -4.0782909 -4.1174436 -4.1607704 -4.1850863 -4.1752348 -4.1696835 -4.1916227 -4.2212181 -4.227015 -4.2233791 -4.2242069 -4.2303658 -4.2242441 -4.1924558][-4.1188803 -4.1303992 -4.1483097 -4.1680007 -4.1816196 -4.17366 -4.1649241 -4.1792693 -4.2055058 -4.2162681 -4.2228785 -4.228447 -4.2352228 -4.22612 -4.1894255][-4.1766586 -4.1804719 -4.1888013 -4.1870761 -4.1734724 -4.1502872 -4.1195779 -4.1130924 -4.1441226 -4.1761379 -4.2011271 -4.2216158 -4.2355938 -4.2261305 -4.1924682][-4.1898026 -4.194797 -4.19958 -4.184073 -4.1481309 -4.0989575 -4.0280542 -3.9849479 -4.033206 -4.1039195 -4.1590958 -4.2012367 -4.2287412 -4.2302179 -4.2042537][-4.17913 -4.1862354 -4.193131 -4.1689973 -4.1182785 -4.0372329 -3.9007788 -3.7914472 -3.8624868 -3.9884944 -4.0830774 -4.1539593 -4.1987681 -4.21108 -4.1967707][-4.1862755 -4.1932797 -4.2026825 -4.1805449 -4.1291833 -4.0377979 -3.8629537 -3.6824694 -3.7423642 -3.8972254 -4.0197358 -4.1085424 -4.1576924 -4.1733913 -4.1678486][-4.197412 -4.2062631 -4.224575 -4.2202911 -4.1825366 -4.1053195 -3.9588714 -3.7946324 -3.810257 -3.9294307 -4.0352206 -4.10837 -4.1440125 -4.1544948 -4.1572013][-4.1971354 -4.200285 -4.2192831 -4.23739 -4.2271047 -4.1780143 -4.0807495 -3.9714994 -3.9643965 -4.0339909 -4.1054406 -4.1505923 -4.168612 -4.1685123 -4.1669431][-4.1906552 -4.1852574 -4.1975546 -4.2269859 -4.2412605 -4.2153082 -4.1520758 -4.0833225 -4.070137 -4.1105266 -4.1552634 -4.1800022 -4.1859269 -4.1728973 -4.1583633][-4.2014337 -4.1883941 -4.1852269 -4.2051234 -4.222476 -4.2057867 -4.15848 -4.1166058 -4.1103506 -4.1404839 -4.173316 -4.1857753 -4.18299 -4.1563568 -4.1282825][-4.2253561 -4.2086544 -4.1941996 -4.1944246 -4.2011194 -4.1803789 -4.1362572 -4.1111555 -4.1197939 -4.1505976 -4.1813745 -4.1882038 -4.1815076 -4.1447129 -4.1091371][-4.2375393 -4.2225833 -4.2083044 -4.198514 -4.1960878 -4.1735821 -4.1321774 -4.1132965 -4.1290011 -4.1612835 -4.1941891 -4.2007179 -4.1880727 -4.1447935 -4.1087003][-4.2382183 -4.2201495 -4.2110496 -4.2069654 -4.2047405 -4.1897535 -4.1598063 -4.1420741 -4.1521325 -4.1775193 -4.2074161 -4.2144928 -4.2013969 -4.1599612 -4.1266379]]...]
INFO - root - 2017-12-07 18:26:06.874474: step 40110, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 54h:54m:59s remains)
INFO - root - 2017-12-07 18:26:13.770672: step 40120, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 56h:14m:27s remains)
INFO - root - 2017-12-07 18:26:20.579484: step 40130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 52h:29m:45s remains)
INFO - root - 2017-12-07 18:26:27.346285: step 40140, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 57h:10m:50s remains)
INFO - root - 2017-12-07 18:26:34.293717: step 40150, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 58h:39m:00s remains)
INFO - root - 2017-12-07 18:26:41.084533: step 40160, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 55h:51m:51s remains)
INFO - root - 2017-12-07 18:26:47.871581: step 40170, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 51h:50m:14s remains)
INFO - root - 2017-12-07 18:26:54.638236: step 40180, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 53h:11m:24s remains)
INFO - root - 2017-12-07 18:27:01.361612: step 40190, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 56h:20m:13s remains)
INFO - root - 2017-12-07 18:27:08.178145: step 40200, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 59h:59m:11s remains)
2017-12-07 18:27:08.831696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3056641 -4.2944622 -4.2756729 -4.2553115 -4.2485485 -4.2489281 -4.2464633 -4.2471395 -4.259397 -4.2704191 -4.2783451 -4.2884679 -4.2961907 -4.2993264 -4.300972][-4.3097472 -4.3008752 -4.2824669 -4.257575 -4.2416763 -4.2363257 -4.2349515 -4.2426991 -4.2598987 -4.272481 -4.28051 -4.2868295 -4.2877808 -4.2842617 -4.2809458][-4.31653 -4.3122559 -4.2960763 -4.2679009 -4.2410851 -4.2237196 -4.2164521 -4.2266726 -4.2506394 -4.269558 -4.2819219 -4.287447 -4.2814474 -4.271975 -4.2655077][-4.3249912 -4.3258295 -4.31172 -4.2788177 -4.2364483 -4.1996384 -4.1798577 -4.1878357 -4.22078 -4.2539697 -4.2757816 -4.2832646 -4.2738132 -4.2598639 -4.2493181][-4.33264 -4.3365278 -4.322382 -4.2810569 -4.2210073 -4.1614866 -4.12589 -4.1318116 -4.1766715 -4.2256227 -4.2581849 -4.26927 -4.2607155 -4.2421331 -4.2268395][-4.3382721 -4.3419838 -4.3233614 -4.272871 -4.1972 -4.1157036 -4.06181 -4.0694771 -4.1298566 -4.1946521 -4.2362485 -4.2526426 -4.2497721 -4.2284408 -4.20805][-4.3431625 -4.3442593 -4.3186607 -4.2604594 -4.1747804 -4.0778117 -4.0114784 -4.0219827 -4.096077 -4.1732068 -4.2205958 -4.2416992 -4.2461042 -4.2251835 -4.2044473][-4.3455839 -4.3438869 -4.3135467 -4.2515135 -4.1633253 -4.0623317 -3.9907184 -4.0009274 -4.0778737 -4.1600037 -4.2133017 -4.239387 -4.2472591 -4.2249942 -4.2000113][-4.3467603 -4.3442674 -4.3149877 -4.2558484 -4.1746054 -4.0815415 -4.0109172 -4.0133247 -4.0773535 -4.1532197 -4.2093472 -4.2376189 -4.2435117 -4.2160306 -4.1831527][-4.346446 -4.3438954 -4.3186197 -4.267045 -4.1979952 -4.120791 -4.0578656 -4.0520306 -4.0969543 -4.1578431 -4.2096539 -4.2362723 -4.2384424 -4.2059827 -4.1660395][-4.3472028 -4.3448024 -4.3243136 -4.2811885 -4.2233329 -4.1629152 -4.1138711 -4.1042552 -4.1292825 -4.173646 -4.2158904 -4.2352738 -4.2328992 -4.1984591 -4.1557369][-4.3478894 -4.3470902 -4.3318515 -4.2954435 -4.2451859 -4.1987848 -4.1645422 -4.1538386 -4.1628122 -4.1909676 -4.21911 -4.2309804 -4.2252331 -4.19274 -4.1537809][-4.347857 -4.3505073 -4.3394089 -4.306519 -4.2615943 -4.2239985 -4.1987324 -4.1877084 -4.188046 -4.2038155 -4.2185831 -4.2162218 -4.2056603 -4.1789474 -4.1532431][-4.3478527 -4.3545294 -4.3480468 -4.31786 -4.2768459 -4.2420197 -4.2199297 -4.2088084 -4.202271 -4.2061691 -4.2081041 -4.1920176 -4.1753058 -4.155983 -4.1458707][-4.3479304 -4.3578825 -4.3551955 -4.3262367 -4.285532 -4.2493024 -4.2267704 -4.2160864 -4.2073622 -4.2022948 -4.193912 -4.1680884 -4.14574 -4.1283007 -4.1262159]]...]
INFO - root - 2017-12-07 18:27:15.387147: step 40210, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.666 sec/batch; 54h:04m:13s remains)
INFO - root - 2017-12-07 18:27:22.208250: step 40220, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 57h:15m:37s remains)
INFO - root - 2017-12-07 18:27:29.064318: step 40230, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 58h:19m:41s remains)
INFO - root - 2017-12-07 18:27:35.634862: step 40240, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 50h:54m:15s remains)
INFO - root - 2017-12-07 18:27:42.480364: step 40250, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 55h:27m:02s remains)
INFO - root - 2017-12-07 18:27:49.389074: step 40260, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 56h:53m:37s remains)
INFO - root - 2017-12-07 18:27:56.259156: step 40270, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 57h:07m:46s remains)
INFO - root - 2017-12-07 18:28:03.093986: step 40280, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 52h:46m:12s remains)
INFO - root - 2017-12-07 18:28:09.855324: step 40290, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 52h:12m:53s remains)
INFO - root - 2017-12-07 18:28:16.695505: step 40300, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 57h:21m:56s remains)
2017-12-07 18:28:17.350670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2616749 -4.2390976 -4.2045035 -4.172183 -4.1681995 -4.1902056 -4.2192788 -4.2485857 -4.2744823 -4.2934036 -4.2977834 -4.2954822 -4.2933173 -4.2905679 -4.2825508][-4.278748 -4.2394724 -4.1801467 -4.1281619 -4.12572 -4.165041 -4.208837 -4.2498436 -4.2847028 -4.3089366 -4.3158255 -4.3135223 -4.3106222 -4.3054953 -4.2902222][-4.296968 -4.2474351 -4.1639419 -4.0857129 -4.0785627 -4.1323004 -4.1871715 -4.2376838 -4.2827444 -4.3143058 -4.3258352 -4.3261576 -4.3273673 -4.3254585 -4.3099537][-4.308939 -4.258431 -4.158649 -4.05311 -4.0315733 -4.0888796 -4.1494164 -4.207334 -4.2626 -4.3038945 -4.3219194 -4.3271728 -4.3335042 -4.3399868 -4.3330169][-4.3132873 -4.2675867 -4.1640439 -4.0380006 -3.9926796 -4.0406852 -4.09944 -4.1581969 -4.2230744 -4.276886 -4.3058248 -4.3178911 -4.3283825 -4.3417821 -4.3457174][-4.31074 -4.2737703 -4.1796312 -4.0445542 -3.9640663 -3.9858272 -4.038743 -4.0980339 -4.1723742 -4.2392268 -4.28235 -4.3037267 -4.318697 -4.3357487 -4.347044][-4.3123088 -4.2841024 -4.2026305 -4.0610495 -3.9337151 -3.913435 -3.9587488 -4.0282106 -4.1188364 -4.2015486 -4.2584176 -4.2886415 -4.3075976 -4.3262949 -4.3415179][-4.3171349 -4.2955055 -4.226995 -4.088932 -3.9294734 -3.8646629 -3.8955526 -3.9758725 -4.0842648 -4.1783929 -4.2429805 -4.2788796 -4.2994633 -4.3174677 -4.3334165][-4.326438 -4.3119106 -4.2613525 -4.149025 -3.9950311 -3.8968303 -3.8938289 -3.9648156 -4.0755548 -4.1698937 -4.2320242 -4.2704082 -4.2943125 -4.3122048 -4.3278365][-4.3376703 -4.3324318 -4.302711 -4.2265544 -4.0987067 -3.9802492 -3.9327371 -3.9720864 -4.0680089 -4.155427 -4.2120309 -4.2520003 -4.2828722 -4.30512 -4.3229375][-4.3454852 -4.3471403 -4.3334055 -4.2886677 -4.1911287 -4.0682573 -3.9811485 -3.9787235 -4.051312 -4.1330934 -4.1893644 -4.2346711 -4.2735467 -4.3027663 -4.3228168][-4.3486571 -4.3519573 -4.3467903 -4.3230939 -4.2559075 -4.1474648 -4.0404673 -3.9938452 -4.0332503 -4.1071386 -4.1657596 -4.2192349 -4.267385 -4.3014212 -4.3217125][-4.3441358 -4.3432727 -4.3414826 -4.3318367 -4.2912054 -4.2094569 -4.1034651 -4.0225625 -4.0226426 -4.0802441 -4.1390619 -4.2003269 -4.25742 -4.2942996 -4.3128238][-4.3315511 -4.3245778 -4.3222327 -4.321701 -4.3040371 -4.2548556 -4.169539 -4.0785413 -4.0464563 -4.0811963 -4.135519 -4.1982379 -4.2566404 -4.2887297 -4.2998757][-4.3152728 -4.3033481 -4.2985697 -4.302784 -4.3024826 -4.2833896 -4.2314663 -4.1584082 -4.1127133 -4.1225009 -4.1649842 -4.2184677 -4.2661586 -4.28667 -4.2854342]]...]
INFO - root - 2017-12-07 18:28:23.574298: step 40310, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 57h:10m:13s remains)
INFO - root - 2017-12-07 18:28:30.506359: step 40320, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 53h:10m:34s remains)
INFO - root - 2017-12-07 18:28:37.340680: step 40330, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 57h:33m:09s remains)
INFO - root - 2017-12-07 18:28:44.205876: step 40340, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 56h:53m:18s remains)
INFO - root - 2017-12-07 18:28:51.061907: step 40350, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 54h:02m:16s remains)
INFO - root - 2017-12-07 18:28:57.887655: step 40360, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 51h:48m:25s remains)
INFO - root - 2017-12-07 18:29:04.714669: step 40370, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 53h:43m:58s remains)
INFO - root - 2017-12-07 18:29:11.595683: step 40380, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 58h:12m:05s remains)
INFO - root - 2017-12-07 18:29:18.389026: step 40390, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 55h:04m:11s remains)
INFO - root - 2017-12-07 18:29:25.193943: step 40400, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 56h:32m:33s remains)
2017-12-07 18:29:25.933962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2819524 -4.2876582 -4.3023596 -4.3121734 -4.3193069 -4.3257818 -4.3239741 -4.3201065 -4.3175907 -4.31749 -4.3206043 -4.325459 -4.3290644 -4.3302712 -4.3274717][-4.2491841 -4.2437172 -4.256969 -4.2729659 -4.2889791 -4.3031154 -4.3027267 -4.2984695 -4.2949839 -4.2936606 -4.2976203 -4.3044763 -4.3085508 -4.3096285 -4.3057637][-4.2249141 -4.2009873 -4.2053962 -4.2269039 -4.253067 -4.2747211 -4.2786517 -4.278748 -4.2814813 -4.284018 -4.290709 -4.2976742 -4.3007493 -4.3004255 -4.294065][-4.2157083 -4.1800542 -4.1735439 -4.189816 -4.2117066 -4.2280979 -4.2333465 -4.24113 -4.2584038 -4.2748561 -4.2885041 -4.29532 -4.2979126 -4.2982836 -4.2950234][-4.2125177 -4.1822 -4.1716447 -4.1734242 -4.1704569 -4.1604419 -4.1497278 -4.1572561 -4.1954713 -4.2404957 -4.2720804 -4.2849593 -4.2897925 -4.2920628 -4.2944684][-4.2116818 -4.1896138 -4.1800818 -4.1652303 -4.1258621 -4.068891 -4.0092587 -3.9916599 -4.0587459 -4.1443262 -4.2043447 -4.2337332 -4.2486773 -4.25668 -4.2675529][-4.2059317 -4.1941242 -4.1904612 -4.1627412 -4.0926523 -3.9904814 -3.8677154 -3.8018579 -3.8952136 -4.0210953 -4.1079378 -4.1539783 -4.175725 -4.186717 -4.2046084][-4.2108994 -4.2135496 -4.2166162 -4.1888247 -4.1169806 -4.0123205 -3.8816137 -3.8000622 -3.8775208 -3.9875042 -4.0619607 -4.0984955 -4.1082921 -4.1124687 -4.1299734][-4.2096424 -4.2240295 -4.2374082 -4.2219357 -4.1777835 -4.1136065 -4.0338907 -3.9799247 -4.0069008 -4.0545077 -4.0836005 -4.0881133 -4.0782733 -4.0701442 -4.0785923][-4.1842752 -4.1992259 -4.2286549 -4.2388635 -4.2267246 -4.1971893 -4.1593108 -4.1284723 -4.1221471 -4.1218667 -4.1142225 -4.0963526 -4.0765252 -4.0601678 -4.0591779][-4.1383719 -4.1431632 -4.1833673 -4.2173405 -4.2306781 -4.2206159 -4.2080355 -4.1946988 -4.1758657 -4.151763 -4.1319194 -4.1147342 -4.0951185 -4.0795584 -4.0738029][-4.111639 -4.0989671 -4.1353121 -4.1734281 -4.1977453 -4.2046623 -4.2105327 -4.2114406 -4.1903687 -4.1596632 -4.1433783 -4.1386724 -4.1257868 -4.1215839 -4.1227584][-4.136642 -4.111506 -4.1295934 -4.1483817 -4.1680226 -4.1858578 -4.2122555 -4.2266669 -4.2078271 -4.1728573 -4.1578059 -4.15985 -4.1528306 -4.1623292 -4.175951][-4.1977515 -4.1764531 -4.1838222 -4.1853871 -4.1930556 -4.2080569 -4.2356734 -4.2491946 -4.23272 -4.2005239 -4.1901379 -4.194304 -4.1918921 -4.2034831 -4.2225609][-4.2588587 -4.2465348 -4.2504773 -4.2458963 -4.2462583 -4.2527142 -4.2657442 -4.2702961 -4.26022 -4.2363434 -4.2327013 -4.2381048 -4.2376819 -4.2445493 -4.2594218]]...]
INFO - root - 2017-12-07 18:29:32.439247: step 40410, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 55h:34m:11s remains)
INFO - root - 2017-12-07 18:29:39.333741: step 40420, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 54h:25m:07s remains)
INFO - root - 2017-12-07 18:29:46.107935: step 40430, loss = 2.08, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 51h:35m:09s remains)
INFO - root - 2017-12-07 18:29:52.897517: step 40440, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 51h:56m:13s remains)
INFO - root - 2017-12-07 18:29:59.813248: step 40450, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 57h:52m:40s remains)
INFO - root - 2017-12-07 18:30:06.676611: step 40460, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.740 sec/batch; 59h:59m:51s remains)
INFO - root - 2017-12-07 18:30:13.529892: step 40470, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 54h:04m:31s remains)
INFO - root - 2017-12-07 18:30:20.394695: step 40480, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 50h:48m:17s remains)
INFO - root - 2017-12-07 18:30:27.186889: step 40490, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 57h:17m:33s remains)
INFO - root - 2017-12-07 18:30:34.044377: step 40500, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 58h:28m:37s remains)
2017-12-07 18:30:34.724143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2370038 -4.230679 -4.2148247 -4.190165 -4.179081 -4.1865368 -4.2079043 -4.2216969 -4.2244716 -4.2237554 -4.2224388 -4.21672 -4.2083626 -4.2051625 -4.2105837][-4.2514305 -4.24022 -4.2107739 -4.1783543 -4.1691051 -4.1811337 -4.2040553 -4.2181325 -4.2189984 -4.2140694 -4.21207 -4.2112408 -4.2069707 -4.206737 -4.2135906][-4.2544351 -4.2408042 -4.2070694 -4.1723995 -4.1630225 -4.1768851 -4.1982584 -4.2120252 -4.2122602 -4.2093468 -4.2119169 -4.2173634 -4.2165051 -4.2177978 -4.2257485][-4.2686582 -4.2513189 -4.2144551 -4.177671 -4.1651816 -4.175766 -4.1899672 -4.1982026 -4.2003918 -4.2069535 -4.2215385 -4.2369566 -4.2386465 -4.2384014 -4.2458463][-4.2743769 -4.2543979 -4.2157145 -4.1781039 -4.1614804 -4.1670671 -4.1724696 -4.1739874 -4.1799831 -4.1987386 -4.2263846 -4.2514029 -4.255662 -4.2531471 -4.2589321][-4.2760444 -4.2524529 -4.2112832 -4.1722651 -4.1500525 -4.1468687 -4.142231 -4.1395607 -4.1523924 -4.1842432 -4.2233067 -4.2550921 -4.2612162 -4.2584953 -4.2628245][-4.2717028 -4.241178 -4.1964631 -4.1584034 -4.1363006 -4.1255302 -4.1086178 -4.0998425 -4.1170278 -4.1592684 -4.2059622 -4.2413945 -4.2487674 -4.2473607 -4.2529626][-4.253283 -4.2120085 -4.1641169 -4.1329784 -4.1213026 -4.1133242 -4.09143 -4.0789404 -4.09597 -4.1389036 -4.1851926 -4.2189922 -4.2277613 -4.2282734 -4.2369952][-4.221107 -4.1668444 -4.1203351 -4.1016631 -4.1053448 -4.1079516 -4.0903583 -4.0777864 -4.0900488 -4.1271725 -4.1682296 -4.1990066 -4.2102709 -4.2141294 -4.2260566][-4.18645 -4.1214757 -4.0779858 -4.0700388 -4.0864496 -4.10288 -4.0938597 -4.0785675 -4.0794091 -4.1063671 -4.1447997 -4.1781173 -4.1944213 -4.202282 -4.2158756][-4.150979 -4.0826664 -4.0438671 -4.0414782 -4.0659518 -4.095428 -4.0956297 -4.0755219 -4.0619879 -4.0795712 -4.1193843 -4.1577206 -4.1803846 -4.1934991 -4.2068343][-4.1200166 -4.0582795 -4.0281553 -4.0301189 -4.0584097 -4.0939403 -4.0994129 -4.0745978 -4.0478039 -4.0592852 -4.1031456 -4.1471744 -4.1769671 -4.1938238 -4.2031565][-4.1001596 -4.0553012 -4.0390434 -4.0464582 -4.07669 -4.111259 -4.1181865 -4.0914845 -4.0573082 -4.0642776 -4.1083851 -4.1531558 -4.1839557 -4.2007008 -4.204567][-4.1015105 -4.0785832 -4.0747957 -4.0856667 -4.1157417 -4.1452527 -4.1518245 -4.1291113 -4.09774 -4.1015892 -4.1391644 -4.1760225 -4.1995 -4.2101731 -4.2084475][-4.1222548 -4.1235714 -4.1299686 -4.1408029 -4.1649547 -4.18707 -4.1924353 -4.1777463 -4.1553774 -4.1558471 -4.1809258 -4.2029128 -4.2131109 -4.2155147 -4.21107]]...]
INFO - root - 2017-12-07 18:30:41.428435: step 40510, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 52h:23m:34s remains)
INFO - root - 2017-12-07 18:30:48.301055: step 40520, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 57h:29m:31s remains)
INFO - root - 2017-12-07 18:30:55.202181: step 40530, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 58h:21m:21s remains)
INFO - root - 2017-12-07 18:31:01.991671: step 40540, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 54h:37m:19s remains)
INFO - root - 2017-12-07 18:31:08.614266: step 40550, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 53h:06m:02s remains)
INFO - root - 2017-12-07 18:31:15.473627: step 40560, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 57h:36m:18s remains)
INFO - root - 2017-12-07 18:31:22.357218: step 40570, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 57h:35m:15s remains)
INFO - root - 2017-12-07 18:31:29.245806: step 40580, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.688 sec/batch; 55h:46m:45s remains)
INFO - root - 2017-12-07 18:31:36.023826: step 40590, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 51h:00m:18s remains)
INFO - root - 2017-12-07 18:31:42.870946: step 40600, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 53h:00m:32s remains)
2017-12-07 18:31:43.673771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3181791 -4.3266129 -4.3282509 -4.3229718 -4.3153825 -4.3122678 -4.3103652 -4.3087325 -4.3099928 -4.306869 -4.2951431 -4.2787056 -4.2634807 -4.2461567 -4.2319336][-4.3189235 -4.3237734 -4.322257 -4.3127646 -4.3015418 -4.2985239 -4.2981377 -4.2977953 -4.3011727 -4.3008757 -4.2899933 -4.2707715 -4.249289 -4.2274528 -4.21655][-4.3176813 -4.3161674 -4.3087573 -4.2927804 -4.2751622 -4.2683496 -4.2647614 -4.2618732 -4.2664657 -4.2718806 -4.27002 -4.2618451 -4.2488565 -4.2345209 -4.2303514][-4.3106055 -4.303823 -4.2907004 -4.2676592 -4.2409954 -4.2221437 -4.204463 -4.1895065 -4.1899529 -4.2052011 -4.22556 -4.24514 -4.256928 -4.2586436 -4.2608652][-4.2995796 -4.2911463 -4.275815 -4.2475719 -4.2101951 -4.1709332 -4.125802 -4.0865536 -4.0740829 -4.1009536 -4.1519613 -4.2096791 -4.2534523 -4.2739053 -4.2803612][-4.2913094 -4.2844429 -4.2704515 -4.2404981 -4.1947551 -4.1320853 -4.0502234 -3.9763682 -3.9464564 -3.9864244 -4.0700245 -4.1647372 -4.239254 -4.2771487 -4.2862153][-4.2889342 -4.2822819 -4.2690768 -4.2384009 -4.1900492 -4.1122093 -4.0010939 -3.8968992 -3.852731 -3.9055533 -4.0140262 -4.1317978 -4.2248292 -4.2724242 -4.2802777][-4.2933168 -4.2861581 -4.27285 -4.24258 -4.198493 -4.1204271 -4.0000134 -3.883075 -3.8324938 -3.8920953 -4.0044389 -4.1216364 -4.2146668 -4.2628813 -4.2673464][-4.2954164 -4.2903409 -4.2806649 -4.2544975 -4.2197123 -4.1531982 -4.0460773 -3.9438317 -3.8988464 -3.9485826 -4.040431 -4.13576 -4.2137728 -4.2572484 -4.2604179][-4.2927871 -4.2957587 -4.2948136 -4.2789044 -4.254837 -4.2035179 -4.116765 -4.0339184 -3.9941123 -4.0274096 -4.0948687 -4.1635342 -4.2228909 -4.2609119 -4.2684035][-4.2799563 -4.2945671 -4.3030076 -4.2973356 -4.2852426 -4.2488084 -4.1784968 -4.1067667 -4.06633 -4.0845184 -4.1337838 -4.1846228 -4.23257 -4.2688828 -4.2834415][-4.259932 -4.2835617 -4.2968025 -4.2963252 -4.2929077 -4.2711272 -4.2172346 -4.1554422 -4.1139841 -4.1208606 -4.1583166 -4.1992416 -4.2409449 -4.2740793 -4.2906671][-4.2572718 -4.28257 -4.2911539 -4.2877522 -4.2896385 -4.2819881 -4.2477756 -4.2029643 -4.1692472 -4.1692739 -4.1943502 -4.2237377 -4.2528052 -4.2749825 -4.2855043][-4.2691336 -4.285224 -4.2823176 -4.2727804 -4.2771029 -4.2831693 -4.2709908 -4.2511272 -4.236073 -4.2355719 -4.2457733 -4.2571669 -4.2662525 -4.2711339 -4.2715149][-4.2825451 -4.2843113 -4.2684526 -4.25292 -4.2596936 -4.2772546 -4.28551 -4.2902212 -4.29454 -4.2966118 -4.2947016 -4.2900167 -4.2817116 -4.2724614 -4.2659683]]...]
INFO - root - 2017-12-07 18:31:50.121672: step 40610, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 54h:15m:47s remains)
INFO - root - 2017-12-07 18:31:57.004754: step 40620, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 53h:26m:16s remains)
INFO - root - 2017-12-07 18:32:03.804017: step 40630, loss = 2.12, batch loss = 2.06 (12.2 examples/sec; 0.656 sec/batch; 53h:08m:41s remains)
INFO - root - 2017-12-07 18:32:10.782285: step 40640, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 59h:01m:43s remains)
INFO - root - 2017-12-07 18:32:17.576030: step 40650, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 52h:03m:56s remains)
INFO - root - 2017-12-07 18:32:24.376749: step 40660, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 54h:23m:38s remains)
INFO - root - 2017-12-07 18:32:31.242849: step 40670, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 52h:48m:37s remains)
INFO - root - 2017-12-07 18:32:38.035818: step 40680, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 56h:14m:27s remains)
INFO - root - 2017-12-07 18:32:44.960973: step 40690, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 59h:53m:36s remains)
INFO - root - 2017-12-07 18:32:51.773261: step 40700, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 55h:23m:44s remains)
2017-12-07 18:32:52.536161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1817675 -4.1811147 -4.1976428 -4.2242322 -4.2375193 -4.2275782 -4.18771 -4.1406856 -4.1171842 -4.1335249 -4.17718 -4.221344 -4.258451 -4.2774048 -4.2860603][-4.1810856 -4.188859 -4.2055068 -4.2258391 -4.23342 -4.2190814 -4.1769323 -4.1315207 -4.11265 -4.1343684 -4.18585 -4.236659 -4.2732635 -4.2918391 -4.2966208][-4.1753554 -4.1861839 -4.2009783 -4.2150559 -4.2178473 -4.1999578 -4.1594625 -4.1183462 -4.1057215 -4.1331477 -4.1907778 -4.2428451 -4.2759671 -4.2932472 -4.2979445][-4.180654 -4.1904116 -4.1987262 -4.2036877 -4.1983323 -4.172657 -4.1289592 -4.0880156 -4.0818315 -4.1186 -4.1805634 -4.2326837 -4.2658143 -4.2843966 -4.2929835][-4.1982813 -4.2005296 -4.1969042 -4.1883178 -4.1719675 -4.1368718 -4.0866327 -4.042109 -4.0417686 -4.0917134 -4.1608744 -4.2186074 -4.2585983 -4.2806606 -4.2921205][-4.2260795 -4.218761 -4.2036376 -4.182024 -4.1541038 -4.1078224 -4.0487089 -3.9977703 -4.0036211 -4.0719318 -4.1524267 -4.2168121 -4.26222 -4.2868218 -4.29726][-4.2414579 -4.2286148 -4.2062559 -4.17483 -4.1379004 -4.0828156 -4.0136857 -3.9559734 -3.9702485 -4.0593834 -4.1517591 -4.2212234 -4.2683496 -4.2943664 -4.302784][-4.2401175 -4.2240186 -4.2021155 -4.1703753 -4.1306896 -4.0709877 -3.9924946 -3.9279122 -3.9494302 -4.0540671 -4.1552815 -4.2286625 -4.2769876 -4.3027949 -4.308455][-4.2251515 -4.2063437 -4.186254 -4.1629071 -4.134706 -4.0852957 -4.0144553 -3.9553134 -3.9744058 -4.0720778 -4.1687145 -4.2403398 -4.2867117 -4.3088231 -4.311563][-4.2055817 -4.1864977 -4.1696296 -4.1552505 -4.1413088 -4.1088986 -4.0545669 -4.00786 -4.0219221 -4.0995064 -4.1818986 -4.24678 -4.2895279 -4.3084359 -4.3119736][-4.1864238 -4.1690283 -4.1552753 -4.1468577 -4.1404657 -4.1206641 -4.0823479 -4.0498991 -4.0617266 -4.1211553 -4.1885338 -4.2459269 -4.2858038 -4.3047442 -4.3118768][-4.1747847 -4.1590791 -4.1461082 -4.1382017 -4.1349273 -4.1255331 -4.1020751 -4.07991 -4.0893106 -4.1362872 -4.1912713 -4.2419472 -4.2795849 -4.2997723 -4.3112149][-4.1606913 -4.1484151 -4.1400561 -4.134172 -4.1335912 -4.1304421 -4.1139941 -4.0954762 -4.1028714 -4.1442847 -4.1935019 -4.2407522 -4.2773337 -4.2983022 -4.3114743][-4.1583743 -4.1497226 -4.1463156 -4.1431165 -4.1425438 -4.1382351 -4.1218796 -4.1032515 -4.1095252 -4.1475549 -4.1956911 -4.2416453 -4.2784996 -4.3016448 -4.3150482][-4.1743989 -4.1683044 -4.1691642 -4.1687508 -4.1658583 -4.1575146 -4.138907 -4.1195688 -4.1238723 -4.15897 -4.2068577 -4.251946 -4.2875419 -4.3099818 -4.3214378]]...]
INFO - root - 2017-12-07 18:32:59.109692: step 40710, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.697 sec/batch; 56h:28m:11s remains)
INFO - root - 2017-12-07 18:33:05.916127: step 40720, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 57h:15m:28s remains)
INFO - root - 2017-12-07 18:33:12.740040: step 40730, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 53h:55m:29s remains)
INFO - root - 2017-12-07 18:33:19.550955: step 40740, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 52h:19m:35s remains)
INFO - root - 2017-12-07 18:33:26.341910: step 40750, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 53h:08m:06s remains)
INFO - root - 2017-12-07 18:33:33.064621: step 40760, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.655 sec/batch; 53h:04m:38s remains)
INFO - root - 2017-12-07 18:33:39.873228: step 40770, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.693 sec/batch; 56h:07m:41s remains)
INFO - root - 2017-12-07 18:33:46.805024: step 40780, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 55h:28m:29s remains)
INFO - root - 2017-12-07 18:33:53.801191: step 40790, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 55h:56m:31s remains)
INFO - root - 2017-12-07 18:34:00.725310: step 40800, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 52h:24m:24s remains)
2017-12-07 18:34:01.501280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1581855 -4.1725526 -4.1779051 -4.174087 -4.1687703 -4.1586704 -4.1408119 -4.1362252 -4.14974 -4.1692042 -4.1667833 -4.1501017 -4.1277962 -4.0901561 -4.0709386][-4.149035 -4.1685576 -4.1756177 -4.1703005 -4.1644869 -4.1547155 -4.136652 -4.1267223 -4.1374869 -4.1562014 -4.1544008 -4.1420841 -4.1352162 -4.1161628 -4.1012287][-4.1394215 -4.1617832 -4.166573 -4.1642303 -4.1627369 -4.1543479 -4.1373434 -4.1251364 -4.135756 -4.1499038 -4.1474018 -4.1420946 -4.1472244 -4.1454163 -4.1368318][-4.1302662 -4.1514764 -4.1579022 -4.1636515 -4.1697135 -4.1617203 -4.1431026 -4.1289868 -4.137125 -4.1446948 -4.1439705 -4.14104 -4.1515751 -4.1572614 -4.1441727][-4.1061769 -4.1272211 -4.1384935 -4.1479149 -4.1517315 -4.1449213 -4.1289234 -4.1168909 -4.1212392 -4.1260505 -4.1291194 -4.1325426 -4.1498613 -4.1602907 -4.1385159][-4.0677729 -4.0835195 -4.0909867 -4.097445 -4.0934525 -4.0905757 -4.0854712 -4.0830469 -4.0892239 -4.0876956 -4.0897017 -4.098259 -4.1238441 -4.1426063 -4.1247692][-4.0445147 -4.0458145 -4.0429296 -4.0451636 -4.0412693 -4.043498 -4.0461969 -4.0514274 -4.0584092 -4.0553284 -4.0561109 -4.067728 -4.102654 -4.12991 -4.1151824][-4.0356741 -4.0231843 -4.0148106 -4.0143723 -4.0103025 -4.0113435 -4.0172453 -4.0287533 -4.0439086 -4.0545254 -4.0633841 -4.0780864 -4.1066952 -4.1287422 -4.1200638][-4.0571532 -4.0406375 -4.0329647 -4.0300074 -4.0237193 -4.0200405 -4.0232997 -4.0365992 -4.058198 -4.0788317 -4.0947604 -4.1073122 -4.117785 -4.1253338 -4.1241345][-4.1020794 -4.0840025 -4.0824842 -4.08369 -4.0797091 -4.0741982 -4.0752959 -4.087883 -4.1079588 -4.1323967 -4.1516724 -4.1588407 -4.1519909 -4.1500711 -4.1574349][-4.1336274 -4.1177249 -4.1218691 -4.1247258 -4.1198568 -4.1135478 -4.1161075 -4.128737 -4.1455736 -4.1722245 -4.195271 -4.2006283 -4.1915407 -4.1919894 -4.2110119][-4.1455631 -4.1333327 -4.1378946 -4.1374235 -4.1270037 -4.1146088 -4.1116986 -4.1201863 -4.1371984 -4.1633177 -4.1865239 -4.1933374 -4.18996 -4.1990905 -4.2251067][-4.1503496 -4.1375618 -4.13546 -4.1303482 -4.1189895 -4.1089964 -4.1059556 -4.1116896 -4.1295075 -4.15009 -4.1618767 -4.1602564 -4.1596775 -4.1794658 -4.2102985][-4.1692557 -4.1559005 -4.1460209 -4.1395178 -4.1351371 -4.1307726 -4.1274061 -4.129509 -4.139751 -4.1478372 -4.1483331 -4.1403842 -4.1377811 -4.1594305 -4.1904368][-4.1842189 -4.1628785 -4.1454253 -4.1408362 -4.1470094 -4.1525455 -4.1528587 -4.1506472 -4.151649 -4.1455946 -4.1408777 -4.1388612 -4.1411309 -4.1569047 -4.1789308]]...]
INFO - root - 2017-12-07 18:34:08.123209: step 40810, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 53h:24m:52s remains)
INFO - root - 2017-12-07 18:34:14.875537: step 40820, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 52h:28m:42s remains)
INFO - root - 2017-12-07 18:34:21.720524: step 40830, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 54h:01m:00s remains)
INFO - root - 2017-12-07 18:34:28.603604: step 40840, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 55h:36m:18s remains)
INFO - root - 2017-12-07 18:34:35.402303: step 40850, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 57h:11m:44s remains)
INFO - root - 2017-12-07 18:34:41.959087: step 40860, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 51h:54m:13s remains)
INFO - root - 2017-12-07 18:34:48.811208: step 40870, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 54h:24m:35s remains)
INFO - root - 2017-12-07 18:34:55.642892: step 40880, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 58h:13m:23s remains)
INFO - root - 2017-12-07 18:35:02.462774: step 40890, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 56h:40m:12s remains)
INFO - root - 2017-12-07 18:35:09.364868: step 40900, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 51h:06m:00s remains)
2017-12-07 18:35:10.158794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1596117 -4.1915631 -4.2271895 -4.2604961 -4.2770658 -4.2749767 -4.2637939 -4.2483258 -4.2297354 -4.2106986 -4.1912775 -4.1720691 -4.1513219 -4.1448722 -4.1520805][-4.1709533 -4.2036381 -4.2396469 -4.2700062 -4.2838182 -4.2802896 -4.2685962 -4.25453 -4.2352996 -4.2124963 -4.1940403 -4.1840005 -4.1804614 -4.1781077 -4.18226][-4.2075925 -4.2281108 -4.25437 -4.271162 -4.2765188 -4.2705297 -4.2598729 -4.2470531 -4.2278404 -4.2058911 -4.1921229 -4.1915245 -4.1989694 -4.2007217 -4.2046237][-4.2353392 -4.2390466 -4.24884 -4.2477274 -4.2447338 -4.2403517 -4.2353244 -4.2255473 -4.2070618 -4.1911445 -4.1848822 -4.1872559 -4.1924515 -4.1967187 -4.2043438][-4.2403016 -4.2298951 -4.2230616 -4.20759 -4.2004886 -4.1969194 -4.1934619 -4.1815944 -4.1655946 -4.1609144 -4.1659532 -4.1698046 -4.16925 -4.1698008 -4.1775417][-4.2088346 -4.1899376 -4.1732817 -4.1523519 -4.1424279 -4.1324806 -4.1176476 -4.0914822 -4.0762534 -4.0863724 -4.1036496 -4.1097717 -4.1037331 -4.0988913 -4.1079121][-4.1367764 -4.1081839 -4.0882273 -4.0776763 -4.0730114 -4.0517344 -4.0126157 -3.9607387 -3.9445298 -3.9808187 -4.0242972 -4.0387173 -4.0303359 -4.0182219 -4.0313182][-4.0444112 -4.0047364 -3.9868479 -3.9993865 -4.0078282 -3.9730632 -3.9040775 -3.819145 -3.7949009 -3.8635778 -3.9415581 -3.972578 -3.9658327 -3.9471166 -3.9623408][-3.9579082 -3.9203119 -3.9186339 -3.9595251 -3.985693 -3.9589241 -3.8951554 -3.8104396 -3.7815146 -3.846673 -3.9271772 -3.9707077 -3.9767225 -3.9630125 -3.9760404][-3.9702489 -3.9463553 -3.9593678 -4.0072918 -4.0408854 -4.0349426 -4.0022411 -3.9537454 -3.931438 -3.9594209 -4.0039868 -4.0383081 -4.0527587 -4.0509143 -4.0627584][-4.0609393 -4.0445347 -4.0546751 -4.0871735 -4.1096115 -4.1143508 -4.1057043 -4.0871639 -4.0752907 -4.0834618 -4.1003685 -4.1202235 -4.1336327 -4.1372261 -4.1470566][-4.1435075 -4.1294432 -4.135118 -4.1548905 -4.1668892 -4.1737046 -4.1758904 -4.17278 -4.1687551 -4.170177 -4.1738729 -4.1859021 -4.1966395 -4.201076 -4.208436][-4.1953006 -4.1812034 -4.1821141 -4.1949148 -4.202405 -4.2080317 -4.2136927 -4.2174225 -4.2213373 -4.2219677 -4.2184062 -4.2197146 -4.2213078 -4.2218246 -4.2270956][-4.2002907 -4.1861482 -4.185205 -4.1976848 -4.2064314 -4.214952 -4.2258024 -4.2349758 -4.2428379 -4.2427459 -4.2373552 -4.2352705 -4.2302856 -4.225389 -4.225287][-4.1911392 -4.1803923 -4.1828032 -4.1956182 -4.207181 -4.2148128 -4.21928 -4.2263131 -4.2342358 -4.2322536 -4.2251797 -4.2221813 -4.2176614 -4.2132196 -4.2127476]]...]
INFO - root - 2017-12-07 18:35:16.755452: step 40910, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 60h:02m:12s remains)
INFO - root - 2017-12-07 18:35:23.523186: step 40920, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 55h:59m:38s remains)
INFO - root - 2017-12-07 18:35:30.303264: step 40930, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.658 sec/batch; 53h:19m:58s remains)
INFO - root - 2017-12-07 18:35:37.015240: step 40940, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 52h:09m:29s remains)
INFO - root - 2017-12-07 18:35:43.869844: step 40950, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 55h:13m:36s remains)
INFO - root - 2017-12-07 18:35:50.725034: step 40960, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.711 sec/batch; 57h:36m:21s remains)
INFO - root - 2017-12-07 18:35:57.508812: step 40970, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 54h:21m:49s remains)
INFO - root - 2017-12-07 18:36:04.218157: step 40980, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 50h:34m:58s remains)
INFO - root - 2017-12-07 18:36:11.071092: step 40990, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 53h:07m:21s remains)
INFO - root - 2017-12-07 18:36:17.959181: step 41000, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 56h:42m:37s remains)
2017-12-07 18:36:18.678227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1825452 -4.1716795 -4.1796327 -4.20138 -4.2149367 -4.2051067 -4.1701846 -4.1052842 -4.0417666 -4.0577822 -4.1465449 -4.2292376 -4.270123 -4.2899647 -4.3118153][-4.2001033 -4.1859341 -4.1816125 -4.187973 -4.1875443 -4.1696992 -4.1398163 -4.083734 -4.0295482 -4.0503469 -4.143918 -4.229784 -4.2731428 -4.292675 -4.31232][-4.2337437 -4.223527 -4.2143445 -4.2022152 -4.1770225 -4.1397734 -4.1011543 -4.0445046 -3.997215 -4.0229077 -4.1240234 -4.2211833 -4.273869 -4.2958794 -4.3140368][-4.2805667 -4.2773757 -4.2661242 -4.2347221 -4.1842809 -4.1243124 -4.0643544 -3.9967597 -3.9594603 -4.004283 -4.1155548 -4.2172456 -4.274507 -4.298378 -4.3149438][-4.3149872 -4.3183408 -4.3034792 -4.2557974 -4.1893592 -4.113523 -4.0350876 -3.9599538 -3.9328909 -3.9948463 -4.1101017 -4.2105703 -4.2721334 -4.29938 -4.3136182][-4.3280468 -4.3362432 -4.3190703 -4.2626104 -4.1890788 -4.105258 -4.0138321 -3.9295683 -3.9036207 -3.9726126 -4.0935392 -4.1973944 -4.2686286 -4.3011045 -4.3122592][-4.3321238 -4.3408794 -4.3201461 -4.2610059 -4.1858335 -4.0988054 -4.005568 -3.9223716 -3.8950183 -3.9629111 -4.0868444 -4.1919789 -4.2671514 -4.3027205 -4.3114719][-4.3319116 -4.3372216 -4.3164167 -4.2612524 -4.1869717 -4.1037006 -4.0178847 -3.9474359 -3.9263186 -3.9889407 -4.1002274 -4.191978 -4.2616777 -4.2980771 -4.3095856][-4.3278232 -4.3308306 -4.3135195 -4.2693505 -4.2015748 -4.1214695 -4.0367055 -3.9782538 -3.9634922 -4.0201931 -4.1143718 -4.1936398 -4.2548428 -4.2890439 -4.3049994][-4.3225474 -4.3277764 -4.3162127 -4.2834105 -4.2263012 -4.1535244 -4.0769815 -4.0242105 -4.0061469 -4.051722 -4.13338 -4.2076058 -4.25933 -4.2860818 -4.3038464][-4.3176327 -4.3279228 -4.3213162 -4.2963324 -4.245265 -4.1793227 -4.1102304 -4.0578761 -4.0320897 -4.0702076 -4.1506157 -4.2274556 -4.2717285 -4.2903585 -4.3072081][-4.3029733 -4.3215141 -4.3222041 -4.3032718 -4.2581215 -4.1954451 -4.1304393 -4.0774026 -4.0456123 -4.0802298 -4.1638041 -4.2413573 -4.2803383 -4.2938972 -4.3093605][-4.2825279 -4.3107357 -4.3233385 -4.3153706 -4.2802114 -4.2209563 -4.15503 -4.0953059 -4.0540733 -4.0861278 -4.1700211 -4.2459965 -4.2828293 -4.2943344 -4.3106332][-4.2675 -4.3017807 -4.324069 -4.3282118 -4.3078513 -4.2574267 -4.1953516 -4.1317844 -4.0810757 -4.10483 -4.1808844 -4.2499375 -4.28455 -4.2975383 -4.31426][-4.2524066 -4.2873874 -4.3164887 -4.3291678 -4.3202648 -4.28176 -4.230166 -4.1721096 -4.1221461 -4.1374955 -4.1990285 -4.2574396 -4.2895312 -4.3051157 -4.3212762]]...]
INFO - root - 2017-12-07 18:36:25.335960: step 41010, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 52h:05m:45s remains)
INFO - root - 2017-12-07 18:36:32.143141: step 41020, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 54h:42m:08s remains)
INFO - root - 2017-12-07 18:36:39.100247: step 41030, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 58h:48m:52s remains)
INFO - root - 2017-12-07 18:36:45.863220: step 41040, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 55h:48m:43s remains)
INFO - root - 2017-12-07 18:36:52.652228: step 41050, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 54h:24m:04s remains)
INFO - root - 2017-12-07 18:36:59.478568: step 41060, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 52h:34m:52s remains)
INFO - root - 2017-12-07 18:37:06.197472: step 41070, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 56h:01m:56s remains)
INFO - root - 2017-12-07 18:37:12.920424: step 41080, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 55h:30m:50s remains)
INFO - root - 2017-12-07 18:37:19.703129: step 41090, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 56h:18m:22s remains)
INFO - root - 2017-12-07 18:37:26.517853: step 41100, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 52h:44m:13s remains)
2017-12-07 18:37:27.171753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2978954 -4.29807 -4.2997365 -4.2966008 -4.2851219 -4.2703948 -4.2656064 -4.26947 -4.2706857 -4.2712684 -4.2730794 -4.270196 -4.2568622 -4.2258644 -4.1870213][-4.2936449 -4.2965231 -4.3005347 -4.2925134 -4.2755604 -4.2530937 -4.2365365 -4.2344108 -4.23852 -4.2453265 -4.2524552 -4.2577515 -4.2589169 -4.2464771 -4.2216187][-4.2854795 -4.2913918 -4.2975945 -4.2826762 -4.2534127 -4.2168427 -4.1836128 -4.1701689 -4.1752405 -4.1965785 -4.2212658 -4.2419815 -4.2605009 -4.2676129 -4.2592754][-4.2900109 -4.2943573 -4.2950459 -4.2665968 -4.2151961 -4.1532359 -4.096518 -4.0692415 -4.076025 -4.1163716 -4.1678181 -4.212811 -4.2516818 -4.2772908 -4.2849574][-4.3008819 -4.29893 -4.2813282 -4.2317576 -4.16151 -4.086935 -4.0126691 -3.9634607 -3.9631541 -4.012229 -4.0773826 -4.1355004 -4.1893792 -4.237824 -4.2712412][-4.3052278 -4.2914267 -4.2487445 -4.1787229 -4.10725 -4.0403671 -3.9610541 -3.8954635 -3.876462 -3.9133451 -3.9740286 -4.035624 -4.10052 -4.1675014 -4.22258][-4.3004742 -4.2696366 -4.2071233 -4.1299257 -4.0712423 -4.0296655 -3.9635079 -3.8920472 -3.8514788 -3.8541348 -3.8910763 -3.9428556 -4.0109177 -4.0848866 -4.1494522][-4.2967381 -4.2596917 -4.1999407 -4.1387529 -4.102397 -4.0849018 -4.0402946 -3.9764137 -3.9258542 -3.9029417 -3.9102552 -3.9381018 -3.9884458 -4.04681 -4.099874][-4.3006563 -4.2684894 -4.2277651 -4.1932874 -4.1770468 -4.170536 -4.1416554 -4.0928454 -4.0485435 -4.016788 -4.003696 -4.0097289 -4.0382876 -4.0723319 -4.1044884][-4.3075128 -4.286242 -4.2655625 -4.252502 -4.2484646 -4.2440162 -4.2250938 -4.1892104 -4.1565328 -4.1320505 -4.1131535 -4.102385 -4.1104088 -4.1275053 -4.1441121][-4.3105073 -4.3003058 -4.295301 -4.2946444 -4.2947721 -4.2883911 -4.2759523 -4.2554059 -4.231524 -4.2122874 -4.1937208 -4.1790652 -4.1771364 -4.1844931 -4.194181][-4.3132224 -4.3092484 -4.3115067 -4.3154588 -4.314558 -4.3093944 -4.3038964 -4.2952757 -4.2786603 -4.2601924 -4.2396622 -4.2205567 -4.2117758 -4.2144537 -4.2209444][-4.3190265 -4.3175254 -4.3202944 -4.3237252 -4.3226414 -4.3196874 -4.31921 -4.3175879 -4.3086104 -4.2936072 -4.2762127 -4.2588525 -4.2502189 -4.2488575 -4.2517085][-4.3246942 -4.3239021 -4.324069 -4.3239918 -4.3213964 -4.3189492 -4.31952 -4.31967 -4.315732 -4.3095293 -4.3010197 -4.2907205 -4.2842298 -4.2825847 -4.2843385][-4.3238611 -4.322546 -4.3216386 -4.3197727 -4.3165741 -4.3139515 -4.3120594 -4.310595 -4.3100924 -4.3101463 -4.3097882 -4.3078289 -4.3055248 -4.3045321 -4.3045793]]...]
INFO - root - 2017-12-07 18:37:33.668106: step 41110, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.698 sec/batch; 56h:32m:08s remains)
INFO - root - 2017-12-07 18:37:40.506014: step 41120, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 55h:15m:53s remains)
INFO - root - 2017-12-07 18:37:47.323561: step 41130, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 51h:29m:04s remains)
INFO - root - 2017-12-07 18:37:54.158116: step 41140, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 53h:16m:58s remains)
INFO - root - 2017-12-07 18:38:01.025087: step 41150, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 55h:52m:31s remains)
INFO - root - 2017-12-07 18:38:07.868899: step 41160, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.709 sec/batch; 57h:20m:53s remains)
INFO - root - 2017-12-07 18:38:14.416840: step 41170, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 54h:16m:42s remains)
INFO - root - 2017-12-07 18:38:21.124711: step 41180, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.627 sec/batch; 50h:44m:48s remains)
INFO - root - 2017-12-07 18:38:28.011348: step 41190, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 54h:54m:06s remains)
INFO - root - 2017-12-07 18:38:34.802956: step 41200, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 55h:03m:32s remains)
2017-12-07 18:38:35.540743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3490429 -4.3465366 -4.3409691 -4.3361874 -4.3342972 -4.3346772 -4.3332262 -4.3313508 -4.3278494 -4.3237839 -4.3202419 -4.3188567 -4.3209176 -4.324286 -4.3312778][-4.3403826 -4.3332243 -4.3227592 -4.3134861 -4.3086586 -4.3084307 -4.3072619 -4.3077688 -4.3036633 -4.2969656 -4.2903795 -4.2869716 -4.2877665 -4.292325 -4.3059521][-4.325686 -4.3131928 -4.2979231 -4.2837453 -4.2742972 -4.2707958 -4.2679577 -4.2702837 -4.2654424 -4.2567511 -4.2482505 -4.2464247 -4.2483673 -4.2560391 -4.2771149][-4.3072319 -4.2879639 -4.2665443 -4.2460327 -4.225935 -4.2129011 -4.2057605 -4.2097597 -4.205678 -4.1985564 -4.1929946 -4.1973295 -4.2039423 -4.2148576 -4.2430248][-4.2880721 -4.2610903 -4.2303405 -4.2000804 -4.1669755 -4.1398811 -4.1254721 -4.1270771 -4.1210542 -4.1177483 -4.1199331 -4.132122 -4.1442037 -4.1607871 -4.1985927][-4.2777457 -4.2429237 -4.20133 -4.1594448 -4.1085176 -4.0603967 -4.0346255 -4.0253925 -4.0102954 -4.0157189 -4.0295706 -4.0516214 -4.0724549 -4.1012239 -4.1527796][-4.2810183 -4.2395258 -4.1869812 -4.1326537 -4.0619569 -3.9906528 -3.9480765 -3.9159124 -3.8819008 -3.8979053 -3.9272344 -3.9600272 -3.9959815 -4.0426664 -4.112905][-4.28529 -4.2427235 -4.1871843 -4.1288447 -4.0489788 -3.9638467 -3.9018083 -3.8324988 -3.7625957 -3.7842202 -3.8255589 -3.8616018 -3.9107308 -3.9765997 -4.0673933][-4.2868619 -4.2501321 -4.2035308 -4.1544495 -4.0847311 -4.0046234 -3.9333491 -3.8431511 -3.7454264 -3.7492971 -3.7761688 -3.7997479 -3.8482218 -3.922962 -4.0260124][-4.2881632 -4.26173 -4.2313933 -4.2017875 -4.1558523 -4.0978804 -4.0363359 -3.9560516 -3.8674283 -3.8406141 -3.8292897 -3.8286142 -3.8598521 -3.9233682 -4.0209394][-4.3002114 -4.284894 -4.2700529 -4.2578087 -4.2362528 -4.2058845 -4.1684208 -4.1154194 -4.052979 -4.0132241 -3.9734561 -3.9505124 -3.9565039 -3.992388 -4.0654764][-4.327415 -4.3215647 -4.3161983 -4.3116183 -4.3038063 -4.2936196 -4.2790303 -4.2539134 -4.2194386 -4.1880226 -4.1467624 -4.1180487 -4.1093059 -4.1221538 -4.164701][-4.34962 -4.349576 -4.347867 -4.3454661 -4.3425641 -4.3428431 -4.3416977 -4.3332052 -4.3176293 -4.299078 -4.2706904 -4.2478333 -4.2375546 -4.2406516 -4.2603788][-4.3583436 -4.3590717 -4.35665 -4.3533 -4.351192 -4.352838 -4.355875 -4.3550892 -4.3509345 -4.3422337 -4.3276711 -4.315021 -4.307034 -4.3070793 -4.3153634][-4.3604989 -4.3607373 -4.3579221 -4.3545246 -4.3519106 -4.3519764 -4.3532238 -4.3532786 -4.3527904 -4.349761 -4.344501 -4.3394823 -4.3349433 -4.3329177 -4.3362069]]...]
INFO - root - 2017-12-07 18:38:42.118808: step 41210, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 52h:36m:59s remains)
INFO - root - 2017-12-07 18:38:48.993988: step 41220, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 58h:34m:29s remains)
INFO - root - 2017-12-07 18:38:55.857008: step 41230, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 56h:08m:08s remains)
INFO - root - 2017-12-07 18:39:02.579221: step 41240, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 54h:09m:32s remains)
INFO - root - 2017-12-07 18:39:09.445551: step 41250, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 51h:16m:01s remains)
INFO - root - 2017-12-07 18:39:16.297752: step 41260, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 55h:00m:47s remains)
INFO - root - 2017-12-07 18:39:23.132680: step 41270, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 56h:43m:46s remains)
INFO - root - 2017-12-07 18:39:29.955189: step 41280, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 57h:59m:42s remains)
INFO - root - 2017-12-07 18:39:36.801504: step 41290, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 55h:54m:06s remains)
INFO - root - 2017-12-07 18:39:43.632881: step 41300, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 51h:38m:20s remains)
2017-12-07 18:39:44.358359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2218628 -4.2273488 -4.2357049 -4.2375846 -4.2422228 -4.25477 -4.2562175 -4.2469068 -4.2250261 -4.1977825 -4.1767817 -4.1643543 -4.142694 -4.1025357 -4.0795217][-4.231061 -4.2468715 -4.2546549 -4.2501378 -4.2476096 -4.2586327 -4.2602973 -4.2528949 -4.2405467 -4.2220149 -4.1999321 -4.1792731 -4.1511278 -4.1126661 -4.0987282][-4.2285681 -4.2486978 -4.2498059 -4.2340603 -4.2239776 -4.2274609 -4.2289295 -4.2284017 -4.2348495 -4.2362041 -4.2196059 -4.1954694 -4.1695457 -4.1467829 -4.152143][-4.2179589 -4.2463717 -4.2465053 -4.2263594 -4.2087827 -4.2020445 -4.1954126 -4.1935778 -4.2138839 -4.2310162 -4.2240644 -4.2015457 -4.1802325 -4.1690955 -4.1873894][-4.1915779 -4.2292643 -4.2388649 -4.2267532 -4.2058444 -4.1884856 -4.1685667 -4.1610327 -4.1858974 -4.2117476 -4.2112451 -4.1896839 -4.1717319 -4.1624947 -4.1804638][-4.1695819 -4.2071371 -4.2200413 -4.2148819 -4.1942778 -4.1681428 -4.1343665 -4.1150832 -4.1407046 -4.1779814 -4.1860113 -4.1678944 -4.1457233 -4.1265435 -4.1327195][-4.1587 -4.1889462 -4.191185 -4.1798182 -4.158278 -4.1280937 -4.0875754 -4.0635853 -4.0918522 -4.1422729 -4.1630096 -4.1468334 -4.1142149 -4.0747142 -4.0660033][-4.1549025 -4.1577878 -4.1305065 -4.1022635 -4.0789466 -4.059804 -4.0335484 -4.0160341 -4.0474119 -4.1063027 -4.133348 -4.1162505 -4.0738845 -4.0178385 -4.0009689][-4.1636515 -4.1348381 -4.0745726 -4.0246348 -4.000906 -3.9997041 -3.9986458 -3.9944904 -4.0238705 -4.0796514 -4.1021366 -4.0823226 -4.03843 -3.9839921 -3.9702][-4.177146 -4.1299939 -4.0554662 -3.9978635 -3.9813609 -3.9986243 -4.0201235 -4.0319743 -4.0576806 -4.0946422 -4.1018457 -4.0750647 -4.038177 -3.9973598 -3.9961646][-4.2012167 -4.1522284 -4.0852661 -4.0377173 -4.0349636 -4.0626945 -4.092526 -4.1117358 -4.129221 -4.1403823 -4.13214 -4.1050997 -4.0786295 -4.0551667 -4.062716][-4.2294006 -4.1965556 -4.1506634 -4.1199884 -4.1238174 -4.1497607 -4.1768126 -4.1930103 -4.1998549 -4.192596 -4.1782708 -4.1582284 -4.1431637 -4.1328163 -4.1437206][-4.2373624 -4.2275996 -4.20467 -4.1888866 -4.1962934 -4.2188869 -4.2399521 -4.2517023 -4.2490973 -4.2331967 -4.2167535 -4.2041788 -4.1977663 -4.1937652 -4.2012777][-4.2272911 -4.2387371 -4.2377148 -4.2307787 -4.2349405 -4.2497149 -4.26359 -4.2709613 -4.2668695 -4.2519832 -4.2387772 -4.23056 -4.2256479 -4.2230945 -4.225852][-4.1996241 -4.2273178 -4.2439942 -4.2437143 -4.2440071 -4.2521749 -4.2599788 -4.2630887 -4.2594719 -4.2496376 -4.2402735 -4.2352762 -4.2299585 -4.2251172 -4.2235689]]...]
INFO - root - 2017-12-07 18:39:51.000613: step 41310, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.724 sec/batch; 58h:35m:23s remains)
INFO - root - 2017-12-07 18:39:57.743708: step 41320, loss = 2.11, batch loss = 2.05 (12.3 examples/sec; 0.651 sec/batch; 52h:37m:44s remains)
INFO - root - 2017-12-07 18:40:04.490035: step 41330, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 51h:48m:18s remains)
INFO - root - 2017-12-07 18:40:11.391973: step 41340, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 56h:42m:12s remains)
INFO - root - 2017-12-07 18:40:18.263042: step 41350, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 58h:07m:31s remains)
INFO - root - 2017-12-07 18:40:25.080302: step 41360, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 55h:34m:21s remains)
INFO - root - 2017-12-07 18:40:31.793408: step 41370, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 51h:12m:07s remains)
INFO - root - 2017-12-07 18:40:38.722609: step 41380, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 55h:21m:56s remains)
INFO - root - 2017-12-07 18:40:45.605713: step 41390, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.727 sec/batch; 58h:45m:40s remains)
INFO - root - 2017-12-07 18:40:52.517959: step 41400, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 57h:48m:45s remains)
2017-12-07 18:40:53.166363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3007884 -4.2940788 -4.2767172 -4.2456374 -4.2181764 -4.2102523 -4.2198629 -4.2365365 -4.2565041 -4.27123 -4.268795 -4.2522588 -4.2299023 -4.2152987 -4.2163796][-4.2975903 -4.2864866 -4.2581816 -4.2133422 -4.1766992 -4.1667919 -4.1817923 -4.2084589 -4.2389746 -4.2621765 -4.2653775 -4.250351 -4.2259827 -4.2117615 -4.2198982][-4.2830472 -4.2675881 -4.2338204 -4.1814137 -4.1374135 -4.1247249 -4.1428647 -4.1783261 -4.2179623 -4.2492471 -4.2595863 -4.2494578 -4.2292113 -4.218688 -4.2322049][-4.2568774 -4.2366805 -4.200449 -4.1451254 -4.0954442 -4.0786572 -4.1002307 -4.1461358 -4.1955538 -4.2333188 -4.2504468 -4.2472878 -4.2356367 -4.2326417 -4.2485194][-4.2323275 -4.2092862 -4.1704164 -4.1125717 -4.0547552 -4.0286613 -4.054709 -4.113584 -4.1728277 -4.2147174 -4.2357492 -4.2375412 -4.2344222 -4.2384496 -4.2535419][-4.2252183 -4.2031779 -4.1605091 -4.0962706 -4.0215392 -3.9759767 -4.0006328 -4.0730195 -4.1408758 -4.1831493 -4.2077932 -4.212615 -4.2147479 -4.2235818 -4.2382216][-4.2215886 -4.20155 -4.1534543 -4.0783348 -3.9801688 -3.909997 -3.9258523 -4.0049295 -4.0797977 -4.1288042 -4.1642027 -4.1757936 -4.1805906 -4.1918421 -4.2093658][-4.2219372 -4.203331 -4.1561441 -4.077065 -3.9679959 -3.8828714 -3.8863597 -3.9604278 -4.0332518 -4.0912104 -4.1374111 -4.1544566 -4.1596808 -4.1713996 -4.1914353][-4.240037 -4.227211 -4.1893764 -4.120194 -4.0223017 -3.9400735 -3.9353125 -3.9909122 -4.0487432 -4.1027665 -4.1478424 -4.1655712 -4.1712222 -4.1833191 -4.2048845][-4.2671967 -4.2588558 -4.2289186 -4.174819 -4.09692 -4.029563 -4.0213513 -4.0581684 -4.0999951 -4.1436481 -4.1822705 -4.2016835 -4.21169 -4.2261906 -4.2493873][-4.2834306 -4.279377 -4.2569633 -4.2166924 -4.160995 -4.1130471 -4.1057644 -4.1279435 -4.1582937 -4.194488 -4.224906 -4.2436767 -4.2576537 -4.2741971 -4.294858][-4.2873893 -4.2837825 -4.2684145 -4.2402692 -4.20484 -4.1802287 -4.1799903 -4.1954155 -4.217804 -4.2442985 -4.2637138 -4.2758093 -4.2879767 -4.3035274 -4.3187346][-4.2836146 -4.2780623 -4.268609 -4.2520895 -4.2327414 -4.223217 -4.2286358 -4.2430682 -4.2603364 -4.2775836 -4.2889409 -4.2954135 -4.3044152 -4.3162131 -4.3246207][-4.2860322 -4.2770414 -4.2676539 -4.2548537 -4.24195 -4.2365689 -4.243793 -4.2568669 -4.2728748 -4.2856226 -4.2925057 -4.2971296 -4.3039556 -4.3133879 -4.3185][-4.295527 -4.2853293 -4.2755795 -4.2641068 -4.2528157 -4.2457809 -4.2486081 -4.2579517 -4.2694941 -4.2758975 -4.2786183 -4.2819657 -4.288177 -4.2964425 -4.3005266]]...]
INFO - root - 2017-12-07 18:40:59.727677: step 41410, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 52h:21m:49s remains)
INFO - root - 2017-12-07 18:41:06.574489: step 41420, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 55h:44m:06s remains)
INFO - root - 2017-12-07 18:41:13.454760: step 41430, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 55h:41m:52s remains)
INFO - root - 2017-12-07 18:41:20.260347: step 41440, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 51h:50m:54s remains)
INFO - root - 2017-12-07 18:41:27.153657: step 41450, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 52h:38m:47s remains)
INFO - root - 2017-12-07 18:41:33.967637: step 41460, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 54h:12m:23s remains)
INFO - root - 2017-12-07 18:41:40.693374: step 41470, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 57h:01m:55s remains)
INFO - root - 2017-12-07 18:41:47.278181: step 41480, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 54h:20m:53s remains)
INFO - root - 2017-12-07 18:41:54.043258: step 41490, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 51h:50m:57s remains)
INFO - root - 2017-12-07 18:42:00.907377: step 41500, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 53h:53m:24s remains)
2017-12-07 18:42:01.668511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3537235 -4.3645797 -4.373239 -4.3654294 -4.32262 -4.26046 -4.1947227 -4.1511335 -4.1436138 -4.1542711 -4.1482368 -4.1664448 -4.20985 -4.2334957 -4.2402015][-4.3571367 -4.3705931 -4.3794436 -4.3675728 -4.3166523 -4.2446547 -4.1641388 -4.1135635 -4.1187034 -4.1583786 -4.1848087 -4.2133656 -4.2502604 -4.2678914 -4.26896][-4.3584008 -4.3734555 -4.380475 -4.362679 -4.3024898 -4.219418 -4.1287704 -4.071434 -4.0828261 -4.1443129 -4.1998448 -4.2406421 -4.2743888 -4.2904787 -4.2919011][-4.359395 -4.3744688 -4.3777037 -4.3534961 -4.2842135 -4.1923366 -4.0951004 -4.0323162 -4.0527811 -4.1358004 -4.2116404 -4.2602544 -4.2908664 -4.30552 -4.3093109][-4.3600497 -4.3732009 -4.3711944 -4.3403463 -4.2592163 -4.1513214 -4.0406971 -3.9701369 -4.0073237 -4.1231017 -4.2189355 -4.2742276 -4.3011479 -4.31328 -4.317657][-4.3614693 -4.371388 -4.3645735 -4.3280063 -4.2357931 -4.1033092 -3.9637623 -3.8743296 -3.9383402 -4.0962811 -4.2191496 -4.2802315 -4.3030233 -4.3120365 -4.315371][-4.3640318 -4.3706088 -4.3610392 -4.3211875 -4.2204776 -4.061563 -3.8840189 -3.7761419 -3.8775125 -4.0780873 -4.2232432 -4.2881179 -4.3063035 -4.3119884 -4.3121943][-4.3665881 -4.3703742 -4.3584309 -4.3171897 -4.2161751 -4.0493493 -3.862751 -3.7668097 -3.8908017 -4.0981617 -4.2402616 -4.3045239 -4.32001 -4.3227024 -4.3168221][-4.368082 -4.3697643 -4.3567486 -4.3170042 -4.2236395 -4.0687041 -3.9047523 -3.8406374 -3.9648395 -4.1491494 -4.2705283 -4.3279085 -4.3394055 -4.3370738 -4.3247309][-4.368042 -4.3690686 -4.359179 -4.3260932 -4.2434273 -4.1058793 -3.97017 -3.9313512 -4.0462704 -4.2016859 -4.301342 -4.3468351 -4.3538914 -4.3461418 -4.3268881][-4.367393 -4.3686981 -4.3634682 -4.337749 -4.2647009 -4.141088 -4.0280232 -4.0074196 -4.111948 -4.2385087 -4.31708 -4.3524275 -4.3562036 -4.3457794 -4.3217583][-4.3660555 -4.3673964 -4.3652883 -4.3446693 -4.2781181 -4.1639953 -4.0675583 -4.0590448 -4.1507187 -4.2513976 -4.3117676 -4.3391142 -4.3450747 -4.3380322 -4.3151832][-4.3649483 -4.3663306 -4.3650956 -4.3463526 -4.2829361 -4.1769505 -4.0923595 -4.0856705 -4.160068 -4.2398009 -4.2886667 -4.3112006 -4.3225269 -4.32533 -4.312242][-4.3642917 -4.3666072 -4.3653436 -4.3463454 -4.2856445 -4.1882968 -4.1127062 -4.1025677 -4.156754 -4.2173595 -4.2564573 -4.2771926 -4.2965364 -4.3139677 -4.31532][-4.3632774 -4.3671169 -4.3658156 -4.3461742 -4.2888694 -4.2015252 -4.1360822 -4.1240115 -4.1600771 -4.2027278 -4.2281823 -4.2427807 -4.2684975 -4.3024063 -4.3190832]]...]
INFO - root - 2017-12-07 18:42:08.245357: step 41510, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 57h:13m:05s remains)
INFO - root - 2017-12-07 18:42:15.060506: step 41520, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 53h:28m:28s remains)
INFO - root - 2017-12-07 18:42:21.898341: step 41530, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 52h:36m:13s remains)
INFO - root - 2017-12-07 18:42:28.647522: step 41540, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.669 sec/batch; 54h:05m:21s remains)
INFO - root - 2017-12-07 18:42:35.494977: step 41550, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 58h:40m:38s remains)
INFO - root - 2017-12-07 18:42:42.328014: step 41560, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 55h:53m:55s remains)
INFO - root - 2017-12-07 18:42:49.059681: step 41570, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 51h:38m:01s remains)
INFO - root - 2017-12-07 18:42:55.918581: step 41580, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 52h:13m:39s remains)
INFO - root - 2017-12-07 18:43:02.707627: step 41590, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 56h:52m:26s remains)
INFO - root - 2017-12-07 18:43:09.593606: step 41600, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.709 sec/batch; 57h:17m:01s remains)
2017-12-07 18:43:10.347951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3129272 -4.3259697 -4.3248973 -4.3165135 -4.3085046 -4.3017497 -4.2954307 -4.2905817 -4.2930541 -4.2972741 -4.3001575 -4.3048749 -4.313314 -4.321806 -4.3303971][-4.3023391 -4.3184071 -4.3131409 -4.2973824 -4.2809992 -4.2643042 -4.2517948 -4.2462311 -4.2530732 -4.2662206 -4.2781615 -4.2899418 -4.3020959 -4.313211 -4.3236728][-4.2849784 -4.2998543 -4.2890573 -4.2655988 -4.2388458 -4.2110524 -4.1945 -4.188343 -4.1986427 -4.22118 -4.2461691 -4.2678003 -4.283958 -4.2972293 -4.3077049][-4.2691541 -4.2780766 -4.2628856 -4.2342939 -4.1989117 -4.1606693 -4.137455 -4.1242313 -4.1342578 -4.1657314 -4.2027092 -4.2322841 -4.25195 -4.2679205 -4.2796292][-4.2605257 -4.2609529 -4.24285 -4.2116256 -4.17117 -4.1216493 -4.0790725 -4.0454726 -4.0483718 -4.0960193 -4.1513019 -4.1908736 -4.216043 -4.2357192 -4.2508698][-4.2654862 -4.261364 -4.2442236 -4.2120218 -4.1636505 -4.09503 -4.0165277 -3.9425619 -3.9365971 -4.0175891 -4.0992761 -4.1527681 -4.1878481 -4.2138977 -4.2347283][-4.259366 -4.2573028 -4.2468967 -4.21875 -4.1680608 -4.0819588 -3.9660926 -3.8417828 -3.8199143 -3.9421177 -4.0530086 -4.1190348 -4.1624074 -4.1960073 -4.2259026][-4.2365527 -4.2351494 -4.2373376 -4.2251959 -4.1910787 -4.1139293 -4.0049448 -3.8801589 -3.8435211 -3.9487996 -4.04374 -4.1000481 -4.1404529 -4.1752048 -4.2105494][-4.1965818 -4.2029018 -4.2219639 -4.2305722 -4.2216606 -4.1739869 -4.1096611 -4.037755 -4.0019903 -4.036263 -4.0737429 -4.1036487 -4.1331816 -4.1613035 -4.1957731][-4.159164 -4.1754522 -4.2132998 -4.2384386 -4.2431931 -4.2145472 -4.1865444 -4.1600771 -4.1306963 -4.1208768 -4.1216159 -4.1324573 -4.1453238 -4.1605844 -4.1930027][-4.1476116 -4.1610036 -4.2041154 -4.2362766 -4.2464447 -4.229248 -4.2207713 -4.223546 -4.2065458 -4.1823845 -4.1720614 -4.1725049 -4.1755528 -4.1800251 -4.2063403][-4.1766071 -4.1778779 -4.2036252 -4.228682 -4.2381845 -4.2325993 -4.2373919 -4.2508297 -4.2434092 -4.2241325 -4.2116618 -4.2056608 -4.2056537 -4.2103386 -4.2322931][-4.237668 -4.2279463 -4.2278986 -4.2341766 -4.2369366 -4.2371054 -4.2490706 -4.2592077 -4.2493119 -4.2395992 -4.2335205 -4.230319 -4.2368269 -4.2492819 -4.2686868][-4.2938805 -4.2803917 -4.2630835 -4.2500758 -4.2438245 -4.2475805 -4.2590709 -4.2638 -4.2535844 -4.2497416 -4.2513542 -4.2582922 -4.2744207 -4.2912235 -4.3071127][-4.3336248 -4.3202467 -4.2941117 -4.2699256 -4.2604318 -4.2678237 -4.2743945 -4.272069 -4.2611752 -4.2579126 -4.264761 -4.2801275 -4.299612 -4.3143611 -4.3264184]]...]
INFO - root - 2017-12-07 18:43:16.926027: step 41610, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 51h:44m:18s remains)
INFO - root - 2017-12-07 18:43:23.796251: step 41620, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 57h:25m:09s remains)
INFO - root - 2017-12-07 18:43:30.715387: step 41630, loss = 2.06, batch loss = 2.01 (10.7 examples/sec; 0.746 sec/batch; 60h:14m:48s remains)
INFO - root - 2017-12-07 18:43:37.580787: step 41640, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 54h:41m:57s remains)
INFO - root - 2017-12-07 18:43:44.388995: step 41650, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 51h:14m:40s remains)
INFO - root - 2017-12-07 18:43:51.159676: step 41660, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 53h:26m:48s remains)
INFO - root - 2017-12-07 18:43:57.978053: step 41670, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 55h:55m:04s remains)
INFO - root - 2017-12-07 18:44:04.774600: step 41680, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 57h:55m:03s remains)
INFO - root - 2017-12-07 18:44:11.566967: step 41690, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 54h:44m:43s remains)
INFO - root - 2017-12-07 18:44:18.338074: step 41700, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 51h:59m:41s remains)
2017-12-07 18:44:19.084129: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2461371 -4.2317405 -4.2117028 -4.19867 -4.1803021 -4.1756458 -4.1821265 -4.1881013 -4.2005334 -4.21633 -4.2291269 -4.235743 -4.2408123 -4.2446818 -4.2494721][-4.2577987 -4.2376409 -4.2073665 -4.1874857 -4.16282 -4.1536846 -4.1612263 -4.1661282 -4.1798959 -4.2077856 -4.2336206 -4.2467747 -4.2545271 -4.2590504 -4.2619472][-4.2695956 -4.2527308 -4.218843 -4.1898146 -4.1560283 -4.1424646 -4.1518855 -4.1551394 -4.1638403 -4.1989 -4.22998 -4.2470112 -4.2580252 -4.2670274 -4.2711763][-4.264122 -4.2555361 -4.2232251 -4.1899242 -4.1521144 -4.12819 -4.1277184 -4.1277032 -4.1350708 -4.177886 -4.217608 -4.2413931 -4.2543707 -4.2636991 -4.2658091][-4.2400475 -4.2348332 -4.2060475 -4.172739 -4.1359262 -4.09791 -4.08065 -4.0753937 -4.0893316 -4.1474261 -4.1983771 -4.230855 -4.2479997 -4.2564411 -4.2573333][-4.2210255 -4.2103438 -4.1773505 -4.1394982 -4.0980229 -4.0495934 -4.0122557 -3.9953349 -4.02564 -4.1041961 -4.1645579 -4.2097626 -4.2348208 -4.2454033 -4.2489839][-4.2244859 -4.20372 -4.1616945 -4.111969 -4.0612044 -4.0028229 -3.9377668 -3.9016392 -3.954011 -4.0477028 -4.111979 -4.16338 -4.1938815 -4.210218 -4.2237344][-4.2510753 -4.2238431 -4.1739297 -4.11172 -4.0445685 -3.9742055 -3.8949242 -3.8593037 -3.9149837 -4.0003495 -4.0573292 -4.1068 -4.1373644 -4.1598053 -4.1914434][-4.287601 -4.2685819 -4.2193942 -4.1508083 -4.0705042 -3.9911163 -3.9162602 -3.8865309 -3.925874 -3.978173 -4.0117335 -4.0468841 -4.0802193 -4.1154127 -4.16495][-4.3079176 -4.303359 -4.2709293 -4.2168884 -4.143868 -4.0718408 -4.0104132 -3.9831407 -3.9952264 -4.0084877 -4.0089369 -4.0189824 -4.0466156 -4.0909195 -4.1479764][-4.3022504 -4.3137426 -4.3041797 -4.2765913 -4.2283778 -4.1759729 -4.1302357 -4.1005831 -4.0885496 -4.0781817 -4.05829 -4.0522017 -4.0656462 -4.0978475 -4.1449566][-4.2783909 -4.2954044 -4.3010697 -4.2978139 -4.2778835 -4.2455015 -4.2124109 -4.18598 -4.162672 -4.1442709 -4.1240115 -4.1148338 -4.1149921 -4.1271892 -4.1525278][-4.2411284 -4.2635684 -4.2795196 -4.2867584 -4.2801342 -4.2609096 -4.2398353 -4.219491 -4.1950989 -4.178833 -4.1660004 -4.1592269 -4.1516156 -4.1540618 -4.1644125][-4.1991839 -4.220685 -4.2405691 -4.253068 -4.2526293 -4.24277 -4.2283034 -4.2110276 -4.1906958 -4.1796832 -4.1727209 -4.1713104 -4.166224 -4.1672974 -4.172152][-4.1676788 -4.1806545 -4.1976504 -4.2120476 -4.2173429 -4.2143922 -4.2004042 -4.1817975 -4.1648254 -4.1569891 -4.1560788 -4.1607409 -4.16122 -4.1637983 -4.1682997]]...]
INFO - root - 2017-12-07 18:44:25.721585: step 41710, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 58h:19m:14s remains)
INFO - root - 2017-12-07 18:44:32.572245: step 41720, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 53h:07m:52s remains)
INFO - root - 2017-12-07 18:44:39.408508: step 41730, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 51h:27m:05s remains)
INFO - root - 2017-12-07 18:44:46.263074: step 41740, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 55h:29m:15s remains)
INFO - root - 2017-12-07 18:44:53.107372: step 41750, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 57h:24m:00s remains)
INFO - root - 2017-12-07 18:44:59.839857: step 41760, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 53h:07m:03s remains)
INFO - root - 2017-12-07 18:45:06.651983: step 41770, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.651 sec/batch; 52h:34m:12s remains)
INFO - root - 2017-12-07 18:45:13.564392: step 41780, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 52h:58m:32s remains)
INFO - root - 2017-12-07 18:45:20.384233: step 41790, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.753 sec/batch; 60h:47m:46s remains)
INFO - root - 2017-12-07 18:45:27.185191: step 41800, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 52h:03m:01s remains)
2017-12-07 18:45:27.982959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.205071 -4.2214308 -4.2456388 -4.2599583 -4.2612152 -4.2541242 -4.2466831 -4.2473726 -4.2557831 -4.2659774 -4.2710938 -4.2646694 -4.25142 -4.2441034 -4.2468209][-4.1810431 -4.207303 -4.2351093 -4.2455688 -4.2420421 -4.2318664 -4.2242937 -4.2262058 -4.2339611 -4.2411771 -4.242125 -4.2321358 -4.217597 -4.2131834 -4.2228885][-4.1856027 -4.2167668 -4.2437849 -4.249342 -4.240943 -4.226933 -4.2185683 -4.2224588 -4.2312493 -4.236866 -4.2341485 -4.221519 -4.2064958 -4.2034459 -4.2171893][-4.2215009 -4.2532234 -4.2761936 -4.2773457 -4.2636061 -4.2449694 -4.2358165 -4.2413149 -4.2516489 -4.2563081 -4.2505836 -4.236002 -4.2221017 -4.2183218 -4.2285547][-4.2240334 -4.2572474 -4.280345 -4.2806654 -4.2641015 -4.2448044 -4.2374158 -4.2467165 -4.2622671 -4.2687507 -4.2651734 -4.2542224 -4.24505 -4.2411852 -4.2451305][-4.2014542 -4.2356644 -4.259376 -4.2569113 -4.233458 -4.207303 -4.1979475 -4.2138844 -4.2381029 -4.2515078 -4.2565308 -4.257051 -4.2568731 -4.2553716 -4.2553844][-4.1525984 -4.1856041 -4.209538 -4.204185 -4.1695871 -4.1261768 -4.1076508 -4.1347818 -4.1758962 -4.2026715 -4.2209983 -4.2344742 -4.2436166 -4.2474227 -4.2465439][-4.0832739 -4.1159949 -4.1415172 -4.1316285 -4.0793643 -4.0091629 -3.9768202 -4.0240841 -4.0939946 -4.1397495 -4.1697545 -4.1917477 -4.2049747 -4.2077689 -4.2017426][-4.0458145 -4.077837 -4.1012979 -4.0875287 -4.0245609 -3.9361951 -3.8884492 -3.9443636 -4.0320683 -4.0917196 -4.1281896 -4.1545086 -4.1691852 -4.1685629 -4.1529903][-4.081254 -4.1099973 -4.1306195 -4.123158 -4.0801578 -4.0182018 -3.9824243 -4.0188804 -4.0825276 -4.1237941 -4.1440744 -4.1583629 -4.1644039 -4.1572156 -4.1339984][-4.1426921 -4.1682186 -4.1831918 -4.1787124 -4.1551876 -4.1208811 -4.0985484 -4.114336 -4.1479039 -4.170167 -4.179091 -4.1854639 -4.1875024 -4.1805506 -4.1567154][-4.1871777 -4.2023921 -4.2084661 -4.2048817 -4.1959109 -4.185709 -4.1782575 -4.183146 -4.1966023 -4.2071433 -4.2123733 -4.2178974 -4.2241545 -4.2229 -4.2033629][-4.1919708 -4.1891503 -4.1854348 -4.1827893 -4.1892414 -4.2019539 -4.2130466 -4.2237349 -4.2309113 -4.2348924 -4.2372613 -4.2422943 -4.2520885 -4.257607 -4.2465539][-4.16323 -4.1481051 -4.1419864 -4.1432972 -4.1622567 -4.1883831 -4.2093897 -4.2210937 -4.2204227 -4.2182341 -4.2174997 -4.2240858 -4.2382183 -4.2503424 -4.2473965][-4.109303 -4.0906239 -4.087944 -4.0943475 -4.1190009 -4.14631 -4.1643143 -4.1720295 -4.163063 -4.1522951 -4.1491709 -4.1578326 -4.1780543 -4.1989708 -4.2066321]]...]
INFO - root - 2017-12-07 18:45:34.534721: step 41810, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 55h:56m:23s remains)
INFO - root - 2017-12-07 18:45:41.370342: step 41820, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 57h:41m:51s remains)
INFO - root - 2017-12-07 18:45:48.200551: step 41830, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 54h:27m:48s remains)
INFO - root - 2017-12-07 18:45:54.974686: step 41840, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.643 sec/batch; 51h:52m:36s remains)
INFO - root - 2017-12-07 18:46:01.835936: step 41850, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 56h:57m:57s remains)
INFO - root - 2017-12-07 18:46:08.619911: step 41860, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 56h:59m:23s remains)
INFO - root - 2017-12-07 18:46:15.315891: step 41870, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 54h:40m:25s remains)
INFO - root - 2017-12-07 18:46:22.130256: step 41880, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 54h:50m:32s remains)
INFO - root - 2017-12-07 18:46:28.936430: step 41890, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 52h:17m:13s remains)
INFO - root - 2017-12-07 18:46:35.928165: step 41900, loss = 2.05, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 58h:40m:07s remains)
2017-12-07 18:46:36.644676: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1796746 -4.1898379 -4.2053924 -4.2181969 -4.2268658 -4.2361064 -4.2505684 -4.2549758 -4.2371483 -4.2173886 -4.2072868 -4.2112126 -4.2191639 -4.2111607 -4.1836185][-4.2234745 -4.2219868 -4.2294168 -4.2342238 -4.2312322 -4.2226663 -4.2273726 -4.2312741 -4.2132697 -4.1960878 -4.1903291 -4.19795 -4.2036524 -4.1892262 -4.1568365][-4.217598 -4.2070541 -4.2159271 -4.2238536 -4.2168126 -4.1969557 -4.1852293 -4.1832747 -4.1704197 -4.1595583 -4.1535845 -4.1559711 -4.1547055 -4.1374407 -4.1063371][-4.18161 -4.1771126 -4.1856937 -4.1928749 -4.1885376 -4.1613135 -4.1313934 -4.1114635 -4.1060848 -4.1126127 -4.1090074 -4.1055026 -4.0931673 -4.0693469 -4.0397625][-4.1328726 -4.1433339 -4.1496649 -4.1464424 -4.1389036 -4.1088114 -4.0597782 -4.0160465 -4.0251913 -4.0621343 -4.0754452 -4.0686951 -4.0496473 -4.0122948 -3.9688807][-4.0626931 -4.0923104 -4.100503 -4.0881648 -4.0808964 -4.0529056 -3.9781244 -3.8967872 -3.9207146 -3.9992952 -4.0308127 -4.0183325 -3.9997509 -3.9646606 -3.9313838][-3.9745338 -4.0242047 -4.0465536 -4.0405006 -4.0353837 -3.9979126 -3.8763323 -3.744431 -3.8045006 -3.9336739 -3.9815745 -3.9777765 -3.9754524 -3.9644659 -3.9613087][-3.8925242 -3.9519725 -3.987752 -4.0018926 -4.0088491 -3.9723697 -3.8311467 -3.6873477 -3.7778387 -3.9209044 -3.9824984 -4.0028014 -4.0194349 -4.0198412 -4.0302067][-3.8847914 -3.9263525 -3.9567852 -3.9862509 -4.018158 -4.007916 -3.9130824 -3.8273623 -3.8860707 -3.9806464 -4.0404706 -4.0731792 -4.0882525 -4.0864892 -4.0982866][-3.9546754 -3.9689569 -3.9861779 -4.0179577 -4.0622478 -4.0735793 -4.0223117 -3.9810903 -4.0104 -4.0550866 -4.1006823 -4.1315904 -4.1381154 -4.1351948 -4.1436381][-4.048636 -4.0372319 -4.0419903 -4.0627694 -4.0995688 -4.1144528 -4.090239 -4.0781894 -4.094451 -4.1118274 -4.1381059 -4.1537442 -4.1554432 -4.1560283 -4.160953][-4.1182041 -4.0917749 -4.0823903 -4.0905685 -4.1147938 -4.1285276 -4.1273737 -4.1368542 -4.1474681 -4.14928 -4.1583457 -4.1627727 -4.1650229 -4.162991 -4.1611576][-4.1540794 -4.1234708 -4.1100783 -4.1136951 -4.1245179 -4.137044 -4.1557474 -4.178689 -4.1826973 -4.1734586 -4.170785 -4.1676359 -4.1677351 -4.1628332 -4.1591854][-4.177959 -4.1580315 -4.1474533 -4.14242 -4.1409512 -4.1498036 -4.175508 -4.1986527 -4.1973548 -4.1816435 -4.1728492 -4.1667786 -4.1669507 -4.1679769 -4.1683249][-4.1937451 -4.1821823 -4.1717529 -4.1611991 -4.1547489 -4.1634483 -4.1856132 -4.2022839 -4.1959667 -4.1770396 -4.1637435 -4.1590552 -4.1630683 -4.1708903 -4.1785178]]...]
INFO - root - 2017-12-07 18:46:43.179334: step 41910, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 51h:58m:44s remains)
INFO - root - 2017-12-07 18:46:50.045403: step 41920, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 57h:42m:28s remains)
INFO - root - 2017-12-07 18:46:56.960242: step 41930, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.730 sec/batch; 58h:53m:16s remains)
INFO - root - 2017-12-07 18:47:03.711413: step 41940, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 53h:26m:17s remains)
INFO - root - 2017-12-07 18:47:10.596497: step 41950, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 54h:27m:13s remains)
INFO - root - 2017-12-07 18:47:17.380036: step 41960, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 53h:16m:19s remains)
INFO - root - 2017-12-07 18:47:24.216633: step 41970, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 55h:34m:34s remains)
INFO - root - 2017-12-07 18:47:31.088421: step 41980, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.741 sec/batch; 59h:48m:23s remains)
INFO - root - 2017-12-07 18:47:37.867278: step 41990, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 53h:40m:54s remains)
INFO - root - 2017-12-07 18:47:44.611573: step 42000, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 52h:19m:50s remains)
2017-12-07 18:47:45.288253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2264452 -4.2266512 -4.2254219 -4.2239509 -4.22341 -4.2256455 -4.2240143 -4.2080517 -4.1856866 -4.1483727 -4.1226921 -4.1306109 -4.1558852 -4.1736417 -4.1851711][-4.2209458 -4.2113457 -4.2067356 -4.2118869 -4.2231369 -4.235148 -4.2312841 -4.2102695 -4.1785707 -4.1370745 -4.1164007 -4.1308403 -4.1563807 -4.1670227 -4.1717334][-4.2141757 -4.1938486 -4.1821752 -4.1891603 -4.208467 -4.2298522 -4.2290454 -4.2072892 -4.1760926 -4.1426516 -4.1303582 -4.1433764 -4.1576514 -4.1531343 -4.1482854][-4.2083282 -4.1743431 -4.15561 -4.1632648 -4.1822658 -4.2055063 -4.2038755 -4.1804304 -4.1591339 -4.1457686 -4.1477785 -4.1576829 -4.1530266 -4.1272516 -4.1086993][-4.191185 -4.1491585 -4.1293483 -4.136405 -4.147016 -4.1605625 -4.1495686 -4.1202621 -4.1132655 -4.1257954 -4.1431103 -4.1540151 -4.1335793 -4.0931363 -4.0645909][-4.1714492 -4.1291308 -4.1129627 -4.1204448 -4.1159096 -4.103683 -4.06424 -4.0230355 -4.0375848 -4.0844817 -4.1198831 -4.1325235 -4.105669 -4.0632238 -4.0360937][-4.1558785 -4.1151419 -4.0956016 -4.0953555 -4.06553 -4.0097809 -3.917604 -3.8473434 -3.9065077 -4.0109649 -4.0804772 -4.1081719 -4.0891685 -4.0603285 -4.0450797][-4.1389656 -4.0983248 -4.0689964 -4.0472474 -3.9819283 -3.8723958 -3.7053835 -3.58564 -3.7200689 -3.9103923 -4.0251479 -4.0754237 -4.0719061 -4.0638862 -4.0663872][-4.1323075 -4.0941648 -4.0565071 -4.0126338 -3.9291716 -3.7976472 -3.6004498 -3.4623334 -3.6460714 -3.8701239 -3.9938605 -4.0455332 -4.0453882 -4.0498471 -4.0699868][-4.1368365 -4.1083364 -4.0744696 -4.0320334 -3.9677603 -3.8751309 -3.7472491 -3.6737518 -3.8004315 -3.9463961 -4.0245113 -4.0540462 -4.0489364 -4.05565 -4.0803747][-4.1530943 -4.1343212 -4.1131673 -4.0855432 -4.0485544 -3.9968064 -3.9324296 -3.8993793 -3.9638679 -4.0367742 -4.0729222 -4.0867186 -4.0816741 -4.0810494 -4.0947962][-4.184114 -4.1731005 -4.1618977 -4.1507378 -4.1349506 -4.10621 -4.074677 -4.0590534 -4.0857019 -4.1149354 -4.1293764 -4.135941 -4.1315241 -4.12338 -4.1239104][-4.2311387 -4.2216015 -4.2143321 -4.2109051 -4.2043457 -4.1878586 -4.1702271 -4.1598425 -4.1694365 -4.1824617 -4.1903238 -4.190309 -4.1825094 -4.171875 -4.1702194][-4.2775178 -4.272243 -4.269948 -4.27075 -4.2690783 -4.2606387 -4.2500167 -4.2412691 -4.2451673 -4.2518587 -4.25408 -4.2488823 -4.2393 -4.2294078 -4.2260962][-4.3107247 -4.3109636 -4.3135304 -4.3172379 -4.3177958 -4.3128977 -4.3077703 -4.3032727 -4.3053312 -4.3090587 -4.3093987 -4.3048663 -4.2977839 -4.287499 -4.28153]]...]
INFO - root - 2017-12-07 18:47:52.030873: step 42010, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 58h:31m:17s remains)
INFO - root - 2017-12-07 18:47:58.887183: step 42020, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 57h:28m:05s remains)
INFO - root - 2017-12-07 18:48:05.684755: step 42030, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 52h:12m:52s remains)
INFO - root - 2017-12-07 18:48:12.500888: step 42040, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 54h:20m:06s remains)
INFO - root - 2017-12-07 18:48:19.292934: step 42050, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 54h:33m:51s remains)
INFO - root - 2017-12-07 18:48:26.155379: step 42060, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 57h:34m:31s remains)
INFO - root - 2017-12-07 18:48:33.053486: step 42070, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 55h:13m:43s remains)
INFO - root - 2017-12-07 18:48:39.922511: step 42080, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 52h:47m:16s remains)
INFO - root - 2017-12-07 18:48:46.655232: step 42090, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 54h:46m:21s remains)
INFO - root - 2017-12-07 18:48:53.371534: step 42100, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 56h:10m:46s remains)
2017-12-07 18:48:54.100243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3260369 -4.3291035 -4.3247442 -4.3237176 -4.3240538 -4.322619 -4.3185 -4.3159952 -4.3189726 -4.3215632 -4.3204856 -4.315486 -4.3089147 -4.3048744 -4.285809][-4.3236928 -4.32536 -4.3211336 -4.3193579 -4.3155413 -4.3120971 -4.3091741 -4.3098688 -4.3152037 -4.318778 -4.3174796 -4.3116484 -4.2991176 -4.2895784 -4.2726855][-4.3186307 -4.3187652 -4.3151979 -4.312634 -4.30256 -4.2966757 -4.2943082 -4.2987738 -4.3081589 -4.3157978 -4.3165503 -4.3058953 -4.2867079 -4.2782335 -4.27699][-4.3134055 -4.311718 -4.3061843 -4.29687 -4.2757082 -4.2640419 -4.2629614 -4.2687945 -4.2832832 -4.3017478 -4.3085079 -4.2908988 -4.2662191 -4.2604365 -4.2759089][-4.310524 -4.3049426 -4.2916627 -4.2709036 -4.2361908 -4.2152128 -4.2107577 -4.20965 -4.2253313 -4.2601051 -4.2756109 -4.2537065 -4.2286415 -4.2264309 -4.2501831][-4.3016963 -4.2921848 -4.2728076 -4.2417884 -4.1954055 -4.1602006 -4.139915 -4.1191454 -4.1307507 -4.1878386 -4.2198768 -4.2047381 -4.1833963 -4.1833839 -4.2029338][-4.2847581 -4.2696266 -4.2438116 -4.2041788 -4.1462922 -4.0912595 -4.0401034 -3.9823322 -3.9835124 -4.07995 -4.1499076 -4.1553855 -4.1418791 -4.1365638 -4.1410346][-4.2646542 -4.2427149 -4.2124877 -4.164763 -4.0943856 -4.0218554 -3.9342299 -3.8232312 -3.7991753 -3.9417698 -4.0612211 -4.0971127 -4.0986342 -4.092875 -4.0835552][-4.2587748 -4.2331891 -4.2003374 -4.14942 -4.0757565 -3.9960995 -3.8847008 -3.7370658 -3.6855555 -3.8498242 -3.9952526 -4.0512581 -4.06699 -4.0711551 -4.0602903][-4.2720866 -4.25473 -4.2280374 -4.1839275 -4.1198668 -4.0534191 -3.9596868 -3.8442729 -3.8065386 -3.9171162 -4.0210857 -4.0632715 -4.08265 -4.0993094 -4.1002288][-4.2941813 -4.2924347 -4.2808156 -4.2538209 -4.2075319 -4.1619654 -4.1014247 -4.0326228 -4.0106444 -4.064034 -4.1160226 -4.1339006 -4.1453 -4.1677756 -4.1814771][-4.3031778 -4.3130631 -4.3134708 -4.3027906 -4.2771158 -4.2509475 -4.2186389 -4.1823592 -4.1718793 -4.1932473 -4.2129517 -4.2132244 -4.2123618 -4.2325535 -4.2514858][-4.2953148 -4.3095613 -4.3161793 -4.3193746 -4.3144703 -4.3064685 -4.2954397 -4.28094 -4.2761893 -4.2811856 -4.2856402 -4.2785106 -4.2713609 -4.2798715 -4.291636][-4.2713342 -4.2891188 -4.2992287 -4.3110075 -4.3227057 -4.3285027 -4.3296661 -4.3257351 -4.3233666 -4.324091 -4.3238521 -4.3158832 -4.3078575 -4.3063879 -4.3063927][-4.2244291 -4.24886 -4.2633934 -4.2830367 -4.3089743 -4.3229747 -4.32883 -4.3285923 -4.3265038 -4.3273354 -4.3267536 -4.3203793 -4.3137612 -4.3098373 -4.306335]]...]
INFO - root - 2017-12-07 18:49:00.619128: step 42110, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 51h:43m:14s remains)
INFO - root - 2017-12-07 18:49:07.406123: step 42120, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 56h:16m:26s remains)
INFO - root - 2017-12-07 18:49:14.296053: step 42130, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 56h:43m:12s remains)
INFO - root - 2017-12-07 18:49:21.109384: step 42140, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 54h:11m:16s remains)
INFO - root - 2017-12-07 18:49:27.862874: step 42150, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.622 sec/batch; 50h:09m:17s remains)
INFO - root - 2017-12-07 18:49:34.705301: step 42160, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 55h:34m:17s remains)
INFO - root - 2017-12-07 18:49:41.471765: step 42170, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 57h:35m:41s remains)
INFO - root - 2017-12-07 18:49:48.257192: step 42180, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 55h:09m:46s remains)
INFO - root - 2017-12-07 18:49:55.092813: step 42190, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 55h:20m:50s remains)
INFO - root - 2017-12-07 18:50:01.874138: step 42200, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 52h:07m:37s remains)
2017-12-07 18:50:02.586162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3176122 -4.31399 -4.3118615 -4.3121777 -4.31257 -4.3129091 -4.3117027 -4.3083324 -4.3081789 -4.31055 -4.3153887 -4.3216953 -4.3270512 -4.3295765 -4.3326817][-4.2943678 -4.2906337 -4.2904525 -4.292562 -4.295692 -4.2960825 -4.2906051 -4.2815223 -4.28376 -4.2910247 -4.2998748 -4.309576 -4.3154016 -4.3155336 -4.3156843][-4.2694383 -4.2630095 -4.2648916 -4.270885 -4.2749705 -4.2702656 -4.2535667 -4.2359514 -4.2441282 -4.2600169 -4.2761383 -4.2923384 -4.2993383 -4.29632 -4.2908273][-4.236558 -4.2264719 -4.226284 -4.2320471 -4.2358928 -4.2245355 -4.1959305 -4.1662397 -4.1761937 -4.2021022 -4.2294989 -4.2554173 -4.2685819 -4.2635617 -4.2546296][-4.2070417 -4.1900272 -4.1828537 -4.1824 -4.1793804 -4.1602235 -4.1218333 -4.0815678 -4.0914226 -4.1242847 -4.1621056 -4.1982474 -4.2228866 -4.2205682 -4.2114835][-4.1818471 -4.1579585 -4.1409292 -4.1267624 -4.1077762 -4.07433 -4.0315247 -3.98477 -3.9889481 -4.0312576 -4.0867333 -4.1348286 -4.1683884 -4.1687822 -4.16212][-4.1683455 -4.1395121 -4.1075945 -4.0690064 -4.0171804 -3.9597211 -3.9141746 -3.8781106 -3.8930447 -3.9565654 -4.0311236 -4.0901022 -4.1272526 -4.1282511 -4.1240616][-4.1801214 -4.1577029 -4.1253338 -4.07298 -3.9903941 -3.9023714 -3.8490257 -3.8274465 -3.8588505 -3.933291 -4.0120525 -4.07452 -4.1116438 -4.1179028 -4.1195555][-4.1893592 -4.1755838 -4.1574039 -4.1231308 -4.0573411 -3.977891 -3.921499 -3.8915803 -3.9101131 -3.9649258 -4.0279422 -4.0780721 -4.1080441 -4.1123357 -4.117187][-4.1871433 -4.177496 -4.1701903 -4.153636 -4.111063 -4.0539522 -4.0115476 -3.9864509 -3.9953463 -4.02968 -4.071548 -4.10758 -4.1319094 -4.135078 -4.1390986][-4.1807752 -4.1703529 -4.1661291 -4.157773 -4.12849 -4.0858874 -4.0547366 -4.0373869 -4.0461822 -4.0718746 -4.1039739 -4.134943 -4.1557488 -4.1620464 -4.17006][-4.1958952 -4.1874909 -4.1860418 -4.1801262 -4.157207 -4.1214514 -4.0922375 -4.0756564 -4.0813818 -4.10169 -4.131031 -4.161562 -4.1813383 -4.1923985 -4.2032609][-4.2349463 -4.2327976 -4.2356863 -4.2333126 -4.2162118 -4.1867843 -4.1597075 -4.1434112 -4.1453266 -4.1604919 -4.1873488 -4.2153244 -4.234005 -4.2452483 -4.2532597][-4.2788067 -4.2815289 -4.2887788 -4.2908435 -4.2810607 -4.2627769 -4.2461615 -4.2352738 -4.2357225 -4.2464342 -4.2655268 -4.285924 -4.2996383 -4.3063006 -4.3076882][-4.3125467 -4.315484 -4.3213086 -4.3225646 -4.3178964 -4.3103142 -4.3039188 -4.2987342 -4.2982297 -4.3037348 -4.3159213 -4.3286362 -4.3364005 -4.3387737 -4.3372746]]...]
INFO - root - 2017-12-07 18:50:09.121884: step 42210, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 52h:07m:12s remains)
INFO - root - 2017-12-07 18:50:15.761243: step 42220, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:56m:08s remains)
INFO - root - 2017-12-07 18:50:22.463091: step 42230, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 51h:43m:58s remains)
INFO - root - 2017-12-07 18:50:29.219697: step 42240, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.707 sec/batch; 56h:59m:50s remains)
INFO - root - 2017-12-07 18:50:35.977618: step 42250, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 54h:24m:50s remains)
INFO - root - 2017-12-07 18:50:42.797918: step 42260, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 54h:10m:31s remains)
INFO - root - 2017-12-07 18:50:49.467335: step 42270, loss = 2.05, batch loss = 2.00 (12.9 examples/sec; 0.620 sec/batch; 49h:58m:14s remains)
INFO - root - 2017-12-07 18:50:56.156688: step 42280, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 51h:50m:45s remains)
INFO - root - 2017-12-07 18:51:02.958747: step 42290, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 53h:42m:20s remains)
INFO - root - 2017-12-07 18:51:09.693055: step 42300, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 55h:37m:51s remains)
2017-12-07 18:51:10.407440: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2632461 -4.2547631 -4.252768 -4.2611389 -4.2713766 -4.27579 -4.2752843 -4.2719131 -4.2685328 -4.268055 -4.2734776 -4.2849436 -4.2963147 -4.2996621 -4.3005071][-4.2424006 -4.2368526 -4.2368364 -4.2421179 -4.2522984 -4.2545457 -4.2466292 -4.235734 -4.2311482 -4.2336025 -4.2411046 -4.2556291 -4.2729487 -4.2787638 -4.2791705][-4.2232752 -4.2255425 -4.23147 -4.2353725 -4.2412195 -4.2377734 -4.21835 -4.2026868 -4.20116 -4.211278 -4.2237921 -4.2393279 -4.2564478 -4.2624092 -4.2616434][-4.19623 -4.2069511 -4.2151189 -4.2164254 -4.2161961 -4.2060103 -4.1790085 -4.161325 -4.1671829 -4.1890039 -4.2110314 -4.2324619 -4.2505188 -4.2556362 -4.2550268][-4.1538534 -4.169631 -4.1784945 -4.18006 -4.1769576 -4.1619482 -4.1334338 -4.116632 -4.1272683 -4.1568637 -4.1881752 -4.2187848 -4.2402039 -4.2477894 -4.2519083][-4.1240849 -4.1338658 -4.1433358 -4.1440191 -4.1377406 -4.1165118 -4.0892243 -4.0787539 -4.0963659 -4.1318178 -4.16879 -4.2065082 -4.23195 -4.2425551 -4.2501454][-4.1227226 -4.1179805 -4.1219912 -4.1143904 -4.09333 -4.0578103 -4.0310993 -4.0313859 -4.0608397 -4.1104412 -4.1559677 -4.2001424 -4.2312384 -4.2439442 -4.2512479][-4.1531162 -4.1281261 -4.1129775 -4.0902519 -4.0473366 -3.9895978 -3.9582453 -3.9647734 -4.0062585 -4.0740066 -4.1305828 -4.1824365 -4.2213488 -4.2394047 -4.24433][-4.1760716 -4.1397786 -4.11426 -4.0845776 -4.0362005 -3.9814172 -3.955827 -3.9613371 -4.0010452 -4.0701957 -4.1268249 -4.1754007 -4.2134171 -4.2305865 -4.232595][-4.175406 -4.1463385 -4.1231885 -4.09887 -4.0679011 -4.043304 -4.0403905 -4.047802 -4.075954 -4.1254249 -4.1623383 -4.1952586 -4.2235875 -4.2315931 -4.2275219][-4.1585608 -4.146503 -4.1323647 -4.1223555 -4.117033 -4.1212783 -4.1344337 -4.139174 -4.153131 -4.1804829 -4.1994352 -4.2160826 -4.2346997 -4.2375879 -4.2307291][-4.147377 -4.1485777 -4.1458669 -4.148901 -4.1599417 -4.1792812 -4.1983142 -4.200398 -4.2041674 -4.2184529 -4.2281232 -4.23398 -4.24555 -4.2448912 -4.234828][-4.1802106 -4.1838856 -4.1842027 -4.18656 -4.1947293 -4.2126694 -4.2303276 -4.2311249 -4.2317386 -4.2425389 -4.2499776 -4.2521067 -4.2588272 -4.2551928 -4.2431245][-4.2384753 -4.2345109 -4.2297845 -4.22526 -4.2251949 -4.2358341 -4.2451844 -4.243176 -4.2456837 -4.2571831 -4.2670383 -4.2713284 -4.2752347 -4.27047 -4.2599144][-4.2805548 -4.2710905 -4.2622485 -4.2520328 -4.2437811 -4.2447786 -4.2462177 -4.2434254 -4.2507958 -4.2647791 -4.2770677 -4.2816162 -4.2819104 -4.2794003 -4.271698]]...]
INFO - root - 2017-12-07 18:51:16.978572: step 42310, loss = 2.05, batch loss = 2.00 (16.0 examples/sec; 0.499 sec/batch; 40h:15m:01s remains)
INFO - root - 2017-12-07 18:51:23.823671: step 42320, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 53h:46m:15s remains)
INFO - root - 2017-12-07 18:51:30.625233: step 42330, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 56h:59m:09s remains)
INFO - root - 2017-12-07 18:51:37.471563: step 42340, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 58h:13m:07s remains)
INFO - root - 2017-12-07 18:51:44.311386: step 42350, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 56h:19m:33s remains)
INFO - root - 2017-12-07 18:51:50.986174: step 42360, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 51h:23m:16s remains)
INFO - root - 2017-12-07 18:51:57.775565: step 42370, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 55h:04m:11s remains)
INFO - root - 2017-12-07 18:52:04.569225: step 42380, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 56h:14m:03s remains)
INFO - root - 2017-12-07 18:52:11.451642: step 42390, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 56h:27m:59s remains)
INFO - root - 2017-12-07 18:52:18.301894: step 42400, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 55h:00m:37s remains)
2017-12-07 18:52:18.981975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3152366 -4.3201537 -4.3224311 -4.325418 -4.3274226 -4.328301 -4.330359 -4.3337655 -4.3383379 -4.3440166 -4.3479338 -4.3472819 -4.3414397 -4.3336473 -4.3271041][-4.2868252 -4.2882023 -4.2884841 -4.2896895 -4.2909284 -4.2905545 -4.2935319 -4.3011818 -4.3093987 -4.3180194 -4.3251152 -4.3244605 -4.3173742 -4.3088889 -4.3032484][-4.2591209 -4.2552938 -4.2506685 -4.2461205 -4.2400951 -4.2304592 -4.2302132 -4.2440042 -4.2615342 -4.275876 -4.2888031 -4.291142 -4.2868 -4.2809062 -4.2771659][-4.2403011 -4.2275448 -4.211503 -4.1925011 -4.1717992 -4.150476 -4.147357 -4.1719503 -4.2027912 -4.2248788 -4.2444844 -4.2525949 -4.2523775 -4.2507825 -4.2504239][-4.2235317 -4.2006416 -4.1684732 -4.1282039 -4.0919228 -4.0655732 -4.0606837 -4.0916815 -4.1329174 -4.1637917 -4.19417 -4.2143459 -4.2217402 -4.22303 -4.2265062][-4.2016363 -4.1696119 -4.123961 -4.0652237 -4.01868 -3.9929833 -3.9879918 -4.0208178 -4.0702443 -4.107491 -4.1465044 -4.1822944 -4.1992311 -4.2046895 -4.2117152][-4.1755028 -4.132679 -4.0817714 -4.0138936 -3.959754 -3.9359546 -3.9282444 -3.9600122 -4.0134287 -4.0549555 -4.1036205 -4.1548719 -4.1816297 -4.1945715 -4.2055612][-4.1605062 -4.1150665 -4.065311 -3.9981399 -3.9460807 -3.9190493 -3.9055042 -3.9352858 -3.986969 -4.028429 -4.08095 -4.1380167 -4.17044 -4.18861 -4.2023025][-4.1650839 -4.1322393 -4.0973711 -4.0440788 -3.9966114 -3.9680572 -3.9514797 -3.9705169 -4.0125756 -4.0507107 -4.0982032 -4.1466303 -4.1756563 -4.1940541 -4.2070565][-4.1794438 -4.15798 -4.138453 -4.101418 -4.0644903 -4.0432353 -4.0357985 -4.0459733 -4.0713944 -4.1002107 -4.1407795 -4.1783681 -4.1977687 -4.2103329 -4.22005][-4.1936116 -4.1758227 -4.164237 -4.1404343 -4.1158662 -4.1057129 -4.1117682 -4.1176977 -4.1283813 -4.1500597 -4.1866188 -4.2163539 -4.2280288 -4.2339044 -4.2394257][-4.2117338 -4.1930041 -4.1835 -4.1712537 -4.1626248 -4.1661868 -4.1825662 -4.1899948 -4.1960521 -4.2133217 -4.2418475 -4.2598681 -4.2636991 -4.2639976 -4.26671][-4.2430005 -4.2278028 -4.2238417 -4.2229114 -4.2253976 -4.2330914 -4.24763 -4.2538919 -4.2581034 -4.2704411 -4.287786 -4.2952681 -4.2956929 -4.2961469 -4.2980404][-4.2738929 -4.2656069 -4.2675138 -4.2718072 -4.2760129 -4.282783 -4.2918806 -4.2952647 -4.2972269 -4.3037109 -4.311584 -4.3138204 -4.314569 -4.3175969 -4.3224211][-4.3080678 -4.3042974 -4.3057294 -4.3071537 -4.3084278 -4.3117189 -4.3155866 -4.3166389 -4.3174319 -4.3201842 -4.3235621 -4.325624 -4.3268008 -4.330214 -4.3345938]]...]
INFO - root - 2017-12-07 18:52:25.345136: step 42410, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 0.540 sec/batch; 43h:29m:55s remains)
INFO - root - 2017-12-07 18:52:32.154869: step 42420, loss = 2.02, batch loss = 1.96 (11.4 examples/sec; 0.700 sec/batch; 56h:21m:56s remains)
INFO - root - 2017-12-07 18:52:38.916766: step 42430, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 52h:33m:38s remains)
INFO - root - 2017-12-07 18:52:45.706393: step 42440, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 52h:30m:47s remains)
INFO - root - 2017-12-07 18:52:52.532127: step 42450, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.724 sec/batch; 58h:20m:12s remains)
INFO - root - 2017-12-07 18:52:59.400370: step 42460, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 57h:28m:08s remains)
INFO - root - 2017-12-07 18:53:06.186089: step 42470, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 54h:56m:31s remains)
INFO - root - 2017-12-07 18:53:13.028032: step 42480, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 50h:10m:00s remains)
INFO - root - 2017-12-07 18:53:19.852995: step 42490, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 53h:59m:16s remains)
INFO - root - 2017-12-07 18:53:26.612583: step 42500, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 57h:01m:13s remains)
2017-12-07 18:53:27.287090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1744494 -4.1625504 -4.1577692 -4.1518064 -4.1498556 -4.1463313 -4.1459332 -4.1457787 -4.14249 -4.1379948 -4.1376796 -4.1386051 -4.1422772 -4.1520257 -4.171886][-4.1518149 -4.1392789 -4.1340256 -4.1268148 -4.1223354 -4.1172285 -4.1186934 -4.1264248 -4.1323037 -4.1292663 -4.1205492 -4.1119785 -4.1100626 -4.1193538 -4.1411066][-4.1586275 -4.1409097 -4.1306 -4.1215897 -4.1185303 -4.1188498 -4.1269855 -4.1417189 -4.1547151 -4.1517663 -4.1363864 -4.1190586 -4.108346 -4.1101065 -4.1231108][-4.1775331 -4.1527834 -4.1353903 -4.1262488 -4.1296854 -4.1419868 -4.15907 -4.1788244 -4.1931524 -4.1871848 -4.1677918 -4.1472645 -4.1311917 -4.1238742 -4.118978][-4.1781931 -4.1477985 -4.1267486 -4.1222034 -4.1331658 -4.1570587 -4.1837745 -4.2075768 -4.2217107 -4.2112832 -4.1850109 -4.1596622 -4.1443157 -4.13361 -4.1173186][-4.1675568 -4.1385422 -4.1197438 -4.1201506 -4.1323824 -4.1558409 -4.1818228 -4.2042422 -4.2155509 -4.2003345 -4.1682634 -4.1421556 -4.1345263 -4.1296735 -4.1156387][-4.1605997 -4.133306 -4.1129284 -4.1128573 -4.1232686 -4.1397529 -4.1563339 -4.1675496 -4.1704469 -4.1518674 -4.1196327 -4.0981965 -4.1055026 -4.1182423 -4.11947][-4.1725116 -4.1499724 -4.1281919 -4.1241469 -4.1323161 -4.14065 -4.143796 -4.1417494 -4.1360435 -4.1168604 -4.090095 -4.0768228 -4.0950756 -4.120038 -4.1316533][-4.2074332 -4.1921043 -4.1751847 -4.1693993 -4.1717062 -4.1722069 -4.1658015 -4.1539459 -4.1414723 -4.1236558 -4.102046 -4.0956154 -4.1131849 -4.1362109 -4.1466947][-4.2496834 -4.242456 -4.2316313 -4.2246237 -4.2207351 -4.2145586 -4.20338 -4.19102 -4.1811681 -4.167809 -4.1512423 -4.14596 -4.1573544 -4.1721048 -4.1776991][-4.28972 -4.288496 -4.2805357 -4.2722793 -4.2629576 -4.2516322 -4.2399297 -4.2310452 -4.2298007 -4.2272334 -4.2193351 -4.2126107 -4.21399 -4.2160549 -4.2144675][-4.3093128 -4.3114471 -4.3059764 -4.299334 -4.2904158 -4.2779427 -4.266304 -4.2615347 -4.2656107 -4.2712126 -4.2699428 -4.2619934 -4.2534571 -4.2445173 -4.2369413][-4.2999797 -4.3045616 -4.3026943 -4.2999907 -4.2947631 -4.2862363 -4.2778368 -4.2754827 -4.2792997 -4.2860155 -4.2875047 -4.2819362 -4.2689395 -4.254343 -4.2453833][-4.27959 -4.2835875 -4.2827997 -4.2835131 -4.2843823 -4.2817082 -4.2756548 -4.2707396 -4.2702913 -4.2737632 -4.2764497 -4.2752285 -4.2669172 -4.2554231 -4.2487812][-4.2624249 -4.2656107 -4.2660251 -4.2691121 -4.2726207 -4.2728519 -4.2686038 -4.2627935 -4.259419 -4.2606149 -4.2639012 -4.2659464 -4.2638454 -4.2591524 -4.2568412]]...]
INFO - root - 2017-12-07 18:53:33.862572: step 42510, loss = 2.11, batch loss = 2.05 (16.2 examples/sec; 0.492 sec/batch; 39h:39m:34s remains)
INFO - root - 2017-12-07 18:53:40.746296: step 42520, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 55h:01m:27s remains)
INFO - root - 2017-12-07 18:53:47.567114: step 42530, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 55h:11m:19s remains)
INFO - root - 2017-12-07 18:53:54.450328: step 42540, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 56h:20m:42s remains)
INFO - root - 2017-12-07 18:54:01.183020: step 42550, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 52h:25m:02s remains)
INFO - root - 2017-12-07 18:54:07.913832: step 42560, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 51h:14m:29s remains)
INFO - root - 2017-12-07 18:54:14.738957: step 42570, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 53h:12m:43s remains)
INFO - root - 2017-12-07 18:54:21.597260: step 42580, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 57h:26m:46s remains)
INFO - root - 2017-12-07 18:54:28.349201: step 42590, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 55h:09m:15s remains)
INFO - root - 2017-12-07 18:54:35.188435: step 42600, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 51h:48m:59s remains)
2017-12-07 18:54:35.892754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.264298 -4.2507954 -4.2458272 -4.2452221 -4.2483473 -4.2587037 -4.2720008 -4.2802024 -4.2773728 -4.2669287 -4.2544084 -4.2514849 -4.2637067 -4.2845812 -4.3026605][-4.1934614 -4.1804638 -4.1838322 -4.1968436 -4.2112322 -4.226696 -4.2372975 -4.2374611 -4.2235327 -4.2031903 -4.18767 -4.18988 -4.2132831 -4.2452602 -4.273077][-4.1276274 -4.1169462 -4.1305056 -4.15871 -4.1841063 -4.2008467 -4.2027559 -4.1896691 -4.1628904 -4.1351523 -4.122211 -4.1362023 -4.1716318 -4.2115746 -4.2463813][-4.0784712 -4.0733852 -4.0983763 -4.1391449 -4.1719923 -4.1846986 -4.1740541 -4.1439347 -4.105391 -4.0778971 -4.077733 -4.1077518 -4.1516285 -4.1935334 -4.2289124][-4.0586958 -4.0721784 -4.1106229 -4.1513314 -4.1745749 -4.1712985 -4.1392832 -4.0883422 -4.0463881 -4.0365887 -4.0651865 -4.115859 -4.1631112 -4.1994238 -4.2277837][-4.0916839 -4.1271467 -4.1663752 -4.1864028 -4.1803169 -4.1452522 -4.0779786 -4.009306 -3.982127 -4.0139871 -4.0832448 -4.1512442 -4.196331 -4.2230978 -4.239192][-4.1574488 -4.1948462 -4.2198467 -4.2135286 -4.1735044 -4.0997944 -3.9904504 -3.9167037 -3.9326243 -4.0235057 -4.1242328 -4.197453 -4.2344923 -4.2481627 -4.2480206][-4.2090688 -4.2335405 -4.2394719 -4.2084732 -4.1422453 -4.0370049 -3.9078236 -3.8669868 -3.9494145 -4.077857 -4.1836061 -4.2455859 -4.2654791 -4.2621274 -4.2465334][-4.231133 -4.2426391 -4.2314224 -4.1848578 -4.1054039 -3.9969792 -3.9008398 -3.9160895 -4.032506 -4.1553316 -4.2411952 -4.2782488 -4.2770205 -4.25858 -4.2334933][-4.229075 -4.2313952 -4.2119374 -4.1628766 -4.0863743 -4.0032611 -3.9624553 -4.0148077 -4.1257915 -4.2211332 -4.2763457 -4.2860675 -4.2659721 -4.2364311 -4.2088037][-4.2189431 -4.21589 -4.1969147 -4.1542211 -4.0943952 -4.0468397 -4.0501761 -4.1130824 -4.2017879 -4.2642155 -4.28875 -4.2745733 -4.2372012 -4.19655 -4.1694107][-4.2174807 -4.209599 -4.1935372 -4.1631017 -4.1211419 -4.102128 -4.1308303 -4.191823 -4.2530055 -4.283854 -4.2834473 -4.2495852 -4.194984 -4.145277 -4.124145][-4.2261233 -4.218709 -4.2050719 -4.1810966 -4.1527872 -4.1530762 -4.1915045 -4.2436328 -4.2792678 -4.2870383 -4.2667484 -4.2149105 -4.1469564 -4.0978394 -4.0936556][-4.2414742 -4.2355776 -4.2229786 -4.2039213 -4.186841 -4.1969872 -4.2347903 -4.2740011 -4.28943 -4.2786093 -4.2421064 -4.1785893 -4.1091719 -4.0719528 -4.0934095][-4.2569842 -4.2525477 -4.2417736 -4.2297115 -4.2234092 -4.2365322 -4.2644224 -4.2852755 -4.2836456 -4.25986 -4.2120342 -4.1467938 -4.0838451 -4.0652437 -4.1095819]]...]
INFO - root - 2017-12-07 18:54:42.481153: step 42610, loss = 2.09, batch loss = 2.03 (15.6 examples/sec; 0.513 sec/batch; 41h:16m:52s remains)
INFO - root - 2017-12-07 18:54:49.316688: step 42620, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.683 sec/batch; 54h:58m:11s remains)
INFO - root - 2017-12-07 18:54:56.061001: step 42630, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 51h:27m:10s remains)
INFO - root - 2017-12-07 18:55:02.921197: step 42640, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 53h:24m:02s remains)
INFO - root - 2017-12-07 18:55:09.772586: step 42650, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 56h:15m:33s remains)
INFO - root - 2017-12-07 18:55:16.576090: step 42660, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 57h:46m:25s remains)
INFO - root - 2017-12-07 18:55:23.369067: step 42670, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 57h:50m:06s remains)
INFO - root - 2017-12-07 18:55:30.148670: step 42680, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 52h:40m:58s remains)
INFO - root - 2017-12-07 18:55:36.918193: step 42690, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 52h:21m:19s remains)
INFO - root - 2017-12-07 18:55:43.803659: step 42700, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 56h:32m:18s remains)
2017-12-07 18:55:44.525774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516146 -4.2198372 -4.1851406 -4.1614728 -4.1694107 -4.198822 -4.2210388 -4.2193103 -4.2104168 -4.207417 -4.2069249 -4.2009759 -4.186892 -4.1661644 -4.1673789][-4.2421565 -4.1997871 -4.1608372 -4.1454554 -4.1694384 -4.2117591 -4.2446456 -4.2481327 -4.2319083 -4.2135315 -4.2035909 -4.19918 -4.1885233 -4.1706171 -4.1739812][-4.2252507 -4.1863813 -4.1566677 -4.1488152 -4.1764226 -4.2154841 -4.246871 -4.2514291 -4.234695 -4.2123628 -4.2041392 -4.2081871 -4.2031555 -4.1872458 -4.1853342][-4.2056141 -4.1788564 -4.1658 -4.16619 -4.1879883 -4.2121167 -4.2319236 -4.2293434 -4.2115078 -4.195519 -4.1981015 -4.21698 -4.2203016 -4.2050362 -4.1956215][-4.1967516 -4.1818638 -4.1739988 -4.1742163 -4.1808033 -4.1875405 -4.1927509 -4.1803036 -4.16208 -4.1545415 -4.1740885 -4.2072129 -4.2153935 -4.2003541 -4.1889815][-4.1946092 -4.182591 -4.1696315 -4.156992 -4.1446867 -4.1351767 -4.1219625 -4.1001029 -4.0882678 -4.1022272 -4.1407924 -4.1837878 -4.1958733 -4.1834617 -4.17527][-4.1983519 -4.1842446 -4.1620216 -4.1365652 -4.111464 -4.0955782 -4.0773382 -4.0613623 -4.0641375 -4.0959749 -4.1404896 -4.1799326 -4.1853113 -4.1684842 -4.15425][-4.2092538 -4.1919756 -4.1664906 -4.1396675 -4.1180305 -4.1097274 -4.1038451 -4.100522 -4.1081033 -4.1336393 -4.162262 -4.1806793 -4.1701493 -4.1446905 -4.1285648][-4.2197189 -4.2018147 -4.1809974 -4.1621404 -4.1517735 -4.1537647 -4.1573381 -4.1591873 -4.1603045 -4.1653376 -4.164084 -4.1559429 -4.1357508 -4.1152654 -4.1119771][-4.225153 -4.2087946 -4.1973186 -4.1906662 -4.1927052 -4.1998434 -4.2042093 -4.2015715 -4.1937165 -4.1817174 -4.156992 -4.1307082 -4.1095953 -4.1031871 -4.1138754][-4.2184467 -4.2116714 -4.2123337 -4.2153478 -4.2249694 -4.2343297 -4.23598 -4.2266488 -4.2081914 -4.1794739 -4.1418686 -4.1126957 -4.1029725 -4.107491 -4.1215296][-4.2099204 -4.2122355 -4.2225657 -4.2320061 -4.24375 -4.2510891 -4.2479787 -4.2313008 -4.2028456 -4.1650252 -4.1263008 -4.1036015 -4.1007609 -4.1063204 -4.1202011][-4.2113166 -4.2194757 -4.2360897 -4.2482071 -4.2555289 -4.2530546 -4.2388644 -4.2140923 -4.1821003 -4.1481943 -4.1180806 -4.1022158 -4.09975 -4.1026993 -4.1152878][-4.2238708 -4.2332749 -4.2491894 -4.2590079 -4.2577066 -4.2446761 -4.2228861 -4.196907 -4.1723695 -4.1511011 -4.1338844 -4.1241264 -4.1199865 -4.1183996 -4.1215134][-4.228888 -4.2425947 -4.2598281 -4.2690258 -4.2618046 -4.2420511 -4.2179661 -4.1982074 -4.1877165 -4.1806369 -4.1748414 -4.1694174 -4.1624746 -4.1532245 -4.1434655]]...]
INFO - root - 2017-12-07 18:55:51.126958: step 42710, loss = 2.05, batch loss = 2.00 (14.1 examples/sec; 0.568 sec/batch; 45h:42m:31s remains)
INFO - root - 2017-12-07 18:55:57.714375: step 42720, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 55h:12m:53s remains)
INFO - root - 2017-12-07 18:56:04.505535: step 42730, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.682 sec/batch; 54h:56m:07s remains)
INFO - root - 2017-12-07 18:56:11.328206: step 42740, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 55h:56m:00s remains)
INFO - root - 2017-12-07 18:56:18.091206: step 42750, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 53h:27m:58s remains)
INFO - root - 2017-12-07 18:56:24.908610: step 42760, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 52h:00m:05s remains)
INFO - root - 2017-12-07 18:56:31.673762: step 42770, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 55h:03m:42s remains)
INFO - root - 2017-12-07 18:56:38.394987: step 42780, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 53h:59m:52s remains)
INFO - root - 2017-12-07 18:56:45.209780: step 42790, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 54h:42m:19s remains)
INFO - root - 2017-12-07 18:56:52.059123: step 42800, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.618 sec/batch; 49h:46m:06s remains)
2017-12-07 18:56:52.838379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2683768 -4.2701368 -4.2755828 -4.2837639 -4.289741 -4.293961 -4.3008456 -4.3075871 -4.3099732 -4.3096738 -4.3104095 -4.3110161 -4.3097472 -4.3085017 -4.3061037][-4.25167 -4.2569451 -4.2672997 -4.2791634 -4.2851582 -4.2851295 -4.2888818 -4.296082 -4.299211 -4.298943 -4.30065 -4.30038 -4.2983241 -4.2975941 -4.295589][-4.2355895 -4.240427 -4.2508864 -4.2624226 -4.2638211 -4.2556968 -4.2538204 -4.2615933 -4.2653284 -4.2649722 -4.2680211 -4.2696271 -4.2717013 -4.2748518 -4.2747631][-4.2197223 -4.220861 -4.2273231 -4.2355781 -4.2323394 -4.21721 -4.2097549 -4.2187881 -4.2242355 -4.2241096 -4.2284074 -4.2344003 -4.2453666 -4.2553463 -4.2601647][-4.2001877 -4.1952138 -4.1935043 -4.19214 -4.177968 -4.1506257 -4.1321187 -4.140718 -4.1501603 -4.1534481 -4.1610312 -4.1716909 -4.1919603 -4.2088928 -4.2218332][-4.1706853 -4.1545362 -4.13964 -4.123498 -4.0926638 -4.0518613 -4.0256295 -4.0367517 -4.0557542 -4.0681038 -4.0841684 -4.1010141 -4.1278877 -4.1516342 -4.1729455][-4.131669 -4.0980473 -4.065371 -4.0301814 -3.9781258 -3.9240625 -3.8956735 -3.9185176 -3.9550822 -3.9810452 -4.01148 -4.0382729 -4.072979 -4.1012087 -4.1263132][-4.1017427 -4.0561256 -4.0159435 -3.9748559 -3.9167662 -3.8603952 -3.8408473 -3.8798089 -3.9302788 -3.9657102 -4.0041752 -4.034194 -4.0674152 -4.0925941 -4.1132679][-4.1103153 -4.0703697 -4.0439205 -4.0189285 -3.9761367 -3.9316907 -3.9225249 -3.9623415 -4.0088067 -4.0378065 -4.0692811 -4.0906563 -4.1153235 -4.1349554 -4.1468382][-4.1478333 -4.1230659 -4.1125326 -4.1023531 -4.0713654 -4.0373168 -4.0283184 -4.0546036 -4.0908546 -4.1110463 -4.1352911 -4.146625 -4.1608028 -4.176878 -4.184968][-4.1940722 -4.1828728 -4.1825948 -4.1791258 -4.1537642 -4.1243048 -4.1109 -4.1229506 -4.1502724 -4.1643496 -4.1825576 -4.185667 -4.1918859 -4.2056479 -4.2141352][-4.2318311 -4.2291107 -4.2327394 -4.2316632 -4.2118473 -4.1893749 -4.1782551 -4.1838293 -4.202435 -4.2119431 -4.2245412 -4.2256107 -4.2338367 -4.2515068 -4.2630048][-4.2549357 -4.2574782 -4.2637973 -4.2654848 -4.252037 -4.2352991 -4.2275281 -4.2319961 -4.2451477 -4.2525 -4.2607007 -4.2641749 -4.2766323 -4.2979946 -4.3117328][-4.2729793 -4.2805548 -4.2905483 -4.29749 -4.2931423 -4.2828469 -4.2794318 -4.284503 -4.2931967 -4.2975621 -4.3022795 -4.3064857 -4.3180318 -4.3369665 -4.3492436][-4.285943 -4.2935939 -4.30184 -4.3107629 -4.311964 -4.3081493 -4.3088307 -4.31421 -4.3188572 -4.3198681 -4.3202043 -4.3228226 -4.3302922 -4.3437066 -4.354507]]...]
INFO - root - 2017-12-07 18:56:59.489328: step 42810, loss = 2.07, batch loss = 2.01 (13.3 examples/sec; 0.601 sec/batch; 48h:20m:41s remains)
INFO - root - 2017-12-07 18:57:06.252619: step 42820, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 55h:01m:21s remains)
INFO - root - 2017-12-07 18:57:13.075843: step 42830, loss = 2.10, batch loss = 2.05 (12.4 examples/sec; 0.644 sec/batch; 51h:51m:29s remains)
INFO - root - 2017-12-07 18:57:19.754937: step 42840, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.664 sec/batch; 53h:23m:46s remains)
INFO - root - 2017-12-07 18:57:26.491639: step 42850, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 56h:46m:33s remains)
INFO - root - 2017-12-07 18:57:33.400997: step 42860, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 58h:23m:11s remains)
INFO - root - 2017-12-07 18:57:40.164636: step 42870, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 55h:16m:01s remains)
INFO - root - 2017-12-07 18:57:46.962840: step 42880, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 55h:11m:29s remains)
INFO - root - 2017-12-07 18:57:53.740525: step 42890, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 52h:08m:15s remains)
INFO - root - 2017-12-07 18:58:00.530603: step 42900, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 57h:14m:06s remains)
2017-12-07 18:58:01.419108: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861562 -4.2880573 -4.289937 -4.2904005 -4.2917562 -4.2936587 -4.2945051 -4.2941532 -4.2929578 -4.29034 -4.2876554 -4.2859764 -4.2850718 -4.2850909 -4.2875023][-4.2732077 -4.2705297 -4.2698436 -4.26893 -4.2695336 -4.2721281 -4.2731624 -4.2707019 -4.2663732 -4.2580643 -4.2517796 -4.2503648 -4.2504339 -4.2511272 -4.2532072][-4.2453594 -4.2371945 -4.2353106 -4.2365918 -4.2416921 -4.2490549 -4.2506275 -4.2447205 -4.2343011 -4.2179174 -4.2052946 -4.2008414 -4.2022538 -4.2051964 -4.209064][-4.2064247 -4.1950865 -4.1937737 -4.198606 -4.2098837 -4.2219806 -4.2238836 -4.212873 -4.1956434 -4.1703167 -4.1507287 -4.1459537 -4.1528206 -4.1601763 -4.1672935][-4.1544642 -4.140595 -4.1404676 -4.1495323 -4.1663179 -4.181303 -4.1813979 -4.163569 -4.1383386 -4.1076884 -4.0895214 -4.0923309 -4.1094494 -4.1248312 -4.13587][-4.086154 -4.0675349 -4.0663757 -4.0778861 -4.0974073 -4.1107779 -4.1028337 -4.0755057 -4.0475836 -4.0264773 -4.024888 -4.0436163 -4.0711017 -4.0898614 -4.0981541][-4.0427122 -4.0187373 -4.0146351 -4.0249853 -4.0405884 -4.0434861 -4.0194955 -3.9802446 -3.9587796 -3.9607232 -3.9834239 -4.0158448 -4.0489473 -4.0667806 -4.0670567][-4.06132 -4.0321908 -4.023756 -4.0308771 -4.0395246 -4.0308423 -3.9961765 -3.9537148 -3.9456758 -3.9698331 -4.0082994 -4.0430365 -4.0713863 -4.0829129 -4.0756311][-4.115416 -4.0895963 -4.0820961 -4.0891776 -4.0955935 -4.0859146 -4.0581617 -4.0294189 -4.0308909 -4.0569425 -4.0907722 -4.113059 -4.1260462 -4.127955 -4.117784][-4.1730509 -4.1519842 -4.1463308 -4.149611 -4.152101 -4.1468534 -4.1332922 -4.1203055 -4.1241069 -4.1393204 -4.1601644 -4.1702485 -4.17057 -4.1654649 -4.1563058][-4.2061548 -4.1895022 -4.1851196 -4.1860194 -4.1880713 -4.1881285 -4.1847267 -4.1812639 -4.1818457 -4.1854577 -4.1921563 -4.1914945 -4.1857929 -4.1805658 -4.1756592][-4.2357497 -4.223875 -4.220304 -4.2207823 -4.2235513 -4.2266154 -4.2274327 -4.2264543 -4.2249022 -4.2223735 -4.2201424 -4.2132859 -4.2049193 -4.2014112 -4.2010374][-4.2751765 -4.2669163 -4.2640333 -4.2649789 -4.2675085 -4.2696576 -4.2703152 -4.2693319 -4.2670932 -4.2630157 -4.2580519 -4.251616 -4.2464204 -4.2453814 -4.2469625][-4.3034883 -4.2962523 -4.2938762 -4.2947721 -4.2964244 -4.2974033 -4.2974024 -4.2961426 -4.2936196 -4.2899394 -4.2862496 -4.2827816 -4.2811146 -4.2822251 -4.2848268][-4.31313 -4.3088655 -4.3069687 -4.3075156 -4.3082795 -4.3084927 -4.3081164 -4.3070726 -4.3053703 -4.3035078 -4.302587 -4.3021421 -4.3027029 -4.3045621 -4.3069158]]...]
INFO - root - 2017-12-07 18:58:08.128628: step 42910, loss = 2.07, batch loss = 2.02 (13.0 examples/sec; 0.614 sec/batch; 49h:23m:59s remains)
INFO - root - 2017-12-07 18:58:14.837200: step 42920, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 54h:28m:28s remains)
INFO - root - 2017-12-07 18:58:21.643705: step 42930, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 56h:16m:37s remains)
INFO - root - 2017-12-07 18:58:28.285526: step 42940, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 53h:57m:23s remains)
INFO - root - 2017-12-07 18:58:35.033797: step 42950, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.637 sec/batch; 51h:16m:25s remains)
INFO - root - 2017-12-07 18:58:41.765049: step 42960, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 52h:43m:46s remains)
INFO - root - 2017-12-07 18:58:48.553357: step 42970, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 55h:59m:15s remains)
INFO - root - 2017-12-07 18:58:55.295383: step 42980, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 56h:43m:05s remains)
INFO - root - 2017-12-07 18:59:01.999166: step 42990, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 55h:21m:00s remains)
INFO - root - 2017-12-07 18:59:08.784453: step 43000, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 56h:18m:10s remains)
2017-12-07 18:59:09.497712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31104 -4.2940211 -4.27695 -4.2636595 -4.2613177 -4.2691684 -4.2803636 -4.2860408 -4.2799115 -4.265718 -4.2468953 -4.2227025 -4.2006059 -4.194386 -4.21725][-4.29623 -4.2773662 -4.2579541 -4.238308 -4.22863 -4.2312288 -4.2408433 -4.2467017 -4.2394648 -4.2228804 -4.1963682 -4.160028 -4.1280451 -4.1190195 -4.1563764][-4.2803349 -4.2630286 -4.243917 -4.2175174 -4.1955447 -4.1871681 -4.1925297 -4.1996 -4.1987081 -4.185986 -4.1569567 -4.1112075 -4.0689459 -4.057436 -4.1093245][-4.2621861 -4.25049 -4.235817 -4.2050943 -4.1715469 -4.1480579 -4.1393919 -4.1438956 -4.155622 -4.1574011 -4.13616 -4.0906878 -4.0480165 -4.0412979 -4.1006575][-4.2476263 -4.2419748 -4.22821 -4.1940622 -4.1494808 -4.1015139 -4.0645008 -4.0585752 -4.0909772 -4.1241732 -4.1237912 -4.0900526 -4.0579696 -4.0627279 -4.1238742][-4.2273288 -4.2210917 -4.2023816 -4.1673417 -4.1162567 -4.0470324 -3.9748895 -3.9428487 -3.9921052 -4.063283 -4.0934558 -4.0831809 -4.0736008 -4.094625 -4.1568222][-4.2025914 -4.1913595 -4.16854 -4.1371484 -4.0858784 -3.9992762 -3.887018 -3.8132524 -3.8663895 -3.973062 -4.0416212 -4.0659814 -4.0858479 -4.12631 -4.1882067][-4.1798239 -4.1707225 -4.1532178 -4.1267552 -4.0722952 -3.9731143 -3.836597 -3.7269356 -3.7733152 -3.9023275 -4.0027957 -4.0591121 -4.1023474 -4.1532264 -4.21252][-4.1785173 -4.1729531 -4.1665444 -4.147047 -4.0960684 -4.005825 -3.886374 -3.7920465 -3.8200483 -3.9243939 -4.0199742 -4.0818653 -4.1334848 -4.1850243 -4.2376733][-4.18303 -4.1824813 -4.188076 -4.1788378 -4.1363339 -4.0667396 -3.9825141 -3.9230003 -3.9405499 -4.0067549 -4.0767932 -4.1288543 -4.174305 -4.219388 -4.262732][-4.1845584 -4.1885109 -4.1985865 -4.197401 -4.1667962 -4.119575 -4.0688887 -4.0392313 -4.0535178 -4.095665 -4.1441827 -4.1842384 -4.2201486 -4.2550569 -4.2873363][-4.181428 -4.1844254 -4.1919017 -4.1953526 -4.1806788 -4.1566229 -4.1342316 -4.1247745 -4.138648 -4.1679945 -4.2018027 -4.2323332 -4.2600656 -4.2854958 -4.3068781][-4.1767535 -4.1782136 -4.1813884 -4.1879807 -4.1900196 -4.1906281 -4.1916819 -4.1950464 -4.2070122 -4.2269058 -4.2494235 -4.2704711 -4.2902622 -4.3074212 -4.3204174][-4.1942663 -4.1971631 -4.199626 -4.2092776 -4.2234049 -4.2380695 -4.2496271 -4.2568693 -4.26424 -4.2755327 -4.2877016 -4.2996349 -4.3114381 -4.3214331 -4.3292427][-4.2458944 -4.2489672 -4.2529569 -4.2614112 -4.2744932 -4.2867951 -4.2953544 -4.3008924 -4.3054328 -4.3109703 -4.3157554 -4.3204246 -4.3260345 -4.3311787 -4.3353434]]...]
INFO - root - 2017-12-07 18:59:16.159877: step 43010, loss = 2.05, batch loss = 1.99 (13.3 examples/sec; 0.600 sec/batch; 48h:16m:23s remains)
INFO - root - 2017-12-07 18:59:22.977820: step 43020, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 55h:35m:08s remains)
INFO - root - 2017-12-07 18:59:29.598641: step 43030, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 52h:21m:04s remains)
INFO - root - 2017-12-07 18:59:36.495498: step 43040, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 54h:15m:45s remains)
INFO - root - 2017-12-07 18:59:43.243986: step 43050, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 56h:04m:24s remains)
INFO - root - 2017-12-07 18:59:50.039468: step 43060, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 58h:57m:37s remains)
INFO - root - 2017-12-07 18:59:56.862351: step 43070, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 54h:02m:24s remains)
INFO - root - 2017-12-07 19:00:03.624835: step 43080, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 51h:34m:20s remains)
INFO - root - 2017-12-07 19:00:10.466893: step 43090, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 52h:13m:16s remains)
INFO - root - 2017-12-07 19:00:17.249725: step 43100, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 56h:34m:31s remains)
2017-12-07 19:00:17.986175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2428 -4.2510524 -4.2544794 -4.2488847 -4.2366538 -4.2250862 -4.2099037 -4.1861215 -4.1727118 -4.1768384 -4.1864705 -4.199688 -4.2107472 -4.223577 -4.2397327][-4.2321734 -4.2382259 -4.2388153 -4.2322674 -4.2189732 -4.2045536 -4.1870813 -4.1613111 -4.1480417 -4.1544914 -4.1664662 -4.184094 -4.1989069 -4.2133079 -4.227869][-4.2249417 -4.2234755 -4.221076 -4.2149434 -4.202786 -4.1881003 -4.1709375 -4.1463289 -4.135139 -4.1453109 -4.1621833 -4.1828408 -4.19889 -4.2117281 -4.2208538][-4.22109 -4.2070622 -4.2018223 -4.1967444 -4.188004 -4.17583 -4.1576405 -4.1304188 -4.1173639 -4.1299405 -4.1526475 -4.1772013 -4.1957722 -4.2077174 -4.2139521][-4.2058687 -4.1830678 -4.1765256 -4.1724939 -4.1632261 -4.1488733 -4.1264181 -4.0949063 -4.0811977 -4.1026287 -4.1346335 -4.1626873 -4.1843233 -4.1996312 -4.2093062][-4.1712518 -4.149404 -4.1440377 -4.1348619 -4.1173372 -4.09587 -4.0653639 -4.0253654 -4.0124607 -4.051568 -4.10092 -4.1373262 -4.1649976 -4.1882477 -4.2053509][-4.1224508 -4.1021075 -4.089673 -4.0683928 -4.0341983 -4.0018253 -3.9594307 -3.9034429 -3.8850989 -3.9422207 -4.01548 -4.0727558 -4.1173072 -4.1547942 -4.1858397][-4.0649462 -4.0442104 -4.0293345 -4.002789 -3.96281 -3.92891 -3.8813736 -3.8128312 -3.7844627 -3.8494158 -3.9391189 -4.01554 -4.0774765 -4.1273513 -4.1697826][-4.0369434 -4.0220361 -4.015749 -4.0024066 -3.9821076 -3.973053 -3.951807 -3.9085586 -3.8836744 -3.9238093 -3.9885538 -4.0511184 -4.1045647 -4.1483154 -4.1868563][-4.0212717 -4.0173707 -4.0234756 -4.0269842 -4.0323129 -4.052381 -4.0601583 -4.0428324 -4.0250893 -4.0436282 -4.0823555 -4.1250443 -4.1626515 -4.1939287 -4.2219014][-4.0191884 -4.0205736 -4.0275564 -4.0363731 -4.0534072 -4.0826139 -4.1010809 -4.0986185 -4.089221 -4.0992928 -4.1266894 -4.1633754 -4.19722 -4.2243361 -4.2482615][-4.0465903 -4.0455041 -4.0451574 -4.0466409 -4.0585737 -4.0821548 -4.10024 -4.1040978 -4.1025319 -4.1134796 -4.1394758 -4.1756859 -4.2100487 -4.2387743 -4.2635756][-4.1208324 -4.1171227 -4.1127 -4.1078997 -4.1109333 -4.1244254 -4.1365991 -4.1419373 -4.1456809 -4.1581078 -4.1809964 -4.2104421 -4.2383337 -4.2626262 -4.2843876][-4.2223306 -4.2198567 -4.2169967 -4.2136459 -4.2142115 -4.2198887 -4.2250409 -4.2276964 -4.2308846 -4.23865 -4.2523489 -4.2692895 -4.2855806 -4.2999892 -4.3134179][-4.29962 -4.2981648 -4.2972159 -4.2962 -4.2959433 -4.2969565 -4.297883 -4.2988024 -4.30067 -4.3044419 -4.310504 -4.317697 -4.3243966 -4.3306575 -4.3368082]]...]
INFO - root - 2017-12-07 19:00:24.471613: step 43110, loss = 2.08, batch loss = 2.02 (16.8 examples/sec; 0.476 sec/batch; 38h:14m:18s remains)
INFO - root - 2017-12-07 19:00:31.254456: step 43120, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 53h:16m:25s remains)
INFO - root - 2017-12-07 19:00:38.182934: step 43130, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 0.770 sec/batch; 61h:55m:11s remains)
INFO - root - 2017-12-07 19:00:45.058469: step 43140, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 57h:08m:27s remains)
INFO - root - 2017-12-07 19:00:51.776836: step 43150, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 53h:01m:44s remains)
INFO - root - 2017-12-07 19:00:58.590488: step 43160, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 51h:53m:19s remains)
INFO - root - 2017-12-07 19:01:05.437495: step 43170, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 57h:38m:54s remains)
INFO - root - 2017-12-07 19:01:12.221511: step 43180, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 56h:20m:52s remains)
INFO - root - 2017-12-07 19:01:19.082565: step 43190, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 53h:52m:55s remains)
INFO - root - 2017-12-07 19:01:25.889957: step 43200, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 51h:49m:00s remains)
2017-12-07 19:01:26.705087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1383386 -4.1263571 -4.1038418 -4.0794444 -4.0636325 -4.0536704 -4.0703249 -4.103981 -4.1237149 -4.1272993 -4.1277838 -4.1270552 -4.1229186 -4.113255 -4.1094222][-4.1423082 -4.1325307 -4.1225696 -4.1095219 -4.0948839 -4.0800934 -4.0831151 -4.0984049 -4.1063328 -4.1043968 -4.1012359 -4.0995541 -4.0988131 -4.0974221 -4.1079607][-4.1384473 -4.1323857 -4.13382 -4.1258087 -4.1080561 -4.0880184 -4.0738516 -4.0720491 -4.0751581 -4.0726008 -4.0680318 -4.0663533 -4.0666575 -4.074019 -4.0968094][-4.1295018 -4.1298289 -4.140676 -4.134059 -4.1093731 -4.0790763 -4.04572 -4.0266676 -4.0283937 -4.03272 -4.0382576 -4.0414066 -4.0381384 -4.0428829 -4.0678134][-4.1158252 -4.1278791 -4.146976 -4.1430755 -4.1125011 -4.0673518 -4.0148873 -3.9808037 -3.9825351 -3.9975686 -4.0198603 -4.0385342 -4.036881 -4.0353346 -4.0600548][-4.1045313 -4.1293583 -4.1521807 -4.1502123 -4.1127357 -4.0501814 -3.9811406 -3.9371843 -3.9406767 -3.9696581 -4.01728 -4.0587497 -4.0665736 -4.06339 -4.087821][-4.103322 -4.1418328 -4.1689386 -4.1682525 -4.122716 -4.0408254 -3.9551573 -3.9029706 -3.9088769 -3.9509976 -4.0241818 -4.0902009 -4.1113596 -4.111454 -4.1302733][-4.1135192 -4.162241 -4.1934233 -4.192884 -4.1363516 -4.0370789 -3.9364188 -3.8788719 -3.8848851 -3.9353507 -4.0258837 -4.1104207 -4.1448722 -4.1518955 -4.1657209][-4.1253147 -4.1802497 -4.2130594 -4.2090368 -4.1445475 -4.0362806 -3.93336 -3.8780272 -3.8846414 -3.9366257 -4.0285544 -4.1187 -4.1626863 -4.17706 -4.1888127][-4.1411428 -4.1913185 -4.2199292 -4.2132931 -4.1503086 -4.0481825 -3.9548864 -3.9061573 -3.9111564 -3.9575129 -4.0396819 -4.1234827 -4.1713982 -4.1904473 -4.1992626][-4.1570807 -4.1958618 -4.2180915 -4.21159 -4.1596012 -4.0757818 -4.0006566 -3.9619298 -3.9634128 -3.9994965 -4.0653491 -4.1336551 -4.1765451 -4.1956964 -4.201304][-4.1737971 -4.1997027 -4.2159524 -4.2098808 -4.1724572 -4.111496 -4.0561972 -4.0260172 -4.0250072 -4.0502272 -4.09879 -4.1505876 -4.1850867 -4.2000856 -4.2022963][-4.1935983 -4.2067552 -4.2164359 -4.2105594 -4.1883593 -4.1523013 -4.1172762 -4.0944719 -4.0923967 -4.1056595 -4.1349683 -4.1699591 -4.1955609 -4.2045488 -4.2049289][-4.2181659 -4.2204652 -4.2238722 -4.2176137 -4.2064576 -4.1916618 -4.1746807 -4.1597314 -4.1583753 -4.163805 -4.1761775 -4.1952205 -4.21085 -4.2135859 -4.2107453][-4.2463741 -4.2381067 -4.2324648 -4.222713 -4.2173843 -4.2166529 -4.2146258 -4.2081709 -4.2090874 -4.212954 -4.2150512 -4.2210464 -4.2269912 -4.2251086 -4.2199144]]...]
INFO - root - 2017-12-07 19:01:33.360211: step 43210, loss = 2.07, batch loss = 2.01 (14.4 examples/sec; 0.557 sec/batch; 44h:43m:25s remains)
INFO - root - 2017-12-07 19:01:40.199961: step 43220, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 58h:09m:18s remains)
INFO - root - 2017-12-07 19:01:47.051723: step 43230, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.618 sec/batch; 49h:38m:51s remains)
INFO - root - 2017-12-07 19:01:54.002376: step 43240, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.749 sec/batch; 60h:12m:06s remains)
INFO - root - 2017-12-07 19:02:00.935291: step 43250, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 58h:09m:46s remains)
INFO - root - 2017-12-07 19:02:07.792190: step 43260, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 56h:07m:19s remains)
INFO - root - 2017-12-07 19:02:14.598230: step 43270, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 52h:41m:23s remains)
INFO - root - 2017-12-07 19:02:21.448913: step 43280, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 54h:20m:02s remains)
INFO - root - 2017-12-07 19:02:28.218130: step 43290, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 53h:23m:40s remains)
INFO - root - 2017-12-07 19:02:35.032362: step 43300, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 56h:56m:00s remains)
2017-12-07 19:02:35.696320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2544942 -4.2648168 -4.2657804 -4.2534976 -4.2450552 -4.2418394 -4.24022 -4.23618 -4.2319264 -4.22486 -4.2129836 -4.2011347 -4.1761503 -4.1558833 -4.1642675][-4.2415533 -4.2625318 -4.2689314 -4.2525291 -4.2353539 -4.2271523 -4.2230811 -4.2138596 -4.2023048 -4.1902161 -4.1779928 -4.1708817 -4.1560597 -4.143188 -4.1562285][-4.2421894 -4.2726092 -4.2793236 -4.255743 -4.2266803 -4.2099752 -4.1985383 -4.187397 -4.1769867 -4.1616373 -4.1470709 -4.14563 -4.1446543 -4.1407514 -4.1518431][-4.2443833 -4.2820683 -4.2863851 -4.252728 -4.2125635 -4.18375 -4.1607051 -4.1481586 -4.1465893 -4.1360288 -4.1251087 -4.1278276 -4.1343389 -4.1349225 -4.1442342][-4.2466264 -4.2893443 -4.2908874 -4.2495427 -4.200161 -4.1553259 -4.1170678 -4.10182 -4.1120715 -4.1188726 -4.1175928 -4.1242309 -4.1246071 -4.1174684 -4.1264086][-4.24741 -4.2820544 -4.2732162 -4.2269545 -4.1678166 -4.1038651 -4.0485768 -4.03539 -4.0676579 -4.0989466 -4.1175523 -4.1332984 -4.1228628 -4.0986657 -4.1000643][-4.2375808 -4.2581134 -4.2394443 -4.1876359 -4.116046 -4.0215516 -3.9323442 -3.9260778 -3.9907925 -4.0528364 -4.1018424 -4.1343327 -4.1259003 -4.0933323 -4.0864358][-4.2292023 -4.2397537 -4.2164049 -4.1669116 -4.0850506 -3.960005 -3.8414588 -3.8423069 -3.9311996 -4.0143337 -4.0856862 -4.1309166 -4.1311145 -4.1035509 -4.1024556][-4.2285943 -4.2334051 -4.2149835 -4.1777625 -4.1053748 -3.9859653 -3.8754272 -3.8751783 -3.9494386 -4.0187907 -4.08714 -4.1348863 -4.1466336 -4.1333957 -4.1383915][-4.219923 -4.2251959 -4.2155447 -4.196671 -4.149148 -4.0669265 -3.9924402 -3.9837306 -4.0148344 -4.0507097 -4.10184 -4.146287 -4.1624246 -4.1568961 -4.161572][-4.2077732 -4.2165527 -4.2169423 -4.2150917 -4.1928816 -4.1446943 -4.0973988 -4.0768747 -4.0747786 -4.0811887 -4.1175361 -4.1580405 -4.1743288 -4.1742077 -4.1732459][-4.2034621 -4.216012 -4.2238312 -4.2303643 -4.224051 -4.1951103 -4.1597881 -4.128942 -4.1088934 -4.1023173 -4.129209 -4.161469 -4.1800909 -4.1852503 -4.1798983][-4.1993871 -4.2174592 -4.2309766 -4.2429538 -4.2473497 -4.2318869 -4.2022748 -4.16229 -4.1298881 -4.1101227 -4.1147437 -4.1300893 -4.1465893 -4.1603165 -4.1566381][-4.1994081 -4.2194324 -4.233531 -4.2500238 -4.2650228 -4.260891 -4.2399473 -4.2028956 -4.1610174 -4.1276321 -4.1064258 -4.097158 -4.1058555 -4.1194086 -4.1140509][-4.20435 -4.225101 -4.2345252 -4.2499313 -4.2688551 -4.2728882 -4.2602735 -4.2336931 -4.1997781 -4.1693268 -4.1350107 -4.1132541 -4.1198196 -4.127461 -4.1157303]]...]
INFO - root - 2017-12-07 19:02:42.264810: step 43310, loss = 2.05, batch loss = 1.99 (15.6 examples/sec; 0.512 sec/batch; 41h:09m:40s remains)
INFO - root - 2017-12-07 19:02:49.154823: step 43320, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 56h:02m:35s remains)
INFO - root - 2017-12-07 19:02:55.954617: step 43330, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 57h:41m:10s remains)
INFO - root - 2017-12-07 19:03:02.587925: step 43340, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 52h:28m:39s remains)
INFO - root - 2017-12-07 19:03:09.541534: step 43350, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 57h:22m:25s remains)
INFO - root - 2017-12-07 19:03:16.399901: step 43360, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.696 sec/batch; 55h:54m:56s remains)
INFO - root - 2017-12-07 19:03:23.241988: step 43370, loss = 2.11, batch loss = 2.06 (11.4 examples/sec; 0.703 sec/batch; 56h:29m:18s remains)
INFO - root - 2017-12-07 19:03:30.058526: step 43380, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 52h:59m:15s remains)
INFO - root - 2017-12-07 19:03:36.762770: step 43390, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 52h:23m:18s remains)
INFO - root - 2017-12-07 19:03:43.538728: step 43400, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 55h:20m:29s remains)
2017-12-07 19:03:44.322303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2334256 -4.2092052 -4.2039056 -4.2170687 -4.2258992 -4.223217 -4.2211504 -4.2335567 -4.2568383 -4.2713561 -4.2786021 -4.282372 -4.2875657 -4.2857423 -4.2749095][-4.2408705 -4.2082062 -4.1962352 -4.2069941 -4.2181554 -4.2209964 -4.2214842 -4.2348824 -4.2592573 -4.2748766 -4.2835245 -4.2845049 -4.2852993 -4.2821074 -4.2756162][-4.2441115 -4.2065563 -4.1854172 -4.1910086 -4.2048831 -4.2157984 -4.2244625 -4.2436934 -4.2686996 -4.2844048 -4.2931933 -4.2904558 -4.2894835 -4.2868862 -4.2846117][-4.2572064 -4.2140441 -4.1793 -4.172677 -4.1812067 -4.1958618 -4.2186384 -4.2510891 -4.2812748 -4.2974524 -4.3064585 -4.3031383 -4.2994204 -4.2953644 -4.2951469][-4.2734075 -4.2262993 -4.1789947 -4.1566529 -4.1514764 -4.1642776 -4.1993713 -4.2490692 -4.288486 -4.3056216 -4.3151541 -4.310998 -4.3030791 -4.2953687 -4.2938571][-4.2866397 -4.2374315 -4.1790209 -4.1386442 -4.116457 -4.1227756 -4.1685185 -4.2345991 -4.2843285 -4.3034482 -4.3104367 -4.3044071 -4.2930889 -4.2818313 -4.2770238][-4.2731309 -4.2252541 -4.1623859 -4.1050658 -4.0644708 -4.0594363 -4.1102805 -4.1930294 -4.25866 -4.2859 -4.2932868 -4.2858109 -4.2714796 -4.2566919 -4.2472172][-4.2343254 -4.1917443 -4.1329656 -4.067523 -4.01147 -3.9920049 -4.0402608 -4.1345992 -4.2151713 -4.2528429 -4.265492 -4.2600684 -4.2462177 -4.2288074 -4.2134352][-4.1806474 -4.1448765 -4.0954075 -4.0318966 -3.9694171 -3.9370592 -3.9726145 -4.0659361 -4.1549006 -4.2038474 -4.2261772 -4.2291493 -4.21931 -4.2023826 -4.1854167][-4.1379542 -4.1063461 -4.0655928 -4.0103583 -3.9458611 -3.9007978 -3.9188375 -4.0028653 -4.0936718 -4.15192 -4.185009 -4.1978378 -4.1939573 -4.1813016 -4.1659856][-4.1371174 -4.1096864 -4.0742054 -4.024579 -3.9584484 -3.9006376 -3.8992634 -3.9676983 -4.0512662 -4.1101756 -4.1485925 -4.16867 -4.1728425 -4.1682534 -4.1591034][-4.1760993 -4.15546 -4.1238117 -4.0773292 -4.0096126 -3.9421632 -3.9237926 -3.971621 -4.0400758 -4.0935941 -4.1333694 -4.1577821 -4.1684113 -4.1696796 -4.1638856][-4.2352419 -4.2221217 -4.1956377 -4.1543713 -4.0913053 -4.0279107 -4.0023365 -4.0263076 -4.0724554 -4.1149764 -4.150661 -4.17345 -4.1832714 -4.1828637 -4.1724524][-4.2882171 -4.2808876 -4.2603874 -4.227664 -4.1772008 -4.1271672 -4.10423 -4.1121941 -4.1373076 -4.1658044 -4.1925473 -4.2093496 -4.2128596 -4.201952 -4.1777954][-4.3235292 -4.3191695 -4.3048663 -4.2819476 -4.2458177 -4.2107997 -4.19438 -4.1943011 -4.2032356 -4.2179952 -4.2347903 -4.247983 -4.24764 -4.226016 -4.1839213]]...]
INFO - root - 2017-12-07 19:03:50.954456: step 43410, loss = 2.06, batch loss = 2.00 (14.9 examples/sec; 0.538 sec/batch; 43h:12m:56s remains)
INFO - root - 2017-12-07 19:03:57.732098: step 43420, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 51h:50m:14s remains)
INFO - root - 2017-12-07 19:04:04.683075: step 43430, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 56h:46m:07s remains)
INFO - root - 2017-12-07 19:04:11.644104: step 43440, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 54h:57m:34s remains)
INFO - root - 2017-12-07 19:04:18.478267: step 43450, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 55h:51m:01s remains)
INFO - root - 2017-12-07 19:04:25.274304: step 43460, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 53h:09m:45s remains)
INFO - root - 2017-12-07 19:04:32.095839: step 43470, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 52h:37m:46s remains)
INFO - root - 2017-12-07 19:04:38.905441: step 43480, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 54h:54m:41s remains)
INFO - root - 2017-12-07 19:04:45.762304: step 43490, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 55h:52m:58s remains)
INFO - root - 2017-12-07 19:04:52.644188: step 43500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 57h:28m:54s remains)
2017-12-07 19:04:53.350451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009024 -4.2855334 -4.271656 -4.2621164 -4.25947 -4.2593412 -4.2519045 -4.2384295 -4.2336926 -4.2465911 -4.2682195 -4.2896738 -4.3068085 -4.3226457 -4.3304648][-4.2894874 -4.2717104 -4.2559643 -4.2442942 -4.2402234 -4.2408757 -4.2318807 -4.215663 -4.2131033 -4.2339892 -4.2660017 -4.2932067 -4.3129563 -4.3263459 -4.3298373][-4.2721367 -4.2518559 -4.2332449 -4.2180128 -4.2118354 -4.2126746 -4.2049341 -4.1881437 -4.1902375 -4.2184916 -4.2596264 -4.2965875 -4.3210163 -4.3318939 -4.3297067][-4.2415524 -4.2168412 -4.1987581 -4.1846619 -4.1772909 -4.1735468 -4.1600447 -4.1384926 -4.143187 -4.1797662 -4.2311821 -4.2796559 -4.3139515 -4.3266578 -4.3240447][-4.1895657 -4.1618547 -4.1502509 -4.1459661 -4.1384435 -4.119956 -4.0825887 -4.0444679 -4.05791 -4.1158423 -4.1826053 -4.2411828 -4.2876196 -4.3088579 -4.3124008][-4.1249833 -4.0923638 -4.0910449 -4.0984573 -4.0886989 -4.0446367 -3.9607196 -3.892396 -3.9269249 -4.0263705 -4.1207685 -4.1945829 -4.2526855 -4.2879691 -4.3018422][-4.0721192 -4.0439076 -4.0531216 -4.0647874 -4.0402074 -3.9517748 -3.7918599 -3.6860611 -3.7723064 -3.930594 -4.0593867 -4.1505661 -4.2182341 -4.2656126 -4.2909436][-4.0517068 -4.0375891 -4.0503755 -4.0499759 -4.0043154 -3.8772225 -3.6580553 -3.5406821 -3.6912379 -3.8917811 -4.0351138 -4.1270027 -4.1944351 -4.2478228 -4.2823462][-4.0718675 -4.0720663 -4.0831957 -4.070015 -4.0213108 -3.90682 -3.7276 -3.655354 -3.7836976 -3.9446597 -4.0632048 -4.1380024 -4.1941705 -4.2429037 -4.2799854][-4.1273212 -4.1327696 -4.1373134 -4.1235862 -4.0912023 -4.0225425 -3.9178545 -3.8818781 -3.9515455 -4.0492764 -4.13235 -4.1872439 -4.225677 -4.2592878 -4.2868643][-4.2120624 -4.2175651 -4.2135081 -4.1985478 -4.17504 -4.136457 -4.0795841 -4.0617442 -4.0962129 -4.1559339 -4.2132592 -4.248096 -4.2686782 -4.2845845 -4.3025837][-4.2880673 -4.2903495 -4.2805262 -4.26698 -4.2503877 -4.2281528 -4.1956191 -4.18411 -4.2047434 -4.2465506 -4.2862816 -4.3062015 -4.3135233 -4.3164721 -4.3254528][-4.3311434 -4.3308282 -4.3204765 -4.3098297 -4.3027277 -4.295722 -4.2801256 -4.2716074 -4.2851882 -4.3138056 -4.33896 -4.3501425 -4.3471284 -4.3425732 -4.3451586][-4.3384953 -4.3366446 -4.3297143 -4.3241739 -4.3209424 -4.3219008 -4.3190069 -4.3155365 -4.3244357 -4.3404269 -4.353003 -4.35873 -4.3547807 -4.3515744 -4.3527622][-4.332696 -4.3303523 -4.3258028 -4.3220415 -4.3190556 -4.3193994 -4.3199558 -4.3168092 -4.3203449 -4.3283014 -4.3375859 -4.344687 -4.3465261 -4.3480315 -4.3489676]]...]
INFO - root - 2017-12-07 19:04:59.956004: step 43510, loss = 2.09, batch loss = 2.03 (14.2 examples/sec; 0.565 sec/batch; 45h:21m:45s remains)
INFO - root - 2017-12-07 19:05:06.764078: step 43520, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 56h:23m:08s remains)
INFO - root - 2017-12-07 19:05:13.657727: step 43530, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 54h:07m:49s remains)
INFO - root - 2017-12-07 19:05:20.515848: step 43540, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 53h:30m:10s remains)
INFO - root - 2017-12-07 19:05:27.363946: step 43550, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 52h:29m:46s remains)
INFO - root - 2017-12-07 19:05:34.170786: step 43560, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 57h:59m:03s remains)
INFO - root - 2017-12-07 19:05:41.110950: step 43570, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 55h:50m:30s remains)
INFO - root - 2017-12-07 19:05:48.011575: step 43580, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.664 sec/batch; 53h:16m:04s remains)
INFO - root - 2017-12-07 19:05:54.886308: step 43590, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 53h:18m:38s remains)
INFO - root - 2017-12-07 19:06:01.795753: step 43600, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 56h:31m:03s remains)
2017-12-07 19:06:02.507799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1613665 -4.1452532 -4.1327033 -4.1128082 -4.1059494 -4.1181917 -4.1459332 -4.1887527 -4.225554 -4.2323008 -4.2179384 -4.1938467 -4.1606555 -4.1273823 -4.0851393][-4.1634417 -4.1542082 -4.1480908 -4.1347136 -4.1310616 -4.1422691 -4.1650996 -4.2003689 -4.2311935 -4.2410388 -4.236568 -4.2226763 -4.1948085 -4.1618519 -4.119864][-4.1789331 -4.1767569 -4.1749735 -4.1660647 -4.1624136 -4.168695 -4.1806512 -4.2011037 -4.2262106 -4.2410841 -4.2440729 -4.2362242 -4.2106805 -4.1792731 -4.1416936][-4.2013812 -4.2027245 -4.2010474 -4.1911 -4.1844311 -4.18391 -4.1825914 -4.1873546 -4.2064266 -4.2224584 -4.2273593 -4.2217846 -4.1981978 -4.1703439 -4.14093][-4.2212043 -4.2222538 -4.2170482 -4.2011089 -4.1881537 -4.1783986 -4.1622071 -4.1534824 -4.1702895 -4.1877613 -4.193841 -4.1921725 -4.175209 -4.1584015 -4.1445909][-4.2305064 -4.2274585 -4.2146735 -4.1885734 -4.1639156 -4.1421671 -4.1127319 -4.0941172 -4.1148224 -4.1401749 -4.1534572 -4.1621985 -4.1594081 -4.1609712 -4.1667891][-4.2253537 -4.2155366 -4.1937933 -4.156322 -4.1191559 -4.0879979 -4.0495315 -4.0252719 -4.0542812 -4.0945783 -4.1215191 -4.1436977 -4.1585908 -4.1775289 -4.1981297][-4.2143593 -4.2003069 -4.1754632 -4.1367235 -4.0949826 -4.0604863 -4.0204353 -3.99233 -4.0210686 -4.0668426 -4.1017013 -4.1299071 -4.1531477 -4.1797924 -4.2078214][-4.2079711 -4.1946397 -4.1733942 -4.1428528 -4.1081595 -4.0783834 -4.0477037 -4.0219541 -4.0332203 -4.0646772 -4.0941372 -4.1164122 -4.1380444 -4.1641474 -4.1964426][-4.2051663 -4.1976 -4.1845827 -4.1649222 -4.1375141 -4.1114435 -4.0893369 -4.065968 -4.0629439 -4.0801635 -4.1015873 -4.1140914 -4.1302381 -4.1526589 -4.1833792][-4.187767 -4.1829495 -4.1751423 -4.1630473 -4.140554 -4.1132545 -4.0942163 -4.0745525 -4.0715046 -4.0901256 -4.1132874 -4.1248584 -4.139513 -4.155993 -4.1781044][-4.1517606 -4.1477232 -4.1463079 -4.1423154 -4.1277027 -4.1054316 -4.0887675 -4.0724106 -4.073 -4.09556 -4.1205497 -4.1350837 -4.1484442 -4.1592913 -4.1708641][-4.0995073 -4.1001382 -4.1068478 -4.1107893 -4.1081643 -4.0977206 -4.0872312 -4.0749269 -4.077 -4.0962439 -4.1157093 -4.1263189 -4.1337762 -4.1390948 -4.143496][-4.0568914 -4.0646029 -4.08212 -4.0963817 -4.105216 -4.1056008 -4.1016521 -4.0945678 -4.0960917 -4.1066966 -4.1153989 -4.1187139 -4.12026 -4.1221108 -4.1220489][-4.0367689 -4.0499458 -4.0754704 -4.0970621 -4.1107397 -4.1155081 -4.1148038 -4.111495 -4.1124358 -4.1161995 -4.1171765 -4.11666 -4.1168976 -4.1183314 -4.1165924]]...]
INFO - root - 2017-12-07 19:06:09.084523: step 43610, loss = 2.05, batch loss = 1.99 (16.8 examples/sec; 0.476 sec/batch; 38h:13m:33s remains)
INFO - root - 2017-12-07 19:06:15.849701: step 43620, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 53h:07m:40s remains)
INFO - root - 2017-12-07 19:06:22.687194: step 43630, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 57h:35m:40s remains)
INFO - root - 2017-12-07 19:06:29.500817: step 43640, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 54h:24m:47s remains)
INFO - root - 2017-12-07 19:06:36.119171: step 43650, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 51h:42m:51s remains)
INFO - root - 2017-12-07 19:06:43.068709: step 43660, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 55h:14m:59s remains)
INFO - root - 2017-12-07 19:06:49.879707: step 43670, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 54h:51m:17s remains)
INFO - root - 2017-12-07 19:06:56.653823: step 43680, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 57h:43m:33s remains)
INFO - root - 2017-12-07 19:07:03.484726: step 43690, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 54h:26m:37s remains)
INFO - root - 2017-12-07 19:07:10.225849: step 43700, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 51h:35m:34s remains)
2017-12-07 19:07:10.978776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2976723 -4.2735739 -4.2507315 -4.24415 -4.2321229 -4.2148342 -4.2101731 -4.2023368 -4.1929803 -4.1934929 -4.194036 -4.1963563 -4.1976471 -4.2010517 -4.2068343][-4.2994847 -4.2725749 -4.2491889 -4.2407231 -4.2238755 -4.2018733 -4.1933284 -4.18265 -4.1739626 -4.1800241 -4.1902485 -4.2012582 -4.2081561 -4.2147188 -4.22162][-4.2950048 -4.2671452 -4.2462015 -4.2376323 -4.2188897 -4.1929111 -4.1812625 -4.171699 -4.1693316 -4.1790338 -4.1944451 -4.2105165 -4.2171988 -4.2204938 -4.2259893][-4.292572 -4.266571 -4.251339 -4.2452607 -4.22397 -4.1947393 -4.1777558 -4.1663523 -4.1669807 -4.1794562 -4.2003913 -4.2199268 -4.2243242 -4.2227836 -4.229403][-4.2945051 -4.2732034 -4.2616549 -4.2534175 -4.2290769 -4.1974254 -4.1739826 -4.1509304 -4.1447148 -4.1608839 -4.1876345 -4.2129765 -4.2214622 -4.223722 -4.2378421][-4.2998004 -4.2830954 -4.2703953 -4.2559462 -4.2229295 -4.1817183 -4.1419115 -4.09589 -4.0813332 -4.1027837 -4.138588 -4.1774831 -4.1972165 -4.2109418 -4.2342887][-4.3054118 -4.2899222 -4.2727885 -4.2456136 -4.1939778 -4.1270533 -4.0525718 -3.9790461 -3.9690552 -4.0162888 -4.0800047 -4.1404486 -4.1725149 -4.1947684 -4.2220459][-4.3043613 -4.2858043 -4.2593579 -4.2163062 -4.13989 -4.0412192 -3.93284 -3.8537164 -3.8827055 -3.9745796 -4.0657139 -4.1357 -4.1726909 -4.1946115 -4.219614][-4.2977486 -4.2724195 -4.2358713 -4.1789722 -4.0888648 -3.9805217 -3.8824778 -3.8458858 -3.9201603 -4.0226116 -4.1032419 -4.1580606 -4.1904383 -4.2076325 -4.2276587][-4.2899117 -4.2593069 -4.2194738 -4.1622 -4.0847569 -4.011529 -3.9618082 -3.9608812 -4.022593 -4.0880828 -4.1367984 -4.1743455 -4.2016721 -4.2154279 -4.2334414][-4.2866039 -4.2567811 -4.2202559 -4.1744151 -4.1194696 -4.0786238 -4.0524235 -4.0489454 -4.0788736 -4.1106396 -4.1356544 -4.1599393 -4.1819124 -4.1999331 -4.22527][-4.2883816 -4.261158 -4.229238 -4.1939054 -4.1504655 -4.11834 -4.0901194 -4.0745835 -4.0834994 -4.1043353 -4.1221628 -4.1359372 -4.1533594 -4.1804829 -4.2157989][-4.29251 -4.2655449 -4.2342081 -4.2021484 -4.1605873 -4.1272726 -4.0911622 -4.0627694 -4.0660667 -4.0938878 -4.1204119 -4.129498 -4.1432266 -4.17396 -4.2145085][-4.2951465 -4.2659678 -4.2318311 -4.1999483 -4.1599469 -4.1264653 -4.0899558 -4.057518 -4.0652184 -4.1066351 -4.1439972 -4.1574249 -4.1663861 -4.1901455 -4.2243423][-4.2954698 -4.2652044 -4.2308307 -4.2025647 -4.1638613 -4.1358094 -4.1119051 -4.0906215 -4.1081123 -4.1503572 -4.1888094 -4.2007022 -4.1982093 -4.20841 -4.2309308]]...]
INFO - root - 2017-12-07 19:07:17.529983: step 43710, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 0.516 sec/batch; 41h:22m:59s remains)
INFO - root - 2017-12-07 19:07:24.399383: step 43720, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 55h:28m:20s remains)
INFO - root - 2017-12-07 19:07:31.169872: step 43730, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 52h:54m:03s remains)
INFO - root - 2017-12-07 19:07:37.964446: step 43740, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 55h:36m:54s remains)
INFO - root - 2017-12-07 19:07:44.874173: step 43750, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 56h:25m:21s remains)
INFO - root - 2017-12-07 19:07:51.698104: step 43760, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.671 sec/batch; 53h:48m:31s remains)
INFO - root - 2017-12-07 19:07:58.543821: step 43770, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 52h:16m:37s remains)
INFO - root - 2017-12-07 19:08:05.243212: step 43780, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 50h:36m:27s remains)
INFO - root - 2017-12-07 19:08:12.057375: step 43790, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 54h:55m:04s remains)
INFO - root - 2017-12-07 19:08:18.841923: step 43800, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 56h:13m:34s remains)
2017-12-07 19:08:19.557346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3031077 -4.3057451 -4.3059092 -4.3055406 -4.3041062 -4.3031764 -4.3038831 -4.3011456 -4.2946787 -4.2892165 -4.2873497 -4.2845674 -4.269187 -4.2418447 -4.2151866][-4.3211489 -4.3211117 -4.3186836 -4.3155141 -4.3107181 -4.3084717 -4.30966 -4.3058262 -4.2967315 -4.2897291 -4.2902532 -4.2895551 -4.2759495 -4.2496653 -4.2146935][-4.3325248 -4.3311291 -4.3263068 -4.3182178 -4.3076196 -4.3019152 -4.3013511 -4.2958355 -4.2834053 -4.2756047 -4.280127 -4.287436 -4.2875595 -4.2731323 -4.2377992][-4.333303 -4.3300366 -4.3195987 -4.3053374 -4.2878284 -4.2761164 -4.2747393 -4.2681584 -4.2539706 -4.2468133 -4.2532244 -4.2709179 -4.2875638 -4.2889462 -4.2622786][-4.3200908 -4.3128023 -4.298265 -4.2805076 -4.2552962 -4.23696 -4.2341485 -4.225822 -4.2112455 -4.2053308 -4.211401 -4.2352476 -4.2637262 -4.2774315 -4.2628884][-4.2986488 -4.2819586 -4.2610917 -4.2385831 -4.2042737 -4.1755834 -4.168263 -4.1600747 -4.1486006 -4.1443367 -4.1500888 -4.176476 -4.2141471 -4.2345862 -4.2336941][-4.2610478 -4.2328663 -4.2041183 -4.1722198 -4.1184216 -4.06692 -4.05601 -4.0578914 -4.0608835 -4.0676703 -4.080276 -4.111968 -4.1573863 -4.1852388 -4.1917109][-4.2165451 -4.1792264 -4.1417055 -4.0946813 -4.0126934 -3.9323218 -3.9169796 -3.9364758 -3.9699044 -4.00358 -4.0331869 -4.0730867 -4.1241345 -4.1556664 -4.1617551][-4.2028513 -4.1657271 -4.1239095 -4.0680447 -3.9741735 -3.8826997 -3.8701229 -3.9033508 -3.9596899 -4.0111895 -4.0557384 -4.1023164 -4.1506743 -4.1761727 -4.1771245][-4.2272682 -4.2001486 -4.164156 -4.117311 -4.0444565 -3.9793379 -3.9764903 -4.0051351 -4.0531821 -4.0979366 -4.1381397 -4.1761003 -4.21212 -4.2333884 -4.2341046][-4.2588444 -4.2471113 -4.2237549 -4.19186 -4.1490045 -4.116251 -4.1203179 -4.1402545 -4.1707888 -4.2006054 -4.22749 -4.2502522 -4.2719231 -4.286026 -4.285][-4.2722306 -4.2722812 -4.261138 -4.247973 -4.2333465 -4.224442 -4.2311544 -4.2443981 -4.2613187 -4.27734 -4.2920947 -4.3026047 -4.3111596 -4.3178988 -4.3181787][-4.2742405 -4.2850513 -4.2841949 -4.2837715 -4.2841544 -4.2852044 -4.2903194 -4.2974505 -4.3069272 -4.3162289 -4.3241887 -4.3263068 -4.3266687 -4.3283691 -4.3294096][-4.2794065 -4.2949972 -4.29893 -4.3024864 -4.3060112 -4.3069472 -4.3068337 -4.307631 -4.3141212 -4.3235526 -4.3308969 -4.3324685 -4.3297095 -4.3272376 -4.3262939][-4.2925572 -4.30421 -4.3045816 -4.3041239 -4.3033495 -4.3008823 -4.2971649 -4.2959294 -4.3017511 -4.3119087 -4.320178 -4.3232961 -4.3229671 -4.3223805 -4.3222127]]...]
INFO - root - 2017-12-07 19:08:26.075917: step 43810, loss = 2.10, batch loss = 2.05 (16.4 examples/sec; 0.489 sec/batch; 39h:11m:40s remains)
INFO - root - 2017-12-07 19:08:32.964784: step 43820, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 55h:05m:36s remains)
INFO - root - 2017-12-07 19:08:39.762169: step 43830, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 55h:50m:05s remains)
INFO - root - 2017-12-07 19:08:46.607904: step 43840, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 52h:44m:06s remains)
INFO - root - 2017-12-07 19:08:53.455339: step 43850, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 52h:06m:55s remains)
INFO - root - 2017-12-07 19:09:00.286188: step 43860, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 52h:13m:30s remains)
INFO - root - 2017-12-07 19:09:07.250604: step 43870, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 57h:50m:24s remains)
INFO - root - 2017-12-07 19:09:14.158708: step 43880, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 56h:19m:03s remains)
INFO - root - 2017-12-07 19:09:20.961676: step 43890, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 52h:10m:53s remains)
INFO - root - 2017-12-07 19:09:27.818911: step 43900, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 53h:46m:23s remains)
2017-12-07 19:09:28.545495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.211978 -4.2185659 -4.2270918 -4.2266221 -4.2204266 -4.2135482 -4.2087908 -4.2070584 -4.2097025 -4.2189054 -4.2277365 -4.2390623 -4.25243 -4.2634568 -4.2659216][-4.2171907 -4.2223597 -4.2296629 -4.2305136 -4.2253022 -4.2173195 -4.2089338 -4.2028818 -4.2049594 -4.216351 -4.2255092 -4.2370496 -4.250845 -4.2628222 -4.2653952][-4.2192273 -4.2247758 -4.232934 -4.2378874 -4.2354012 -4.2252893 -4.2107544 -4.1976166 -4.1974149 -4.2106462 -4.2224503 -4.2368646 -4.2528129 -4.2657905 -4.2672954][-4.2159734 -4.2235126 -4.2345066 -4.2443409 -4.2443986 -4.2332745 -4.2127814 -4.1926055 -4.1889324 -4.2028623 -4.2174191 -4.2357278 -4.2548227 -4.27009 -4.2710896][-4.2027707 -4.21286 -4.2287269 -4.2449923 -4.2501869 -4.2402987 -4.2156487 -4.1886239 -4.1803589 -4.1931138 -4.2099152 -4.2327209 -4.2558346 -4.2738409 -4.2747517][-4.1816368 -4.194097 -4.2148695 -4.2382464 -4.2505374 -4.2448545 -4.2185397 -4.1849856 -4.171226 -4.1817541 -4.2008295 -4.2278223 -4.2549448 -4.2747655 -4.2751713][-4.1600156 -4.1740255 -4.1990848 -4.2285147 -4.2474079 -4.2467136 -4.2211356 -4.1832671 -4.1639562 -4.17129 -4.1918325 -4.2218089 -4.2520137 -4.2729211 -4.2730761][-4.1429491 -4.156549 -4.1837726 -4.216013 -4.2392869 -4.2436748 -4.2208905 -4.1816974 -4.1580186 -4.1624379 -4.1837668 -4.2155657 -4.2479186 -4.2692018 -4.2689037][-4.1336312 -4.144783 -4.171206 -4.2034407 -4.2283721 -4.2370791 -4.2182355 -4.1797361 -4.1533771 -4.1550732 -4.176394 -4.2092342 -4.2432237 -4.2649612 -4.2648764][-4.1280856 -4.1363268 -4.1605844 -4.1910706 -4.2170944 -4.2295146 -4.215024 -4.1790905 -4.15141 -4.1506524 -4.1712179 -4.204164 -4.2390423 -4.2614675 -4.2625675][-4.1281176 -4.1326647 -4.1547093 -4.1837277 -4.2105589 -4.2254333 -4.2146277 -4.182313 -4.1551237 -4.1523647 -4.1716166 -4.2031736 -4.2365832 -4.2591305 -4.261632][-4.1375523 -4.1389127 -4.1587868 -4.1861567 -4.2126026 -4.2280278 -4.2195373 -4.1909833 -4.1656547 -4.1621008 -4.1797452 -4.2082748 -4.2376032 -4.2585669 -4.26205][-4.152277 -4.1514835 -4.16925 -4.1945591 -4.2196193 -4.2342305 -4.2268791 -4.2021031 -4.1797361 -4.1764731 -4.1923609 -4.2169962 -4.2413106 -4.2596836 -4.2631197][-4.1686263 -4.1665177 -4.1821752 -4.2051015 -4.2279854 -4.2413378 -4.2347865 -4.2133994 -4.1941123 -4.1914797 -4.2053242 -4.2262793 -4.2464867 -4.2624283 -4.2657671][-4.1837425 -4.1805906 -4.19328 -4.2131705 -4.2339568 -4.2462072 -4.2409029 -4.2226396 -4.2066231 -4.2045946 -4.2163892 -4.2344823 -4.2519269 -4.265934 -4.2692056]]...]
INFO - root - 2017-12-07 19:09:35.188301: step 43910, loss = 2.07, batch loss = 2.01 (13.4 examples/sec; 0.597 sec/batch; 47h:51m:16s remains)
INFO - root - 2017-12-07 19:09:41.967765: step 43920, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 52h:37m:14s remains)
INFO - root - 2017-12-07 19:09:48.774978: step 43930, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 51h:16m:11s remains)
INFO - root - 2017-12-07 19:09:55.637328: step 43940, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 57h:15m:18s remains)
INFO - root - 2017-12-07 19:10:02.539304: step 43950, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 57h:25m:22s remains)
INFO - root - 2017-12-07 19:10:09.165648: step 43960, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 53h:21m:37s remains)
INFO - root - 2017-12-07 19:10:15.930258: step 43970, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 53h:56m:04s remains)
INFO - root - 2017-12-07 19:10:22.727270: step 43980, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 55h:21m:47s remains)
INFO - root - 2017-12-07 19:10:29.566167: step 43990, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 58h:06m:11s remains)
INFO - root - 2017-12-07 19:10:36.531256: step 44000, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 58h:00m:40s remains)
2017-12-07 19:10:37.341327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1897707 -4.1474504 -4.1133952 -4.0946317 -4.0966873 -4.1095243 -4.1221538 -4.1435432 -4.18588 -4.2398739 -4.2840195 -4.3101754 -4.3243556 -4.3281584 -4.3353796][-4.1631322 -4.1089005 -4.0641441 -4.0411272 -4.0469694 -4.0657125 -4.0781307 -4.0995879 -4.1470847 -4.2117114 -4.267684 -4.299118 -4.3176169 -4.3230786 -4.3300743][-4.1566057 -4.1003847 -4.0513654 -4.0249381 -4.031085 -4.0551076 -4.0702858 -4.0935059 -4.1403551 -4.2056108 -4.263423 -4.2940369 -4.313406 -4.3179502 -4.3240905][-4.1630788 -4.1165361 -4.0731554 -4.0511203 -4.0619216 -4.0929141 -4.11336 -4.1325741 -4.1665931 -4.2202454 -4.2692113 -4.2944818 -4.3124285 -4.3144488 -4.3187418][-4.1755395 -4.1367483 -4.1027179 -4.0928483 -4.1084547 -4.1408982 -4.1622405 -4.169951 -4.1848183 -4.2232156 -4.2639322 -4.2883673 -4.3071189 -4.3098893 -4.3133688][-4.1929936 -4.1517782 -4.1231484 -4.1264305 -4.1429296 -4.1647654 -4.1735063 -4.1629858 -4.1627831 -4.1909027 -4.2300034 -4.2613354 -4.2875962 -4.2969131 -4.3046341][-4.2105584 -4.1662912 -4.140965 -4.149251 -4.1561093 -4.1564455 -4.1424031 -4.1047125 -4.0902925 -4.1153278 -4.1637554 -4.2107854 -4.2531528 -4.2735953 -4.2901607][-4.2160726 -4.1726856 -4.1466537 -4.1481023 -4.1391788 -4.1207757 -4.0875874 -4.0263395 -3.9991374 -4.0268092 -4.0911016 -4.1599879 -4.2193003 -4.251606 -4.2774935][-4.2072325 -4.1650462 -4.1360564 -4.1256123 -4.1057954 -4.08343 -4.0483732 -3.98409 -3.9549785 -3.9870844 -4.0631876 -4.1465049 -4.2144046 -4.2533784 -4.2798162][-4.1885796 -4.1517453 -4.12431 -4.1067982 -4.0846691 -4.0679159 -4.0458708 -4.0008593 -3.9855702 -4.0239263 -4.0979357 -4.1790862 -4.2445879 -4.279851 -4.2967415][-4.1738877 -4.1433759 -4.1232705 -4.1076894 -4.0903816 -4.0852475 -4.0788512 -4.0563006 -4.05622 -4.0940051 -4.1557183 -4.2247677 -4.280179 -4.3051 -4.3125882][-4.1912761 -4.1651483 -4.1489973 -4.1343322 -4.1239047 -4.1285915 -4.133029 -4.12679 -4.13459 -4.1653662 -4.2104125 -4.263761 -4.30517 -4.318769 -4.3209362][-4.2229133 -4.2032847 -4.1867371 -4.167882 -4.1561089 -4.1621885 -4.1717634 -4.17863 -4.1962571 -4.2222319 -4.2524557 -4.2881227 -4.3142843 -4.3194923 -4.3223472][-4.2370038 -4.2239847 -4.2086568 -4.1881781 -4.1745553 -4.1790447 -4.1870713 -4.2020268 -4.2279143 -4.2524147 -4.269515 -4.2885332 -4.3035355 -4.3069997 -4.3148732][-4.2251921 -4.2168875 -4.2037997 -4.1884079 -4.181509 -4.1840458 -4.1846666 -4.1984124 -4.224956 -4.2486043 -4.2571921 -4.2654481 -4.2746177 -4.28286 -4.3006144]]...]
INFO - root - 2017-12-07 19:10:44.005626: step 44010, loss = 2.06, batch loss = 2.00 (13.4 examples/sec; 0.597 sec/batch; 47h:50m:08s remains)
INFO - root - 2017-12-07 19:10:50.736026: step 44020, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 55h:14m:01s remains)
INFO - root - 2017-12-07 19:10:57.509125: step 44030, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 50h:31m:03s remains)
INFO - root - 2017-12-07 19:11:04.189209: step 44040, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 51h:21m:39s remains)
INFO - root - 2017-12-07 19:11:10.909213: step 44050, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.637 sec/batch; 51h:03m:46s remains)
INFO - root - 2017-12-07 19:11:17.693760: step 44060, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 54h:37m:02s remains)
INFO - root - 2017-12-07 19:11:24.540001: step 44070, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 57h:08m:14s remains)
INFO - root - 2017-12-07 19:11:31.391013: step 44080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 54h:25m:49s remains)
INFO - root - 2017-12-07 19:11:38.090430: step 44090, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 51h:55m:47s remains)
INFO - root - 2017-12-07 19:11:44.994836: step 44100, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 54h:17m:04s remains)
2017-12-07 19:11:45.748458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695322 -4.2298236 -4.2043643 -4.2126794 -4.2319446 -4.2269516 -4.2013741 -4.1710978 -4.138936 -4.1288719 -4.1551471 -4.1777182 -4.1938276 -4.2134914 -4.224576][-4.2508307 -4.2156768 -4.18186 -4.1778955 -4.1908655 -4.1874208 -4.1697426 -4.1475124 -4.1229029 -4.1228466 -4.1580858 -4.1911764 -4.2145953 -4.2316008 -4.2344027][-4.2350974 -4.2054787 -4.1672096 -4.1529746 -4.1588507 -4.1602798 -4.1507578 -4.1320019 -4.1180267 -4.1388245 -4.1888819 -4.2338576 -4.2607064 -4.2666397 -4.2549019][-4.2287931 -4.2101378 -4.1811972 -4.1667304 -4.1673484 -4.1705585 -4.1621032 -4.1380792 -4.1231728 -4.1574864 -4.2207947 -4.272861 -4.2933874 -4.2855306 -4.2634549][-4.2224259 -4.2116489 -4.2018929 -4.1977177 -4.1983957 -4.1915016 -4.1747832 -4.1355691 -4.1086164 -4.1520271 -4.2229557 -4.2748361 -4.2845478 -4.2593942 -4.233851][-4.2272182 -4.2227173 -4.2217112 -4.2185321 -4.2181482 -4.1983876 -4.1537442 -4.0865574 -4.0538583 -4.1045361 -4.174191 -4.2194977 -4.219873 -4.1844578 -4.1616778][-4.2207637 -4.2199392 -4.2232671 -4.2080321 -4.1989746 -4.1609635 -4.0802288 -3.9833076 -3.9584208 -4.0270967 -4.0992336 -4.1397328 -4.1345134 -4.0966191 -4.0781674][-4.1790004 -4.1771207 -4.186132 -4.1616364 -4.135046 -4.0767293 -3.9657717 -3.8494709 -3.8471169 -3.9432294 -4.0239706 -4.0730443 -4.0823984 -4.0581408 -4.0459471][-4.1403461 -4.1300111 -4.1332603 -4.1047497 -4.06211 -3.9903247 -3.8746328 -3.7604558 -3.7733705 -3.8790393 -3.9687998 -4.0310445 -4.05832 -4.056355 -4.0594759][-4.1458583 -4.1249533 -4.1174212 -4.0891271 -4.046772 -3.9933732 -3.9176073 -3.8382735 -3.8499022 -3.9317575 -4.0110626 -4.0700922 -4.097353 -4.1001658 -4.1125255][-4.204514 -4.1772904 -4.1551423 -4.1230826 -4.0900273 -4.0543432 -4.0162067 -3.9747841 -3.9846025 -4.0374436 -4.0950894 -4.1455021 -4.1676168 -4.1667285 -4.178524][-4.2606716 -4.2320514 -4.2084517 -4.1790829 -4.1571641 -4.1368957 -4.1201425 -4.1009922 -4.1088367 -4.1390843 -4.1734171 -4.2116504 -4.2293835 -4.225071 -4.2310929][-4.3045292 -4.2775688 -4.2546539 -4.2327366 -4.2218914 -4.2151079 -4.2093925 -4.2019715 -4.2069764 -4.2227354 -4.2401543 -4.2627587 -4.2726669 -4.2664618 -4.2683768][-4.3127728 -4.2877746 -4.2673507 -4.2565746 -4.2581673 -4.2594728 -4.259059 -4.2559595 -4.2573395 -4.2648931 -4.2724533 -4.2819705 -4.2830758 -4.2767458 -4.2790546][-4.3019714 -4.2843204 -4.2682633 -4.2606831 -4.2633862 -4.2665639 -4.2700472 -4.269918 -4.2699666 -4.2745657 -4.2778015 -4.2792978 -4.2754083 -4.2703133 -4.271462]]...]
INFO - root - 2017-12-07 19:11:52.246977: step 44110, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 0.524 sec/batch; 41h:56m:47s remains)
INFO - root - 2017-12-07 19:11:59.103126: step 44120, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 52h:26m:41s remains)
INFO - root - 2017-12-07 19:12:05.956016: step 44130, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 53h:53m:51s remains)
INFO - root - 2017-12-07 19:12:12.805891: step 44140, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.734 sec/batch; 58h:49m:42s remains)
INFO - root - 2017-12-07 19:12:19.709630: step 44150, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 54h:43m:24s remains)
INFO - root - 2017-12-07 19:12:26.486589: step 44160, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 51h:48m:20s remains)
INFO - root - 2017-12-07 19:12:33.307299: step 44170, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 49h:43m:20s remains)
INFO - root - 2017-12-07 19:12:40.158584: step 44180, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 55h:55m:20s remains)
INFO - root - 2017-12-07 19:12:46.889120: step 44190, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 57h:04m:52s remains)
INFO - root - 2017-12-07 19:12:53.729221: step 44200, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 56h:48m:10s remains)
2017-12-07 19:12:54.485769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.320024 -4.3242188 -4.3273282 -4.3276882 -4.3250303 -4.3211293 -4.3157215 -4.3121686 -4.3093619 -4.30447 -4.2985396 -4.2978988 -4.3030744 -4.3123827 -4.3240242][-4.3064137 -4.3124113 -4.3161097 -4.3145781 -4.3062978 -4.2963505 -4.2868381 -4.2808619 -4.2785411 -4.2784414 -4.27652 -4.2776666 -4.2859755 -4.2980094 -4.3119617][-4.2748785 -4.2839684 -4.2914963 -4.2906361 -4.2749019 -4.2529426 -4.2336888 -4.2237711 -4.2238464 -4.2317643 -4.2397356 -4.2477703 -4.2613029 -4.2792144 -4.2972417][-4.2234216 -4.2341871 -4.2481804 -4.2490158 -4.2231793 -4.1813264 -4.145308 -4.1329789 -4.1469717 -4.175334 -4.2004867 -4.2190595 -4.2377572 -4.2588596 -4.2791328][-4.1675849 -4.1786442 -4.2008858 -4.2047472 -4.1647019 -4.092494 -4.0251503 -4.0013218 -4.0370212 -4.0999317 -4.1524911 -4.1879163 -4.212522 -4.2355824 -4.2584677][-4.1266994 -4.1356235 -4.1619263 -4.1654 -4.1111956 -4.00557 -3.895972 -3.8470335 -3.9034948 -4.009161 -4.0971594 -4.1557131 -4.18928 -4.2146168 -4.2390523][-4.0972543 -4.107275 -4.13776 -4.1433086 -4.0843644 -3.9574826 -3.8145809 -3.7354443 -3.8027856 -3.9392953 -4.0535645 -4.127388 -4.16812 -4.1949134 -4.2215238][-4.0781279 -4.0920057 -4.1256337 -4.1355467 -4.08249 -3.9664912 -3.8336558 -3.7571931 -3.8087964 -3.9340792 -4.043251 -4.1139584 -4.1529403 -4.1797838 -4.2094078][-4.062716 -4.0808544 -4.1180868 -4.1343 -4.1001315 -4.0199857 -3.9263241 -3.8726404 -3.8999796 -3.9822035 -4.0615568 -4.1147218 -4.1462288 -4.173748 -4.2076659][-4.0506191 -4.0686769 -4.1066518 -4.1328282 -4.12669 -4.091743 -4.0436063 -4.013216 -4.0212626 -4.0584836 -4.0997515 -4.1329865 -4.15768 -4.1867695 -4.2218337][-4.0500231 -4.0673103 -4.1065264 -4.1406908 -4.1548204 -4.1526985 -4.1405091 -4.1321154 -4.131424 -4.1400681 -4.1541767 -4.1705403 -4.1887903 -4.2162428 -4.2494712][-4.0847597 -4.0999889 -4.137588 -4.1715851 -4.1928291 -4.2059903 -4.2128625 -4.2185163 -4.2195673 -4.2187424 -4.2205191 -4.2267523 -4.238143 -4.2601619 -4.2867808][-4.168036 -4.1764927 -4.2036929 -4.2289104 -4.2458591 -4.2585759 -4.2690163 -4.27829 -4.2828722 -4.2831836 -4.2821732 -4.282969 -4.2881341 -4.3022037 -4.3192973][-4.2595687 -4.2619972 -4.2742629 -4.2854757 -4.2937865 -4.30134 -4.3093429 -4.3157835 -4.3196926 -4.3215761 -4.3212819 -4.32071 -4.3224797 -4.3299146 -4.3389049][-4.3151016 -4.3148913 -4.3181 -4.32017 -4.3220186 -4.3242545 -4.3279963 -4.3313403 -4.3331146 -4.3339872 -4.3344316 -4.3351641 -4.3374524 -4.3421969 -4.347054]]...]
INFO - root - 2017-12-07 19:13:00.953876: step 44210, loss = 2.05, batch loss = 2.00 (14.7 examples/sec; 0.544 sec/batch; 43h:34m:36s remains)
INFO - root - 2017-12-07 19:13:07.829816: step 44220, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 57h:45m:25s remains)
INFO - root - 2017-12-07 19:13:14.694570: step 44230, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.655 sec/batch; 52h:27m:40s remains)
INFO - root - 2017-12-07 19:13:21.448914: step 44240, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 52h:20m:06s remains)
INFO - root - 2017-12-07 19:13:28.187302: step 44250, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 56h:54m:18s remains)
INFO - root - 2017-12-07 19:13:34.980184: step 44260, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 54h:13m:05s remains)
INFO - root - 2017-12-07 19:13:41.573270: step 44270, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 53h:54m:48s remains)
INFO - root - 2017-12-07 19:13:48.327552: step 44280, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 51h:57m:39s remains)
INFO - root - 2017-12-07 19:13:55.175642: step 44290, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 54h:52m:15s remains)
INFO - root - 2017-12-07 19:14:02.064353: step 44300, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 58h:56m:05s remains)
2017-12-07 19:14:02.886061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3421192 -4.3405995 -4.3330479 -4.3243475 -4.3194714 -4.3191905 -4.321301 -4.3254223 -4.3298292 -4.3339372 -4.3376236 -4.3392887 -4.3391404 -4.3370152 -4.3335166][-4.3351126 -4.328557 -4.3153114 -4.3021197 -4.2959533 -4.2950668 -4.297492 -4.3044667 -4.3136783 -4.3226528 -4.3288236 -4.330574 -4.3284793 -4.3225193 -4.315599][-4.3074646 -4.2939367 -4.2752662 -4.2582259 -4.2505345 -4.247653 -4.2508616 -4.2620692 -4.2793775 -4.2944279 -4.3022 -4.3027124 -4.2964749 -4.28716 -4.2807112][-4.2552052 -4.2333961 -4.2079339 -4.1851764 -4.1733284 -4.1675682 -4.1721182 -4.1908364 -4.2181482 -4.2389097 -4.2472916 -4.2459626 -4.2360096 -4.2289186 -4.2302222][-4.2010918 -4.1716638 -4.1384859 -4.1066771 -4.0854311 -4.0731874 -4.0788651 -4.1047997 -4.1396213 -4.1660118 -4.1778 -4.1778245 -4.166677 -4.1642089 -4.178328][-4.1530967 -4.1236973 -4.0916057 -4.0576353 -4.03006 -4.0111856 -4.0136766 -4.0415645 -4.0771184 -4.1039009 -4.1190481 -4.1238723 -4.1163168 -4.1194658 -4.14418][-4.102344 -4.0791693 -4.0581141 -4.037643 -4.016233 -3.9974897 -3.9957135 -4.0178633 -4.046526 -4.0675097 -4.0837545 -4.0922956 -4.0920315 -4.0999637 -4.1295609][-4.0671453 -4.0554662 -4.0514274 -4.0483351 -4.0360885 -4.0206919 -4.014194 -4.0277843 -4.0474458 -4.0643883 -4.0824904 -4.0949612 -4.1002502 -4.1077733 -4.1351771][-4.0715203 -4.0707231 -4.0779886 -4.0841551 -4.0781054 -4.0662322 -4.0606093 -4.0711532 -4.0864716 -4.1033068 -4.1208491 -4.1333737 -4.1392879 -4.1405244 -4.1587281][-4.105751 -4.1106486 -4.1234026 -4.1333838 -4.12973 -4.1198816 -4.1169429 -4.1270227 -4.1413569 -4.1594815 -4.1754942 -4.1855936 -4.1884785 -4.181787 -4.1888623][-4.1477022 -4.1560917 -4.1704855 -4.17972 -4.1771345 -4.1701202 -4.1686454 -4.1757956 -4.1856627 -4.2009959 -4.213975 -4.2204046 -4.2232575 -4.2128477 -4.2120361][-4.1878276 -4.1972175 -4.2106051 -4.2173548 -4.2153335 -4.2120881 -4.2116714 -4.2130728 -4.2136965 -4.220695 -4.2268653 -4.229423 -4.2331181 -4.2237096 -4.2198434][-4.2216449 -4.2316589 -4.2427 -4.2454033 -4.242137 -4.2402945 -4.2394042 -4.2356739 -4.22887 -4.22941 -4.2288179 -4.2271276 -4.2307072 -4.2228131 -4.2185397][-4.2473621 -4.2577477 -4.2649541 -4.2628078 -4.2562094 -4.250885 -4.246685 -4.241293 -4.2353206 -4.2345209 -4.2295971 -4.2244592 -4.2258148 -4.2178116 -4.2129831][-4.2558203 -4.265666 -4.2692394 -4.2619257 -4.2515321 -4.2441363 -4.2409496 -4.2406058 -4.2404017 -4.2390246 -4.2291965 -4.219533 -4.2169561 -4.2077341 -4.2042627]]...]
INFO - root - 2017-12-07 19:14:09.561565: step 44310, loss = 2.04, batch loss = 1.99 (13.8 examples/sec; 0.581 sec/batch; 46h:28m:30s remains)
INFO - root - 2017-12-07 19:14:16.388913: step 44320, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.714 sec/batch; 57h:08m:08s remains)
INFO - root - 2017-12-07 19:14:23.154629: step 44330, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 55h:56m:32s remains)
INFO - root - 2017-12-07 19:14:29.992877: step 44340, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 52h:07m:38s remains)
INFO - root - 2017-12-07 19:14:36.738023: step 44350, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 54h:00m:39s remains)
INFO - root - 2017-12-07 19:14:43.537236: step 44360, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 56h:12m:48s remains)
INFO - root - 2017-12-07 19:14:50.241611: step 44370, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 53h:58m:08s remains)
INFO - root - 2017-12-07 19:14:56.974807: step 44380, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 52h:35m:24s remains)
INFO - root - 2017-12-07 19:15:03.650652: step 44390, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 52h:32m:23s remains)
INFO - root - 2017-12-07 19:15:10.341018: step 44400, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 51h:32m:39s remains)
2017-12-07 19:15:11.109513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3134918 -4.31024 -4.3136744 -4.3283544 -4.3438683 -4.3532906 -4.3583441 -4.3578949 -4.3542609 -4.3503685 -4.346045 -4.3396425 -4.3303933 -4.3202338 -4.3181486][-4.2766881 -4.2719555 -4.2762094 -4.2969408 -4.3182678 -4.3294058 -4.3324771 -4.3287268 -4.3211164 -4.3158517 -4.3132162 -4.3076763 -4.2952809 -4.2810006 -4.27918][-4.2283759 -4.22592 -4.2331438 -4.2582417 -4.27992 -4.2879667 -4.2847052 -4.2744927 -4.2631688 -4.2596526 -4.2649035 -4.2632155 -4.24886 -4.2316723 -4.2301078][-4.2015305 -4.2021585 -4.2125044 -4.2353721 -4.2463107 -4.2426338 -4.230392 -4.2161903 -4.206162 -4.2102132 -4.2294974 -4.2386484 -4.2271528 -4.2085996 -4.2051978][-4.2034464 -4.2044234 -4.2126532 -4.2207913 -4.2063046 -4.1797652 -4.1554885 -4.136342 -4.1334758 -4.1524358 -4.1917233 -4.2188258 -4.2185073 -4.2081904 -4.2040682][-4.2096319 -4.2085981 -4.208528 -4.1953483 -4.1491141 -4.09137 -4.0476322 -4.0233126 -4.0347886 -4.0779858 -4.1404781 -4.1872625 -4.2043681 -4.2059989 -4.2062464][-4.212266 -4.2038569 -4.1883292 -4.147469 -4.0666413 -3.9744132 -3.9027286 -3.8647966 -3.8882434 -3.9689937 -4.0750136 -4.1527867 -4.1897688 -4.197041 -4.2006636][-4.2085705 -4.1879091 -4.1555991 -4.0912342 -3.9858994 -3.865962 -3.7713149 -3.7231364 -3.7597404 -3.8794208 -4.0267234 -4.1302667 -4.1777396 -4.1878486 -4.1940937][-4.2046957 -4.1796041 -4.1441836 -4.0845366 -3.9987402 -3.9048417 -3.8428128 -3.8241138 -3.8644471 -3.9560506 -4.0753326 -4.1589255 -4.1960244 -4.20495 -4.211946][-4.1905537 -4.1652756 -4.1382484 -4.0990839 -4.0518379 -4.0053077 -3.9797564 -3.975502 -4.0038209 -4.0588045 -4.1342292 -4.1870527 -4.2130842 -4.2266684 -4.2368069][-4.1672759 -4.1390686 -4.11496 -4.0907016 -4.0650992 -4.0419407 -4.0306931 -4.0292311 -4.0465956 -4.0831089 -4.1393881 -4.1790133 -4.20246 -4.2238088 -4.2417874][-4.150279 -4.117785 -4.0911078 -4.075726 -4.0639906 -4.0467744 -4.0339913 -4.0276332 -4.0325665 -4.0628414 -4.1216612 -4.1639 -4.1902437 -4.2145686 -4.2406883][-4.1560097 -4.1268587 -4.101244 -4.0894389 -4.0805526 -4.0612092 -4.0397077 -4.0237789 -4.0201907 -4.047493 -4.1071482 -4.1540427 -4.1830778 -4.2081513 -4.2376051][-4.1787786 -4.1647024 -4.152154 -4.148984 -4.1458974 -4.131073 -4.1088285 -4.0868464 -4.0743647 -4.0885973 -4.1327996 -4.1701951 -4.1924553 -4.2134714 -4.2407351][-4.2047668 -4.2036157 -4.2041297 -4.2096753 -4.211699 -4.2024837 -4.185935 -4.1659942 -4.1497831 -4.1506705 -4.1730008 -4.1952763 -4.20922 -4.2238274 -4.2434626]]...]
INFO - root - 2017-12-07 19:15:17.738430: step 44410, loss = 2.06, batch loss = 2.00 (13.3 examples/sec; 0.601 sec/batch; 48h:06m:35s remains)
INFO - root - 2017-12-07 19:15:24.434769: step 44420, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 51h:55m:41s remains)
INFO - root - 2017-12-07 19:15:31.086548: step 44430, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 50h:29m:50s remains)
INFO - root - 2017-12-07 19:15:37.916084: step 44440, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 51h:57m:05s remains)
INFO - root - 2017-12-07 19:15:44.659465: step 44450, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 56h:12m:10s remains)
INFO - root - 2017-12-07 19:15:51.493864: step 44460, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 55h:21m:34s remains)
INFO - root - 2017-12-07 19:15:58.253913: step 44470, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 56h:52m:32s remains)
INFO - root - 2017-12-07 19:16:04.996070: step 44480, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.675 sec/batch; 54h:01m:15s remains)
INFO - root - 2017-12-07 19:16:11.819891: step 44490, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 52h:46m:30s remains)
INFO - root - 2017-12-07 19:16:18.588580: step 44500, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 54h:52m:34s remains)
2017-12-07 19:16:19.266912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2987614 -4.2908187 -4.2814121 -4.2714853 -4.257731 -4.247324 -4.2435436 -4.2484193 -4.2566781 -4.2596307 -4.2419682 -4.209331 -4.1860723 -4.1763635 -4.1725554][-4.2781076 -4.2636466 -4.2432194 -4.2224054 -4.1998911 -4.18497 -4.1795306 -4.1896763 -4.2119818 -4.2255988 -4.2133422 -4.1857104 -4.1671243 -4.1636996 -4.1596532][-4.2626982 -4.2410502 -4.208498 -4.1745563 -4.1420622 -4.1241474 -4.1153326 -4.1331797 -4.1758595 -4.2099943 -4.2103119 -4.1900868 -4.1747422 -4.1714606 -4.1637535][-4.2615218 -4.2362304 -4.1933875 -4.1471024 -4.10213 -4.0744219 -4.0554137 -4.0775304 -4.1436372 -4.2059336 -4.2280855 -4.2186131 -4.2020636 -4.1940479 -4.1775541][-4.2709413 -4.2511153 -4.2037258 -4.1438885 -4.0761824 -4.0207286 -3.979013 -4.0061903 -4.1038895 -4.1999931 -4.2489595 -4.2531939 -4.2350354 -4.2185206 -4.1927872][-4.276659 -4.2655792 -4.2189689 -4.1429262 -4.0411105 -3.9337482 -3.849545 -3.8851991 -4.0264878 -4.1638131 -4.2429543 -4.2653546 -4.2532535 -4.2313519 -4.19852][-4.2739735 -4.26892 -4.2231283 -4.1289864 -3.987947 -3.8165295 -3.6807113 -3.7328863 -3.9310853 -4.1116896 -4.2193737 -4.2630587 -4.2624636 -4.2341609 -4.1921406][-4.2691727 -4.2685666 -4.2236228 -4.1185541 -3.9523869 -3.7326596 -3.5482469 -3.6150267 -3.863096 -4.0740495 -4.2000852 -4.2595315 -4.269031 -4.2350612 -4.1816087][-4.2653985 -4.2729878 -4.2388945 -4.14411 -3.9897153 -3.7740078 -3.5868447 -3.6427498 -3.8828447 -4.0873561 -4.2056932 -4.262639 -4.2722931 -4.2306919 -4.166697][-4.2583241 -4.2774696 -4.2624021 -4.1974616 -4.0845356 -3.9198141 -3.7793951 -3.8134441 -3.9888535 -4.1468043 -4.2341723 -4.271636 -4.2722564 -4.2234607 -4.1537471][-4.2398148 -4.2673655 -4.2697539 -4.2326984 -4.1564374 -4.0416112 -3.9498508 -3.9700541 -4.08335 -4.1925621 -4.250483 -4.2692389 -4.2594123 -4.2064118 -4.1397557][-4.224431 -4.2540755 -4.2672687 -4.251153 -4.2013726 -4.1219907 -4.0607619 -4.0724692 -4.1477036 -4.225563 -4.2652969 -4.2717361 -4.2518926 -4.199718 -4.1435637][-4.2309837 -4.2583823 -4.2744493 -4.269556 -4.2388167 -4.1849322 -4.1424284 -4.150197 -4.2031245 -4.2590823 -4.2872548 -4.2879043 -4.2639084 -4.2174416 -4.1756229][-4.259995 -4.2819877 -4.2946157 -4.2949581 -4.2775736 -4.2430487 -4.213376 -4.217514 -4.2523355 -4.2906504 -4.3083749 -4.3071051 -4.2864041 -4.2536116 -4.2276573][-4.294776 -4.3097677 -4.3182569 -4.3218188 -4.3166003 -4.2990932 -4.2794719 -4.2785935 -4.2990437 -4.3221393 -4.3298664 -4.3251805 -4.3099642 -4.2911305 -4.2771616]]...]
INFO - root - 2017-12-07 19:16:25.868663: step 44510, loss = 2.05, batch loss = 1.99 (16.3 examples/sec; 0.492 sec/batch; 39h:22m:05s remains)
INFO - root - 2017-12-07 19:16:32.693811: step 44520, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 53h:27m:22s remains)
INFO - root - 2017-12-07 19:16:39.443315: step 44530, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 56h:00m:40s remains)
INFO - root - 2017-12-07 19:16:46.186570: step 44540, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 57h:38m:04s remains)
INFO - root - 2017-12-07 19:16:52.968352: step 44550, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 55h:02m:54s remains)
INFO - root - 2017-12-07 19:16:59.724837: step 44560, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 53h:52m:46s remains)
INFO - root - 2017-12-07 19:17:06.543649: step 44570, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 57h:46m:24s remains)
INFO - root - 2017-12-07 19:17:13.054389: step 44580, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 50h:53m:50s remains)
INFO - root - 2017-12-07 19:17:19.842874: step 44590, loss = 2.04, batch loss = 1.99 (13.0 examples/sec; 0.614 sec/batch; 49h:06m:29s remains)
INFO - root - 2017-12-07 19:17:26.644224: step 44600, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 52h:51m:29s remains)
2017-12-07 19:17:27.324933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2938952 -4.2909079 -4.2879605 -4.2850714 -4.2856021 -4.2907681 -4.29727 -4.299346 -4.2973914 -4.2963858 -4.297596 -4.3010011 -4.3083105 -4.3170218 -4.3242702][-4.2708483 -4.2632318 -4.256568 -4.252295 -4.2535658 -4.2601085 -4.2675586 -4.2695866 -4.267005 -4.2660475 -4.2673807 -4.2726045 -4.2844057 -4.297224 -4.307838][-4.2442861 -4.226603 -4.2142487 -4.2101512 -4.2138028 -4.2196412 -4.2239294 -4.2258277 -4.2252564 -4.2259274 -4.2272062 -4.2347412 -4.25243 -4.2704568 -4.285573][-4.2156057 -4.1833262 -4.1652093 -4.1632075 -4.1692543 -4.1691794 -4.1628118 -4.1626062 -4.1681409 -4.1750107 -4.1778388 -4.1885576 -4.2115149 -4.2346096 -4.2561769][-4.1829333 -4.1413708 -4.1203747 -4.1181602 -4.1201262 -4.1081672 -4.0843368 -4.0806208 -4.09671 -4.1148167 -4.1216097 -4.1358137 -4.1651392 -4.1958861 -4.2260742][-4.1568513 -4.1157074 -4.0955477 -4.0896225 -4.0783439 -4.044035 -3.9977589 -3.992409 -4.0254221 -4.05976 -4.0743556 -4.0905337 -4.122962 -4.1613212 -4.2013664][-4.1464992 -4.1123562 -4.0956516 -4.0841155 -4.0548844 -3.9924803 -3.9232981 -3.9204216 -3.9740739 -4.02603 -4.0482068 -4.0614228 -4.08793 -4.1269212 -4.1727176][-4.1456833 -4.1147 -4.1005554 -4.092669 -4.0600939 -3.9847274 -3.905941 -3.9111817 -3.9753952 -4.0261636 -4.0407324 -4.0416021 -4.0578756 -4.0926905 -4.13955][-4.1521826 -4.1210227 -4.1063795 -4.1058311 -4.0868044 -4.0257111 -3.96461 -3.9750681 -4.0245667 -4.0527363 -4.0472302 -4.0323863 -4.0390873 -4.0658007 -4.107595][-4.1641088 -4.1341171 -4.1174068 -4.1194892 -4.1125512 -4.0756297 -4.0427737 -4.0534854 -4.077004 -4.0784669 -4.0560093 -4.0344691 -4.0345306 -4.0489292 -4.0790653][-4.1821561 -4.1553955 -4.1398649 -4.1394386 -4.1365757 -4.1171255 -4.1052518 -4.1132736 -4.1183815 -4.1086144 -4.0833645 -4.0654535 -4.0642447 -4.0658522 -4.080256][-4.2231984 -4.1986713 -4.1859221 -4.1843977 -4.1818314 -4.1731868 -4.1710749 -4.1761961 -4.1750288 -4.16547 -4.1500511 -4.1418614 -4.1417565 -4.1349335 -4.1351352][-4.2716131 -4.2537909 -4.2444725 -4.2409458 -4.2380629 -4.2360916 -4.2378454 -4.2423992 -4.2420487 -4.236834 -4.2311907 -4.2299 -4.2313094 -4.2226315 -4.2160244][-4.3094893 -4.3010416 -4.2959824 -4.293633 -4.2916508 -4.2925415 -4.2946687 -4.2969747 -4.2972269 -4.2955823 -4.296793 -4.3006043 -4.3031039 -4.2961016 -4.2878346][-4.3298273 -4.3275948 -4.3256736 -4.3246241 -4.324194 -4.3258023 -4.3278322 -4.3287897 -4.32837 -4.3283048 -4.3308787 -4.3350096 -4.3368564 -4.3327718 -4.32728]]...]
INFO - root - 2017-12-07 19:17:34.078343: step 44610, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 53h:17m:13s remains)
INFO - root - 2017-12-07 19:17:40.785357: step 44620, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 51h:55m:50s remains)
INFO - root - 2017-12-07 19:17:47.417308: step 44630, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 50h:38m:32s remains)
INFO - root - 2017-12-07 19:17:54.047649: step 44640, loss = 2.09, batch loss = 2.04 (11.8 examples/sec; 0.676 sec/batch; 54h:04m:22s remains)
INFO - root - 2017-12-07 19:18:00.749245: step 44650, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 55h:17m:29s remains)
INFO - root - 2017-12-07 19:18:07.496320: step 44660, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 57h:48m:28s remains)
INFO - root - 2017-12-07 19:18:14.241949: step 44670, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 54h:31m:04s remains)
INFO - root - 2017-12-07 19:18:20.940905: step 44680, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 52h:10m:03s remains)
INFO - root - 2017-12-07 19:18:27.733707: step 44690, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 52h:42m:02s remains)
INFO - root - 2017-12-07 19:18:34.482445: step 44700, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 57h:15m:23s remains)
2017-12-07 19:18:35.139568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2744942 -4.2767763 -4.2797003 -4.2826161 -4.2817564 -4.276422 -4.2680874 -4.2565312 -4.2479596 -4.2481141 -4.2516561 -4.2547693 -4.2591066 -4.2689595 -4.2818151][-4.2495055 -4.2543383 -4.2568488 -4.2576613 -4.2539034 -4.2446113 -4.2259636 -4.2041492 -4.193367 -4.2013755 -4.2122331 -4.2153563 -4.2195649 -4.2345858 -4.2542791][-4.2331681 -4.2392774 -4.2400723 -4.235949 -4.2264285 -4.2086468 -4.1761193 -4.1451211 -4.1380644 -4.1602464 -4.1786652 -4.1803761 -4.1832614 -4.2009883 -4.2281685][-4.2158604 -4.22084 -4.2202992 -4.2128911 -4.1979527 -4.1693883 -4.1280241 -4.0980625 -4.1021852 -4.1330547 -4.1511908 -4.1510596 -4.1538019 -4.1743264 -4.2047586][-4.188694 -4.1923704 -4.191071 -4.18118 -4.1609807 -4.126524 -4.0847864 -4.0664873 -4.0861974 -4.1179285 -4.1280661 -4.1265688 -4.1348968 -4.1578341 -4.1875958][-4.1511216 -4.1535411 -4.1485085 -4.1365829 -4.1137586 -4.0707192 -4.0256228 -4.0172777 -4.0550575 -4.0882754 -4.0883818 -4.0864582 -4.1032505 -4.1322241 -4.1641994][-4.1184421 -4.1161952 -4.1056447 -4.0926061 -4.071661 -4.0156665 -3.9576871 -3.9535832 -4.0037937 -4.0387611 -4.0351868 -4.0332079 -4.0577788 -4.0956454 -4.1342721][-4.1021857 -4.092802 -4.0827036 -4.0729923 -4.0581665 -3.9946437 -3.9221089 -3.9114079 -3.9576104 -3.9855442 -3.980372 -3.9834833 -4.0165081 -4.0636687 -4.1097622][-4.1041479 -4.0947366 -4.0937881 -4.1006708 -4.097847 -4.046133 -3.9761927 -3.9542966 -3.9795947 -3.988847 -3.9766631 -3.9823112 -4.0166368 -4.0566983 -4.0973244][-4.116528 -4.1108427 -4.1210809 -4.1479034 -4.1641679 -4.1388407 -4.0941482 -4.0746007 -4.07934 -4.0673661 -4.0465655 -4.0502348 -4.0738144 -4.0934982 -4.1151552][-4.1290131 -4.133132 -4.1537313 -4.1868644 -4.2106 -4.2018 -4.1825891 -4.1749206 -4.1699481 -4.1459613 -4.1179962 -4.116065 -4.1337361 -4.1413 -4.1477704][-4.1359706 -4.1500192 -4.1805811 -4.214396 -4.2321339 -4.2255969 -4.2180147 -4.2184072 -4.2151217 -4.1901731 -4.158206 -4.1513486 -4.1703277 -4.1791811 -4.18645][-4.1521049 -4.1697354 -4.2012219 -4.2327266 -4.2455349 -4.23842 -4.2329082 -4.236516 -4.2368207 -4.2154016 -4.1805453 -4.1720033 -4.1949463 -4.2117457 -4.227139][-4.1841817 -4.2029862 -4.2307754 -4.2549267 -4.26505 -4.2587595 -4.2515521 -4.2544093 -4.2597446 -4.2480078 -4.2166181 -4.2044878 -4.2244682 -4.2434492 -4.2616949][-4.2243328 -4.2442079 -4.2659922 -4.2803216 -4.2871633 -4.2804089 -4.2718687 -4.2726884 -4.2792497 -4.2761955 -4.2546892 -4.2452407 -4.2585297 -4.2733154 -4.2876062]]...]
INFO - root - 2017-12-07 19:18:41.700896: step 44710, loss = 2.09, batch loss = 2.03 (16.4 examples/sec; 0.488 sec/batch; 38h:59m:33s remains)
INFO - root - 2017-12-07 19:18:48.505468: step 44720, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 53h:14m:22s remains)
INFO - root - 2017-12-07 19:18:55.349112: step 44730, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 54h:33m:37s remains)
INFO - root - 2017-12-07 19:19:02.108675: step 44740, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 53h:31m:36s remains)
INFO - root - 2017-12-07 19:19:08.872419: step 44750, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 58h:08m:12s remains)
INFO - root - 2017-12-07 19:19:15.627271: step 44760, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 52h:13m:20s remains)
INFO - root - 2017-12-07 19:19:22.450049: step 44770, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 51h:51m:04s remains)
INFO - root - 2017-12-07 19:19:29.324709: step 44780, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 58h:02m:22s remains)
INFO - root - 2017-12-07 19:19:36.199240: step 44790, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 58h:09m:38s remains)
INFO - root - 2017-12-07 19:19:43.023878: step 44800, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 51h:04m:50s remains)
2017-12-07 19:19:43.765762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1602559 -4.1306877 -4.1346889 -4.1667209 -4.1986861 -4.2176104 -4.2288518 -4.2353921 -4.234056 -4.2255545 -4.2196484 -4.2199335 -4.2294283 -4.2351103 -4.23519][-4.0938129 -4.0873952 -4.1216207 -4.1750584 -4.2121091 -4.2308512 -4.2343297 -4.2292871 -4.2141895 -4.2012129 -4.20055 -4.2086334 -4.2211504 -4.2308111 -4.2284927][-4.0290833 -4.0590286 -4.1240635 -4.1870904 -4.2192984 -4.2300196 -4.2169271 -4.1953712 -4.1734381 -4.1655517 -4.17846 -4.1994662 -4.2205977 -4.2343068 -4.2291827][-4.0078311 -4.0525517 -4.1213417 -4.1743746 -4.1970596 -4.1990204 -4.1766524 -4.1466751 -4.1266942 -4.1357942 -4.1701317 -4.2066703 -4.2353125 -4.248899 -4.2400684][-4.0510235 -4.0831332 -4.1239147 -4.1472883 -4.1550732 -4.1490679 -4.1198955 -4.0845256 -4.0777922 -4.1119943 -4.167572 -4.2161074 -4.2488556 -4.264473 -4.257421][-4.1026349 -4.1094766 -4.1161575 -4.1084795 -4.1027575 -4.0930028 -4.0665407 -4.0387831 -4.0537786 -4.1125903 -4.1786275 -4.2272515 -4.2546811 -4.2679505 -4.2632322][-4.140305 -4.1291547 -4.1124072 -4.0887213 -4.0731416 -4.065546 -4.050209 -4.0357032 -4.0620108 -4.1266394 -4.1891971 -4.2273149 -4.246552 -4.2620535 -4.2580338][-4.1739178 -4.1553392 -4.1254168 -4.088326 -4.0630307 -4.0550976 -4.0404787 -4.0266809 -4.0507579 -4.1071672 -4.1638579 -4.1987362 -4.2210193 -4.2413406 -4.2382526][-4.1971264 -4.1744618 -4.1400795 -4.09694 -4.0634589 -4.0478544 -4.0196872 -3.9919314 -4.0128241 -4.0672383 -4.1214452 -4.1613989 -4.1905184 -4.2079091 -4.198668][-4.2110982 -4.187706 -4.1587648 -4.1254144 -4.0959091 -4.0742779 -4.0323625 -3.9946594 -4.0079737 -4.0516615 -4.0961618 -4.1306367 -4.1549363 -4.1658578 -4.1516948][-4.2044158 -4.1839252 -4.168447 -4.1528072 -4.132524 -4.1086774 -4.0642214 -4.0276184 -4.0341392 -4.0659456 -4.1000681 -4.1266103 -4.1452465 -4.1530471 -4.1365738][-4.1886725 -4.1709261 -4.1618915 -4.1527424 -4.1358638 -4.1141067 -4.0795646 -4.0592513 -4.0725536 -4.105557 -4.1372619 -4.1591158 -4.1726074 -4.1726279 -4.1487269][-4.1650376 -4.1436834 -4.1296763 -4.1185193 -4.1082954 -4.1001391 -4.0863495 -4.0855451 -4.1075439 -4.1451211 -4.176415 -4.19446 -4.199636 -4.19082 -4.166285][-4.1138983 -4.094099 -4.0834594 -4.0813217 -4.0838 -4.0895433 -4.0946198 -4.1100082 -4.1401367 -4.1797514 -4.2114925 -4.2283621 -4.2272081 -4.2163467 -4.199399][-4.0434389 -4.0380406 -4.0428166 -4.0586753 -4.078414 -4.0986772 -4.1182971 -4.1430826 -4.1740484 -4.2085309 -4.2374334 -4.2519317 -4.2507763 -4.2446613 -4.2386408]]...]
INFO - root - 2017-12-07 19:19:50.417739: step 44810, loss = 2.04, batch loss = 1.98 (14.5 examples/sec; 0.551 sec/batch; 44h:02m:34s remains)
INFO - root - 2017-12-07 19:19:57.265619: step 44820, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 55h:29m:57s remains)
INFO - root - 2017-12-07 19:20:04.023825: step 44830, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 50h:10m:19s remains)
INFO - root - 2017-12-07 19:20:10.919847: step 44840, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 54h:25m:40s remains)
INFO - root - 2017-12-07 19:20:17.633150: step 44850, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 54h:43m:51s remains)
INFO - root - 2017-12-07 19:20:24.389374: step 44860, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 56h:20m:08s remains)
INFO - root - 2017-12-07 19:20:31.140954: step 44870, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 55h:42m:55s remains)
INFO - root - 2017-12-07 19:20:37.798137: step 44880, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 50h:58m:30s remains)
INFO - root - 2017-12-07 19:20:44.456134: step 44890, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 50h:55m:51s remains)
INFO - root - 2017-12-07 19:20:51.245722: step 44900, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 54h:19m:53s remains)
2017-12-07 19:20:51.951297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2933931 -4.2796164 -4.2643752 -4.2532663 -4.2500386 -4.2459412 -4.2362814 -4.2290978 -4.2317233 -4.2388539 -4.2484064 -4.2603216 -4.2679391 -4.2698283 -4.2730818][-4.2726946 -4.2572856 -4.2394528 -4.2310348 -4.2332277 -4.2271409 -4.2113719 -4.2004356 -4.2032919 -4.2116122 -4.2235947 -4.2383947 -4.2482977 -4.2503705 -4.2538142][-4.2393117 -4.2243853 -4.2070513 -4.2019658 -4.2068777 -4.1993909 -4.1776614 -4.1630344 -4.1712151 -4.1839557 -4.1972885 -4.2132697 -4.2234225 -4.2238178 -4.2265248][-4.2081804 -4.1963453 -4.178299 -4.1736 -4.1797862 -4.1697159 -4.1387691 -4.1220727 -4.1426229 -4.1689959 -4.1874824 -4.2060971 -4.2169728 -4.2155461 -4.2165842][-4.1840882 -4.1801805 -4.16351 -4.1539664 -4.1510468 -4.1213365 -4.063427 -4.0378003 -4.0781097 -4.1302004 -4.1625185 -4.1915531 -4.2095771 -4.21361 -4.2169738][-4.1538267 -4.1604 -4.1540971 -4.1381345 -4.111455 -4.0413637 -3.9341521 -3.8944602 -3.9680004 -4.0532422 -4.1078982 -4.1575112 -4.1921964 -4.2092142 -4.2182841][-4.1227665 -4.1348071 -4.1376424 -4.1149459 -4.0559082 -3.9270911 -3.7541046 -3.7164433 -3.8466501 -3.9720969 -4.0517282 -4.1217937 -4.1729364 -4.2003336 -4.2136869][-4.1200213 -4.1363263 -4.1381469 -4.1027589 -4.0189853 -3.8448596 -3.6253414 -3.6114407 -3.7953274 -3.9473879 -4.035388 -4.1082311 -4.1664419 -4.1941895 -4.2054982][-4.1449208 -4.1665144 -4.1670375 -4.1279058 -4.0485282 -3.9000947 -3.7316051 -3.7334912 -3.8864288 -4.0126348 -4.0811419 -4.1343293 -4.186193 -4.2069707 -4.2080693][-4.1775 -4.198565 -4.2030597 -4.17501 -4.12471 -4.0363283 -3.9510906 -3.9562376 -4.0398254 -4.1138954 -4.1585259 -4.1903434 -4.2229304 -4.2323337 -4.2237706][-4.2174697 -4.2336073 -4.2389297 -4.2220507 -4.1955972 -4.1491513 -4.1122651 -4.1169472 -4.1546526 -4.1939845 -4.2240033 -4.2468004 -4.2618966 -4.2621036 -4.2456126][-4.253685 -4.2617903 -4.2588849 -4.24276 -4.2252212 -4.1999021 -4.1786141 -4.1821637 -4.205761 -4.2343454 -4.2620282 -4.2801189 -4.2876468 -4.2837114 -4.2651696][-4.2847762 -4.2820139 -4.2704864 -4.2460284 -4.2252989 -4.2019429 -4.1849284 -4.1876078 -4.2123361 -4.2446566 -4.2738895 -4.2905149 -4.294971 -4.2913618 -4.278367][-4.2924676 -4.2814074 -4.2680464 -4.2462735 -4.2222338 -4.197145 -4.1804333 -4.1809454 -4.2093611 -4.2425261 -4.2701554 -4.2864885 -4.2914395 -4.2909193 -4.2861509][-4.27894 -4.2683134 -4.2609973 -4.2505674 -4.2318592 -4.2065115 -4.1875248 -4.1850643 -4.2117548 -4.2429719 -4.2660923 -4.2813611 -4.2877545 -4.2893219 -4.2899489]]...]
INFO - root - 2017-12-07 19:20:58.399672: step 44910, loss = 2.09, batch loss = 2.03 (18.7 examples/sec; 0.427 sec/batch; 34h:07m:40s remains)
INFO - root - 2017-12-07 19:21:05.369626: step 44920, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 57h:15m:46s remains)
INFO - root - 2017-12-07 19:21:12.242415: step 44930, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 55h:44m:39s remains)
INFO - root - 2017-12-07 19:21:19.084665: step 44940, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 54h:16m:43s remains)
INFO - root - 2017-12-07 19:21:25.887981: step 44950, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 50h:32m:02s remains)
INFO - root - 2017-12-07 19:21:32.617335: step 44960, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 53h:40m:03s remains)
INFO - root - 2017-12-07 19:21:39.391920: step 44970, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 56h:49m:10s remains)
INFO - root - 2017-12-07 19:21:46.130088: step 44980, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 57h:18m:22s remains)
INFO - root - 2017-12-07 19:21:52.912777: step 44990, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 54h:34m:14s remains)
INFO - root - 2017-12-07 19:21:59.616101: step 45000, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 51h:27m:24s remains)
2017-12-07 19:22:00.331508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1238327 -4.1296287 -4.1446166 -4.1519456 -4.1494484 -4.149981 -4.1471457 -4.1305952 -4.1098733 -4.10183 -4.1048918 -4.1047473 -4.1068678 -4.11303 -4.1212797][-4.1186466 -4.1206794 -4.1391907 -4.1531553 -4.15309 -4.1530514 -4.1485572 -4.1308017 -4.1120863 -4.106564 -4.1035848 -4.0908651 -4.0819049 -4.0845828 -4.0958471][-4.1256261 -4.1275086 -4.1490293 -4.167027 -4.1650791 -4.1562347 -4.1437631 -4.1242051 -4.109189 -4.1096616 -4.1113796 -4.0995855 -4.0851235 -4.0768938 -4.0807457][-4.1313424 -4.1342125 -4.1577725 -4.1829915 -4.1814227 -4.164505 -4.1458917 -4.1256251 -4.1108165 -4.1137505 -4.1220589 -4.119205 -4.1080251 -4.0958056 -4.0893116][-4.1407948 -4.1417122 -4.1611924 -4.1867061 -4.1829185 -4.1550827 -4.129303 -4.1077461 -4.092423 -4.0966597 -4.110199 -4.1160007 -4.1155081 -4.1122632 -4.1046257][-4.1470938 -4.1439052 -4.1568136 -4.1762791 -4.1695418 -4.1377382 -4.1112127 -4.0927525 -4.0797334 -4.082828 -4.0937853 -4.1022596 -4.1108747 -4.1202488 -4.1211329][-4.1383948 -4.13405 -4.1401134 -4.1484866 -4.1361327 -4.1042089 -4.0832353 -4.0773277 -4.0752206 -4.0804825 -4.0877681 -4.0969963 -4.109817 -4.1268539 -4.1362581][-4.1142926 -4.1121931 -4.1164422 -4.118957 -4.1015348 -4.065805 -4.0456672 -4.0523572 -4.06503 -4.0783224 -4.0929546 -4.1102252 -4.1284494 -4.1484404 -4.1610069][-4.1021414 -4.1036077 -4.106554 -4.110034 -4.0976334 -4.064436 -4.0415392 -4.0484934 -4.0651054 -4.0854459 -4.1137033 -4.1432505 -4.1612 -4.1751032 -4.1855021][-4.1171255 -4.1178789 -4.11662 -4.1193466 -4.1187124 -4.1044507 -4.0891914 -4.0889082 -4.0944376 -4.11017 -4.1410871 -4.1719842 -4.1837792 -4.1867266 -4.188623][-4.12788 -4.1242518 -4.1211543 -4.1248412 -4.137373 -4.1424785 -4.1404138 -4.1416025 -4.1417184 -4.1486154 -4.1676679 -4.1849284 -4.1870403 -4.1816926 -4.1797066][-4.1251836 -4.1171055 -4.1117692 -4.117672 -4.1396379 -4.1568389 -4.1671476 -4.17834 -4.1823215 -4.1815815 -4.1832824 -4.1825151 -4.1773806 -4.1693449 -4.1678963][-4.1314273 -4.1168227 -4.1025124 -4.1022663 -4.1240697 -4.1464744 -4.1679635 -4.1916766 -4.2023363 -4.1965342 -4.18316 -4.167098 -4.1532292 -4.1428828 -4.1437192][-4.1536531 -4.1363626 -4.1145649 -4.1013508 -4.1114621 -4.1355743 -4.1658778 -4.1963511 -4.2071476 -4.1953793 -4.1682186 -4.1379671 -4.1163387 -4.1059618 -4.1133809][-4.1807036 -4.1671333 -4.1475353 -4.1267395 -4.1254616 -4.1459303 -4.1770763 -4.206738 -4.2133732 -4.1954274 -4.1588593 -4.12122 -4.0961671 -4.0889792 -4.1012216]]...]
INFO - root - 2017-12-07 19:22:06.894007: step 45010, loss = 2.08, batch loss = 2.02 (15.3 examples/sec; 0.524 sec/batch; 41h:52m:07s remains)
INFO - root - 2017-12-07 19:22:13.745330: step 45020, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 53h:05m:18s remains)
INFO - root - 2017-12-07 19:22:20.550848: step 45030, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.661 sec/batch; 52h:47m:28s remains)
INFO - root - 2017-12-07 19:22:27.260400: step 45040, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 52h:17m:07s remains)
INFO - root - 2017-12-07 19:22:34.047115: step 45050, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 56h:08m:31s remains)
INFO - root - 2017-12-07 19:22:40.856413: step 45060, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 58h:29m:58s remains)
INFO - root - 2017-12-07 19:22:47.587397: step 45070, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 55h:00m:24s remains)
INFO - root - 2017-12-07 19:22:54.342349: step 45080, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 54h:19m:03s remains)
INFO - root - 2017-12-07 19:23:01.168816: step 45090, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.635 sec/batch; 50h:42m:57s remains)
INFO - root - 2017-12-07 19:23:07.956405: step 45100, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 55h:31m:18s remains)
2017-12-07 19:23:08.762727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9238741 -3.9327629 -3.9698079 -4.0089755 -4.0615921 -4.1245632 -4.1520276 -4.14681 -4.1226492 -4.0795579 -4.0290318 -4.0096927 -4.0324454 -4.0567069 -4.0518193][-3.822556 -3.8372436 -3.8854384 -3.9445841 -4.0243778 -4.1049795 -4.1446056 -4.1392288 -4.1073713 -4.049408 -3.9749084 -3.9314833 -3.9371758 -3.9633269 -3.9771547][-3.8583913 -3.8737793 -3.919241 -3.9755054 -4.0423603 -4.1054163 -4.1354561 -4.1262822 -4.0910196 -4.0270472 -3.94245 -3.8850343 -3.8715982 -3.8902793 -3.9202814][-3.9743257 -3.9781756 -4.0029392 -4.0324774 -4.0651522 -4.1015558 -4.1229119 -4.1175623 -4.0912356 -4.0445957 -3.9856334 -3.9442339 -3.9277411 -3.9375117 -3.9726524][-4.0969887 -4.0841913 -4.0766268 -4.0636244 -4.0571923 -4.0759616 -4.0973291 -4.1012936 -4.0899715 -4.0784607 -4.0683985 -4.0631623 -4.0577164 -4.0599122 -4.0787182][-4.1787939 -4.1472788 -4.1022878 -4.0440507 -4.003521 -4.0085034 -4.0289803 -4.0416961 -4.0530939 -4.0884953 -4.1284971 -4.1500382 -4.1547914 -4.1542368 -4.1611695][-4.2203484 -4.169868 -4.0858164 -3.9865615 -3.9139442 -3.9020731 -3.9222867 -3.9496741 -3.990202 -4.061749 -4.1317072 -4.1639838 -4.176187 -4.1863647 -4.1994982][-4.2333727 -4.1700339 -4.0621319 -3.9420233 -3.8545606 -3.8311074 -3.8539033 -3.8988798 -3.9633536 -4.0492024 -4.1256852 -4.1628389 -4.1816835 -4.2016039 -4.221601][-4.2369933 -4.1762867 -4.0750184 -3.9672842 -3.894994 -3.8784981 -3.9056456 -3.9567063 -4.0186071 -4.0894165 -4.1498728 -4.1824241 -4.2037339 -4.2275391 -4.249012][-4.2384968 -4.1924682 -4.1172328 -4.044992 -4.0033221 -3.9968483 -4.0176692 -4.0552988 -4.0988226 -4.1480851 -4.1918669 -4.21946 -4.2387052 -4.2587719 -4.2777667][-4.2411475 -4.21696 -4.175241 -4.141933 -4.1254721 -4.1234846 -4.1350603 -4.1545668 -4.1801176 -4.2117872 -4.2406316 -4.2594271 -4.2745614 -4.2896657 -4.3019838][-4.2467008 -4.2422285 -4.2267923 -4.220736 -4.2191215 -4.219913 -4.2272053 -4.238143 -4.2532029 -4.2731376 -4.2909632 -4.302032 -4.3097587 -4.3153815 -4.31637][-4.2561116 -4.2651391 -4.2675185 -4.278132 -4.2885723 -4.2934418 -4.2983379 -4.302937 -4.3082676 -4.3160405 -4.3229012 -4.32476 -4.3225603 -4.3168588 -4.3072696][-4.2708931 -4.2858796 -4.2973037 -4.3148427 -4.3288169 -4.3342938 -4.3369575 -4.3368783 -4.3350792 -4.334806 -4.3343587 -4.3303375 -4.3226638 -4.3105292 -4.2956853][-4.290802 -4.3104486 -4.3256578 -4.3427653 -4.3536992 -4.3562379 -4.3542972 -4.3488336 -4.343256 -4.339499 -4.335978 -4.3312635 -4.3234611 -4.3111143 -4.2962542]]...]
INFO - root - 2017-12-07 19:23:15.389087: step 45110, loss = 2.04, batch loss = 1.98 (14.6 examples/sec; 0.549 sec/batch; 43h:51m:40s remains)
INFO - root - 2017-12-07 19:23:22.085521: step 45120, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 52h:33m:12s remains)
INFO - root - 2017-12-07 19:23:28.858502: step 45130, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 53h:59m:24s remains)
INFO - root - 2017-12-07 19:23:35.665403: step 45140, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.700 sec/batch; 55h:50m:16s remains)
INFO - root - 2017-12-07 19:23:42.534683: step 45150, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 54h:57m:02s remains)
INFO - root - 2017-12-07 19:23:49.390034: step 45160, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 51h:48m:40s remains)
INFO - root - 2017-12-07 19:23:56.264386: step 45170, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.633 sec/batch; 50h:32m:11s remains)
INFO - root - 2017-12-07 19:24:03.050033: step 45180, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 54h:44m:15s remains)
INFO - root - 2017-12-07 19:24:09.910362: step 45190, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 55h:44m:06s remains)
INFO - root - 2017-12-07 19:24:16.602821: step 45200, loss = 2.06, batch loss = 2.00 (14.1 examples/sec; 0.567 sec/batch; 45h:13m:57s remains)
2017-12-07 19:24:17.239339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104115 -4.3081779 -4.3048272 -4.2984347 -4.2925463 -4.2891145 -4.2876968 -4.2867141 -4.2849665 -4.283484 -4.2834668 -4.2847586 -4.2860208 -4.2836928 -4.2757454][-4.31331 -4.31126 -4.3043275 -4.2919269 -4.2816162 -4.2777128 -4.2769289 -4.2768364 -4.2760282 -4.2756286 -4.2760239 -4.2779045 -4.2806616 -4.2802534 -4.2734056][-4.3189487 -4.3162031 -4.3021059 -4.2785921 -4.2613025 -4.2568226 -4.2569842 -4.2599211 -4.2642059 -4.2680674 -4.2712336 -4.2747254 -4.2794733 -4.28123 -4.27733][-4.3077888 -4.3045177 -4.2834816 -4.2493916 -4.2243824 -4.2175117 -4.2161126 -4.2224951 -4.2353654 -4.2459145 -4.2536669 -4.2604456 -4.2683053 -4.2731237 -4.274684][-4.2916303 -4.2875171 -4.2624464 -4.2210279 -4.1874075 -4.1721907 -4.1656556 -4.177072 -4.203208 -4.2229228 -4.2355514 -4.2456746 -4.2559762 -4.2634182 -4.2685933][-4.2734981 -4.2650418 -4.235302 -4.1872883 -4.1435475 -4.1118145 -4.0903969 -4.1075926 -4.1542416 -4.1903195 -4.2114387 -4.2275076 -4.2402668 -4.2493777 -4.2569733][-4.2508373 -4.2377267 -4.2057838 -4.1538391 -4.1001515 -4.0436487 -3.9952853 -4.0188737 -4.0962715 -4.1541653 -4.1860318 -4.2067604 -4.221662 -4.2335277 -4.2460089][-4.212532 -4.1939831 -4.162046 -4.1089492 -4.0444894 -3.9579206 -3.8702261 -3.8990507 -4.0155807 -4.0992661 -4.1406431 -4.1640482 -4.1801295 -4.1981373 -4.2203221][-4.2019634 -4.1791506 -4.1515484 -4.1058555 -4.041307 -3.9420559 -3.83458 -3.8614342 -3.988203 -4.0754747 -4.1164207 -4.1378484 -4.155931 -4.1821766 -4.2138209][-4.2132754 -4.1891789 -4.1694288 -4.1395941 -4.0916862 -4.0114055 -3.9256115 -3.9399979 -4.032867 -4.0960922 -4.1241703 -4.1379585 -4.1543331 -4.1831012 -4.2200489][-4.2208548 -4.1996341 -4.1847982 -4.1662769 -4.135386 -4.0798364 -4.0189757 -4.0275192 -4.0914726 -4.1323252 -4.1471324 -4.1526718 -4.1626997 -4.1884313 -4.2224569][-4.2298908 -4.2121773 -4.2006893 -4.1888466 -4.1687169 -4.12948 -4.0866246 -4.09215 -4.13766 -4.1661634 -4.1751437 -4.1738153 -4.1777258 -4.1975517 -4.2227349][-4.2444067 -4.2316885 -4.2218909 -4.2126389 -4.1982121 -4.1712337 -4.1422153 -4.1446161 -4.1766458 -4.197752 -4.2042594 -4.1990695 -4.1989384 -4.2123871 -4.22667][-4.2614555 -4.2552733 -4.2485623 -4.2426038 -4.2348123 -4.2186108 -4.2003517 -4.2000875 -4.2202411 -4.2331734 -4.2372847 -4.2311444 -4.2284627 -4.2345543 -4.2357788][-4.2779808 -4.2767038 -4.2729177 -4.27108 -4.2703166 -4.2650928 -4.256793 -4.2557411 -4.2653213 -4.2700415 -4.2703428 -4.2652736 -4.263062 -4.2639265 -4.2561026]]...]
INFO - root - 2017-12-07 19:24:23.690391: step 45210, loss = 2.07, batch loss = 2.01 (16.5 examples/sec; 0.485 sec/batch; 38h:44m:35s remains)
INFO - root - 2017-12-07 19:24:30.564730: step 45220, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 54h:40m:33s remains)
INFO - root - 2017-12-07 19:24:37.452438: step 45230, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 53h:29m:58s remains)
INFO - root - 2017-12-07 19:24:44.300243: step 45240, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 56h:32m:30s remains)
INFO - root - 2017-12-07 19:24:51.056219: step 45250, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 53h:11m:15s remains)
INFO - root - 2017-12-07 19:24:57.808397: step 45260, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 52h:49m:05s remains)
INFO - root - 2017-12-07 19:25:04.561133: step 45270, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 51h:38m:38s remains)
INFO - root - 2017-12-07 19:25:11.378834: step 45280, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 50h:35m:25s remains)
INFO - root - 2017-12-07 19:25:18.133261: step 45290, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 54h:55m:33s remains)
INFO - root - 2017-12-07 19:25:25.102032: step 45300, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 57h:30m:28s remains)
2017-12-07 19:25:25.804159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3065195 -4.2933049 -4.2811966 -4.2707515 -4.2612677 -4.2561235 -4.2591372 -4.2655325 -4.2702427 -4.2693772 -4.265008 -4.2618108 -4.2641044 -4.2664666 -4.2668033][-4.3033595 -4.2884417 -4.2753668 -4.2614045 -4.2497821 -4.2413673 -4.2443376 -4.2553253 -4.2630963 -4.2585649 -4.2467761 -4.2408738 -4.2413468 -4.2438889 -4.2457113][-4.2963223 -4.2785406 -4.2628865 -4.2450547 -4.2320042 -4.2214346 -4.2230687 -4.242415 -4.2572465 -4.2470627 -4.2245727 -4.2162566 -4.2174635 -4.22187 -4.2269564][-4.2740889 -4.247788 -4.2299666 -4.2093625 -4.1906142 -4.1763139 -4.1822109 -4.2162333 -4.2415586 -4.22829 -4.1950655 -4.1856661 -4.19074 -4.1981673 -4.2037292][-4.2380672 -4.2049074 -4.1844845 -4.1584473 -4.1315618 -4.1069293 -4.1171517 -4.1643867 -4.1992655 -4.1876736 -4.1515026 -4.1408944 -4.1493993 -4.1598759 -4.1644773][-4.204289 -4.1643972 -4.1368871 -4.1028409 -4.0577922 -4.0193477 -4.0313029 -4.0898347 -4.1340594 -4.1279259 -4.1048746 -4.097702 -4.105629 -4.1194553 -4.1274986][-4.1864409 -4.14012 -4.1045923 -4.06378 -3.9992416 -3.9453242 -3.9580538 -4.0298891 -4.0818624 -4.0849032 -4.0833125 -4.089035 -4.1024809 -4.1205077 -4.1316638][-4.1859059 -4.1413336 -4.1004596 -4.047472 -3.9705067 -3.9105854 -3.9216733 -4.0001631 -4.0601921 -4.0789304 -4.097187 -4.1153965 -4.1330214 -4.15384 -4.1661472][-4.2095518 -4.172637 -4.1331596 -4.077693 -4.0037341 -3.9493897 -3.9569659 -4.0227675 -4.075006 -4.0961866 -4.1151428 -4.1362085 -4.1585217 -4.1851559 -4.2054][-4.2600689 -4.2365928 -4.2091331 -4.1645226 -4.1014247 -4.0523295 -4.0495453 -4.0940313 -4.1331172 -4.148922 -4.1583753 -4.1754279 -4.2003937 -4.2319283 -4.2569728][-4.30871 -4.2984138 -4.2832251 -4.2534227 -4.207006 -4.1622357 -4.152565 -4.1838927 -4.21491 -4.226409 -4.22754 -4.2347641 -4.2543354 -4.2826691 -4.30937][-4.3257251 -4.3247628 -4.3203273 -4.3073688 -4.2783065 -4.2436385 -4.2332788 -4.2587385 -4.2801218 -4.2848568 -4.2811246 -4.2786026 -4.2900109 -4.3131309 -4.3372684][-4.319067 -4.3220825 -4.3222518 -4.316731 -4.296793 -4.2719817 -4.2699852 -4.2915149 -4.3020477 -4.299633 -4.2926593 -4.2867079 -4.2921062 -4.30927 -4.3281941][-4.3055553 -4.3072805 -4.3088117 -4.3051419 -4.2914467 -4.2758341 -4.2772851 -4.293561 -4.2992172 -4.2934136 -4.2862697 -4.2814741 -4.285254 -4.2949252 -4.305531][-4.3040018 -4.3028555 -4.3028159 -4.2989578 -4.2902637 -4.2835455 -4.28546 -4.2937264 -4.2960234 -4.2925968 -4.2895393 -4.2877746 -4.2894888 -4.2937021 -4.2983875]]...]
INFO - root - 2017-12-07 19:25:32.362185: step 45310, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.499 sec/batch; 39h:48m:33s remains)
INFO - root - 2017-12-07 19:25:39.268314: step 45320, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 56h:21m:32s remains)
INFO - root - 2017-12-07 19:25:46.117109: step 45330, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.699 sec/batch; 55h:43m:31s remains)
INFO - root - 2017-12-07 19:25:52.969713: step 45340, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 56h:10m:54s remains)
INFO - root - 2017-12-07 19:25:59.778410: step 45350, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 53h:00m:51s remains)
INFO - root - 2017-12-07 19:26:06.522964: step 45360, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 52h:59m:49s remains)
INFO - root - 2017-12-07 19:26:13.405723: step 45370, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 58h:53m:37s remains)
INFO - root - 2017-12-07 19:26:20.185778: step 45380, loss = 2.11, batch loss = 2.06 (11.4 examples/sec; 0.700 sec/batch; 55h:51m:21s remains)
INFO - root - 2017-12-07 19:26:26.979772: step 45390, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 51h:59m:37s remains)
INFO - root - 2017-12-07 19:26:33.754463: step 45400, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 53h:31m:53s remains)
2017-12-07 19:26:34.471227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2065296 -4.2075491 -4.1990147 -4.189579 -4.192636 -4.2028513 -4.2124543 -4.2201443 -4.2261677 -4.2191296 -4.1886005 -4.1408186 -4.1001353 -4.0914297 -4.1031508][-4.2063527 -4.2097983 -4.199348 -4.1866961 -4.1850834 -4.1975751 -4.2154279 -4.2298875 -4.2396197 -4.2346587 -4.2065816 -4.1659169 -4.133831 -4.127769 -4.138474][-4.2256994 -4.2318945 -4.2215648 -4.2077665 -4.2037621 -4.2133436 -4.2318625 -4.2498083 -4.2608418 -4.2560058 -4.2305737 -4.1965919 -4.1727052 -4.169888 -4.1772318][-4.2480459 -4.2526751 -4.2394562 -4.2227659 -4.218204 -4.2241869 -4.2371626 -4.2561264 -4.26873 -4.264143 -4.2414927 -4.2102208 -4.1907983 -4.1892209 -4.1918821][-4.2548847 -4.2543817 -4.2407951 -4.2227149 -4.2133188 -4.2057486 -4.2056093 -4.2245607 -4.2418656 -4.2400365 -4.2186818 -4.1910748 -4.1823115 -4.1901751 -4.1909084][-4.2472243 -4.2364016 -4.222146 -4.1976042 -4.1694169 -4.1349349 -4.1165185 -4.1432433 -4.1756735 -4.1804543 -4.1551929 -4.1310687 -4.1400471 -4.16497 -4.1678843][-4.2114196 -4.1857619 -4.1706424 -4.1405697 -4.0873914 -4.0209885 -3.9870493 -4.0342169 -4.092536 -4.1050072 -4.0697579 -4.0467525 -4.0829239 -4.1313276 -4.1469803][-4.1673632 -4.1302719 -4.1159129 -4.0850859 -4.0177112 -3.930829 -3.8935747 -3.9613519 -4.0401807 -4.0626512 -4.0231428 -4.00535 -4.0615406 -4.1262794 -4.1559563][-4.1438408 -4.1052175 -4.0928993 -4.0681615 -4.0044675 -3.9270086 -3.9039235 -3.9747491 -4.0558 -4.0853186 -4.0563354 -4.0454993 -4.0947647 -4.1487656 -4.17673][-4.1380105 -4.1141496 -4.1079206 -4.0938377 -4.0543985 -4.0095539 -4.0049686 -4.0567455 -4.1191111 -4.1487565 -4.1377397 -4.1351094 -4.1639724 -4.1943588 -4.2108984][-4.1466384 -4.1385179 -4.1392527 -4.1365414 -4.1197615 -4.1030521 -4.1087208 -4.140913 -4.1800814 -4.2026668 -4.2035394 -4.2089267 -4.2263885 -4.2402625 -4.2475481][-4.1636348 -4.1639304 -4.1695008 -4.1735563 -4.1727424 -4.1715746 -4.1793933 -4.1980681 -4.2190747 -4.2347341 -4.2400246 -4.2480893 -4.2611647 -4.26937 -4.2718625][-4.1818619 -4.1854444 -4.1926351 -4.1990023 -4.2057009 -4.2101226 -4.2150578 -4.2251816 -4.2352614 -4.2444196 -4.250793 -4.2581377 -4.2695875 -4.2786589 -4.2812953][-4.2006359 -4.2054806 -4.2119503 -4.2175736 -4.2236142 -4.2280116 -4.2312274 -4.2372627 -4.2440996 -4.2508774 -4.25581 -4.2615318 -4.2718372 -4.28188 -4.2866306][-4.2166762 -4.220366 -4.2243586 -4.22917 -4.234252 -4.2378097 -4.2407012 -4.2463727 -4.2528796 -4.2599869 -4.2665782 -4.2737741 -4.283113 -4.2918448 -4.2966743]]...]
INFO - root - 2017-12-07 19:26:40.997897: step 45410, loss = 2.06, batch loss = 2.01 (16.2 examples/sec; 0.493 sec/batch; 39h:19m:48s remains)
INFO - root - 2017-12-07 19:26:47.835182: step 45420, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 55h:25m:18s remains)
INFO - root - 2017-12-07 19:26:54.621730: step 45430, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 51h:29m:32s remains)
INFO - root - 2017-12-07 19:27:01.405102: step 45440, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 55h:54m:20s remains)
INFO - root - 2017-12-07 19:27:08.321316: step 45450, loss = 2.08, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 58h:24m:27s remains)
INFO - root - 2017-12-07 19:27:15.112347: step 45460, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 52h:57m:03s remains)
INFO - root - 2017-12-07 19:27:21.831408: step 45470, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 51h:34m:11s remains)
INFO - root - 2017-12-07 19:27:28.765926: step 45480, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 55h:51m:42s remains)
INFO - root - 2017-12-07 19:27:35.576812: step 45490, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 54h:30m:05s remains)
INFO - root - 2017-12-07 19:27:42.388368: step 45500, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 54h:57m:26s remains)
2017-12-07 19:27:43.072177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2174149 -4.22277 -4.2278357 -4.2293639 -4.2303085 -4.2352562 -4.2373581 -4.232348 -4.2212939 -4.2031331 -4.1845031 -4.1694794 -4.1612444 -4.15809 -4.1536527][-4.2245479 -4.2327404 -4.2357721 -4.2280893 -4.2162061 -4.209847 -4.2101321 -4.2141991 -4.218431 -4.2142506 -4.2069874 -4.2011085 -4.1951203 -4.1870174 -4.1759548][-4.2215195 -4.2304688 -4.2300944 -4.2141223 -4.190444 -4.1737351 -4.1722856 -4.1873169 -4.2092571 -4.2216468 -4.2247238 -4.2218804 -4.2148743 -4.2032194 -4.1923709][-4.2179484 -4.2258563 -4.2232256 -4.2028112 -4.171545 -4.1431918 -4.1333151 -4.1537313 -4.1908922 -4.2177587 -4.2279859 -4.22654 -4.2182727 -4.2075624 -4.2006087][-4.2129426 -4.2137742 -4.2051754 -4.1820903 -4.1479325 -4.1100621 -4.0888019 -4.1087227 -4.1561718 -4.1948347 -4.2111588 -4.2115245 -4.2022724 -4.1957731 -4.194675][-4.2033682 -4.19355 -4.1776524 -4.1505542 -4.1133022 -4.0639567 -4.0282459 -4.0467758 -4.1081452 -4.1619797 -4.1874881 -4.1926851 -4.18389 -4.1793952 -4.18308][-4.2009454 -4.1829081 -4.1592412 -4.122108 -4.071629 -4.0018306 -3.94485 -3.9585571 -4.0389094 -4.1162777 -4.1604905 -4.1800032 -4.1797352 -4.1774335 -4.1833363][-4.2011762 -4.1784124 -4.1511889 -4.1092391 -4.0494542 -3.965595 -3.8903542 -3.897444 -3.9896991 -4.0852337 -4.1482458 -4.1843038 -4.1957088 -4.1975865 -4.2045646][-4.2069554 -4.1862836 -4.1639743 -4.1303377 -4.081 -4.0133667 -3.9493427 -3.9473441 -4.0223727 -4.1064734 -4.1665006 -4.203609 -4.218915 -4.2245049 -4.2313976][-4.2125816 -4.2006702 -4.1890717 -4.1723876 -4.1465635 -4.1081562 -4.065021 -4.0530958 -4.0944853 -4.1483774 -4.1914411 -4.2210288 -4.235117 -4.2430358 -4.2497096][-4.2274766 -4.2238808 -4.2205768 -4.2155495 -4.2059846 -4.1862721 -4.1558027 -4.1353741 -4.1502142 -4.1772881 -4.2024384 -4.2229 -4.2350526 -4.2435436 -4.2507834][-4.2415457 -4.2439723 -4.2460461 -4.2455993 -4.2430949 -4.2327695 -4.2072749 -4.1797366 -4.1780457 -4.1877418 -4.1973019 -4.2065258 -4.2154942 -4.226625 -4.2372041][-4.2372885 -4.2435746 -4.2494173 -4.2512579 -4.2534885 -4.2516012 -4.2341957 -4.2058911 -4.1932192 -4.1919312 -4.1901231 -4.1904411 -4.197371 -4.2105069 -4.223217][-4.2188931 -4.2266531 -4.23538 -4.2385788 -4.2434006 -4.249752 -4.2466516 -4.2279363 -4.2125988 -4.205163 -4.196321 -4.1896787 -4.1939683 -4.2059293 -4.2152271][-4.192482 -4.2013807 -4.212894 -4.2165775 -4.2195411 -4.2292943 -4.2384176 -4.23201 -4.2195544 -4.2113452 -4.2030616 -4.1980472 -4.2027044 -4.2110391 -4.2166343]]...]
INFO - root - 2017-12-07 19:27:49.325495: step 45510, loss = 2.06, batch loss = 2.00 (19.0 examples/sec; 0.421 sec/batch; 33h:33m:57s remains)
INFO - root - 2017-12-07 19:27:56.097565: step 45520, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 55h:19m:51s remains)
INFO - root - 2017-12-07 19:28:02.849115: step 45530, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 54h:24m:55s remains)
INFO - root - 2017-12-07 19:28:09.628499: step 45540, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 50h:43m:08s remains)
INFO - root - 2017-12-07 19:28:16.508539: step 45550, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 52h:42m:55s remains)
INFO - root - 2017-12-07 19:28:23.466259: step 45560, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.742 sec/batch; 59h:07m:22s remains)
INFO - root - 2017-12-07 19:28:30.264991: step 45570, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 55h:42m:05s remains)
INFO - root - 2017-12-07 19:28:36.987333: step 45580, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 53h:54m:41s remains)
INFO - root - 2017-12-07 19:28:43.754708: step 45590, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 50h:08m:46s remains)
INFO - root - 2017-12-07 19:28:50.615271: step 45600, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 56h:32m:50s remains)
2017-12-07 19:28:51.385836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2815065 -4.2941051 -4.3036022 -4.3029175 -4.3022642 -4.3031325 -4.3007865 -4.2980857 -4.2964792 -4.2972403 -4.3007355 -4.3046088 -4.3059325 -4.3051834 -4.2967372][-4.2652354 -4.2699375 -4.2774124 -4.2802534 -4.2872005 -4.29456 -4.2935576 -4.289588 -4.2873597 -4.2882028 -4.2943211 -4.3008065 -4.30245 -4.298367 -4.2855668][-4.2360172 -4.2238779 -4.2242475 -4.2340045 -4.255373 -4.2737551 -4.2787285 -4.2791848 -4.2810969 -4.2832475 -4.2908955 -4.2982626 -4.2988062 -4.2903876 -4.2728915][-4.2100339 -4.1758156 -4.1608686 -4.171288 -4.2016726 -4.2292953 -4.24389 -4.2545624 -4.2672825 -4.277791 -4.2902403 -4.2984405 -4.2985644 -4.2886915 -4.2707853][-4.2057886 -4.1585917 -4.1289563 -4.1313524 -4.1549416 -4.1775331 -4.1906543 -4.208333 -4.2364492 -4.263422 -4.285502 -4.2967753 -4.2995377 -4.2936754 -4.2823567][-4.2147961 -4.1728315 -4.1400418 -4.124927 -4.1167784 -4.10096 -4.0832443 -4.0976486 -4.1527009 -4.2111039 -4.2516994 -4.2731752 -4.2849569 -4.2903948 -4.2917757][-4.2189279 -4.1992517 -4.1766868 -4.143023 -4.0882068 -4.0109882 -3.9271619 -3.9152052 -4.0068884 -4.1102767 -4.1798291 -4.2157011 -4.2337451 -4.2473669 -4.2591071][-4.2195597 -4.2311783 -4.2294755 -4.1926594 -4.1118121 -3.9895725 -3.8355212 -3.7726541 -3.8875237 -4.023706 -4.107511 -4.1449714 -4.1574907 -4.1684074 -4.1846685][-4.2077942 -4.2474008 -4.2705965 -4.255012 -4.1887045 -4.0771251 -3.9340959 -3.8589549 -3.9298837 -4.031805 -4.0915027 -4.1046953 -4.0958996 -4.0883856 -4.099215][-4.17747 -4.2292085 -4.2750454 -4.2901897 -4.2586355 -4.1852784 -4.092577 -4.0333428 -4.0519085 -4.0967641 -4.1168585 -4.1027164 -4.0743008 -4.0500193 -4.0473065][-4.1333127 -4.1833272 -4.2425385 -4.2853317 -4.2838063 -4.2459874 -4.1947336 -4.1518626 -4.1427679 -4.1535368 -4.1498475 -4.1252646 -4.0912585 -4.0613804 -4.0429325][-4.1077209 -4.1380687 -4.1909008 -4.2425313 -4.2609072 -4.2522779 -4.2339168 -4.2110825 -4.19281 -4.1848669 -4.1784563 -4.1624565 -4.1386418 -4.11597 -4.0910616][-4.1231794 -4.1319709 -4.1642809 -4.2019291 -4.2247634 -4.2355728 -4.2456379 -4.2404623 -4.2175136 -4.2008624 -4.1955104 -4.1902533 -4.1831431 -4.1794729 -4.1648426][-4.1793027 -4.1784859 -4.1932788 -4.2073579 -4.2182331 -4.23198 -4.2538581 -4.2574925 -4.23429 -4.2139111 -4.2091389 -4.21108 -4.2192249 -4.2316465 -4.2321286][-4.2475781 -4.2476435 -4.252615 -4.2533078 -4.2511344 -4.2568784 -4.2722759 -4.2761288 -4.2592206 -4.2433805 -4.2421584 -4.2479057 -4.25987 -4.2743716 -4.2805538]]...]
INFO - root - 2017-12-07 19:28:57.986546: step 45610, loss = 2.07, batch loss = 2.02 (13.8 examples/sec; 0.580 sec/batch; 46h:11m:15s remains)
INFO - root - 2017-12-07 19:29:04.751039: step 45620, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 52h:17m:39s remains)
INFO - root - 2017-12-07 19:29:11.541067: step 45630, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 54h:56m:17s remains)
INFO - root - 2017-12-07 19:29:18.288447: step 45640, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 53h:57m:13s remains)
INFO - root - 2017-12-07 19:29:24.952738: step 45650, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 52h:29m:34s remains)
INFO - root - 2017-12-07 19:29:31.780958: step 45660, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 52h:20m:54s remains)
INFO - root - 2017-12-07 19:29:38.539246: step 45670, loss = 2.06, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 50h:14m:06s remains)
INFO - root - 2017-12-07 19:29:45.351323: step 45680, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.724 sec/batch; 57h:43m:15s remains)
INFO - root - 2017-12-07 19:29:52.358155: step 45690, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 56h:49m:16s remains)
INFO - root - 2017-12-07 19:29:59.207769: step 45700, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 54h:16m:56s remains)
2017-12-07 19:29:59.930433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3277574 -4.3265657 -4.3249316 -4.3201175 -4.3135195 -4.3082538 -4.3046269 -4.30212 -4.3030996 -4.3052969 -4.3066425 -4.3072524 -4.3070951 -4.3070965 -4.3072][-4.309793 -4.3070564 -4.3032389 -4.2947474 -4.2856779 -4.2784643 -4.2743683 -4.2708278 -4.2723241 -4.2759109 -4.2792883 -4.2818933 -4.2820678 -4.2822042 -4.283319][-4.2834253 -4.2741666 -4.26168 -4.2455449 -4.2335181 -4.2274909 -4.2253304 -4.2232118 -4.2272358 -4.234231 -4.2397308 -4.2435985 -4.243618 -4.2429414 -4.2450304][-4.254981 -4.2366567 -4.2132983 -4.187686 -4.1735921 -4.1695061 -4.1700335 -4.1711135 -4.1772552 -4.18719 -4.1948118 -4.2007861 -4.2018456 -4.2018542 -4.2055721][-4.2305136 -4.2062378 -4.1736431 -4.1400132 -4.1223512 -4.1216569 -4.1256709 -4.1267266 -4.1304474 -4.1402674 -4.1472211 -4.151691 -4.1510072 -4.1495795 -4.1531692][-4.2097645 -4.1778049 -4.1296711 -4.0817471 -4.0585456 -4.063745 -4.0716548 -4.0684686 -4.0673394 -4.0775104 -4.0836186 -4.0865369 -4.080338 -4.0742607 -4.0741563][-4.2011395 -4.1613374 -4.096642 -4.03588 -4.0072556 -4.0146923 -4.0191684 -4.008007 -4.0012345 -4.0113845 -4.0155516 -4.0168376 -4.0095291 -4.0013137 -4.0005603][-4.207871 -4.1680775 -4.1041141 -4.0505719 -4.0246162 -4.0261612 -4.021503 -4.0029049 -3.992013 -3.9996862 -4.0014791 -4.0025797 -3.9955242 -3.9894745 -3.9887362][-4.222435 -4.1914763 -4.1474061 -4.1197572 -4.1051574 -4.1006193 -4.0872512 -4.0638213 -4.0504446 -4.0569053 -4.0529652 -4.0469966 -4.0338616 -4.0262122 -4.0212][-4.2356238 -4.213304 -4.1884623 -4.1824546 -4.1801977 -4.176403 -4.162993 -4.1411858 -4.1297369 -4.1357679 -4.125967 -4.1102352 -4.0883703 -4.0742536 -4.0675511][-4.247335 -4.2301645 -4.21581 -4.2195373 -4.2245321 -4.2222643 -4.2133031 -4.1983619 -4.1938763 -4.20331 -4.1937661 -4.1759539 -4.150157 -4.1313782 -4.1212931][-4.2582455 -4.2420106 -4.230526 -4.2342644 -4.2400007 -4.2386322 -4.2333393 -4.2284527 -4.232008 -4.2457509 -4.2412686 -4.2270942 -4.2056236 -4.1887755 -4.1787491][-4.2697206 -4.2543917 -4.2439871 -4.246233 -4.2500596 -4.2474647 -4.2416067 -4.2398129 -4.2463622 -4.2619953 -4.2633505 -4.2563391 -4.2460661 -4.2388268 -4.235043][-4.2800732 -4.2658167 -4.2563381 -4.2576423 -4.260077 -4.255496 -4.2487 -4.2472434 -4.2534881 -4.2668567 -4.2714391 -4.2720852 -4.2721486 -4.2728205 -4.2746372][-4.2914782 -4.2785091 -4.2693019 -4.2691822 -4.2703524 -4.2650771 -4.2598028 -4.2598844 -4.2663083 -4.2760205 -4.2791557 -4.2820663 -4.2855487 -4.2890644 -4.2921438]]...]
INFO - root - 2017-12-07 19:30:06.561746: step 45710, loss = 2.08, batch loss = 2.02 (13.6 examples/sec; 0.590 sec/batch; 46h:59m:33s remains)
INFO - root - 2017-12-07 19:30:13.408956: step 45720, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 55h:46m:50s remains)
INFO - root - 2017-12-07 19:30:20.166846: step 45730, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 52h:06m:11s remains)
INFO - root - 2017-12-07 19:30:26.991661: step 45740, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 50h:00m:00s remains)
INFO - root - 2017-12-07 19:30:33.830023: step 45750, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 55h:27m:37s remains)
INFO - root - 2017-12-07 19:30:40.596292: step 45760, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 57h:52m:21s remains)
INFO - root - 2017-12-07 19:30:47.512580: step 45770, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:34m:08s remains)
INFO - root - 2017-12-07 19:30:54.216374: step 45780, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.621 sec/batch; 49h:27m:29s remains)
INFO - root - 2017-12-07 19:31:01.072196: step 45790, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 56h:08m:40s remains)
INFO - root - 2017-12-07 19:31:07.934190: step 45800, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 54h:45m:53s remains)
2017-12-07 19:31:08.755625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2184696 -4.1947789 -4.1598816 -4.138742 -4.1266603 -4.1326761 -4.1427507 -4.1463022 -4.1550612 -4.1676674 -4.1883039 -4.2068553 -4.2185278 -4.2262487 -4.2275405][-4.1976662 -4.1797619 -4.1506619 -4.1294274 -4.1152697 -4.1235514 -4.1377115 -4.1408505 -4.1513977 -4.1656976 -4.1868215 -4.2046719 -4.2195177 -4.2316551 -4.2359123][-4.176765 -4.1670985 -4.1457658 -4.1286883 -4.11713 -4.1247978 -4.13597 -4.1345057 -4.1423841 -4.1549087 -4.1780319 -4.1955462 -4.2136703 -4.2296829 -4.2400308][-4.181963 -4.1788397 -4.1624074 -4.1479011 -4.1330514 -4.1295967 -4.1331453 -4.128396 -4.1310682 -4.1413693 -4.1640921 -4.18172 -4.1993847 -4.2167225 -4.2312341][-4.1818233 -4.1796355 -4.1630511 -4.1432672 -4.1163378 -4.0933814 -4.0866685 -4.086175 -4.1032925 -4.1277509 -4.1598849 -4.1822 -4.1943512 -4.201324 -4.2117734][-4.1768 -4.1756606 -4.1597347 -4.1314688 -4.0821524 -4.0284777 -4.0057878 -4.0069294 -4.0443487 -4.0963397 -4.1466093 -4.1819167 -4.1888852 -4.1884918 -4.1971312][-4.1678853 -4.1645103 -4.1436925 -4.0984707 -4.0236254 -3.9392834 -3.8988111 -3.9072742 -3.9728868 -4.0566554 -4.1286139 -4.1777086 -4.1870747 -4.1874027 -4.1964006][-4.1439428 -4.1321955 -4.1022797 -4.0416017 -3.949614 -3.8538547 -3.8153453 -3.8446372 -3.9347699 -4.0360932 -4.11434 -4.1684804 -4.1857905 -4.1903391 -4.2015681][-4.1223292 -4.1066504 -4.077136 -4.0288768 -3.9621298 -3.9005032 -3.8779981 -3.9070392 -3.9845116 -4.0616665 -4.1227932 -4.1708713 -4.1876273 -4.1908832 -4.2016025][-4.1123118 -4.1019211 -4.0858045 -4.0635805 -4.0402083 -4.0198455 -4.0087008 -4.0216856 -4.0731478 -4.117682 -4.1541104 -4.1886616 -4.2004046 -4.1992669 -4.2066522][-4.1192865 -4.1190515 -4.1169429 -4.1089759 -4.1029158 -4.0969658 -4.0885906 -4.0924582 -4.1332092 -4.1653156 -4.1836019 -4.2050004 -4.2090592 -4.2044978 -4.209919][-4.1376376 -4.1455894 -4.1486816 -4.1432505 -4.1451359 -4.1449976 -4.1326828 -4.1238985 -4.1487341 -4.1726546 -4.1892385 -4.2093449 -4.2125173 -4.20477 -4.2046962][-4.1458278 -4.1548142 -4.1575789 -4.1515074 -4.1539822 -4.1579437 -4.1455364 -4.1295786 -4.14042 -4.1547036 -4.1689825 -4.1949215 -4.2074828 -4.203989 -4.2016172][-4.1548486 -4.1579041 -4.1548219 -4.1459985 -4.1405792 -4.1419849 -4.130414 -4.1217222 -4.1370573 -4.1477213 -4.16388 -4.1901603 -4.2025743 -4.1978126 -4.1988726][-4.1632514 -4.1607285 -4.1564155 -4.1450291 -4.1363354 -4.1302838 -4.116653 -4.1155806 -4.13298 -4.1450162 -4.1676931 -4.1948757 -4.2039022 -4.1974792 -4.2022676]]...]
INFO - root - 2017-12-07 19:31:15.317678: step 45810, loss = 2.06, batch loss = 2.00 (14.1 examples/sec; 0.567 sec/batch; 45h:10m:02s remains)
INFO - root - 2017-12-07 19:31:21.909930: step 45820, loss = 2.10, batch loss = 2.05 (11.5 examples/sec; 0.696 sec/batch; 55h:25m:45s remains)
INFO - root - 2017-12-07 19:31:28.722093: step 45830, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 52h:34m:35s remains)
INFO - root - 2017-12-07 19:31:35.497491: step 45840, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 52h:57m:42s remains)
INFO - root - 2017-12-07 19:31:42.241379: step 45850, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 50h:43m:50s remains)
INFO - root - 2017-12-07 19:31:49.027161: step 45860, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 55h:24m:04s remains)
INFO - root - 2017-12-07 19:31:55.860894: step 45870, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.705 sec/batch; 56h:07m:46s remains)
INFO - root - 2017-12-07 19:32:02.662935: step 45880, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 52h:03m:37s remains)
INFO - root - 2017-12-07 19:32:09.548079: step 45890, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 50h:35m:29s remains)
INFO - root - 2017-12-07 19:32:16.295747: step 45900, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 51h:06m:57s remains)
2017-12-07 19:32:17.034793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3046865 -4.2971487 -4.2992473 -4.3027568 -4.3075404 -4.3059154 -4.2973938 -4.2882528 -4.2848239 -4.2870779 -4.2982426 -4.3141217 -4.3289485 -4.3406529 -4.3446226][-4.2653351 -4.2491822 -4.2524648 -4.2572074 -4.269762 -4.2739339 -4.2664089 -4.2583013 -4.2557039 -4.2575674 -4.2718363 -4.2941303 -4.3153424 -4.3335543 -4.3393512][-4.2163625 -4.1920648 -4.2020154 -4.206181 -4.2265372 -4.2362061 -4.2294436 -4.2229443 -4.2184734 -4.2238092 -4.2462564 -4.2766762 -4.3024716 -4.3242831 -4.3317871][-4.1674786 -4.1380181 -4.1512165 -4.1516428 -4.1759009 -4.1860838 -4.1757684 -4.1661024 -4.1540761 -4.1631989 -4.1952095 -4.2374315 -4.2723413 -4.30153 -4.3142776][-4.1096563 -4.0768223 -4.0906086 -4.0883455 -4.1147604 -4.1178102 -4.0957818 -4.0800104 -4.0627337 -4.0851097 -4.1331873 -4.1888371 -4.2354321 -4.27426 -4.2933669][-4.0577936 -4.0213528 -4.0271983 -4.02145 -4.0511627 -4.0363808 -3.9952607 -3.9647002 -3.9434202 -3.9839182 -4.0575366 -4.1309934 -4.1910315 -4.2384644 -4.2677855][-4.0345993 -3.9927154 -3.9800403 -3.9629207 -3.98838 -3.9397569 -3.8661785 -3.8040628 -3.7758367 -3.8480802 -3.9616804 -4.058527 -4.1333714 -4.1941881 -4.2349725][-4.0268612 -3.9772992 -3.9460745 -3.9137809 -3.9247129 -3.8469944 -3.7473636 -3.6555266 -3.6268814 -3.7347741 -3.887814 -3.9992778 -4.0867009 -4.1588964 -4.2091889][-4.0415325 -3.9911551 -3.951499 -3.9108813 -3.9092321 -3.8198147 -3.7168136 -3.6176953 -3.6032083 -3.7301133 -3.8910162 -4.0002904 -4.0857611 -4.1580005 -4.2071042][-4.1021347 -4.0675983 -4.0399632 -4.0102005 -4.0029488 -3.916472 -3.8266838 -3.7407074 -3.7410531 -3.8498776 -3.9821908 -4.0726686 -4.1382413 -4.1891751 -4.2255344][-4.1549349 -4.1366911 -4.1251445 -4.10856 -4.1011858 -4.0323081 -3.9656672 -3.9068451 -3.9216466 -4.00577 -4.0973811 -4.1585917 -4.202342 -4.2289276 -4.2516203][-4.191834 -4.1783919 -4.1745358 -4.1684575 -4.1662021 -4.1181717 -4.0744038 -4.0348563 -4.0530772 -4.1135988 -4.1717048 -4.2147169 -4.2432661 -4.2584991 -4.2733765][-4.2213149 -4.2133217 -4.2152715 -4.2186613 -4.2244391 -4.2001061 -4.1755786 -4.1538382 -4.1715889 -4.2099881 -4.2415309 -4.2665567 -4.2819524 -4.2898431 -4.2977562][-4.2421551 -4.2364593 -4.2404881 -4.2475967 -4.2576222 -4.2502794 -4.2400079 -4.23039 -4.245575 -4.2714791 -4.291522 -4.3058233 -4.3128691 -4.3156123 -4.3187609][-4.2685866 -4.2604523 -4.2606711 -4.2654591 -4.2743778 -4.2747231 -4.2710867 -4.2697458 -4.2796135 -4.2938828 -4.3067231 -4.3169928 -4.3240285 -4.328855 -4.332509]]...]
INFO - root - 2017-12-07 19:32:23.533856: step 45910, loss = 2.08, batch loss = 2.02 (16.5 examples/sec; 0.485 sec/batch; 38h:37m:44s remains)
INFO - root - 2017-12-07 19:32:30.260733: step 45920, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 51h:19m:54s remains)
INFO - root - 2017-12-07 19:32:37.089499: step 45930, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 52h:33m:11s remains)
INFO - root - 2017-12-07 19:32:43.994213: step 45940, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 59h:18m:29s remains)
INFO - root - 2017-12-07 19:32:50.889852: step 45950, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.666 sec/batch; 53h:00m:49s remains)
INFO - root - 2017-12-07 19:32:57.667559: step 45960, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 51h:17m:22s remains)
INFO - root - 2017-12-07 19:33:04.509439: step 45970, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 53h:31m:01s remains)
INFO - root - 2017-12-07 19:33:11.414201: step 45980, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 59h:24m:01s remains)
INFO - root - 2017-12-07 19:33:18.274887: step 45990, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.717 sec/batch; 57h:05m:08s remains)
INFO - root - 2017-12-07 19:33:25.098126: step 46000, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 53h:46m:20s remains)
2017-12-07 19:33:25.750474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0358896 -4.0459967 -4.0326662 -4.0282722 -4.0585251 -4.0886807 -4.1118822 -4.1256132 -4.100626 -4.0620036 -4.03941 -4.0291128 -4.0120296 -3.9923217 -3.9863491][-4.0516868 -4.0726848 -4.0653143 -4.0646696 -4.0891719 -4.1142488 -4.1329322 -4.1474452 -4.1306119 -4.0980425 -4.075367 -4.0656853 -4.0507107 -4.0301108 -4.0142875][-4.0778842 -4.0999308 -4.0960217 -4.0899034 -4.1003389 -4.1142011 -4.1261444 -4.1432166 -4.146224 -4.1275373 -4.1124487 -4.1072826 -4.099031 -4.0851502 -4.0726771][-4.0729232 -4.0952024 -4.0932336 -4.0825896 -4.08279 -4.0921912 -4.1031342 -4.1278663 -4.1536784 -4.1561961 -4.1523948 -4.1531577 -4.1531434 -4.1538353 -4.1571937][-4.054853 -4.0738335 -4.0684671 -4.0534134 -4.0485129 -4.0574059 -4.0722642 -4.1074409 -4.1506386 -4.1715369 -4.1809154 -4.19458 -4.204905 -4.2173171 -4.2334347][-4.0508404 -4.065021 -4.0537872 -4.0325227 -4.0203791 -4.0257292 -4.0428343 -4.0809107 -4.1301761 -4.1665692 -4.1934733 -4.2246985 -4.2465677 -4.2648897 -4.2854233][-4.0717607 -4.079423 -4.0636721 -4.0349488 -4.0087156 -4.0022287 -4.0139365 -4.0446014 -4.0915537 -4.13783 -4.1810346 -4.2271857 -4.2607346 -4.2844486 -4.3061914][-4.1149449 -4.1109128 -4.085372 -4.0454617 -4.0028729 -3.9762995 -3.9756267 -4.001266 -4.0502691 -4.1045203 -4.1590466 -4.2131066 -4.2544971 -4.2835522 -4.3043227][-4.1645164 -4.1439905 -4.1044397 -4.0492611 -3.9877207 -3.9392531 -3.9282131 -3.9593782 -4.0188165 -4.0819159 -4.1420851 -4.1980162 -4.2423224 -4.2736 -4.2939215][-4.2029819 -4.1652884 -4.1114984 -4.0463 -3.9763353 -3.9190555 -3.9041226 -3.9390559 -4.0033746 -4.0708265 -4.1312017 -4.1879354 -4.2350507 -4.2661028 -4.2847457][-4.2333565 -4.1806126 -4.1145487 -4.0464482 -3.9806237 -3.9333146 -3.9224389 -3.9550128 -4.0149975 -4.0819774 -4.1437969 -4.2026062 -4.24949 -4.2749596 -4.2861156][-4.2630382 -4.2062559 -4.1356177 -4.0668287 -4.0068007 -3.9688363 -3.9635568 -3.9964519 -4.05446 -4.1222935 -4.186657 -4.2425456 -4.2822886 -4.29674 -4.29413][-4.2934542 -4.246809 -4.1853991 -4.1227603 -4.0686145 -4.0350657 -4.0318184 -4.0619345 -4.1142797 -4.1756124 -4.2333508 -4.2799792 -4.3089356 -4.3124642 -4.2969403][-4.3152952 -4.2855744 -4.2437048 -4.1985784 -4.1598749 -4.135663 -4.1330342 -4.1537614 -4.1896238 -4.232821 -4.2750306 -4.308383 -4.3243341 -4.3170805 -4.2888489][-4.3252134 -4.3082862 -4.2828665 -4.2532544 -4.2279148 -4.2137213 -4.21287 -4.2256246 -4.2480516 -4.2767415 -4.3041744 -4.3233032 -4.3254628 -4.3051453 -4.2666659]]...]
INFO - root - 2017-12-07 19:33:32.364513: step 46010, loss = 2.04, batch loss = 1.98 (13.9 examples/sec; 0.574 sec/batch; 45h:40m:38s remains)
INFO - root - 2017-12-07 19:33:39.261346: step 46020, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 55h:19m:22s remains)
INFO - root - 2017-12-07 19:33:45.959905: step 46030, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 51h:50m:22s remains)
INFO - root - 2017-12-07 19:33:52.733962: step 46040, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 53h:33m:53s remains)
INFO - root - 2017-12-07 19:33:59.469333: step 46050, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 54h:13m:43s remains)
INFO - root - 2017-12-07 19:34:06.309620: step 46060, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 54h:49m:48s remains)
INFO - root - 2017-12-07 19:34:13.023044: step 46070, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 55h:44m:18s remains)
INFO - root - 2017-12-07 19:34:19.827559: step 46080, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 51h:25m:53s remains)
INFO - root - 2017-12-07 19:34:26.629073: step 46090, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 51h:49m:54s remains)
INFO - root - 2017-12-07 19:34:33.443283: step 46100, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 54h:47m:15s remains)
2017-12-07 19:34:34.226043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2835493 -4.2950764 -4.3039513 -4.2986646 -4.2891755 -4.2688079 -4.2565985 -4.2634873 -4.2796187 -4.2934103 -4.3035831 -4.3083949 -4.3074722 -4.3094759 -4.3107395][-4.2810793 -4.2904134 -4.2972121 -4.28836 -4.2735748 -4.2472639 -4.2319984 -4.23894 -4.258184 -4.27943 -4.2954025 -4.3052635 -4.3071656 -4.3103561 -4.3133893][-4.2716517 -4.2801509 -4.2841077 -4.268415 -4.2436213 -4.2012911 -4.175458 -4.1844177 -4.2121677 -4.2444181 -4.2692947 -4.2873721 -4.295156 -4.2975497 -4.2992916][-4.2647858 -4.2695184 -4.2663774 -4.2447443 -4.2097015 -4.1486721 -4.0967259 -4.092773 -4.1330347 -4.1816363 -4.2181768 -4.2481022 -4.264874 -4.2679672 -4.2680659][-4.2739687 -4.2756414 -4.2693009 -4.2460718 -4.2104239 -4.1345654 -4.0421133 -4.0045676 -4.0496378 -4.1131659 -4.1611581 -4.2018008 -4.2281981 -4.2357764 -4.2399211][-4.285512 -4.2849417 -4.2755265 -4.2524815 -4.2202778 -4.1359768 -3.9993038 -3.9127641 -3.963886 -4.0464106 -4.1025968 -4.1491175 -4.1867943 -4.2113934 -4.2293653][-4.2813025 -4.2780657 -4.2625122 -4.2322636 -4.1996074 -4.1069007 -3.9345162 -3.8047569 -3.8746438 -3.9824932 -4.0480895 -4.1033587 -4.1562524 -4.2003908 -4.2312407][-4.2728477 -4.2617288 -4.2365432 -4.1994705 -4.1664391 -4.0773239 -3.9160807 -3.7990448 -3.8732042 -3.9822149 -4.049726 -4.108602 -4.1668458 -4.2191806 -4.2522154][-4.26418 -4.2477765 -4.218235 -4.1835508 -4.1545625 -4.0888243 -3.986685 -3.9290509 -3.9866836 -4.0601668 -4.1114674 -4.1619663 -4.2120914 -4.2574749 -4.28334][-4.260407 -4.2456689 -4.2220869 -4.1986074 -4.1789246 -4.1394053 -4.0939183 -4.0862489 -4.1336193 -4.1752996 -4.2096753 -4.2468591 -4.27974 -4.3096414 -4.3244839][-4.2571597 -4.242619 -4.2207685 -4.2061605 -4.1987906 -4.1804566 -4.1656151 -4.18522 -4.2334509 -4.2573338 -4.2758136 -4.2995453 -4.3175225 -4.3351936 -4.3421955][-4.2509837 -4.2340226 -4.2141414 -4.2030253 -4.2014771 -4.1925125 -4.1881604 -4.2151361 -4.2594371 -4.2762771 -4.2842674 -4.2924395 -4.2992148 -4.3089819 -4.3116455][-4.2420769 -4.2241173 -4.2086396 -4.2020507 -4.1991649 -4.1900659 -4.1860189 -4.2068553 -4.2387409 -4.2472291 -4.247467 -4.2481527 -4.2514176 -4.2581654 -4.2607908][-4.2290072 -4.2166419 -4.2111816 -4.2103605 -4.2080913 -4.1965976 -4.1917105 -4.2054577 -4.2254453 -4.2292895 -4.2283082 -4.2286053 -4.2314558 -4.2353129 -4.236949][-4.2298055 -4.2253265 -4.2292137 -4.2321692 -4.2311664 -4.2189426 -4.2122293 -4.2251873 -4.2384953 -4.2401729 -4.2410221 -4.241641 -4.2428856 -4.2443705 -4.2446561]]...]
INFO - root - 2017-12-07 19:34:40.790272: step 46110, loss = 2.05, batch loss = 1.99 (15.1 examples/sec; 0.528 sec/batch; 42h:00m:40s remains)
INFO - root - 2017-12-07 19:34:47.490786: step 46120, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 50h:25m:18s remains)
INFO - root - 2017-12-07 19:34:54.171250: step 46130, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 52h:14m:31s remains)
INFO - root - 2017-12-07 19:35:00.973616: step 46140, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 53h:53m:39s remains)
INFO - root - 2017-12-07 19:35:07.761127: step 46150, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 56h:42m:22s remains)
INFO - root - 2017-12-07 19:35:14.520852: step 46160, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.649 sec/batch; 51h:35m:42s remains)
INFO - root - 2017-12-07 19:35:21.296333: step 46170, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 51h:22m:57s remains)
INFO - root - 2017-12-07 19:35:28.047518: step 46180, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 57h:06m:42s remains)
INFO - root - 2017-12-07 19:35:34.910837: step 46190, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.746 sec/batch; 59h:17m:30s remains)
INFO - root - 2017-12-07 19:35:41.653857: step 46200, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 53h:21m:37s remains)
2017-12-07 19:35:42.425587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556443 -4.2443419 -4.2497716 -4.2676988 -4.2816839 -4.2844019 -4.2659264 -4.2367506 -4.2368245 -4.2675419 -4.2922535 -4.285696 -4.2697487 -4.2533875 -4.2199078][-4.2408848 -4.2388377 -4.2540221 -4.2763319 -4.291214 -4.2908111 -4.2663288 -4.2314897 -4.2324796 -4.2680535 -4.2941442 -4.28294 -4.26048 -4.2423782 -4.2120581][-4.2260742 -4.2329783 -4.2581444 -4.2819891 -4.2912765 -4.2825089 -4.2497811 -4.2139177 -4.2175851 -4.2561016 -4.2852206 -4.2784047 -4.2551503 -4.2380538 -4.2127876][-4.2181325 -4.2292261 -4.2611322 -4.2819953 -4.2762389 -4.2514973 -4.2095661 -4.1733303 -4.1820278 -4.2309647 -4.2674675 -4.2704868 -4.2542515 -4.2414775 -4.2198992][-4.2046132 -4.2184639 -4.2545824 -4.2706547 -4.2482729 -4.2053456 -4.1493206 -4.1062236 -4.1235 -4.1939855 -4.244823 -4.2590542 -4.2547917 -4.246098 -4.2248449][-4.1952038 -4.209136 -4.2426419 -4.2482567 -4.2086725 -4.1449504 -4.0633307 -4.0005736 -4.038641 -4.1411119 -4.209291 -4.2378321 -4.2456717 -4.2420692 -4.2279429][-4.1782913 -4.1907358 -4.2179475 -4.2168455 -4.1702003 -4.0844975 -3.9597852 -3.8660753 -3.9402742 -4.0815539 -4.1698384 -4.2176628 -4.2335596 -4.2367287 -4.2347879][-4.1527772 -4.1656408 -4.1917505 -4.1941957 -4.1545506 -4.0672183 -3.9247129 -3.8110349 -3.9093094 -4.0617638 -4.1547346 -4.2092328 -4.2286935 -4.2373047 -4.2417774][-4.1450877 -4.1614046 -4.1891894 -4.1993074 -4.1752186 -4.1130733 -4.0088782 -3.9390769 -4.0083947 -4.1129775 -4.1806321 -4.2183514 -4.2344751 -4.2460136 -4.2483659][-4.1572509 -4.1737037 -4.2013474 -4.2181783 -4.2058697 -4.1681218 -4.1097007 -4.0833144 -4.1280656 -4.184382 -4.2245703 -4.2454619 -4.2536416 -4.2582145 -4.2468352][-4.1755152 -4.1914077 -4.2180147 -4.2353668 -4.22805 -4.2041368 -4.1735797 -4.1726074 -4.2052121 -4.2386436 -4.2660565 -4.274673 -4.2707658 -4.2610087 -4.2361474][-4.1964269 -4.2158537 -4.2388477 -4.2528906 -4.2502661 -4.2335415 -4.2130489 -4.2157464 -4.2356334 -4.2593713 -4.2775435 -4.2754846 -4.2626829 -4.2473297 -4.2216945][-4.2066021 -4.233181 -4.2609591 -4.2764621 -4.2758064 -4.2589736 -4.232492 -4.2235212 -4.228024 -4.2389374 -4.2493 -4.245636 -4.2374339 -4.2294431 -4.2149715][-4.2128053 -4.2474427 -4.2833371 -4.300303 -4.3013282 -4.2834673 -4.2506685 -4.2243972 -4.2128882 -4.2099843 -4.2100024 -4.2051978 -4.2039237 -4.2118239 -4.2151866][-4.2138066 -4.2547956 -4.2964878 -4.3152733 -4.314487 -4.2958045 -4.2646079 -4.2331972 -4.2123828 -4.1988645 -4.193049 -4.1864324 -4.1881995 -4.2089229 -4.2286634]]...]
INFO - root - 2017-12-07 19:35:48.973514: step 46210, loss = 2.06, batch loss = 2.00 (14.1 examples/sec; 0.568 sec/batch; 45h:11m:50s remains)
INFO - root - 2017-12-07 19:35:55.760573: step 46220, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 56h:12m:25s remains)
INFO - root - 2017-12-07 19:36:02.451981: step 46230, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.648 sec/batch; 51h:29m:50s remains)
INFO - root - 2017-12-07 19:36:09.097380: step 46240, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.662 sec/batch; 52h:38m:22s remains)
INFO - root - 2017-12-07 19:36:15.956952: step 46250, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 52h:45m:28s remains)
INFO - root - 2017-12-07 19:36:22.847057: step 46260, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:28m:05s remains)
INFO - root - 2017-12-07 19:36:29.628353: step 46270, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 54h:36m:31s remains)
INFO - root - 2017-12-07 19:36:36.361590: step 46280, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 50h:59m:19s remains)
INFO - root - 2017-12-07 19:36:43.120525: step 46290, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 49h:57m:02s remains)
INFO - root - 2017-12-07 19:36:49.886304: step 46300, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 53h:14m:17s remains)
2017-12-07 19:36:50.675046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2131152 -4.1914067 -4.1660213 -4.1606064 -4.1715856 -4.1760616 -4.1594853 -4.1020322 -4.0095558 -3.9650009 -4.01371 -4.100255 -4.1729507 -4.2155871 -4.2335443][-4.2313075 -4.2114778 -4.180366 -4.1715283 -4.1882958 -4.1989727 -4.1863351 -4.1378746 -4.0595107 -4.022954 -4.0624456 -4.1309171 -4.1844306 -4.2127724 -4.2241149][-4.2452621 -4.2281694 -4.19727 -4.186389 -4.2010746 -4.2145824 -4.2109318 -4.1798005 -4.1293445 -4.1097593 -4.1391907 -4.1863742 -4.2161465 -4.2255378 -4.229023][-4.25035 -4.2396493 -4.2121119 -4.1935253 -4.1937485 -4.1951342 -4.1831608 -4.1624746 -4.1421227 -4.14935 -4.1810412 -4.2136431 -4.2291136 -4.228929 -4.2284341][-4.2433848 -4.2369471 -4.2110834 -4.1839938 -4.1632004 -4.1385446 -4.0942993 -4.0623884 -4.0748916 -4.1249981 -4.1746387 -4.2060733 -4.223691 -4.2302995 -4.2353125][-4.2300844 -4.2268167 -4.1998544 -4.1652689 -4.1241207 -4.0658984 -3.967834 -3.910321 -3.9606342 -4.063343 -4.1392593 -4.175869 -4.206089 -4.2294722 -4.2442207][-4.2253046 -4.2214527 -4.1904392 -4.147295 -4.09161 -4.0081859 -3.8717871 -3.80239 -3.8878815 -4.0228515 -4.1091614 -4.1505556 -4.191361 -4.2270756 -4.2455282][-4.2212353 -4.2127333 -4.1793933 -4.1335244 -4.0776458 -3.9971545 -3.8751192 -3.8216259 -3.9084289 -4.0363331 -4.1173067 -4.15643 -4.1978836 -4.2304688 -4.2410913][-4.2108612 -4.2013097 -4.1767321 -4.142015 -4.1008964 -4.0527272 -3.9718616 -3.9354653 -3.9945383 -4.0881939 -4.1482697 -4.1774645 -4.20861 -4.2312355 -4.2315283][-4.2009063 -4.1976466 -4.1907258 -4.1761942 -4.1562448 -4.1338015 -4.0878153 -4.0585828 -4.0754094 -4.120965 -4.1498675 -4.1686964 -4.194303 -4.2091546 -4.2055063][-4.1844988 -4.1976838 -4.2165484 -4.2199545 -4.2101769 -4.1947594 -4.16066 -4.1263146 -4.1038594 -4.101203 -4.0981722 -4.1112852 -4.1425047 -4.1595306 -4.1623025][-4.1581631 -4.1895742 -4.2282872 -4.2397122 -4.2296996 -4.2102408 -4.1747718 -4.1347346 -4.092113 -4.0562968 -4.0365176 -4.0506892 -4.0898213 -4.1155758 -4.1298456][-4.1228614 -4.1620522 -4.2067409 -4.2165327 -4.2033129 -4.1815844 -4.1529241 -4.1256137 -4.094418 -4.0596514 -4.0433693 -4.05765 -4.0938597 -4.1207476 -4.1363945][-4.0873961 -4.1154771 -4.1482277 -4.1565642 -4.1514249 -4.1469297 -4.1480374 -4.1498933 -4.1447558 -4.1325488 -4.1253738 -4.1304946 -4.1511965 -4.1674504 -4.1739187][-4.0623589 -4.0708942 -4.0827007 -4.0941272 -4.1078215 -4.1258597 -4.1546159 -4.1862593 -4.2091336 -4.2186346 -4.2170954 -4.2141256 -4.2194419 -4.2214761 -4.2196951]]...]
INFO - root - 2017-12-07 19:36:57.257105: step 46310, loss = 2.06, batch loss = 2.00 (15.2 examples/sec; 0.525 sec/batch; 41h:45m:51s remains)
INFO - root - 2017-12-07 19:37:04.064721: step 46320, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 51h:38m:12s remains)
INFO - root - 2017-12-07 19:37:10.900070: step 46330, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 55h:40m:25s remains)
INFO - root - 2017-12-07 19:37:17.662517: step 46340, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 56h:49m:00s remains)
INFO - root - 2017-12-07 19:37:24.450137: step 46350, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 57h:40m:18s remains)
INFO - root - 2017-12-07 19:37:31.239539: step 46360, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 50h:47m:01s remains)
INFO - root - 2017-12-07 19:37:38.038762: step 46370, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:28m:10s remains)
INFO - root - 2017-12-07 19:37:44.889267: step 46380, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 56h:01m:51s remains)
INFO - root - 2017-12-07 19:37:51.781074: step 46390, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 54h:35m:24s remains)
INFO - root - 2017-12-07 19:37:58.501136: step 46400, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 50h:40m:33s remains)
2017-12-07 19:37:59.202327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2963424 -4.3043313 -4.31298 -4.316968 -4.3174129 -4.3115993 -4.3034835 -4.296545 -4.292417 -4.3003654 -4.3078208 -4.3059654 -4.3002343 -4.2936344 -4.2914648][-4.2743173 -4.2857485 -4.3003135 -4.3085423 -4.3114963 -4.3078561 -4.2930775 -4.2746859 -4.2656741 -4.27894 -4.2895365 -4.28623 -4.2776451 -4.26801 -4.2627039][-4.2539692 -4.2660761 -4.2858653 -4.2985263 -4.3068037 -4.30186 -4.2771931 -4.2419686 -4.2224107 -4.2401357 -4.2559767 -4.2541952 -4.2470517 -4.234849 -4.2229915][-4.234077 -4.2419639 -4.2652869 -4.2826982 -4.2948475 -4.2862349 -4.2532463 -4.1962914 -4.1565905 -4.1752343 -4.2043262 -4.2091436 -4.2036581 -4.1876612 -4.1685658][-4.2144165 -4.2134376 -4.2381444 -4.2645526 -4.2804246 -4.2605786 -4.205729 -4.1151724 -4.0596781 -4.092793 -4.1439109 -4.1593552 -4.1605105 -4.1432805 -4.1172957][-4.1963863 -4.1861191 -4.2137866 -4.247714 -4.256269 -4.2121143 -4.1306443 -4.0160003 -3.9607732 -4.0228863 -4.0970669 -4.1286316 -4.1384778 -4.1197128 -4.087935][-4.188601 -4.1771026 -4.206183 -4.2401333 -4.23496 -4.1698585 -4.0678358 -3.9448848 -3.9042768 -3.9975643 -4.0874219 -4.1312528 -4.1444383 -4.1192126 -4.0812821][-4.1927352 -4.1819897 -4.207293 -4.2331271 -4.2209868 -4.1514482 -4.0423379 -3.9261343 -3.9126382 -4.0243464 -4.1111274 -4.1518321 -4.1571746 -4.1346474 -4.10803][-4.199872 -4.1872797 -4.2064242 -4.228756 -4.2220578 -4.1599636 -4.0595837 -3.9664423 -3.9690678 -4.0616183 -4.1295867 -4.1650906 -4.1732292 -4.1668715 -4.159934][-4.2079296 -4.1957 -4.2135205 -4.2389889 -4.2416158 -4.1921344 -4.1092563 -4.0325642 -4.0304489 -4.1008906 -4.1549973 -4.18436 -4.2002258 -4.2110147 -4.2139969][-4.2233953 -4.2101917 -4.2237287 -4.2488022 -4.2574015 -4.2205453 -4.1515369 -4.0825357 -4.0806465 -4.1434402 -4.1924009 -4.2170339 -4.2334867 -4.2454548 -4.2462125][-4.2416935 -4.2222662 -4.2251577 -4.2466831 -4.26124 -4.2395215 -4.1865993 -4.1314697 -4.1354542 -4.1924939 -4.2315183 -4.2495937 -4.2596083 -4.2628989 -4.258522][-4.2585936 -4.2352371 -4.2310181 -4.248539 -4.2638392 -4.2546468 -4.2215462 -4.1845431 -4.1915369 -4.2346745 -4.2603846 -4.2684078 -4.2698722 -4.2674079 -4.26165][-4.2742038 -4.2503362 -4.2437148 -4.2569075 -4.2698874 -4.2700515 -4.2574105 -4.2375727 -4.2397928 -4.2622423 -4.2731752 -4.274632 -4.2725077 -4.2673635 -4.2615232][-4.2925014 -4.2717361 -4.2633858 -4.2699857 -4.2797346 -4.2837453 -4.2820315 -4.2743473 -4.2759838 -4.2848444 -4.2864251 -4.2839613 -4.2809353 -4.275589 -4.2711368]]...]
INFO - root - 2017-12-07 19:38:05.828958: step 46410, loss = 2.04, batch loss = 1.98 (14.0 examples/sec; 0.572 sec/batch; 45h:28m:01s remains)
INFO - root - 2017-12-07 19:38:12.587740: step 46420, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 54h:16m:23s remains)
INFO - root - 2017-12-07 19:38:19.277104: step 46430, loss = 2.10, batch loss = 2.05 (12.7 examples/sec; 0.629 sec/batch; 49h:58m:02s remains)
INFO - root - 2017-12-07 19:38:25.998057: step 46440, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 49h:52m:49s remains)
INFO - root - 2017-12-07 19:38:32.764742: step 46450, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.733 sec/batch; 58h:14m:03s remains)
INFO - root - 2017-12-07 19:38:39.492706: step 46460, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 54h:54m:27s remains)
INFO - root - 2017-12-07 19:38:46.295937: step 46470, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 53h:42m:21s remains)
INFO - root - 2017-12-07 19:38:53.033496: step 46480, loss = 2.04, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 50h:38m:14s remains)
INFO - root - 2017-12-07 19:38:59.950219: step 46490, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 57h:42m:31s remains)
INFO - root - 2017-12-07 19:39:06.777776: step 46500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:57m:15s remains)
2017-12-07 19:39:07.576237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2868991 -4.2841582 -4.28213 -4.2739468 -4.2666373 -4.2647195 -4.2621536 -4.2636218 -4.2644391 -4.2670074 -4.2723007 -4.2755284 -4.2776375 -4.2856731 -4.2949252][-4.2762265 -4.273046 -4.2691708 -4.2555356 -4.2476926 -4.2478061 -4.2441864 -4.2461557 -4.2485523 -4.2525773 -4.2569456 -4.2573953 -4.2571073 -4.26708 -4.2781353][-4.2653775 -4.2649875 -4.26271 -4.2468295 -4.2390437 -4.236021 -4.2280288 -4.2261648 -4.2303071 -4.24165 -4.2500715 -4.2492747 -4.2485266 -4.2579093 -4.2689261][-4.2495365 -4.2533455 -4.256988 -4.2439752 -4.2340355 -4.2219114 -4.199739 -4.1838479 -4.1910729 -4.2225246 -4.2458768 -4.2507682 -4.2515731 -4.2582545 -4.2670627][-4.228435 -4.2358031 -4.2413082 -4.2315969 -4.2206373 -4.1941795 -4.1475348 -4.1108217 -4.1279669 -4.1875224 -4.2300692 -4.2457442 -4.24957 -4.2532797 -4.2604575][-4.2035294 -4.2147894 -4.2234955 -4.2175369 -4.2051411 -4.1634789 -4.0875692 -4.0265636 -4.0630326 -4.155159 -4.2158885 -4.240715 -4.2401152 -4.2365794 -4.2402759][-4.181283 -4.1959515 -4.2098608 -4.2107344 -4.1933265 -4.131402 -4.0197587 -3.9373813 -4.0048757 -4.1302042 -4.2076664 -4.236589 -4.2263603 -4.2085919 -4.2063413][-4.16881 -4.1861439 -4.2087116 -4.214654 -4.1840153 -4.0954661 -3.9487004 -3.8526912 -3.9561453 -4.1123252 -4.2038937 -4.230123 -4.2103643 -4.1774049 -4.1685572][-4.170042 -4.190814 -4.219471 -4.2262731 -4.1806178 -4.0718408 -3.9072058 -3.8132696 -3.9435544 -4.1177468 -4.2083497 -4.2222528 -4.1867056 -4.1398525 -4.1304669][-4.1809797 -4.2057142 -4.2367072 -4.2428722 -4.191576 -4.0810127 -3.9286859 -3.8563352 -3.9824617 -4.1398492 -4.2127142 -4.2070918 -4.1537871 -4.1073327 -4.1151533][-4.1977181 -4.22825 -4.2583323 -4.2642426 -4.2174296 -4.1195126 -3.9997444 -3.9582388 -4.0574937 -4.1720185 -4.215878 -4.1873159 -4.1251068 -4.09399 -4.1283226][-4.2209668 -4.2523127 -4.2789192 -4.2832441 -4.2458434 -4.1683173 -4.0859795 -4.0691056 -4.1391692 -4.209837 -4.2244186 -4.1807246 -4.1231937 -4.1138964 -4.1650653][-4.2490134 -4.276721 -4.2991138 -4.3003039 -4.2702804 -4.2130017 -4.1632853 -4.1604595 -4.2039809 -4.2392569 -4.2340417 -4.1897597 -4.1509385 -4.1639476 -4.218091][-4.2696342 -4.2906685 -4.3074985 -4.3069086 -4.2846603 -4.2479143 -4.2190261 -4.2204595 -4.2415786 -4.2534885 -4.2415314 -4.2129602 -4.1973028 -4.2196932 -4.2584715][-4.2859421 -4.297586 -4.3073363 -4.3064051 -4.2932835 -4.2734952 -4.2595124 -4.2609239 -4.2662625 -4.2648892 -4.2556429 -4.2443314 -4.2448397 -4.2638769 -4.2835255]]...]
INFO - root - 2017-12-07 19:39:14.119852: step 46510, loss = 2.07, batch loss = 2.01 (14.9 examples/sec; 0.536 sec/batch; 42h:33m:12s remains)
INFO - root - 2017-12-07 19:39:21.100290: step 46520, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.742 sec/batch; 58h:55m:23s remains)
INFO - root - 2017-12-07 19:39:27.813230: step 46530, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 53h:40m:57s remains)
INFO - root - 2017-12-07 19:39:34.549944: step 46540, loss = 2.03, batch loss = 1.97 (12.6 examples/sec; 0.635 sec/batch; 50h:25m:43s remains)
INFO - root - 2017-12-07 19:39:41.374589: step 46550, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 52h:46m:26s remains)
INFO - root - 2017-12-07 19:39:48.160085: step 46560, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 55h:11m:54s remains)
INFO - root - 2017-12-07 19:39:54.887866: step 46570, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.713 sec/batch; 56h:37m:16s remains)
INFO - root - 2017-12-07 19:40:01.807803: step 46580, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 52h:42m:03s remains)
INFO - root - 2017-12-07 19:40:08.579485: step 46590, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 49h:32m:13s remains)
INFO - root - 2017-12-07 19:40:15.417606: step 46600, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 56h:50m:04s remains)
2017-12-07 19:40:16.207247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1947112 -4.2176328 -4.2327843 -4.2366319 -4.226748 -4.2090745 -4.1893892 -4.1691275 -4.1704388 -4.2031636 -4.2283897 -4.2356858 -4.2377229 -4.2431979 -4.2517095][-4.1915021 -4.21275 -4.2250385 -4.2250543 -4.2126789 -4.1989393 -4.1792188 -4.1628056 -4.1615739 -4.1901932 -4.2238269 -4.2401414 -4.244348 -4.24303 -4.2445011][-4.1904349 -4.2048187 -4.2170029 -4.2209506 -4.2118154 -4.198472 -4.1757483 -4.1588984 -4.1523247 -4.1686997 -4.202486 -4.22512 -4.2333069 -4.2287183 -4.22652][-4.1930652 -4.2008572 -4.2153711 -4.2271147 -4.2217579 -4.2066817 -4.1810207 -4.1626091 -4.1559596 -4.168036 -4.1998944 -4.2214422 -4.2288594 -4.224165 -4.2201543][-4.1944246 -4.19626 -4.2105403 -4.2265453 -4.2191515 -4.1969028 -4.1701365 -4.162087 -4.1652913 -4.1822252 -4.2139678 -4.2318525 -4.2385659 -4.2356505 -4.2337136][-4.177784 -4.172328 -4.1901965 -4.2138624 -4.204432 -4.1698513 -4.1391945 -4.1507344 -4.1727014 -4.193265 -4.2209797 -4.2380929 -4.2489686 -4.2521 -4.25531][-4.13704 -4.1217747 -4.1448078 -4.172873 -4.1550589 -4.0966721 -4.0470562 -4.079845 -4.1330991 -4.1646442 -4.1899581 -4.2111382 -4.2345271 -4.2506 -4.2612281][-4.0902181 -4.0598235 -4.0829592 -4.1100698 -4.0752497 -3.9737523 -3.8779919 -3.933388 -4.0330396 -4.0903883 -4.1234851 -4.1527128 -4.1906691 -4.224216 -4.2437286][-4.0503764 -4.0126648 -4.0351715 -4.0559907 -4.0075803 -3.8760779 -3.7379065 -3.810853 -3.9538052 -4.0368433 -4.0793948 -4.1098747 -4.14897 -4.1859746 -4.2109895][-4.0603638 -4.0353279 -4.0597115 -4.080019 -4.0449338 -3.9513507 -3.8513484 -3.8957887 -4.0015669 -4.0682807 -4.1046696 -4.1283159 -4.1505857 -4.1736288 -4.194665][-4.1147518 -4.1073837 -4.1317353 -4.1471257 -4.1231422 -4.0697074 -4.0065289 -4.0187716 -4.0782881 -4.1208167 -4.1473351 -4.165534 -4.17735 -4.1866727 -4.198884][-4.1689453 -4.17052 -4.194315 -4.2041841 -4.1854086 -4.1537848 -4.1117926 -4.1009569 -4.124155 -4.1502786 -4.1720443 -4.1912265 -4.2053871 -4.2124362 -4.2195015][-4.22923 -4.2299504 -4.2435613 -4.2462244 -4.2315073 -4.2123494 -4.1841598 -4.1659703 -4.1710835 -4.1897378 -4.2106252 -4.2333941 -4.2501922 -4.2569051 -4.2589254][-4.2876458 -4.2865715 -4.2901688 -4.2856722 -4.2737293 -4.2633591 -4.2504153 -4.239429 -4.2400188 -4.2533488 -4.2696023 -4.286304 -4.2982707 -4.3028893 -4.300209][-4.3203821 -4.3188024 -4.3156176 -4.3091388 -4.3018003 -4.2967615 -4.2920785 -4.2886372 -4.2906542 -4.2992611 -4.3095851 -4.3192806 -4.3265157 -4.3287988 -4.3239145]]...]
INFO - root - 2017-12-07 19:40:22.696476: step 46610, loss = 2.11, batch loss = 2.05 (17.4 examples/sec; 0.460 sec/batch; 36h:33m:26s remains)
INFO - root - 2017-12-07 19:40:29.524281: step 46620, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 51h:22m:50s remains)
INFO - root - 2017-12-07 19:40:36.317906: step 46630, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 55h:48m:58s remains)
INFO - root - 2017-12-07 19:40:43.066912: step 46640, loss = 2.03, batch loss = 1.97 (11.0 examples/sec; 0.730 sec/batch; 57h:56m:54s remains)
INFO - root - 2017-12-07 19:40:49.899570: step 46650, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 56h:25m:12s remains)
INFO - root - 2017-12-07 19:40:56.703496: step 46660, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 53h:51m:05s remains)
INFO - root - 2017-12-07 19:41:03.550422: step 46670, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 51h:11m:42s remains)
INFO - root - 2017-12-07 19:41:10.379380: step 46680, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 54h:32m:08s remains)
INFO - root - 2017-12-07 19:41:17.194685: step 46690, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 56h:16m:34s remains)
INFO - root - 2017-12-07 19:41:23.950789: step 46700, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.675 sec/batch; 53h:33m:32s remains)
2017-12-07 19:41:24.719988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2350807 -4.188693 -4.12717 -4.07699 -4.0654731 -4.0882297 -4.1124578 -4.128809 -4.1346121 -4.1466308 -4.1677327 -4.1877952 -4.1944985 -4.18941 -4.1860609][-4.2496567 -4.211247 -4.1494818 -4.0909119 -4.058044 -4.0580177 -4.0766487 -4.1090593 -4.1450253 -4.1809425 -4.2102785 -4.2343006 -4.2436609 -4.2407808 -4.2384086][-4.2616034 -4.2358856 -4.1811204 -4.1163917 -4.06021 -4.0262289 -4.0260825 -4.0686264 -4.1372886 -4.2014375 -4.2414918 -4.2685924 -4.279099 -4.2770514 -4.2749329][-4.2713556 -4.25986 -4.2178245 -4.1529784 -4.0771337 -4.0074048 -3.9773822 -4.0152845 -4.1016216 -4.1854248 -4.2381721 -4.2728429 -4.2866096 -4.2851858 -4.2818508][-4.280601 -4.2759204 -4.2441006 -4.1851387 -4.099082 -4.0022264 -3.9357128 -3.9528785 -4.044579 -4.14441 -4.2115736 -4.257483 -4.2778397 -4.2765861 -4.2702723][-4.2996287 -4.2935143 -4.2634873 -4.2052054 -4.1173158 -4.0063038 -3.9097595 -3.902427 -3.9943056 -4.1109438 -4.193264 -4.25133 -4.276123 -4.2731881 -4.2641897][-4.3110065 -4.29742 -4.2644587 -4.2071724 -4.1299491 -4.0269294 -3.9335265 -3.9223886 -4.0095344 -4.1256876 -4.2095 -4.2657256 -4.2863531 -4.2755151 -4.258781][-4.3035073 -4.2791696 -4.238143 -4.1811028 -4.1208272 -4.0487137 -3.9938397 -4.0019703 -4.0791483 -4.1780229 -4.2470379 -4.286397 -4.2921267 -4.2699981 -4.245677][-4.2831845 -4.2500572 -4.1993985 -4.1381264 -4.0900903 -4.0531974 -4.0438929 -4.0777626 -4.1495132 -4.2292657 -4.2809262 -4.3019056 -4.2923565 -4.2592149 -4.2257972][-4.26421 -4.2220335 -4.1642246 -4.0931182 -4.0439396 -4.02607 -4.053503 -4.1147385 -4.1868024 -4.2528911 -4.2952065 -4.3029761 -4.28144 -4.2370982 -4.1940165][-4.2536411 -4.2124629 -4.1552315 -4.0807562 -4.0228419 -4.0048323 -4.0477376 -4.1256752 -4.1997938 -4.258019 -4.2947106 -4.2980862 -4.2721038 -4.2186971 -4.1694446][-4.2498808 -4.2159214 -4.1659036 -4.0959358 -4.0339394 -4.014833 -4.0641718 -4.1492147 -4.2233257 -4.2756433 -4.309772 -4.3111558 -4.2849889 -4.23354 -4.184267][-4.2540655 -4.2340155 -4.1964355 -4.1387587 -4.0834546 -4.0676427 -4.1166353 -4.19425 -4.2600174 -4.3019671 -4.3300948 -4.3304844 -4.3085847 -4.2673979 -4.2235613][-4.2455359 -4.2407293 -4.21905 -4.1795216 -4.1401768 -4.134387 -4.1798906 -4.2445569 -4.2949347 -4.3189368 -4.3337512 -4.3318219 -4.315495 -4.289259 -4.2601719][-4.2219057 -4.2259765 -4.2136927 -4.1884351 -4.1622858 -4.1683226 -4.2128105 -4.2656178 -4.3019052 -4.312356 -4.3159685 -4.3106194 -4.2982068 -4.2813025 -4.26756]]...]
INFO - root - 2017-12-07 19:41:31.283654: step 46710, loss = 2.06, batch loss = 2.00 (14.5 examples/sec; 0.553 sec/batch; 43h:51m:46s remains)
INFO - root - 2017-12-07 19:41:38.035612: step 46720, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 52h:31m:23s remains)
INFO - root - 2017-12-07 19:41:44.857131: step 46730, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 53h:45m:02s remains)
INFO - root - 2017-12-07 19:41:51.611094: step 46740, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 51h:04m:27s remains)
INFO - root - 2017-12-07 19:41:58.264640: step 46750, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:23m:55s remains)
INFO - root - 2017-12-07 19:42:05.011348: step 46760, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 53h:29m:05s remains)
INFO - root - 2017-12-07 19:42:11.772006: step 46770, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 51h:34m:41s remains)
INFO - root - 2017-12-07 19:42:18.550766: step 46780, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 51h:13m:43s remains)
INFO - root - 2017-12-07 19:42:25.408678: step 46790, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 57h:24m:16s remains)
INFO - root - 2017-12-07 19:42:32.292819: step 46800, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 56h:47m:24s remains)
2017-12-07 19:42:32.942061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2793627 -4.270689 -4.2664833 -4.27571 -4.2827997 -4.2800632 -4.2736368 -4.270956 -4.2685952 -4.2638626 -4.260695 -4.2555594 -4.2546663 -4.26694 -4.2828841][-4.2418242 -4.2320714 -4.22995 -4.2482281 -4.2651153 -4.2650962 -4.2542048 -4.2469983 -4.2403479 -4.2305708 -4.2229705 -4.2138762 -4.210485 -4.2225275 -4.2411556][-4.2001982 -4.1916037 -4.1937704 -4.2197361 -4.2439952 -4.2430663 -4.2244487 -4.2110696 -4.2007928 -4.1853504 -4.1735249 -4.1637774 -4.1628094 -4.1786194 -4.2019448][-4.1617336 -4.1520438 -4.1587725 -4.1860318 -4.2110062 -4.2039385 -4.1732416 -4.1519213 -4.1408992 -4.1229076 -4.1109805 -4.1079159 -4.1169848 -4.1443954 -4.1761293][-4.1301961 -4.1179876 -4.1245909 -4.146616 -4.1664548 -4.1505 -4.1062908 -4.0806689 -4.0751991 -4.0623593 -4.0558405 -4.0605211 -4.0808468 -4.1207733 -4.1633482][-4.1136551 -4.0930095 -4.0904026 -4.0988665 -4.1054192 -4.0736933 -4.010571 -3.9826558 -3.9932077 -3.997716 -4.0051966 -4.0228343 -4.0584126 -4.1123104 -4.1647387][-4.1237459 -4.0940175 -4.0772195 -4.0730209 -4.0653944 -4.0174589 -3.9376657 -3.9085851 -3.9370375 -3.9635806 -3.9886351 -4.0200658 -4.068469 -4.1296668 -4.1831727][-4.1524887 -4.1180429 -4.0904608 -4.07921 -4.0707035 -4.02741 -3.9538589 -3.9307244 -3.9677291 -4.0040903 -4.0367689 -4.0686717 -4.1091762 -4.1590667 -4.202796][-4.1625094 -4.1271558 -4.0963349 -4.0869107 -4.0926528 -4.0754538 -4.030087 -4.0122313 -4.0340447 -4.0587626 -4.0862551 -4.1122746 -4.1371589 -4.1693869 -4.2025828][-4.1575222 -4.1211443 -4.0887089 -4.0838795 -4.1041231 -4.1140013 -4.0943432 -4.0761256 -4.0786433 -4.0893517 -4.1125903 -4.13793 -4.1525078 -4.1664453 -4.1898179][-4.1512175 -4.1144786 -4.0810533 -4.0778918 -4.1032434 -4.1284003 -4.1240907 -4.1054788 -4.0968728 -4.0998135 -4.1210451 -4.1493526 -4.1623631 -4.1658382 -4.1835809][-4.145793 -4.1024318 -4.0617709 -4.0531716 -4.0786023 -4.1091156 -4.1118221 -4.0940423 -4.0806179 -4.0782061 -4.0987411 -4.1318612 -4.1482835 -4.1530509 -4.1745458][-4.1396976 -4.0921941 -4.04396 -4.02587 -4.0445533 -4.0706754 -4.0753126 -4.06075 -4.0474205 -4.0426812 -4.0639777 -4.1022482 -4.1251464 -4.1357574 -4.1633334][-4.1545143 -4.1140118 -4.0697412 -4.05147 -4.0650735 -4.0827193 -4.0845838 -4.0695367 -4.0555639 -4.0497212 -4.0671077 -4.1032777 -4.1309147 -4.148777 -4.1780438][-4.1826334 -4.1589336 -4.1301069 -4.119545 -4.1295319 -4.1395564 -4.1407728 -4.1293864 -4.1174068 -4.110497 -4.1192532 -4.1451859 -4.1726131 -4.1956267 -4.2248764]]...]
INFO - root - 2017-12-07 19:42:39.485999: step 46810, loss = 2.09, batch loss = 2.04 (18.2 examples/sec; 0.438 sec/batch; 34h:47m:51s remains)
INFO - root - 2017-12-07 19:42:46.414299: step 46820, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 56h:42m:29s remains)
INFO - root - 2017-12-07 19:42:53.335483: step 46830, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 55h:17m:33s remains)
INFO - root - 2017-12-07 19:43:00.048564: step 46840, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 51h:17m:38s remains)
INFO - root - 2017-12-07 19:43:06.925451: step 46850, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 52h:29m:05s remains)
INFO - root - 2017-12-07 19:43:13.707150: step 46860, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 53h:53m:10s remains)
INFO - root - 2017-12-07 19:43:20.529784: step 46870, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 58h:45m:29s remains)
INFO - root - 2017-12-07 19:43:27.337797: step 46880, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 56h:31m:15s remains)
INFO - root - 2017-12-07 19:43:33.993697: step 46890, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 52h:03m:10s remains)
INFO - root - 2017-12-07 19:43:40.733359: step 46900, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 51h:32m:03s remains)
2017-12-07 19:43:41.484596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.14961 -4.1260943 -4.09254 -4.080009 -4.0977955 -4.1188736 -4.1191764 -4.1055365 -4.0932174 -4.0939288 -4.0998077 -4.1003847 -4.0884666 -4.0813112 -4.0954018][-4.1890621 -4.1726613 -4.1402626 -4.1221728 -4.1332707 -4.1467814 -4.1399879 -4.1260862 -4.1225553 -4.1307015 -4.1371231 -4.1332979 -4.1152725 -4.1020484 -4.1105871][-4.2105236 -4.1932669 -4.1618309 -4.1436467 -4.1549592 -4.1685791 -4.1577625 -4.144937 -4.1488671 -4.1628122 -4.1720309 -4.171751 -4.1522212 -4.13463 -4.1360393][-4.2030306 -4.1866531 -4.1588917 -4.1466804 -4.168272 -4.1883211 -4.172255 -4.1511335 -4.1497211 -4.163106 -4.1799831 -4.1890035 -4.1731791 -4.1521664 -4.147501][-4.1566558 -4.1423712 -4.1217327 -4.1247191 -4.1633162 -4.1880403 -4.1630273 -4.1221166 -4.10598 -4.12084 -4.1519952 -4.1787043 -4.1731119 -4.1542177 -4.1461873][-4.1005726 -4.0899305 -4.079988 -4.1017156 -4.1535487 -4.1763535 -4.1387615 -4.0754833 -4.0397038 -4.05506 -4.104682 -4.1488409 -4.1551414 -4.1411343 -4.1348152][-4.0643649 -4.053895 -4.0470195 -4.0747657 -4.1282358 -4.1456957 -4.0948896 -4.0140586 -3.9647088 -3.9833169 -4.0515261 -4.1100216 -4.1221142 -4.1076765 -4.0982513][-4.079864 -4.0619106 -4.0501528 -4.071393 -4.1067491 -4.0977726 -4.0185127 -3.9070554 -3.8406982 -3.8689172 -3.9631896 -4.0434279 -4.0709295 -4.06516 -4.0571451][-4.1326494 -4.1166983 -4.113029 -4.1309896 -4.1424026 -4.1026926 -3.9954648 -3.8615932 -3.7822719 -3.8076565 -3.9075851 -4.0014668 -4.044847 -4.047864 -4.0405135][-4.1952353 -4.1895342 -4.1926432 -4.2038126 -4.2004161 -4.155684 -4.0619993 -3.9535527 -3.8901155 -3.8936112 -3.9542613 -4.0226693 -4.0565486 -4.0551314 -4.04358][-4.2437634 -4.2431536 -4.2425089 -4.24367 -4.2322149 -4.1938949 -4.1219645 -4.04188 -3.9947329 -3.9896407 -4.0206785 -4.0607243 -4.0796409 -4.0716949 -4.0572619][-4.2756729 -4.2785535 -4.2748947 -4.2699289 -4.2561655 -4.223371 -4.1663632 -4.10755 -4.0757666 -4.0753474 -4.0951681 -4.1176572 -4.1232829 -4.1050105 -4.0853233][-4.3031816 -4.3093138 -4.3040504 -4.2966647 -4.2833085 -4.2559738 -4.2114458 -4.1682019 -4.146379 -4.15072 -4.166976 -4.1789479 -4.1751537 -4.1507683 -4.1292744][-4.3032527 -4.3144546 -4.3117723 -4.3067608 -4.2933979 -4.2676868 -4.2314038 -4.1993322 -4.1838508 -4.1907668 -4.2042885 -4.2118206 -4.207963 -4.1903429 -4.1740789][-4.28245 -4.2964821 -4.3005633 -4.3029819 -4.2941203 -4.2723961 -4.2438731 -4.22205 -4.2143049 -4.2224474 -4.2330418 -4.2365537 -4.2332168 -4.2228026 -4.2131968]]...]
INFO - root - 2017-12-07 19:43:48.097254: step 46910, loss = 2.09, batch loss = 2.03 (14.0 examples/sec; 0.572 sec/batch; 45h:21m:22s remains)
INFO - root - 2017-12-07 19:43:54.816886: step 46920, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 53h:38m:27s remains)
INFO - root - 2017-12-07 19:44:01.454915: step 46930, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 52h:05m:43s remains)
INFO - root - 2017-12-07 19:44:08.177575: step 46940, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 52h:25m:03s remains)
INFO - root - 2017-12-07 19:44:14.938513: step 46950, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 56h:22m:34s remains)
INFO - root - 2017-12-07 19:44:21.767221: step 46960, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 55h:23m:56s remains)
INFO - root - 2017-12-07 19:44:28.463324: step 46970, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 54h:11m:21s remains)
INFO - root - 2017-12-07 19:44:35.224541: step 46980, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 50h:11m:21s remains)
INFO - root - 2017-12-07 19:44:41.949060: step 46990, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 54h:02m:08s remains)
INFO - root - 2017-12-07 19:44:48.796279: step 47000, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 55h:49m:40s remains)
2017-12-07 19:44:49.533672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2121921 -4.2165103 -4.2140355 -4.2060256 -4.1955028 -4.186214 -4.1849794 -4.1874375 -4.1877632 -4.1943283 -4.207974 -4.2199016 -4.2112894 -4.1968441 -4.1940246][-4.2047791 -4.2123923 -4.211751 -4.2061276 -4.19822 -4.1896629 -4.190299 -4.1984172 -4.2075682 -4.2201319 -4.2354441 -4.2476759 -4.2354317 -4.2180028 -4.2115116][-4.1979508 -4.2062359 -4.2078276 -4.2041979 -4.1971169 -4.1930366 -4.1989584 -4.2130313 -4.2300653 -4.2421851 -4.2529378 -4.2601647 -4.2473 -4.229969 -4.2215266][-4.2110152 -4.2166538 -4.214642 -4.2084112 -4.201664 -4.2005463 -4.2101474 -4.2267795 -4.2462 -4.255115 -4.2601085 -4.2629056 -4.2507472 -4.2352915 -4.2296224][-4.2285371 -4.2289424 -4.2246709 -4.21904 -4.2155333 -4.2146297 -4.2204781 -4.23515 -4.2512794 -4.2551317 -4.2518892 -4.2495065 -4.2390642 -4.2266665 -4.2238712][-4.244329 -4.238028 -4.2282228 -4.2179475 -4.2101846 -4.205802 -4.202774 -4.2100263 -4.2221994 -4.224781 -4.2180133 -4.2158155 -4.2121973 -4.206821 -4.2085485][-4.2423477 -4.2293148 -4.2109585 -4.1933422 -4.1760192 -4.1601992 -4.1423092 -4.1368895 -4.1496067 -4.1553078 -4.1536889 -4.1630664 -4.17398 -4.1789742 -4.1852584][-4.2098188 -4.1905427 -4.1660514 -4.144897 -4.120801 -4.0922184 -4.05376 -4.030098 -4.0418019 -4.058157 -4.0720973 -4.1019864 -4.1343665 -4.1534481 -4.1650472][-4.1556683 -4.1300364 -4.103507 -4.0825782 -4.0599222 -4.0237813 -3.9649606 -3.9277558 -3.9547427 -4.003664 -4.0438881 -4.0864577 -4.1273065 -4.1519217 -4.16528][-4.0875826 -4.0594344 -4.0430789 -4.0334558 -4.0232506 -3.9967439 -3.9405165 -3.9069467 -3.9545646 -4.0309744 -4.0835085 -4.1193857 -4.1497321 -4.1712518 -4.181848][-4.0378342 -4.0212173 -4.0221281 -4.02847 -4.0339088 -4.0276279 -3.9962423 -3.9853885 -4.034585 -4.1076574 -4.1508718 -4.1719666 -4.1868482 -4.1981373 -4.1990967][-4.0336132 -4.0337667 -4.0466003 -4.0586224 -4.0722246 -4.0805283 -4.0726438 -4.0806 -4.1229482 -4.1746745 -4.1993289 -4.2086372 -4.2138481 -4.2160811 -4.2102141][-4.0551252 -4.0585523 -4.0767479 -4.0913181 -4.1068792 -4.1222243 -4.1288266 -4.1439133 -4.1761165 -4.2067466 -4.2200632 -4.2211847 -4.219696 -4.2182388 -4.2161851][-4.0566635 -4.0606394 -4.0830555 -4.1036234 -4.1223416 -4.1443019 -4.1584325 -4.1710396 -4.1884532 -4.2049384 -4.2176275 -4.2217159 -4.2189922 -4.2142725 -4.214581][-4.0366516 -4.0508037 -4.0827866 -4.1098213 -4.1348953 -4.1609859 -4.1760221 -4.1828423 -4.1897798 -4.1999388 -4.2152176 -4.2254491 -4.2226877 -4.2121735 -4.2113771]]...]
INFO - root - 2017-12-07 19:44:56.036722: step 47010, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.499 sec/batch; 39h:35m:28s remains)
INFO - root - 2017-12-07 19:45:02.976162: step 47020, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 56h:18m:51s remains)
INFO - root - 2017-12-07 19:45:09.848429: step 47030, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 55h:00m:52s remains)
INFO - root - 2017-12-07 19:45:16.666607: step 47040, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 53h:17m:07s remains)
INFO - root - 2017-12-07 19:45:23.379127: step 47050, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 52h:01m:28s remains)
INFO - root - 2017-12-07 19:45:30.110594: step 47060, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.672 sec/batch; 53h:15m:54s remains)
INFO - root - 2017-12-07 19:45:36.921713: step 47070, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 58h:36m:27s remains)
INFO - root - 2017-12-07 19:45:43.662389: step 47080, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 53h:22m:40s remains)
INFO - root - 2017-12-07 19:45:50.453666: step 47090, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:30m:31s remains)
INFO - root - 2017-12-07 19:45:57.284209: step 47100, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 56h:00m:52s remains)
2017-12-07 19:45:58.069714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.321353 -4.3209286 -4.3195653 -4.317544 -4.3163338 -4.316421 -4.3174863 -4.3190174 -4.3198609 -4.319809 -4.3189898 -4.3178325 -4.3174505 -4.3177881 -4.3182354][-4.3293695 -4.3282533 -4.3250427 -4.3202138 -4.3159609 -4.3138852 -4.3144007 -4.3168807 -4.3198252 -4.3219233 -4.3233762 -4.3239365 -4.3251958 -4.3265166 -4.3268156][-4.3347769 -4.3324347 -4.3265185 -4.3162417 -4.3048525 -4.2961183 -4.2927527 -4.2944818 -4.2991705 -4.3047638 -4.3118324 -4.3189993 -4.327065 -4.3334332 -4.3360567][-4.3375673 -4.3342791 -4.3235083 -4.3029432 -4.2772503 -4.2547092 -4.2425137 -4.2392282 -4.2433343 -4.253407 -4.2707777 -4.2912974 -4.3137765 -4.3312263 -4.3400574][-4.3342595 -4.3281846 -4.3101368 -4.2755914 -4.2307096 -4.1888061 -4.161768 -4.1463223 -4.143611 -4.1565065 -4.188395 -4.229311 -4.2749381 -4.3114281 -4.3319278][-4.3153105 -4.30384 -4.2770967 -4.2283807 -4.1658769 -4.105422 -4.0605745 -4.0242147 -4.004117 -4.0159497 -4.0665483 -4.136024 -4.2128506 -4.2750812 -4.3123274][-4.2760782 -4.2600684 -4.22832 -4.1730056 -4.1008558 -4.0269732 -3.9634383 -3.8967512 -3.8481965 -3.8569322 -3.9288514 -4.029356 -4.1391287 -4.2289538 -4.2852902][-4.2148905 -4.200479 -4.1747637 -4.1266994 -4.0582256 -3.9817016 -3.9044709 -3.8076749 -3.7296689 -3.7365832 -3.8305421 -3.9580264 -4.09208 -4.2002358 -4.267992][-4.1441007 -4.141849 -4.1355724 -4.1116109 -4.0643291 -4.0034232 -3.9297621 -3.8243527 -3.7347379 -3.7357709 -3.8289196 -3.9571669 -4.0922723 -4.2026057 -4.2676105][-4.1070046 -4.1240082 -4.1413474 -4.1453075 -4.1270766 -4.0921941 -4.0376091 -3.9487135 -3.8717966 -3.8661766 -3.9340165 -4.0356746 -4.1454616 -4.2348528 -4.2823348][-4.142508 -4.169251 -4.1953259 -4.2110467 -4.2112412 -4.1981754 -4.1670332 -4.1068144 -4.0535951 -4.0464234 -4.0875545 -4.1534362 -4.2244506 -4.2807183 -4.3047323][-4.2116728 -4.2359877 -4.2573376 -4.2703824 -4.2745113 -4.2719092 -4.2586088 -4.2271681 -4.1975579 -4.1911116 -4.2109809 -4.2447047 -4.2800078 -4.3065381 -4.3125458][-4.271678 -4.2884059 -4.300025 -4.3047562 -4.3054624 -4.30511 -4.3021851 -4.2918134 -4.2798624 -4.2767415 -4.2834496 -4.2938356 -4.3027282 -4.3074269 -4.3022642][-4.2994623 -4.3085837 -4.3116736 -4.3084831 -4.3045216 -4.3034019 -4.3049431 -4.3059082 -4.3046832 -4.3042741 -4.3043809 -4.2999654 -4.2917032 -4.2831078 -4.273026][-4.2986007 -4.3025265 -4.299499 -4.290133 -4.2819653 -4.2793293 -4.2828813 -4.2885046 -4.2907567 -4.2899923 -4.2854037 -4.2736945 -4.2580194 -4.2459216 -4.2381792]]...]
INFO - root - 2017-12-07 19:46:04.595888: step 47110, loss = 2.05, batch loss = 1.99 (15.8 examples/sec; 0.506 sec/batch; 40h:06m:57s remains)
INFO - root - 2017-12-07 19:46:11.460524: step 47120, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 52h:34m:20s remains)
INFO - root - 2017-12-07 19:46:18.417081: step 47130, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 56h:48m:08s remains)
INFO - root - 2017-12-07 19:46:25.274536: step 47140, loss = 2.09, batch loss = 2.04 (10.8 examples/sec; 0.744 sec/batch; 58h:56m:21s remains)
INFO - root - 2017-12-07 19:46:32.081953: step 47150, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 53h:26m:56s remains)
INFO - root - 2017-12-07 19:46:38.884403: step 47160, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 55h:20m:44s remains)
INFO - root - 2017-12-07 19:46:45.648276: step 47170, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 52h:08m:06s remains)
INFO - root - 2017-12-07 19:46:52.524440: step 47180, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 54h:36m:30s remains)
INFO - root - 2017-12-07 19:46:59.357746: step 47190, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 57h:08m:23s remains)
INFO - root - 2017-12-07 19:47:06.168726: step 47200, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 52h:10m:04s remains)
2017-12-07 19:47:06.901722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2653527 -4.2561679 -4.2463851 -4.2371736 -4.2312684 -4.2299004 -4.232224 -4.236176 -4.2409244 -4.2474737 -4.2552667 -4.2648549 -4.2728815 -4.2779107 -4.2820792][-4.2536941 -4.2420821 -4.229486 -4.2174139 -4.2101536 -4.20833 -4.208786 -4.2096939 -4.2140746 -4.22347 -4.2360134 -4.24951 -4.258379 -4.2638612 -4.2697997][-4.2327771 -4.2217937 -4.2077827 -4.192462 -4.1808367 -4.176352 -4.1752734 -4.1750064 -4.1802235 -4.1944308 -4.2122536 -4.2281637 -4.2356358 -4.2419891 -4.2493887][-4.2011027 -4.1957226 -4.184617 -4.1668487 -4.1510787 -4.1416054 -4.1381197 -4.1371684 -4.1440926 -4.1620941 -4.1835818 -4.2022038 -4.212647 -4.2214971 -4.2295341][-4.1771555 -4.1792989 -4.1706381 -4.1476607 -4.1247635 -4.1109462 -4.1089535 -4.1112924 -4.1186676 -4.1371865 -4.157424 -4.1769743 -4.19304 -4.2058177 -4.2160187][-4.1652055 -4.1670246 -4.1539888 -4.1200423 -4.0843515 -4.0672321 -4.0756235 -4.0884824 -4.0984421 -4.11523 -4.134635 -4.1550379 -4.1744647 -4.1919174 -4.2055955][-4.1513228 -4.1450744 -4.1192155 -4.0707965 -4.0165906 -3.9909525 -4.0153027 -4.0405111 -4.0535784 -4.069437 -4.0946527 -4.1246958 -4.1524992 -4.1730914 -4.1901417][-4.1421909 -4.1345367 -4.1085076 -4.0599513 -3.9976315 -3.9620993 -3.9856994 -4.0105486 -4.0190077 -4.0310936 -4.0613918 -4.0989933 -4.1334867 -4.15799 -4.1791267][-4.1517792 -4.1491175 -4.1369438 -4.1084609 -4.0653133 -4.0314822 -4.0344496 -4.0392451 -4.0325794 -4.0379825 -4.0694518 -4.1072969 -4.1439228 -4.1683564 -4.1904168][-4.1874418 -4.1893616 -4.1899009 -4.1811066 -4.1592 -4.1372547 -4.1301913 -4.1199479 -4.1015134 -4.0999122 -4.1239228 -4.1507869 -4.178874 -4.1996284 -4.2200208][-4.2292724 -4.22921 -4.2306404 -4.2307591 -4.2236838 -4.2128143 -4.2069445 -4.1932297 -4.171412 -4.1671672 -4.1827326 -4.2006073 -4.2219319 -4.2432065 -4.2617116][-4.2624893 -4.2594261 -4.2576385 -4.2581296 -4.2560992 -4.2506418 -4.2453303 -4.2313709 -4.2123566 -4.2088275 -4.2216125 -4.2376575 -4.2572842 -4.2771473 -4.2923217][-4.2757173 -4.2709608 -4.266233 -4.2683249 -4.272274 -4.273818 -4.2716618 -4.2637587 -4.2520032 -4.248621 -4.2582173 -4.2717595 -4.2895422 -4.3055534 -4.3129067][-4.26462 -4.2594657 -4.255208 -4.2609696 -4.2694678 -4.2756634 -4.2781043 -4.277936 -4.2773509 -4.2784228 -4.2860208 -4.2971096 -4.3088465 -4.3153558 -4.3131671][-4.2437391 -4.2419105 -4.2419019 -4.2501278 -4.2606645 -4.2677011 -4.2717652 -4.2760811 -4.2840419 -4.2925944 -4.3023262 -4.3104653 -4.31538 -4.314991 -4.3087192]]...]
INFO - root - 2017-12-07 19:47:13.496299: step 47210, loss = 2.06, batch loss = 2.00 (14.8 examples/sec; 0.539 sec/batch; 42h:44m:04s remains)
INFO - root - 2017-12-07 19:47:20.379580: step 47220, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 57h:00m:14s remains)
INFO - root - 2017-12-07 19:47:27.219288: step 47230, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 53h:26m:57s remains)
INFO - root - 2017-12-07 19:47:33.990697: step 47240, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.633 sec/batch; 50h:08m:48s remains)
INFO - root - 2017-12-07 19:47:40.811333: step 47250, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 51h:54m:56s remains)
INFO - root - 2017-12-07 19:47:47.723776: step 47260, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 55h:45m:33s remains)
INFO - root - 2017-12-07 19:47:54.448721: step 47270, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 51h:25m:35s remains)
INFO - root - 2017-12-07 19:48:01.304586: step 47280, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 50h:50m:55s remains)
INFO - root - 2017-12-07 19:48:08.247998: step 47290, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 54h:06m:29s remains)
INFO - root - 2017-12-07 19:48:15.084149: step 47300, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 55h:46m:58s remains)
2017-12-07 19:48:15.766512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281779 -4.3226285 -4.3164735 -4.3128614 -4.3134537 -4.3178773 -4.3250809 -4.3301377 -4.3303962 -4.3238335 -4.3158112 -4.3144274 -4.321373 -4.3322458 -4.3431406][-4.3163333 -4.3092213 -4.3009238 -4.2938004 -4.2893705 -4.2903762 -4.2992234 -4.3097596 -4.3152246 -4.310524 -4.3002634 -4.29626 -4.3028593 -4.3164649 -4.331696][-4.2964296 -4.2900105 -4.2810645 -4.2707863 -4.2582445 -4.250814 -4.2565126 -4.2730703 -4.2895417 -4.2957797 -4.2919116 -4.2891431 -4.2937536 -4.3050704 -4.320931][-4.2701139 -4.2651606 -4.2555513 -4.2402697 -4.2170119 -4.1969795 -4.1952128 -4.2159309 -4.2454391 -4.2701168 -4.2832279 -4.2906938 -4.2972188 -4.3050437 -4.3181992][-4.2497663 -4.2447677 -4.2316494 -4.2059393 -4.1655645 -4.1213579 -4.1010809 -4.1212239 -4.1684914 -4.2171783 -4.2537222 -4.2810116 -4.2990413 -4.3105555 -4.3226919][-4.2417092 -4.2359042 -4.2142873 -4.1724687 -4.111145 -4.0343285 -3.9783392 -3.9854672 -4.0526023 -4.1313672 -4.1938691 -4.242486 -4.2798543 -4.3060904 -4.3252616][-4.2250762 -4.2174191 -4.1897354 -4.1410151 -4.0677981 -3.9622028 -3.8575 -3.8290992 -3.9041319 -4.0099468 -4.0995789 -4.1673636 -4.2251849 -4.2735915 -4.309432][-4.2047167 -4.1935797 -4.1654863 -4.1185818 -4.0487547 -3.941833 -3.8204453 -3.7521613 -3.792964 -3.8870444 -3.9810169 -4.0625229 -4.1392841 -4.2109623 -4.2677197][-4.1946044 -4.17842 -4.1523986 -4.1166387 -4.0629811 -3.9814785 -3.8922138 -3.8228941 -3.8059835 -3.8353362 -3.891216 -3.9630442 -4.0462689 -4.1329036 -4.2102356][-4.2110486 -4.1927977 -4.1713114 -4.1504755 -4.1189828 -4.0679836 -4.0154066 -3.9637089 -3.9173636 -3.8934479 -3.9025786 -3.9442794 -4.0100527 -4.0869503 -4.1660414][-4.2584777 -4.2447171 -4.2310495 -4.2244606 -4.2158465 -4.1906915 -4.1606159 -4.1223631 -4.0738573 -4.0343261 -4.0162253 -4.0262265 -4.0624895 -4.1146059 -4.1746073][-4.3061132 -4.2999773 -4.2928882 -4.2937427 -4.2974238 -4.2893982 -4.2745609 -4.2505231 -4.2164083 -4.1841316 -4.1593013 -4.149437 -4.1610551 -4.1875505 -4.2216568][-4.336267 -4.3354635 -4.3334985 -4.3353739 -4.3405452 -4.3395214 -4.3341804 -4.3222394 -4.3057618 -4.2879729 -4.2672915 -4.2515287 -4.2499337 -4.2588325 -4.272809][-4.3499479 -4.352694 -4.3537016 -4.354826 -4.3570495 -4.3568625 -4.3539796 -4.3489656 -4.3428173 -4.3356023 -4.3245406 -4.3139882 -4.3090253 -4.3084025 -4.310009][-4.35685 -4.3595982 -4.3611989 -4.3618693 -4.3616357 -4.3599763 -4.3580346 -4.3565292 -4.355515 -4.3534918 -4.3483415 -4.3423724 -4.3380322 -4.3345494 -4.3326826]]...]
INFO - root - 2017-12-07 19:48:22.323395: step 47310, loss = 2.05, batch loss = 2.00 (16.3 examples/sec; 0.492 sec/batch; 38h:56m:21s remains)
INFO - root - 2017-12-07 19:48:29.269280: step 47320, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 56h:11m:36s remains)
INFO - root - 2017-12-07 19:48:36.103577: step 47330, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 55h:47m:21s remains)
INFO - root - 2017-12-07 19:48:42.906590: step 47340, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 54h:40m:41s remains)
INFO - root - 2017-12-07 19:48:49.624628: step 47350, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 53h:03m:48s remains)
INFO - root - 2017-12-07 19:48:56.344961: step 47360, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 50h:51m:46s remains)
INFO - root - 2017-12-07 19:49:03.038456: step 47370, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 51h:51m:36s remains)
INFO - root - 2017-12-07 19:49:09.819828: step 47380, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 54h:59m:23s remains)
INFO - root - 2017-12-07 19:49:16.577395: step 47390, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 51h:06m:51s remains)
INFO - root - 2017-12-07 19:49:23.354916: step 47400, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 50h:47m:18s remains)
2017-12-07 19:49:24.036369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1298647 -4.1037173 -4.1098 -4.1379786 -4.1741114 -4.1967549 -4.1972637 -4.1734219 -4.1351004 -4.1103015 -4.1145406 -4.1306071 -4.1395106 -4.1400061 -4.1177368][-4.1087446 -4.0757685 -4.0907097 -4.1334257 -4.1783624 -4.2001848 -4.192224 -4.1572528 -4.1030531 -4.0661774 -4.0767789 -4.1089625 -4.1278138 -4.1339507 -4.1128407][-4.1061573 -4.0634775 -4.073523 -4.1152735 -4.160078 -4.1793666 -4.1676755 -4.1295071 -4.0694909 -4.0241251 -4.0326653 -4.0708327 -4.0970798 -4.1126714 -4.1026897][-4.1137104 -4.0728483 -4.0751085 -4.1050882 -4.145021 -4.16214 -4.1539516 -4.1215577 -4.0695996 -4.0261731 -4.0273337 -4.0524154 -4.0776076 -4.0978312 -4.1007404][-4.1198254 -4.0926485 -4.0916171 -4.1056433 -4.1366572 -4.1538954 -4.1556845 -4.1367178 -4.0974736 -4.0619197 -4.0544229 -4.0579596 -4.074512 -4.0964727 -4.1076918][-4.129014 -4.1162415 -4.1182885 -4.1202054 -4.136857 -4.1486344 -4.1565032 -4.1480856 -4.1184478 -4.090435 -4.0771565 -4.0696244 -4.0812454 -4.1070471 -4.1234074][-4.1382246 -4.1351562 -4.1420069 -4.1388803 -4.1442289 -4.1468 -4.1541476 -4.15184 -4.129343 -4.1094732 -4.0934668 -4.0816035 -4.0904307 -4.1143413 -4.1330967][-4.1401749 -4.1440482 -4.1558948 -4.1563697 -4.1620216 -4.1583948 -4.1625414 -4.1600761 -4.137022 -4.12 -4.1042948 -4.0936589 -4.1007481 -4.1174092 -4.1367579][-4.1393385 -4.1543603 -4.1733642 -4.1778164 -4.1833897 -4.1678138 -4.1637344 -4.159749 -4.1378336 -4.1237969 -4.1080294 -4.0997534 -4.1054916 -4.1156669 -4.1346231][-4.1400948 -4.1671462 -4.1916738 -4.1975851 -4.1952105 -4.1627975 -4.1458287 -4.1368866 -4.1168017 -4.10837 -4.0999632 -4.0968819 -4.1037769 -4.1128087 -4.1284361][-4.1322241 -4.1606326 -4.1870079 -4.1933951 -4.1838889 -4.1433825 -4.1212468 -4.1091962 -4.0928807 -4.0951705 -4.0973144 -4.097218 -4.1034112 -4.1120143 -4.128325][-4.1046772 -4.1306944 -4.1609149 -4.1691608 -4.1571465 -4.1229553 -4.1089206 -4.1008887 -4.0883508 -4.0934954 -4.1004124 -4.1058497 -4.1153569 -4.1296043 -4.1499953][-4.07949 -4.0979452 -4.121696 -4.1307316 -4.1227622 -4.10305 -4.10832 -4.1117492 -4.1013832 -4.1045523 -4.1155481 -4.1249914 -4.1397405 -4.16173 -4.1831532][-4.0784259 -4.0800624 -4.0895505 -4.0997887 -4.0996213 -4.0983229 -4.1139507 -4.1231251 -4.113503 -4.1162462 -4.1305413 -4.1464944 -4.1673884 -4.19214 -4.2094231][-4.091805 -4.081996 -4.0797834 -4.092443 -4.0976076 -4.1053677 -4.1158376 -4.1151304 -4.109395 -4.1204062 -4.1420164 -4.166357 -4.1908345 -4.2065272 -4.2093792]]...]
INFO - root - 2017-12-07 19:49:30.607187: step 47410, loss = 2.07, batch loss = 2.01 (15.7 examples/sec; 0.510 sec/batch; 40h:23m:54s remains)
INFO - root - 2017-12-07 19:49:37.444727: step 47420, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 51h:15m:49s remains)
INFO - root - 2017-12-07 19:49:44.229157: step 47430, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 51h:14m:29s remains)
INFO - root - 2017-12-07 19:49:51.057652: step 47440, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 56h:08m:03s remains)
INFO - root - 2017-12-07 19:49:57.873015: step 47450, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 58h:17m:35s remains)
INFO - root - 2017-12-07 19:50:04.651057: step 47460, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 52h:37m:36s remains)
INFO - root - 2017-12-07 19:50:11.317763: step 47470, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 50h:20m:43s remains)
INFO - root - 2017-12-07 19:50:18.142696: step 47480, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 53h:31m:10s remains)
INFO - root - 2017-12-07 19:50:25.039442: step 47490, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 58h:13m:52s remains)
INFO - root - 2017-12-07 19:50:31.772785: step 47500, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 54h:31m:41s remains)
2017-12-07 19:50:32.427658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1988077 -4.1764703 -4.1563916 -4.1500525 -4.162652 -4.1791248 -4.1911149 -4.19436 -4.1938152 -4.1976428 -4.1934714 -4.1865726 -4.1794286 -4.1751938 -4.1785779][-4.1800547 -4.1457071 -4.1187377 -4.11014 -4.1248322 -4.1473789 -4.1669526 -4.1774259 -4.187808 -4.1982036 -4.1915 -4.179553 -4.1674933 -4.1642985 -4.173594][-4.1527987 -4.1135912 -4.0872622 -4.0798178 -4.0962095 -4.1251364 -4.1544619 -4.1724029 -4.1871896 -4.1960807 -4.1830397 -4.1654668 -4.1501355 -4.1474762 -4.1612749][-4.1177478 -4.0829878 -4.0679445 -4.0726886 -4.0908456 -4.1152487 -4.1441078 -4.1694908 -4.1915488 -4.1988859 -4.1796565 -4.1515541 -4.126884 -4.1194944 -4.134357][-4.06925 -4.0441666 -4.0443873 -4.0650353 -4.0812488 -4.0948949 -4.1170897 -4.1470852 -4.17896 -4.1906013 -4.173038 -4.1396637 -4.1110921 -4.105722 -4.1233377][-4.0215921 -4.0081091 -4.0203657 -4.0485721 -4.0556231 -4.0499277 -4.0622163 -4.0921025 -4.1331029 -4.1578846 -4.149972 -4.1214142 -4.1018925 -4.1081181 -4.1334352][-4.025291 -4.0171247 -4.0277805 -4.0425406 -4.02484 -3.9885333 -3.9850075 -4.0150685 -4.068058 -4.11119 -4.1190271 -4.1056042 -4.1029186 -4.1206269 -4.1501975][-4.0797319 -4.0698223 -4.0688763 -4.0659838 -4.0255694 -3.9627686 -3.9406965 -3.9637299 -4.0241146 -4.0800171 -4.0991912 -4.101624 -4.1146355 -4.1391463 -4.16958][-4.1453176 -4.1326995 -4.1243472 -4.1113133 -4.065074 -3.9982858 -3.9672112 -3.979908 -4.0316463 -4.0820942 -4.10052 -4.1123915 -4.1348095 -4.1557016 -4.179512][-4.2013216 -4.1873727 -4.1780767 -4.1624684 -4.1202354 -4.0668883 -4.0402355 -4.0450354 -4.0807281 -4.1144848 -4.1261616 -4.1380334 -4.1597743 -4.1724539 -4.1881275][-4.2477288 -4.2365441 -4.2292333 -4.213686 -4.1788139 -4.1409254 -4.1205049 -4.1182756 -4.1359324 -4.155086 -4.1619554 -4.1709437 -4.1869755 -4.1927748 -4.200326][-4.2731533 -4.2657838 -4.2612138 -4.2500682 -4.2248383 -4.1995187 -4.1860938 -4.1791911 -4.18171 -4.189465 -4.19499 -4.1993432 -4.2046709 -4.2041216 -4.2067695][-4.2884693 -4.2817111 -4.2761316 -4.2676582 -4.2524319 -4.2384214 -4.2317629 -4.2252212 -4.2197795 -4.217977 -4.2171016 -4.213531 -4.2057176 -4.1966872 -4.1963472][-4.3039484 -4.2971373 -4.2887235 -4.2782421 -4.2657437 -4.2574525 -4.2545748 -4.2489581 -4.2413807 -4.2364192 -4.2264462 -4.210475 -4.1875319 -4.1709032 -4.1670284][-4.317421 -4.3117757 -4.303165 -4.2921839 -4.2790265 -4.2693415 -4.2628241 -4.2544785 -4.24664 -4.2425551 -4.2272592 -4.1990967 -4.1635442 -4.1383705 -4.1313934]]...]
INFO - root - 2017-12-07 19:50:39.066619: step 47510, loss = 2.10, batch loss = 2.04 (14.3 examples/sec; 0.559 sec/batch; 44h:13m:28s remains)
INFO - root - 2017-12-07 19:50:45.942743: step 47520, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 57h:30m:02s remains)
INFO - root - 2017-12-07 19:50:52.722001: step 47530, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 53h:18m:22s remains)
INFO - root - 2017-12-07 19:50:59.496698: step 47540, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 56h:28m:04s remains)
INFO - root - 2017-12-07 19:51:06.415028: step 47550, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 54h:21m:10s remains)
INFO - root - 2017-12-07 19:51:13.290235: step 47560, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 56h:38m:23s remains)
INFO - root - 2017-12-07 19:51:20.138069: step 47570, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 56h:23m:40s remains)
INFO - root - 2017-12-07 19:51:26.923995: step 47580, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.695 sec/batch; 54h:58m:23s remains)
INFO - root - 2017-12-07 19:51:33.742078: step 47590, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 52h:53m:07s remains)
INFO - root - 2017-12-07 19:51:40.482706: step 47600, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 51h:36m:54s remains)
2017-12-07 19:51:41.291417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043834 -4.2015858 -4.2049131 -4.2069097 -4.2252231 -4.2412872 -4.23849 -4.241406 -4.25606 -4.267169 -4.2694926 -4.2749314 -4.2806344 -4.2846222 -4.2937365][-4.1946912 -4.1907158 -4.2009387 -4.212254 -4.2363825 -4.2536268 -4.2464409 -4.2401085 -4.2487378 -4.2552061 -4.2498288 -4.2489238 -4.2513113 -4.2543416 -4.2693105][-4.1588211 -4.1642032 -4.1841764 -4.208262 -4.2392159 -4.2569704 -4.2445378 -4.2259951 -4.2270203 -4.2290626 -4.2183514 -4.2122312 -4.2117772 -4.2136283 -4.2356739][-4.1265688 -4.1537795 -4.1850247 -4.2157836 -4.2456746 -4.2550831 -4.2337322 -4.2016554 -4.1956787 -4.1965718 -4.1876955 -4.1830335 -4.1859903 -4.189641 -4.216073][-4.1428914 -4.1873775 -4.2195978 -4.2391915 -4.2515135 -4.2380695 -4.198575 -4.1521807 -4.148716 -4.164515 -4.168045 -4.1713619 -4.1787486 -4.1851826 -4.2104931][-4.1823564 -4.2257457 -4.2432709 -4.2408137 -4.2302737 -4.1934347 -4.1260896 -4.0613384 -4.06802 -4.1113095 -4.137816 -4.153194 -4.1655307 -4.1769867 -4.1992874][-4.1931152 -4.2224278 -4.2176194 -4.1936975 -4.1644626 -4.1085825 -4.0075669 -3.9202406 -3.9421661 -4.0193229 -4.068923 -4.0986357 -4.1198573 -4.1337252 -4.1510725][-4.1715226 -4.1878715 -4.1669035 -4.131104 -4.092906 -4.0257621 -3.9058912 -3.8029077 -3.8348045 -3.9314826 -3.9943483 -4.0355005 -4.0601053 -4.066782 -4.0712347][-4.1567287 -4.1646013 -4.1365752 -4.1012592 -4.0669169 -4.0084982 -3.9145474 -3.8417678 -3.8692865 -3.9408875 -3.985599 -4.0190363 -4.0346889 -4.02561 -4.008503][-4.1740046 -4.1755495 -4.1447849 -4.1170988 -4.0960569 -4.0550609 -3.9990215 -3.9699435 -3.9915557 -4.0263205 -4.0434546 -4.0605807 -4.0668578 -4.0476708 -4.0188723][-4.2216611 -4.2189741 -4.1879506 -4.1652894 -4.1573839 -4.1337924 -4.1024103 -4.09445 -4.1094127 -4.1222849 -4.1248517 -4.134738 -4.14172 -4.127264 -4.0997753][-4.2730422 -4.2682781 -4.240921 -4.2198939 -4.214983 -4.1998973 -4.1790104 -4.1745734 -4.1843729 -4.1926985 -4.1945691 -4.206151 -4.22046 -4.2158442 -4.1950178][-4.2965894 -4.2915339 -4.2697992 -4.2495813 -4.2424831 -4.2321634 -4.2172217 -4.2115726 -4.218472 -4.2273283 -4.2331386 -4.2467895 -4.2675853 -4.2722292 -4.259831][-4.2918673 -4.2881379 -4.271955 -4.25488 -4.24664 -4.23821 -4.2242351 -4.217905 -4.2237158 -4.23346 -4.2402434 -4.25134 -4.2741408 -4.2867064 -4.281178][-4.2647338 -4.2655334 -4.257576 -4.2476768 -4.2409773 -4.2328458 -4.21914 -4.2106371 -4.2135038 -4.223269 -4.2289386 -4.2333632 -4.2506771 -4.2670116 -4.2674236]]...]
INFO - root - 2017-12-07 19:51:47.856342: step 47610, loss = 2.06, batch loss = 2.00 (13.8 examples/sec; 0.578 sec/batch; 45h:45m:35s remains)
INFO - root - 2017-12-07 19:51:54.574158: step 47620, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 51h:41m:57s remains)
INFO - root - 2017-12-07 19:52:01.392208: step 47630, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 52h:21m:20s remains)
INFO - root - 2017-12-07 19:52:08.075424: step 47640, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 55h:28m:25s remains)
INFO - root - 2017-12-07 19:52:14.896126: step 47650, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 56h:54m:41s remains)
INFO - root - 2017-12-07 19:52:21.640376: step 47660, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:41m:56s remains)
INFO - root - 2017-12-07 19:52:28.348881: step 47670, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.653 sec/batch; 51h:39m:25s remains)
INFO - root - 2017-12-07 19:52:34.966394: step 47680, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 50h:56m:57s remains)
INFO - root - 2017-12-07 19:52:41.685045: step 47690, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 56h:50m:07s remains)
INFO - root - 2017-12-07 19:52:48.506891: step 47700, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 53h:08m:26s remains)
2017-12-07 19:52:49.182513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3022513 -4.2895727 -4.2810454 -4.274004 -4.2705455 -4.2650561 -4.2469931 -4.2099242 -4.1750407 -4.1709328 -4.2000875 -4.2412066 -4.2789454 -4.3061686 -4.3193145][-4.2919841 -4.271451 -4.2523532 -4.2384372 -4.2331038 -4.22815 -4.2061729 -4.164506 -4.125524 -4.1205878 -4.1539278 -4.2001967 -4.2422452 -4.2759352 -4.2980447][-4.2776132 -4.2493048 -4.2191958 -4.1976566 -4.1924815 -4.1939845 -4.1785188 -4.1443806 -4.10896 -4.10065 -4.1293979 -4.1731548 -4.2151933 -4.2529984 -4.2816644][-4.25362 -4.2185254 -4.1799278 -4.1538248 -4.1493096 -4.1573019 -4.1545835 -4.1348486 -4.1083679 -4.0975289 -4.1176367 -4.1552906 -4.1971073 -4.2394342 -4.2718158][-4.2318206 -4.1961961 -4.1595349 -4.1340017 -4.1267042 -4.1350813 -4.1400418 -4.1302967 -4.1083989 -4.0917997 -4.1026931 -4.1373219 -4.1824536 -4.2300963 -4.2644773][-4.2240143 -4.1964474 -4.170156 -4.1473031 -4.1285396 -4.1217489 -4.1202636 -4.1141071 -4.0995388 -4.084867 -4.0943084 -4.13358 -4.1844473 -4.2353415 -4.2685056][-4.2045279 -4.1837163 -4.1663179 -4.1437593 -4.1108289 -4.0781312 -4.0571785 -4.0475621 -4.0463252 -4.052526 -4.0802836 -4.1312909 -4.1901541 -4.2445693 -4.2763219][-4.1858759 -4.1686282 -4.1510324 -4.1191511 -4.0651941 -4.0004759 -3.9542918 -3.9378145 -3.9477534 -3.9753208 -4.02701 -4.0938258 -4.1658483 -4.2313285 -4.2720227][-4.199451 -4.1826925 -4.1619229 -4.1244736 -4.0606441 -3.9774921 -3.9165597 -3.8930898 -3.8983178 -3.9223058 -3.9785335 -4.0543375 -4.1341715 -4.2079344 -4.2600904][-4.2359233 -4.2219539 -4.2052808 -4.1785893 -4.1320925 -4.0663481 -4.0142145 -3.9770038 -3.9585137 -3.9613554 -4.0003295 -4.0644393 -4.13286 -4.1999731 -4.2562408][-4.2670116 -4.2600164 -4.2514849 -4.2401123 -4.2171431 -4.1786065 -4.1406665 -4.0969009 -4.0627656 -4.0487709 -4.066041 -4.1105847 -4.15955 -4.2120771 -4.2607784][-4.2812757 -4.2799644 -4.2785263 -4.2776241 -4.2691789 -4.2491646 -4.2238059 -4.182858 -4.1463523 -4.1268234 -4.1320505 -4.1583672 -4.1892424 -4.2270579 -4.2678404][-4.28394 -4.2867651 -4.2883568 -4.2896891 -4.2871175 -4.2766519 -4.2600183 -4.2258253 -4.194448 -4.18013 -4.184114 -4.2014689 -4.2187171 -4.2434492 -4.2783027][-4.2944012 -4.2983303 -4.297595 -4.2960315 -4.291986 -4.2841582 -4.2731762 -4.2505846 -4.2302051 -4.2224588 -4.2277117 -4.2426453 -4.252594 -4.2678347 -4.2953711][-4.3026752 -4.3068538 -4.3052387 -4.3013153 -4.2955537 -4.2884889 -4.2818508 -4.2691708 -4.2607794 -4.2607841 -4.2666831 -4.2768655 -4.2819343 -4.2904906 -4.3102689]]...]
INFO - root - 2017-12-07 19:52:55.684842: step 47710, loss = 2.04, batch loss = 1.98 (15.9 examples/sec; 0.504 sec/batch; 39h:53m:13s remains)
INFO - root - 2017-12-07 19:53:02.578922: step 47720, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 56h:39m:40s remains)
INFO - root - 2017-12-07 19:53:09.305329: step 47730, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 52h:35m:39s remains)
INFO - root - 2017-12-07 19:53:16.149177: step 47740, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 52h:49m:41s remains)
INFO - root - 2017-12-07 19:53:22.944072: step 47750, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.658 sec/batch; 52h:04m:21s remains)
INFO - root - 2017-12-07 19:53:29.746735: step 47760, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 57h:16m:47s remains)
INFO - root - 2017-12-07 19:53:36.660774: step 47770, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 53h:32m:32s remains)
INFO - root - 2017-12-07 19:53:43.542694: step 47780, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 52h:29m:44s remains)
INFO - root - 2017-12-07 19:53:50.316968: step 47790, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 50h:11m:58s remains)
INFO - root - 2017-12-07 19:53:57.235773: step 47800, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 52h:41m:54s remains)
2017-12-07 19:53:57.959152: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2853651 -4.2848206 -4.2710676 -4.2604594 -4.247191 -4.2252059 -4.2231803 -4.2424035 -4.2533107 -4.2587438 -4.2570419 -4.2554374 -4.258482 -4.2590261 -4.2568693][-4.2776785 -4.2762461 -4.2636213 -4.2573528 -4.2483072 -4.2256584 -4.2214622 -4.2443943 -4.2605348 -4.2705932 -4.2732048 -4.2727079 -4.272171 -4.2677078 -4.26204][-4.2721596 -4.2692437 -4.2564936 -4.2511973 -4.2423868 -4.2169085 -4.2122884 -4.2410121 -4.2644634 -4.279387 -4.2873411 -4.288763 -4.2837148 -4.2729764 -4.2614403][-4.2674775 -4.261642 -4.2468123 -4.2403383 -4.2295089 -4.201983 -4.1987605 -4.2328782 -4.2619343 -4.2812152 -4.293581 -4.297544 -4.2880254 -4.2700009 -4.2513828][-4.26398 -4.2545328 -4.2365618 -4.2282534 -4.2166786 -4.1905518 -4.1918268 -4.2296405 -4.260788 -4.2829404 -4.2977414 -4.3017378 -4.2862744 -4.2630153 -4.2399836][-4.2596488 -4.2459693 -4.2255154 -4.2157288 -4.2048459 -4.1840148 -4.1928582 -4.2307549 -4.2566719 -4.2761769 -4.2896428 -4.28819 -4.2660851 -4.241518 -4.2213874][-4.2554989 -4.238142 -4.2148218 -4.2039013 -4.1932788 -4.176208 -4.1899004 -4.2270746 -4.246829 -4.2604523 -4.2695227 -4.2600632 -4.2309852 -4.2060761 -4.1918979][-4.2497082 -4.228158 -4.2012239 -4.1871018 -4.1736422 -4.1573033 -4.1742458 -4.2149343 -4.2319155 -4.2395306 -4.2439666 -4.227963 -4.1933813 -4.1695371 -4.1643677][-4.2418475 -4.216083 -4.185041 -4.1649966 -4.1464777 -4.1280365 -4.1470509 -4.1929255 -4.2090812 -4.2100477 -4.2095337 -4.1930852 -4.1627088 -4.1448064 -4.1473351][-4.235024 -4.2062616 -4.1720691 -4.1472664 -4.123477 -4.1015983 -4.12074 -4.169004 -4.1855245 -4.182189 -4.1783361 -4.1660304 -4.1430087 -4.1290412 -4.1366148][-4.2308974 -4.2006254 -4.1646934 -4.1369038 -4.1102433 -4.0860739 -4.1024494 -4.1493387 -4.1641917 -4.1573362 -4.1487021 -4.1384487 -4.1212788 -4.1120543 -4.1265731][-4.2290792 -4.1971555 -4.159194 -4.1293907 -4.1019921 -4.0767436 -4.0885892 -4.1306381 -4.1445808 -4.1378708 -4.1276932 -4.1216931 -4.1110582 -4.1066718 -4.12822][-4.2279615 -4.1929946 -4.1533093 -4.12325 -4.098268 -4.076458 -4.0884938 -4.1263838 -4.138629 -4.13158 -4.1204529 -4.1177182 -4.1140866 -4.1133976 -4.1380692][-4.2292957 -4.1906428 -4.1489062 -4.118515 -4.0963049 -4.0805507 -4.096384 -4.1317625 -4.1418362 -4.1337395 -4.1204028 -4.116869 -4.1162791 -4.1176152 -4.1413689][-4.2338767 -4.1927414 -4.1498256 -4.1199036 -4.1000762 -4.0898757 -4.1084542 -4.1409569 -4.1500797 -4.1439691 -4.1330018 -4.1295729 -4.1311178 -4.1334696 -4.150764]]...]
INFO - root - 2017-12-07 19:54:04.540899: step 47810, loss = 2.06, batch loss = 2.01 (17.0 examples/sec; 0.470 sec/batch; 37h:10m:57s remains)
INFO - root - 2017-12-07 19:54:11.434850: step 47820, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 52h:01m:09s remains)
INFO - root - 2017-12-07 19:54:18.338804: step 47830, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 56h:26m:17s remains)
INFO - root - 2017-12-07 19:54:25.301025: step 47840, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 58h:02m:05s remains)
INFO - root - 2017-12-07 19:54:32.156087: step 47850, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 55h:41m:43s remains)
INFO - root - 2017-12-07 19:54:38.976437: step 47860, loss = 2.03, batch loss = 1.97 (12.4 examples/sec; 0.645 sec/batch; 51h:01m:58s remains)
INFO - root - 2017-12-07 19:54:45.725439: step 47870, loss = 2.09, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 52h:07m:52s remains)
INFO - root - 2017-12-07 19:54:52.449414: step 47880, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 54h:52m:40s remains)
INFO - root - 2017-12-07 19:54:59.229134: step 47890, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.705 sec/batch; 55h:42m:26s remains)
INFO - root - 2017-12-07 19:55:05.941697: step 47900, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 52h:17m:14s remains)
2017-12-07 19:55:06.622434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2562013 -4.2677364 -4.2770247 -4.2819743 -4.2829113 -4.2835484 -4.282825 -4.2768388 -4.2722521 -4.2696843 -4.2678633 -4.2664289 -4.2654986 -4.2598457 -4.2547541][-4.2215161 -4.2391391 -4.2512708 -4.2580023 -4.2624063 -4.2693224 -4.2793903 -4.2856731 -4.2893472 -4.2899752 -4.2871246 -4.2781353 -4.2667456 -4.2516685 -4.2376962][-4.19996 -4.2218571 -4.2332034 -4.2351661 -4.23596 -4.2438889 -4.2592783 -4.2747631 -4.2858834 -4.2903514 -4.288949 -4.2771173 -4.2603989 -4.2414432 -4.2226291][-4.2006168 -4.2188644 -4.2249346 -4.21561 -4.2038951 -4.2059336 -4.2243004 -4.2509842 -4.2722225 -4.2811871 -4.2793522 -4.2647548 -4.2454939 -4.2289009 -4.2143793][-4.214407 -4.2195311 -4.2105865 -4.1830211 -4.1522865 -4.1385512 -4.1558814 -4.1960077 -4.2304277 -4.248435 -4.2510467 -4.2374077 -4.2175751 -4.2049866 -4.2001362][-4.2324438 -4.2232661 -4.1966496 -4.1473093 -4.0901175 -4.0518551 -4.0594335 -4.1121321 -4.1640878 -4.1983447 -4.2141776 -4.2112269 -4.1974854 -4.1899867 -4.1918073][-4.25002 -4.2243147 -4.1782956 -4.106142 -4.0209079 -3.9545984 -3.9476469 -4.0143666 -4.0920367 -4.1510057 -4.1897349 -4.2066007 -4.2072482 -4.205369 -4.2064857][-4.2544746 -4.2083898 -4.1401596 -4.0441833 -3.9316452 -3.8335106 -3.8059068 -3.8877535 -3.9989147 -4.0911131 -4.1602826 -4.2040577 -4.2269378 -4.2363939 -4.2402725][-4.2459412 -4.1872926 -4.1098995 -4.0112033 -3.9000065 -3.8014414 -3.7684259 -3.8456855 -3.9596658 -4.0630245 -4.1497483 -4.2118344 -4.2515268 -4.2715263 -4.2789655][-4.2423539 -4.187458 -4.120873 -4.0436392 -3.9646277 -3.9012485 -3.8830066 -3.9365754 -4.0214214 -4.1075549 -4.186727 -4.247674 -4.2890358 -4.3109441 -4.3192396][-4.2518106 -4.2103972 -4.161499 -4.1091042 -4.0653896 -4.0383306 -4.0365224 -4.0726204 -4.1280589 -4.1868596 -4.2426496 -4.2872148 -4.3193989 -4.3377328 -4.3452387][-4.2798181 -4.2561789 -4.2278237 -4.2000589 -4.1814866 -4.1757374 -4.1820226 -4.2037215 -4.2321949 -4.2639241 -4.2943149 -4.3192463 -4.3384604 -4.3494315 -4.3538456][-4.3102894 -4.3007016 -4.29068 -4.28114 -4.2767186 -4.2802377 -4.287055 -4.2980461 -4.3079939 -4.3184896 -4.3297882 -4.3399925 -4.3490543 -4.353507 -4.3549347][-4.3289342 -4.3269248 -4.3275762 -4.3275146 -4.3284817 -4.3336492 -4.3382859 -4.3443408 -4.3467636 -4.3478503 -4.3497458 -4.3518438 -4.3550472 -4.3565183 -4.3560781][-4.3382936 -4.339221 -4.343504 -4.3474231 -4.3496614 -4.3523746 -4.3534737 -4.3554459 -4.3568125 -4.3577433 -4.357965 -4.3577833 -4.3582997 -4.3581243 -4.3567519]]...]
INFO - root - 2017-12-07 19:55:13.190469: step 47910, loss = 2.07, batch loss = 2.01 (13.7 examples/sec; 0.584 sec/batch; 46h:11m:50s remains)
INFO - root - 2017-12-07 19:55:20.105042: step 47920, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 57h:21m:59s remains)
INFO - root - 2017-12-07 19:55:26.887874: step 47930, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 54h:10m:20s remains)
INFO - root - 2017-12-07 19:55:33.669314: step 47940, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.648 sec/batch; 51h:12m:12s remains)
INFO - root - 2017-12-07 19:55:40.456233: step 47950, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 53h:28m:18s remains)
INFO - root - 2017-12-07 19:55:47.325899: step 47960, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 55h:06m:24s remains)
INFO - root - 2017-12-07 19:55:54.006028: step 47970, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 52h:03m:39s remains)
INFO - root - 2017-12-07 19:56:00.916201: step 47980, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 55h:35m:18s remains)
INFO - root - 2017-12-07 19:56:07.512711: step 47990, loss = 2.05, batch loss = 1.99 (13.6 examples/sec; 0.590 sec/batch; 46h:39m:06s remains)
INFO - root - 2017-12-07 19:56:14.295682: step 48000, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 55h:26m:50s remains)
2017-12-07 19:56:15.034054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3200145 -4.3169084 -4.3162684 -4.3182635 -4.3193245 -4.3191462 -4.3174934 -4.314652 -4.3112779 -4.308723 -4.3092613 -4.3114963 -4.3127017 -4.313673 -4.3163385][-4.3118849 -4.3068905 -4.3056574 -4.3075767 -4.3069043 -4.3027878 -4.2937455 -4.2858829 -4.2806396 -4.27805 -4.2808514 -4.2864347 -4.2896175 -4.29197 -4.29907][-4.2987328 -4.290813 -4.2873726 -4.2873497 -4.2825465 -4.270731 -4.2509403 -4.2390118 -4.2329822 -4.2308011 -4.2369752 -4.2467937 -4.2530622 -4.2577267 -4.2716556][-4.2833176 -4.2695894 -4.2617111 -4.2585173 -4.2505445 -4.2307782 -4.2003837 -4.1833534 -4.172924 -4.1657925 -4.1733451 -4.1863704 -4.1965742 -4.2044172 -4.2282043][-4.2662835 -4.2488365 -4.2364006 -4.2271881 -4.21291 -4.1821623 -4.1365051 -4.110239 -4.0923223 -4.0817637 -4.0935717 -4.111587 -4.1265326 -4.1384649 -4.1735015][-4.24665 -4.2256355 -4.2081618 -4.1867108 -4.1572013 -4.1117525 -4.0539665 -4.0239162 -4.0027213 -3.9930112 -4.0147877 -4.0393691 -4.0579205 -4.0734515 -4.1192455][-4.2298865 -4.2029824 -4.1765356 -4.1381068 -4.0875635 -4.0282836 -3.9702892 -3.9466171 -3.9319258 -3.9287307 -3.9555688 -3.9821076 -4.0030541 -4.0224285 -4.0775867][-4.2130156 -4.1772242 -4.1396637 -4.089457 -4.0277691 -3.968946 -3.9225492 -3.9147363 -3.9148009 -3.9155328 -3.9364798 -3.9592834 -3.9763384 -3.9953337 -4.0566468][-4.2027059 -4.1605678 -4.1197348 -4.071414 -4.0181203 -3.975131 -3.9449697 -3.9531765 -3.9666312 -3.9687142 -3.9831586 -3.9956498 -3.9991307 -4.0072045 -4.0651965][-4.2123446 -4.1739397 -4.1419606 -4.1078138 -4.0731759 -4.0523958 -4.0366259 -4.0455737 -4.0601072 -4.0611143 -4.0698662 -4.0705562 -4.0601711 -4.057723 -4.1050844][-4.2385426 -4.2124486 -4.19474 -4.1771755 -4.1606903 -4.1544843 -4.1440964 -4.1453004 -4.1534495 -4.1527624 -4.1572132 -4.1525645 -4.1374974 -4.13008 -4.164269][-4.2725215 -4.2591271 -4.2531252 -4.2489104 -4.2427244 -4.2419167 -4.2335858 -4.2276525 -4.227551 -4.2266326 -4.2289381 -4.2246804 -4.2123132 -4.20524 -4.2255268][-4.299458 -4.2965322 -4.2989569 -4.3012948 -4.2983093 -4.2970371 -4.2889209 -4.2801981 -4.275321 -4.2741575 -4.2770524 -4.2771339 -4.2711139 -4.2657523 -4.2756896][-4.3103995 -4.3117828 -4.3162928 -4.31998 -4.3178816 -4.3156328 -4.3103604 -4.3048568 -4.3013854 -4.3020635 -4.3054423 -4.3076372 -4.3058276 -4.303 -4.3076639][-4.3086562 -4.3088632 -4.3103657 -4.3114276 -4.3096766 -4.3074021 -4.3045897 -4.3032103 -4.3037672 -4.3065691 -4.3111343 -4.3144212 -4.3156157 -4.3166075 -4.3193851]]...]
INFO - root - 2017-12-07 19:56:21.547578: step 48010, loss = 2.09, batch loss = 2.03 (16.7 examples/sec; 0.480 sec/batch; 37h:57m:21s remains)
INFO - root - 2017-12-07 19:56:28.470589: step 48020, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 55h:39m:06s remains)
INFO - root - 2017-12-07 19:56:35.308876: step 48030, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.727 sec/batch; 57h:28m:53s remains)
INFO - root - 2017-12-07 19:56:42.062826: step 48040, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 51h:38m:43s remains)
INFO - root - 2017-12-07 19:56:48.808016: step 48050, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 51h:21m:46s remains)
INFO - root - 2017-12-07 19:56:55.629927: step 48060, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 52h:38m:08s remains)
INFO - root - 2017-12-07 19:57:02.491726: step 48070, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 55h:55m:39s remains)
INFO - root - 2017-12-07 19:57:09.268095: step 48080, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 56h:53m:16s remains)
INFO - root - 2017-12-07 19:57:16.118286: step 48090, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 53h:10m:13s remains)
INFO - root - 2017-12-07 19:57:22.895625: step 48100, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 52h:22m:27s remains)
2017-12-07 19:57:23.735891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2409229 -4.2518878 -4.2692914 -4.2759337 -4.2747154 -4.2680717 -4.2589955 -4.260704 -4.2739029 -4.2876735 -4.2925358 -4.2892284 -4.2824612 -4.2797022 -4.2860489][-4.233973 -4.2291918 -4.2363811 -4.2423134 -4.2425632 -4.229979 -4.2245045 -4.2364 -4.2616577 -4.2852964 -4.2962642 -4.2958107 -4.2902904 -4.2896328 -4.2994442][-4.20453 -4.188354 -4.1826062 -4.1885529 -4.19475 -4.1887426 -4.1963811 -4.2239456 -4.2547226 -4.2721152 -4.2783246 -4.2755742 -4.2721524 -4.27697 -4.2928338][-4.1672812 -4.1504273 -4.1337643 -4.1348581 -4.1451268 -4.1477475 -4.1701331 -4.2097378 -4.2379169 -4.2447953 -4.2421775 -4.2364335 -4.2356977 -4.2472878 -4.2692733][-4.1351075 -4.1332459 -4.1143403 -4.104578 -4.1082458 -4.1115971 -4.1381693 -4.1756139 -4.1952715 -4.2022953 -4.2008677 -4.1947479 -4.1978388 -4.2124004 -4.2364154][-4.1213164 -4.1434913 -4.129652 -4.1079264 -4.0981817 -4.0901875 -4.0992017 -4.1170468 -4.1249251 -4.1364632 -4.1447186 -4.1474276 -4.1628523 -4.1864834 -4.21344][-4.1256852 -4.1616683 -4.1517453 -4.117291 -4.0879574 -4.0576777 -4.0382295 -4.0319195 -4.034255 -4.0558476 -4.0837064 -4.104713 -4.1376195 -4.1757779 -4.2092137][-4.1496096 -4.175643 -4.1583571 -4.1054959 -4.0549431 -4.0104966 -3.9758661 -3.9628496 -3.9719131 -4.0075908 -4.0576696 -4.0920038 -4.1354055 -4.1838827 -4.2253461][-4.1698809 -4.16754 -4.1353984 -4.0774946 -4.0284853 -3.9934969 -3.9706578 -3.9682946 -3.9887309 -4.0326285 -4.0930214 -4.1311674 -4.1749525 -4.2188745 -4.25896][-4.1997666 -4.1708608 -4.1255965 -4.0767655 -4.0453916 -4.0277586 -4.023818 -4.0317316 -4.0537295 -4.0886683 -4.1434288 -4.1821785 -4.2157941 -4.2477179 -4.2801256][-4.2140079 -4.1783161 -4.1356611 -4.1003041 -4.0859137 -4.08508 -4.0976748 -4.1096039 -4.126873 -4.1434641 -4.1820993 -4.2152863 -4.2382526 -4.2617044 -4.2875843][-4.2124257 -4.1839609 -4.1526356 -4.1321616 -4.1304936 -4.140944 -4.1576309 -4.16664 -4.1773686 -4.1800618 -4.2091479 -4.2324524 -4.2449603 -4.26196 -4.2878885][-4.2044291 -4.18332 -4.1625447 -4.1534867 -4.1587234 -4.1722131 -4.1885419 -4.1886878 -4.1905828 -4.1831484 -4.2035837 -4.2181735 -4.2230616 -4.2414865 -4.2709637][-4.2010417 -4.179513 -4.1578283 -4.1485167 -4.151546 -4.1602073 -4.1780276 -4.1771512 -4.1759338 -4.1636524 -4.174479 -4.1829996 -4.18427 -4.2095079 -4.2447968][-4.2035503 -4.1770988 -4.144999 -4.1234426 -4.1191421 -4.1249447 -4.1427183 -4.1473851 -4.1518984 -4.14282 -4.1508207 -4.1601086 -4.1594553 -4.1837831 -4.213697]]...]
INFO - root - 2017-12-07 19:57:30.395041: step 48110, loss = 2.06, batch loss = 2.00 (13.2 examples/sec; 0.605 sec/batch; 47h:47m:15s remains)
INFO - root - 2017-12-07 19:57:37.186324: step 48120, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.676 sec/batch; 53h:25m:52s remains)
INFO - root - 2017-12-07 19:57:43.900568: step 48130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 51h:06m:20s remains)
INFO - root - 2017-12-07 19:57:50.634146: step 48140, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 49h:50m:02s remains)
INFO - root - 2017-12-07 19:57:57.386228: step 48150, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 56h:42m:45s remains)
INFO - root - 2017-12-07 19:58:04.168580: step 48160, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 55h:38m:11s remains)
INFO - root - 2017-12-07 19:58:10.909378: step 48170, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 52h:28m:26s remains)
INFO - root - 2017-12-07 19:58:17.661904: step 48180, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 53h:13m:36s remains)
INFO - root - 2017-12-07 19:58:24.479360: step 48190, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 51h:16m:41s remains)
INFO - root - 2017-12-07 19:58:31.346823: step 48200, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 57h:34m:50s remains)
2017-12-07 19:58:32.089807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33932 -4.3363562 -4.3257117 -4.3187833 -4.3143854 -4.3102913 -4.307085 -4.2996159 -4.2947669 -4.2917905 -4.2953653 -4.2977796 -4.2994714 -4.3019342 -4.3035784][-4.3286953 -4.3167887 -4.3028188 -4.29408 -4.2840762 -4.2723413 -4.2643456 -4.2498684 -4.2387705 -4.2351584 -4.2490215 -4.2632937 -4.276382 -4.2848015 -4.2878618][-4.3112183 -4.294445 -4.2789655 -4.269238 -4.2559419 -4.2407465 -4.2290397 -4.2105231 -4.1959872 -4.1893249 -4.20953 -4.2359734 -4.26206 -4.27722 -4.2820539][-4.2804132 -4.2611365 -4.2462072 -4.2356014 -4.2240338 -4.2094569 -4.1937685 -4.1726246 -4.1581411 -4.1545672 -4.1836734 -4.221705 -4.2572637 -4.2767239 -4.2815557][-4.2497563 -4.2254038 -4.2117262 -4.2020497 -4.1918225 -4.1784854 -4.1624975 -4.1414657 -4.1305542 -4.131125 -4.1628814 -4.2079945 -4.2524405 -4.2799735 -4.2868376][-4.2175975 -4.1885567 -4.1725168 -4.1582732 -4.1429839 -4.1270723 -4.1110244 -4.0931005 -4.0941038 -4.1068921 -4.1476312 -4.2010541 -4.2522597 -4.2858744 -4.2945361][-4.1757321 -4.1431918 -4.1184983 -4.088748 -4.0617323 -4.0352182 -4.015048 -4.0060048 -4.0284753 -4.0671997 -4.129559 -4.1978064 -4.2550139 -4.2904358 -4.3018394][-4.1632152 -4.1267114 -4.0874534 -4.0421429 -4.0012312 -3.9560397 -3.9233818 -3.9198918 -3.9610803 -4.02635 -4.1092844 -4.1847367 -4.2435231 -4.2795143 -4.2950907][-4.1823092 -4.1488953 -4.1087065 -4.0706396 -4.0452671 -4.0145307 -3.984977 -3.961736 -3.9720917 -4.0132327 -4.0831537 -4.1550708 -4.2161846 -4.2590976 -4.2806053][-4.2026691 -4.18048 -4.1528416 -4.133379 -4.1283059 -4.1129942 -4.0876584 -4.0535541 -4.0367832 -4.0442333 -4.08468 -4.1441836 -4.2059283 -4.2527218 -4.2766867][-4.2101641 -4.1956449 -4.1788955 -4.1722059 -4.1773019 -4.1738081 -4.1590562 -4.129395 -4.1086197 -4.1050458 -4.12353 -4.1613879 -4.210906 -4.2545028 -4.2791615][-4.2101307 -4.1989131 -4.185864 -4.1825528 -4.1923962 -4.2001104 -4.2022214 -4.1892304 -4.179821 -4.1781363 -4.187006 -4.2079029 -4.2410593 -4.270895 -4.2869258][-4.217957 -4.2100081 -4.197144 -4.1908307 -4.1970835 -4.2079587 -4.2175345 -4.2154984 -4.2151885 -4.2205248 -4.2314453 -4.2483816 -4.2711544 -4.2893548 -4.2964454][-4.2203474 -4.2149539 -4.2066889 -4.2022486 -4.2046623 -4.2103982 -4.2155447 -4.2117486 -4.2109828 -4.2188511 -4.2343535 -4.2538948 -4.2744761 -4.2889547 -4.2954755][-4.2129154 -4.2096057 -4.2061305 -4.2041912 -4.2047391 -4.2067642 -4.2087183 -4.2056279 -4.2044759 -4.2113075 -4.2246776 -4.2408729 -4.2594738 -4.2755051 -4.2861676]]...]
INFO - root - 2017-12-07 19:58:38.696151: step 48210, loss = 2.07, batch loss = 2.01 (15.9 examples/sec; 0.502 sec/batch; 39h:36m:52s remains)
INFO - root - 2017-12-07 19:58:45.440266: step 48220, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 51h:45m:40s remains)
INFO - root - 2017-12-07 19:58:52.307617: step 48230, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 56h:04m:22s remains)
INFO - root - 2017-12-07 19:58:59.038403: step 48240, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 52h:58m:19s remains)
INFO - root - 2017-12-07 19:59:05.761482: step 48250, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 52h:33m:14s remains)
INFO - root - 2017-12-07 19:59:12.608523: step 48260, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 53h:24m:38s remains)
INFO - root - 2017-12-07 19:59:19.374612: step 48270, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 50h:37m:45s remains)
INFO - root - 2017-12-07 19:59:26.259147: step 48280, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 54h:40m:03s remains)
INFO - root - 2017-12-07 19:59:33.076685: step 48290, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 54h:35m:30s remains)
INFO - root - 2017-12-07 19:59:39.823521: step 48300, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 53h:53m:08s remains)
2017-12-07 19:59:40.477663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2365203 -4.2376122 -4.2265015 -4.2053871 -4.179915 -4.17084 -4.1789508 -4.18825 -4.1986766 -4.2192287 -4.2313762 -4.2294154 -4.220583 -4.2202625 -4.2233787][-4.2373967 -4.2292686 -4.2193251 -4.2044644 -4.186254 -4.1805606 -4.1825933 -4.1877584 -4.1998439 -4.2145443 -4.2195292 -4.2142854 -4.2077875 -4.2082562 -4.2131662][-4.2388988 -4.2316527 -4.2246227 -4.2099485 -4.1932788 -4.1871834 -4.1816878 -4.1821675 -4.1973066 -4.2065196 -4.2014637 -4.1967626 -4.1962137 -4.199492 -4.2049875][-4.2240639 -4.2213964 -4.2237153 -4.2148471 -4.1998649 -4.1901426 -4.1787591 -4.1746087 -4.1952729 -4.2033772 -4.1919394 -4.1875176 -4.1941633 -4.198729 -4.2017241][-4.1984563 -4.1937747 -4.2020779 -4.2004952 -4.1881595 -4.1725516 -4.1486459 -4.13439 -4.1592097 -4.1768174 -4.1675715 -4.1643629 -4.1861229 -4.1986365 -4.2000046][-4.168231 -4.1595812 -4.164422 -4.1599469 -4.1484575 -4.1278162 -4.0877409 -4.0610852 -4.0963788 -4.1345468 -4.13461 -4.1345692 -4.1663671 -4.190537 -4.194222][-4.14626 -4.1186872 -4.1046782 -4.0925851 -4.0770988 -4.0512304 -3.9958744 -3.9542456 -4.00813 -4.0765343 -4.0933809 -4.1025162 -4.139523 -4.172555 -4.1839895][-4.1402097 -4.0994434 -4.0670052 -4.0438051 -4.0208144 -3.9841518 -3.9072104 -3.8404281 -3.9134016 -4.0138621 -4.052958 -4.0748191 -4.1141839 -4.1537085 -4.1762419][-4.149241 -4.1095371 -4.07335 -4.0487232 -4.0285468 -4.0025663 -3.9378252 -3.8735545 -3.9278903 -4.01803 -4.0598321 -4.0811539 -4.11269 -4.1497493 -4.176228][-4.1592846 -4.1322322 -4.105031 -4.0870872 -4.0812273 -4.0804267 -4.0537624 -4.0209951 -4.0538096 -4.1081495 -4.1298814 -4.1346259 -4.147964 -4.1693368 -4.1857462][-4.1620612 -4.152401 -4.1395979 -4.1266384 -4.1318488 -4.1507215 -4.1518459 -4.1395617 -4.1603618 -4.1897154 -4.1953444 -4.1872606 -4.1866026 -4.1959944 -4.2034168][-4.1444368 -4.1445432 -4.1441669 -4.1422486 -4.1592879 -4.1887479 -4.2029023 -4.1963415 -4.2047367 -4.2179646 -4.2142539 -4.20268 -4.1999645 -4.2094541 -4.2176838][-4.1110134 -4.1090546 -4.1107016 -4.1187472 -4.1450791 -4.182169 -4.205461 -4.201468 -4.20391 -4.2094073 -4.2009335 -4.1903343 -4.1960092 -4.2141132 -4.2270474][-4.0820608 -4.0745144 -4.0734363 -4.0857859 -4.1129403 -4.1454082 -4.1685872 -4.1626921 -4.16383 -4.1750793 -4.1734505 -4.1731348 -4.1900783 -4.2168679 -4.233551][-4.0708246 -4.0594716 -4.0544205 -4.0688324 -4.0932035 -4.1151209 -4.1255531 -4.1097913 -4.1098418 -4.1301465 -4.1424294 -4.1576662 -4.1841307 -4.2141342 -4.2335353]]...]
INFO - root - 2017-12-07 19:59:47.110118: step 48310, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 49h:52m:11s remains)
INFO - root - 2017-12-07 19:59:53.761850: step 48320, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 54h:54m:48s remains)
INFO - root - 2017-12-07 20:00:00.467496: step 48330, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 53h:51m:04s remains)
INFO - root - 2017-12-07 20:00:07.172768: step 48340, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:20m:16s remains)
INFO - root - 2017-12-07 20:00:14.104047: step 48350, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 53h:55m:40s remains)
INFO - root - 2017-12-07 20:00:20.906059: step 48360, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 54h:57m:03s remains)
INFO - root - 2017-12-07 20:00:27.771101: step 48370, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 57h:13m:08s remains)
INFO - root - 2017-12-07 20:00:34.532187: step 48380, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 55h:48m:07s remains)
INFO - root - 2017-12-07 20:00:41.314446: step 48390, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 49h:36m:54s remains)
INFO - root - 2017-12-07 20:00:48.134037: step 48400, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 53h:14m:27s remains)
2017-12-07 20:00:48.900666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3685379 -4.3603315 -4.3405271 -4.3075414 -4.2386446 -4.1417294 -4.0456948 -4.00546 -4.0833311 -4.1863046 -4.2292628 -4.2217531 -4.1963844 -4.1885972 -4.1764464][-4.3714457 -4.3639865 -4.345583 -4.3134069 -4.2432022 -4.1404529 -4.0338082 -3.9887853 -4.0735807 -4.1824751 -4.228219 -4.2236137 -4.2042928 -4.201973 -4.1916623][-4.3722615 -4.3650446 -4.3476295 -4.3162374 -4.2445545 -4.1356716 -4.0188651 -3.9670212 -4.0558462 -4.1714392 -4.2228365 -4.2254944 -4.2154174 -4.2179642 -4.2096243][-4.3731308 -4.3656731 -4.3485904 -4.3184838 -4.2473087 -4.1355762 -4.00999 -3.9484851 -4.0419841 -4.1631336 -4.21891 -4.2280431 -4.2253051 -4.2321606 -4.224545][-4.373539 -4.3660665 -4.3488088 -4.3194432 -4.2485614 -4.1351805 -4.0015664 -3.9272366 -4.0251923 -4.1517854 -4.21317 -4.2279305 -4.2299719 -4.2398629 -4.2342591][-4.373651 -4.3662968 -4.3487439 -4.3191614 -4.2483511 -4.1331878 -3.9921775 -3.906534 -4.0083771 -4.1396732 -4.2043509 -4.2235155 -4.2303858 -4.2431641 -4.2411][-4.3742037 -4.3664622 -4.3486767 -4.3184147 -4.2478027 -4.1323581 -3.9888654 -3.8980391 -4.00118 -4.1337729 -4.1994824 -4.2208414 -4.2339215 -4.2511516 -4.253264][-4.3741312 -4.3665628 -4.3486185 -4.3178396 -4.2479367 -4.1348863 -3.9969606 -3.9107609 -4.0124068 -4.1410818 -4.2036085 -4.2241335 -4.2404504 -4.2587595 -4.2609739][-4.3740015 -4.3665781 -4.3488173 -4.318769 -4.2510862 -4.1428294 -4.0153809 -3.9391971 -4.0363545 -4.1587114 -4.2171559 -4.2342839 -4.2484155 -4.26336 -4.2653637][-4.3732071 -4.3662481 -4.3486753 -4.3200912 -4.2552757 -4.1514244 -4.0341043 -3.9693363 -4.0615592 -4.177774 -4.2300467 -4.2414069 -4.2522559 -4.2632031 -4.2656269][-4.3723059 -4.3650408 -4.3478713 -4.3204775 -4.2579355 -4.1581454 -4.0491309 -3.9939442 -4.0819068 -4.1934114 -4.2400393 -4.2476974 -4.25651 -4.2658381 -4.2700429][-4.3708048 -4.3627338 -4.3452568 -4.3175592 -4.2569342 -4.1608443 -4.0587864 -4.012857 -4.0974836 -4.202517 -4.2445006 -4.2525835 -4.2624159 -4.272305 -4.2780647][-4.3690643 -4.3600297 -4.3416862 -4.3125615 -4.2523661 -4.16164 -4.0681515 -4.03133 -4.1105638 -4.2063546 -4.2453895 -4.2558436 -4.2678356 -4.2784042 -4.2853618][-4.3673878 -4.3574791 -4.3389468 -4.3094182 -4.2511892 -4.1685476 -4.0872135 -4.0578308 -4.1268864 -4.209609 -4.2455063 -4.2593751 -4.273603 -4.2843275 -4.2916083][-4.3661633 -4.3550606 -4.3365378 -4.3074794 -4.2534823 -4.1808157 -4.1125178 -4.0886817 -4.14583 -4.2141514 -4.2487187 -4.2668104 -4.2819204 -4.2918911 -4.2972264]]...]
INFO - root - 2017-12-07 20:00:55.510536: step 48410, loss = 2.07, batch loss = 2.01 (15.0 examples/sec; 0.535 sec/batch; 42h:11m:12s remains)
INFO - root - 2017-12-07 20:01:02.287803: step 48420, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 51h:34m:19s remains)
INFO - root - 2017-12-07 20:01:09.148663: step 48430, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 53h:46m:49s remains)
INFO - root - 2017-12-07 20:01:15.929019: step 48440, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 54h:54m:36s remains)
INFO - root - 2017-12-07 20:01:22.728474: step 48450, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.718 sec/batch; 56h:40m:19s remains)
INFO - root - 2017-12-07 20:01:29.569399: step 48460, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 54h:49m:42s remains)
INFO - root - 2017-12-07 20:01:36.275947: step 48470, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 51h:12m:44s remains)
INFO - root - 2017-12-07 20:01:43.197186: step 48480, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 52h:53m:18s remains)
INFO - root - 2017-12-07 20:01:49.986354: step 48490, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 55h:28m:21s remains)
INFO - root - 2017-12-07 20:01:56.788818: step 48500, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 57h:30m:57s remains)
2017-12-07 20:01:57.513517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3334112 -4.3304343 -4.327435 -4.3285136 -4.3317413 -4.3328824 -4.330585 -4.3296976 -4.331183 -4.3337536 -4.341536 -4.3523245 -4.3589177 -4.3611941 -4.3643694][-4.3098474 -4.2997174 -4.2932577 -4.2934508 -4.2973843 -4.2982893 -4.2952242 -4.2953539 -4.3001885 -4.30768 -4.3224478 -4.3388767 -4.3480535 -4.3500276 -4.354691][-4.2771273 -4.2581482 -4.2446237 -4.2405334 -4.2403889 -4.2389669 -4.2370868 -4.2413483 -4.2541256 -4.2731314 -4.2988243 -4.3189635 -4.3280425 -4.3286309 -4.3327479][-4.2415695 -4.2137656 -4.19178 -4.1828461 -4.177351 -4.1695104 -4.1647573 -4.170918 -4.1930552 -4.2278733 -4.2656965 -4.2868271 -4.2928791 -4.2927737 -4.2968807][-4.210453 -4.1773138 -4.152328 -4.1411786 -4.1295781 -4.112586 -4.0996466 -4.1001568 -4.1294956 -4.1786819 -4.2256737 -4.245399 -4.2516761 -4.253356 -4.25823][-4.1850967 -4.1511412 -4.1269507 -4.1111536 -4.090332 -4.0586209 -4.0282097 -4.0170255 -4.053606 -4.1158061 -4.1699553 -4.1959271 -4.2120118 -4.2218132 -4.2289419][-4.1580753 -4.1226845 -4.0993824 -4.0767241 -4.0431395 -3.9943748 -3.9422038 -3.923039 -3.969233 -4.0397286 -4.0979152 -4.1360378 -4.1685081 -4.1915059 -4.2065439][-4.1316786 -4.0920029 -4.0697327 -4.0469289 -4.0064878 -3.9437497 -3.8762624 -3.8565245 -3.909085 -3.975908 -4.0358081 -4.0867047 -4.1344585 -4.1688385 -4.193574][-4.1288071 -4.0885386 -4.0695982 -4.0529027 -4.0168319 -3.9581218 -3.9017277 -3.8919683 -3.9338756 -3.9852793 -4.0364237 -4.0875955 -4.1365471 -4.1718068 -4.2004962][-4.1611142 -4.1250997 -4.1083083 -4.0922146 -4.0619836 -4.0179834 -3.9835718 -3.9837053 -4.0121169 -4.049366 -4.0876055 -4.1280122 -4.1646953 -4.1925626 -4.2187328][-4.21728 -4.1869726 -4.1685414 -4.14801 -4.1204443 -4.0886726 -4.0695734 -4.0713162 -4.09284 -4.12198 -4.1490083 -4.1771064 -4.2007279 -4.2207475 -4.2413774][-4.270884 -4.2472606 -4.2288537 -4.2087083 -4.1870651 -4.1674194 -4.159831 -4.1644058 -4.1826363 -4.203351 -4.2187228 -4.2349076 -4.2484646 -4.2599082 -4.2719178][-4.3085494 -4.2916527 -4.2761903 -4.262269 -4.2486424 -4.2390738 -4.2394962 -4.2470016 -4.2621446 -4.2751379 -4.2825909 -4.2917795 -4.2995076 -4.3042912 -4.3084354][-4.3330054 -4.3219671 -4.3103395 -4.3023424 -4.2945557 -4.2899189 -4.2938509 -4.3019381 -4.3123326 -4.3218307 -4.3281317 -4.3343081 -4.3390183 -4.341495 -4.3425603][-4.3488846 -4.3426714 -4.3351884 -4.3312197 -4.3265185 -4.323667 -4.3262138 -4.331389 -4.337791 -4.3441482 -4.3497033 -4.3547387 -4.3586078 -4.3613014 -4.3620434]]...]
INFO - root - 2017-12-07 20:02:04.033042: step 48510, loss = 2.06, batch loss = 2.00 (15.6 examples/sec; 0.514 sec/batch; 40h:31m:30s remains)
INFO - root - 2017-12-07 20:02:10.901759: step 48520, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 58h:04m:56s remains)
INFO - root - 2017-12-07 20:02:17.773205: step 48530, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 54h:48m:23s remains)
INFO - root - 2017-12-07 20:02:24.535991: step 48540, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 50h:32m:06s remains)
INFO - root - 2017-12-07 20:02:31.323736: step 48550, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 51h:02m:25s remains)
INFO - root - 2017-12-07 20:02:38.106709: step 48560, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 55h:17m:10s remains)
INFO - root - 2017-12-07 20:02:44.970593: step 48570, loss = 2.10, batch loss = 2.05 (11.0 examples/sec; 0.730 sec/batch; 57h:33m:05s remains)
INFO - root - 2017-12-07 20:02:51.784770: step 48580, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 53h:59m:21s remains)
INFO - root - 2017-12-07 20:02:58.524962: step 48590, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 51h:30m:48s remains)
INFO - root - 2017-12-07 20:03:05.299259: step 48600, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 50h:44m:48s remains)
2017-12-07 20:03:06.079430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3513436 -4.3442607 -4.3355036 -4.3295355 -4.3277688 -4.3292637 -4.3332424 -4.3380208 -4.3430281 -4.346396 -4.3477931 -4.34817 -4.3493438 -4.3500676 -4.3514147][-4.3348408 -4.3203068 -4.3053079 -4.2961841 -4.295547 -4.3024778 -4.3112555 -4.3198876 -4.3285074 -4.3345809 -4.3368835 -4.3358 -4.3359294 -4.3369207 -4.3402138][-4.3045764 -4.281497 -4.25896 -4.2440562 -4.2416911 -4.2511997 -4.2616529 -4.271102 -4.2804637 -4.28693 -4.2894568 -4.2897053 -4.2920728 -4.2965994 -4.308229][-4.2684216 -4.2394743 -4.2099476 -4.1889586 -4.1818175 -4.1848164 -4.18697 -4.1890655 -4.1962476 -4.2021713 -4.2039075 -4.2064252 -4.2144742 -4.2292976 -4.2560782][-4.2273035 -4.1969814 -4.1657944 -4.1430645 -4.129941 -4.1185632 -4.1019354 -4.0844283 -4.0812564 -4.0891576 -4.0942068 -4.1000857 -4.1174574 -4.1494246 -4.1968455][-4.1810951 -4.1535277 -4.12857 -4.1072083 -4.0840192 -4.0509148 -4.00161 -3.9582407 -3.9459209 -3.9628415 -3.983397 -4.0029888 -4.0354643 -4.087646 -4.1540909][-4.146174 -4.1225595 -4.1054382 -4.087647 -4.057765 -4.00135 -3.9212582 -3.8656766 -3.8627625 -3.8995256 -3.9409633 -3.9765358 -4.0179243 -4.0781174 -4.1484513][-4.1411252 -4.1268034 -4.1196203 -4.1077132 -4.07931 -4.0143409 -3.9287381 -3.885709 -3.9020991 -3.9516459 -4.001183 -4.0399995 -4.0760942 -4.1288857 -4.1869211][-4.1582656 -4.153182 -4.1553454 -4.1534953 -4.1367865 -4.0896316 -4.0329242 -4.0113192 -4.0307431 -4.0687695 -4.1049447 -4.12888 -4.1482005 -4.1863627 -4.2292805][-4.1860337 -4.1816325 -4.1842742 -4.1891646 -4.1886888 -4.1706867 -4.1450739 -4.1363244 -4.151155 -4.1714988 -4.1861243 -4.1882968 -4.1910462 -4.2171178 -4.2498369][-4.2075419 -4.1963878 -4.192719 -4.195003 -4.2054763 -4.2103891 -4.2033134 -4.203454 -4.217339 -4.2290597 -4.2244987 -4.2056203 -4.1921759 -4.2099543 -4.24083][-4.2090645 -4.1890059 -4.1754155 -4.1712832 -4.1867375 -4.2033744 -4.2081118 -4.2205038 -4.2426243 -4.250196 -4.2323418 -4.1982708 -4.1732626 -4.186245 -4.2203064][-4.1932635 -4.16278 -4.135159 -4.1237383 -4.1422281 -4.1690316 -4.1878982 -4.21992 -4.2489681 -4.2522712 -4.2267513 -4.1861744 -4.1567545 -4.1688433 -4.2053256][-4.1831923 -4.1424427 -4.09957 -4.0838218 -4.1051521 -4.1405997 -4.1708708 -4.2124467 -4.2402644 -4.2387152 -4.2155523 -4.1803923 -4.1519156 -4.1641469 -4.2007151][-4.1746955 -4.123817 -4.0657659 -4.0485024 -4.0730309 -4.1129689 -4.1522403 -4.1957393 -4.2201424 -4.2223511 -4.2078052 -4.1797538 -4.1544795 -4.1682844 -4.203784]]...]
INFO - root - 2017-12-07 20:03:12.623533: step 48610, loss = 2.08, batch loss = 2.02 (16.0 examples/sec; 0.501 sec/batch; 39h:31m:53s remains)
INFO - root - 2017-12-07 20:03:19.267860: step 48620, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 49h:36m:03s remains)
INFO - root - 2017-12-07 20:03:26.081104: step 48630, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.693 sec/batch; 54h:40m:14s remains)
INFO - root - 2017-12-07 20:03:32.844479: step 48640, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 54h:44m:47s remains)
INFO - root - 2017-12-07 20:03:39.708102: step 48650, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 54h:54m:43s remains)
INFO - root - 2017-12-07 20:03:46.542930: step 48660, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 53h:22m:18s remains)
INFO - root - 2017-12-07 20:03:53.428986: step 48670, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 51h:53m:58s remains)
INFO - root - 2017-12-07 20:04:00.284782: step 48680, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 55h:19m:40s remains)
INFO - root - 2017-12-07 20:04:07.087309: step 48690, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 53h:38m:33s remains)
INFO - root - 2017-12-07 20:04:13.915986: step 48700, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 54h:03m:10s remains)
2017-12-07 20:04:14.655826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1558094 -4.1641622 -4.1959558 -4.2036777 -4.1805758 -4.1541705 -4.1550164 -4.1757054 -4.2042279 -4.2264519 -4.23394 -4.2320304 -4.2192006 -4.1975617 -4.1748114][-4.1285925 -4.1404767 -4.1790676 -4.1955743 -4.1745977 -4.1397839 -4.1359067 -4.1602774 -4.1894274 -4.2112851 -4.2182679 -4.21675 -4.2112665 -4.2017093 -4.1930938][-4.10043 -4.11443 -4.1521378 -4.1733589 -4.159626 -4.127399 -4.1284165 -4.159586 -4.1871738 -4.205102 -4.2140713 -4.2164912 -4.2137523 -4.2082858 -4.208559][-4.0823374 -4.0942307 -4.1252975 -4.1450834 -4.1393743 -4.1168771 -4.1271758 -4.1672568 -4.1919203 -4.2016573 -4.2100391 -4.217041 -4.2141619 -4.2075515 -4.2124619][-4.0780463 -4.08899 -4.1106057 -4.1232114 -4.1209183 -4.1084991 -4.121212 -4.158349 -4.1801295 -4.188787 -4.2004881 -4.2137661 -4.2133727 -4.2060847 -4.2107391][-4.0747175 -4.0953069 -4.1126714 -4.1185913 -4.1146183 -4.1052051 -4.1062603 -4.12304 -4.1410856 -4.1618032 -4.1877384 -4.2084808 -4.2117338 -4.2034435 -4.2062092][-4.0721707 -4.1031046 -4.1200185 -4.1244335 -4.1175117 -4.0999303 -4.0755892 -4.0631771 -4.0818586 -4.1243935 -4.1733046 -4.2005186 -4.2040472 -4.1917562 -4.1923275][-4.0855966 -4.1206756 -4.1377254 -4.1442566 -4.1400161 -4.1199484 -4.0777979 -4.0423169 -4.0563245 -4.1099877 -4.1693039 -4.1990013 -4.1984229 -4.1821222 -4.1808844][-4.1111383 -4.1439581 -4.158289 -4.1671615 -4.1675816 -4.1526566 -4.1108494 -4.0657487 -4.0687518 -4.1205568 -4.1774197 -4.2045016 -4.2016783 -4.1870213 -4.1907868][-4.145277 -4.1767373 -4.1886587 -4.1930795 -4.1900387 -4.1752005 -4.1386442 -4.0936127 -4.0901613 -4.1379514 -4.1926908 -4.2184925 -4.2180967 -4.2074137 -4.2140093][-4.1810822 -4.2045832 -4.21505 -4.2158051 -4.2091861 -4.1926675 -4.1598949 -4.11897 -4.1134915 -4.1576614 -4.20778 -4.2351952 -4.238307 -4.2327833 -4.2386522][-4.2057972 -4.2186761 -4.2282534 -4.2315912 -4.2285776 -4.2162008 -4.1897268 -4.1544623 -4.1443114 -4.1779122 -4.2174683 -4.2426724 -4.2472386 -4.2425179 -4.2420731][-4.2222447 -4.2238693 -4.2309427 -4.2381959 -4.2413955 -4.2360525 -4.2190843 -4.1919279 -4.1773124 -4.1962442 -4.2198567 -4.2389545 -4.242578 -4.2370868 -4.2286019][-4.2327332 -4.2263265 -4.2274966 -4.2336812 -4.2381215 -4.2363777 -4.2254295 -4.2051735 -4.188406 -4.1945729 -4.2087131 -4.2263932 -4.2330313 -4.2269306 -4.2116251][-4.2236109 -4.2165179 -4.2168083 -4.2221274 -4.2243242 -4.2223063 -4.2139845 -4.1975646 -4.1826067 -4.1824918 -4.1908293 -4.2051229 -4.2137337 -4.2093716 -4.1939778]]...]
INFO - root - 2017-12-07 20:04:21.437146: step 48710, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 55h:34m:42s remains)
INFO - root - 2017-12-07 20:04:28.290950: step 48720, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 54h:25m:43s remains)
INFO - root - 2017-12-07 20:04:35.201284: step 48730, loss = 2.08, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 55h:43m:16s remains)
INFO - root - 2017-12-07 20:04:42.060094: step 48740, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 53h:27m:05s remains)
INFO - root - 2017-12-07 20:04:48.831286: step 48750, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 55h:50m:37s remains)
INFO - root - 2017-12-07 20:04:55.592454: step 48760, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 55h:13m:07s remains)
INFO - root - 2017-12-07 20:05:02.331557: step 48770, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 57h:00m:04s remains)
INFO - root - 2017-12-07 20:05:09.046427: step 48780, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 49h:35m:32s remains)
INFO - root - 2017-12-07 20:05:15.899781: step 48790, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 51h:45m:51s remains)
INFO - root - 2017-12-07 20:05:22.653040: step 48800, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 54h:24m:29s remains)
2017-12-07 20:05:23.414618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2335362 -4.1794858 -4.1334434 -4.0921125 -4.0557852 -4.0540524 -4.1003828 -4.1708808 -4.2347832 -4.276967 -4.2936931 -4.2985978 -4.3008189 -4.3015532 -4.3021][-4.233078 -4.2092094 -4.19751 -4.1747589 -4.1352634 -4.108727 -4.1211925 -4.1691284 -4.2266755 -4.2707725 -4.2933521 -4.3027573 -4.3062081 -4.305984 -4.3053269][-4.2259846 -4.2266731 -4.2399268 -4.2349291 -4.2083073 -4.1797571 -4.1702189 -4.1879025 -4.2250419 -4.261785 -4.288291 -4.3024597 -4.3080935 -4.308084 -4.3068457][-4.2222986 -4.2389021 -4.2655897 -4.2714243 -4.2530446 -4.2259455 -4.2017813 -4.1966858 -4.2147779 -4.2457676 -4.2768879 -4.2971482 -4.3071628 -4.3096676 -4.309176][-4.2263131 -4.250515 -4.2803469 -4.288413 -4.2687416 -4.2365961 -4.2014751 -4.1814985 -4.1872597 -4.2178388 -4.2566338 -4.2851887 -4.3013358 -4.3097248 -4.3121881][-4.2371392 -4.2609396 -4.2862 -4.2881551 -4.2609057 -4.2176933 -4.171268 -4.1414404 -4.140193 -4.1737914 -4.2227969 -4.26405 -4.2903137 -4.3067589 -4.3136339][-4.2560263 -4.2755103 -4.2913089 -4.2802482 -4.236927 -4.1783614 -4.1196856 -4.0838962 -4.0828748 -4.1221328 -4.1805105 -4.2349606 -4.2741876 -4.299511 -4.312068][-4.2788472 -4.2933707 -4.3033938 -4.2847352 -4.2293448 -4.1568193 -4.083375 -4.0356135 -4.0327911 -4.0763574 -4.1405959 -4.2021437 -4.2523084 -4.2864995 -4.3048406][-4.2944989 -4.3054075 -4.3125296 -4.2953835 -4.2454019 -4.1767845 -4.1029944 -4.0425005 -4.0244284 -4.0538621 -4.1093206 -4.1691842 -4.2244086 -4.2644777 -4.2889333][-4.3033452 -4.3084145 -4.30736 -4.2926965 -4.2569323 -4.2087369 -4.157475 -4.1130247 -4.0864139 -4.0826612 -4.1044197 -4.1467295 -4.1971545 -4.2392683 -4.267714][-4.3046241 -4.3030586 -4.2947941 -4.2783365 -4.2520976 -4.2230811 -4.1974096 -4.1783657 -4.16597 -4.1514158 -4.1465969 -4.1607456 -4.19361 -4.2280536 -4.2536492][-4.2980404 -4.2923269 -4.2800579 -4.2592773 -4.2321777 -4.2128735 -4.20757 -4.2138095 -4.2231479 -4.2166009 -4.2057109 -4.2050877 -4.2190456 -4.2393842 -4.2566843][-4.2870054 -4.2791944 -4.267592 -4.2453113 -4.2169933 -4.202868 -4.2093768 -4.2314253 -4.2556953 -4.260324 -4.2541595 -4.25142 -4.2556615 -4.2630582 -4.2701025][-4.2812953 -4.272541 -4.2635741 -4.2438793 -4.2183676 -4.205205 -4.2127633 -4.2417483 -4.274384 -4.2870812 -4.283392 -4.2799363 -4.2802892 -4.2809896 -4.2813787][-4.2847137 -4.2756639 -4.2683296 -4.2521496 -4.23157 -4.2175283 -4.2209978 -4.2475686 -4.2802067 -4.2957668 -4.2945371 -4.291749 -4.290627 -4.289156 -4.2873259]]...]
INFO - root - 2017-12-07 20:05:29.957102: step 48810, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.661 sec/batch; 52h:05m:14s remains)
INFO - root - 2017-12-07 20:05:36.748780: step 48820, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 54h:11m:28s remains)
INFO - root - 2017-12-07 20:05:43.677236: step 48830, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 57h:59m:43s remains)
INFO - root - 2017-12-07 20:05:50.526312: step 48840, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 54h:03m:00s remains)
INFO - root - 2017-12-07 20:05:57.322078: step 48850, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 50h:04m:00s remains)
INFO - root - 2017-12-07 20:06:04.143686: step 48860, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 51h:03m:32s remains)
INFO - root - 2017-12-07 20:06:10.964551: step 48870, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 55h:01m:05s remains)
INFO - root - 2017-12-07 20:06:17.775280: step 48880, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 57h:48m:54s remains)
INFO - root - 2017-12-07 20:06:24.661311: step 48890, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 55h:33m:59s remains)
INFO - root - 2017-12-07 20:06:31.383931: step 48900, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 49h:39m:41s remains)
2017-12-07 20:06:32.206742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1196179 -4.1005917 -4.0886359 -4.0826216 -4.0697203 -4.0645342 -4.0674686 -4.0724268 -4.0708075 -4.0566478 -4.0406561 -4.0385871 -4.0528793 -4.0777688 -4.1115565][-4.0824003 -4.0523057 -4.0421944 -4.0527391 -4.0552158 -4.0596857 -4.0730104 -4.0909944 -4.1072254 -4.1088028 -4.1056232 -4.1169286 -4.1326785 -4.1445374 -4.1575456][-4.040134 -4.0019011 -4.0075188 -4.0496855 -4.07926 -4.0988765 -4.1215925 -4.1406984 -4.1587257 -4.1582346 -4.1574683 -4.1692114 -4.1747823 -4.17095 -4.164917][-4.0423493 -4.009305 -4.0296445 -4.0856848 -4.1221843 -4.1385188 -4.1554813 -4.1662369 -4.1772609 -4.1788068 -4.1826725 -4.1891017 -4.1871262 -4.176806 -4.1602383][-4.0960283 -4.0618954 -4.0755358 -4.121387 -4.1525068 -4.1633425 -4.1706161 -4.1713352 -4.1758771 -4.1864891 -4.2047195 -4.212944 -4.2069721 -4.1923766 -4.1715856][-4.151051 -4.1095352 -4.1044145 -4.1304727 -4.1564112 -4.1679311 -4.1713653 -4.1657281 -4.1673894 -4.1862135 -4.2190542 -4.2409725 -4.2401519 -4.226264 -4.2037649][-4.1895709 -4.1390209 -4.1102238 -4.1096683 -4.12523 -4.1411414 -4.1475081 -4.1385603 -4.1369462 -4.1571646 -4.1990666 -4.2383652 -4.2550535 -4.252718 -4.2387366][-4.221849 -4.1685252 -4.1237111 -4.0959363 -4.0904336 -4.1043425 -4.1207314 -4.1180305 -4.11304 -4.1205511 -4.1588039 -4.20892 -4.2426805 -4.25803 -4.2610517][-4.2395525 -4.1969333 -4.150775 -4.1136746 -4.09719 -4.1019058 -4.1205964 -4.1302958 -4.1279826 -4.1278825 -4.1513968 -4.1930642 -4.233593 -4.2603469 -4.2748661][-4.2297173 -4.2059569 -4.1710434 -4.1445351 -4.1382723 -4.1443996 -4.1582165 -4.1685905 -4.1706762 -4.1726618 -4.1890388 -4.2191234 -4.2514634 -4.2735662 -4.2875896][-4.2057095 -4.2009807 -4.185842 -4.1775651 -4.183495 -4.1948261 -4.207221 -4.21653 -4.2173004 -4.2164574 -4.2278242 -4.2538047 -4.277586 -4.2876658 -4.2925482][-4.20302 -4.2114482 -4.2142835 -4.2200117 -4.2297831 -4.2394643 -4.2517219 -4.2580686 -4.2550697 -4.2505603 -4.2575788 -4.2766333 -4.2932811 -4.2946262 -4.2919445][-4.2321568 -4.2417808 -4.2475805 -4.2576985 -4.2675705 -4.2729931 -4.282824 -4.2890816 -4.2855835 -4.2789879 -4.2803559 -4.2894645 -4.2952085 -4.2901092 -4.2830911][-4.2623715 -4.2655149 -4.265749 -4.2787251 -4.2934313 -4.3020329 -4.3096375 -4.3136344 -4.3090367 -4.2991371 -4.2944779 -4.2935362 -4.2893562 -4.2785187 -4.2673817][-4.2740407 -4.2767463 -4.2791142 -4.2922134 -4.3078637 -4.3192635 -4.3245287 -4.3255048 -4.3192959 -4.307066 -4.2975378 -4.2903142 -4.2791772 -4.2633767 -4.2489009]]...]
INFO - root - 2017-12-07 20:06:38.844069: step 48910, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 54h:05m:33s remains)
INFO - root - 2017-12-07 20:06:45.527790: step 48920, loss = 2.09, batch loss = 2.03 (14.4 examples/sec; 0.555 sec/batch; 43h:42m:37s remains)
INFO - root - 2017-12-07 20:06:52.378513: step 48930, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 55h:25m:38s remains)
INFO - root - 2017-12-07 20:06:59.213725: step 48940, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 54h:37m:36s remains)
INFO - root - 2017-12-07 20:07:06.106436: step 48950, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 55h:25m:23s remains)
INFO - root - 2017-12-07 20:07:13.007072: step 48960, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 53h:06m:52s remains)
INFO - root - 2017-12-07 20:07:19.840056: step 48970, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 52h:09m:14s remains)
INFO - root - 2017-12-07 20:07:26.728707: step 48980, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.732 sec/batch; 57h:40m:38s remains)
INFO - root - 2017-12-07 20:07:33.527060: step 48990, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 55h:30m:36s remains)
INFO - root - 2017-12-07 20:07:40.445924: step 49000, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 52h:45m:57s remains)
2017-12-07 20:07:41.113099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3072233 -4.30529 -4.3048768 -4.2983027 -4.2924018 -4.2907872 -4.2891836 -4.2787623 -4.2588177 -4.2536373 -4.2584109 -4.2677703 -4.2780671 -4.2937951 -4.3085384][-4.2936645 -4.2934985 -4.2942085 -4.2827621 -4.2758446 -4.275835 -4.2758379 -4.265666 -4.2417178 -4.234199 -4.2365394 -4.2458162 -4.2579203 -4.2729816 -4.289341][-4.2700205 -4.2733407 -4.2760525 -4.2620659 -4.2552986 -4.25559 -4.2516136 -4.2424107 -4.2198148 -4.2125483 -4.2122688 -4.2236352 -4.2401361 -4.2553754 -4.2738838][-4.25229 -4.2572093 -4.2619643 -4.2458143 -4.236701 -4.2333531 -4.2264042 -4.2180419 -4.194335 -4.1848321 -4.18379 -4.1987524 -4.2207618 -4.2401018 -4.263989][-4.2414618 -4.235435 -4.2298846 -4.2096224 -4.197175 -4.1873775 -4.1754136 -4.164711 -4.1336741 -4.1207294 -4.1225834 -4.1471381 -4.1823668 -4.2173729 -4.2550845][-4.2356644 -4.2188077 -4.2036638 -4.1778069 -4.1523 -4.1332812 -4.1171594 -4.1027651 -4.06478 -4.0532146 -4.0591474 -4.0887761 -4.137918 -4.1904578 -4.2415047][-4.2238531 -4.1958456 -4.173255 -4.1414242 -4.10408 -4.0775414 -4.055028 -4.0319872 -3.9886987 -3.9830053 -3.9905231 -4.0177231 -4.0795889 -4.1555557 -4.2218661][-4.2034211 -4.1655526 -4.1367941 -4.099946 -4.0569592 -4.0236192 -3.9932725 -3.9575891 -3.9077139 -3.9124167 -3.9263027 -3.9541564 -4.0291109 -4.1240959 -4.2030158][-4.1864567 -4.1445761 -4.1145806 -4.0810294 -4.0445266 -4.0138121 -3.9809802 -3.9419928 -3.8911786 -3.9051244 -3.9319005 -3.9681211 -4.04308 -4.1263 -4.1959748][-4.1832061 -4.146492 -4.1279507 -4.1062961 -4.0833 -4.0615697 -4.0363836 -4.0042844 -3.9581037 -3.9676986 -3.9949775 -4.0351033 -4.09836 -4.1581254 -4.2069712][-4.1971908 -4.1691461 -4.1581817 -4.1461496 -4.1333694 -4.1203756 -4.1031694 -4.0827856 -4.0454812 -4.0451956 -4.0646133 -4.1049614 -4.1538606 -4.1944022 -4.2300177][-4.2173023 -4.1978793 -4.1922774 -4.1862512 -4.1804981 -4.1743746 -4.1650472 -4.1538138 -4.1251469 -4.1169424 -4.12508 -4.15644 -4.1937194 -4.2240782 -4.2531142][-4.2413564 -4.2292666 -4.2268033 -4.2222819 -4.2163663 -4.21333 -4.2109504 -4.2069597 -4.1871638 -4.1774158 -4.1792989 -4.1999755 -4.228847 -4.2544928 -4.2790051][-4.2651768 -4.2596259 -4.2568016 -4.2531757 -4.2509394 -4.2506785 -4.24869 -4.2467289 -4.2355733 -4.2277832 -4.2267313 -4.239459 -4.260181 -4.2821302 -4.3011055][-4.2820826 -4.2795386 -4.2784491 -4.2782774 -4.2794847 -4.2823615 -4.2833586 -4.2822485 -4.2765226 -4.27145 -4.2698736 -4.277245 -4.290411 -4.3062162 -4.3188329]]...]
INFO - root - 2017-12-07 20:07:47.706908: step 49010, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 53h:31m:04s remains)
INFO - root - 2017-12-07 20:07:54.549212: step 49020, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 57h:51m:17s remains)
INFO - root - 2017-12-07 20:08:01.408110: step 49030, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 53h:00m:24s remains)
INFO - root - 2017-12-07 20:08:08.198064: step 49040, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 50h:38m:47s remains)
INFO - root - 2017-12-07 20:08:15.111214: step 49050, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 53h:34m:56s remains)
INFO - root - 2017-12-07 20:08:22.038848: step 49060, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 59h:13m:43s remains)
INFO - root - 2017-12-07 20:08:28.835567: step 49070, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 54h:38m:28s remains)
INFO - root - 2017-12-07 20:08:35.616249: step 49080, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 56h:23m:49s remains)
INFO - root - 2017-12-07 20:08:42.315512: step 49090, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 49h:32m:43s remains)
INFO - root - 2017-12-07 20:08:49.135901: step 49100, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 53h:19m:46s remains)
2017-12-07 20:08:49.860524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3298416 -4.3288951 -4.3058591 -4.2705231 -4.2454414 -4.2042632 -4.1690259 -4.1744385 -4.1807094 -4.185081 -4.1946321 -4.2140641 -4.2372136 -4.2610693 -4.2800293][-4.3345189 -4.3316841 -4.3002729 -4.2552176 -4.2181363 -4.1603508 -4.1128321 -4.11859 -4.1278172 -4.1338434 -4.1470261 -4.175127 -4.2025394 -4.2284737 -4.2485323][-4.3353944 -4.3286748 -4.2892447 -4.235054 -4.1853337 -4.1137819 -4.0587587 -4.0713162 -4.0886335 -4.0977778 -4.1132746 -4.1422806 -4.1661777 -4.1896548 -4.2092733][-4.3335147 -4.3215895 -4.2761173 -4.2160363 -4.1535578 -4.0732646 -4.0216608 -4.0468049 -4.0743852 -4.0867677 -4.0973148 -4.1103997 -4.1153584 -4.1255665 -4.1476803][-4.33052 -4.3128495 -4.2627211 -4.1946907 -4.1203318 -4.0418825 -4.0062304 -4.0506167 -4.0874882 -4.0982471 -4.1003003 -4.0919013 -4.0649657 -4.0537314 -4.0859551][-4.3297911 -4.3084378 -4.2550697 -4.1785088 -4.0887303 -4.0104241 -3.9902754 -4.050528 -4.0984826 -4.1087818 -4.1056967 -4.08103 -4.0269818 -3.998451 -4.049387][-4.3277392 -4.3065128 -4.2525206 -4.167552 -4.0631161 -3.9741032 -3.9550164 -4.0248632 -4.0840411 -4.0996089 -4.1039906 -4.0815134 -4.0247927 -3.9947348 -4.0556555][-4.3235507 -4.3034635 -4.2542491 -4.169383 -4.055162 -3.9517832 -3.9243925 -3.994256 -4.0550036 -4.0726452 -4.0889583 -4.0817184 -4.042604 -4.02356 -4.0782394][-4.322175 -4.30447 -4.264708 -4.19269 -4.0880709 -3.987947 -3.9533205 -4.0004497 -4.0485797 -4.0661144 -4.0896878 -4.09611 -4.0770073 -4.0705876 -4.1135154][-4.3250561 -4.312922 -4.2797546 -4.2211308 -4.14094 -4.0639777 -4.0309291 -4.0593796 -4.0977979 -4.1161776 -4.1420455 -4.1559243 -4.1487703 -4.1468358 -4.1734505][-4.3317647 -4.3233385 -4.2941408 -4.2440996 -4.1862521 -4.1323118 -4.1051178 -4.1296821 -4.16429 -4.1822853 -4.2024422 -4.2144589 -4.2130141 -4.2129035 -4.2272034][-4.3365107 -4.3305392 -4.3043079 -4.2590942 -4.2129564 -4.17175 -4.1480412 -4.1739364 -4.2103925 -4.2281661 -4.240449 -4.2485366 -4.2499971 -4.2510104 -4.2571344][-4.329711 -4.3234997 -4.2981334 -4.2574458 -4.2176213 -4.1811118 -4.1589069 -4.1896248 -4.2336473 -4.2556462 -4.2657185 -4.2721624 -4.2724972 -4.2717571 -4.2727146][-4.32052 -4.3115177 -4.28647 -4.2452931 -4.2065277 -4.1713052 -4.1494317 -4.1867323 -4.2380075 -4.2632165 -4.2725778 -4.279274 -4.2785945 -4.2778363 -4.2782035][-4.3163271 -4.3064008 -4.2783775 -4.2339706 -4.1942372 -4.1580262 -4.1366644 -4.17991 -4.2359419 -4.2659373 -4.2774796 -4.2849689 -4.2857571 -4.2840786 -4.282516]]...]
INFO - root - 2017-12-07 20:08:56.488148: step 49110, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.641 sec/batch; 50h:29m:32s remains)
INFO - root - 2017-12-07 20:09:03.249874: step 49120, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 50h:59m:25s remains)
INFO - root - 2017-12-07 20:09:10.104402: step 49130, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.690 sec/batch; 54h:17m:20s remains)
INFO - root - 2017-12-07 20:09:16.998804: step 49140, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 56h:43m:15s remains)
INFO - root - 2017-12-07 20:09:23.845019: step 49150, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 57h:07m:59s remains)
INFO - root - 2017-12-07 20:09:30.724239: step 49160, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 53h:24m:20s remains)
INFO - root - 2017-12-07 20:09:37.538105: step 49170, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 50h:24m:12s remains)
INFO - root - 2017-12-07 20:09:44.363057: step 49180, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 53h:26m:10s remains)
INFO - root - 2017-12-07 20:09:51.216042: step 49190, loss = 2.10, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 56h:01m:09s remains)
INFO - root - 2017-12-07 20:09:57.965718: step 49200, loss = 2.11, batch loss = 2.05 (12.0 examples/sec; 0.665 sec/batch; 52h:18m:43s remains)
2017-12-07 20:09:58.693119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1879873 -4.1895504 -4.2068477 -4.2241 -4.236125 -4.2414074 -4.2428513 -4.241775 -4.23574 -4.2260084 -4.2109537 -4.211359 -4.229991 -4.2587981 -4.2819381][-4.18403 -4.1749129 -4.1817217 -4.1947193 -4.2061987 -4.21273 -4.2126679 -4.2085304 -4.2032762 -4.1984868 -4.1878333 -4.1918659 -4.2113113 -4.2404742 -4.2632232][-4.184268 -4.1664767 -4.1645637 -4.1738205 -4.18591 -4.1910524 -4.1820307 -4.1675572 -4.1604171 -4.158494 -4.1542239 -4.1631403 -4.1850371 -4.2193484 -4.2477055][-4.1819844 -4.1590571 -4.1530552 -4.1653714 -4.1790609 -4.1781244 -4.155344 -4.1279397 -4.1209497 -4.1286373 -4.1333251 -4.1455574 -4.1676927 -4.2060957 -4.2407866][-4.1504555 -4.1283774 -4.1251903 -4.1432328 -4.1559148 -4.1456027 -4.1108027 -4.0773282 -4.07961 -4.1067419 -4.1273565 -4.1453085 -4.1659927 -4.2021866 -4.2387166][-4.1119313 -4.0980158 -4.0964456 -4.1090894 -4.1101313 -4.0815315 -4.0333948 -4.000031 -4.0235767 -4.0789309 -4.1168327 -4.1414618 -4.1649342 -4.2024546 -4.2400203][-4.109458 -4.0973945 -4.0901165 -4.0894337 -4.0677938 -4.0129328 -3.9469237 -3.9192936 -3.9697397 -4.0476232 -4.0967078 -4.1277189 -4.1596088 -4.2044945 -4.2458286][-4.1438012 -4.1314659 -4.1175518 -4.1030717 -4.0637012 -3.9953299 -3.9248319 -3.9112747 -3.9755685 -4.0531511 -4.0965786 -4.129467 -4.1697555 -4.2209764 -4.2651243][-4.1718211 -4.16129 -4.1426992 -4.1198993 -4.0800261 -4.0232005 -3.9733946 -3.9786923 -4.0357714 -4.0937471 -4.1209178 -4.1466208 -4.1872296 -4.2383938 -4.2789969][-4.1731782 -4.1601777 -4.1363149 -4.1131349 -4.0856237 -4.055048 -4.0311327 -4.04716 -4.0906281 -4.1294007 -4.1426892 -4.1590743 -4.1942062 -4.2407217 -4.2780743][-4.163362 -4.1447783 -4.1212821 -4.1053772 -4.0921135 -4.084497 -4.0823421 -4.101028 -4.1291814 -4.15056 -4.1548481 -4.1677294 -4.1988735 -4.2396116 -4.2735305][-4.1724153 -4.1544704 -4.134635 -4.1220188 -4.1152816 -4.1219087 -4.1359768 -4.1545863 -4.169014 -4.1775541 -4.1783457 -4.1904116 -4.2187328 -4.2548809 -4.2843056][-4.2139864 -4.2044969 -4.1898046 -4.1792321 -4.1757393 -4.1871676 -4.2068849 -4.2220845 -4.2257442 -4.2254853 -4.2246447 -4.2353578 -4.258069 -4.2866778 -4.3085117][-4.2675104 -4.2693954 -4.261148 -4.25403 -4.2539239 -4.2629685 -4.2782636 -4.2876453 -4.2852368 -4.27941 -4.275651 -4.2831712 -4.2981629 -4.3154645 -4.327744][-4.3131871 -4.3203788 -4.3164949 -4.3132625 -4.3137341 -4.3182774 -4.327836 -4.3338418 -4.329093 -4.3208985 -4.3166661 -4.3224149 -4.3303008 -4.3371568 -4.3407831]]...]
INFO - root - 2017-12-07 20:10:05.377147: step 49210, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 55h:19m:12s remains)
INFO - root - 2017-12-07 20:10:12.052443: step 49220, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 51h:33m:41s remains)
INFO - root - 2017-12-07 20:10:18.804024: step 49230, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 50h:03m:48s remains)
INFO - root - 2017-12-07 20:10:25.543939: step 49240, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 52h:28m:16s remains)
INFO - root - 2017-12-07 20:10:32.370259: step 49250, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.690 sec/batch; 54h:16m:38s remains)
INFO - root - 2017-12-07 20:10:39.173102: step 49260, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 57h:08m:21s remains)
INFO - root - 2017-12-07 20:10:46.010768: step 49270, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 55h:21m:18s remains)
INFO - root - 2017-12-07 20:10:52.829120: step 49280, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 51h:13m:42s remains)
INFO - root - 2017-12-07 20:10:59.704292: step 49290, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 51h:15m:10s remains)
INFO - root - 2017-12-07 20:11:06.608894: step 49300, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 55h:18m:30s remains)
2017-12-07 20:11:07.354077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3132572 -4.3069811 -4.3006625 -4.2954693 -4.2919297 -4.292665 -4.2925754 -4.2944164 -4.2972264 -4.2939715 -4.2889905 -4.2854242 -4.2791343 -4.272531 -4.2684989][-4.2948589 -4.28249 -4.2749681 -4.2682228 -4.2570567 -4.2502 -4.2487264 -4.25527 -4.2609158 -4.2541456 -4.2462888 -4.2441673 -4.2403173 -4.234839 -4.2323613][-4.271605 -4.2542267 -4.2462349 -4.2379179 -4.2228885 -4.2115974 -4.2119474 -4.2218285 -4.2254248 -4.2163157 -4.2095633 -4.2093463 -4.2065806 -4.1997094 -4.1982722][-4.2542248 -4.2388487 -4.2318616 -4.2230415 -4.204226 -4.1902456 -4.188952 -4.1970658 -4.1978912 -4.1870184 -4.1854844 -4.1893849 -4.1900034 -4.1848426 -4.1828971][-4.2483377 -4.2367334 -4.2296247 -4.21537 -4.1857996 -4.1634355 -4.1521215 -4.1515179 -4.1522975 -4.1441841 -4.1507778 -4.1631379 -4.1724734 -4.176055 -4.1814957][-4.2500048 -4.237174 -4.2240176 -4.1958103 -4.148634 -4.1100683 -4.08171 -4.0700445 -4.0777473 -4.0842443 -4.1045165 -4.1263628 -4.1452436 -4.162909 -4.1814628][-4.2511539 -4.234973 -4.214035 -4.1700621 -4.1007676 -4.038002 -3.993223 -3.9847717 -4.0101914 -4.0389309 -4.0717521 -4.1003709 -4.126349 -4.1528339 -4.1810975][-4.2402835 -4.2194281 -4.1898661 -4.1302333 -4.0372324 -3.9527459 -3.9080496 -3.9211473 -3.9791648 -4.0285945 -4.0672121 -4.095037 -4.1222005 -4.1544709 -4.1846523][-4.2237248 -4.1960468 -4.1605334 -4.0915751 -3.9916353 -3.914968 -3.8965046 -3.9373798 -4.0119476 -4.0672517 -4.1015558 -4.1249084 -4.1476808 -4.177969 -4.2036963][-4.2195983 -4.1928535 -4.1593723 -4.1019278 -4.0243549 -3.9779356 -3.9858787 -4.03513 -4.101357 -4.1411138 -4.1640277 -4.1806197 -4.1956716 -4.21669 -4.2342448][-4.2316155 -4.2118368 -4.1853952 -4.147635 -4.09638 -4.0754533 -4.0938091 -4.1329021 -4.1763186 -4.1997104 -4.2161183 -4.2349405 -4.2487268 -4.2605872 -4.2701368][-4.2565446 -4.2464128 -4.228539 -4.2052894 -4.1788721 -4.173439 -4.1913323 -4.2112913 -4.2289071 -4.2409964 -4.2577739 -4.2791357 -4.2928286 -4.2994761 -4.3053355][-4.2858982 -4.2822018 -4.2709851 -4.2574635 -4.2463388 -4.2513657 -4.2662487 -4.274282 -4.2771683 -4.2826924 -4.2946072 -4.3098116 -4.3182454 -4.3221416 -4.3264136][-4.3076687 -4.3044791 -4.3001418 -4.2966766 -4.2966285 -4.3055167 -4.3159242 -4.3178124 -4.3148966 -4.3160882 -4.3214941 -4.3290133 -4.3325534 -4.3350325 -4.3384995][-4.3181806 -4.3141179 -4.3125758 -4.314539 -4.31813 -4.325017 -4.3314357 -4.3315148 -4.3290005 -4.3284049 -4.3311963 -4.3349237 -4.336308 -4.3383417 -4.3416128]]...]
INFO - root - 2017-12-07 20:11:13.819274: step 49310, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 52h:11m:20s remains)
INFO - root - 2017-12-07 20:11:20.665874: step 49320, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 51h:46m:10s remains)
INFO - root - 2017-12-07 20:11:27.559940: step 49330, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 55h:02m:57s remains)
INFO - root - 2017-12-07 20:11:34.453485: step 49340, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 57h:41m:50s remains)
INFO - root - 2017-12-07 20:11:41.324882: step 49350, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 54h:32m:42s remains)
INFO - root - 2017-12-07 20:11:48.174912: step 49360, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 51h:40m:54s remains)
INFO - root - 2017-12-07 20:11:55.057856: step 49370, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 52h:17m:29s remains)
INFO - root - 2017-12-07 20:12:01.913995: step 49380, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 55h:05m:42s remains)
INFO - root - 2017-12-07 20:12:08.870938: step 49390, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.711 sec/batch; 55h:55m:58s remains)
INFO - root - 2017-12-07 20:12:15.550731: step 49400, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 53h:17m:10s remains)
2017-12-07 20:12:16.260972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765369 -4.2747331 -4.2705703 -4.2653837 -4.2632408 -4.2630239 -4.2596831 -4.249135 -4.2329717 -4.1978054 -4.1729331 -4.1668525 -4.1731339 -4.186377 -4.2180858][-4.2701759 -4.2684932 -4.2631364 -4.2585168 -4.2564688 -4.2524691 -4.2408357 -4.2235827 -4.2062516 -4.1705265 -4.142693 -4.1392293 -4.1545858 -4.1792245 -4.2174807][-4.2496085 -4.2458858 -4.242157 -4.2368283 -4.232861 -4.2183561 -4.1914845 -4.1641445 -4.1507611 -4.1285315 -4.1073956 -4.1127405 -4.1416249 -4.1802206 -4.2226076][-4.2008877 -4.1897597 -4.1865144 -4.182302 -4.1759152 -4.1529493 -4.1079826 -4.0630393 -4.0554476 -4.0617018 -4.0631742 -4.0815883 -4.1223927 -4.1748948 -4.2201796][-4.1637158 -4.1337004 -4.1236496 -4.1213813 -4.1175823 -4.0954223 -4.0312977 -3.9524045 -3.9421158 -3.9916577 -4.03465 -4.0680103 -4.1097221 -4.1668983 -4.2108755][-4.1280465 -4.0792809 -4.0580354 -4.0539346 -4.0516996 -4.03293 -3.948005 -3.8178358 -3.7899272 -3.8903618 -3.9844277 -4.033618 -4.0711842 -4.1284924 -4.1803379][-4.0686522 -4.0129523 -3.9794354 -3.9686277 -3.964083 -3.9411397 -3.8362157 -3.6641583 -3.6168756 -3.7588599 -3.9028339 -3.9756455 -4.0202427 -4.0830736 -4.1529303][-4.0573635 -4.0134139 -3.9854877 -3.9830697 -3.9908435 -3.9823081 -3.8946328 -3.7483325 -3.6987059 -3.804157 -3.9309592 -3.9964702 -4.0301585 -4.0868521 -4.1618757][-4.1033487 -4.072926 -4.05523 -4.0633183 -4.0814209 -4.0827174 -4.0247512 -3.9276645 -3.8836379 -3.9357095 -4.0200143 -4.0673275 -4.091414 -4.1363897 -4.2010674][-4.1425304 -4.113739 -4.102582 -4.1159058 -4.1402488 -4.1479807 -4.11555 -4.0598497 -4.0239177 -4.0438623 -4.0965552 -4.1318913 -4.1595159 -4.1976876 -4.24465][-4.178576 -4.143846 -4.1315737 -4.146204 -4.177844 -4.195507 -4.183589 -4.1516037 -4.11862 -4.1218495 -4.1581893 -4.1833787 -4.2056293 -4.2376375 -4.2730231][-4.2060552 -4.172791 -4.15798 -4.170918 -4.2028117 -4.2227464 -4.2183895 -4.1896 -4.1508689 -4.1449289 -4.1692452 -4.1884265 -4.2059369 -4.235589 -4.269383][-4.2136092 -4.1885204 -4.1740804 -4.1834469 -4.2139907 -4.2359204 -4.2298546 -4.1953135 -4.1520214 -4.1384716 -4.1533847 -4.169138 -4.1833563 -4.2075086 -4.2420511][-4.2117767 -4.1928029 -4.1801233 -4.1841264 -4.2099071 -4.2339883 -4.2262793 -4.1870623 -4.1431489 -4.1227179 -4.1292815 -4.1470981 -4.1615615 -4.183917 -4.2215362][-4.2193813 -4.2074523 -4.2003436 -4.2015166 -4.2212934 -4.2446485 -4.2375703 -4.1970148 -4.1522164 -4.12486 -4.1235142 -4.1434746 -4.1620893 -4.1868138 -4.2288365]]...]
INFO - root - 2017-12-07 20:12:22.875987: step 49410, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 54h:42m:26s remains)
INFO - root - 2017-12-07 20:12:29.685954: step 49420, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 55h:56m:13s remains)
INFO - root - 2017-12-07 20:12:36.481468: step 49430, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 51h:49m:18s remains)
INFO - root - 2017-12-07 20:12:43.377245: step 49440, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 51h:48m:48s remains)
INFO - root - 2017-12-07 20:12:50.259341: step 49450, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 53h:34m:04s remains)
INFO - root - 2017-12-07 20:12:57.123662: step 49460, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 55h:04m:40s remains)
INFO - root - 2017-12-07 20:13:04.056406: step 49470, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 50h:59m:27s remains)
INFO - root - 2017-12-07 20:13:10.778555: step 49480, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 49h:48m:44s remains)
INFO - root - 2017-12-07 20:13:17.704460: step 49490, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 57h:37m:12s remains)
INFO - root - 2017-12-07 20:13:24.517001: step 49500, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 56h:12m:48s remains)
2017-12-07 20:13:25.256160: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1794734 -4.1634665 -4.1559544 -4.1501303 -4.1388044 -4.1211562 -4.1179643 -4.1203752 -4.122746 -4.12794 -4.1477919 -4.1782427 -4.2069936 -4.2394137 -4.2715425][-4.1421537 -4.1313548 -4.1352792 -4.1467357 -4.1510792 -4.14239 -4.1466808 -4.1556773 -4.1557641 -4.1556621 -4.1717172 -4.19622 -4.2170181 -4.2440057 -4.2724085][-4.1174631 -4.1148615 -4.12744 -4.1495442 -4.1678352 -4.1697469 -4.1796646 -4.1952028 -4.2012463 -4.2032156 -4.2150059 -4.2284861 -4.2385478 -4.2559471 -4.2772369][-4.1205063 -4.1217442 -4.1315708 -4.1488557 -4.1648369 -4.1676679 -4.1779075 -4.1979742 -4.2138128 -4.2275424 -4.2458687 -4.2567782 -4.26038 -4.2729511 -4.2902045][-4.1307087 -4.1252241 -4.1242728 -4.1285009 -4.128448 -4.121541 -4.1218119 -4.1359797 -4.1555576 -4.1823983 -4.2182393 -4.2411313 -4.2525177 -4.2705774 -4.2923355][-4.1473007 -4.1267371 -4.1107635 -4.0989089 -4.0787325 -4.0522041 -4.031558 -4.0292683 -4.0456519 -4.0834684 -4.1411777 -4.1850715 -4.2147059 -4.2446055 -4.2750039][-4.1676927 -4.1350222 -4.1069322 -4.0834446 -4.0518174 -4.0117054 -3.970747 -3.94648 -3.941746 -3.9725361 -4.0438871 -4.1087432 -4.1593289 -4.205564 -4.2478857][-4.182322 -4.1463747 -4.1156807 -4.0943394 -4.0677781 -4.0320563 -3.99025 -3.9588945 -3.9401596 -3.9590967 -4.0196762 -4.08131 -4.1359568 -4.1866136 -4.2322445][-4.1984673 -4.171011 -4.1478539 -4.1352983 -4.1211424 -4.0986376 -4.0699711 -4.0499692 -4.0374851 -4.0528073 -4.0935864 -4.1334386 -4.1711559 -4.2092848 -4.243907][-4.2095027 -4.1945677 -4.1849942 -4.1831689 -4.1794772 -4.1664295 -4.1476607 -4.138186 -4.1352768 -4.1444259 -4.1701059 -4.1957555 -4.2203231 -4.244493 -4.2660418][-4.2193341 -4.2136211 -4.2154551 -4.2208724 -4.2215471 -4.2123685 -4.19807 -4.1905384 -4.1860418 -4.186626 -4.20252 -4.2195773 -4.2382994 -4.2585859 -4.2769785][-4.2361212 -4.2331195 -4.2367916 -4.2426596 -4.2451444 -4.2397647 -4.2269416 -4.2135382 -4.1961551 -4.1891894 -4.2018871 -4.2143497 -4.2316675 -4.2520604 -4.2718306][-4.2392135 -4.23377 -4.2335405 -4.234489 -4.2339993 -4.2297564 -4.2167392 -4.1979556 -4.168642 -4.15865 -4.1731806 -4.1870584 -4.2076077 -4.2294974 -4.2538209][-4.2236567 -4.2154856 -4.2132883 -4.2109942 -4.2073078 -4.2030582 -4.1906109 -4.1726179 -4.1434703 -4.1375923 -4.1570916 -4.173408 -4.1921482 -4.2118473 -4.2381811][-4.2165012 -4.2091022 -4.2071624 -4.2050309 -4.202446 -4.1997137 -4.1893244 -4.1752048 -4.153501 -4.1517906 -4.1723585 -4.1893559 -4.2032347 -4.22066 -4.24455]]...]
INFO - root - 2017-12-07 20:13:31.867103: step 49510, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 51h:58m:29s remains)
INFO - root - 2017-12-07 20:13:38.560428: step 49520, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 53h:08m:31s remains)
INFO - root - 2017-12-07 20:13:45.310412: step 49530, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.699 sec/batch; 54h:55m:23s remains)
INFO - root - 2017-12-07 20:13:52.123220: step 49540, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 56h:00m:22s remains)
INFO - root - 2017-12-07 20:13:58.799425: step 49550, loss = 2.03, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 53h:20m:28s remains)
INFO - root - 2017-12-07 20:14:05.600717: step 49560, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 53h:22m:03s remains)
INFO - root - 2017-12-07 20:14:12.331516: step 49570, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 56h:55m:15s remains)
INFO - root - 2017-12-07 20:14:19.054541: step 49580, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 55h:21m:18s remains)
INFO - root - 2017-12-07 20:14:25.911156: step 49590, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 55h:05m:54s remains)
INFO - root - 2017-12-07 20:14:32.725084: step 49600, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 52h:41m:29s remains)
2017-12-07 20:14:33.422624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2333369 -4.2309136 -4.2391186 -4.2446289 -4.2415686 -4.2249274 -4.2137532 -4.2076092 -4.2072024 -4.222559 -4.2440033 -4.2645831 -4.2875853 -4.3105545 -4.3309388][-4.2094336 -4.2087955 -4.2191143 -4.2220526 -4.2149873 -4.1901765 -4.1739321 -4.164597 -4.1626606 -4.1798863 -4.2094655 -4.2366014 -4.2658591 -4.2942309 -4.3204203][-4.200357 -4.2046022 -4.2181182 -4.218771 -4.2055583 -4.1710973 -4.1503191 -4.1379004 -4.1358662 -4.1545749 -4.1891794 -4.2205272 -4.2517724 -4.282629 -4.3130255][-4.1900396 -4.1936927 -4.20666 -4.2020311 -4.1847267 -4.1477017 -4.1292043 -4.120007 -4.1309495 -4.1547818 -4.1879525 -4.2175617 -4.2472048 -4.277492 -4.3081794][-4.1682868 -4.1693568 -4.1765723 -4.1600242 -4.1383829 -4.0994391 -4.0791378 -4.0772634 -4.1072559 -4.1433043 -4.1787 -4.2105608 -4.24315 -4.2752075 -4.3052969][-4.1416206 -4.1374197 -4.1342678 -4.1028628 -4.0690708 -4.0189023 -3.9793324 -3.975549 -4.0347252 -4.0995774 -4.14694 -4.1904254 -4.2352118 -4.2730665 -4.304266][-4.1174316 -4.1139565 -4.1003966 -4.0552573 -4.0034928 -3.9250107 -3.837955 -3.8122559 -3.9042683 -4.0158854 -4.0922728 -4.1528673 -4.2134595 -4.2620187 -4.2980828][-4.0989184 -4.1018863 -4.084733 -4.0306735 -3.9640965 -3.8598716 -3.7206397 -3.6537688 -3.7618022 -3.9105792 -4.0200024 -4.1054773 -4.1833959 -4.2450342 -4.2888408][-4.0998535 -4.1135125 -4.1085324 -4.0704112 -4.0168667 -3.9227567 -3.7882221 -3.7083206 -3.7801204 -3.9025378 -4.0083747 -4.0988836 -4.1780162 -4.2422853 -4.2877946][-4.1225948 -4.1401572 -4.142693 -4.1231017 -4.0947561 -4.0365939 -3.9464645 -3.8894901 -3.9210241 -3.9899971 -4.0648222 -4.1383 -4.2015481 -4.2549944 -4.2939172][-4.1769543 -4.1927657 -4.1965332 -4.1841927 -4.1665616 -4.1321011 -4.0793552 -4.0460596 -4.0586443 -4.0975609 -4.1446047 -4.1958904 -4.239903 -4.2760754 -4.304666][-4.2380786 -4.2492628 -4.2514443 -4.2428446 -4.2319469 -4.2124619 -4.1838841 -4.1673794 -4.1741323 -4.1971135 -4.2258925 -4.2565079 -4.2809043 -4.3012314 -4.3190084][-4.2875161 -4.2947812 -4.2962523 -4.2884073 -4.2790432 -4.2669721 -4.2527757 -4.2485638 -4.2569385 -4.2694616 -4.2848625 -4.301434 -4.3142462 -4.3248825 -4.3340769][-4.3171835 -4.3204269 -4.3213997 -4.3148737 -4.3067341 -4.29882 -4.2905521 -4.2905884 -4.298821 -4.305747 -4.3134851 -4.3244777 -4.3342628 -4.3414054 -4.3455715][-4.3304243 -4.3299427 -4.3299479 -4.3247385 -4.3186355 -4.3139129 -4.309278 -4.3090224 -4.3138609 -4.3172812 -4.3217244 -4.3307567 -4.3401585 -4.3470054 -4.3492451]]...]
INFO - root - 2017-12-07 20:14:40.184465: step 49610, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 54h:06m:31s remains)
INFO - root - 2017-12-07 20:14:46.979395: step 49620, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.693 sec/batch; 54h:24m:56s remains)
INFO - root - 2017-12-07 20:14:53.758403: step 49630, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 51h:24m:37s remains)
INFO - root - 2017-12-07 20:15:00.584010: step 49640, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 52h:56m:50s remains)
INFO - root - 2017-12-07 20:15:07.473118: step 49650, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 56h:28m:35s remains)
INFO - root - 2017-12-07 20:15:14.285697: step 49660, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 52h:49m:23s remains)
INFO - root - 2017-12-07 20:15:21.187354: step 49670, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 51h:10m:42s remains)
INFO - root - 2017-12-07 20:15:28.000431: step 49680, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.636 sec/batch; 49h:56m:08s remains)
INFO - root - 2017-12-07 20:15:34.888787: step 49690, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 53h:36m:12s remains)
INFO - root - 2017-12-07 20:15:41.745210: step 49700, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 54h:25m:50s remains)
2017-12-07 20:15:42.393736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1782813 -4.1317859 -4.1020103 -4.1022043 -4.110476 -4.1050076 -4.0751033 -4.051137 -4.0610948 -4.105 -4.1657176 -4.2286353 -4.2822418 -4.3098722 -4.326674][-4.1531162 -4.0959473 -4.0596962 -4.063508 -4.0835271 -4.0853982 -4.05537 -4.0400758 -4.0622368 -4.1097207 -4.1658759 -4.2245474 -4.2790914 -4.3064241 -4.3238511][-4.1363454 -4.0740023 -4.03551 -4.0424 -4.0685573 -4.0733376 -4.0455379 -4.0410738 -4.0813494 -4.1352444 -4.1887746 -4.2411342 -4.2892246 -4.3106146 -4.3238072][-4.1340933 -4.0767307 -4.0419436 -4.0512543 -4.0754337 -4.0755539 -4.0422039 -4.0410795 -4.091773 -4.1538053 -4.209743 -4.2598081 -4.3024426 -4.316658 -4.3251505][-4.1556673 -4.1067829 -4.0791078 -4.0869317 -4.1050725 -4.0941129 -4.0529523 -4.0457091 -4.0928168 -4.1608176 -4.2236118 -4.2754545 -4.31447 -4.323432 -4.3276772][-4.1921115 -4.1533895 -4.131176 -4.1320887 -4.1456971 -4.1293192 -4.0805616 -4.0572109 -4.0904994 -4.1599007 -4.2322335 -4.2857928 -4.3226562 -4.3286486 -4.3305373][-4.2200117 -4.1908469 -4.1684279 -4.161911 -4.1696606 -4.1527319 -4.0997772 -4.062283 -4.0809636 -4.1497498 -4.228332 -4.2855134 -4.3233228 -4.330925 -4.3332624][-4.2418194 -4.2161164 -4.1947379 -4.1824241 -4.1810546 -4.1599159 -4.1053963 -4.0644174 -4.0801067 -4.1431079 -4.21827 -4.2769847 -4.3176255 -4.3302717 -4.3349051][-4.2555408 -4.2327561 -4.2134509 -4.1986637 -4.1865883 -4.1580281 -4.1077647 -4.0732031 -4.08917 -4.1453195 -4.2094235 -4.2668095 -4.3098388 -4.3277893 -4.3356047][-4.2596459 -4.2404394 -4.2251182 -4.2129622 -4.1910377 -4.1548514 -4.1136289 -4.0908742 -4.1064844 -4.1550055 -4.2093182 -4.2617464 -4.30672 -4.326488 -4.3358974][-4.2602549 -4.2447577 -4.2325568 -4.2250857 -4.2044148 -4.1650023 -4.1311731 -4.121892 -4.1370115 -4.1757421 -4.2211113 -4.2678447 -4.3101244 -4.3271255 -4.33575][-4.2603326 -4.2489405 -4.2389245 -4.236711 -4.2234697 -4.1888986 -4.16098 -4.1582193 -4.1740904 -4.2082677 -4.2450857 -4.2828369 -4.3176188 -4.3296 -4.3362074][-4.2487926 -4.2439504 -4.2376676 -4.2349639 -4.2242975 -4.2012329 -4.1872311 -4.1948209 -4.2176046 -4.2503128 -4.2758088 -4.3016443 -4.3263044 -4.3332152 -4.3378797][-4.2422843 -4.2414641 -4.2406406 -4.2362971 -4.2253523 -4.2144413 -4.2153635 -4.2308788 -4.2549844 -4.2813129 -4.2986503 -4.3174567 -4.3350425 -4.3385377 -4.3407722][-4.2613516 -4.2636333 -4.2661362 -4.2594156 -4.2423854 -4.2346525 -4.2395492 -4.253798 -4.2736263 -4.2935214 -4.3087564 -4.3270273 -4.3412371 -4.3437672 -4.3435168]]...]
INFO - root - 2017-12-07 20:15:49.034718: step 49710, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 52h:52m:42s remains)
INFO - root - 2017-12-07 20:15:55.912340: step 49720, loss = 2.10, batch loss = 2.04 (10.6 examples/sec; 0.752 sec/batch; 59h:04m:46s remains)
INFO - root - 2017-12-07 20:16:02.729306: step 49730, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 53h:12m:59s remains)
INFO - root - 2017-12-07 20:16:09.402752: step 49740, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 50h:24m:49s remains)
INFO - root - 2017-12-07 20:16:16.184691: step 49750, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 52h:21m:50s remains)
INFO - root - 2017-12-07 20:16:23.009324: step 49760, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 56h:14m:45s remains)
INFO - root - 2017-12-07 20:16:29.847846: step 49770, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 55h:21m:34s remains)
INFO - root - 2017-12-07 20:16:36.694553: step 49780, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 51h:43m:49s remains)
INFO - root - 2017-12-07 20:16:43.520446: step 49790, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:04m:57s remains)
INFO - root - 2017-12-07 20:16:50.332933: step 49800, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 56h:00m:06s remains)
2017-12-07 20:16:51.008669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3055682 -4.3156343 -4.3286691 -4.3375506 -4.339056 -4.3330817 -4.3169332 -4.2989879 -4.285727 -4.2810864 -4.2905245 -4.3035231 -4.3133097 -4.3164411 -4.3106565][-4.3199239 -4.3305893 -4.3402562 -4.342082 -4.3334284 -4.3151 -4.2889247 -4.2650638 -4.2478113 -4.2436271 -4.2610345 -4.2839508 -4.3045254 -4.3170977 -4.3171673][-4.314642 -4.3248587 -4.3297205 -4.3230696 -4.3014426 -4.2669129 -4.2250156 -4.1897321 -4.1641107 -4.1597404 -4.1894321 -4.2287545 -4.265317 -4.2907329 -4.299078][-4.2850289 -4.2950583 -4.2966533 -4.2834954 -4.2509871 -4.2016125 -4.1444316 -4.0955529 -4.0583968 -4.0504966 -4.0910058 -4.1443458 -4.1921825 -4.2268958 -4.2431064][-4.2452717 -4.25408 -4.2535009 -4.2356863 -4.196023 -4.1382895 -4.0743413 -4.0201225 -3.9793236 -3.9724936 -4.0167565 -4.0692945 -4.1147137 -4.149086 -4.1689997][-4.213151 -4.2221522 -4.2222958 -4.2044711 -4.1658959 -4.1117463 -4.0542078 -4.0069647 -3.9748201 -3.9753749 -4.0097194 -4.0443377 -4.0735435 -4.0950556 -4.1090665][-4.2060318 -4.2160554 -4.2187095 -4.2068834 -4.1786704 -4.1393728 -4.09767 -4.0638666 -4.0453506 -4.0510311 -4.0699644 -4.0839763 -4.0924821 -4.0936747 -4.091269][-4.2254734 -4.2334046 -4.2368526 -4.2318764 -4.2173376 -4.1945815 -4.1698918 -4.1492219 -4.1392059 -4.143054 -4.1481719 -4.148973 -4.1439447 -4.1314869 -4.1164651][-4.2485819 -4.2515917 -4.2531095 -4.2515216 -4.2467947 -4.2373695 -4.2249403 -4.2118673 -4.2055197 -4.2070861 -4.2052865 -4.2002234 -4.1896181 -4.1729031 -4.1554561][-4.2640529 -4.2622495 -4.2619748 -4.2625542 -4.2639179 -4.26199 -4.254807 -4.2455597 -4.2429533 -4.2457805 -4.2435541 -4.2369766 -4.2253666 -4.2095647 -4.1942177][-4.27851 -4.2747107 -4.2731256 -4.2732158 -4.2750387 -4.2742214 -4.267992 -4.259923 -4.2600732 -4.2658787 -4.2674108 -4.2632356 -4.2544675 -4.2429156 -4.231678][-4.2914281 -4.2880707 -4.2871838 -4.2880325 -4.2902179 -4.2897463 -4.2844453 -4.2770739 -4.277597 -4.2839031 -4.2876287 -4.285943 -4.2807488 -4.2741132 -4.2668056][-4.2983265 -4.295826 -4.2959485 -4.2981992 -4.3007274 -4.2993221 -4.2931 -4.2851443 -4.2834349 -4.28742 -4.29035 -4.2896824 -4.2870817 -4.2849407 -4.2822471][-4.3018584 -4.2993975 -4.2993841 -4.3025875 -4.305644 -4.3034563 -4.295929 -4.2861829 -4.280582 -4.2798333 -4.2801375 -4.2793026 -4.278728 -4.2810397 -4.28276][-4.3103471 -4.3078322 -4.3072672 -4.3102865 -4.3129544 -4.3097253 -4.3008857 -4.289465 -4.2801924 -4.2749257 -4.2728815 -4.2721753 -4.2736874 -4.2797074 -4.2841611]]...]
INFO - root - 2017-12-07 20:16:57.542379: step 49810, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 52h:02m:21s remains)
INFO - root - 2017-12-07 20:17:04.303957: step 49820, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 49h:36m:47s remains)
INFO - root - 2017-12-07 20:17:11.143954: step 49830, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 50h:52m:36s remains)
INFO - root - 2017-12-07 20:17:17.915562: step 49840, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 55h:37m:57s remains)
INFO - root - 2017-12-07 20:17:24.725172: step 49850, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 55h:16m:23s remains)
INFO - root - 2017-12-07 20:17:31.293659: step 49860, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 48h:50m:29s remains)
INFO - root - 2017-12-07 20:17:38.059873: step 49870, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 52h:57m:53s remains)
INFO - root - 2017-12-07 20:17:44.836787: step 49880, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 56h:40m:55s remains)
INFO - root - 2017-12-07 20:17:51.676759: step 49890, loss = 2.03, batch loss = 1.98 (11.4 examples/sec; 0.701 sec/batch; 55h:00m:51s remains)
INFO - root - 2017-12-07 20:17:58.526478: step 49900, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.692 sec/batch; 54h:18m:30s remains)
2017-12-07 20:17:59.202116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1699715 -4.1635413 -4.1819196 -4.2172589 -4.2497044 -4.2663097 -4.2718043 -4.271235 -4.2606745 -4.2282743 -4.1713624 -4.1116891 -4.0673842 -4.0443044 -4.0503678][-4.1666126 -4.1626983 -4.1948237 -4.2459726 -4.2883568 -4.3036518 -4.2941084 -4.2719054 -4.23849 -4.1906705 -4.1374621 -4.0892916 -4.0611439 -4.05054 -4.0708227][-4.1599216 -4.1677685 -4.2147183 -4.2745943 -4.3164306 -4.322721 -4.2946305 -4.2469978 -4.19251 -4.1424108 -4.1090784 -4.084394 -4.0720272 -4.066925 -4.0967197][-4.1664543 -4.1971545 -4.2574863 -4.3135738 -4.336134 -4.3126888 -4.2538815 -4.1736717 -4.101769 -4.0718861 -4.0771947 -4.0844269 -4.0885382 -4.0922604 -4.1234436][-4.1942859 -4.2453575 -4.3051305 -4.3412981 -4.3298631 -4.2659345 -4.1624403 -4.0403075 -3.9567084 -3.9725986 -4.0394855 -4.0884566 -4.1151247 -4.1317492 -4.1549468][-4.2254767 -4.2856336 -4.3368139 -4.3469973 -4.2979965 -4.1907172 -4.0329461 -3.8578823 -3.7675633 -3.8533912 -3.9936776 -4.0905132 -4.1445928 -4.1712413 -4.1823149][-4.2559924 -4.31189 -4.347486 -4.3325248 -4.2540283 -4.1121373 -3.9165888 -3.7159605 -3.6524353 -3.8068478 -3.9939249 -4.1172166 -4.1812124 -4.20764 -4.2052503][-4.2765689 -4.3164291 -4.3339553 -4.3036079 -4.2152119 -4.0712953 -3.896569 -3.7464678 -3.7444966 -3.8982368 -4.0641384 -4.1743808 -4.2285457 -4.2445188 -4.2321444][-4.2796793 -4.3026433 -4.3094978 -4.2785063 -4.2005968 -4.0881596 -3.9761193 -3.9069278 -3.9405532 -4.048511 -4.1570349 -4.2352724 -4.2720876 -4.275806 -4.2593517][-4.276711 -4.2895789 -4.2931056 -4.2691875 -4.215426 -4.1459713 -4.0914702 -4.0760016 -4.1142578 -4.1769605 -4.2364945 -4.2818556 -4.2989063 -4.2930317 -4.2756009][-4.2754683 -4.2807875 -4.2836523 -4.2698092 -4.2399411 -4.2051058 -4.186842 -4.1976523 -4.2307463 -4.2655683 -4.29427 -4.3118653 -4.3096428 -4.2965569 -4.2804451][-4.2833743 -4.2830677 -4.2843528 -4.2786541 -4.2649612 -4.250864 -4.2531719 -4.2736387 -4.2976751 -4.3161545 -4.3238015 -4.3198333 -4.3045163 -4.2896566 -4.2774382][-4.2904668 -4.2871122 -4.2850537 -4.2823477 -4.2772117 -4.2768655 -4.2905169 -4.313365 -4.3303733 -4.3354607 -4.326683 -4.305356 -4.2840624 -4.27631 -4.2735147][-4.2828841 -4.2780008 -4.26976 -4.263689 -4.2629094 -4.2746811 -4.2981486 -4.3214579 -4.3315043 -4.3230915 -4.297585 -4.2620153 -4.2379861 -4.2415085 -4.2514191][-4.2469058 -4.2421956 -4.2275004 -4.216496 -4.216753 -4.2366977 -4.2678256 -4.2944813 -4.3010731 -4.2818489 -4.2446866 -4.2018194 -4.17943 -4.19355 -4.2145729]]...]
INFO - root - 2017-12-07 20:18:05.746070: step 49910, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 52h:13m:12s remains)
INFO - root - 2017-12-07 20:18:12.491635: step 49920, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 54h:48m:13s remains)
INFO - root - 2017-12-07 20:18:19.282285: step 49930, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 53h:42m:31s remains)
INFO - root - 2017-12-07 20:18:26.012578: step 49940, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 53h:01m:17s remains)
INFO - root - 2017-12-07 20:18:32.812089: step 49950, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 50h:55m:05s remains)
INFO - root - 2017-12-07 20:18:39.768976: step 49960, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 56h:13m:34s remains)
INFO - root - 2017-12-07 20:18:46.613873: step 49970, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 55h:41m:28s remains)
INFO - root - 2017-12-07 20:18:53.371676: step 49980, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 53h:47m:09s remains)
INFO - root - 2017-12-07 20:19:00.037751: step 49990, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 50h:04m:07s remains)
INFO - root - 2017-12-07 20:19:06.903319: step 50000, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 52h:38m:23s remains)
2017-12-07 20:19:07.753302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1797805 -4.1756449 -4.1834517 -4.1851 -4.1920972 -4.1925049 -4.1861959 -4.18478 -4.183598 -4.1841097 -4.1931729 -4.2024627 -4.2088566 -4.2152457 -4.218987][-4.1547017 -4.1623788 -4.1802254 -4.1848683 -4.1821809 -4.1724277 -4.1640949 -4.166841 -4.1720328 -4.1765556 -4.1880836 -4.1963773 -4.1979203 -4.1986027 -4.1997991][-4.1299186 -4.1435461 -4.1667714 -4.1752834 -4.1658354 -4.1455288 -4.13522 -4.1437335 -4.1521864 -4.1564317 -4.1655312 -4.1715283 -4.1678171 -4.1605778 -4.15897][-4.1296835 -4.1419787 -4.1619639 -4.1671605 -4.1507773 -4.1227021 -4.1140594 -4.132298 -4.1440921 -4.1425104 -4.1440921 -4.1448731 -4.1348062 -4.1208296 -4.1171675][-4.13963 -4.1462288 -4.1542315 -4.1500192 -4.1254349 -4.0968614 -4.0991135 -4.13563 -4.1553793 -4.1498461 -4.1415367 -4.1333137 -4.1145334 -4.0979457 -4.0986314][-4.1244793 -4.1284118 -4.1280942 -4.1128469 -4.0794892 -4.0478768 -4.0643158 -4.1216054 -4.1543312 -4.1520348 -4.1396441 -4.1245646 -4.0992389 -4.0809464 -4.0839453][-4.1077919 -4.1109362 -4.1008325 -4.0768151 -4.0383863 -3.9979298 -4.0102444 -4.0748911 -4.116652 -4.1220665 -4.1141748 -4.0989394 -4.0707636 -4.0551724 -4.0614161][-4.1189766 -4.119626 -4.1007996 -4.0736804 -4.0427456 -4.002902 -3.9951897 -4.039917 -4.0769472 -4.0817204 -4.0752907 -4.0656743 -4.0454125 -4.0396929 -4.0536771][-4.1341553 -4.1293321 -4.109374 -4.0887718 -4.079186 -4.0668497 -4.056675 -4.0783529 -4.0983443 -4.0918288 -4.0790334 -4.0695691 -4.0609951 -4.0640459 -4.0787725][-4.1480446 -4.1271176 -4.1060052 -4.0981712 -4.1078429 -4.1192288 -4.117382 -4.1309352 -4.1434803 -4.1333609 -4.1211267 -4.1087708 -4.102891 -4.105165 -4.1167469][-4.1571293 -4.1283407 -4.1081877 -4.1114779 -4.1275434 -4.1398778 -4.1383753 -4.1483521 -4.1600351 -4.1521111 -4.1501975 -4.1440411 -4.1418686 -4.1466131 -4.1558475][-4.164124 -4.1404696 -4.1292143 -4.1361694 -4.1493163 -4.1529026 -4.14659 -4.1501665 -4.15558 -4.1505075 -4.1607447 -4.16957 -4.1731081 -4.1724019 -4.1733489][-4.1834507 -4.1714087 -4.1714616 -4.1768589 -4.1847148 -4.1842237 -4.1792417 -4.1777353 -4.1760821 -4.1708503 -4.183146 -4.193553 -4.1970329 -4.1891308 -4.1738105][-4.2066913 -4.2042527 -4.211123 -4.2165108 -4.2225771 -4.2212157 -4.2202992 -4.2208409 -4.2161713 -4.2128077 -4.2261243 -4.2323189 -4.22978 -4.2178335 -4.1914258][-4.2190204 -4.2222838 -4.2316527 -4.2361193 -4.2427082 -4.2386165 -4.2374144 -4.2365184 -4.2303 -4.2312546 -4.2441826 -4.2447796 -4.2422247 -4.2360063 -4.2134266]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 20:19:15.090674: step 50010, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 54h:52m:32s remains)
INFO - root - 2017-12-07 20:19:21.951959: step 50020, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 54h:53m:23s remains)
INFO - root - 2017-12-07 20:19:28.712094: step 50030, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 53h:54m:49s remains)
INFO - root - 2017-12-07 20:19:35.542661: step 50040, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 51h:01m:36s remains)
INFO - root - 2017-12-07 20:19:42.329294: step 50050, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 50h:52m:51s remains)
INFO - root - 2017-12-07 20:19:49.159199: step 50060, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 55h:22m:49s remains)
INFO - root - 2017-12-07 20:19:56.057231: step 50070, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 57h:15m:38s remains)
INFO - root - 2017-12-07 20:20:02.888693: step 50080, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 54h:44m:06s remains)
INFO - root - 2017-12-07 20:20:09.655850: step 50090, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 53h:48m:03s remains)
INFO - root - 2017-12-07 20:20:16.495926: step 50100, loss = 2.03, batch loss = 1.97 (12.2 examples/sec; 0.655 sec/batch; 51h:23m:57s remains)
2017-12-07 20:20:17.309055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0872669 -4.1157532 -4.1570139 -4.1932812 -4.2229972 -4.2350488 -4.2252445 -4.2138638 -4.2136683 -4.2107658 -4.2047791 -4.1955667 -4.2009368 -4.2127109 -4.2257133][-4.0919323 -4.1191864 -4.1759276 -4.2247376 -4.2521839 -4.2589083 -4.2531714 -4.2388496 -4.2281642 -4.2244935 -4.2260938 -4.225553 -4.2336855 -4.241498 -4.2434888][-4.1081128 -4.1261482 -4.1831012 -4.235342 -4.2580481 -4.2586131 -4.2519803 -4.2398272 -4.228322 -4.225132 -4.231986 -4.2379651 -4.2420735 -4.243556 -4.2369494][-4.1270976 -4.1361551 -4.18952 -4.2400622 -4.2557726 -4.2472134 -4.23428 -4.2248039 -4.2146854 -4.211494 -4.219491 -4.2258172 -4.2256794 -4.2271628 -4.2255149][-4.1386294 -4.1379819 -4.1830316 -4.2278566 -4.2348242 -4.2119465 -4.1894073 -4.182128 -4.1801643 -4.1809192 -4.1844254 -4.183774 -4.1842847 -4.1952047 -4.2066035][-4.1535926 -4.140533 -4.171217 -4.2028847 -4.19829 -4.1657124 -4.132688 -4.1208315 -4.1289721 -4.1391916 -4.135767 -4.1205139 -4.1178465 -4.1411686 -4.17057][-4.1757245 -4.1482558 -4.1609263 -4.1745987 -4.1593747 -4.1235003 -4.0816541 -4.0629511 -4.0806828 -4.1057162 -4.1040716 -4.0704527 -4.0533147 -4.0811768 -4.1256528][-4.21473 -4.1768894 -4.1689138 -4.1559315 -4.1261883 -4.0893526 -4.0464339 -4.0326023 -4.0627623 -4.1012387 -4.1071062 -4.0607872 -4.0206404 -4.0450988 -4.1000438][-4.250402 -4.2154145 -4.1988807 -4.1713247 -4.1297736 -4.0907211 -4.0531416 -4.0492191 -4.0919886 -4.1438274 -4.1591716 -4.1057315 -4.0425658 -4.0580931 -4.1100054][-4.254261 -4.2294474 -4.2157431 -4.1906548 -4.1569376 -4.1345558 -4.1166596 -4.125803 -4.1650825 -4.2077484 -4.22322 -4.1777072 -4.1172876 -4.1211276 -4.1578112][-4.2321992 -4.2190132 -4.2142582 -4.208931 -4.19729 -4.1983595 -4.2027459 -4.2221947 -4.2501631 -4.2736406 -4.2802963 -4.250802 -4.2109418 -4.207993 -4.2242131][-4.200913 -4.204484 -4.2108378 -4.2211719 -4.2324786 -4.2486916 -4.2609563 -4.2792816 -4.2991762 -4.3115635 -4.3118181 -4.2939553 -4.2711763 -4.2679591 -4.27467][-4.1687088 -4.18613 -4.2030768 -4.2256618 -4.2512188 -4.2682672 -4.2743649 -4.2864227 -4.3048592 -4.3135009 -4.3116975 -4.300056 -4.289566 -4.291585 -4.300313][-4.1425934 -4.1624236 -4.1827641 -4.2142849 -4.2464318 -4.257494 -4.252635 -4.2577782 -4.276207 -4.2895412 -4.2932162 -4.2886429 -4.2852325 -4.2922053 -4.30366][-4.1395607 -4.1558838 -4.17071 -4.2006545 -4.2329283 -4.23953 -4.224649 -4.2178226 -4.2320924 -4.2500134 -4.2650375 -4.2724032 -4.2759414 -4.2831244 -4.2901921]]...]
INFO - root - 2017-12-07 20:20:23.840615: step 50110, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 53h:19m:01s remains)
INFO - root - 2017-12-07 20:20:30.644992: step 50120, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 51h:47m:55s remains)
INFO - root - 2017-12-07 20:20:37.406176: step 50130, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 53h:08m:14s remains)
INFO - root - 2017-12-07 20:20:44.228981: step 50140, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.730 sec/batch; 57h:15m:20s remains)
INFO - root - 2017-12-07 20:20:50.982747: step 50150, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 53h:49m:56s remains)
INFO - root - 2017-12-07 20:20:57.694479: step 50160, loss = 2.05, batch loss = 1.99 (14.1 examples/sec; 0.569 sec/batch; 44h:36m:15s remains)
INFO - root - 2017-12-07 20:21:04.420520: step 50170, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 52h:56m:41s remains)
INFO - root - 2017-12-07 20:21:11.238506: step 50180, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 55h:14m:51s remains)
INFO - root - 2017-12-07 20:21:18.098154: step 50190, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 55h:12m:49s remains)
INFO - root - 2017-12-07 20:21:24.897632: step 50200, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 50h:33m:30s remains)
2017-12-07 20:21:25.685454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3426476 -4.3367686 -4.340198 -4.3461108 -4.3502483 -4.3490491 -4.3448668 -4.3424797 -4.3414879 -4.3384 -4.3321686 -4.3248954 -4.3184505 -4.3139105 -4.3126392][-4.3312168 -4.3203 -4.3295269 -4.3450289 -4.3541746 -4.3505363 -4.3391175 -4.3305726 -4.3245006 -4.3163023 -4.3037214 -4.2906027 -4.2817988 -4.2793241 -4.28338][-4.3129473 -4.2946258 -4.3064284 -4.3291469 -4.3405275 -4.3362923 -4.3159118 -4.2984395 -4.2891817 -4.28136 -4.2682867 -4.2521982 -4.2416778 -4.2395177 -4.2505145][-4.2970533 -4.2696729 -4.276485 -4.2980924 -4.3069048 -4.3038435 -4.2778215 -4.2528763 -4.2473807 -4.2453728 -4.2357426 -4.2212138 -4.2110438 -4.2105994 -4.2285638][-4.2755294 -4.2386346 -4.2377954 -4.2523117 -4.2540612 -4.2490425 -4.2139254 -4.1837463 -4.1937857 -4.2078328 -4.2087359 -4.2019181 -4.1966181 -4.2003379 -4.2233181][-4.250875 -4.2061658 -4.1886625 -4.1895418 -4.1779561 -4.1602845 -4.1119385 -4.08149 -4.1180973 -4.1542072 -4.1673312 -4.17229 -4.177464 -4.1910324 -4.2201772][-4.2256203 -4.1682992 -4.128974 -4.1100459 -4.0749378 -4.0282407 -3.94975 -3.9277434 -4.0116472 -4.0798264 -4.1077876 -4.1243286 -4.1460791 -4.1777825 -4.2191219][-4.18586 -4.1189518 -4.0698137 -4.0404029 -3.9897859 -3.912142 -3.8023486 -3.7940366 -3.9247036 -4.0163541 -4.0535727 -4.08118 -4.1142712 -4.1568222 -4.2076578][-4.1630087 -4.1002674 -4.0607939 -4.0407267 -3.9986265 -3.9290788 -3.8422403 -3.8560824 -3.9724314 -4.043375 -4.0705218 -4.09634 -4.12669 -4.164031 -4.2103519][-4.1677728 -4.1168313 -4.0927272 -4.0884871 -4.0650873 -4.0256252 -3.985393 -4.0069041 -4.0768695 -4.1127133 -4.1230426 -4.1414585 -4.1653337 -4.1942372 -4.2299738][-4.1783042 -4.1395254 -4.1278625 -4.1383963 -4.1325912 -4.1189575 -4.104435 -4.1230211 -4.1615391 -4.1749687 -4.1711955 -4.1767716 -4.1928153 -4.2159476 -4.2438531][-4.2036018 -4.1734266 -4.1712894 -4.190484 -4.1950259 -4.1899819 -4.1846337 -4.1965656 -4.218358 -4.2184472 -4.2056952 -4.2070203 -4.2222052 -4.2422285 -4.2621183][-4.242259 -4.2189021 -4.2219019 -4.2421918 -4.2487936 -4.2456703 -4.2438703 -4.2540965 -4.2659903 -4.2570677 -4.2417593 -4.2454515 -4.2629166 -4.2789278 -4.2920613][-4.2774534 -4.2625742 -4.2663927 -4.2809396 -4.2849741 -4.281363 -4.2809038 -4.28864 -4.2931514 -4.28207 -4.2697444 -4.2786469 -4.2962308 -4.3090692 -4.3178883][-4.2936592 -4.2844505 -4.2863731 -4.2936435 -4.2935753 -4.2901368 -4.2900858 -4.295929 -4.2985129 -4.2891655 -4.2815161 -4.2901177 -4.3033819 -4.3137197 -4.3221655]]...]
INFO - root - 2017-12-07 20:21:32.380156: step 50210, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 55h:19m:41s remains)
INFO - root - 2017-12-07 20:21:39.096245: step 50220, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 54h:21m:41s remains)
INFO - root - 2017-12-07 20:21:45.903260: step 50230, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.662 sec/batch; 51h:55m:42s remains)
INFO - root - 2017-12-07 20:21:52.554834: step 50240, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 49h:24m:59s remains)
INFO - root - 2017-12-07 20:21:59.392802: step 50250, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 53h:06m:35s remains)
INFO - root - 2017-12-07 20:22:06.055308: step 50260, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 54h:06m:09s remains)
INFO - root - 2017-12-07 20:22:12.824457: step 50270, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 54h:36m:35s remains)
INFO - root - 2017-12-07 20:22:19.587733: step 50280, loss = 2.03, batch loss = 1.97 (11.9 examples/sec; 0.672 sec/batch; 52h:42m:45s remains)
INFO - root - 2017-12-07 20:22:26.270858: step 50290, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 49h:47m:45s remains)
INFO - root - 2017-12-07 20:22:33.019686: step 50300, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 51h:53m:25s remains)
2017-12-07 20:22:33.708558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.234786 -4.2446232 -4.2542882 -4.2553868 -4.2446494 -4.238862 -4.2403245 -4.245842 -4.2500591 -4.251318 -4.25018 -4.24897 -4.2507229 -4.2472529 -4.2322459][-4.1969433 -4.2082458 -4.2142544 -4.2083478 -4.1938624 -4.1853647 -4.1835093 -4.1917844 -4.2034173 -4.2129011 -4.22079 -4.2268033 -4.2307959 -4.2237916 -4.1996822][-4.1553736 -4.1681714 -4.1714253 -4.1601129 -4.1392517 -4.1247807 -4.1174622 -4.1254597 -4.140964 -4.1596627 -4.18356 -4.2026858 -4.20694 -4.1934586 -4.168323][-4.1295047 -4.1459885 -4.152236 -4.1394668 -4.108129 -4.0814242 -4.0618877 -4.0600548 -4.0778809 -4.1078024 -4.1470418 -4.1768613 -4.181108 -4.1674118 -4.14847][-4.1249566 -4.1411228 -4.1449604 -4.1293688 -4.0891309 -4.0441556 -3.9979682 -3.9775314 -4.0016222 -4.0481567 -4.1013927 -4.1385374 -4.1491046 -4.145802 -4.1372242][-4.1100698 -4.1167197 -4.115716 -4.099957 -4.0503592 -3.9770896 -3.8907492 -3.8526225 -3.902504 -3.9854662 -4.0619431 -4.1073952 -4.1242247 -4.1300282 -4.1309824][-4.0748992 -4.0691128 -4.0670824 -4.049624 -3.9888222 -3.8727219 -3.728812 -3.6736591 -3.7803695 -3.9240394 -4.0310831 -4.0865083 -4.1060572 -4.1161451 -4.1212258][-4.0521665 -4.0317321 -4.0243754 -3.9945767 -3.916172 -3.7595339 -3.5715175 -3.5120182 -3.6945968 -3.8981066 -4.02619 -4.0841136 -4.1025009 -4.1074677 -4.10829][-4.0669127 -4.0499091 -4.0382452 -4.0014963 -3.9176779 -3.7641087 -3.6018677 -3.5773704 -3.7622259 -3.9522176 -4.0622711 -4.1069593 -4.1135631 -4.107585 -4.0977316][-4.1141477 -4.1079087 -4.0948768 -4.0587797 -3.9888716 -3.8776979 -3.7847209 -3.7919588 -3.9203339 -4.0448008 -4.1157336 -4.1427717 -4.14071 -4.1248837 -4.1023097][-4.1487937 -4.15442 -4.1439576 -4.112792 -4.0658183 -3.9976597 -3.9535511 -3.9668474 -4.0445838 -4.1182261 -4.16359 -4.1774588 -4.1713119 -4.1450276 -4.1086135][-4.1605735 -4.170155 -4.1612439 -4.1394463 -4.1136708 -4.0789862 -4.06014 -4.0700555 -4.1133294 -4.1561751 -4.1889906 -4.1955557 -4.1855154 -4.159296 -4.1261044][-4.1720018 -4.1724424 -4.161252 -4.1474152 -4.1349688 -4.1180329 -4.1102448 -4.1176338 -4.1419115 -4.1721072 -4.1998057 -4.2013583 -4.1890254 -4.1680808 -4.1480079][-4.2061505 -4.1936822 -4.1772056 -4.1669092 -4.158761 -4.1468568 -4.1396122 -4.1445618 -4.1612468 -4.1856327 -4.2077742 -4.2094145 -4.2011514 -4.190815 -4.1841726][-4.2420721 -4.2221456 -4.2046595 -4.1964312 -4.1901121 -4.1808224 -4.1747074 -4.1788239 -4.1894298 -4.2079535 -4.2251081 -4.2336893 -4.2371244 -4.234375 -4.2288961]]...]
INFO - root - 2017-12-07 20:22:39.948444: step 50310, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 55h:30m:48s remains)
INFO - root - 2017-12-07 20:22:46.739872: step 50320, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.659 sec/batch; 51h:39m:40s remains)
INFO - root - 2017-12-07 20:22:53.615056: step 50330, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.652 sec/batch; 51h:05m:31s remains)
INFO - root - 2017-12-07 20:23:00.489168: step 50340, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 58h:08m:57s remains)
INFO - root - 2017-12-07 20:23:07.326865: step 50350, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.737 sec/batch; 57h:47m:51s remains)
INFO - root - 2017-12-07 20:23:14.148995: step 50360, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 50h:40m:46s remains)
INFO - root - 2017-12-07 20:23:20.934645: step 50370, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 49h:56m:57s remains)
INFO - root - 2017-12-07 20:23:27.707383: step 50380, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 55h:48m:16s remains)
INFO - root - 2017-12-07 20:23:34.519228: step 50390, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 55h:03m:44s remains)
INFO - root - 2017-12-07 20:23:41.284435: step 50400, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 52h:18m:54s remains)
2017-12-07 20:23:41.991173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1760373 -4.1832895 -4.1886835 -4.1904025 -4.1900482 -4.1899734 -4.1967449 -4.2064452 -4.2120028 -4.2179747 -4.21944 -4.2089 -4.1875358 -4.1679759 -4.1539283][-4.1753292 -4.186408 -4.1971517 -4.2058783 -4.2093873 -4.20897 -4.2121263 -4.2177749 -4.2213159 -4.225656 -4.2262621 -4.2158508 -4.1931796 -4.1686635 -4.1492805][-4.1835337 -4.200974 -4.2169166 -4.2311673 -4.238905 -4.2401352 -4.2412758 -4.24616 -4.2489405 -4.2520013 -4.2509975 -4.2406063 -4.2151442 -4.18369 -4.157218][-4.1774969 -4.1993532 -4.2193813 -4.2400479 -4.2541718 -4.2568588 -4.2562695 -4.2586932 -4.2649055 -4.2733235 -4.2750773 -4.2647867 -4.2392845 -4.204143 -4.17252][-4.161211 -4.1840787 -4.2036366 -4.22331 -4.235673 -4.2333107 -4.2303195 -4.2312188 -4.2456017 -4.2676215 -4.2832561 -4.2808371 -4.2592664 -4.2264357 -4.1914458][-4.1548014 -4.1759114 -4.1857314 -4.1878915 -4.1791878 -4.1534181 -4.133625 -4.1335187 -4.1674886 -4.218926 -4.2610612 -4.2782464 -4.2704558 -4.2453213 -4.2098393][-4.1717243 -4.1843085 -4.1742415 -4.14443 -4.1029091 -4.0402374 -3.9836555 -3.9719687 -4.0357451 -4.1366768 -4.2173576 -4.2624569 -4.2760305 -4.2631364 -4.2321405][-4.2008996 -4.2017946 -4.1739078 -4.1153879 -4.0399852 -3.9356148 -3.8279114 -3.7876959 -3.8843367 -4.0396538 -4.1561456 -4.22415 -4.2570791 -4.2588224 -4.241075][-4.212585 -4.2077823 -4.1795807 -4.1141658 -4.0267606 -3.9108944 -3.7894883 -3.7321477 -3.830292 -3.9954724 -4.114604 -4.1871734 -4.2332239 -4.2484856 -4.2433429][-4.2044029 -4.1994648 -4.1846538 -4.1410956 -4.0769987 -3.9918761 -3.9112663 -3.8709469 -3.9270587 -4.0370159 -4.1204834 -4.1759272 -4.2208161 -4.2412663 -4.2383227][-4.1976562 -4.1939521 -4.19387 -4.1787949 -4.1495767 -4.1053386 -4.0604553 -4.0283031 -4.0407829 -4.0924859 -4.1444907 -4.183167 -4.2176623 -4.2331352 -4.2268806][-4.1905556 -4.1907582 -4.201478 -4.2095318 -4.2084908 -4.1947947 -4.1731977 -4.1453996 -4.1301537 -4.1438866 -4.1706347 -4.1927409 -4.2104988 -4.21646 -4.2090225][-4.1868572 -4.1870937 -4.2002344 -4.2189231 -4.2313013 -4.2371864 -4.2331605 -4.2146254 -4.1862354 -4.1720457 -4.1761851 -4.1840167 -4.1887717 -4.1915832 -4.1900082][-4.1876535 -4.1936316 -4.2094212 -4.2276864 -4.2396855 -4.2474675 -4.2494788 -4.2373228 -4.2065511 -4.1749983 -4.1594825 -4.1598854 -4.1641583 -4.1732183 -4.1804295][-4.1840682 -4.1967006 -4.2140732 -4.2284455 -4.2343178 -4.239584 -4.2420878 -4.233686 -4.2074628 -4.1742234 -4.1539593 -4.1533933 -4.1606326 -4.1741047 -4.1852226]]...]
INFO - root - 2017-12-07 20:23:48.582972: step 50410, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 56h:54m:41s remains)
INFO - root - 2017-12-07 20:23:55.467365: step 50420, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.717 sec/batch; 56h:13m:10s remains)
INFO - root - 2017-12-07 20:24:02.277990: step 50430, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 54h:27m:15s remains)
INFO - root - 2017-12-07 20:24:09.087326: step 50440, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 50h:54m:44s remains)
INFO - root - 2017-12-07 20:24:15.893669: step 50450, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.685 sec/batch; 53h:39m:22s remains)
INFO - root - 2017-12-07 20:24:22.752370: step 50460, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 53h:47m:01s remains)
INFO - root - 2017-12-07 20:24:29.546550: step 50470, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 55h:02m:31s remains)
INFO - root - 2017-12-07 20:24:36.124726: step 50480, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 52h:39m:34s remains)
INFO - root - 2017-12-07 20:24:43.080702: step 50490, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 52h:14m:16s remains)
INFO - root - 2017-12-07 20:24:49.970301: step 50500, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 54h:35m:54s remains)
2017-12-07 20:24:50.714837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2491217 -4.265934 -4.2832232 -4.2941484 -4.2919769 -4.2792192 -4.2519455 -4.2204723 -4.2020597 -4.2058277 -4.2163143 -4.2233891 -4.2267542 -4.223948 -4.221077][-4.2490206 -4.2627215 -4.2825232 -4.2970772 -4.295856 -4.2778792 -4.2401009 -4.1969781 -4.1736073 -4.1833534 -4.1989522 -4.2059913 -4.2145052 -4.2192178 -4.2221384][-4.2622309 -4.2673597 -4.2836609 -4.2985668 -4.2937279 -4.2678232 -4.219871 -4.16747 -4.14018 -4.157443 -4.1789789 -4.1893148 -4.2033234 -4.2148733 -4.227282][-4.2803078 -4.2785244 -4.2888851 -4.2989192 -4.2877164 -4.2566457 -4.1989937 -4.1369667 -4.1061754 -4.131074 -4.1606345 -4.1767135 -4.191906 -4.2069664 -4.2281804][-4.2962694 -4.2898555 -4.2927494 -4.2948313 -4.2811651 -4.2473407 -4.1827092 -4.1139374 -4.0827632 -4.1149225 -4.1542368 -4.1786766 -4.1956058 -4.2084694 -4.22801][-4.3082786 -4.3030343 -4.2985868 -4.2886996 -4.26704 -4.2274909 -4.1569991 -4.0831251 -4.054986 -4.0956588 -4.1491776 -4.1886964 -4.2156615 -4.2303047 -4.2376513][-4.316155 -4.3161116 -4.3073478 -4.284411 -4.2485113 -4.1985388 -4.1220059 -4.0495195 -4.0305147 -4.0791259 -4.1445112 -4.200386 -4.2412448 -4.2634325 -4.2632184][-4.3211126 -4.3281546 -4.3191543 -4.2890525 -4.2422256 -4.1825709 -4.1079497 -4.0434513 -4.0290847 -4.073091 -4.1372242 -4.2016215 -4.2537622 -4.2858691 -4.2917442][-4.3217282 -4.3327446 -4.3274179 -4.2975879 -4.2509665 -4.1937218 -4.12855 -4.0754576 -4.0586286 -4.0821996 -4.1284981 -4.188746 -4.2465315 -4.288609 -4.30788][-4.3203616 -4.3323 -4.3322153 -4.3096647 -4.2715769 -4.2275047 -4.1779408 -4.1402831 -4.1214914 -4.1209903 -4.1394591 -4.1789088 -4.2283611 -4.2732239 -4.3045039][-4.3194146 -4.3309855 -4.3368821 -4.3240232 -4.2960587 -4.2656746 -4.2313895 -4.2018194 -4.1830792 -4.1677504 -4.1648993 -4.1797609 -4.2104983 -4.2477155 -4.2822685][-4.3203616 -4.3294253 -4.3380828 -4.3343353 -4.3152323 -4.2925653 -4.2671671 -4.2394924 -4.2173929 -4.1953564 -4.1816134 -4.1813889 -4.1987524 -4.2223868 -4.251483][-4.3221312 -4.327631 -4.3335519 -4.3337121 -4.3221755 -4.3054433 -4.28751 -4.265573 -4.2434564 -4.2169313 -4.1962714 -4.1874204 -4.194407 -4.2074361 -4.2255855][-4.3217044 -4.3250465 -4.3287106 -4.32848 -4.3212223 -4.3110251 -4.30156 -4.2870927 -4.2705355 -4.2469649 -4.2242107 -4.2073493 -4.2012582 -4.2040648 -4.2118244][-4.3192267 -4.3201885 -4.3227348 -4.3203712 -4.31399 -4.3087354 -4.3067384 -4.3017588 -4.2940655 -4.2773213 -4.2581506 -4.2383356 -4.2195039 -4.2097926 -4.2060156]]...]
INFO - root - 2017-12-07 20:24:57.215406: step 50510, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.646 sec/batch; 50h:34m:02s remains)
INFO - root - 2017-12-07 20:25:04.186772: step 50520, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.775 sec/batch; 60h:40m:23s remains)
INFO - root - 2017-12-07 20:25:10.985542: step 50530, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 55h:50m:38s remains)
INFO - root - 2017-12-07 20:25:17.752598: step 50540, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 54h:33m:46s remains)
INFO - root - 2017-12-07 20:25:24.486455: step 50550, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 50h:48m:36s remains)
INFO - root - 2017-12-07 20:25:31.268315: step 50560, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 52h:09m:25s remains)
INFO - root - 2017-12-07 20:25:38.032289: step 50570, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 56h:06m:36s remains)
INFO - root - 2017-12-07 20:25:44.809104: step 50580, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 53h:00m:36s remains)
INFO - root - 2017-12-07 20:25:51.527164: step 50590, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 52h:07m:06s remains)
INFO - root - 2017-12-07 20:25:58.299191: step 50600, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 49h:26m:25s remains)
2017-12-07 20:25:58.980453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2427607 -4.2698956 -4.2850337 -4.2967048 -4.2982574 -4.283411 -4.2622232 -4.2427516 -4.2399297 -4.25284 -4.266479 -4.2748504 -4.2766528 -4.2831287 -4.2863765][-4.2762027 -4.3037128 -4.3226385 -4.3343215 -4.3256645 -4.2951164 -4.2583838 -4.2235274 -4.2195063 -4.2375059 -4.2583437 -4.27891 -4.2904286 -4.2999959 -4.3038507][-4.30789 -4.3348451 -4.3517246 -4.3557639 -4.3363886 -4.2874289 -4.2279577 -4.1843076 -4.1851153 -4.21325 -4.2433724 -4.2760315 -4.2981009 -4.3125958 -4.319222][-4.3220487 -4.3439736 -4.3559065 -4.3504505 -4.3207879 -4.2549744 -4.17491 -4.1223259 -4.1274195 -4.1723828 -4.2188563 -4.2619176 -4.2910676 -4.3096471 -4.3215375][-4.3217559 -4.3362937 -4.3408 -4.3267274 -4.2852726 -4.2056036 -4.1047935 -4.0329185 -4.0447292 -4.1176925 -4.1887336 -4.2420921 -4.2747455 -4.2965622 -4.3136592][-4.3240113 -4.3277326 -4.320991 -4.2910008 -4.2339187 -4.1369576 -3.9994142 -3.8931379 -3.9223642 -4.0445561 -4.1537147 -4.2190595 -4.256578 -4.2827106 -4.3006849][-4.3213148 -4.314775 -4.2971292 -4.2532797 -4.17945 -4.0589824 -3.8721237 -3.7166932 -3.7824519 -3.975255 -4.12134 -4.1985669 -4.2428765 -4.2755756 -4.2915888][-4.3149171 -4.304162 -4.281898 -4.2294955 -4.1432161 -4.0059352 -3.7839684 -3.6014352 -3.7185144 -3.9590335 -4.1153922 -4.1947346 -4.2466655 -4.2836776 -4.2964][-4.3079429 -4.2978044 -4.2745476 -4.2184472 -4.1342683 -4.0062652 -3.8173327 -3.6961606 -3.8157852 -4.0128822 -4.1364222 -4.2064586 -4.2634392 -4.2992415 -4.3073683][-4.3093419 -4.30122 -4.2776322 -4.2251697 -4.1578965 -4.0649719 -3.9443908 -3.8936057 -3.9751697 -4.087935 -4.1651554 -4.2231269 -4.2788377 -4.30865 -4.3149705][-4.3181071 -4.30843 -4.2828646 -4.2400889 -4.1980233 -4.1453166 -4.0843191 -4.0665941 -4.1035147 -4.152597 -4.1984921 -4.2450943 -4.2907944 -4.3140559 -4.3228235][-4.3247561 -4.3110704 -4.2832837 -4.2518368 -4.2341719 -4.2172632 -4.1978292 -4.1910849 -4.1945114 -4.20438 -4.2330036 -4.2685318 -4.3002009 -4.3187985 -4.330555][-4.3312955 -4.316679 -4.289062 -4.2663031 -4.2609324 -4.2637525 -4.2643113 -4.2597346 -4.252317 -4.2469411 -4.2623553 -4.2874174 -4.3092165 -4.3228269 -4.3309369][-4.3348951 -4.3269358 -4.3028231 -4.2835355 -4.2817121 -4.2883997 -4.295579 -4.2930746 -4.2834563 -4.2733231 -4.2766132 -4.2901669 -4.3018188 -4.3088984 -4.3094387][-4.3310013 -4.330503 -4.3105035 -4.2918835 -4.2912269 -4.3009653 -4.3127484 -4.3113461 -4.2987323 -4.2882462 -4.283164 -4.2829647 -4.282712 -4.2803774 -4.2736945]]...]
INFO - root - 2017-12-07 20:26:05.631738: step 50610, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 54h:58m:28s remains)
INFO - root - 2017-12-07 20:26:12.446963: step 50620, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 53h:41m:31s remains)
INFO - root - 2017-12-07 20:26:19.333997: step 50630, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.638 sec/batch; 49h:54m:57s remains)
INFO - root - 2017-12-07 20:26:26.193308: step 50640, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 52h:49m:07s remains)
INFO - root - 2017-12-07 20:26:33.005686: step 50650, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 54h:28m:38s remains)
INFO - root - 2017-12-07 20:26:39.896229: step 50660, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 56h:37m:52s remains)
INFO - root - 2017-12-07 20:26:46.782854: step 50670, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 56h:08m:37s remains)
INFO - root - 2017-12-07 20:26:53.536000: step 50680, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 51h:59m:10s remains)
INFO - root - 2017-12-07 20:27:00.347292: step 50690, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 52h:12m:28s remains)
INFO - root - 2017-12-07 20:27:07.078310: step 50700, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 54h:19m:50s remains)
2017-12-07 20:27:07.977700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1647363 -4.1408782 -4.1151628 -4.104641 -4.1007247 -4.1054063 -4.1266279 -4.1487651 -4.1661196 -4.1843843 -4.1860261 -4.1662545 -4.1393671 -4.1143136 -4.0964489][-4.14738 -4.1357412 -4.1189265 -4.1084719 -4.1031322 -4.1054773 -4.1357746 -4.1709189 -4.1889038 -4.1999292 -4.1978059 -4.1790619 -4.1532621 -4.1232829 -4.0933895][-4.1290793 -4.1305289 -4.1219163 -4.1060352 -4.0907044 -4.0873346 -4.1264224 -4.1803455 -4.2033844 -4.2074614 -4.2035556 -4.195478 -4.1809373 -4.155005 -4.1222367][-4.1146259 -4.1277266 -4.1266303 -4.1051259 -4.0789762 -4.0713644 -4.1131887 -4.1802883 -4.2092953 -4.2095642 -4.2070532 -4.2118821 -4.2094588 -4.1881728 -4.1539087][-4.1087341 -4.1299772 -4.135541 -4.1144466 -4.084866 -4.0717797 -4.0991211 -4.1608276 -4.19646 -4.2026663 -4.2068982 -4.2196774 -4.2228217 -4.2026272 -4.1669908][-4.1122751 -4.137887 -4.1477923 -4.1315327 -4.1024427 -4.0794353 -4.0729995 -4.1075273 -4.1450248 -4.164988 -4.1888556 -4.2145939 -4.2223487 -4.2034516 -4.1688061][-4.1376452 -4.1672497 -4.1803207 -4.1630859 -4.1254592 -4.0821948 -4.0264869 -4.0124321 -4.0417356 -4.0899944 -4.1518464 -4.20374 -4.2209268 -4.2050138 -4.1698656][-4.1671062 -4.2035303 -4.2203765 -4.2022018 -4.1533065 -4.0859962 -3.9802086 -3.9039354 -3.9190836 -4.0022879 -4.1089964 -4.1898432 -4.2192125 -4.2092562 -4.1761255][-4.1886406 -4.2286973 -4.2460012 -4.2289648 -4.1746449 -4.0883412 -3.9580684 -3.8465924 -3.849858 -3.9494321 -4.0748305 -4.1668453 -4.2033868 -4.2041411 -4.1813855][-4.1986213 -4.2421775 -4.2591944 -4.245605 -4.1974721 -4.1112485 -3.9928567 -3.8964078 -3.8959146 -3.9749618 -4.0719333 -4.1462879 -4.1819649 -4.193666 -4.1851759][-4.20757 -4.2551279 -4.2753115 -4.266542 -4.2289577 -4.158277 -4.0745406 -4.0135732 -4.0126004 -4.055418 -4.1016545 -4.14137 -4.166431 -4.1797 -4.1798811][-4.2017741 -4.2530847 -4.28116 -4.2783689 -4.2480006 -4.195426 -4.1421909 -4.1092486 -4.10603 -4.1208234 -4.130549 -4.1473079 -4.1547422 -4.154429 -4.155035][-4.1767616 -4.2243223 -4.2559919 -4.2589464 -4.236537 -4.203896 -4.1721153 -4.1565137 -4.1575909 -4.1599259 -4.1529927 -4.1569533 -4.1479969 -4.1258621 -4.1130276][-4.1353455 -4.1704488 -4.1936164 -4.1977363 -4.1885448 -4.1794477 -4.1658545 -4.1658578 -4.1818085 -4.1895356 -4.1794682 -4.1756215 -4.1554728 -4.1142349 -4.0815921][-4.1063924 -4.1276073 -4.1397486 -4.14052 -4.1376987 -4.1389642 -4.1369138 -4.1477585 -4.1776805 -4.2016482 -4.2014866 -4.1989751 -4.18043 -4.1362076 -4.0956736]]...]
INFO - root - 2017-12-07 20:27:14.566045: step 50710, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 49h:34m:24s remains)
INFO - root - 2017-12-07 20:27:21.456937: step 50720, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 55h:24m:32s remains)
INFO - root - 2017-12-07 20:27:28.412150: step 50730, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 57h:55m:52s remains)
INFO - root - 2017-12-07 20:27:35.310343: step 50740, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 55h:02m:14s remains)
INFO - root - 2017-12-07 20:27:42.098274: step 50750, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 50h:39m:55s remains)
INFO - root - 2017-12-07 20:27:48.852979: step 50760, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 56h:33m:40s remains)
INFO - root - 2017-12-07 20:27:55.715447: step 50770, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 56h:34m:09s remains)
INFO - root - 2017-12-07 20:28:02.535943: step 50780, loss = 2.05, batch loss = 1.99 (13.2 examples/sec; 0.604 sec/batch; 47h:17m:47s remains)
INFO - root - 2017-12-07 20:28:09.229928: step 50790, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 49h:33m:45s remains)
INFO - root - 2017-12-07 20:28:16.065424: step 50800, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 55h:20m:46s remains)
2017-12-07 20:28:16.780859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.181253 -4.1752625 -4.1873231 -4.1828938 -4.170886 -4.1603427 -4.1531696 -4.1560259 -4.179615 -4.1986661 -4.1962376 -4.1876588 -4.1908789 -4.1967487 -4.1962738][-4.2086043 -4.208406 -4.2184391 -4.2061963 -4.1909904 -4.1791635 -4.1712184 -4.1756859 -4.204175 -4.2272162 -4.229362 -4.2225375 -4.2213507 -4.2217684 -4.2169032][-4.2053728 -4.215559 -4.2263031 -4.20887 -4.1871119 -4.1721005 -4.162703 -4.1668663 -4.1969595 -4.2275243 -4.2382364 -4.2358561 -4.2326069 -4.2288065 -4.221652][-4.1747446 -4.1939158 -4.2077703 -4.1963348 -4.1744771 -4.1520953 -4.1315732 -4.130084 -4.1594062 -4.19259 -4.2061596 -4.2063155 -4.2056437 -4.2039652 -4.2003431][-4.1365633 -4.1519876 -4.1614928 -4.1487827 -4.1233058 -4.0932913 -4.0555806 -4.0415468 -4.0793591 -4.120914 -4.138936 -4.1449881 -4.1539145 -4.1619382 -4.1653638][-4.1003752 -4.1023674 -4.0968742 -4.0692067 -4.0341611 -3.993897 -3.9317322 -3.9059105 -3.9670129 -4.0277405 -4.0563178 -4.0750656 -4.0973849 -4.1193604 -4.1353731][-4.0700717 -4.0608163 -4.0359931 -3.9879136 -3.9441967 -3.8939366 -3.8006587 -3.7615681 -3.8552794 -3.941829 -3.9856513 -4.0171719 -4.0471845 -4.0774722 -4.1053696][-4.0588045 -4.05111 -4.0226016 -3.976464 -3.9486718 -3.9172883 -3.8393481 -3.8112354 -3.8984194 -3.9715719 -4.0050344 -4.0266786 -4.0445371 -4.0677805 -4.0956936][-4.0512066 -4.0538764 -4.0347643 -4.0068789 -4.0024433 -3.9934227 -3.9485235 -3.940516 -4.0016232 -4.0493207 -4.0642624 -4.0664105 -4.0693612 -4.0825276 -4.104094][-4.0446715 -4.0620451 -4.0563107 -4.0432048 -4.0532303 -4.05473 -4.0319886 -4.0332103 -4.06967 -4.0964975 -4.1005173 -4.090055 -4.083673 -4.0901923 -4.1038575][-4.0673237 -4.0919652 -4.0965252 -4.0929666 -4.1092472 -4.1148973 -4.1039281 -4.1065822 -4.1235394 -4.1342473 -4.1320462 -4.116971 -4.1081729 -4.1119494 -4.1213841][-4.1174021 -4.1388249 -4.1455693 -4.1463442 -4.1594973 -4.1645403 -4.158823 -4.1600695 -4.1681919 -4.1715937 -4.1677775 -4.1527925 -4.1437082 -4.1484861 -4.1585331][-4.1773677 -4.1894937 -4.1904716 -4.1873331 -4.1903915 -4.1900277 -4.1865416 -4.1879191 -4.1929135 -4.1951408 -4.1929111 -4.182775 -4.1773148 -4.1842737 -4.1962709][-4.2320137 -4.2352738 -4.2303715 -4.2238135 -4.2196627 -4.2142987 -4.2110162 -4.212296 -4.2159276 -4.2183537 -4.2180424 -4.2143855 -4.2149577 -4.2235904 -4.2353816][-4.2584696 -4.2582936 -4.2536263 -4.2495837 -4.2475595 -4.2450151 -4.2438645 -4.2444034 -4.2468023 -4.2495546 -4.2507892 -4.2513824 -4.2551546 -4.2632227 -4.2723827]]...]
INFO - root - 2017-12-07 20:28:23.338222: step 50810, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 50h:28m:09s remains)
INFO - root - 2017-12-07 20:28:30.182916: step 50820, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 50h:03m:46s remains)
INFO - root - 2017-12-07 20:28:36.945350: step 50830, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 56h:12m:18s remains)
INFO - root - 2017-12-07 20:28:43.655062: step 50840, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 55h:09m:14s remains)
INFO - root - 2017-12-07 20:28:50.467067: step 50850, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 54h:00m:09s remains)
INFO - root - 2017-12-07 20:28:57.222570: step 50860, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 49h:52m:59s remains)
INFO - root - 2017-12-07 20:29:04.052415: step 50870, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 51h:12m:49s remains)
INFO - root - 2017-12-07 20:29:10.930548: step 50880, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 55h:16m:41s remains)
INFO - root - 2017-12-07 20:29:17.688562: step 50890, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 56h:33m:44s remains)
INFO - root - 2017-12-07 20:29:24.561673: step 50900, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 53h:43m:27s remains)
2017-12-07 20:29:25.226206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2288661 -4.1961832 -4.1857815 -4.1857042 -4.1804943 -4.1855674 -4.1977029 -4.2109179 -4.2072363 -4.1928806 -4.1737943 -4.1558342 -4.1539874 -4.1604295 -4.1626339][-4.2173786 -4.1776361 -4.1655464 -4.1658363 -4.1650229 -4.1783643 -4.19809 -4.2124462 -4.2053275 -4.1924596 -4.1754704 -4.1564875 -4.15284 -4.1607895 -4.1637044][-4.2027326 -4.1622868 -4.1522007 -4.156436 -4.1592426 -4.1733637 -4.1928892 -4.2054715 -4.2008348 -4.1952448 -4.1823473 -4.163784 -4.1581416 -4.16598 -4.1717811][-4.1897225 -4.1580491 -4.1547256 -4.1611047 -4.1589718 -4.1629825 -4.1724553 -4.1808934 -4.1824141 -4.1849461 -4.1779294 -4.1634097 -4.1589103 -4.1680937 -4.1804576][-4.19484 -4.1735005 -4.1737914 -4.1761432 -4.1619473 -4.1458735 -4.1372418 -4.1430626 -4.1536436 -4.1642556 -4.1614709 -4.1535015 -4.1525311 -4.1654186 -4.1831708][-4.211597 -4.1954088 -4.1928205 -4.1861472 -4.1542492 -4.1154518 -4.0888443 -4.0906487 -4.1143208 -4.1358809 -4.14174 -4.1442552 -4.1486974 -4.1613436 -4.1849928][-4.1977763 -4.173769 -4.1627049 -4.1474924 -4.1026316 -4.044971 -4.0002356 -4.0014796 -4.0434308 -4.079246 -4.0962954 -4.1116614 -4.1309867 -4.1512651 -4.1819854][-4.1559067 -4.1176672 -4.0934839 -4.0722928 -4.0199008 -3.9455678 -3.8846812 -3.8903143 -3.9534028 -4.0105419 -4.04659 -4.0787077 -4.1161742 -4.1491761 -4.1800122][-4.132618 -4.0811949 -4.0460615 -4.0281796 -3.9891105 -3.9216788 -3.8647282 -3.8781407 -3.9432681 -4.006135 -4.050725 -4.0878067 -4.13194 -4.1669254 -4.1869173][-4.132576 -4.0791163 -4.0522108 -4.056025 -4.0528836 -4.0173512 -3.9810991 -3.991255 -4.0341506 -4.0800672 -4.1155438 -4.1440806 -4.1770158 -4.1983862 -4.199048][-4.1431718 -4.0933909 -4.0821719 -4.1075659 -4.1316404 -4.120697 -4.1005368 -4.1020131 -4.1218019 -4.1508036 -4.1810694 -4.20507 -4.2256207 -4.2283783 -4.2120104][-4.1376233 -4.0886116 -4.0901074 -4.1312051 -4.1714692 -4.172821 -4.1625524 -4.1619911 -4.1705909 -4.1925688 -4.2216883 -4.2435317 -4.2509613 -4.2373266 -4.2124281][-4.1265192 -4.074657 -4.0805235 -4.129775 -4.1771774 -4.1888604 -4.1891966 -4.1938291 -4.2035389 -4.2229366 -4.2451253 -4.2582855 -4.2525096 -4.2294841 -4.2060905][-4.1299119 -4.0775285 -4.080224 -4.12613 -4.1752043 -4.19525 -4.2090878 -4.2245269 -4.2389073 -4.2504611 -4.259057 -4.2583218 -4.243124 -4.2190614 -4.2021871][-4.1588726 -4.1124129 -4.1099529 -4.1439266 -4.1861491 -4.2096329 -4.2310233 -4.2508206 -4.2614808 -4.2642336 -4.2594957 -4.2493448 -4.2321234 -4.2132182 -4.2023044]]...]
INFO - root - 2017-12-07 20:29:31.886473: step 50910, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 56h:25m:35s remains)
INFO - root - 2017-12-07 20:29:38.689494: step 50920, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 57h:08m:48s remains)
INFO - root - 2017-12-07 20:29:45.589257: step 50930, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 51h:56m:46s remains)
INFO - root - 2017-12-07 20:29:52.379770: step 50940, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 51h:55m:02s remains)
INFO - root - 2017-12-07 20:29:59.256636: step 50950, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.705 sec/batch; 55h:06m:50s remains)
INFO - root - 2017-12-07 20:30:06.159449: step 50960, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.687 sec/batch; 53h:44m:06s remains)
INFO - root - 2017-12-07 20:30:12.963958: step 50970, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 54h:53m:12s remains)
INFO - root - 2017-12-07 20:30:19.795183: step 50980, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 51h:52m:18s remains)
INFO - root - 2017-12-07 20:30:26.609242: step 50990, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 52h:05m:22s remains)
INFO - root - 2017-12-07 20:30:33.443169: step 51000, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 55h:41m:18s remains)
2017-12-07 20:30:34.138346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2990193 -4.2959194 -4.2924938 -4.2931557 -4.2965636 -4.2932992 -4.2827282 -4.2695627 -4.2646842 -4.2691746 -4.2769871 -4.2873511 -4.30216 -4.315784 -4.3247485][-4.29303 -4.2865829 -4.2806721 -4.2775822 -4.2772808 -4.2670646 -4.2465668 -4.2246413 -4.2168422 -4.223083 -4.2381015 -4.2582688 -4.283751 -4.3037615 -4.3156848][-4.2928491 -4.2826638 -4.2762637 -4.2678671 -4.2602248 -4.2395339 -4.2051659 -4.1728492 -4.1628246 -4.1720328 -4.195384 -4.2272925 -4.263597 -4.2908816 -4.3076754][-4.2885275 -4.2764359 -4.2696471 -4.2604713 -4.2456784 -4.213305 -4.1638913 -4.1197262 -4.1100411 -4.1289225 -4.1610842 -4.200314 -4.2420006 -4.2758794 -4.2985954][-4.2663 -4.2511425 -4.2393355 -4.2278218 -4.2074656 -4.1672373 -4.1034074 -4.0411644 -4.0308628 -4.0645342 -4.1084337 -4.1572218 -4.2095761 -4.2545643 -4.2866235][-4.2207756 -4.2015405 -4.1852536 -4.1704559 -4.145534 -4.0963783 -4.0060964 -3.9041553 -3.8936439 -3.9606841 -4.02956 -4.0971494 -4.168654 -4.229331 -4.2735043][-4.1857243 -4.1649513 -4.145525 -4.1299338 -4.1071806 -4.0574036 -3.9464002 -3.8008637 -3.78999 -3.8964944 -3.994601 -4.0773087 -4.157721 -4.2245502 -4.27217][-4.1663437 -4.1490893 -4.1349449 -4.1254482 -4.115129 -4.0885372 -4.00822 -3.8810711 -3.8595266 -3.9513245 -4.0395727 -4.111444 -4.1788955 -4.2363815 -4.2785845][-4.1549711 -4.1417327 -4.138082 -4.1390905 -4.1411881 -4.1357317 -4.0957947 -4.0186548 -3.998672 -4.0545912 -4.1148205 -4.1633134 -4.2082214 -4.2495828 -4.28309][-4.1488709 -4.1344938 -4.1372337 -4.1443725 -4.1516714 -4.1518903 -4.1276069 -4.0825725 -4.0755982 -4.1195498 -4.1662645 -4.2026296 -4.2326918 -4.2622056 -4.2890167][-4.1765242 -4.153873 -4.1482606 -4.1518536 -4.1554794 -4.1516504 -4.1355114 -4.1084204 -4.107317 -4.1436987 -4.1837959 -4.2165918 -4.2428074 -4.2701235 -4.2959528][-4.2283 -4.20606 -4.1934342 -4.1899734 -4.1862154 -4.1779952 -4.1703014 -4.1575069 -4.1572905 -4.1786561 -4.2070756 -4.2340479 -4.2593 -4.2847247 -4.3079534][-4.2706351 -4.2552938 -4.2411184 -4.2338557 -4.2281671 -4.2227497 -4.2222147 -4.2190275 -4.2196131 -4.2301049 -4.2458272 -4.2640529 -4.2843037 -4.30584 -4.3230863][-4.2975664 -4.2869735 -4.2757974 -4.2682905 -4.2625623 -4.2618623 -4.2664142 -4.2707353 -4.2749472 -4.2825952 -4.2914171 -4.3017139 -4.3145642 -4.3287997 -4.3386674][-4.3193455 -4.312242 -4.3030572 -4.2963896 -4.2895465 -4.2880158 -4.2924871 -4.2995448 -4.3069825 -4.3140268 -4.32029 -4.326725 -4.3346262 -4.3428726 -4.3471723]]...]
INFO - root - 2017-12-07 20:30:40.710268: step 51010, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 49h:16m:23s remains)
INFO - root - 2017-12-07 20:30:47.657285: step 51020, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 53h:14m:57s remains)
INFO - root - 2017-12-07 20:30:54.610262: step 51030, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 55h:23m:04s remains)
INFO - root - 2017-12-07 20:31:01.460160: step 51040, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 54h:31m:31s remains)
INFO - root - 2017-12-07 20:31:08.314666: step 51050, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 55h:03m:10s remains)
INFO - root - 2017-12-07 20:31:15.106436: step 51060, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 50h:10m:58s remains)
INFO - root - 2017-12-07 20:31:21.891216: step 51070, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.655 sec/batch; 51h:11m:30s remains)
INFO - root - 2017-12-07 20:31:28.777923: step 51080, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 53h:34m:34s remains)
INFO - root - 2017-12-07 20:31:35.518725: step 51090, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 54h:54m:54s remains)
INFO - root - 2017-12-07 20:31:42.105109: step 51100, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 49h:20m:13s remains)
2017-12-07 20:31:42.831277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2702389 -4.2694426 -4.2677107 -4.2663975 -4.26593 -4.265933 -4.26567 -4.2629805 -4.2581859 -4.2518249 -4.239851 -4.2278557 -4.2216825 -4.2220697 -4.2235808][-4.2684412 -4.2676892 -4.265945 -4.2640347 -4.2623739 -4.2603211 -4.25745 -4.2520204 -4.2445335 -4.236392 -4.2250171 -4.2158642 -4.2140541 -4.2192507 -4.2242303][-4.2589869 -4.2576761 -4.2548189 -4.2506623 -4.2457523 -4.2399006 -4.2332673 -4.2243333 -4.2141194 -4.2050352 -4.1962419 -4.1922922 -4.1979289 -4.2108765 -4.2220044][-4.2441859 -4.2424588 -4.23756 -4.2291207 -4.2185826 -4.2067137 -4.1940832 -4.1812563 -4.1705751 -4.1633072 -4.1591725 -4.1617947 -4.1757116 -4.197094 -4.2160082][-4.2270622 -4.2231255 -4.2141595 -4.1999178 -4.1835165 -4.1650314 -4.1458588 -4.1311359 -4.1245942 -4.1238513 -4.1262727 -4.1358924 -4.1563177 -4.1839 -4.2091746][-4.209311 -4.201622 -4.1877022 -4.1684871 -4.1480689 -4.124588 -4.100872 -4.088593 -4.0913081 -4.098773 -4.1059437 -4.1192436 -4.1419239 -4.1701579 -4.1976466][-4.2007213 -4.1892152 -4.1719437 -4.1519256 -4.132977 -4.1105332 -4.0875378 -4.079453 -4.0886512 -4.1000857 -4.1071768 -4.1171727 -4.13467 -4.158092 -4.1832886][-4.2022653 -4.1902795 -4.1746316 -4.1602688 -4.1499872 -4.1385317 -4.1256638 -4.1213856 -4.1285496 -4.1352949 -4.1344128 -4.1327462 -4.138638 -4.152463 -4.1721716][-4.2128568 -4.2038374 -4.1938505 -4.1877794 -4.1874981 -4.1882262 -4.1862788 -4.1838908 -4.1843815 -4.1828094 -4.1724634 -4.1599789 -4.1550617 -4.1597137 -4.1720862][-4.2216 -4.215878 -4.2116718 -4.2119169 -4.2175164 -4.2262759 -4.2333217 -4.2342486 -4.2310658 -4.2247529 -4.2085891 -4.1886148 -4.1756706 -4.1723056 -4.1784687][-4.2252874 -4.2205338 -4.2215123 -4.2269096 -4.2353907 -4.2480631 -4.2625084 -4.2692013 -4.2667627 -4.2596693 -4.2410169 -4.2156229 -4.1961412 -4.1860976 -4.1876755][-4.2275558 -4.2234416 -4.2300358 -4.2408624 -4.2512474 -4.2650514 -4.2837005 -4.294138 -4.2934103 -4.2854314 -4.2640982 -4.2357593 -4.2136827 -4.2003222 -4.1987448][-4.2347503 -4.2333775 -4.2443891 -4.258729 -4.2690663 -4.2811542 -4.2987928 -4.3077374 -4.3056316 -4.2955303 -4.2725244 -4.2437658 -4.2236934 -4.2114811 -4.2088408][-4.2428126 -4.2459526 -4.2593446 -4.2728248 -4.2805882 -4.2884536 -4.3011837 -4.3061452 -4.3017607 -4.2899661 -4.2671461 -4.2424369 -4.2292027 -4.22192 -4.2198758][-4.2545657 -4.2619448 -4.2750092 -4.2847505 -4.2890816 -4.2928925 -4.3000164 -4.3011012 -4.294971 -4.2826581 -4.2612624 -4.2407351 -4.2336721 -4.2305236 -4.2296848]]...]
INFO - root - 2017-12-07 20:31:49.431587: step 51110, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 56h:15m:44s remains)
INFO - root - 2017-12-07 20:31:56.307944: step 51120, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.669 sec/batch; 52h:15m:15s remains)
INFO - root - 2017-12-07 20:32:03.141565: step 51130, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 50h:36m:18s remains)
INFO - root - 2017-12-07 20:32:10.039708: step 51140, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 50h:21m:00s remains)
INFO - root - 2017-12-07 20:32:16.877569: step 51150, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 54h:56m:52s remains)
INFO - root - 2017-12-07 20:32:23.748990: step 51160, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 51h:44m:52s remains)
INFO - root - 2017-12-07 20:32:30.580681: step 51170, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 52h:24m:09s remains)
INFO - root - 2017-12-07 20:32:37.353052: step 51180, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.643 sec/batch; 50h:15m:06s remains)
INFO - root - 2017-12-07 20:32:44.224436: step 51190, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 53h:49m:17s remains)
INFO - root - 2017-12-07 20:32:51.024971: step 51200, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 57h:13m:57s remains)
2017-12-07 20:32:51.722659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3212466 -4.319335 -4.3186374 -4.3184524 -4.3174019 -4.3143291 -4.3111115 -4.3105006 -4.3123212 -4.3151803 -4.319067 -4.3227148 -4.323822 -4.32282 -4.3218794][-4.308507 -4.3079338 -4.3099003 -4.3118043 -4.3110657 -4.3060226 -4.2981138 -4.2932639 -4.2943993 -4.2997341 -4.3061953 -4.3113189 -4.3136034 -4.3149958 -4.3174176][-4.2964993 -4.2974005 -4.3012815 -4.3040609 -4.3021331 -4.2933583 -4.2791271 -4.2685266 -4.269104 -4.277215 -4.2859325 -4.2912154 -4.2937632 -4.2972364 -4.3034434][-4.2895751 -4.2918143 -4.2947025 -4.2943149 -4.28716 -4.2733088 -4.2532864 -4.2375913 -4.23907 -4.250968 -4.2639871 -4.270854 -4.2738233 -4.2772579 -4.282937][-4.2749052 -4.2786317 -4.2767577 -4.267776 -4.2507696 -4.2278619 -4.1992855 -4.1775517 -4.1827378 -4.2055159 -4.229681 -4.2445312 -4.2496667 -4.2506237 -4.2519131][-4.2380047 -4.2438989 -4.2391057 -4.2228317 -4.1973758 -4.1640668 -4.1204391 -4.0834146 -4.0892506 -4.1301093 -4.1731896 -4.2003555 -4.2096848 -4.2089062 -4.2060909][-4.2050886 -4.211659 -4.2057858 -4.185008 -4.1535268 -4.1084313 -4.0463495 -3.9872146 -3.9904227 -4.050652 -4.1147132 -4.1548734 -4.1691151 -4.1677957 -4.1622429][-4.1945167 -4.1999269 -4.1969862 -4.1784759 -4.1464009 -4.0980921 -4.0316014 -3.966635 -3.9671566 -4.028677 -4.0966306 -4.1410851 -4.1578531 -4.157403 -4.1489558][-4.2057934 -4.2119055 -4.2149725 -4.2047791 -4.17807 -4.137136 -4.0839987 -4.0376558 -4.0397758 -4.0846663 -4.1365547 -4.17249 -4.1866708 -4.1840262 -4.1699357][-4.2103987 -4.2160568 -4.2251062 -4.22237 -4.2034984 -4.1747289 -4.1393266 -4.1106024 -4.1161957 -4.1465907 -4.1800175 -4.2044725 -4.2154479 -4.2140079 -4.201088][-4.1938391 -4.1964693 -4.2138486 -4.2232752 -4.214159 -4.1923704 -4.16401 -4.1401505 -4.1443849 -4.1718426 -4.20252 -4.2275014 -4.2443075 -4.2484403 -4.240674][-4.1635079 -4.1576738 -4.1795187 -4.2012634 -4.2054358 -4.1918316 -4.1659322 -4.1379657 -4.1341424 -4.1614008 -4.1987357 -4.2334943 -4.2595825 -4.2713017 -4.2692261][-4.1390615 -4.1237664 -4.1430931 -4.1725097 -4.1921878 -4.1931863 -4.175797 -4.1442461 -4.1274624 -4.1493382 -4.1866097 -4.2230697 -4.2521787 -4.269649 -4.2756286][-4.1353192 -4.1167316 -4.1308827 -4.1587324 -4.1867461 -4.2042723 -4.2011533 -4.1762552 -4.1536326 -4.1627131 -4.1863933 -4.2114525 -4.2370906 -4.2580042 -4.2689762][-4.1611152 -4.1406646 -4.1449351 -4.1598964 -4.1829791 -4.2112565 -4.2246289 -4.2153916 -4.198947 -4.1983175 -4.2042913 -4.2119188 -4.2288966 -4.2480092 -4.2610893]]...]
INFO - root - 2017-12-07 20:32:58.334779: step 51210, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 49h:38m:45s remains)
INFO - root - 2017-12-07 20:33:05.254773: step 51220, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 56h:51m:38s remains)
INFO - root - 2017-12-07 20:33:12.034932: step 51230, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 54h:47m:43s remains)
INFO - root - 2017-12-07 20:33:18.977130: step 51240, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 55h:03m:16s remains)
INFO - root - 2017-12-07 20:33:25.805603: step 51250, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.640 sec/batch; 50h:00m:14s remains)
INFO - root - 2017-12-07 20:33:32.660207: step 51260, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 54h:46m:52s remains)
INFO - root - 2017-12-07 20:33:39.580468: step 51270, loss = 2.03, batch loss = 1.97 (11.0 examples/sec; 0.727 sec/batch; 56h:45m:47s remains)
INFO - root - 2017-12-07 20:33:46.406477: step 51280, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 53h:29m:30s remains)
INFO - root - 2017-12-07 20:33:53.124678: step 51290, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 52h:33m:07s remains)
INFO - root - 2017-12-07 20:33:59.961863: step 51300, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 50h:28m:31s remains)
2017-12-07 20:34:00.694627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.245923 -4.2281742 -4.2125931 -4.1913109 -4.1727943 -4.1562977 -4.1537075 -4.160676 -4.1638584 -4.1473007 -4.1167512 -4.0934219 -4.07043 -4.0686932 -4.0948586][-4.2199259 -4.2034473 -4.191247 -4.173214 -4.1660566 -4.1624041 -4.1628914 -4.1739769 -4.1832409 -4.1752419 -4.1589332 -4.1359582 -4.1007843 -4.0862842 -4.1004171][-4.1960011 -4.1929231 -4.1878324 -4.1731663 -4.1656661 -4.1592889 -4.1476746 -4.1566391 -4.175952 -4.1842403 -4.1863132 -4.16889 -4.1307259 -4.108428 -4.1110034][-4.1937866 -4.2007084 -4.2027278 -4.1902962 -4.1745038 -4.1487513 -4.1136951 -4.1104884 -4.1432648 -4.1726823 -4.1917992 -4.1820493 -4.1478925 -4.1205249 -4.1121716][-4.207623 -4.2167354 -4.2178931 -4.2013941 -4.1743922 -4.1234713 -4.0537667 -4.028255 -4.0743127 -4.1347642 -4.1725187 -4.1750321 -4.1573067 -4.1394734 -4.1240416][-4.2252541 -4.2322273 -4.2223058 -4.1926355 -4.1512308 -4.0756378 -3.9683807 -3.9095984 -3.972198 -4.0755529 -4.1361256 -4.1555195 -4.158978 -4.1604662 -4.1490316][-4.2444224 -4.239975 -4.2110214 -4.166616 -4.1159797 -4.0276308 -3.895983 -3.8053827 -3.8816917 -4.0191584 -4.1024284 -4.1412196 -4.1629243 -4.1791615 -4.1717434][-4.2512417 -4.23272 -4.1921711 -4.1450806 -4.0989633 -4.0283389 -3.9174905 -3.8364315 -3.8959732 -4.016418 -4.0984058 -4.1396265 -4.1600008 -4.1735988 -4.1646218][-4.2387457 -4.2098937 -4.1715784 -4.1367888 -4.1110654 -4.0798607 -4.0189934 -3.9690142 -3.9982171 -4.0749903 -4.1316323 -4.1506538 -4.145752 -4.1389608 -4.1273541][-4.2044635 -4.1670575 -4.140532 -4.1289663 -4.1315846 -4.1344833 -4.112886 -4.088264 -4.1002808 -4.1401272 -4.167943 -4.1589355 -4.1287432 -4.1045094 -4.0887289][-4.1714869 -4.1345215 -4.122869 -4.1307087 -4.1523366 -4.1769409 -4.1798334 -4.1723275 -4.1770959 -4.1891065 -4.1880469 -4.1615763 -4.1170006 -4.0769687 -4.0539284][-4.177073 -4.1512775 -4.14681 -4.1583319 -4.1814737 -4.2108254 -4.2204022 -4.2156496 -4.2124777 -4.2099624 -4.1943359 -4.1657877 -4.1237974 -4.0781636 -4.053586][-4.2055521 -4.191154 -4.1885929 -4.1939073 -4.2093434 -4.22912 -4.2351508 -4.2287111 -4.2190294 -4.206027 -4.18573 -4.1657977 -4.1356916 -4.0959024 -4.0751119][-4.2380843 -4.2337761 -4.2313371 -4.2255578 -4.2238235 -4.2263989 -4.2239161 -4.21767 -4.2022009 -4.1809392 -4.1644287 -4.157536 -4.1461649 -4.1190095 -4.1064115][-4.2605424 -4.2570944 -4.2505083 -4.2376242 -4.223743 -4.2115541 -4.2015266 -4.1950374 -4.1795149 -4.1560316 -4.1427922 -4.1500788 -4.1613111 -4.1539736 -4.152194]]...]
INFO - root - 2017-12-07 20:34:07.260659: step 51310, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 52h:52m:47s remains)
INFO - root - 2017-12-07 20:34:14.163320: step 51320, loss = 2.09, batch loss = 2.04 (11.9 examples/sec; 0.671 sec/batch; 52h:23m:38s remains)
INFO - root - 2017-12-07 20:34:20.985644: step 51330, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 50h:56m:19s remains)
INFO - root - 2017-12-07 20:34:27.760955: step 51340, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.701 sec/batch; 54h:45m:09s remains)
INFO - root - 2017-12-07 20:34:34.597943: step 51350, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 55h:20m:11s remains)
INFO - root - 2017-12-07 20:34:41.473105: step 51360, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 54h:45m:13s remains)
INFO - root - 2017-12-07 20:34:48.279984: step 51370, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 49h:45m:38s remains)
INFO - root - 2017-12-07 20:34:55.157533: step 51380, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 50h:13m:45s remains)
INFO - root - 2017-12-07 20:35:01.965898: step 51390, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 52h:31m:22s remains)
INFO - root - 2017-12-07 20:35:08.661818: step 51400, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 53h:30m:28s remains)
2017-12-07 20:35:09.354097: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2999582 -4.2976246 -4.2954149 -4.2922049 -4.2883039 -4.2849379 -4.2834735 -4.2860146 -4.2922573 -4.3011317 -4.3099446 -4.31489 -4.3156419 -4.3150129 -4.3154235][-4.2817297 -4.2799287 -4.2798009 -4.2763066 -4.2686329 -4.2596707 -4.254086 -4.2558184 -4.2653928 -4.2821426 -4.2991009 -4.3086219 -4.31125 -4.3111367 -4.3120131][-4.2502894 -4.2483606 -4.2494388 -4.245234 -4.2325039 -4.21435 -4.2010989 -4.200983 -4.2155738 -4.2430563 -4.2703104 -4.2856374 -4.291801 -4.2953696 -4.301116][-4.2066979 -4.2029757 -4.2022924 -4.1951113 -4.1781678 -4.1529617 -4.1318913 -4.1293054 -4.1495032 -4.1881418 -4.2257643 -4.2464027 -4.2570357 -4.2678156 -4.2806978][-4.1615472 -4.1557608 -4.1516371 -4.1423941 -4.1228871 -4.0932074 -4.0670857 -4.0625253 -4.0894742 -4.1387777 -4.1828585 -4.2066789 -4.2207637 -4.237946 -4.2573829][-4.1355228 -4.1291766 -4.1224203 -4.11049 -4.0873504 -4.0524755 -4.0218253 -4.0195823 -4.0545506 -4.1118946 -4.1592517 -4.1846538 -4.1990676 -4.2176075 -4.2397623][-4.1379361 -4.135354 -4.1307225 -4.1187358 -4.0930891 -4.0529976 -4.0193605 -4.02125 -4.0626245 -4.1234283 -4.1704144 -4.19463 -4.204226 -4.2156353 -4.234098][-4.155838 -4.161788 -4.16657 -4.1633511 -4.1436129 -4.1006622 -4.0617075 -4.0640841 -4.1062279 -4.1636043 -4.2044163 -4.2225051 -4.2252083 -4.2275143 -4.2402649][-4.1675 -4.1810155 -4.1958618 -4.2014322 -4.1884809 -4.1469946 -4.1065731 -4.1091204 -4.1503243 -4.2001042 -4.2333198 -4.2476668 -4.248 -4.2449636 -4.25511][-4.177567 -4.19371 -4.2116971 -4.218524 -4.2085171 -4.1714392 -4.1391263 -4.1478815 -4.185648 -4.2248898 -4.2532263 -4.2663403 -4.26593 -4.2611308 -4.2696962][-4.1957507 -4.2108397 -4.2254357 -4.2289071 -4.2205076 -4.19355 -4.1742077 -4.1878886 -4.2213364 -4.2525797 -4.2747669 -4.2850022 -4.2837729 -4.2778015 -4.2838902][-4.224658 -4.2365808 -4.247045 -4.2484703 -4.2414522 -4.2245016 -4.2156072 -4.2306085 -4.2586584 -4.2844582 -4.3004737 -4.3063307 -4.3035903 -4.2971177 -4.2997403][-4.264678 -4.2728949 -4.2805457 -4.2819214 -4.276895 -4.2673144 -4.2632031 -4.275682 -4.2961683 -4.3153524 -4.3257794 -4.3267879 -4.3223391 -4.3151069 -4.3137479][-4.2973442 -4.3019252 -4.3069959 -4.3092618 -4.3069348 -4.3019619 -4.2998757 -4.307374 -4.3192763 -4.3309283 -4.3361511 -4.3347778 -4.3306289 -4.3248038 -4.3218169][-4.3152013 -4.3171482 -4.3199706 -4.3221731 -4.3222914 -4.3203416 -4.3186879 -4.3213143 -4.3259859 -4.33063 -4.3326421 -4.3317471 -4.3296585 -4.3269682 -4.3246789]]...]
INFO - root - 2017-12-07 20:35:16.007108: step 51410, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 55h:10m:35s remains)
INFO - root - 2017-12-07 20:35:22.880084: step 51420, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 58h:11m:59s remains)
INFO - root - 2017-12-07 20:35:29.766837: step 51430, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 53h:06m:41s remains)
INFO - root - 2017-12-07 20:35:36.530839: step 51440, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 49h:36m:29s remains)
INFO - root - 2017-12-07 20:35:43.330192: step 51450, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 55h:19m:10s remains)
INFO - root - 2017-12-07 20:35:50.233132: step 51460, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 55h:54m:31s remains)
INFO - root - 2017-12-07 20:35:57.109656: step 51470, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 54h:17m:47s remains)
INFO - root - 2017-12-07 20:36:03.753717: step 51480, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.622 sec/batch; 48h:31m:57s remains)
INFO - root - 2017-12-07 20:36:10.539774: step 51490, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 50h:07m:33s remains)
INFO - root - 2017-12-07 20:36:17.209483: step 51500, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 54h:15m:00s remains)
2017-12-07 20:36:17.946664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1399255 -4.1603818 -4.171227 -4.1621761 -4.1553488 -4.1795521 -4.2124591 -4.237999 -4.242528 -4.2450314 -4.2556787 -4.2674618 -4.2669549 -4.2442284 -4.2059278][-4.1043196 -4.1326632 -4.14925 -4.1417422 -4.1414371 -4.1738663 -4.2088571 -4.2296429 -4.2365761 -4.2502789 -4.2732897 -4.2896705 -4.2898946 -4.2721171 -4.2464051][-4.0993414 -4.1098185 -4.1116915 -4.09753 -4.1048932 -4.151772 -4.2026334 -4.2304778 -4.2440972 -4.26451 -4.2907491 -4.3080649 -4.3074846 -4.2980542 -4.2889409][-4.1206541 -4.0952234 -4.0616655 -4.0310168 -4.0428495 -4.1054149 -4.1745496 -4.21713 -4.2403159 -4.2661729 -4.2965569 -4.3143353 -4.3122292 -4.3071613 -4.3057828][-4.1562624 -4.09417 -4.0239682 -3.9767969 -3.9886849 -4.0547996 -4.1325207 -4.1833072 -4.2103767 -4.2407537 -4.277473 -4.2996168 -4.2971311 -4.2927179 -4.2912889][-4.1891255 -4.1084461 -4.0199447 -3.9662628 -3.9744992 -4.0328841 -4.1034317 -4.1478658 -4.1728315 -4.2063031 -4.2463484 -4.2694221 -4.2674484 -4.2619195 -4.2591858][-4.2112141 -4.137361 -4.0635219 -4.0230327 -4.0258889 -4.0607252 -4.1031294 -4.1219168 -4.1310396 -4.1595078 -4.198915 -4.2268486 -4.2292194 -4.2230687 -4.2198987][-4.2352533 -4.1860943 -4.1427236 -4.1174817 -4.1131611 -4.1183524 -4.1235929 -4.1096835 -4.0925341 -4.1079531 -4.1471286 -4.1846352 -4.1978803 -4.1922 -4.1853485][-4.2590351 -4.2338796 -4.2148581 -4.2004108 -4.1910977 -4.17602 -4.1551442 -4.1238565 -4.0945306 -4.0991139 -4.1351147 -4.1720128 -4.1877389 -4.1825228 -4.1767335][-4.2620211 -4.2548237 -4.2529526 -4.2529068 -4.2503781 -4.2323122 -4.2068357 -4.1782846 -4.1510434 -4.1486421 -4.1677794 -4.186924 -4.19385 -4.1892538 -4.189424][-4.2526655 -4.258009 -4.269033 -4.2777014 -4.2781253 -4.2644534 -4.2469778 -4.2295322 -4.2116175 -4.2100821 -4.2189713 -4.2239623 -4.2216368 -4.216136 -4.2201958][-4.2410059 -4.2514319 -4.2629724 -4.26688 -4.2661824 -4.2572727 -4.2482195 -4.2438459 -4.24041 -4.2442627 -4.2527475 -4.2592387 -4.2606316 -4.2597237 -4.2631979][-4.2308545 -4.2341628 -4.2381411 -4.2378931 -4.2358394 -4.2292218 -4.2256737 -4.2312188 -4.240201 -4.2506237 -4.2613859 -4.272655 -4.283432 -4.2927313 -4.3015184][-4.2193656 -4.2143359 -4.2081437 -4.1988626 -4.1936316 -4.194221 -4.2026157 -4.2131944 -4.2215719 -4.2323761 -4.2516036 -4.2724204 -4.291677 -4.3078222 -4.3207579][-4.2122893 -4.199223 -4.1824508 -4.1640339 -4.1592245 -4.1741123 -4.197473 -4.2119727 -4.2152228 -4.2228088 -4.2459292 -4.2717166 -4.2926497 -4.30647 -4.3183193]]...]
INFO - root - 2017-12-07 20:36:24.558546: step 51510, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 49h:34m:25s remains)
INFO - root - 2017-12-07 20:36:31.242935: step 51520, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 51h:02m:54s remains)
INFO - root - 2017-12-07 20:36:37.991600: step 51530, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 52h:55m:42s remains)
INFO - root - 2017-12-07 20:36:44.799379: step 51540, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 53h:49m:10s remains)
INFO - root - 2017-12-07 20:36:51.596739: step 51550, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 52h:09m:57s remains)
INFO - root - 2017-12-07 20:36:58.376831: step 51560, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 50h:48m:01s remains)
INFO - root - 2017-12-07 20:37:05.082801: step 51570, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 50h:47m:43s remains)
INFO - root - 2017-12-07 20:37:11.932877: step 51580, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 54h:29m:01s remains)
INFO - root - 2017-12-07 20:37:18.798302: step 51590, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 56h:34m:26s remains)
INFO - root - 2017-12-07 20:37:25.646195: step 51600, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.730 sec/batch; 56h:59m:44s remains)
2017-12-07 20:37:26.396707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2861962 -4.2659259 -4.2387161 -4.2124672 -4.1999617 -4.2029891 -4.216918 -4.2317863 -4.2403417 -4.2442107 -4.24466 -4.2453632 -4.2454281 -4.2471428 -4.2512307][-4.2894926 -4.275753 -4.2548013 -4.2316022 -4.2185712 -4.2217574 -4.2374496 -4.2528791 -4.2578511 -4.2555485 -4.2486191 -4.2428865 -4.2381167 -4.2374296 -4.2400131][-4.2756424 -4.2672424 -4.2513962 -4.2320676 -4.2192726 -4.2214403 -4.2378397 -4.25587 -4.2623324 -4.2572956 -4.2463946 -4.2380791 -4.233542 -4.2330742 -4.2339125][-4.2505646 -4.2430663 -4.22867 -4.2132158 -4.203299 -4.2063565 -4.2238331 -4.2467852 -4.2583365 -4.2550364 -4.2438517 -4.2351117 -4.23155 -4.2321749 -4.2324743][-4.2205067 -4.2121949 -4.1945391 -4.1799464 -4.1743956 -4.1828718 -4.2046461 -4.2326074 -4.2489905 -4.2490315 -4.2397475 -4.2306385 -4.2281961 -4.231195 -4.2331414][-4.191205 -4.1782017 -4.1511731 -4.1288643 -4.1237822 -4.1402731 -4.1694903 -4.2020965 -4.2256684 -4.2345972 -4.2297835 -4.2211823 -4.2218695 -4.2283449 -4.233417][-4.1714735 -4.1515093 -4.1162429 -4.0863929 -4.0767674 -4.0919194 -4.1231809 -4.1612587 -4.1961102 -4.2152791 -4.2156625 -4.2075219 -4.2113643 -4.2212014 -4.2295942][-4.1593318 -4.1366644 -4.1004643 -4.0662103 -4.0475407 -4.0490351 -4.0740738 -4.1178751 -4.1654062 -4.1953926 -4.2016006 -4.1946039 -4.199554 -4.2121577 -4.2227015][-4.1646781 -4.1499171 -4.1186824 -4.0801783 -4.0432224 -4.0153403 -4.0198979 -4.0685258 -4.1327996 -4.173954 -4.184114 -4.1771393 -4.1812849 -4.1949391 -4.2078233][-4.17251 -4.1657705 -4.1408529 -4.1000853 -4.0473342 -3.9862108 -3.9623647 -4.0170889 -4.097826 -4.147964 -4.1575861 -4.1500034 -4.1528735 -4.1628847 -4.1738329][-4.16776 -4.1669564 -4.1503859 -4.1170049 -4.0619841 -3.9832463 -3.934978 -3.9810271 -4.0635653 -4.11981 -4.136487 -4.1348653 -4.134047 -4.1330748 -4.1377425][-4.1461363 -4.1553731 -4.1505423 -4.1327624 -4.0921803 -4.0260291 -3.9722948 -3.9850402 -4.0427856 -4.094624 -4.1218824 -4.12897 -4.1242065 -4.1100163 -4.1058354][-4.1255074 -4.1429038 -4.1489267 -4.1461568 -4.1233153 -4.0747709 -4.0242643 -4.00814 -4.0348573 -4.0759587 -4.1127443 -4.1298695 -4.1252551 -4.1040244 -4.0925984][-4.1114969 -4.13315 -4.1470995 -4.1522555 -4.1386232 -4.1021605 -4.0591464 -4.0286469 -4.0292811 -4.0575919 -4.0990596 -4.1287093 -4.134182 -4.1176386 -4.101707][-4.0950785 -4.1167474 -4.1385641 -4.1495938 -4.1404467 -4.1171126 -4.0872908 -4.0561476 -4.03929 -4.048347 -4.0814619 -4.1177506 -4.1345 -4.1244092 -4.1046667]]...]
INFO - root - 2017-12-07 20:37:33.064325: step 51610, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 55h:12m:45s remains)
INFO - root - 2017-12-07 20:37:39.898155: step 51620, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 56h:13m:40s remains)
INFO - root - 2017-12-07 20:37:46.688078: step 51630, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 52h:06m:04s remains)
INFO - root - 2017-12-07 20:37:53.360651: step 51640, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 50h:25m:01s remains)
INFO - root - 2017-12-07 20:38:00.174654: step 51650, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 53h:35m:36s remains)
INFO - root - 2017-12-07 20:38:06.977670: step 51660, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 56h:48m:59s remains)
INFO - root - 2017-12-07 20:38:13.737617: step 51670, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 53h:09m:28s remains)
INFO - root - 2017-12-07 20:38:20.561162: step 51680, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 50h:12m:02s remains)
INFO - root - 2017-12-07 20:38:27.384362: step 51690, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 50h:56m:54s remains)
INFO - root - 2017-12-07 20:38:34.115426: step 51700, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.715 sec/batch; 55h:45m:15s remains)
2017-12-07 20:38:34.813714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2112346 -4.1771269 -4.1371269 -4.12188 -4.115057 -4.1128607 -4.1310792 -4.1624532 -4.1715817 -4.1341109 -4.0707345 -4.0280261 -4.0390878 -4.097261 -4.1676826][-4.2185826 -4.1853704 -4.1476469 -4.1327381 -4.1156797 -4.0958791 -4.0972648 -4.1208124 -4.1339774 -4.1081066 -4.06441 -4.0374556 -4.0505376 -4.1002603 -4.1623783][-4.2264829 -4.1988378 -4.1643772 -4.1470585 -4.1201859 -4.0805798 -4.0606079 -4.076776 -4.1044645 -4.0943279 -4.0657277 -4.0541782 -4.06896 -4.1085696 -4.1613617][-4.2298145 -4.2077179 -4.1750765 -4.1547742 -4.122117 -4.065105 -4.0164752 -4.0243049 -4.0739756 -4.085557 -4.0696597 -4.0687485 -4.0859113 -4.1221032 -4.1685925][-4.23054 -4.2117891 -4.1781211 -4.1524396 -4.1129966 -4.0388041 -3.9594147 -3.95229 -4.0258155 -4.0669994 -4.0664907 -4.0762234 -4.0956335 -4.133462 -4.1766291][-4.2265062 -4.2090168 -4.17443 -4.1412234 -4.089663 -3.9926262 -3.8784869 -3.8501585 -3.9507797 -4.0296049 -4.0479746 -4.0691724 -4.0922012 -4.1302509 -4.1708035][-4.2230535 -4.2113967 -4.1802583 -4.1427908 -4.0763559 -3.9504745 -3.7998359 -3.7413824 -3.868351 -3.9851611 -4.0279131 -4.0623779 -4.0891232 -4.12414 -4.1625581][-4.2405086 -4.2320533 -4.2071652 -4.1728163 -4.1034083 -3.971446 -3.8144441 -3.7379279 -3.858707 -3.9871185 -4.0414829 -4.0831575 -4.1097655 -4.1392264 -4.1729732][-4.2740073 -4.2670131 -4.2481484 -4.2217426 -4.1643386 -4.0552192 -3.9288189 -3.8603277 -3.9457994 -4.0496159 -4.093235 -4.1299891 -4.1530738 -4.1752629 -4.201139][-4.2950292 -4.2875538 -4.2707515 -4.24903 -4.2046266 -4.1261244 -4.0374279 -3.9830511 -4.0366626 -4.11084 -4.1342411 -4.15946 -4.1793957 -4.199368 -4.2222166][-4.3053451 -4.2981253 -4.2800121 -4.2616038 -4.2277741 -4.1728597 -4.1113167 -4.0678625 -4.1013436 -4.1545076 -4.1604958 -4.1715693 -4.1862135 -4.205019 -4.228466][-4.30834 -4.3023362 -4.2851825 -4.2692327 -4.2432861 -4.2036753 -4.1608734 -4.1311483 -4.1541996 -4.1911674 -4.1837134 -4.1819658 -4.1904573 -4.2065177 -4.2284155][-4.3102822 -4.3086348 -4.2966027 -4.283258 -4.2616615 -4.2317696 -4.2022624 -4.1860023 -4.2038159 -4.2270646 -4.2117271 -4.201561 -4.2049704 -4.2176 -4.2333965][-4.3196621 -4.3212743 -4.3131509 -4.3009062 -4.2819719 -4.2582593 -4.2385058 -4.2314072 -4.2467685 -4.2623234 -4.2456021 -4.2315211 -4.2292891 -4.2365537 -4.2441254][-4.3328729 -4.3371229 -4.3306336 -4.317862 -4.3010297 -4.2816744 -4.2685671 -4.2650161 -4.2791619 -4.2910566 -4.2755833 -4.261313 -4.255178 -4.2572813 -4.2602677]]...]
INFO - root - 2017-12-07 20:38:41.306749: step 51710, loss = 2.08, batch loss = 2.02 (16.3 examples/sec; 0.492 sec/batch; 38h:23m:31s remains)
INFO - root - 2017-12-07 20:38:48.128288: step 51720, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 54h:32m:34s remains)
INFO - root - 2017-12-07 20:38:54.998605: step 51730, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.738 sec/batch; 57h:31m:51s remains)
INFO - root - 2017-12-07 20:39:01.757446: step 51740, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 53h:36m:11s remains)
INFO - root - 2017-12-07 20:39:08.528631: step 51750, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 50h:09m:49s remains)
INFO - root - 2017-12-07 20:39:15.321450: step 51760, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 49h:38m:08s remains)
INFO - root - 2017-12-07 20:39:22.085451: step 51770, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 51h:59m:44s remains)
INFO - root - 2017-12-07 20:39:28.958780: step 51780, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 56h:14m:05s remains)
INFO - root - 2017-12-07 20:39:35.805745: step 51790, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 54h:22m:59s remains)
INFO - root - 2017-12-07 20:39:42.537561: step 51800, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 50h:59m:45s remains)
2017-12-07 20:39:43.234987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2254419 -4.23843 -4.2431049 -4.2326503 -4.2219529 -4.2165885 -4.2116108 -4.2076206 -4.2037206 -4.1964407 -4.1889729 -4.1868839 -4.187171 -4.1898832 -4.2009377][-4.2261815 -4.2365017 -4.2374091 -4.2261844 -4.215683 -4.211319 -4.2051067 -4.1974983 -4.189291 -4.1795297 -4.1711731 -4.1715059 -4.175528 -4.181704 -4.1959515][-4.2229509 -4.2239723 -4.2172194 -4.2049913 -4.1951447 -4.187851 -4.1770248 -4.1666842 -4.1592026 -4.1535048 -4.1498241 -4.1561303 -4.1683912 -4.1822243 -4.1993566][-4.2152872 -4.2076817 -4.1933446 -4.1766586 -4.1596861 -4.1431842 -4.1277647 -4.1200867 -4.120697 -4.1236086 -4.1262627 -4.1373148 -4.1587272 -4.1817722 -4.20239][-4.2020092 -4.1883354 -4.1702633 -4.1462731 -4.1124892 -4.0795059 -4.0610046 -4.0659871 -4.0820608 -4.0928969 -4.1017718 -4.1208749 -4.1507277 -4.1816783 -4.2038064][-4.1834946 -4.1681037 -4.1507707 -4.1173644 -4.0583444 -3.997046 -3.9747908 -4.0055904 -4.0510931 -4.0753903 -4.088779 -4.1108494 -4.1434193 -4.17775 -4.199688][-4.1621656 -4.148972 -4.13311 -4.0898337 -4.0003419 -3.8982377 -3.8662543 -3.9362385 -4.0204592 -4.0682693 -4.0922241 -4.113142 -4.1405883 -4.1691389 -4.1863351][-4.13926 -4.1267214 -4.1174488 -4.0794539 -3.9845104 -3.8572083 -3.8103874 -3.905196 -4.0182533 -4.0847816 -4.1168346 -4.1362629 -4.153614 -4.1705432 -4.1800613][-4.1043077 -4.0878887 -4.0917382 -4.0825663 -4.0280256 -3.9422519 -3.9054754 -3.9632223 -4.0495 -4.1089826 -4.14351 -4.1674666 -4.1834421 -4.1937633 -4.1974578][-4.080039 -4.0649896 -4.0803785 -4.1002526 -4.0902882 -4.0541344 -4.0326262 -4.0439281 -4.0809488 -4.1199284 -4.1551695 -4.1871405 -4.2101674 -4.2226944 -4.2247763][-4.1000557 -4.0869689 -4.0984073 -4.1220803 -4.1336079 -4.12465 -4.1114225 -4.0984678 -4.1060591 -4.1366553 -4.1725516 -4.204751 -4.2274632 -4.2376432 -4.2381372][-4.1525612 -4.134068 -4.1295524 -4.1436176 -4.1614084 -4.1685896 -4.1638966 -4.1476502 -4.1444182 -4.1709628 -4.2063646 -4.2317333 -4.2435565 -4.24058 -4.233129][-4.2003145 -4.1771832 -4.1583977 -4.1574512 -4.17116 -4.186554 -4.1971116 -4.1934762 -4.193491 -4.2139683 -4.24136 -4.25707 -4.2533517 -4.23509 -4.2187042][-4.2183261 -4.194706 -4.1733022 -4.1612267 -4.1686468 -4.1884408 -4.2099133 -4.220911 -4.2291012 -4.2461362 -4.2655077 -4.2727971 -4.258194 -4.2293434 -4.2062764][-4.2281837 -4.2044444 -4.183485 -4.1699162 -4.1750379 -4.197485 -4.2216907 -4.2358127 -4.2433419 -4.2554584 -4.266829 -4.2684259 -4.2539511 -4.2278757 -4.2037153]]...]
INFO - root - 2017-12-07 20:39:49.913983: step 51810, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 57h:34m:03s remains)
INFO - root - 2017-12-07 20:39:56.702767: step 51820, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 54h:17m:42s remains)
INFO - root - 2017-12-07 20:40:03.474086: step 51830, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 49h:45m:37s remains)
INFO - root - 2017-12-07 20:40:10.293910: step 51840, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 51h:01m:59s remains)
INFO - root - 2017-12-07 20:40:17.091165: step 51850, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 54h:15m:58s remains)
INFO - root - 2017-12-07 20:40:24.005474: step 51860, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 55h:21m:45s remains)
INFO - root - 2017-12-07 20:40:30.803633: step 51870, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 53h:31m:57s remains)
INFO - root - 2017-12-07 20:40:37.564915: step 51880, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 50h:52m:59s remains)
INFO - root - 2017-12-07 20:40:44.443546: step 51890, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 53h:47m:42s remains)
INFO - root - 2017-12-07 20:40:51.273186: step 51900, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 55h:55m:32s remains)
2017-12-07 20:40:52.030480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2464919 -4.2562156 -4.2702756 -4.2820005 -4.2917504 -4.2981009 -4.2936068 -4.2832956 -4.2810206 -4.2762508 -4.2647338 -4.2537704 -4.2445374 -4.2444806 -4.2537246][-4.1987252 -4.2118559 -4.2292337 -4.245666 -4.2607732 -4.2699547 -4.2660313 -4.2549806 -4.2559032 -4.2547951 -4.2449841 -4.2345839 -4.2272272 -4.2305374 -4.24309][-4.1543908 -4.1660318 -4.1790028 -4.1946473 -4.2108178 -4.2233 -4.2236524 -4.2152691 -4.2250814 -4.2317476 -4.22776 -4.222115 -4.2190266 -4.2271729 -4.2408361][-4.1422019 -4.1475039 -4.1558342 -4.16787 -4.1832323 -4.1969829 -4.1970396 -4.1886578 -4.2040806 -4.219636 -4.2191062 -4.2177157 -4.2178392 -4.2284508 -4.242703][-4.12979 -4.1291223 -4.1381712 -4.1485834 -4.1624627 -4.1745157 -4.1656232 -4.1514587 -4.1760063 -4.2058506 -4.2097626 -4.213737 -4.2192 -4.2304993 -4.2448635][-4.1000915 -4.0888987 -4.0902486 -4.0920992 -4.0932903 -4.0932412 -4.06731 -4.0407672 -4.0778112 -4.1306181 -4.1528311 -4.1739569 -4.19583 -4.2176514 -4.2358866][-4.0079503 -3.9970958 -3.993449 -3.9880097 -3.980226 -3.9645224 -3.9122221 -3.8662708 -3.9167428 -4.0011706 -4.0550432 -4.1043782 -4.151391 -4.1927161 -4.2224884][-3.9296637 -3.9158733 -3.8996482 -3.8831582 -3.8621693 -3.8273849 -3.7535799 -3.6962104 -3.763797 -3.8822474 -3.9720604 -4.0502443 -4.1172009 -4.1742091 -4.2142467][-3.9953392 -3.9843504 -3.9566016 -3.9214058 -3.8861609 -3.8382294 -3.7626896 -3.7122722 -3.7700479 -3.8808455 -3.9681845 -4.0446234 -4.1113453 -4.1683555 -4.2111039][-4.110095 -4.1072693 -4.0835862 -4.0497003 -4.023066 -3.9906044 -3.936461 -3.9048433 -3.9468296 -4.0185909 -4.0655956 -4.1067829 -4.148932 -4.187079 -4.21938][-4.1995134 -4.1993837 -4.1850929 -4.1608305 -4.1414762 -4.1177478 -4.0787249 -4.0569863 -4.084157 -4.1263757 -4.1460223 -4.1619978 -4.1835356 -4.20755 -4.2299485][-4.2623458 -4.2622538 -4.2547483 -4.2381468 -4.2218733 -4.2019873 -4.1713333 -4.1544628 -4.1700377 -4.1934161 -4.2027278 -4.2078667 -4.2179761 -4.2326641 -4.2457542][-4.31015 -4.3133492 -4.3098164 -4.3012114 -4.2939377 -4.2851281 -4.2641749 -4.2531805 -4.2598538 -4.2700887 -4.2728324 -4.2714758 -4.2743096 -4.276823 -4.2779231][-4.3084393 -4.311512 -4.3139129 -4.3135223 -4.3137569 -4.313446 -4.3038039 -4.2997413 -4.3090763 -4.31831 -4.3184905 -4.3162642 -4.3141003 -4.3095803 -4.3039823][-4.2992764 -4.29922 -4.3010387 -4.301424 -4.3023391 -4.3021288 -4.2943058 -4.2917681 -4.3047624 -4.3210168 -4.3278546 -4.332366 -4.3340421 -4.3285751 -4.3212948]]...]
INFO - root - 2017-12-07 20:40:58.636916: step 51910, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 49h:43m:09s remains)
INFO - root - 2017-12-07 20:41:05.352044: step 51920, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 54h:06m:13s remains)
INFO - root - 2017-12-07 20:41:12.155625: step 51930, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 54h:08m:54s remains)
INFO - root - 2017-12-07 20:41:18.968072: step 51940, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 53h:44m:29s remains)
INFO - root - 2017-12-07 20:41:25.805242: step 51950, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 52h:03m:02s remains)
INFO - root - 2017-12-07 20:41:32.682471: step 51960, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 51h:17m:38s remains)
INFO - root - 2017-12-07 20:41:39.553642: step 51970, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 54h:32m:59s remains)
INFO - root - 2017-12-07 20:41:46.377828: step 51980, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 55h:45m:28s remains)
INFO - root - 2017-12-07 20:41:53.083307: step 51990, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 53h:06m:05s remains)
INFO - root - 2017-12-07 20:41:59.830649: step 52000, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 49h:13m:57s remains)
2017-12-07 20:42:00.544073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163171 -4.3218966 -4.3250165 -4.323369 -4.3162241 -4.3069549 -4.2977281 -4.2912416 -4.2894888 -4.294867 -4.30547 -4.3157067 -4.3204956 -4.3163338 -4.3112431][-4.3042111 -4.3098764 -4.3105655 -4.3028026 -4.2892575 -4.27553 -4.2619524 -4.2519951 -4.2495189 -4.2580752 -4.2748823 -4.2927475 -4.3046265 -4.3027568 -4.29738][-4.2992826 -4.3028355 -4.2970104 -4.2804942 -4.2604012 -4.2425876 -4.22881 -4.21784 -4.2158279 -4.2287664 -4.2526884 -4.2767749 -4.2937112 -4.2928967 -4.2872272][-4.3006153 -4.2995682 -4.286232 -4.2596755 -4.2297955 -4.2059593 -4.1919913 -4.1826453 -4.1826119 -4.2017684 -4.2348781 -4.265729 -4.2856483 -4.285502 -4.27898][-4.2960234 -4.2911878 -4.26748 -4.2231879 -4.1708322 -4.129118 -4.1054273 -4.0917034 -4.0954752 -4.1335812 -4.1882672 -4.2354741 -4.265254 -4.2728372 -4.2694955][-4.2847915 -4.2750983 -4.2423925 -4.1790786 -4.0970411 -4.0229254 -3.9730506 -3.9412034 -3.9438674 -4.0126576 -4.1095538 -4.187449 -4.2361975 -4.2589474 -4.2630033][-4.2726345 -4.2612696 -4.2294612 -4.165462 -4.0711284 -3.96785 -3.8775651 -3.8104048 -3.8038354 -3.8992195 -4.0374627 -4.1469464 -4.2152014 -4.2542949 -4.2678962][-4.2555451 -4.2469635 -4.2298813 -4.1914082 -4.1211562 -4.0204091 -3.9101639 -3.8186727 -3.7969527 -3.8858089 -4.0287361 -4.1451545 -4.2171245 -4.2597518 -4.2774234][-4.2334719 -4.2308779 -4.2344155 -4.2297058 -4.2007031 -4.1345186 -4.0402827 -3.9499509 -3.9160361 -3.9729221 -4.0833364 -4.1757259 -4.231225 -4.2624331 -4.2759285][-4.2091856 -4.2146339 -4.2347045 -4.2548361 -4.2537956 -4.2211576 -4.1558895 -4.0868711 -4.0525179 -4.0792747 -4.1505527 -4.2096806 -4.2388873 -4.2524734 -4.2587676][-4.1708989 -4.1837773 -4.2179112 -4.2531834 -4.2696519 -4.26292 -4.2279716 -4.1845889 -4.1574931 -4.1635609 -4.1999412 -4.2305393 -4.2406135 -4.240952 -4.2426114][-4.0784545 -4.1013403 -4.1616054 -4.2205572 -4.2538261 -4.2658348 -4.2571974 -4.2422404 -4.2317514 -4.2285514 -4.2379746 -4.2463179 -4.2422876 -4.2345819 -4.2333121][-3.944365 -3.970432 -4.0662379 -4.1607523 -4.2146 -4.245728 -4.2605538 -4.271956 -4.2817187 -4.2823672 -4.2762012 -4.2641139 -4.2450156 -4.2300291 -4.226696][-3.8223059 -3.8499587 -3.9793792 -4.1102304 -4.1875806 -4.2329278 -4.2582908 -4.2790704 -4.3007154 -4.3111849 -4.3037796 -4.2825804 -4.2548614 -4.234952 -4.2291][-3.8234463 -3.8567395 -3.9830692 -4.11412 -4.1923571 -4.2358122 -4.2595739 -4.277575 -4.301024 -4.318573 -4.3185067 -4.3032303 -4.2771277 -4.25246 -4.2399144]]...]
INFO - root - 2017-12-07 20:42:07.285625: step 52010, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 55h:20m:07s remains)
INFO - root - 2017-12-07 20:42:13.999551: step 52020, loss = 2.05, batch loss = 1.99 (14.1 examples/sec; 0.568 sec/batch; 44h:16m:53s remains)
INFO - root - 2017-12-07 20:42:20.652680: step 52030, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 50h:18m:25s remains)
INFO - root - 2017-12-07 20:42:27.387804: step 52040, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 54h:15m:02s remains)
INFO - root - 2017-12-07 20:42:34.037302: step 52050, loss = 2.11, batch loss = 2.05 (11.8 examples/sec; 0.677 sec/batch; 52h:44m:57s remains)
INFO - root - 2017-12-07 20:42:40.816493: step 52060, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 58h:01m:58s remains)
INFO - root - 2017-12-07 20:42:47.513219: step 52070, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 50h:50m:02s remains)
INFO - root - 2017-12-07 20:42:54.214381: step 52080, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 49h:45m:16s remains)
INFO - root - 2017-12-07 20:43:00.956170: step 52090, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 53h:14m:35s remains)
INFO - root - 2017-12-07 20:43:07.704057: step 52100, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 52h:21m:05s remains)
2017-12-07 20:43:08.505416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2088985 -4.2083549 -4.2099562 -4.209661 -4.2067666 -4.2064714 -4.209765 -4.2036948 -4.1853213 -4.1658278 -4.1562757 -4.1566648 -4.1710787 -4.1979141 -4.2173691][-4.1854911 -4.1861291 -4.1936827 -4.1984749 -4.1965361 -4.1973867 -4.2011061 -4.1942611 -4.1769819 -4.1618748 -4.1529627 -4.1489463 -4.1628075 -4.193656 -4.2143588][-4.18349 -4.1797671 -4.1846652 -4.1871815 -4.18698 -4.1919832 -4.198 -4.1927695 -4.1800261 -4.1751475 -4.1739292 -4.170577 -4.17988 -4.2096972 -4.22971][-4.2018247 -4.192441 -4.1943641 -4.1964383 -4.1966209 -4.19925 -4.2003822 -4.1943207 -4.1873732 -4.1960688 -4.2084341 -4.2082644 -4.2113609 -4.2321978 -4.2449031][-4.2424445 -4.2293582 -4.226429 -4.224154 -4.2210722 -4.2138543 -4.196105 -4.1727343 -4.1666889 -4.1946926 -4.2272115 -4.2375221 -4.2433267 -4.2541738 -4.2579975][-4.27858 -4.2595787 -4.2466221 -4.2374387 -4.2251987 -4.1982617 -4.1502638 -4.0932336 -4.0833855 -4.1437888 -4.2040524 -4.2325554 -4.2504692 -4.261023 -4.2624831][-4.2872066 -4.2605643 -4.2412825 -4.2229743 -4.1986017 -4.1485276 -4.0594888 -3.951129 -3.9316895 -4.0437016 -4.1466489 -4.2005491 -4.2329559 -4.2519212 -4.25714][-4.2601428 -4.2243209 -4.2003694 -4.1794882 -4.1505117 -4.092134 -3.9814363 -3.8354006 -3.8124721 -3.9753695 -4.1123719 -4.1794467 -4.2188525 -4.2412133 -4.2457252][-4.2089076 -4.1687126 -4.150816 -4.1450272 -4.1336765 -4.093935 -4.0050554 -3.8904295 -3.8814363 -4.0132952 -4.12111 -4.1735716 -4.2064657 -4.2226934 -4.2208576][-4.1786537 -4.1439219 -4.1414623 -4.1563382 -4.167479 -4.1546273 -4.0989943 -4.0294375 -4.0261111 -4.0945463 -4.1497626 -4.1759877 -4.19952 -4.2100825 -4.2046685][-4.1835952 -4.1576915 -4.1634746 -4.1852098 -4.2039242 -4.2020268 -4.1682692 -4.1331348 -4.1343555 -4.1656461 -4.189919 -4.2012219 -4.2155991 -4.2221584 -4.2211361][-4.2085266 -4.1886878 -4.1907005 -4.210969 -4.2281451 -4.2240829 -4.20469 -4.1945148 -4.2045059 -4.2231574 -4.2352581 -4.242569 -4.2494707 -4.2550006 -4.2605333][-4.2386265 -4.2264071 -4.22719 -4.2436056 -4.2559752 -4.2478828 -4.2371922 -4.2426963 -4.2586918 -4.27028 -4.2743464 -4.2762241 -4.2759728 -4.2844081 -4.2955561][-4.2682881 -4.26257 -4.2618856 -4.275434 -4.2855039 -4.2769241 -4.2721534 -4.28373 -4.2981009 -4.3024049 -4.2996902 -4.2923164 -4.279633 -4.2837467 -4.2967134][-4.292829 -4.2871795 -4.2801838 -4.2876487 -4.2954736 -4.2921872 -4.2929435 -4.3032155 -4.3123569 -4.3129683 -4.3076763 -4.2935224 -4.2726631 -4.2714376 -4.28375]]...]
INFO - root - 2017-12-07 20:43:15.052477: step 52110, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 50h:40m:37s remains)
INFO - root - 2017-12-07 20:43:21.912835: step 52120, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.724 sec/batch; 56h:22m:49s remains)
INFO - root - 2017-12-07 20:43:28.745865: step 52130, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 57h:07m:11s remains)
INFO - root - 2017-12-07 20:43:35.667289: step 52140, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 54h:18m:09s remains)
INFO - root - 2017-12-07 20:43:42.493718: step 52150, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 49h:43m:57s remains)
INFO - root - 2017-12-07 20:43:49.367968: step 52160, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 54h:09m:39s remains)
INFO - root - 2017-12-07 20:43:56.248193: step 52170, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 53h:52m:28s remains)
INFO - root - 2017-12-07 20:44:03.015611: step 52180, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 52h:26m:26s remains)
INFO - root - 2017-12-07 20:44:09.803508: step 52190, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 49h:29m:25s remains)
INFO - root - 2017-12-07 20:44:16.650270: step 52200, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 52h:09m:41s remains)
2017-12-07 20:44:17.407038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2724881 -4.2736497 -4.2727213 -4.2675195 -4.26141 -4.2561145 -4.253468 -4.2537193 -4.255764 -4.2581553 -4.2597084 -4.2616715 -4.2635164 -4.2646561 -4.2673492][-4.2765 -4.2750616 -4.2721686 -4.2631893 -4.2512717 -4.2388124 -4.2308869 -4.2273841 -4.2275953 -4.2319117 -4.2376876 -4.2451372 -4.2512207 -4.2554526 -4.2626696][-4.282711 -4.2768726 -4.2691622 -4.2534928 -4.2314286 -4.20709 -4.18978 -4.17981 -4.1803985 -4.1935072 -4.2123036 -4.2314487 -4.2439079 -4.2507229 -4.2614946][-4.2802191 -4.2725463 -4.2582803 -4.230938 -4.193717 -4.1535578 -4.1231046 -4.10813 -4.1174355 -4.1487346 -4.1842084 -4.2155995 -4.2367344 -4.24789 -4.2612171][-4.26289 -4.2532892 -4.2299557 -4.1904716 -4.1371756 -4.0781751 -4.0288849 -4.0105 -4.0396709 -4.0946846 -4.1471491 -4.1866884 -4.2123375 -4.2275081 -4.2442222][-4.23281 -4.2213492 -4.1881642 -4.1331778 -4.0594788 -3.9736056 -3.8984306 -3.8812509 -3.9420505 -4.0293994 -4.0983291 -4.1412807 -4.169117 -4.1867595 -4.2044826][-4.189857 -4.1754422 -4.1300941 -4.0552015 -3.9557042 -3.833406 -3.7284269 -3.7216439 -3.8255179 -3.9469604 -4.0305753 -4.0788908 -4.1112967 -4.1317525 -4.145638][-4.1569543 -4.1402512 -4.0844693 -3.9919789 -3.8705034 -3.7261186 -3.6150577 -3.6346531 -3.7658968 -3.8987968 -3.9889567 -4.041245 -4.07654 -4.0964141 -4.1021085][-4.1426606 -4.1287107 -4.076241 -3.987541 -3.8777544 -3.76588 -3.7042394 -3.7418885 -3.8479581 -3.9506419 -4.02523 -4.0705152 -4.0986805 -4.1083345 -4.1004953][-4.1498108 -4.1427546 -4.1070824 -4.0466948 -3.9762404 -3.9177098 -3.9024074 -3.9353652 -3.9971628 -4.0609689 -4.1111631 -4.1384025 -4.1490173 -4.1429858 -4.121912][-4.1750331 -4.1743269 -4.1591825 -4.1308327 -4.0940895 -4.0712256 -4.0759745 -4.0938869 -4.1220603 -4.1574268 -4.1870041 -4.1966686 -4.1915736 -4.1732073 -4.1467338][-4.2115326 -4.2134013 -4.2106075 -4.2008333 -4.1828208 -4.1735697 -4.1780419 -4.1840258 -4.1951094 -4.2116213 -4.2251558 -4.2250996 -4.21565 -4.1970086 -4.1734619][-4.2528591 -4.2497807 -4.2481112 -4.243659 -4.2338915 -4.2294745 -4.22986 -4.2315383 -4.2375903 -4.2441697 -4.2495351 -4.2485762 -4.2431388 -4.2318306 -4.2145267][-4.284759 -4.2775984 -4.27129 -4.2645454 -4.2576733 -4.25486 -4.2529116 -4.2514691 -4.2526364 -4.2544427 -4.2590203 -4.2638106 -4.2670951 -4.2636738 -4.2515335][-4.3052349 -4.2967529 -4.2876778 -4.2783318 -4.2696295 -4.2647505 -4.2610579 -4.2584596 -4.260006 -4.2630591 -4.2687631 -4.2782736 -4.2865562 -4.2865486 -4.2766318]]...]
INFO - root - 2017-12-07 20:44:24.015270: step 52210, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 55h:03m:28s remains)
INFO - root - 2017-12-07 20:44:30.848026: step 52220, loss = 2.09, batch loss = 2.04 (12.1 examples/sec; 0.662 sec/batch; 51h:30m:37s remains)
INFO - root - 2017-12-07 20:44:37.711946: step 52230, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 51h:23m:02s remains)
INFO - root - 2017-12-07 20:44:44.520078: step 52240, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 53h:22m:18s remains)
INFO - root - 2017-12-07 20:44:51.298662: step 52250, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 53h:07m:20s remains)
INFO - root - 2017-12-07 20:44:58.061172: step 52260, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 51h:44m:54s remains)
INFO - root - 2017-12-07 20:45:04.748779: step 52270, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 49h:48m:06s remains)
INFO - root - 2017-12-07 20:45:11.510235: step 52280, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 51h:37m:32s remains)
INFO - root - 2017-12-07 20:45:18.311979: step 52290, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 56h:21m:05s remains)
INFO - root - 2017-12-07 20:45:25.116716: step 52300, loss = 2.09, batch loss = 2.04 (11.0 examples/sec; 0.730 sec/batch; 56h:49m:44s remains)
2017-12-07 20:45:25.783034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2881522 -4.2902541 -4.2906103 -4.2891393 -4.2808681 -4.2713623 -4.2603197 -4.2448068 -4.2226334 -4.2111845 -4.2226667 -4.2335706 -4.2401257 -4.2397971 -4.2350426][-4.2685871 -4.2710981 -4.2735405 -4.2699852 -4.2550941 -4.2431579 -4.2303214 -4.2117753 -4.1822634 -4.1685858 -4.1851125 -4.2005196 -4.2073069 -4.2045279 -4.196775][-4.2545033 -4.2544632 -4.2560029 -4.249332 -4.2278881 -4.211165 -4.1958957 -4.1730156 -4.1410923 -4.1289034 -4.146987 -4.1641192 -4.1725364 -4.173944 -4.1668744][-4.2428174 -4.2410307 -4.2411408 -4.2298245 -4.2019262 -4.1759095 -4.1521525 -4.1236072 -4.0901809 -4.0759635 -4.0930042 -4.1115456 -4.1270719 -4.13608 -4.1339245][-4.2255864 -4.2249165 -4.2218947 -4.2028284 -4.1677575 -4.1341677 -4.1020708 -4.0698781 -4.0415292 -4.0325766 -4.0513425 -4.0694432 -4.0889912 -4.1043882 -4.1072731][-4.193109 -4.1978903 -4.1926179 -4.1625385 -4.120533 -4.0815673 -4.0473695 -4.0130396 -3.9908187 -3.9960566 -4.0256972 -4.0517955 -4.0754933 -4.0923414 -4.0974336][-4.1549187 -4.1718807 -4.1746955 -4.138896 -4.0889053 -4.0411286 -4.0060568 -3.9651279 -3.9360576 -3.9621451 -4.0134659 -4.0539527 -4.0862865 -4.1032815 -4.1049371][-4.1179938 -4.147037 -4.1622815 -4.1260424 -4.0798163 -4.0304646 -3.9848766 -3.9157135 -3.8591158 -3.9015458 -3.9850471 -4.0441508 -4.0922718 -4.1115251 -4.1147642][-4.0993886 -4.125988 -4.1425495 -4.1181192 -4.0855279 -4.0415068 -3.9932268 -3.911855 -3.8455083 -3.8923993 -3.9809558 -4.0439835 -4.0958147 -4.1154113 -4.1242504][-4.1192265 -4.1385503 -4.1508107 -4.1321063 -4.1069012 -4.0713015 -4.0360227 -3.9799998 -3.9376426 -3.9706404 -4.0303106 -4.0712142 -4.106565 -4.1206269 -4.1303558][-4.1786551 -4.1890359 -4.1955781 -4.1838536 -4.164515 -4.1318426 -4.102006 -4.06149 -4.030056 -4.0530677 -4.0957379 -4.1209755 -4.1442761 -4.1527805 -4.1558042][-4.2359281 -4.2388983 -4.2382503 -4.2317524 -4.2224035 -4.2008991 -4.1771183 -4.1444788 -4.1152325 -4.1343083 -4.1705465 -4.1896296 -4.207211 -4.2112541 -4.2058268][-4.2714639 -4.2722054 -4.2693944 -4.2664261 -4.2633986 -4.2539272 -4.2410769 -4.2216778 -4.1977124 -4.2047763 -4.2281365 -4.2456126 -4.2598696 -4.2654252 -4.2607212][-4.2819529 -4.2817173 -4.2802634 -4.2819867 -4.2839589 -4.2836027 -4.2807035 -4.2741323 -4.2598557 -4.2591348 -4.2706661 -4.281846 -4.2908788 -4.2951303 -4.292625][-4.2764468 -4.2756786 -4.2753448 -4.2782412 -4.2832627 -4.2870259 -4.2882509 -4.2895231 -4.2850595 -4.2834296 -4.290297 -4.2965822 -4.3010869 -4.3017964 -4.2995195]]...]
INFO - root - 2017-12-07 20:45:32.370174: step 52310, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 52h:55m:15s remains)
INFO - root - 2017-12-07 20:45:39.220114: step 52320, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 55h:05m:39s remains)
INFO - root - 2017-12-07 20:45:46.003212: step 52330, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 54h:53m:35s remains)
INFO - root - 2017-12-07 20:45:52.496712: step 52340, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.621 sec/batch; 48h:17m:34s remains)
INFO - root - 2017-12-07 20:45:59.355927: step 52350, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 54h:25m:22s remains)
INFO - root - 2017-12-07 20:46:06.280303: step 52360, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 55h:33m:22s remains)
INFO - root - 2017-12-07 20:46:13.161271: step 52370, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 53h:47m:07s remains)
INFO - root - 2017-12-07 20:46:19.904821: step 52380, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 52h:39m:04s remains)
INFO - root - 2017-12-07 20:46:26.697341: step 52390, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.675 sec/batch; 52h:33m:23s remains)
INFO - root - 2017-12-07 20:46:33.545829: step 52400, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 49h:41m:28s remains)
2017-12-07 20:46:34.341506: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2950974 -4.3031077 -4.3089948 -4.3119378 -4.3101583 -4.3029938 -4.2865038 -4.2700076 -4.2610717 -4.2534924 -4.2388306 -4.2055416 -4.156333 -4.1042948 -4.0706258][-4.2868795 -4.2930069 -4.2966905 -4.2980995 -4.2927604 -4.2775311 -4.2564549 -4.2424688 -4.2381845 -4.2366915 -4.2292171 -4.2022367 -4.158926 -4.1148033 -4.0914211][-4.2774196 -4.2762237 -4.2759171 -4.2735004 -4.2566442 -4.2285161 -4.2051449 -4.197216 -4.2022748 -4.2076392 -4.2060709 -4.1853309 -4.1539197 -4.1262407 -4.1183543][-4.2610331 -4.2518039 -4.2460065 -4.2389493 -4.2089229 -4.170321 -4.1458807 -4.1464152 -4.1624303 -4.1753769 -4.1772075 -4.1554527 -4.1311088 -4.1225176 -4.1342196][-4.2397618 -4.2283807 -4.2215233 -4.2108669 -4.1703682 -4.1241007 -4.0979438 -4.1052361 -4.1351814 -4.1592059 -4.1609483 -4.129353 -4.0964465 -4.0968351 -4.1244645][-4.2149439 -4.2097635 -4.205461 -4.1892047 -4.1382713 -4.0725145 -4.0279341 -4.0393581 -4.0955915 -4.1421776 -4.1501136 -4.1087713 -4.0654292 -4.0659666 -4.098042][-4.1878705 -4.1916184 -4.1902819 -4.1667891 -4.1028204 -4.0031853 -3.9183772 -3.9310246 -4.0323157 -4.1140556 -4.1326823 -4.0908809 -4.0463324 -4.0444655 -4.0683436][-4.1594543 -4.1679311 -4.1685658 -4.1429968 -4.0723643 -3.9434292 -3.8094077 -3.814158 -3.9564462 -4.0693512 -4.0987067 -4.071506 -4.04403 -4.0446177 -4.059104][-4.1387105 -4.1474357 -4.1468945 -4.1253948 -4.0692534 -3.957854 -3.8328068 -3.835145 -3.9576142 -4.0605068 -4.090661 -4.08065 -4.0732164 -4.0825458 -4.0952048][-4.1398859 -4.1510968 -4.1504464 -4.1378145 -4.1091313 -4.0475469 -3.9830031 -3.9884117 -4.0548177 -4.1115541 -4.1267171 -4.1204548 -4.12162 -4.1363106 -4.1530414][-4.1681557 -4.1841192 -4.1879511 -4.1847572 -4.1746583 -4.1533914 -4.1328115 -4.142396 -4.1721764 -4.1951113 -4.1948371 -4.1841226 -4.1830006 -4.1957593 -4.2125626][-4.2115908 -4.2320323 -4.2416968 -4.2456183 -4.2436967 -4.2372861 -4.2337446 -4.2428546 -4.2568493 -4.267242 -4.2648177 -4.2538595 -4.2474294 -4.2518721 -4.2639651][-4.2568517 -4.2764745 -4.289443 -4.2945457 -4.2906785 -4.28518 -4.2862282 -4.2948074 -4.3050084 -4.3144526 -4.3151741 -4.3073978 -4.2980785 -4.2948537 -4.3009734][-4.2904873 -4.3077984 -4.3204508 -4.3212004 -4.3096337 -4.299118 -4.299118 -4.3030405 -4.3096709 -4.3155851 -4.316371 -4.313447 -4.308332 -4.3072248 -4.3118553][-4.3006406 -4.3158083 -4.3231096 -4.3172231 -4.2996483 -4.2847629 -4.2781734 -4.2731657 -4.2701993 -4.2702847 -4.2695727 -4.2710376 -4.2744083 -4.27845 -4.2818646]]...]
INFO - root - 2017-12-07 20:46:40.952455: step 52410, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 53h:03m:15s remains)
INFO - root - 2017-12-07 20:46:47.650754: step 52420, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 50h:06m:26s remains)
INFO - root - 2017-12-07 20:46:54.541341: step 52430, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 53h:16m:41s remains)
INFO - root - 2017-12-07 20:47:01.368280: step 52440, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 54h:45m:07s remains)
INFO - root - 2017-12-07 20:47:08.165638: step 52450, loss = 2.06, batch loss = 2.01 (10.8 examples/sec; 0.744 sec/batch; 57h:52m:30s remains)
INFO - root - 2017-12-07 20:47:14.977053: step 52460, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 51h:27m:52s remains)
INFO - root - 2017-12-07 20:47:21.684415: step 52470, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 51h:54m:18s remains)
INFO - root - 2017-12-07 20:47:28.500917: step 52480, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 53h:04m:38s remains)
INFO - root - 2017-12-07 20:47:35.418496: step 52490, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 56h:36m:30s remains)
INFO - root - 2017-12-07 20:47:42.188121: step 52500, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 54h:28m:04s remains)
2017-12-07 20:47:42.949956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3065286 -4.3020182 -4.3050289 -4.3110294 -4.3178082 -4.3163376 -4.2994509 -4.2679057 -4.2350335 -4.2067533 -4.1871519 -4.1833191 -4.1902981 -4.205564 -4.2262568][-4.3122826 -4.3137441 -4.3186841 -4.3224993 -4.3200588 -4.3041945 -4.271718 -4.2270279 -4.1901298 -4.1646748 -4.1491814 -4.153461 -4.1678772 -4.18371 -4.2015834][-4.3135939 -4.3211026 -4.327198 -4.32594 -4.3124156 -4.2825432 -4.2372136 -4.1844087 -4.146771 -4.1232171 -4.1149025 -4.1328907 -4.1572094 -4.1737022 -4.1866989][-4.3072119 -4.3203149 -4.3285537 -4.3232012 -4.3019977 -4.2634349 -4.2109232 -4.1552544 -4.1189184 -4.0981865 -4.0981236 -4.13126 -4.1636953 -4.1797733 -4.1886115][-4.2876139 -4.3052945 -4.317513 -4.31176 -4.2859192 -4.2444973 -4.1920767 -4.1420021 -4.1110382 -4.0964828 -4.1076808 -4.15572 -4.1957674 -4.212091 -4.2202039][-4.2576413 -4.2765274 -4.2928963 -4.2891827 -4.2624354 -4.2221003 -4.1746454 -4.1352978 -4.1144094 -4.1084371 -4.1301012 -4.1859307 -4.2289953 -4.2447114 -4.252892][-4.2327023 -4.2469277 -4.2612815 -4.2571869 -4.2302551 -4.1915989 -4.1502852 -4.1196089 -4.1038861 -4.0973935 -4.1226544 -4.179348 -4.2199583 -4.2345462 -4.2464504][-4.2278333 -4.232935 -4.2402616 -4.2311978 -4.2018876 -4.1622539 -4.124929 -4.0980954 -4.0797062 -4.0655022 -4.0905342 -4.1450448 -4.1820602 -4.1973276 -4.2151327][-4.2470913 -4.2442465 -4.2449722 -4.2311158 -4.1996694 -4.1595016 -4.1240425 -4.0987196 -4.0758247 -4.0582538 -4.0828867 -4.1288223 -4.1581321 -4.1727009 -4.1922131][-4.2773452 -4.2705445 -4.2683835 -4.250886 -4.2187858 -4.1811008 -4.1490645 -4.1257091 -4.10426 -4.0949073 -4.1203294 -4.153738 -4.1716814 -4.183764 -4.2025328][-4.2972302 -4.289382 -4.2874022 -4.269927 -4.2402592 -4.20832 -4.1810446 -4.1626372 -4.1492581 -4.1522555 -4.1772323 -4.1994705 -4.208148 -4.2156477 -4.23032][-4.2967758 -4.2898197 -4.2904153 -4.2776217 -4.2556553 -4.23488 -4.2173219 -4.2096505 -4.2083187 -4.22026 -4.2403522 -4.2546778 -4.2560272 -4.2543297 -4.2578759][-4.2792039 -4.2712607 -4.2748165 -4.2715855 -4.2622223 -4.2550063 -4.24871 -4.2517147 -4.259294 -4.2747369 -4.2888131 -4.2982769 -4.2977142 -4.2889547 -4.2811041][-4.2467742 -4.2358685 -4.2436452 -4.253665 -4.2583756 -4.2623491 -4.2644992 -4.2721672 -4.2811837 -4.2940483 -4.3041372 -4.3101969 -4.3088861 -4.298274 -4.2860503][-4.2084861 -4.1942196 -4.2061706 -4.2288356 -4.2463756 -4.2574015 -4.2628832 -4.269063 -4.2750993 -4.2846708 -4.2949619 -4.3011861 -4.2985191 -4.2843637 -4.2670779]]...]
INFO - root - 2017-12-07 20:47:49.574048: step 52510, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 55h:50m:44s remains)
INFO - root - 2017-12-07 20:47:56.446039: step 52520, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 54h:17m:40s remains)
INFO - root - 2017-12-07 20:48:03.256734: step 52530, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 54h:48m:01s remains)
INFO - root - 2017-12-07 20:48:09.989979: step 52540, loss = 2.04, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 49h:07m:56s remains)
INFO - root - 2017-12-07 20:48:16.823684: step 52550, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 52h:27m:21s remains)
INFO - root - 2017-12-07 20:48:23.652110: step 52560, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 53h:17m:19s remains)
INFO - root - 2017-12-07 20:48:30.545588: step 52570, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 52h:50m:42s remains)
INFO - root - 2017-12-07 20:48:37.369339: step 52580, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 49h:50m:57s remains)
INFO - root - 2017-12-07 20:48:44.148695: step 52590, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.653 sec/batch; 50h:45m:55s remains)
INFO - root - 2017-12-07 20:48:50.957431: step 52600, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 53h:59m:31s remains)
2017-12-07 20:48:51.840947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2974725 -4.2986107 -4.2917981 -4.2752013 -4.2564297 -4.2478585 -4.2514181 -4.2596021 -4.2650418 -4.2653122 -4.2606792 -4.26637 -4.2883782 -4.3095613 -4.3184915][-4.2922034 -4.2921276 -4.284945 -4.2683506 -4.2514348 -4.2452259 -4.2511306 -4.2614012 -4.2692032 -4.2718 -4.2672844 -4.2715211 -4.2892 -4.3049703 -4.3099842][-4.2908225 -4.2907319 -4.283597 -4.2689667 -4.2551045 -4.2510772 -4.256187 -4.2656136 -4.2740703 -4.2765188 -4.2723656 -4.2755504 -4.2880225 -4.2971196 -4.2980032][-4.2898397 -4.2921662 -4.2873297 -4.2765293 -4.2655153 -4.2612162 -4.2616229 -4.2660704 -4.2715483 -4.2740455 -4.2727785 -4.2764874 -4.2852745 -4.2897553 -4.2884145][-4.2895231 -4.2967072 -4.294888 -4.2863774 -4.274725 -4.2655697 -4.2553468 -4.2485275 -4.2503209 -4.2577848 -4.2644858 -4.2727623 -4.281981 -4.2856054 -4.2842617][-4.2908416 -4.3037462 -4.3042369 -4.2947 -4.2770548 -4.2572575 -4.23129 -4.2130685 -4.2133737 -4.229774 -4.2484274 -4.2640386 -4.277153 -4.282876 -4.2835388][-4.2912149 -4.3080711 -4.3113017 -4.29948 -4.272202 -4.2385225 -4.1989188 -4.1767874 -4.182044 -4.2069888 -4.2343106 -4.2553015 -4.2718377 -4.2804713 -4.2838326][-4.2894325 -4.3079967 -4.31414 -4.3011889 -4.2674994 -4.2232146 -4.1771178 -4.1554251 -4.1650877 -4.1923714 -4.22184 -4.2456322 -4.2648978 -4.2769656 -4.283742][-4.288517 -4.3051891 -4.3113608 -4.2981386 -4.2642469 -4.2213688 -4.1784649 -4.158906 -4.1646028 -4.184608 -4.2077312 -4.2314157 -4.2555614 -4.2723441 -4.2824492][-4.2926493 -4.3052964 -4.3088531 -4.2970133 -4.2675934 -4.2333455 -4.2001462 -4.184433 -4.1815443 -4.1869292 -4.1973033 -4.2176704 -4.2451077 -4.2673316 -4.2808542][-4.3020768 -4.3104029 -4.3098207 -4.2992535 -4.2760267 -4.2509289 -4.2287478 -4.2175403 -4.2094936 -4.2022147 -4.199482 -4.2119422 -4.2373347 -4.2630696 -4.2786322][-4.3090849 -4.3144426 -4.3087935 -4.2951069 -4.2757058 -4.2571974 -4.2448287 -4.2406297 -4.2328548 -4.2178903 -4.2030048 -4.20661 -4.2302556 -4.2598553 -4.2778091][-4.312943 -4.3147559 -4.3029127 -4.2833042 -4.2627587 -4.2474108 -4.2425842 -4.2439809 -4.2380834 -4.2208543 -4.2024994 -4.2053328 -4.2311654 -4.2630286 -4.2805452][-4.315649 -4.3124862 -4.2940774 -4.2690129 -4.247685 -4.2366748 -4.2372346 -4.2417541 -4.2373223 -4.2222009 -4.2062492 -4.2110934 -4.2381024 -4.2689533 -4.2839417][-4.315321 -4.3094163 -4.2868729 -4.2587442 -4.2377849 -4.2304044 -4.2356224 -4.2418785 -4.2380042 -4.2257457 -4.2129068 -4.2173405 -4.2409072 -4.26907 -4.2815723]]...]
INFO - root - 2017-12-07 20:48:58.576319: step 52610, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 52h:00m:24s remains)
INFO - root - 2017-12-07 20:49:05.384801: step 52620, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 52h:36m:15s remains)
INFO - root - 2017-12-07 20:49:12.177273: step 52630, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.734 sec/batch; 57h:02m:04s remains)
INFO - root - 2017-12-07 20:49:19.059988: step 52640, loss = 2.07, batch loss = 2.01 (13.2 examples/sec; 0.607 sec/batch; 47h:11m:07s remains)
INFO - root - 2017-12-07 20:49:25.787075: step 52650, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 51h:29m:38s remains)
INFO - root - 2017-12-07 20:49:32.663195: step 52660, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 53h:16m:21s remains)
INFO - root - 2017-12-07 20:49:39.425599: step 52670, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 53h:35m:11s remains)
INFO - root - 2017-12-07 20:49:46.336139: step 52680, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 51h:20m:15s remains)
INFO - root - 2017-12-07 20:49:53.134942: step 52690, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 50h:58m:23s remains)
INFO - root - 2017-12-07 20:49:59.882945: step 52700, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 52h:58m:55s remains)
2017-12-07 20:50:00.610328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21769 -4.2147756 -4.2291117 -4.2377162 -4.2344737 -4.2335286 -4.2319651 -4.2231312 -4.2222281 -4.2237687 -4.2074656 -4.1917195 -4.1971912 -4.22187 -4.2450395][-4.1613293 -4.1600227 -4.1808968 -4.1953526 -4.19664 -4.2021694 -4.2046967 -4.1922297 -4.1851444 -4.1837249 -4.1620541 -4.1448712 -4.1537747 -4.1878619 -4.2180142][-4.1272054 -4.1206646 -4.14227 -4.1627021 -4.17187 -4.1821227 -4.1854286 -4.1672883 -4.1569161 -4.1558871 -4.1404891 -4.1300025 -4.139143 -4.1708307 -4.199][-4.1207719 -4.1103907 -4.1209855 -4.1332521 -4.1438708 -4.1569633 -4.1569095 -4.1324167 -4.1220675 -4.128828 -4.1288552 -4.1345148 -4.1469727 -4.1637821 -4.1794386][-4.1259012 -4.1213732 -4.1239829 -4.1211643 -4.1208258 -4.1241775 -4.110055 -4.0749731 -4.0694571 -4.0984812 -4.1245036 -4.1470256 -4.1623507 -4.1679645 -4.170651][-4.1344795 -4.1421618 -4.1456 -4.1260233 -4.1067705 -4.0913258 -4.0503488 -3.996139 -3.9987795 -4.0602007 -4.1175046 -4.1530046 -4.16879 -4.1702442 -4.1695547][-4.1360765 -4.1511369 -4.1612492 -4.1362615 -4.0965734 -4.0565562 -3.9870353 -3.90388 -3.9121058 -4.0083284 -4.0927663 -4.1391926 -4.1601572 -4.1669922 -4.1703882][-4.1458712 -4.1604972 -4.1759963 -4.1568346 -4.1141872 -4.054635 -3.9601607 -3.8533864 -3.8578558 -3.9704459 -4.0709834 -4.1259656 -4.1556182 -4.1718907 -4.1825194][-4.1705523 -4.1857576 -4.2045803 -4.1945558 -4.1620259 -4.1076717 -4.0206566 -3.9240308 -3.9122782 -3.9942396 -4.0787578 -4.1262159 -4.1584935 -4.1794043 -4.1907721][-4.1950397 -4.2095189 -4.2284236 -4.2255778 -4.20335 -4.1628661 -4.1063604 -4.0447216 -4.0303946 -4.0705853 -4.1207638 -4.1483412 -4.1704912 -4.181706 -4.187398][-4.21497 -4.2255125 -4.2394958 -4.2386074 -4.22221 -4.1966071 -4.1653624 -4.134984 -4.1289692 -4.1470242 -4.1718154 -4.18651 -4.1966395 -4.1951413 -4.1902938][-4.2361321 -4.244668 -4.2540545 -4.2472429 -4.22863 -4.2134447 -4.203485 -4.192934 -4.1907754 -4.2063832 -4.2220759 -4.232513 -4.2390728 -4.2323766 -4.2169623][-4.2596979 -4.264502 -4.2680926 -4.2584658 -4.2400184 -4.2309709 -4.2329187 -4.2313867 -4.2308297 -4.2484488 -4.2636523 -4.2736387 -4.2793775 -4.2740092 -4.2554359][-4.2718883 -4.2735314 -4.2743177 -4.2641487 -4.2471585 -4.2410641 -4.246274 -4.24814 -4.2519231 -4.27152 -4.2885079 -4.2981548 -4.305954 -4.3039093 -4.2860823][-4.2792439 -4.2780371 -4.2772374 -4.2688732 -4.2542362 -4.2474327 -4.253953 -4.2586322 -4.2646055 -4.2824392 -4.29724 -4.3048177 -4.31133 -4.3107157 -4.29677]]...]
INFO - root - 2017-12-07 20:50:07.211549: step 52710, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 50h:39m:22s remains)
INFO - root - 2017-12-07 20:50:14.097375: step 52720, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 51h:20m:23s remains)
INFO - root - 2017-12-07 20:50:20.904616: step 52730, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.670 sec/batch; 52h:02m:17s remains)
INFO - root - 2017-12-07 20:50:27.720182: step 52740, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 54h:29m:32s remains)
INFO - root - 2017-12-07 20:50:34.614461: step 52750, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 54h:15m:19s remains)
INFO - root - 2017-12-07 20:50:41.492485: step 52760, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 49h:04m:00s remains)
INFO - root - 2017-12-07 20:50:48.276657: step 52770, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 50h:34m:38s remains)
INFO - root - 2017-12-07 20:50:55.119220: step 52780, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 57h:13m:24s remains)
INFO - root - 2017-12-07 20:51:01.950983: step 52790, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.744 sec/batch; 57h:47m:36s remains)
INFO - root - 2017-12-07 20:51:08.884436: step 52800, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 50h:12m:33s remains)
2017-12-07 20:51:09.732755: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22115 -4.2275591 -4.2368155 -4.2455997 -4.25265 -4.25348 -4.2531281 -4.2549438 -4.2584724 -4.2647309 -4.2706885 -4.2751813 -4.2775121 -4.2779245 -4.2759447][-4.2357197 -4.2353516 -4.2412882 -4.2483983 -4.2559309 -4.2588296 -4.2609506 -4.2661033 -4.27343 -4.281229 -4.2874722 -4.290668 -4.2910123 -4.2902775 -4.2889628][-4.2400961 -4.2354913 -4.239398 -4.2468343 -4.2562485 -4.2618027 -4.2666006 -4.2742748 -4.282742 -4.2901449 -4.29539 -4.2974272 -4.2969122 -4.2963905 -4.29743][-4.2061052 -4.2015123 -4.2055764 -4.2114277 -4.2192817 -4.2266612 -4.2368207 -4.251061 -4.2624736 -4.2692938 -4.2720909 -4.2715878 -4.2718186 -4.2746878 -4.281311][-4.1361084 -4.1335373 -4.1384673 -4.1377063 -4.133389 -4.1355562 -4.1506982 -4.1768718 -4.1990561 -4.2106943 -4.2141409 -4.2138228 -4.216084 -4.223124 -4.2362103][-4.084445 -4.0804443 -4.0790505 -4.0598783 -4.0265083 -4.0071278 -4.0229459 -4.066761 -4.10897 -4.1333289 -4.14298 -4.1464705 -4.1512818 -4.1590505 -4.1733828][-4.089653 -4.0967131 -4.0916247 -4.0564375 -3.9914129 -3.9325109 -3.9274924 -3.9784567 -4.0382738 -4.0778756 -4.0971174 -4.1053615 -4.1105967 -4.1141615 -4.122005][-4.1349015 -4.1553707 -4.1586733 -4.1282682 -4.0651579 -3.996294 -3.9662974 -3.9933515 -4.0455313 -4.0872397 -4.1110868 -4.1218596 -4.1263 -4.1240697 -4.1226635][-4.1799741 -4.2099285 -4.2229719 -4.2045565 -4.1584444 -4.1022081 -4.0711479 -4.0796237 -4.1115322 -4.1434731 -4.16631 -4.1765084 -4.1794076 -4.1749649 -4.1703229][-4.1985292 -4.2319474 -4.2498245 -4.241724 -4.2132363 -4.1793752 -4.1643729 -4.1701975 -4.1888375 -4.2059751 -4.2179551 -4.2241983 -4.2278113 -4.2259426 -4.2245088][-4.2107973 -4.2359171 -4.2461047 -4.2362556 -4.2153907 -4.2000847 -4.2058449 -4.2240467 -4.2437167 -4.2560205 -4.2599845 -4.2611122 -4.2647171 -4.2704363 -4.277813][-4.2237825 -4.2425756 -4.2399526 -4.2208881 -4.1978354 -4.1895804 -4.2058058 -4.2358093 -4.2644458 -4.2807746 -4.2850356 -4.2822819 -4.2821865 -4.2918048 -4.3076129][-4.239037 -4.2581673 -4.2506895 -4.2248712 -4.1951594 -4.1780491 -4.1900115 -4.2213311 -4.2549357 -4.2787685 -4.2880244 -4.2858458 -4.2804027 -4.2865825 -4.3052382][-4.2461982 -4.2721519 -4.2686772 -4.243784 -4.2096572 -4.1814137 -4.180243 -4.198432 -4.2234321 -4.2459621 -4.2601376 -4.2620878 -4.2556338 -4.2589831 -4.2776523][-4.2453647 -4.2778912 -4.283814 -4.2632217 -4.2292728 -4.1975718 -4.1879711 -4.1937032 -4.2042789 -4.2161613 -4.2255497 -4.226593 -4.2202997 -4.2221303 -4.2396064]]...]
INFO - root - 2017-12-07 20:51:16.363998: step 52810, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 56h:25m:07s remains)
INFO - root - 2017-12-07 20:51:23.185420: step 52820, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 53h:36m:42s remains)
INFO - root - 2017-12-07 20:51:29.910343: step 52830, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 50h:19m:04s remains)
INFO - root - 2017-12-07 20:51:36.681368: step 52840, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 51h:06m:42s remains)
INFO - root - 2017-12-07 20:51:43.587713: step 52850, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 54h:11m:23s remains)
INFO - root - 2017-12-07 20:51:50.465555: step 52860, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 53h:20m:40s remains)
INFO - root - 2017-12-07 20:51:57.315506: step 52870, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 48h:39m:08s remains)
INFO - root - 2017-12-07 20:52:04.125063: step 52880, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 50h:51m:40s remains)
INFO - root - 2017-12-07 20:52:10.932051: step 52890, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 56h:33m:47s remains)
INFO - root - 2017-12-07 20:52:17.769750: step 52900, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.743 sec/batch; 57h:42m:19s remains)
2017-12-07 20:52:18.500683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2855372 -4.2896357 -4.2829766 -4.2649436 -4.244215 -4.2156348 -4.1897655 -4.1784744 -4.1915684 -4.2236276 -4.248414 -4.2548785 -4.2569847 -4.2619596 -4.2641373][-4.2820067 -4.28452 -4.2729254 -4.2452617 -4.2203846 -4.1850805 -4.1502833 -4.1305323 -4.1358314 -4.1658783 -4.1917996 -4.2044182 -4.2170091 -4.22901 -4.2357974][-4.2817254 -4.2833009 -4.2641449 -4.2257595 -4.1901679 -4.1425257 -4.0972481 -4.0692325 -4.068027 -4.093348 -4.1204052 -4.1401606 -4.1651282 -4.1869459 -4.1990108][-4.2867641 -4.289639 -4.2651896 -4.2226853 -4.1757612 -4.1116185 -4.0530353 -4.0109386 -4.00031 -4.0210838 -4.0541906 -4.0849466 -4.120471 -4.1478353 -4.1661224][-4.2947206 -4.3001328 -4.2776871 -4.2364345 -4.1758761 -4.0915227 -4.0126958 -3.9463992 -3.9320638 -3.9687803 -4.0177555 -4.0619521 -4.1017485 -4.1304026 -4.1517][-4.3038168 -4.3113956 -4.291297 -4.2461624 -4.1689272 -4.0612173 -3.9588773 -3.8691306 -3.8623362 -3.9346032 -4.0164161 -4.0776882 -4.1144166 -4.1358566 -4.1533031][-4.3097644 -4.3164887 -4.2959776 -4.2377844 -4.1429734 -4.0158992 -3.8967147 -3.8034396 -3.8263583 -3.9409418 -4.0468259 -4.1142335 -4.1441364 -4.1547165 -4.1614032][-4.311626 -4.3136396 -4.2886839 -4.2223911 -4.1224627 -3.9928823 -3.8700621 -3.7926364 -3.8593841 -3.9980042 -4.1079245 -4.1667638 -4.18506 -4.1784205 -4.1654797][-4.3089705 -4.3074 -4.2780852 -4.2106652 -4.1195507 -4.0124454 -3.9075913 -3.8633041 -3.9526577 -4.0816803 -4.1709218 -4.2099624 -4.2132683 -4.1892281 -4.1535268][-4.3073382 -4.3050752 -4.2741332 -4.2115369 -4.1367054 -4.0598564 -3.9839644 -3.9727995 -4.06135 -4.1592989 -4.217823 -4.235527 -4.2212882 -4.1815629 -4.1271076][-4.3083439 -4.30546 -4.2770023 -4.222137 -4.1628857 -4.1079817 -4.0554113 -4.0650911 -4.1391516 -4.2048144 -4.236412 -4.2367172 -4.2063971 -4.1534667 -4.0895848][-4.3116241 -4.3095722 -4.2869215 -4.2395382 -4.1917629 -4.1492743 -4.1103621 -4.12101 -4.1737208 -4.214354 -4.2299762 -4.2223372 -4.187408 -4.1351962 -4.0734844][-4.3167124 -4.3173666 -4.2978168 -4.2531848 -4.2120976 -4.1776142 -4.1452923 -4.1464267 -4.1787696 -4.20458 -4.2167077 -4.2120032 -4.1831985 -4.1389928 -4.0843439][-4.3219538 -4.3247929 -4.3048353 -4.260601 -4.2198319 -4.1848855 -4.151947 -4.1403136 -4.1591644 -4.179215 -4.192811 -4.1942134 -4.1753092 -4.1394715 -4.0906119][-4.3269868 -4.3306251 -4.3103909 -4.26931 -4.2254372 -4.1791987 -4.1344376 -4.1047068 -4.1137567 -4.1348071 -4.1537395 -4.1659532 -4.1553984 -4.1246209 -4.0774]]...]
INFO - root - 2017-12-07 20:52:25.058068: step 52910, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 52h:50m:25s remains)
INFO - root - 2017-12-07 20:52:31.883077: step 52920, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 55h:58m:31s remains)
INFO - root - 2017-12-07 20:52:38.788707: step 52930, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 55h:37m:04s remains)
INFO - root - 2017-12-07 20:52:45.636277: step 52940, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 53h:56m:26s remains)
INFO - root - 2017-12-07 20:52:52.358508: step 52950, loss = 2.08, batch loss = 2.02 (15.7 examples/sec; 0.510 sec/batch; 39h:34m:28s remains)
INFO - root - 2017-12-07 20:52:59.171195: step 52960, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 55h:46m:06s remains)
INFO - root - 2017-12-07 20:53:05.968203: step 52970, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 53h:32m:02s remains)
INFO - root - 2017-12-07 20:53:12.818681: step 52980, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 55h:49m:01s remains)
INFO - root - 2017-12-07 20:53:19.632152: step 52990, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 49h:07m:21s remains)
INFO - root - 2017-12-07 20:53:26.306695: step 53000, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 50h:56m:33s remains)
2017-12-07 20:53:27.167697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3273969 -4.3253403 -4.3260293 -4.3272028 -4.3262434 -4.3232026 -4.3184891 -4.3150568 -4.3118172 -4.3072977 -4.303648 -4.3044758 -4.3112841 -4.3223686 -4.3350496][-4.3032932 -4.301177 -4.3039136 -4.308104 -4.3103213 -4.309134 -4.3048196 -4.3007474 -4.2934837 -4.2818637 -4.2714462 -4.26778 -4.2733092 -4.2898397 -4.3121614][-4.2794857 -4.2765183 -4.2809825 -4.2882752 -4.2953906 -4.2977066 -4.2940431 -4.2888384 -4.2770729 -4.2582388 -4.2424226 -4.2344422 -4.2364469 -4.2547116 -4.2874384][-4.2643127 -4.2598929 -4.2624 -4.2694778 -4.2809634 -4.2868619 -4.2853231 -4.2798581 -4.2653193 -4.2421169 -4.2222762 -4.2109561 -4.2086864 -4.2248173 -4.2643156][-4.251451 -4.2462182 -4.2434793 -4.2463064 -4.2602553 -4.2705874 -4.2740064 -4.2704911 -4.2535849 -4.2263188 -4.2018094 -4.1898489 -4.1873732 -4.201704 -4.2430372][-4.2289033 -4.2188644 -4.2064648 -4.2042627 -4.2211447 -4.2383385 -4.24581 -4.241137 -4.2180867 -4.1839943 -4.1544032 -4.1434059 -4.150857 -4.1739378 -4.2209764][-4.2023082 -4.183938 -4.1626415 -4.157021 -4.1755953 -4.1996622 -4.2113466 -4.2059207 -4.17805 -4.1358118 -4.1008172 -4.0929704 -4.1126842 -4.1474586 -4.2011542][-4.1815991 -4.1566133 -4.1285419 -4.1192193 -4.1376386 -4.1653652 -4.1831493 -4.184082 -4.16182 -4.1207724 -4.0840282 -4.0759325 -4.0981503 -4.1356621 -4.1899023][-4.1813779 -4.1594687 -4.1320863 -4.1188073 -4.1319156 -4.156044 -4.1742873 -4.1821294 -4.1714978 -4.13906 -4.1032028 -4.0912185 -4.10756 -4.1401324 -4.1901646][-4.2001181 -4.1888833 -4.1701317 -4.1571693 -4.164804 -4.1846313 -4.2010365 -4.2121239 -4.2087712 -4.1850185 -4.15046 -4.1331553 -4.1409111 -4.1640863 -4.2045665][-4.2334647 -4.232935 -4.22239 -4.2078691 -4.2080812 -4.223722 -4.2394443 -4.250608 -4.2503123 -4.232028 -4.2024684 -4.1821604 -4.1811748 -4.1955228 -4.2276621][-4.2589035 -4.2598877 -4.254621 -4.2396288 -4.2363839 -4.2511678 -4.2670369 -4.2766652 -4.2780213 -4.2660389 -4.2445908 -4.2242875 -4.2183232 -4.2266726 -4.2516427][-4.2881737 -4.2885613 -4.2864118 -4.2772846 -4.2745066 -4.2850957 -4.2956204 -4.3023605 -4.3045425 -4.2981749 -4.2848582 -4.268311 -4.2608395 -4.2637806 -4.2790613][-4.2991371 -4.2983265 -4.3004808 -4.2991214 -4.2976203 -4.3026123 -4.3093982 -4.3143773 -4.3171043 -4.3161979 -4.310287 -4.2996545 -4.2936864 -4.29549 -4.304801][-4.2940426 -4.2951336 -4.2984724 -4.30075 -4.3001552 -4.3014874 -4.3062463 -4.3105712 -4.3137836 -4.3169484 -4.318254 -4.3155189 -4.3140106 -4.3182631 -4.32714]]...]
INFO - root - 2017-12-07 20:53:33.676084: step 53010, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 53h:05m:33s remains)
INFO - root - 2017-12-07 20:53:40.434393: step 53020, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 50h:32m:04s remains)
INFO - root - 2017-12-07 20:53:47.181530: step 53030, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 49h:16m:04s remains)
INFO - root - 2017-12-07 20:53:53.970216: step 53040, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 53h:11m:14s remains)
INFO - root - 2017-12-07 20:54:00.615811: step 53050, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 54h:45m:50s remains)
INFO - root - 2017-12-07 20:54:07.271360: step 53060, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 50h:35m:53s remains)
INFO - root - 2017-12-07 20:54:13.993919: step 53070, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 49h:27m:47s remains)
INFO - root - 2017-12-07 20:54:20.847075: step 53080, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 51h:38m:40s remains)
INFO - root - 2017-12-07 20:54:27.639649: step 53090, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 53h:38m:18s remains)
INFO - root - 2017-12-07 20:54:34.548785: step 53100, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 52h:59m:34s remains)
2017-12-07 20:54:35.245376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.15668 -4.1319551 -4.1242127 -4.1515174 -4.183001 -4.1904569 -4.1803112 -4.1709733 -4.1672087 -4.1710539 -4.1807537 -4.1901836 -4.1916561 -4.1726823 -4.1414533][-4.159029 -4.1453805 -4.1387377 -4.1592908 -4.1906819 -4.2061415 -4.2033134 -4.1926188 -4.1842036 -4.1872554 -4.19823 -4.20894 -4.2123346 -4.1984234 -4.176177][-4.1564693 -4.1557307 -4.1506109 -4.1599245 -4.1866183 -4.2080541 -4.2142887 -4.2084684 -4.2009268 -4.2021127 -4.20992 -4.2177815 -4.2199121 -4.2112684 -4.1996317][-4.1467357 -4.15111 -4.1405778 -4.1392264 -4.1613712 -4.1834474 -4.1958995 -4.1975164 -4.1928887 -4.1930614 -4.1974869 -4.2028594 -4.2025123 -4.1966424 -4.1929159][-4.1380181 -4.1325727 -4.1074648 -4.0982733 -4.1186004 -4.1401892 -4.1560421 -4.16276 -4.1595011 -4.15785 -4.1616306 -4.1669183 -4.1642766 -4.1578107 -4.1575975][-4.129396 -4.1083617 -4.0703454 -4.058857 -4.0755615 -4.0937757 -4.1133204 -4.1250281 -4.1208291 -4.1145105 -4.1189833 -4.1312189 -4.1319227 -4.1257524 -4.1264868][-4.1183128 -4.0904427 -4.0562415 -4.0503478 -4.0638576 -4.0763855 -4.0912943 -4.0979147 -4.0853925 -4.0699873 -4.0778742 -4.0996008 -4.1097913 -4.1085563 -4.1117687][-4.0991559 -4.0811329 -4.0629735 -4.0615587 -4.0694475 -4.0771646 -4.0798936 -4.0715523 -4.05229 -4.039463 -4.0567551 -4.0906916 -4.114583 -4.1224155 -4.127409][-4.075295 -4.0707436 -4.0661726 -4.0654836 -4.0655756 -4.0690131 -4.0652952 -4.051352 -4.0362215 -4.0381503 -4.0655193 -4.1067195 -4.1386242 -4.1512671 -4.1547294][-4.0544624 -4.0556664 -4.055408 -4.0544138 -4.0506516 -4.0523462 -4.0517635 -4.042634 -4.0365424 -4.0513926 -4.0884132 -4.131865 -4.1620636 -4.1700678 -4.1682067][-4.0525985 -4.0473404 -4.0357203 -4.0298867 -4.0247602 -4.029212 -4.0373683 -4.0356169 -4.0374384 -4.0607243 -4.1043696 -4.14265 -4.1634693 -4.1648693 -4.1609097][-4.0804286 -4.0632234 -4.03717 -4.0304928 -4.0318103 -4.0401559 -4.0482621 -4.0436749 -4.0463591 -4.0701356 -4.1086559 -4.1314526 -4.137145 -4.1347718 -4.1381526][-4.1089964 -4.0851984 -4.058557 -4.0615726 -4.0772815 -4.0897374 -4.0876508 -4.0702481 -4.068234 -4.08384 -4.1057296 -4.108839 -4.0980043 -4.0948277 -4.1102686][-4.1238456 -4.0937719 -4.0675492 -4.073709 -4.0941586 -4.1069994 -4.1002746 -4.0816402 -4.0785861 -4.0859442 -4.0916572 -4.0799093 -4.0657706 -4.072103 -4.0994792][-4.1329713 -4.1020923 -4.0713449 -4.066267 -4.0793924 -4.0929556 -4.0871239 -4.0720072 -4.0667267 -4.0690064 -4.0661793 -4.0513096 -4.0487027 -4.0715313 -4.1040258]]...]
INFO - root - 2017-12-07 20:54:41.897138: step 53110, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 50h:53m:14s remains)
INFO - root - 2017-12-07 20:54:48.709935: step 53120, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 54h:12m:23s remains)
INFO - root - 2017-12-07 20:54:55.547706: step 53130, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.699 sec/batch; 54h:14m:53s remains)
INFO - root - 2017-12-07 20:55:02.285117: step 53140, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 50h:52m:57s remains)
INFO - root - 2017-12-07 20:55:09.091933: step 53150, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 50h:40m:08s remains)
INFO - root - 2017-12-07 20:55:15.904865: step 53160, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 49h:38m:56s remains)
INFO - root - 2017-12-07 20:55:22.689643: step 53170, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.727 sec/batch; 56h:23m:13s remains)
INFO - root - 2017-12-07 20:55:29.557540: step 53180, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 54h:42m:23s remains)
INFO - root - 2017-12-07 20:55:36.342897: step 53190, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 50h:16m:26s remains)
INFO - root - 2017-12-07 20:55:43.250130: step 53200, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 50h:01m:07s remains)
2017-12-07 20:55:43.964706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2778769 -4.2444835 -4.23199 -4.2348228 -4.2380328 -4.241858 -4.2410736 -4.2337952 -4.2205248 -4.2052622 -4.2040696 -4.2225347 -4.2442293 -4.2569952 -4.2659311][-4.2314458 -4.1889515 -4.1792145 -4.2003574 -4.2279229 -4.2472749 -4.2549167 -4.24763 -4.2289872 -4.2131605 -4.2147894 -4.2355351 -4.2569833 -4.269752 -4.2749066][-4.1557512 -4.1043973 -4.1012545 -4.1477556 -4.2053413 -4.2450719 -4.2673478 -4.2721753 -4.2622595 -4.2566447 -4.26503 -4.2790852 -4.2903814 -4.2942 -4.2894912][-4.08712 -4.0256753 -4.0274644 -4.0963593 -4.1836782 -4.2416811 -4.2714572 -4.2841783 -4.2852292 -4.291646 -4.3069611 -4.3157859 -4.3169093 -4.3115387 -4.296741][-4.0709639 -4.01589 -4.0244355 -4.0973988 -4.1871614 -4.2384839 -4.2530551 -4.2596664 -4.2682276 -4.2899723 -4.3153977 -4.3244014 -4.3217425 -4.3127403 -4.2946334][-4.132925 -4.1004152 -4.108901 -4.156724 -4.2079492 -4.2209082 -4.2016554 -4.1932979 -4.2105875 -4.2525353 -4.2944717 -4.312408 -4.3128905 -4.3042254 -4.2875195][-4.2135658 -4.197638 -4.2015657 -4.21319 -4.2081909 -4.1658664 -4.0981417 -4.0691156 -4.1031523 -4.1763 -4.247324 -4.2843509 -4.2951918 -4.2906513 -4.2773647][-4.2513018 -4.2353339 -4.2331009 -4.2175603 -4.169302 -4.06788 -3.938664 -3.8917277 -3.9654517 -4.0881243 -4.1951303 -4.2563868 -4.2821636 -4.2822566 -4.271709][-4.2554688 -4.2388706 -4.2300973 -4.2022095 -4.1351542 -4.0065522 -3.8490279 -3.8047886 -3.9200702 -4.07273 -4.1899118 -4.25634 -4.2837987 -4.2801776 -4.2687359][-4.2330565 -4.2205906 -4.216495 -4.1986747 -4.153017 -4.0593138 -3.9553394 -3.9375119 -4.03623 -4.1579895 -4.2440133 -4.2880187 -4.2991309 -4.28632 -4.2696786][-4.1984358 -4.1933103 -4.2025909 -4.2088847 -4.197628 -4.1554174 -4.1116433 -4.114625 -4.1755257 -4.2482529 -4.2951355 -4.31224 -4.3066068 -4.2896504 -4.2708511][-4.1809368 -4.1818156 -4.2060637 -4.2291374 -4.2378926 -4.2241411 -4.2056637 -4.2047749 -4.2294183 -4.2646265 -4.2876697 -4.2954483 -4.2904344 -4.2819991 -4.2720428][-4.1913881 -4.1953025 -4.2237873 -4.2464275 -4.2566347 -4.2524028 -4.2350807 -4.2149458 -4.2082934 -4.2187719 -4.2347131 -4.2482195 -4.2578 -4.2676072 -4.2723918][-4.2137337 -4.22436 -4.2481809 -4.2580328 -4.2607961 -4.2549844 -4.2288346 -4.1866641 -4.1561775 -4.1486735 -4.166399 -4.196991 -4.2281218 -4.2569566 -4.2735462][-4.2231679 -4.2336855 -4.2483721 -4.243896 -4.2377396 -4.2323303 -4.203495 -4.1482177 -4.0984421 -4.0783739 -4.1041074 -4.1569681 -4.2112784 -4.2553444 -4.2796421]]...]
INFO - root - 2017-12-07 20:55:50.632334: step 53210, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 55h:10m:09s remains)
INFO - root - 2017-12-07 20:55:57.497179: step 53220, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 50h:48m:06s remains)
INFO - root - 2017-12-07 20:56:04.313438: step 53230, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 49h:34m:06s remains)
INFO - root - 2017-12-07 20:56:11.091923: step 53240, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 49h:39m:04s remains)
INFO - root - 2017-12-07 20:56:17.928379: step 53250, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 56h:18m:42s remains)
INFO - root - 2017-12-07 20:56:24.786940: step 53260, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 56h:28m:39s remains)
INFO - root - 2017-12-07 20:56:31.328889: step 53270, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 49h:47m:50s remains)
INFO - root - 2017-12-07 20:56:38.320989: step 53280, loss = 2.11, batch loss = 2.05 (11.9 examples/sec; 0.671 sec/batch; 52h:02m:29s remains)
INFO - root - 2017-12-07 20:56:45.170863: step 53290, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 54h:51m:46s remains)
INFO - root - 2017-12-07 20:56:51.965041: step 53300, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 54h:44m:49s remains)
2017-12-07 20:56:52.665418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1584949 -4.1653175 -4.2013664 -4.2198367 -4.204813 -4.1724725 -4.1465549 -4.1292233 -4.1269846 -4.1550751 -4.2029281 -4.2434268 -4.2730789 -4.287776 -4.2942152][-4.1657906 -4.1678081 -4.1966591 -4.2007289 -4.1739817 -4.14509 -4.1343703 -4.1259561 -4.1214657 -4.1389651 -4.1847081 -4.2323461 -4.2685704 -4.2845922 -4.28847][-4.1805291 -4.1798296 -4.1953497 -4.1869874 -4.1490536 -4.1234627 -4.1313639 -4.1409826 -4.1433449 -4.1511555 -4.1885471 -4.2380366 -4.2784777 -4.2921553 -4.2902966][-4.200211 -4.2005835 -4.2084308 -4.1927409 -4.1469617 -4.116652 -4.12744 -4.1496906 -4.1637683 -4.1732373 -4.2108154 -4.2629333 -4.3010483 -4.309073 -4.3007774][-4.2175732 -4.2220712 -4.2251744 -4.202404 -4.1464086 -4.1008821 -4.096993 -4.119555 -4.1459818 -4.167779 -4.2093043 -4.2670736 -4.3093271 -4.3190818 -4.3113413][-4.2325134 -4.23928 -4.2343731 -4.1977758 -4.1194239 -4.040247 -4.0056381 -4.0277953 -4.0803475 -4.1328068 -4.186748 -4.2498136 -4.2966719 -4.3138051 -4.3133268][-4.2384577 -4.2483015 -4.24019 -4.1882825 -4.0817623 -3.9574509 -3.8762436 -3.8928597 -3.9802482 -4.0757513 -4.154223 -4.2256222 -4.2758846 -4.2997937 -4.3070688][-4.2403984 -4.2511425 -4.2453256 -4.1933331 -4.0827527 -3.9382105 -3.8205981 -3.8120904 -3.910094 -4.031569 -4.1313095 -4.2106576 -4.2638216 -4.2908406 -4.3032908][-4.2577806 -4.2645354 -4.2638726 -4.2307396 -4.1494293 -4.0393047 -3.9463408 -3.9267442 -3.9869797 -4.081109 -4.16645 -4.2345924 -4.2783918 -4.2973404 -4.3057022][-4.2694578 -4.2702675 -4.2747173 -4.2619796 -4.2169518 -4.1507688 -4.0940804 -4.0754981 -4.1041112 -4.1623807 -4.2238965 -4.2741237 -4.3054051 -4.3119116 -4.3123307][-4.2685542 -4.2639933 -4.2691364 -4.2702675 -4.251235 -4.2145109 -4.1774592 -4.1557455 -4.1676369 -4.2077775 -4.25484 -4.291266 -4.3132319 -4.3145261 -4.314085][-4.2586555 -4.2490416 -4.2538295 -4.2620459 -4.2538333 -4.2279906 -4.1929221 -4.1658568 -4.170732 -4.2033052 -4.2469931 -4.284493 -4.3092418 -4.3148556 -4.3172359][-4.243505 -4.22973 -4.2315927 -4.2381563 -4.2285752 -4.1990323 -4.1548052 -4.1236248 -4.1269932 -4.1556945 -4.1994214 -4.2476292 -4.2859221 -4.3042707 -4.3136511][-4.2164063 -4.2025867 -4.2009492 -4.2041087 -4.1926827 -4.1609836 -4.1101656 -4.0790105 -4.0861559 -4.1153021 -4.156745 -4.2048936 -4.2511816 -4.2793288 -4.2971725][-4.1823063 -4.172255 -4.1712036 -4.1724424 -4.1623068 -4.1338139 -4.0865245 -4.0545487 -4.0579562 -4.0825458 -4.1213536 -4.1687169 -4.217164 -4.251771 -4.2776208]]...]
INFO - root - 2017-12-07 20:56:59.214115: step 53310, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 50h:45m:52s remains)
INFO - root - 2017-12-07 20:57:05.998803: step 53320, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.695 sec/batch; 53h:52m:43s remains)
INFO - root - 2017-12-07 20:57:12.754694: step 53330, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 51h:01m:43s remains)
INFO - root - 2017-12-07 20:57:19.359974: step 53340, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 52h:43m:58s remains)
INFO - root - 2017-12-07 20:57:26.156222: step 53350, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 49h:56m:32s remains)
INFO - root - 2017-12-07 20:57:33.025645: step 53360, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 50h:52m:49s remains)
INFO - root - 2017-12-07 20:57:39.879384: step 53370, loss = 2.10, batch loss = 2.04 (10.7 examples/sec; 0.751 sec/batch; 58h:12m:56s remains)
INFO - root - 2017-12-07 20:57:46.479494: step 53380, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 51h:36m:20s remains)
INFO - root - 2017-12-07 20:57:53.265475: step 53390, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 52h:30m:59s remains)
INFO - root - 2017-12-07 20:57:59.986577: step 53400, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 52h:05m:54s remains)
2017-12-07 20:58:00.728637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1286664 -4.1309466 -4.1495061 -4.1804647 -4.2080154 -4.2134271 -4.1988869 -4.1793451 -4.157485 -4.1405067 -4.1430964 -4.1670318 -4.1894021 -4.1965432 -4.1962709][-4.1358232 -4.1334887 -4.1494164 -4.1801138 -4.207581 -4.2045007 -4.1838856 -4.1582408 -4.1300392 -4.1100812 -4.1138983 -4.1425986 -4.1662149 -4.1723762 -4.172523][-4.1349664 -4.119657 -4.1263504 -4.1579018 -4.1902328 -4.1904807 -4.1684432 -4.1377983 -4.1031351 -4.0809026 -4.0862565 -4.1177483 -4.1423163 -4.1416578 -4.1304703][-4.1121163 -4.0837722 -4.0811949 -4.1112704 -4.1468081 -4.1534686 -4.132772 -4.1015072 -4.0761981 -4.0678678 -4.0798039 -4.1066017 -4.12408 -4.1155787 -4.0976477][-4.0638371 -4.0351405 -4.0316453 -4.0571566 -4.0898552 -4.0927691 -4.0736237 -4.0537934 -4.0512981 -4.0642071 -4.0814843 -4.1014132 -4.11425 -4.1072564 -4.0965056][-4.0395532 -4.0215812 -4.0194783 -4.0351648 -4.0558534 -4.0524516 -4.0351434 -4.0227461 -4.036099 -4.0574312 -4.072032 -4.0869389 -4.1046834 -4.11526 -4.1186161][-4.0846424 -4.08053 -4.0830636 -4.0902176 -4.0943403 -4.081953 -4.0575943 -4.0373826 -4.0407023 -4.0528283 -4.06047 -4.0750456 -4.1014423 -4.1245308 -4.1306548][-4.1592245 -4.1613665 -4.1673126 -4.1698022 -4.1637516 -4.1441946 -4.1104035 -4.0762525 -4.0632439 -4.0639958 -4.0636482 -4.0756545 -4.1009278 -4.1228275 -4.1215053][-4.2134471 -4.2166538 -4.220109 -4.2173648 -4.2040052 -4.1801438 -4.141036 -4.1054878 -4.0864038 -4.0882916 -4.0967054 -4.110105 -4.1249313 -4.129384 -4.112287][-4.2264886 -4.2280784 -4.2303352 -4.2292333 -4.2162347 -4.1881032 -4.1457243 -4.1143 -4.1032758 -4.1179934 -4.1449556 -4.1633887 -4.1674237 -4.1558452 -4.1259127][-4.1900682 -4.1894717 -4.1947484 -4.2049623 -4.20301 -4.1750369 -4.13171 -4.1061707 -4.104279 -4.1303253 -4.1715965 -4.2003012 -4.2060313 -4.1981864 -4.1702876][-4.1248775 -4.1187978 -4.1343675 -4.1642389 -4.176971 -4.148798 -4.1037178 -4.0808015 -4.0831275 -4.1121521 -4.1592817 -4.1989617 -4.2251339 -4.2363005 -4.2239742][-4.0889082 -4.0875845 -4.1175318 -4.1591492 -4.1761651 -4.1447144 -4.0931745 -4.0615711 -4.0583739 -4.0774407 -4.1179113 -4.1646843 -4.2086873 -4.2416444 -4.243711][-4.1061144 -4.1132908 -4.1486592 -4.1882081 -4.1999497 -4.1684709 -4.1144915 -4.0721989 -4.0523686 -4.0534773 -4.0814509 -4.1236486 -4.1703663 -4.2106113 -4.2217412][-4.1576014 -4.1661825 -4.193356 -4.2211037 -4.2282424 -4.2027497 -4.1540966 -4.1062217 -4.0763531 -4.0642924 -4.0753255 -4.1037641 -4.1424503 -4.1801906 -4.1965976]]...]
INFO - root - 2017-12-07 20:58:07.444595: step 53410, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 54h:19m:17s remains)
INFO - root - 2017-12-07 20:58:14.255564: step 53420, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.724 sec/batch; 56h:07m:53s remains)
INFO - root - 2017-12-07 20:58:21.103079: step 53430, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 51h:24m:50s remains)
INFO - root - 2017-12-07 20:58:27.947061: step 53440, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 49h:39m:59s remains)
INFO - root - 2017-12-07 20:58:34.825535: step 53450, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 53h:27m:12s remains)
INFO - root - 2017-12-07 20:58:41.767869: step 53460, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 56h:30m:38s remains)
INFO - root - 2017-12-07 20:58:48.551826: step 53470, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 53h:30m:20s remains)
INFO - root - 2017-12-07 20:58:55.291808: step 53480, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 48h:00m:39s remains)
INFO - root - 2017-12-07 20:59:02.088137: step 53490, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 50h:53m:08s remains)
INFO - root - 2017-12-07 20:59:08.850055: step 53500, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 53h:24m:39s remains)
2017-12-07 20:59:09.593131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2172065 -4.2014632 -4.1902652 -4.1925616 -4.2087555 -4.23 -4.2434659 -4.2430353 -4.2258706 -4.2061696 -4.1928396 -4.182621 -4.1766682 -4.1834478 -4.1957417][-4.20157 -4.1803141 -4.1685791 -4.1722922 -4.1889172 -4.2110567 -4.2259688 -4.225749 -4.2082438 -4.192215 -4.1868896 -4.182148 -4.176218 -4.1786995 -4.1834345][-4.1904888 -4.1685872 -4.1586833 -4.1639509 -4.180429 -4.2020507 -4.2192159 -4.22146 -4.2077966 -4.1997271 -4.2043872 -4.2048063 -4.1979346 -4.1944027 -4.1912537][-4.1764793 -4.1551237 -4.1452179 -4.1503062 -4.1671681 -4.1887875 -4.2071257 -4.2113047 -4.205266 -4.2082038 -4.2225428 -4.22923 -4.2227912 -4.2136426 -4.2006383][-4.1641426 -4.1428809 -4.1303477 -4.134665 -4.1525292 -4.173677 -4.1889668 -4.1878071 -4.1801977 -4.1883688 -4.2119288 -4.2298908 -4.2310739 -4.2224135 -4.2025714][-4.163486 -4.1471243 -4.1349463 -4.1388812 -4.1562023 -4.1711044 -4.1706605 -4.1503963 -4.1300545 -4.1393523 -4.1738114 -4.2071643 -4.2213359 -4.2180924 -4.1940589][-4.1603689 -4.1490059 -4.1382155 -4.1393757 -4.1492033 -4.1480947 -4.1215649 -4.0720363 -4.0326567 -4.0487051 -4.1067953 -4.1641183 -4.1978645 -4.205276 -4.18465][-4.1443534 -4.13398 -4.1212158 -4.1137486 -4.1074576 -4.0834303 -4.0301785 -3.9515111 -3.8951812 -3.9287806 -4.0228338 -4.11113 -4.16861 -4.1909089 -4.1785135][-4.1441126 -4.1313982 -4.1135907 -4.0970535 -4.077714 -4.0415173 -3.9811006 -3.8975229 -3.8410616 -3.8858647 -3.9948902 -4.0902848 -4.1512 -4.1776032 -4.1702232][-4.1669612 -4.1542106 -4.1343455 -4.1145487 -4.0951614 -4.067728 -4.0248022 -3.969847 -3.9378033 -3.9756944 -4.0609584 -4.1303077 -4.1679821 -4.1831274 -4.1776075][-4.2059374 -4.1984887 -4.1799831 -4.1600094 -4.1440763 -4.1263943 -4.1007843 -4.0701609 -4.0528245 -4.0751805 -4.12768 -4.1663036 -4.1783266 -4.1804247 -4.1774106][-4.2268677 -4.223156 -4.2096987 -4.1962333 -4.1881738 -4.180469 -4.1693959 -4.1553297 -4.1429248 -4.148098 -4.1697145 -4.1821461 -4.1749759 -4.1670513 -4.166532][-4.21758 -4.2134619 -4.2033668 -4.1950283 -4.1924186 -4.19126 -4.1904173 -4.1875973 -4.1800733 -4.1759748 -4.1790609 -4.1780477 -4.1645775 -4.1547012 -4.1572733][-4.1904016 -4.1853957 -4.1810417 -4.1811152 -4.1850581 -4.1881547 -4.1903882 -4.1894865 -4.1839552 -4.1785316 -4.1762147 -4.1721821 -4.1625614 -4.1560531 -4.1570139][-4.1604218 -4.15331 -4.1536021 -4.1614223 -4.1707549 -4.1751928 -4.175518 -4.1729956 -4.169477 -4.1674709 -4.169807 -4.172482 -4.1710634 -4.1680508 -4.1654124]]...]
INFO - root - 2017-12-07 20:59:16.223985: step 53510, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 48h:15m:01s remains)
INFO - root - 2017-12-07 20:59:23.051179: step 53520, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 52h:12m:05s remains)
INFO - root - 2017-12-07 20:59:29.898406: step 53530, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 53h:41m:26s remains)
INFO - root - 2017-12-07 20:59:36.849279: step 53540, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 54h:27m:46s remains)
INFO - root - 2017-12-07 20:59:43.731314: step 53550, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 53h:24m:34s remains)
INFO - root - 2017-12-07 20:59:50.559609: step 53560, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 49h:52m:48s remains)
INFO - root - 2017-12-07 20:59:57.427968: step 53570, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 52h:33m:17s remains)
INFO - root - 2017-12-07 21:00:04.096895: step 53580, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 51h:01m:53s remains)
INFO - root - 2017-12-07 21:00:10.909271: step 53590, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 51h:01m:10s remains)
INFO - root - 2017-12-07 21:00:17.659523: step 53600, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 48h:29m:50s remains)
2017-12-07 21:00:18.337664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2193847 -4.2141066 -4.21212 -4.2126336 -4.2220664 -4.2525973 -4.2761583 -4.283432 -4.2875719 -4.29238 -4.2958 -4.2946663 -4.2841249 -4.2624426 -4.2374682][-4.2134 -4.2034178 -4.2017021 -4.2021108 -4.2135739 -4.2412133 -4.2623696 -4.2702513 -4.2811871 -4.2974453 -4.3112416 -4.3189316 -4.3136358 -4.29387 -4.2699642][-4.2065649 -4.196475 -4.1975532 -4.1991391 -4.2075868 -4.2259831 -4.2339334 -4.2337246 -4.2476068 -4.2756925 -4.3040004 -4.3245459 -4.3275414 -4.3111982 -4.2910113][-4.2048154 -4.1991534 -4.2044029 -4.2047262 -4.2033968 -4.2000532 -4.1819248 -4.1638389 -4.1813173 -4.2237411 -4.2716656 -4.310112 -4.32507 -4.3131914 -4.295579][-4.2118955 -4.2086062 -4.2124186 -4.2084603 -4.1889567 -4.1571074 -4.1062875 -4.0590353 -4.0812726 -4.1426606 -4.2144914 -4.2731876 -4.3024707 -4.2969389 -4.279676][-4.218842 -4.2153168 -4.2150779 -4.2028103 -4.1622686 -4.1016579 -4.0136809 -3.9326918 -3.9728949 -4.0617476 -4.1544127 -4.226439 -4.2679348 -4.2687845 -4.2501707][-4.2204013 -4.2172794 -4.2131681 -4.1917548 -4.1322618 -4.0433269 -3.9151878 -3.7936058 -3.8594015 -3.9822536 -4.0945334 -4.1770086 -4.2286329 -4.2384706 -4.2195134][-4.228179 -4.2219267 -4.2129841 -4.183588 -4.1198359 -4.0273285 -3.8838558 -3.7370012 -3.8142867 -3.9483104 -4.0646157 -4.1498561 -4.2056637 -4.2218819 -4.2069588][-4.2393031 -4.2283177 -4.2164688 -4.1895294 -4.1480851 -4.0875387 -3.979985 -3.8620617 -3.9151025 -4.0112052 -4.10139 -4.1710663 -4.2182279 -4.2344947 -4.224134][-4.2576213 -4.2423291 -4.2260041 -4.2044311 -4.1854086 -4.1556783 -4.0909905 -4.0165243 -4.0504336 -4.1077728 -4.163095 -4.2088175 -4.2441573 -4.2595344 -4.2572007][-4.2729807 -4.2573757 -4.2410097 -4.2253919 -4.2171659 -4.2032347 -4.1675925 -4.1281242 -4.1507964 -4.1819491 -4.2080059 -4.2348142 -4.2602315 -4.27583 -4.2823935][-4.2823238 -4.2713823 -4.262711 -4.25435 -4.2494054 -4.2417412 -4.22312 -4.2045946 -4.2210364 -4.2364469 -4.2431431 -4.2560515 -4.2740936 -4.2874255 -4.2951565][-4.2888622 -4.2834969 -4.2824383 -4.2787504 -4.2770367 -4.2742925 -4.2679195 -4.2583861 -4.2669296 -4.2705474 -4.2672238 -4.2719908 -4.2848954 -4.29491 -4.2993431][-4.2901731 -4.2882428 -4.2896771 -4.2885761 -4.2876706 -4.2884727 -4.2899113 -4.2830453 -4.2811236 -4.2776532 -4.27219 -4.2736416 -4.2833948 -4.2921743 -4.29513][-4.2892675 -4.2913561 -4.2952213 -4.2928376 -4.2892876 -4.2906957 -4.2942338 -4.2881217 -4.2799053 -4.2723603 -4.2697568 -4.27264 -4.2809329 -4.289135 -4.2904129]]...]
INFO - root - 2017-12-07 21:00:25.044948: step 53610, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 54h:03m:08s remains)
INFO - root - 2017-12-07 21:00:31.891905: step 53620, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 52h:41m:33s remains)
INFO - root - 2017-12-07 21:00:38.808566: step 53630, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 51h:43m:20s remains)
INFO - root - 2017-12-07 21:00:45.615965: step 53640, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 56h:15m:41s remains)
INFO - root - 2017-12-07 21:00:52.459466: step 53650, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 55h:25m:01s remains)
INFO - root - 2017-12-07 21:00:59.280530: step 53660, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 53h:36m:44s remains)
INFO - root - 2017-12-07 21:01:06.038878: step 53670, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.637 sec/batch; 49h:22m:33s remains)
INFO - root - 2017-12-07 21:01:12.800045: step 53680, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 50h:16m:36s remains)
INFO - root - 2017-12-07 21:01:19.682607: step 53690, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 56h:16m:06s remains)
INFO - root - 2017-12-07 21:01:26.519147: step 53700, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 55h:53m:40s remains)
2017-12-07 21:01:27.206513: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.271327 -4.2738113 -4.27333 -4.2671113 -4.2554913 -4.241075 -4.2316847 -4.2265587 -4.2236652 -4.221457 -4.2340565 -4.2418752 -4.2458019 -4.2539926 -4.2646532][-4.3058109 -4.3055143 -4.301681 -4.2950358 -4.2862663 -4.27357 -4.260179 -4.2574534 -4.2576404 -4.2480516 -4.2415829 -4.2329836 -4.2303762 -4.2452717 -4.2616034][-4.3238153 -4.3221283 -4.316658 -4.3098621 -4.30081 -4.2839451 -4.2676153 -4.2660336 -4.2732477 -4.2632132 -4.2444067 -4.2223363 -4.2186289 -4.2418423 -4.2602787][-4.3025026 -4.2996645 -4.2937031 -4.2883453 -4.27688 -4.2535939 -4.2373576 -4.2417121 -4.2561665 -4.2502785 -4.2355857 -4.2147145 -4.2152357 -4.242537 -4.25979][-4.2884026 -4.2838454 -4.2777672 -4.2719378 -4.2557521 -4.2255445 -4.1972651 -4.1910772 -4.2008057 -4.1990285 -4.1969743 -4.1922603 -4.2096748 -4.2411284 -4.2565265][-4.28783 -4.282927 -4.2724853 -4.25894 -4.2355547 -4.1994944 -4.150816 -4.1160417 -4.1057339 -4.1042857 -4.1222329 -4.1470838 -4.1908484 -4.2344804 -4.2498808][-4.2730846 -4.2629437 -4.2372675 -4.2070622 -4.1801004 -4.1461024 -4.0780935 -4.0025759 -3.9550431 -3.954402 -4.0159149 -4.0868535 -4.161284 -4.219233 -4.2422657][-4.2362223 -4.2106118 -4.160975 -4.1117783 -4.0892773 -4.0640049 -3.9866467 -3.8856764 -3.8043714 -3.8070366 -3.9247017 -4.0364704 -4.1325283 -4.2019014 -4.2352858][-4.2034636 -4.160212 -4.0893569 -4.0241513 -4.0018253 -3.9871805 -3.9147248 -3.8242645 -3.7649271 -3.7932997 -3.9194951 -4.0294528 -4.1237473 -4.1933722 -4.2320065][-4.1956 -4.1475549 -4.0793185 -4.0165763 -3.9914608 -3.9805474 -3.9220366 -3.8654571 -3.8614302 -3.9125221 -3.9965048 -4.0666852 -4.1340055 -4.1929488 -4.232933][-4.2008572 -4.1588869 -4.1094131 -4.0630078 -4.038619 -4.0224733 -3.9788024 -3.9544127 -3.9753158 -4.0233746 -4.0755215 -4.1179118 -4.16314 -4.207902 -4.2434239][-4.2044239 -4.1739607 -4.1480365 -4.1204515 -4.1012149 -4.0882773 -4.0612974 -4.049243 -4.0688162 -4.1042256 -4.1405873 -4.1746778 -4.2070303 -4.2383389 -4.2657366][-4.1984134 -4.1829734 -4.1805649 -4.1735225 -4.1669264 -4.1665578 -4.1524873 -4.1412644 -4.1506729 -4.1704011 -4.1965094 -4.2250361 -4.2480531 -4.268796 -4.2887316][-4.1909075 -4.1952662 -4.2099695 -4.2203107 -4.2226567 -4.226181 -4.2171335 -4.2072 -4.2108054 -4.2254753 -4.2452636 -4.265295 -4.2793422 -4.2913861 -4.3052993][-4.1808453 -4.2039266 -4.2303495 -4.2516303 -4.2599349 -4.2647109 -4.2578611 -4.2503295 -4.2506452 -4.2618532 -4.2757554 -4.2866497 -4.2937055 -4.3001537 -4.3091378]]...]
INFO - root - 2017-12-07 21:01:33.870202: step 53710, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 53h:05m:45s remains)
INFO - root - 2017-12-07 21:01:40.616513: step 53720, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 53h:43m:01s remains)
INFO - root - 2017-12-07 21:01:47.370226: step 53730, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 54h:51m:33s remains)
INFO - root - 2017-12-07 21:01:54.224970: step 53740, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 54h:55m:14s remains)
INFO - root - 2017-12-07 21:02:00.968532: step 53750, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 50h:09m:13s remains)
INFO - root - 2017-12-07 21:02:07.775686: step 53760, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.635 sec/batch; 49h:07m:51s remains)
INFO - root - 2017-12-07 21:02:14.677301: step 53770, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 54h:59m:04s remains)
INFO - root - 2017-12-07 21:02:21.363089: step 53780, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 55h:37m:53s remains)
INFO - root - 2017-12-07 21:02:28.135891: step 53790, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 52h:53m:39s remains)
INFO - root - 2017-12-07 21:02:34.930878: step 53800, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 50h:45m:55s remains)
2017-12-07 21:02:35.783144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.196147 -4.214272 -4.2414918 -4.2578754 -4.263154 -4.2526474 -4.2460604 -4.2482696 -4.2461805 -4.2441182 -4.2310195 -4.2089353 -4.1864066 -4.1592555 -4.1644559][-4.1875644 -4.21753 -4.2521224 -4.2693267 -4.2711172 -4.253644 -4.2417393 -4.2505164 -4.2613873 -4.2658405 -4.2533331 -4.2396855 -4.2264094 -4.2043791 -4.2016668][-4.2150335 -4.2458487 -4.2761593 -4.2822671 -4.2663889 -4.2292585 -4.2038851 -4.2138147 -4.2360406 -4.2501044 -4.2493677 -4.2515721 -4.2565427 -4.2554164 -4.2596397][-4.2584109 -4.2822976 -4.2969637 -4.2821383 -4.2397203 -4.1792369 -4.1397996 -4.14695 -4.1811614 -4.205596 -4.2174892 -4.232007 -4.2475529 -4.263555 -4.284781][-4.2892838 -4.3010278 -4.2992945 -4.2592654 -4.1806836 -4.0885797 -4.0231686 -4.0152764 -4.0632005 -4.1141529 -4.1442876 -4.1695004 -4.1906166 -4.21806 -4.254899][-4.3039055 -4.302505 -4.2856121 -4.2234125 -4.1108546 -3.9803538 -3.8758266 -3.8364773 -3.8906565 -3.9688711 -4.0153518 -4.0563583 -4.0900168 -4.1256456 -4.1814976][-4.3037095 -4.2929349 -4.2629275 -4.1845508 -4.0596619 -3.9146681 -3.7862031 -3.7227218 -3.7745481 -3.8599646 -3.9050789 -3.9520307 -3.9970703 -4.0348177 -4.0994115][-4.290967 -4.2822175 -4.2550392 -4.1868148 -4.0840058 -3.966188 -3.8622558 -3.8149633 -3.8512247 -3.9034669 -3.9130077 -3.9342766 -3.9753754 -4.0031767 -4.0537214][-4.2762704 -4.2751918 -4.2599549 -4.2176309 -4.1528225 -4.0766172 -4.0109229 -3.9853604 -4.0018764 -4.0165544 -3.9928932 -3.9856088 -4.0104327 -4.0233068 -4.0534663][-4.2659807 -4.2726994 -4.2733412 -4.2557535 -4.224021 -4.1788745 -4.136209 -4.1186333 -4.1172895 -4.1104312 -4.0779591 -4.0595069 -4.0683393 -4.068593 -4.0853014][-4.2684717 -4.2785544 -4.2858505 -4.2843094 -4.2754827 -4.2513747 -4.2228889 -4.2080173 -4.200501 -4.1913981 -4.1675072 -4.1494255 -4.1485109 -4.1428494 -4.1523004][-4.2859612 -4.293808 -4.2967515 -4.2966943 -4.2944307 -4.2806683 -4.26385 -4.2575712 -4.2548008 -4.253438 -4.2455688 -4.2369585 -4.2344 -4.2305183 -4.2340093][-4.3083587 -4.3135362 -4.3092818 -4.3046646 -4.2983246 -4.2873969 -4.275969 -4.2776823 -4.28489 -4.2923832 -4.2952971 -4.2959094 -4.296916 -4.2965441 -4.2960973][-4.3232169 -4.3235235 -4.3146935 -4.3084164 -4.3012934 -4.2915554 -4.2823524 -4.2893353 -4.304491 -4.3162122 -4.3210831 -4.3230257 -4.32354 -4.3234124 -4.3244963][-4.3230667 -4.3182135 -4.30827 -4.3030553 -4.2968893 -4.2907643 -4.2880282 -4.29943 -4.3155079 -4.3261194 -4.33066 -4.3325539 -4.3319306 -4.3310332 -4.33151]]...]
INFO - root - 2017-12-07 21:02:42.353387: step 53810, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 49h:45m:59s remains)
INFO - root - 2017-12-07 21:02:49.050780: step 53820, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 50h:35m:24s remains)
INFO - root - 2017-12-07 21:02:55.850198: step 53830, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 52h:24m:58s remains)
INFO - root - 2017-12-07 21:03:02.647735: step 53840, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 56h:10m:16s remains)
INFO - root - 2017-12-07 21:03:09.449022: step 53850, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 50h:44m:25s remains)
INFO - root - 2017-12-07 21:03:16.278640: step 53860, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 51h:08m:37s remains)
INFO - root - 2017-12-07 21:03:23.107274: step 53870, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 50h:43m:36s remains)
INFO - root - 2017-12-07 21:03:30.019096: step 53880, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 53h:14m:49s remains)
INFO - root - 2017-12-07 21:03:36.733856: step 53890, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.733 sec/batch; 56h:43m:30s remains)
INFO - root - 2017-12-07 21:03:43.545966: step 53900, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 52h:08m:45s remains)
2017-12-07 21:03:44.320665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1676493 -4.1964746 -4.2214842 -4.2388678 -4.2483568 -4.2643352 -4.2800436 -4.291832 -4.298636 -4.3078356 -4.3074369 -4.2736778 -4.22234 -4.1740918 -4.1567292][-4.1865444 -4.2146211 -4.2347393 -4.246027 -4.2554679 -4.2743335 -4.2867746 -4.2943048 -4.2987366 -4.3035502 -4.306766 -4.2831378 -4.2461743 -4.2087965 -4.1873722][-4.2195277 -4.2304792 -4.2405133 -4.2512584 -4.26156 -4.2756948 -4.2788124 -4.279027 -4.2839155 -4.2910609 -4.296649 -4.2798615 -4.2532921 -4.2246604 -4.2060966][-4.2507482 -4.2407513 -4.2402968 -4.2469716 -4.247406 -4.2456694 -4.234251 -4.2269468 -4.2422476 -4.2667613 -4.2799244 -4.2655563 -4.2431684 -4.2226868 -4.2162437][-4.2731371 -4.253109 -4.2408614 -4.2271295 -4.2035751 -4.1788516 -4.15296 -4.1463251 -4.1806226 -4.2271457 -4.2547474 -4.2518721 -4.2375441 -4.2226162 -4.2215881][-4.2791109 -4.2561808 -4.2303405 -4.1880918 -4.1319866 -4.0764923 -4.025938 -4.0202975 -4.0878582 -4.169559 -4.2185993 -4.2301159 -4.2313795 -4.22348 -4.2205706][-4.2704363 -4.2436047 -4.212296 -4.1564803 -4.076901 -3.9890435 -3.8942306 -3.8696289 -3.9724355 -4.0941863 -4.1685443 -4.1996226 -4.2188544 -4.2196431 -4.2151442][-4.257894 -4.2342067 -4.2098393 -4.1682377 -4.1054873 -4.0223675 -3.912077 -3.8698754 -3.9726527 -4.091342 -4.1634197 -4.200973 -4.2254062 -4.2244253 -4.217042][-4.254179 -4.2381897 -4.2259374 -4.2039227 -4.1702366 -4.1245704 -4.0564756 -4.0286741 -4.0943995 -4.1645603 -4.2040172 -4.2269053 -4.2393355 -4.2324324 -4.2271042][-4.2586832 -4.2526345 -4.2504516 -4.2402487 -4.2255917 -4.20282 -4.164495 -4.1540518 -4.1924953 -4.2228227 -4.2345567 -4.2417727 -4.2421832 -4.233057 -4.2291451][-4.2592807 -4.2570844 -4.2606554 -4.2598052 -4.2558327 -4.2396374 -4.2177553 -4.2238917 -4.2499409 -4.2588892 -4.2543087 -4.2455831 -4.2315931 -4.2204394 -4.2164345][-4.2543669 -4.2525849 -4.2543154 -4.2542114 -4.2512555 -4.2390046 -4.2362518 -4.2550874 -4.2789693 -4.2868772 -4.2756004 -4.2549396 -4.2273803 -4.2066426 -4.1945987][-4.2480607 -4.2411857 -4.2336626 -4.2252626 -4.2174535 -4.2110081 -4.2278633 -4.2553034 -4.2795558 -4.2937088 -4.2868595 -4.2656288 -4.2362013 -4.20799 -4.1890035][-4.2389755 -4.230063 -4.22063 -4.2039213 -4.1836534 -4.17493 -4.2034245 -4.2401357 -4.2658949 -4.2838116 -4.2867546 -4.2782125 -4.2597556 -4.2355213 -4.2188349][-4.2348194 -4.2244635 -4.2117662 -4.1861615 -4.1568127 -4.1484327 -4.1813755 -4.2238469 -4.2531362 -4.2730002 -4.28398 -4.2848363 -4.2744541 -4.2574644 -4.2423563]]...]
INFO - root - 2017-12-07 21:03:50.960675: step 53910, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 55h:06m:59s remains)
INFO - root - 2017-12-07 21:03:57.908165: step 53920, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 54h:27m:08s remains)
INFO - root - 2017-12-07 21:04:04.702175: step 53930, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 49h:21m:11s remains)
INFO - root - 2017-12-07 21:04:11.570541: step 53940, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 52h:20m:24s remains)
INFO - root - 2017-12-07 21:04:18.363637: step 53950, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 53h:24m:17s remains)
INFO - root - 2017-12-07 21:04:25.179045: step 53960, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 52h:09m:57s remains)
INFO - root - 2017-12-07 21:04:31.887738: step 53970, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.692 sec/batch; 53h:30m:32s remains)
INFO - root - 2017-12-07 21:04:38.640473: step 53980, loss = 2.08, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 52h:23m:03s remains)
INFO - root - 2017-12-07 21:04:45.468657: step 53990, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 52h:07m:20s remains)
INFO - root - 2017-12-07 21:04:52.177143: step 54000, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 53h:30m:24s remains)
2017-12-07 21:04:52.926684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2760077 -4.2781167 -4.2794428 -4.2799091 -4.2792463 -4.27772 -4.2752924 -4.2728686 -4.2718425 -4.2725511 -4.2742882 -4.2768369 -4.2796688 -4.2825427 -4.2856402][-4.2751746 -4.2769885 -4.2781014 -4.2783589 -4.277349 -4.2750378 -4.2714858 -4.2678928 -4.2663674 -4.2672753 -4.2694631 -4.2728424 -4.2766547 -4.2805648 -4.2844543][-4.272963 -4.2734432 -4.273335 -4.2724113 -4.2700725 -4.2663713 -4.2619004 -4.2580242 -4.2565851 -4.2575822 -4.26031 -4.26484 -4.2700086 -4.275537 -4.280736][-4.2694306 -4.2671003 -4.2642369 -4.2612085 -4.2580671 -4.2535567 -4.2481375 -4.2442355 -4.2428627 -4.2438817 -4.2470078 -4.2526431 -4.2597446 -4.2677021 -4.2754145][-4.2598877 -4.2552018 -4.2503796 -4.2466826 -4.2438846 -4.2389979 -4.2329049 -4.2284594 -4.2246389 -4.2226038 -4.2239208 -4.2299418 -4.2394791 -4.2513433 -4.2636366][-4.2422576 -4.2370362 -4.232173 -4.2295504 -4.2274871 -4.2212238 -4.2137923 -4.2076082 -4.2003489 -4.1935911 -4.1914039 -4.1965303 -4.2078695 -4.2236814 -4.2405534][-4.222507 -4.2193065 -4.2182045 -4.2203431 -4.2207971 -4.2138624 -4.2046318 -4.1953773 -4.1838784 -4.17151 -4.1639323 -4.1649976 -4.1736293 -4.1880121 -4.205657][-4.211 -4.2103572 -4.2128882 -4.2184558 -4.219964 -4.2127075 -4.2019477 -4.1904178 -4.17584 -4.1601782 -4.1483784 -4.144176 -4.147233 -4.1560144 -4.1700754][-4.2207084 -4.2200994 -4.222888 -4.2275391 -4.2277055 -4.2212191 -4.2114043 -4.2002769 -4.1868253 -4.1731744 -4.1610265 -4.1530066 -4.1502414 -4.1512151 -4.1566782][-4.2342896 -4.2326732 -4.2354846 -4.2410312 -4.2434731 -4.2414322 -4.2361736 -4.2298741 -4.2218394 -4.2126846 -4.2015457 -4.1914487 -4.1846342 -4.1798806 -4.1785259][-4.2358365 -4.2341495 -4.2382741 -4.2476487 -4.2553859 -4.2591386 -4.2593036 -4.2588267 -4.2579718 -4.2553043 -4.2474504 -4.2373672 -4.2289648 -4.2216349 -4.2165375][-4.2367535 -4.2348576 -4.2401686 -4.2527146 -4.2644815 -4.2719178 -4.2753367 -4.278842 -4.2822247 -4.28363 -4.2795324 -4.2715282 -4.2647834 -4.2587247 -4.2543039][-4.2371993 -4.2356381 -4.241322 -4.2535915 -4.266489 -4.2761803 -4.2818813 -4.2857523 -4.2886868 -4.2904286 -4.2888989 -4.2843747 -4.2812791 -4.2794852 -4.2785454][-4.2390928 -4.2371821 -4.2416568 -4.2503047 -4.2594695 -4.2667217 -4.2705007 -4.2717419 -4.2720146 -4.2734914 -4.2751436 -4.2758689 -4.2774086 -4.2798209 -4.2826128][-4.2489696 -4.2465463 -4.24802 -4.2500219 -4.2536659 -4.2561283 -4.2559261 -4.2542796 -4.2525854 -4.2533851 -4.2560978 -4.2603927 -4.2656245 -4.27072 -4.2757812]]...]
INFO - root - 2017-12-07 21:04:59.579912: step 54010, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 48h:56m:22s remains)
INFO - root - 2017-12-07 21:05:06.391644: step 54020, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 51h:40m:55s remains)
INFO - root - 2017-12-07 21:05:13.317268: step 54030, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 54h:48m:22s remains)
INFO - root - 2017-12-07 21:05:20.133299: step 54040, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 55h:21m:28s remains)
INFO - root - 2017-12-07 21:05:27.030750: step 54050, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 54h:29m:11s remains)
INFO - root - 2017-12-07 21:05:33.814272: step 54060, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 48h:48m:30s remains)
INFO - root - 2017-12-07 21:05:40.699915: step 54070, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 51h:52m:21s remains)
INFO - root - 2017-12-07 21:05:47.505444: step 54080, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 54h:00m:18s remains)
INFO - root - 2017-12-07 21:05:54.280412: step 54090, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 53h:02m:27s remains)
INFO - root - 2017-12-07 21:06:00.998587: step 54100, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 52h:57m:26s remains)
2017-12-07 21:06:01.722121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1921515 -4.171308 -4.167469 -4.1794038 -4.1936569 -4.2086086 -4.2215509 -4.232471 -4.2458749 -4.2577114 -4.2541246 -4.2365732 -4.2181873 -4.2085786 -4.2124925][-4.2189293 -4.1982484 -4.1963997 -4.213903 -4.2382221 -4.2607574 -4.2763252 -4.2851696 -4.2918062 -4.2926464 -4.2803268 -4.2581291 -4.2391238 -4.2311754 -4.2359042][-4.2289138 -4.2120466 -4.2181759 -4.2422657 -4.2699509 -4.2907844 -4.302772 -4.3066912 -4.3054328 -4.2984672 -4.2814975 -4.2605891 -4.2462015 -4.2424693 -4.2472267][-4.2214875 -4.209424 -4.2218065 -4.2478857 -4.2692537 -4.2813663 -4.2890739 -4.2901344 -4.281867 -4.2680593 -4.2500024 -4.2372141 -4.2323446 -4.2342353 -4.2392592][-4.2195354 -4.2077847 -4.2157111 -4.2320433 -4.2391949 -4.2387462 -4.2433567 -4.2454934 -4.2345757 -4.2133646 -4.1956434 -4.1928816 -4.197505 -4.2049174 -4.2135072][-4.2290931 -4.2127786 -4.2071486 -4.2046628 -4.19059 -4.1758075 -4.1793084 -4.1875887 -4.1807566 -4.1573782 -4.143239 -4.1498222 -4.1604729 -4.1724296 -4.1841717][-4.2388449 -4.2137141 -4.1914287 -4.166635 -4.1308441 -4.1073394 -4.1182804 -4.1429262 -4.1463671 -4.1279783 -4.1229677 -4.1365457 -4.1454735 -4.15045 -4.1573095][-4.2377338 -4.2059393 -4.171268 -4.1303306 -4.0868559 -4.0715971 -4.098526 -4.137125 -4.1480184 -4.1339254 -4.1328449 -4.1449571 -4.1449785 -4.138083 -4.1344581][-4.2193465 -4.1879539 -4.1554484 -4.1184492 -4.0906792 -4.094429 -4.1270924 -4.1623878 -4.1705079 -4.1561546 -4.1524835 -4.1596994 -4.1534243 -4.1363597 -4.1274834][-4.2008619 -4.1763616 -4.1522579 -4.129281 -4.1234617 -4.1405358 -4.1677361 -4.1935339 -4.1994748 -4.1853175 -4.1776538 -4.1790795 -4.1685433 -4.1458926 -4.1379142][-4.1890922 -4.1723237 -4.156889 -4.1459827 -4.1573663 -4.1800313 -4.2000737 -4.2202573 -4.2281256 -4.2177405 -4.2079253 -4.2043943 -4.1936541 -4.1737289 -4.1700139][-4.1808891 -4.1723084 -4.1623559 -4.1554356 -4.1719851 -4.1942525 -4.2106419 -4.2275662 -4.2384396 -4.2359924 -4.2292595 -4.2256961 -4.218689 -4.204628 -4.20059][-4.1814289 -4.1793642 -4.1721587 -4.1639447 -4.17651 -4.1926465 -4.2030473 -4.2133636 -4.2230096 -4.2275915 -4.2253008 -4.2215233 -4.2180529 -4.2109046 -4.2056546][-4.2007637 -4.1992435 -4.1917772 -4.1826077 -4.1883965 -4.1945167 -4.19865 -4.2060685 -4.2152309 -4.2224426 -4.2221904 -4.2179523 -4.2174721 -4.213788 -4.2042322][-4.23235 -4.2257586 -4.2158394 -4.2066412 -4.2077575 -4.206109 -4.206213 -4.2135262 -4.221889 -4.2261381 -4.2243681 -4.22034 -4.218719 -4.21397 -4.2009225]]...]
INFO - root - 2017-12-07 21:06:08.406591: step 54110, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 54h:41m:44s remains)
INFO - root - 2017-12-07 21:06:15.324752: step 54120, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 52h:09m:35s remains)
INFO - root - 2017-12-07 21:06:22.066412: step 54130, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 49h:33m:37s remains)
INFO - root - 2017-12-07 21:06:28.930204: step 54140, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 51h:13m:49s remains)
INFO - root - 2017-12-07 21:06:35.718230: step 54150, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 52h:26m:19s remains)
INFO - root - 2017-12-07 21:06:42.517251: step 54160, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 53h:30m:53s remains)
INFO - root - 2017-12-07 21:06:49.296012: step 54170, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 52h:22m:20s remains)
INFO - root - 2017-12-07 21:06:56.122738: step 54180, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 51h:19m:09s remains)
INFO - root - 2017-12-07 21:07:03.012659: step 54190, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.669 sec/batch; 51h:42m:18s remains)
INFO - root - 2017-12-07 21:07:09.741483: step 54200, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 54h:48m:48s remains)
2017-12-07 21:07:10.414015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2436962 -4.2304921 -4.2073593 -4.1697173 -4.1381803 -4.1332936 -4.150846 -4.1646619 -4.1773844 -4.1912713 -4.1936073 -4.1832805 -4.1710334 -4.1609759 -4.1718125][-4.2720671 -4.2673759 -4.2526865 -4.2240396 -4.1893125 -4.1703186 -4.1760607 -4.184104 -4.1891322 -4.2009182 -4.2144628 -4.2184367 -4.2157922 -4.2154689 -4.227098][-4.2810044 -4.2806735 -4.2744074 -4.252656 -4.2173343 -4.188653 -4.1824574 -4.1820116 -4.1779823 -4.1855745 -4.2076426 -4.2288508 -4.2410083 -4.2532353 -4.2688317][-4.2799425 -4.2734747 -4.2644057 -4.24285 -4.21315 -4.189518 -4.1800175 -4.1709452 -4.1602626 -4.1681643 -4.1973176 -4.2269163 -4.2493963 -4.2704983 -4.2880707][-4.259583 -4.2506528 -4.2375164 -4.2179542 -4.1946716 -4.1770482 -4.1640668 -4.1469517 -4.1315928 -4.1467767 -4.1867542 -4.2184429 -4.2449884 -4.2710443 -4.2919674][-4.236876 -4.2198238 -4.1999702 -4.1843038 -4.1656327 -4.1493716 -4.1296682 -4.1026139 -4.0735888 -4.0921097 -4.1490126 -4.1946392 -4.2315893 -4.2633729 -4.289443][-4.2191424 -4.1887865 -4.1591511 -4.1409354 -4.11756 -4.0987163 -4.074079 -4.0346603 -3.9853046 -4.0074887 -4.0899215 -4.1596284 -4.211535 -4.25093 -4.2810555][-4.2087688 -4.1680307 -4.1323209 -4.1062727 -4.0663047 -4.0368447 -4.0118918 -3.9707134 -3.9163897 -3.9491754 -4.0508642 -4.1361384 -4.2002172 -4.2447472 -4.2750106][-4.1972666 -4.1563115 -4.1291761 -4.103549 -4.0534711 -4.0196481 -4.0066104 -3.9807115 -3.9498823 -3.9961984 -4.08595 -4.1617641 -4.2219882 -4.2629342 -4.2863979][-4.1974111 -4.16445 -4.1499572 -4.1266961 -4.0735087 -4.0446935 -4.0485663 -4.0497446 -4.0511384 -4.0980768 -4.1604819 -4.2114167 -4.2564526 -4.2884917 -4.3063617][-4.2117791 -4.180213 -4.170126 -4.1459417 -4.0975294 -4.0711956 -4.0813737 -4.1027164 -4.1300263 -4.1787977 -4.2273669 -4.2601547 -4.2895041 -4.3113327 -4.3241873][-4.2363639 -4.206264 -4.1957531 -4.1745882 -4.1358 -4.1091952 -4.113368 -4.1351171 -4.172472 -4.2243171 -4.2695684 -4.2968516 -4.3139849 -4.3248706 -4.3338809][-4.2736011 -4.2505021 -4.24134 -4.2250924 -4.1949191 -4.1692286 -4.1639338 -4.177454 -4.2105618 -4.2563148 -4.2952948 -4.3161488 -4.3241115 -4.3287592 -4.3372173][-4.3041511 -4.2897429 -4.2852907 -4.274899 -4.2549415 -4.2344632 -4.2236414 -4.2290707 -4.2527161 -4.2850423 -4.31221 -4.3250251 -4.3272796 -4.3299451 -4.3370743][-4.3228912 -4.314784 -4.3131552 -4.3069959 -4.2958269 -4.2834377 -4.2734933 -4.2749834 -4.289844 -4.3081007 -4.3232012 -4.3292761 -4.3305297 -4.3331676 -4.3390312]]...]
INFO - root - 2017-12-07 21:07:16.991263: step 54210, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 52h:31m:51s remains)
INFO - root - 2017-12-07 21:07:23.968392: step 54220, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 54h:57m:14s remains)
INFO - root - 2017-12-07 21:07:30.729274: step 54230, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 52h:01m:07s remains)
INFO - root - 2017-12-07 21:07:37.478500: step 54240, loss = 2.08, batch loss = 2.03 (13.0 examples/sec; 0.618 sec/batch; 47h:44m:04s remains)
INFO - root - 2017-12-07 21:07:44.323362: step 54250, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 50h:07m:58s remains)
INFO - root - 2017-12-07 21:07:51.147642: step 54260, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 52h:13m:12s remains)
INFO - root - 2017-12-07 21:07:57.968082: step 54270, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 52h:18m:54s remains)
INFO - root - 2017-12-07 21:08:04.801152: step 54280, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 53h:30m:02s remains)
INFO - root - 2017-12-07 21:08:11.628449: step 54290, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 53h:10m:06s remains)
INFO - root - 2017-12-07 21:08:18.467105: step 54300, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 49h:15m:02s remains)
2017-12-07 21:08:19.168343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3484855 -4.349575 -4.3072672 -4.2344556 -4.1614866 -4.1072192 -4.0801148 -4.1043186 -4.1597929 -4.2069044 -4.2141418 -4.2032666 -4.1995668 -4.1965809 -4.2196622][-4.3516769 -4.3543673 -4.3154168 -4.2479267 -4.1749368 -4.1154394 -4.0825772 -4.1068683 -4.1724329 -4.2276826 -4.2502041 -4.260962 -4.264801 -4.2517815 -4.2513824][-4.3536263 -4.3552132 -4.3131328 -4.2469487 -4.172224 -4.10004 -4.0526557 -4.0753479 -4.1502595 -4.2147694 -4.2549763 -4.2842946 -4.2942977 -4.2782373 -4.2622519][-4.3565154 -4.3550634 -4.3069925 -4.2352057 -4.1559496 -4.0721216 -4.0048485 -4.0190039 -4.0959425 -4.173769 -4.2326183 -4.2811942 -4.2994928 -4.2892523 -4.2623248][-4.3574433 -4.3525987 -4.2982407 -4.2194638 -4.134995 -4.0398726 -3.9506476 -3.9450676 -4.0253005 -4.11749 -4.1991258 -4.2661939 -4.296751 -4.2906647 -4.2531147][-4.3563671 -4.3484721 -4.2892509 -4.2041388 -4.1125865 -4.0024824 -3.8875239 -3.8639798 -3.9533608 -4.0703292 -4.1779394 -4.2607746 -4.2940416 -4.2863 -4.2395058][-4.3515959 -4.3407607 -4.2779222 -4.1859221 -4.0821838 -3.9547963 -3.8146935 -3.7875755 -3.8942182 -4.039556 -4.1735244 -4.267694 -4.2988005 -4.28484 -4.2284632][-4.3439422 -4.3330574 -4.2706466 -4.1748786 -4.061296 -3.9234676 -3.7833004 -3.7764366 -3.8906386 -4.0447373 -4.1878939 -4.2797608 -4.3057914 -4.2830114 -4.2103019][-4.3382626 -4.3295569 -4.2706184 -4.1811457 -4.0764055 -3.9527707 -3.8429184 -3.8617787 -3.9633141 -4.0961943 -4.2223587 -4.2979813 -4.3136511 -4.277936 -4.1876993][-4.3336067 -4.3276143 -4.275156 -4.2003317 -4.1195931 -4.0281057 -3.9580832 -3.9904397 -4.0680451 -4.1691241 -4.2648339 -4.3161039 -4.319479 -4.2733574 -4.1776662][-4.3296485 -4.3271914 -4.2856464 -4.229394 -4.1777887 -4.1190515 -4.0826297 -4.1147962 -4.1667805 -4.2374139 -4.3043346 -4.3316588 -4.3231091 -4.2720561 -4.185668][-4.3259192 -4.3278961 -4.2976046 -4.25892 -4.2303457 -4.2009077 -4.1877413 -4.212121 -4.2443519 -4.2915931 -4.3380008 -4.3515821 -4.3350945 -4.2844343 -4.2127857][-4.3200221 -4.3242726 -4.3053155 -4.2821293 -4.2704058 -4.2606053 -4.2574577 -4.2714119 -4.2914395 -4.3233638 -4.3569384 -4.3658075 -4.3474751 -4.3036542 -4.2466035][-4.3127537 -4.3196917 -4.3125529 -4.301033 -4.2983375 -4.2960963 -4.2992163 -4.30509 -4.3160977 -4.335887 -4.3588123 -4.36471 -4.3487277 -4.3145528 -4.2761488][-4.30417 -4.3107305 -4.3105211 -4.3066249 -4.308671 -4.31092 -4.3164344 -4.3188615 -4.323503 -4.3329797 -4.3465352 -4.3509665 -4.3413529 -4.3206449 -4.3010197]]...]
INFO - root - 2017-12-07 21:08:25.805323: step 54310, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 51h:54m:44s remains)
INFO - root - 2017-12-07 21:08:32.661287: step 54320, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 51h:47m:58s remains)
INFO - root - 2017-12-07 21:08:39.457534: step 54330, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 50h:24m:45s remains)
INFO - root - 2017-12-07 21:08:46.261294: step 54340, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 51h:59m:24s remains)
INFO - root - 2017-12-07 21:08:53.136453: step 54350, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 52h:19m:21s remains)
INFO - root - 2017-12-07 21:08:59.946962: step 54360, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 53h:46m:24s remains)
INFO - root - 2017-12-07 21:09:06.765886: step 54370, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 49h:41m:31s remains)
INFO - root - 2017-12-07 21:09:13.478166: step 54380, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 50h:24m:22s remains)
INFO - root - 2017-12-07 21:09:20.285049: step 54390, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 53h:24m:46s remains)
INFO - root - 2017-12-07 21:09:27.192843: step 54400, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 53h:57m:33s remains)
2017-12-07 21:09:27.884706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2688909 -4.2710352 -4.2766204 -4.2842655 -4.2863531 -4.2846465 -4.2832561 -4.2835121 -4.2849312 -4.2872615 -4.2889075 -4.2898035 -4.2910028 -4.2881851 -4.2817864][-4.2638764 -4.2633638 -4.2679062 -4.2746778 -4.2761393 -4.271976 -4.2662444 -4.2636495 -4.2659483 -4.2724614 -4.2784042 -4.2823949 -4.2861128 -4.2853 -4.27988][-4.2492237 -4.2455573 -4.248589 -4.2533159 -4.2523403 -4.2413111 -4.2265792 -4.2176242 -4.220232 -4.2331877 -4.2462196 -4.2569523 -4.2652731 -4.2667322 -4.2602406][-4.2326508 -4.2278943 -4.2282577 -4.2285719 -4.223208 -4.2037082 -4.1761203 -4.1558781 -4.1584444 -4.179214 -4.2036366 -4.2253151 -4.2392292 -4.2422161 -4.235136][-4.2058997 -4.2026191 -4.201715 -4.1986303 -4.1887989 -4.1596651 -4.1177845 -4.0856366 -4.0888052 -4.1214352 -4.1614923 -4.1950235 -4.21209 -4.216774 -4.2095008][-4.1688948 -4.1708264 -4.1710558 -4.165978 -4.1536927 -4.1151938 -4.0566006 -4.0060897 -4.0094981 -4.0624938 -4.1234746 -4.1682425 -4.1870589 -4.1929932 -4.1870918][-4.1333303 -4.1441793 -4.1473923 -4.1406608 -4.1227279 -4.0713606 -3.9847558 -3.9003463 -3.9010038 -3.9856033 -4.0772643 -4.135797 -4.1607289 -4.1738677 -4.1736388][-4.1060848 -4.1262536 -4.1331592 -4.1219139 -4.0959659 -4.0342159 -3.9225445 -3.8011713 -3.8020856 -3.9238167 -4.0436444 -4.1161027 -4.1499815 -4.1685591 -4.1727729][-4.0987258 -4.1257772 -4.1375022 -4.1246948 -4.0982685 -4.043921 -3.9412334 -3.8314884 -3.8447089 -3.9621675 -4.0736275 -4.1383514 -4.1693153 -4.1825566 -4.18355][-4.1231833 -4.1542764 -4.1691651 -4.1573 -4.1336989 -4.0946755 -4.0262976 -3.9606357 -3.9793949 -4.0630784 -4.1466651 -4.1916466 -4.2105107 -4.2127771 -4.2059512][-4.1615205 -4.1887531 -4.1991487 -4.1872129 -4.1701665 -4.1469941 -4.1100545 -4.0789537 -4.1000624 -4.1555533 -4.213593 -4.2436719 -4.255239 -4.2488403 -4.2346158][-4.1968379 -4.2196889 -4.2224274 -4.2107449 -4.2017961 -4.1915426 -4.1734462 -4.1613555 -4.1794591 -4.2160516 -4.2565675 -4.2777719 -4.2848067 -4.2748528 -4.2548003][-4.22749 -4.2440996 -4.2394142 -4.2282686 -4.2272754 -4.2297258 -4.2223973 -4.2189112 -4.22981 -4.2505903 -4.2777576 -4.2924252 -4.2935948 -4.2812886 -4.258965][-4.2502294 -4.2616467 -4.2537189 -4.2440286 -4.24756 -4.2580018 -4.2586641 -4.260139 -4.2680306 -4.2800908 -4.2965369 -4.3041086 -4.2988029 -4.2851186 -4.2623291][-4.2688775 -4.2776408 -4.2706704 -4.2619338 -4.2643476 -4.276051 -4.282835 -4.2896805 -4.295887 -4.3028269 -4.3134484 -4.318233 -4.3112416 -4.2989244 -4.2792187]]...]
INFO - root - 2017-12-07 21:09:34.490000: step 54410, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 50h:51m:23s remains)
INFO - root - 2017-12-07 21:09:41.349573: step 54420, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 51h:22m:15s remains)
INFO - root - 2017-12-07 21:09:48.234633: step 54430, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.728 sec/batch; 56h:13m:17s remains)
INFO - root - 2017-12-07 21:09:54.952484: step 54440, loss = 2.08, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 51h:37m:42s remains)
INFO - root - 2017-12-07 21:10:01.836063: step 54450, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 52h:07m:14s remains)
INFO - root - 2017-12-07 21:10:08.547833: step 54460, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.638 sec/batch; 49h:15m:15s remains)
INFO - root - 2017-12-07 21:10:15.398703: step 54470, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 54h:36m:43s remains)
INFO - root - 2017-12-07 21:10:22.090131: step 54480, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 53h:02m:03s remains)
INFO - root - 2017-12-07 21:10:28.798476: step 54490, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.682 sec/batch; 52h:38m:42s remains)
INFO - root - 2017-12-07 21:10:35.441347: step 54500, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 50h:58m:46s remains)
2017-12-07 21:10:36.129533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.10912 -4.11505 -4.1178684 -4.1238942 -4.135323 -4.1465988 -4.1584635 -4.1731267 -4.1928625 -4.2090497 -4.2128224 -4.1988492 -4.1783447 -4.1557164 -4.1442618][-4.1010427 -4.1095829 -4.1139774 -4.1182122 -4.1291785 -4.148632 -4.1641073 -4.1765766 -4.1893215 -4.2027507 -4.2044444 -4.188457 -4.1638856 -4.1399679 -4.129632][-4.1117134 -4.1178026 -4.1235561 -4.1294084 -4.1428833 -4.1637244 -4.1737671 -4.1770763 -4.1829443 -4.1955013 -4.1996264 -4.1863661 -4.1619158 -4.1390295 -4.1297679][-4.1309013 -4.1356783 -4.1386256 -4.1415 -4.15271 -4.1651225 -4.1631432 -4.1546092 -4.1569004 -4.1722708 -4.182653 -4.1782703 -4.1598196 -4.1361456 -4.1235189][-4.1559849 -4.1559839 -4.150671 -4.1438069 -4.1410389 -4.137517 -4.1229329 -4.1100864 -4.119792 -4.1439972 -4.1600485 -4.16245 -4.1532059 -4.1405511 -4.1318212][-4.1786556 -4.16654 -4.1477327 -4.1285095 -4.105504 -4.0755992 -4.0407047 -4.0303459 -4.0611362 -4.1022153 -4.1329584 -4.1483545 -4.156177 -4.1617551 -4.1598678][-4.2021246 -4.1798835 -4.1532612 -4.1198826 -4.0765104 -4.0125408 -3.9417794 -3.9326737 -3.9994686 -4.0675745 -4.1185665 -4.1528931 -4.1780753 -4.1987252 -4.199789][-4.2259493 -4.1983571 -4.167201 -4.1274962 -4.0727482 -3.9884834 -3.8908296 -3.8831773 -3.9730186 -4.0612006 -4.1270885 -4.1740746 -4.2084751 -4.2364588 -4.2446275][-4.2420444 -4.2121067 -4.1813684 -4.1422305 -4.091835 -4.02285 -3.949322 -3.9462764 -4.0162616 -4.0909071 -4.1509457 -4.1961608 -4.2288017 -4.2589483 -4.2740989][-4.2437825 -4.2200351 -4.1906857 -4.1572347 -4.1133676 -4.0677776 -4.0295529 -4.0355997 -4.0850673 -4.13625 -4.1797028 -4.214787 -4.2376142 -4.2616267 -4.2788987][-4.2327857 -4.2172775 -4.1886106 -4.1551929 -4.1139822 -4.0833077 -4.075727 -4.0977869 -4.1402726 -4.17825 -4.2078404 -4.2325382 -4.2467284 -4.2628245 -4.2782068][-4.2193236 -4.2071176 -4.1793995 -4.142015 -4.1025753 -4.0812788 -4.0957561 -4.1325917 -4.1744752 -4.2068768 -4.2283325 -4.2439437 -4.248301 -4.2535038 -4.261157][-4.2127414 -4.2017708 -4.1760592 -4.1398106 -4.1105862 -4.10344 -4.1231232 -4.1614037 -4.2000446 -4.2293139 -4.2449927 -4.2511716 -4.2470183 -4.2410078 -4.2390757][-4.2011123 -4.1945105 -4.1777697 -4.1557236 -4.1441183 -4.1485324 -4.1654153 -4.193965 -4.2233419 -4.2434335 -4.2521815 -4.2517705 -4.2429295 -4.2289991 -4.2206888][-4.194418 -4.1919713 -4.1863551 -4.1799016 -4.1789317 -4.184494 -4.1953707 -4.214807 -4.233057 -4.2422485 -4.2445369 -4.2429419 -4.2360172 -4.224174 -4.2156644]]...]
INFO - root - 2017-12-07 21:10:42.584104: step 54510, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 55h:14m:51s remains)
INFO - root - 2017-12-07 21:10:49.486283: step 54520, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 54h:00m:27s remains)
INFO - root - 2017-12-07 21:10:56.270115: step 54530, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 49h:36m:03s remains)
INFO - root - 2017-12-07 21:11:03.068643: step 54540, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 50h:12m:44s remains)
INFO - root - 2017-12-07 21:11:09.993815: step 54550, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 57h:15m:32s remains)
INFO - root - 2017-12-07 21:11:16.898340: step 54560, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 55h:23m:56s remains)
INFO - root - 2017-12-07 21:11:23.732665: step 54570, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 53h:48m:43s remains)
INFO - root - 2017-12-07 21:11:30.598765: step 54580, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 50h:30m:13s remains)
INFO - root - 2017-12-07 21:11:37.484480: step 54590, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 51h:08m:35s remains)
INFO - root - 2017-12-07 21:11:44.400957: step 54600, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.753 sec/batch; 58h:06m:56s remains)
2017-12-07 21:11:45.111384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3095007 -4.3012891 -4.2891617 -4.2754946 -4.259562 -4.2467341 -4.2402534 -4.2383595 -4.2464008 -4.2711973 -4.2998748 -4.3241754 -4.3389015 -4.345345 -4.3446984][-4.2901578 -4.277339 -4.2557335 -4.2326956 -4.2090826 -4.1924272 -4.18818 -4.1930509 -4.2131767 -4.2515545 -4.2907424 -4.3192406 -4.3345337 -4.3395963 -4.335505][-4.2617497 -4.2450047 -4.2189622 -4.1906538 -4.1655912 -4.1494713 -4.1482849 -4.1583724 -4.190011 -4.2413697 -4.2856836 -4.3150878 -4.3302546 -4.3335018 -4.3280382][-4.2315245 -4.2103958 -4.1853585 -4.1594691 -4.1387668 -4.1267433 -4.1276131 -4.1456556 -4.1866388 -4.239058 -4.2824764 -4.3107033 -4.3239741 -4.3274412 -4.3220015][-4.2061076 -4.1807413 -4.1557884 -4.1304293 -4.1083293 -4.100759 -4.1095715 -4.140151 -4.1903276 -4.2404108 -4.2816505 -4.30799 -4.3192987 -4.3226509 -4.3168721][-4.1881118 -4.1569562 -4.1221466 -4.0845327 -4.0536962 -4.0481024 -4.070117 -4.1181407 -4.1802373 -4.2354264 -4.2765722 -4.3015423 -4.313849 -4.3181858 -4.3117604][-4.1701674 -4.1396518 -4.091651 -4.0344954 -3.99308 -3.9776595 -4.0023174 -4.06626 -4.1451874 -4.2124443 -4.2614388 -4.2899008 -4.3041353 -4.3091946 -4.3037367][-4.1610951 -4.1367807 -4.0838661 -4.0170012 -3.9643462 -3.9331007 -3.9505644 -4.0240531 -4.120656 -4.1984978 -4.250813 -4.281126 -4.2945986 -4.3009243 -4.2990522][-4.1613536 -4.1371632 -4.0813818 -4.014894 -3.9609706 -3.9284806 -3.9462488 -4.0225673 -4.1237831 -4.2013307 -4.249999 -4.2781625 -4.2916894 -4.3002176 -4.299758][-4.1775365 -4.1441374 -4.08586 -4.0242505 -3.9746757 -3.9486139 -3.9758747 -4.0498648 -4.1439581 -4.2149596 -4.2565737 -4.2812176 -4.2955036 -4.3035331 -4.3010488][-4.2044311 -4.1626687 -4.1076584 -4.0547504 -4.0105042 -3.9915009 -4.0250955 -4.0927815 -4.1730509 -4.2349391 -4.2711635 -4.29443 -4.3060517 -4.3082461 -4.3048882][-4.2334428 -4.1885858 -4.1435943 -4.1045561 -4.0698476 -4.058938 -4.0896215 -4.1485324 -4.2141895 -4.2631025 -4.294127 -4.3172474 -4.32601 -4.3212194 -4.3156071][-4.2622261 -4.2213073 -4.1880474 -4.1610036 -4.1357083 -4.131629 -4.1582279 -4.2064676 -4.2583184 -4.2966328 -4.32042 -4.3399515 -4.34565 -4.3360496 -4.3275003][-4.286581 -4.2539268 -4.2311316 -4.212451 -4.19467 -4.1953058 -4.2182093 -4.2539344 -4.2925997 -4.324451 -4.3442106 -4.3584371 -4.3605022 -4.3486533 -4.3383551][-4.3000469 -4.2776351 -4.2646794 -4.2543173 -4.24385 -4.2448759 -4.259418 -4.2823534 -4.3108821 -4.3369846 -4.3526173 -4.3628111 -4.3629065 -4.3525562 -4.344574]]...]
INFO - root - 2017-12-07 21:11:51.758460: step 54610, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 52h:01m:51s remains)
INFO - root - 2017-12-07 21:11:58.588352: step 54620, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 52h:50m:40s remains)
INFO - root - 2017-12-07 21:12:05.477474: step 54630, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.731 sec/batch; 56h:23m:04s remains)
INFO - root - 2017-12-07 21:12:12.272246: step 54640, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.678 sec/batch; 52h:20m:15s remains)
INFO - root - 2017-12-07 21:12:18.972841: step 54650, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 48h:41m:43s remains)
INFO - root - 2017-12-07 21:12:25.770932: step 54660, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 49h:26m:20s remains)
INFO - root - 2017-12-07 21:12:32.635998: step 54670, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 57h:37m:59s remains)
INFO - root - 2017-12-07 21:12:39.447949: step 54680, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 55h:36m:02s remains)
INFO - root - 2017-12-07 21:12:46.153461: step 54690, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 51h:57m:52s remains)
INFO - root - 2017-12-07 21:12:52.918755: step 54700, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 50h:55m:06s remains)
2017-12-07 21:12:53.688466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.255209 -4.2562389 -4.2612696 -4.2589073 -4.247438 -4.2337003 -4.2277985 -4.2383657 -4.2665825 -4.290215 -4.2940936 -4.28466 -4.2643623 -4.2562065 -4.24742][-4.2519479 -4.2481627 -4.2505851 -4.250371 -4.2405586 -4.2265878 -4.2222214 -4.2349148 -4.2620635 -4.2835279 -4.2864494 -4.2740946 -4.2495866 -4.2412286 -4.2396221][-4.2392855 -4.2300329 -4.2257848 -4.2283387 -4.2245193 -4.213109 -4.2089262 -4.2264729 -4.2539835 -4.2714467 -4.2707548 -4.2556534 -4.2283716 -4.2163577 -4.2167044][-4.2176332 -4.2007656 -4.1972852 -4.20501 -4.2101927 -4.2065105 -4.2076182 -4.2279096 -4.2539482 -4.2636795 -4.2567492 -4.2328806 -4.2026358 -4.1900992 -4.1922226][-4.1867585 -4.1648254 -4.1646209 -4.1788297 -4.19347 -4.1993413 -4.2031393 -4.2203512 -4.24023 -4.2453947 -4.2296247 -4.1946921 -4.16908 -4.1714196 -4.1784673][-4.16318 -4.1442323 -4.1490493 -4.164413 -4.1826482 -4.1906581 -4.1931024 -4.2012725 -4.2116709 -4.2130027 -4.1916485 -4.1561961 -4.1384683 -4.1545749 -4.1657124][-4.1336451 -4.1287923 -4.1445518 -4.1624513 -4.182025 -4.1876025 -4.1847806 -4.1827536 -4.1852188 -4.1832004 -4.1602397 -4.1247611 -4.1092634 -4.13334 -4.1430597][-4.1203785 -4.12274 -4.1418438 -4.1606994 -4.1763353 -4.177002 -4.1663094 -4.1536231 -4.1496277 -4.1491375 -4.1343541 -4.1052523 -4.0908041 -4.113246 -4.119854][-4.1505194 -4.1584182 -4.1798186 -4.1983304 -4.2065177 -4.1977038 -4.175952 -4.148634 -4.1334887 -4.1273694 -4.1191769 -4.1044097 -4.0974269 -4.1171074 -4.1211853][-4.1905718 -4.191442 -4.2039981 -4.2206397 -4.2282877 -4.2199192 -4.2016854 -4.1753516 -4.1521873 -4.1357131 -4.1297088 -4.1289816 -4.1297579 -4.1483479 -4.1519527][-4.2522173 -4.24366 -4.24525 -4.249711 -4.2503195 -4.2422295 -4.2278032 -4.2096176 -4.190424 -4.1685119 -4.1575475 -4.1601987 -4.1627374 -4.178688 -4.1811485][-4.2940497 -4.28459 -4.2781096 -4.2736816 -4.2701025 -4.2643061 -4.2580929 -4.2517643 -4.2414069 -4.2239418 -4.2126679 -4.2117672 -4.2067304 -4.2132497 -4.2137008][-4.3114409 -4.302712 -4.2919016 -4.2820153 -4.2765403 -4.2725215 -4.2738142 -4.2748337 -4.2688479 -4.25589 -4.2483582 -4.2438908 -4.2324042 -4.228209 -4.2234397][-4.3102517 -4.3003569 -4.2866631 -4.2745466 -4.2674537 -4.2656407 -4.2742805 -4.2827725 -4.27853 -4.267261 -4.25932 -4.2538824 -4.24341 -4.2374969 -4.2295294][-4.3069263 -4.2970982 -4.2816029 -4.26883 -4.26068 -4.2596035 -4.2698722 -4.2795787 -4.2769914 -4.2668805 -4.2608151 -4.2572284 -4.2500796 -4.2481585 -4.2425723]]...]
INFO - root - 2017-12-07 21:13:00.338331: step 54710, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 53h:52m:40s remains)
INFO - root - 2017-12-07 21:13:07.170326: step 54720, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 50h:24m:26s remains)
INFO - root - 2017-12-07 21:13:13.907504: step 54730, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 50h:03m:47s remains)
INFO - root - 2017-12-07 21:13:20.655135: step 54740, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 51h:55m:21s remains)
INFO - root - 2017-12-07 21:13:27.472798: step 54750, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 55h:04m:44s remains)
INFO - root - 2017-12-07 21:13:34.214446: step 54760, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 53h:58m:05s remains)
INFO - root - 2017-12-07 21:13:41.092927: step 54770, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 54h:13m:11s remains)
INFO - root - 2017-12-07 21:13:47.912916: step 54780, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 49h:03m:00s remains)
INFO - root - 2017-12-07 21:13:54.745980: step 54790, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 51h:32m:12s remains)
INFO - root - 2017-12-07 21:14:01.618188: step 54800, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.724 sec/batch; 55h:52m:54s remains)
2017-12-07 21:14:02.325583: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2256322 -4.2107282 -4.2097726 -4.22459 -4.2321844 -4.2265258 -4.2176204 -4.2128005 -4.2195354 -4.2193875 -4.2189236 -4.2220616 -4.2227607 -4.2142296 -4.2077913][-4.2207837 -4.2080593 -4.2047806 -4.2155981 -4.2170515 -4.2051034 -4.1918736 -4.1866364 -4.1944008 -4.1965537 -4.19584 -4.1991825 -4.1986647 -4.186934 -4.1755877][-4.2228088 -4.2195439 -4.2186842 -4.2239027 -4.2171307 -4.1972656 -4.1807375 -4.1745262 -4.1856256 -4.190033 -4.1894684 -4.1906543 -4.1861348 -4.1702819 -4.1553531][-4.2211037 -4.2320218 -4.2422819 -4.2459393 -4.231 -4.2055659 -4.1871228 -4.1801143 -4.1925325 -4.201426 -4.2011466 -4.1971092 -4.1890979 -4.1759577 -4.1683779][-4.1881876 -4.2127542 -4.2338715 -4.2399335 -4.2238417 -4.1994925 -4.1859875 -4.1816754 -4.1962252 -4.2108169 -4.2162642 -4.2147741 -4.2103748 -4.209 -4.2139091][-4.1156173 -4.1511788 -4.1829844 -4.1970954 -4.1854324 -4.1703472 -4.1687064 -4.1717896 -4.1847272 -4.2043762 -4.220613 -4.2255592 -4.2298613 -4.2405996 -4.2538137][-4.0301366 -4.0695014 -4.1095061 -4.1294932 -4.1235666 -4.1219072 -4.1367579 -4.150959 -4.1648316 -4.1878343 -4.2128878 -4.2287292 -4.2428174 -4.2584033 -4.2717118][-4.004117 -4.0238547 -4.0543489 -4.06849 -4.0620732 -4.07133 -4.0988312 -4.1235127 -4.1440077 -4.1721983 -4.2030182 -4.2238398 -4.2395625 -4.2571859 -4.2682724][-4.0549779 -4.0542488 -4.0659513 -4.0687671 -4.05847 -4.0665712 -4.0930009 -4.1154084 -4.1343141 -4.1593184 -4.1844211 -4.1983767 -4.2095146 -4.2302608 -4.242743][-4.1196589 -4.1154947 -4.1179447 -4.117147 -4.1111979 -4.1141148 -4.126286 -4.1336956 -4.1412086 -4.1559324 -4.1606369 -4.1557574 -4.1583323 -4.1800528 -4.1941113][-4.173161 -4.1732206 -4.174304 -4.1719456 -4.1689305 -4.1669316 -4.1661234 -4.1623321 -4.1618347 -4.1632895 -4.1455622 -4.1184859 -4.108839 -4.1257668 -4.1383667][-4.2206364 -4.2224941 -4.2222738 -4.2149925 -4.208137 -4.2008815 -4.193779 -4.1877313 -4.1830268 -4.1739254 -4.1418715 -4.0966063 -4.0722051 -4.0768986 -4.0837736][-4.2522206 -4.2541118 -4.2535725 -4.2436528 -4.2324553 -4.2223959 -4.2119722 -4.2062078 -4.1991525 -4.1864877 -4.1500373 -4.0981374 -4.06486 -4.0571671 -4.0565834][-4.264956 -4.2652483 -4.2615533 -4.2504387 -4.2396226 -4.2286329 -4.2197714 -4.2181926 -4.2140908 -4.2054772 -4.1766071 -4.1316881 -4.1024213 -4.0920391 -4.0918217][-4.2755475 -4.2741776 -4.2667894 -4.25456 -4.2436886 -4.2305007 -4.2189779 -4.221045 -4.2238822 -4.2236714 -4.2087936 -4.1824856 -4.1663055 -4.1603966 -4.161541]]...]
INFO - root - 2017-12-07 21:14:08.896890: step 54810, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 49h:23m:25s remains)
INFO - root - 2017-12-07 21:14:15.248330: step 54820, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 49h:22m:17s remains)
INFO - root - 2017-12-07 21:14:22.035319: step 54830, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 51h:49m:50s remains)
INFO - root - 2017-12-07 21:14:28.887975: step 54840, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 53h:22m:21s remains)
INFO - root - 2017-12-07 21:14:35.583147: step 54850, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 51h:15m:59s remains)
INFO - root - 2017-12-07 21:14:42.364658: step 54860, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 49h:45m:01s remains)
INFO - root - 2017-12-07 21:14:49.310924: step 54870, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 52h:15m:04s remains)
INFO - root - 2017-12-07 21:14:56.053394: step 54880, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 52h:59m:23s remains)
INFO - root - 2017-12-07 21:15:02.874796: step 54890, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 52h:29m:00s remains)
INFO - root - 2017-12-07 21:15:09.610054: step 54900, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 51h:01m:47s remains)
2017-12-07 21:15:10.356332: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16285 -4.1788454 -4.1916966 -4.1983066 -4.2111826 -4.2271695 -4.2312984 -4.2086577 -4.1736073 -4.14699 -4.1226163 -4.1094685 -4.1124067 -4.11208 -4.1068511][-4.1763644 -4.1901846 -4.1999669 -4.195755 -4.1965184 -4.2013516 -4.1926355 -4.1653671 -4.1324425 -4.1065874 -4.0858727 -4.0833893 -4.1001863 -4.1119452 -4.1121826][-4.193192 -4.2019858 -4.202714 -4.1832123 -4.1726127 -4.1695089 -4.1534019 -4.1279578 -4.1043692 -4.0879331 -4.077003 -4.0820446 -4.1028833 -4.1197014 -4.1213965][-4.1869345 -4.1946359 -4.1933117 -4.1693792 -4.1506906 -4.1395669 -4.1141505 -4.082859 -4.0709982 -4.0694113 -4.0627866 -4.0637631 -4.07944 -4.0957637 -4.09661][-4.1518035 -4.1594028 -4.1607695 -4.1373644 -4.1075811 -4.0762858 -4.0245008 -3.9793718 -3.9885488 -4.0195227 -4.0272808 -4.0303373 -4.0433383 -4.0530744 -4.0479918][-4.1264529 -4.128624 -4.1274881 -4.102623 -4.0619268 -4.0035334 -3.9163127 -3.8594022 -3.9106977 -3.9917278 -4.0252342 -4.038291 -4.0545397 -4.0551958 -4.0373545][-4.1180487 -4.1137929 -4.1112595 -4.0908756 -4.058815 -4.0050507 -3.9269848 -3.8915105 -3.962873 -4.0528383 -4.0934811 -4.107224 -4.1163268 -4.1064792 -4.0817456][-4.1252666 -4.1154747 -4.1172538 -4.1165681 -4.1137619 -4.0905027 -4.0489931 -4.0362535 -4.0901051 -4.1480155 -4.1726532 -4.178143 -4.1745191 -4.1565356 -4.1370697][-4.1368661 -4.1290426 -4.1390986 -4.1522484 -4.1732879 -4.1734457 -4.1585884 -4.1595864 -4.1945415 -4.2233729 -4.2316251 -4.2282138 -4.2184234 -4.2001572 -4.1886082][-4.1287665 -4.1276 -4.1477828 -4.1665173 -4.1972575 -4.217093 -4.2227073 -4.2297111 -4.2522521 -4.2657695 -4.2670341 -4.26099 -4.2499442 -4.2366991 -4.2296438][-4.1267767 -4.1318955 -4.1586256 -4.1825876 -4.2165704 -4.2465315 -4.2603493 -4.2669663 -4.2813754 -4.2898746 -4.289125 -4.2845435 -4.2764659 -4.2667742 -4.2608786][-4.1546249 -4.1648922 -4.1958427 -4.2225833 -4.2515616 -4.2736135 -4.2820792 -4.28554 -4.2972937 -4.3062339 -4.3073721 -4.3046031 -4.298388 -4.2899938 -4.2845831][-4.2066374 -4.220365 -4.2497311 -4.2749467 -4.2949262 -4.3018489 -4.2980084 -4.2964544 -4.3053608 -4.315382 -4.3176622 -4.3153677 -4.3104415 -4.3027806 -4.2955952][-4.2538233 -4.27128 -4.2979736 -4.316987 -4.3257675 -4.3178506 -4.3060656 -4.3010154 -4.3063383 -4.3147306 -4.3185554 -4.3183856 -4.3145 -4.3078446 -4.2996988][-4.2710481 -4.290556 -4.3146105 -4.3259816 -4.3267546 -4.3155727 -4.3054333 -4.3013759 -4.3044906 -4.309732 -4.314445 -4.3167105 -4.3158894 -4.3116779 -4.3009863]]...]
INFO - root - 2017-12-07 21:15:16.922529: step 54910, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 55h:56m:33s remains)
INFO - root - 2017-12-07 21:15:23.864183: step 54920, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 55h:31m:15s remains)
INFO - root - 2017-12-07 21:15:30.688393: step 54930, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 51h:41m:36s remains)
INFO - root - 2017-12-07 21:15:37.582631: step 54940, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 50h:56m:35s remains)
INFO - root - 2017-12-07 21:15:44.437148: step 54950, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 54h:15m:17s remains)
INFO - root - 2017-12-07 21:15:51.305967: step 54960, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 55h:26m:44s remains)
INFO - root - 2017-12-07 21:15:57.964342: step 54970, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 50h:57m:37s remains)
INFO - root - 2017-12-07 21:16:04.722822: step 54980, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 51h:15m:42s remains)
INFO - root - 2017-12-07 21:16:11.624202: step 54990, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 50h:44m:48s remains)
INFO - root - 2017-12-07 21:16:18.419387: step 55000, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 53h:17m:37s remains)
2017-12-07 21:16:19.140636: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2952566 -4.29772 -4.2901578 -4.2612066 -4.2279277 -4.2025161 -4.1837211 -4.1807108 -4.1920867 -4.21359 -4.2301736 -4.2419791 -4.2570505 -4.2730503 -4.2762132][-4.2967119 -4.298471 -4.288301 -4.2549281 -4.2121711 -4.17432 -4.1502876 -4.1500926 -4.1658092 -4.1913185 -4.209506 -4.2171068 -4.2300134 -4.2502012 -4.2608476][-4.2943993 -4.2937541 -4.2801323 -4.2447081 -4.1953869 -4.149755 -4.1239471 -4.1245208 -4.1428471 -4.1761422 -4.1975427 -4.1998963 -4.2036443 -4.2207751 -4.234498][-4.295774 -4.2916269 -4.2739992 -4.2388144 -4.188169 -4.1414804 -4.116169 -4.1113858 -4.1271672 -4.1637106 -4.1846814 -4.1822309 -4.1824012 -4.1984987 -4.2134657][-4.2965584 -4.2919383 -4.2723303 -4.2379427 -4.191678 -4.1481719 -4.1202226 -4.1054049 -4.115788 -4.15084 -4.1684923 -4.1605506 -4.1613955 -4.1754336 -4.19136][-4.293643 -4.2948794 -4.27787 -4.2442055 -4.2003818 -4.1578407 -4.1228018 -4.0952463 -4.094121 -4.1269846 -4.1478081 -4.1460776 -4.1512513 -4.1627121 -4.1780438][-4.2888856 -4.2967472 -4.2828875 -4.2488403 -4.2036753 -4.1573386 -4.1074462 -4.057971 -4.0373321 -4.0746317 -4.11812 -4.1364031 -4.1509423 -4.1615715 -4.1743631][-4.2790556 -4.2902651 -4.2784252 -4.2462006 -4.2018709 -4.1534157 -4.0878849 -4.0117936 -3.96621 -4.0092163 -4.0871119 -4.1369905 -4.1636553 -4.1726913 -4.180645][-4.2669992 -4.2802815 -4.2712927 -4.2417641 -4.2001653 -4.1529746 -4.0852013 -3.9996181 -3.937151 -3.9730413 -4.0677633 -4.1432796 -4.1855783 -4.1966128 -4.2006311][-4.2601657 -4.2707438 -4.2628121 -4.238349 -4.2040381 -4.1657634 -4.1153655 -4.0510321 -3.9995856 -4.0186291 -4.094099 -4.1660395 -4.2098541 -4.2206593 -4.222486][-4.2636595 -4.2702918 -4.2597036 -4.235559 -4.2052469 -4.1741762 -4.1429815 -4.1052818 -4.06941 -4.0736489 -4.1221561 -4.1785645 -4.2196941 -4.237083 -4.2461543][-4.2647128 -4.2693377 -4.2579384 -4.2331076 -4.2018046 -4.1745672 -4.1567879 -4.1369877 -4.1137614 -4.1096997 -4.1387439 -4.1820674 -4.2225919 -4.2491884 -4.2663069][-4.2564831 -4.2610655 -4.2546906 -4.2358804 -4.2055416 -4.1762052 -4.1597962 -4.1477776 -4.1348934 -4.132565 -4.1531115 -4.1908407 -4.2301922 -4.259985 -4.281363][-4.23656 -4.240272 -4.2392569 -4.2316437 -4.2103395 -4.1848106 -4.16996 -4.1628237 -4.1600151 -4.1636095 -4.1825266 -4.2173529 -4.2522392 -4.278471 -4.2975516][-4.2182364 -4.2165937 -4.2182474 -4.2213373 -4.2136192 -4.197454 -4.1887197 -4.1863847 -4.19203 -4.2027669 -4.2220683 -4.2515306 -4.2791796 -4.3005385 -4.31622]]...]
INFO - root - 2017-12-07 21:16:25.823167: step 55010, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 53h:27m:42s remains)
INFO - root - 2017-12-07 21:16:32.701929: step 55020, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.637 sec/batch; 49h:06m:53s remains)
INFO - root - 2017-12-07 21:16:39.469152: step 55030, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 54h:02m:02s remains)
INFO - root - 2017-12-07 21:16:46.336446: step 55040, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 55h:00m:48s remains)
INFO - root - 2017-12-07 21:16:53.112211: step 55050, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 54h:37m:35s remains)
INFO - root - 2017-12-07 21:16:59.797811: step 55060, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.624 sec/batch; 48h:03m:55s remains)
INFO - root - 2017-12-07 21:17:06.601463: step 55070, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 50h:33m:26s remains)
INFO - root - 2017-12-07 21:17:13.374642: step 55080, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.732 sec/batch; 56h:25m:51s remains)
INFO - root - 2017-12-07 21:17:20.226015: step 55090, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 54h:30m:13s remains)
INFO - root - 2017-12-07 21:17:26.956863: step 55100, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 52h:43m:39s remains)
2017-12-07 21:17:27.650734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2846403 -4.273726 -4.26659 -4.2625771 -4.2641873 -4.2712593 -4.2742538 -4.2746649 -4.2765651 -4.2773547 -4.2791953 -4.2856984 -4.2939191 -4.3020267 -4.3084884][-4.2568312 -4.2417192 -4.2320094 -4.2285089 -4.234344 -4.2486477 -4.2545915 -4.2561121 -4.2609992 -4.2615132 -4.2632856 -4.2722359 -4.28278 -4.2930708 -4.3012204][-4.2232022 -4.205811 -4.1990051 -4.2002163 -4.2129955 -4.2348652 -4.2405968 -4.2379594 -4.2414951 -4.2400403 -4.2432818 -4.2555265 -4.2681713 -4.2800913 -4.2882614][-4.1867461 -4.1712 -4.1694822 -4.1763573 -4.1949553 -4.2182226 -4.2199473 -4.2062263 -4.2033186 -4.1975107 -4.203733 -4.223289 -4.2410321 -4.2573895 -4.2674556][-4.1546183 -4.1406593 -4.1417155 -4.1495624 -4.1680503 -4.18545 -4.1761379 -4.1496592 -4.1406827 -4.13883 -4.1521635 -4.1796618 -4.20345 -4.2239304 -4.2386637][-4.1290016 -4.1113987 -4.1094246 -4.1117992 -4.1161194 -4.1239777 -4.10278 -4.0596132 -4.0444083 -4.0532932 -4.0783048 -4.1159987 -4.1514697 -4.1838255 -4.2070575][-4.1125884 -4.0837488 -4.06934 -4.0593863 -4.0537944 -4.0542436 -4.01335 -3.9425282 -3.9238932 -3.9587648 -4.0112991 -4.06787 -4.1167374 -4.1582661 -4.191195][-4.1066332 -4.0674062 -4.042923 -4.0256872 -4.0049038 -3.9791956 -3.9060369 -3.8099504 -3.8103681 -3.8912239 -3.9767685 -4.0520554 -4.1122622 -4.1589179 -4.1957521][-4.1094217 -4.0678158 -4.0437207 -4.0193439 -3.9808147 -3.9333868 -3.8485365 -3.756979 -3.7876992 -3.8908877 -3.9877791 -4.0703492 -4.1319323 -4.1807251 -4.209383][-4.1224089 -4.0805459 -4.0546403 -4.0243835 -3.9773066 -3.9378083 -3.8820167 -3.8238323 -3.8716903 -3.9675553 -4.0480027 -4.1209903 -4.1749792 -4.2116876 -4.2260971][-4.1503186 -4.1160417 -4.0954218 -4.0716839 -4.0400043 -4.0245776 -3.9996393 -3.9623673 -4.002924 -4.0783877 -4.1377678 -4.1890507 -4.2242813 -4.24458 -4.2473316][-4.1750178 -4.153759 -4.1465034 -4.1401653 -4.1294913 -4.1319332 -4.1196718 -4.0904031 -4.1217766 -4.1794424 -4.2210093 -4.2531772 -4.2739296 -4.2793984 -4.2722344][-4.1905713 -4.1837721 -4.1916761 -4.1996274 -4.2065616 -4.216558 -4.2047391 -4.1801696 -4.2023349 -4.2418957 -4.2710958 -4.2956877 -4.3120017 -4.3130836 -4.2995591][-4.2041245 -4.2096519 -4.2328095 -4.2510223 -4.2641225 -4.2731633 -4.2625928 -4.2427607 -4.2580681 -4.2856231 -4.3061171 -4.3220758 -4.335886 -4.3363719 -4.321393][-4.2275076 -4.2445526 -4.2746987 -4.2928028 -4.2995281 -4.3029 -4.2936888 -4.2792153 -4.2876782 -4.3067241 -4.3222432 -4.3339362 -4.3433952 -4.341516 -4.3267274]]...]
INFO - root - 2017-12-07 21:17:34.268963: step 55110, loss = 2.03, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 54h:40m:01s remains)
INFO - root - 2017-12-07 21:17:41.039086: step 55120, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 53h:18m:57s remains)
INFO - root - 2017-12-07 21:17:47.559276: step 55130, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 51h:40m:54s remains)
INFO - root - 2017-12-07 21:17:54.319531: step 55140, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 51h:46m:11s remains)
INFO - root - 2017-12-07 21:18:01.123913: step 55150, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 51h:59m:06s remains)
INFO - root - 2017-12-07 21:18:07.898948: step 55160, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 52h:14m:03s remains)
INFO - root - 2017-12-07 21:18:14.771053: step 55170, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.745 sec/batch; 57h:22m:48s remains)
INFO - root - 2017-12-07 21:18:21.538375: step 55180, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 50h:00m:16s remains)
INFO - root - 2017-12-07 21:18:28.285584: step 55190, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 50h:31m:36s remains)
INFO - root - 2017-12-07 21:18:35.077450: step 55200, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 53h:31m:41s remains)
2017-12-07 21:18:35.820821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22087 -4.2200222 -4.2205386 -4.2202129 -4.2208948 -4.22123 -4.2204757 -4.2207265 -4.2221093 -4.2246084 -4.229403 -4.2373991 -4.2514992 -4.2710972 -4.2883487][-4.1919737 -4.198195 -4.2070541 -4.215147 -4.2237639 -4.2295232 -4.2307978 -4.230866 -4.231709 -4.2319751 -4.2324314 -4.2329445 -4.2382307 -4.251493 -4.2661395][-4.1658697 -4.1771121 -4.1927371 -4.2084031 -4.2238293 -4.2346249 -4.2387104 -4.2404594 -4.2446275 -4.2469993 -4.2462397 -4.2410269 -4.2359862 -4.2363892 -4.2408614][-4.1644597 -4.1748037 -4.1889009 -4.2026625 -4.2154331 -4.2239919 -4.2274075 -4.2326632 -4.2462487 -4.2581053 -4.2632289 -4.2586436 -4.2472143 -4.2344537 -4.2248569][-4.1731362 -4.1739974 -4.1783481 -4.1821389 -4.184267 -4.182785 -4.1808977 -4.1901927 -4.2157288 -4.240128 -4.2550182 -4.2566624 -4.2452474 -4.2259727 -4.2065406][-4.1625395 -4.1504884 -4.1397252 -4.1266465 -4.1093636 -4.0871053 -4.0709748 -4.083147 -4.1264544 -4.17256 -4.2053795 -4.2201304 -4.2138824 -4.1918426 -4.16615][-4.1374865 -4.1167521 -4.0950437 -4.0688429 -4.0317888 -3.9810262 -3.9393537 -3.9441862 -4.0013013 -4.0714331 -4.1268945 -4.158936 -4.1626825 -4.1429124 -4.1154432][-4.1351819 -4.1155944 -4.0979147 -4.0773177 -4.0436783 -3.9901218 -3.9401124 -3.9305947 -3.972893 -4.0364332 -4.0937138 -4.1337919 -4.1486144 -4.1388969 -4.1184196][-4.1645408 -4.1507797 -4.1430283 -4.1361475 -4.1210332 -4.089941 -4.0594907 -4.0519738 -4.075561 -4.1132627 -4.148962 -4.1760211 -4.1881852 -4.1848574 -4.1736565][-4.1977725 -4.186687 -4.184268 -4.1854291 -4.1836524 -4.1717238 -4.1596518 -4.1588988 -4.1738882 -4.1952739 -4.2138228 -4.2261195 -4.2296247 -4.2243428 -4.21444][-4.2235909 -4.2141461 -4.2120352 -4.2152648 -4.2189794 -4.2172904 -4.21535 -4.2177749 -4.227273 -4.2381287 -4.244534 -4.2459216 -4.241745 -4.2319736 -4.2194967][-4.2246709 -4.2187405 -4.2174282 -4.2210884 -4.2268953 -4.2308636 -4.2353935 -4.2403979 -4.2466445 -4.2511239 -4.2500057 -4.2433352 -4.2326517 -4.2211123 -4.2099581][-4.20962 -4.2053332 -4.2039762 -4.2063909 -4.2111559 -4.2166533 -4.2237692 -4.2308064 -4.2357836 -4.2358017 -4.2287679 -4.2160978 -4.2011909 -4.1908207 -4.1872211][-4.2018976 -4.1974063 -4.19477 -4.1943145 -4.1958933 -4.19938 -4.2053428 -4.2117476 -4.2154961 -4.2134643 -4.2036843 -4.1877756 -4.1693387 -4.159277 -4.1602635][-4.2081289 -4.2013035 -4.1966376 -4.193656 -4.193696 -4.1969175 -4.2026963 -4.2088318 -4.2123904 -4.2100196 -4.2006083 -4.185359 -4.1663752 -4.1546745 -4.153873]]...]
INFO - root - 2017-12-07 21:18:42.348253: step 55210, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 49h:55m:34s remains)
INFO - root - 2017-12-07 21:18:49.171938: step 55220, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 52h:06m:28s remains)
INFO - root - 2017-12-07 21:18:55.919757: step 55230, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 55h:06m:31s remains)
INFO - root - 2017-12-07 21:19:02.720480: step 55240, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 55h:39m:51s remains)
INFO - root - 2017-12-07 21:19:09.543939: step 55250, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 53h:53m:33s remains)
INFO - root - 2017-12-07 21:19:16.331926: step 55260, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.635 sec/batch; 48h:53m:03s remains)
INFO - root - 2017-12-07 21:19:23.193477: step 55270, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 51h:16m:25s remains)
INFO - root - 2017-12-07 21:19:30.081936: step 55280, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 56h:31m:29s remains)
INFO - root - 2017-12-07 21:19:36.966417: step 55290, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 54h:34m:37s remains)
INFO - root - 2017-12-07 21:19:43.774060: step 55300, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 53h:02m:08s remains)
2017-12-07 21:19:44.514045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2415829 -4.2472429 -4.2681661 -4.2878141 -4.2885952 -4.2665071 -4.2350245 -4.2123084 -4.2120996 -4.2240791 -4.2325487 -4.2323327 -4.2205505 -4.20958 -4.218327][-4.2235889 -4.2335682 -4.2567821 -4.2760048 -4.2731543 -4.2450285 -4.2074561 -4.1813669 -4.1817312 -4.1976371 -4.2067385 -4.2055197 -4.1916776 -4.1774049 -4.1867776][-4.1976695 -4.2123666 -4.236989 -4.2544889 -4.2485061 -4.21726 -4.179316 -4.1542554 -4.1561465 -4.1766057 -4.1861267 -4.1832457 -4.1648755 -4.1445494 -4.1523438][-4.1669483 -4.1878037 -4.2157192 -4.2306581 -4.2215867 -4.1877823 -4.1504393 -4.1276374 -4.1335039 -4.1607575 -4.1717916 -4.1663432 -4.1419034 -4.1169677 -4.1249528][-4.1428413 -4.1680374 -4.1977644 -4.2100148 -4.1973605 -4.1604595 -4.1218185 -4.0985327 -4.1077571 -4.1415148 -4.1583338 -4.1542344 -4.1279817 -4.100894 -4.1093693][-4.1171989 -4.1436372 -4.1741767 -4.1872144 -4.176487 -4.1382704 -4.0981946 -4.0739527 -4.0872874 -4.1261196 -4.1492443 -4.1482391 -4.1223078 -4.0945849 -4.1031446][-4.0962496 -4.1210127 -4.15311 -4.1724238 -4.1674433 -4.1284127 -4.0828509 -4.0544376 -4.0712404 -4.1159148 -4.146781 -4.15105 -4.1260495 -4.0971045 -4.1059403][-4.0904846 -4.1141171 -4.1503711 -4.1760788 -4.172596 -4.1270638 -4.0677624 -4.0279942 -4.0438919 -4.0954657 -4.1369333 -4.1489882 -4.1279855 -4.1002 -4.1109805][-4.0920639 -4.118865 -4.1594677 -4.1867204 -4.1798592 -4.1282396 -4.0573874 -4.0068641 -4.0205617 -4.0768952 -4.1261883 -4.1457157 -4.1293907 -4.1034846 -4.1170197][-4.0868173 -4.1157875 -4.1574273 -4.1844306 -4.1760516 -4.1262975 -4.0571547 -4.0066171 -4.0192142 -4.0759597 -4.1255288 -4.1467118 -4.1309361 -4.1051226 -4.122757][-4.0810461 -4.1093197 -4.1491151 -4.1751289 -4.1675982 -4.1275539 -4.0721097 -4.0310187 -4.0406342 -4.0901041 -4.1330948 -4.1477413 -4.127491 -4.101747 -4.1246095][-4.0841289 -4.1076045 -4.1404386 -4.1651297 -4.1636472 -4.1374493 -4.0994735 -4.071054 -4.0773773 -4.114747 -4.1472955 -4.15261 -4.128336 -4.1038809 -4.1278591][-4.0876794 -4.1061678 -4.1304364 -4.1535578 -4.1586728 -4.1437168 -4.1192613 -4.1018362 -4.1075597 -4.1352558 -4.1590605 -4.1565533 -4.1300859 -4.1086369 -4.1300254][-4.0918546 -4.1082239 -4.1272807 -4.1483474 -4.1566057 -4.1499104 -4.1347694 -4.1248488 -4.1315751 -4.1524868 -4.1688824 -4.1594324 -4.1309013 -4.1110187 -4.1282825][-4.0953884 -4.11176 -4.1277595 -4.1465306 -4.1561584 -4.1573138 -4.1512671 -4.1489277 -4.1574841 -4.1721134 -4.1816688 -4.1655693 -4.1332588 -4.1122561 -4.1248474]]...]
INFO - root - 2017-12-07 21:19:51.203102: step 55310, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 55h:20m:51s remains)
INFO - root - 2017-12-07 21:19:57.967752: step 55320, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 54h:52m:40s remains)
INFO - root - 2017-12-07 21:20:04.701657: step 55330, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 53h:22m:37s remains)
INFO - root - 2017-12-07 21:20:11.465832: step 55340, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 49h:07m:05s remains)
INFO - root - 2017-12-07 21:20:18.119290: step 55350, loss = 2.03, batch loss = 1.97 (12.8 examples/sec; 0.626 sec/batch; 48h:09m:49s remains)
INFO - root - 2017-12-07 21:20:25.003433: step 55360, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 54h:47m:27s remains)
INFO - root - 2017-12-07 21:20:31.752880: step 55370, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 53h:15m:56s remains)
INFO - root - 2017-12-07 21:20:38.549507: step 55380, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 50h:33m:23s remains)
INFO - root - 2017-12-07 21:20:45.297264: step 55390, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 49h:46m:31s remains)
INFO - root - 2017-12-07 21:20:52.097979: step 55400, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 51h:09m:47s remains)
2017-12-07 21:20:52.989503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1797171 -4.1610193 -4.1440511 -4.1278758 -4.127492 -4.141768 -4.1761312 -4.2080412 -4.2273622 -4.2263341 -4.2110157 -4.1966853 -4.198297 -4.2113843 -4.2171335][-4.1842995 -4.1705375 -4.1538873 -4.13285 -4.1285729 -4.1399107 -4.1715531 -4.2024632 -4.224967 -4.2317009 -4.2258468 -4.2124023 -4.2051196 -4.207408 -4.2034788][-4.2017422 -4.1982994 -4.1866879 -4.1668949 -4.1592627 -4.16335 -4.182085 -4.2031341 -4.2268977 -4.24523 -4.2543039 -4.2465057 -4.234169 -4.2258611 -4.2103844][-4.22169 -4.224978 -4.2190776 -4.2027836 -4.1912136 -4.1826396 -4.1800957 -4.1826057 -4.2056608 -4.238658 -4.2637 -4.2691236 -4.2583652 -4.2441 -4.2203][-4.2477679 -4.2533426 -4.2469182 -4.229362 -4.2070823 -4.1730108 -4.1347303 -4.1092625 -4.1342478 -4.18851 -4.2340555 -4.2555985 -4.2540188 -4.2427254 -4.2199812][-4.260304 -4.2644329 -4.2561064 -4.2358813 -4.1982579 -4.1299634 -4.0488758 -3.9913189 -4.0219879 -4.1027246 -4.1746321 -4.2170644 -4.2300096 -4.2273822 -4.208673][-4.2469268 -4.251646 -4.2480497 -4.22805 -4.1794419 -4.0847864 -3.9679635 -3.8839781 -3.9187367 -4.0191731 -4.1097727 -4.1711259 -4.1975265 -4.2028656 -4.1861925][-4.2227306 -4.2317944 -4.2390103 -4.2277722 -4.1859765 -4.0997462 -3.9943242 -3.9145722 -3.9366634 -4.0193233 -4.0945525 -4.14765 -4.1760902 -4.1826553 -4.1669035][-4.2059174 -4.2185378 -4.2359748 -4.2388659 -4.2180305 -4.1647587 -4.1002426 -4.0466647 -4.0522742 -4.0959797 -4.1365275 -4.1657677 -4.1829481 -4.1835952 -4.1651564][-4.2159457 -4.2232475 -4.2396541 -4.2498784 -4.2461524 -4.2212639 -4.1906013 -4.1631222 -4.1638269 -4.1831336 -4.2003059 -4.2130508 -4.2174411 -4.207294 -4.1817164][-4.2310128 -4.2293019 -4.2370014 -4.2468762 -4.2540503 -4.2459793 -4.2346544 -4.2226596 -4.224371 -4.2338572 -4.2430172 -4.2515559 -4.251215 -4.2383633 -4.2137241][-4.2367587 -4.2303524 -4.2321539 -4.2410259 -4.253787 -4.2563877 -4.2530227 -4.2460213 -4.2462754 -4.2507524 -4.2586355 -4.2671733 -4.2684436 -4.2615118 -4.2446404][-4.2440138 -4.2378416 -4.2371407 -4.2437983 -4.2556958 -4.262641 -4.2624364 -4.2575731 -4.2555985 -4.2550464 -4.2599335 -4.2683387 -4.2727432 -4.2721248 -4.261405][-4.254642 -4.246489 -4.2423873 -4.2438951 -4.2498522 -4.2545767 -4.257051 -4.2541256 -4.2495728 -4.2450538 -4.2468996 -4.2527752 -4.2596664 -4.2635536 -4.2551894][-4.2506943 -4.2419481 -4.2380295 -4.2377219 -4.2369571 -4.2353554 -4.23546 -4.2293549 -4.21903 -4.2082462 -4.2058372 -4.2088561 -4.2167978 -4.2214446 -4.2108383]]...]
INFO - root - 2017-12-07 21:20:59.582650: step 55410, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 51h:32m:44s remains)
INFO - root - 2017-12-07 21:21:06.273505: step 55420, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 49h:23m:37s remains)
INFO - root - 2017-12-07 21:21:13.007883: step 55430, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 49h:35m:30s remains)
INFO - root - 2017-12-07 21:21:19.591317: step 55440, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 53h:06m:09s remains)
INFO - root - 2017-12-07 21:21:26.426605: step 55450, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 50h:52m:56s remains)
INFO - root - 2017-12-07 21:21:33.203288: step 55460, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 49h:35m:12s remains)
INFO - root - 2017-12-07 21:21:39.898459: step 55470, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 52h:05m:33s remains)
INFO - root - 2017-12-07 21:21:46.672277: step 55480, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 54h:13m:20s remains)
INFO - root - 2017-12-07 21:21:53.419103: step 55490, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 54h:10m:19s remains)
INFO - root - 2017-12-07 21:22:00.177794: step 55500, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 51h:49m:36s remains)
2017-12-07 21:22:01.072855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2549791 -4.2461228 -4.2521973 -4.2683282 -4.2832322 -4.2883492 -4.2774529 -4.2680397 -4.2732067 -4.2765951 -4.2743177 -4.2698865 -4.2674613 -4.2688875 -4.2712674][-4.2433639 -4.236361 -4.248733 -4.2697892 -4.2842984 -4.2863097 -4.2723103 -4.2604437 -4.2655821 -4.2696481 -4.2677917 -4.2634239 -4.255465 -4.2500944 -4.2561226][-4.2191739 -4.2186646 -4.2410297 -4.2648978 -4.278646 -4.2759614 -4.2584467 -4.2507143 -4.2629352 -4.2670937 -4.2606616 -4.2514162 -4.23732 -4.2253675 -4.2354422][-4.2034097 -4.2045121 -4.2281733 -4.2492108 -4.2578468 -4.2471275 -4.2270103 -4.2286181 -4.2528243 -4.2597942 -4.2488952 -4.2342863 -4.2133126 -4.195868 -4.2089338][-4.1987262 -4.1978436 -4.2177739 -4.2298713 -4.2272396 -4.2027888 -4.1720319 -4.1742849 -4.2175488 -4.2424145 -4.236896 -4.2217646 -4.1929603 -4.1676459 -4.1802435][-4.2013912 -4.2082715 -4.2240205 -4.2195334 -4.1907253 -4.1414213 -4.0816655 -4.0759525 -4.1435747 -4.1965141 -4.205163 -4.1980877 -4.1689038 -4.1365852 -4.1421242][-4.1960092 -4.2103953 -4.2151284 -4.1838031 -4.117239 -4.0296268 -3.9161961 -3.8949256 -4.0030422 -4.0983348 -4.1332922 -4.1488109 -4.1388197 -4.1120009 -4.1137829][-4.1852407 -4.1983767 -4.1862607 -4.1275907 -4.0327735 -3.9141974 -3.7529583 -3.7148807 -3.8673422 -4.000628 -4.0565696 -4.0915971 -4.105876 -4.097806 -4.1088576][-4.2048006 -4.2183514 -4.2017689 -4.1420932 -4.05573 -3.9492376 -3.8043413 -3.7720146 -3.909781 -4.0231104 -4.0642128 -4.093874 -4.1160903 -4.1234136 -4.14272][-4.2463946 -4.2641544 -4.2576985 -4.2197871 -4.160367 -4.0889163 -3.998471 -3.9846282 -4.0740752 -4.1410794 -4.1543469 -4.1663179 -4.18611 -4.1996489 -4.2171435][-4.2777324 -4.2966361 -4.3008785 -4.2827983 -4.2484169 -4.2080321 -4.1620512 -4.157733 -4.207273 -4.2410316 -4.2408018 -4.2445049 -4.2595448 -4.27392 -4.2866707][-4.2891121 -4.3049865 -4.3139372 -4.3094683 -4.296145 -4.2798653 -4.2599335 -4.2572708 -4.2793384 -4.2928424 -4.2906842 -4.2923274 -4.3006535 -4.3098946 -4.3190341][-4.2863035 -4.2960887 -4.3026323 -4.3018007 -4.2988753 -4.2950935 -4.2857704 -4.2803297 -4.2882376 -4.2951131 -4.2966547 -4.3007221 -4.3056397 -4.3101921 -4.31512][-4.2840381 -4.2889638 -4.2922997 -4.2925696 -4.2934594 -4.2946134 -4.2881832 -4.2791905 -4.2806916 -4.2869515 -4.2917395 -4.2947145 -4.2975206 -4.29963 -4.302073][-4.2747154 -4.2790585 -4.2794042 -4.2771349 -4.2771969 -4.2793617 -4.2741246 -4.2645159 -4.2631965 -4.2687383 -4.2740188 -4.2765427 -4.2789631 -4.2816429 -4.2838931]]...]
INFO - root - 2017-12-07 21:22:07.695646: step 55510, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 53h:04m:34s remains)
INFO - root - 2017-12-07 21:22:14.457029: step 55520, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 53h:30m:26s remains)
INFO - root - 2017-12-07 21:22:21.196782: step 55530, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 52h:19m:56s remains)
INFO - root - 2017-12-07 21:22:27.874736: step 55540, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 49h:41m:59s remains)
INFO - root - 2017-12-07 21:22:34.730700: step 55550, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 49h:27m:45s remains)
INFO - root - 2017-12-07 21:22:41.597499: step 55560, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 53h:41m:41s remains)
INFO - root - 2017-12-07 21:22:48.410453: step 55570, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 55h:15m:11s remains)
INFO - root - 2017-12-07 21:22:55.195419: step 55580, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 53h:12m:41s remains)
INFO - root - 2017-12-07 21:23:02.001532: step 55590, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 49h:03m:37s remains)
INFO - root - 2017-12-07 21:23:08.770684: step 55600, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 51h:31m:26s remains)
2017-12-07 21:23:09.531098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2010365 -4.156261 -4.1248107 -4.1070232 -4.1051784 -4.10026 -4.0867467 -4.0834808 -4.1051273 -4.1462126 -4.1829252 -4.2048135 -4.2074203 -4.1914034 -4.1643291][-4.1790934 -4.1279249 -4.0891328 -4.0645914 -4.0594187 -4.0586219 -4.049531 -4.0553 -4.0853109 -4.132205 -4.1730318 -4.1983328 -4.2032166 -4.18349 -4.1460214][-4.1606536 -4.1129131 -4.0800471 -4.0554028 -4.0433574 -4.0353961 -4.02658 -4.0339432 -4.0652065 -4.1107445 -4.1533446 -4.1845355 -4.1943526 -4.1767492 -4.1363544][-4.1733007 -4.1347461 -4.1104946 -4.0989227 -4.0849113 -4.0612254 -4.0373273 -4.0327458 -4.04945 -4.0796528 -4.1193242 -4.1595058 -4.1750402 -4.1609817 -4.1220684][-4.2098327 -4.178246 -4.15822 -4.1531739 -4.1385212 -4.097826 -4.0540714 -4.0333114 -4.0330434 -4.0450439 -4.0747638 -4.1176505 -4.1456146 -4.142231 -4.1077285][-4.241807 -4.2140183 -4.1919703 -4.1854424 -4.1633072 -4.1114664 -4.0588732 -4.0279241 -4.0096273 -4.0118537 -4.0359774 -4.0813255 -4.122519 -4.1330237 -4.1093836][-4.2439933 -4.2213774 -4.1983094 -4.188961 -4.1608448 -4.1019096 -4.0472312 -4.0086288 -3.9833107 -3.9862418 -4.0109673 -4.0614605 -4.1128826 -4.1295934 -4.1166][-4.2296081 -4.2125206 -4.1901093 -4.1740551 -4.1418982 -4.0844493 -4.03255 -3.9922304 -3.9669328 -3.9753323 -4.009829 -4.067976 -4.1231008 -4.1360636 -4.1208177][-4.2109947 -4.1942039 -4.1706557 -4.1508851 -4.1265292 -4.0816822 -4.0402679 -4.0133114 -3.9999864 -4.007154 -4.0335751 -4.0834875 -4.1336579 -4.1410708 -4.1219773][-4.205966 -4.1875443 -4.1624374 -4.1469984 -4.1374478 -4.113833 -4.091835 -4.0789351 -4.0720363 -4.0727682 -4.0814242 -4.1096759 -4.1462278 -4.1489863 -4.1279674][-4.2016959 -4.1828127 -4.16025 -4.1510029 -4.1503973 -4.1424036 -4.1353912 -4.1338639 -4.1316338 -4.1262507 -4.1203671 -4.1258025 -4.146409 -4.1495256 -4.1317329][-4.2141771 -4.1931663 -4.1704588 -4.16089 -4.1590552 -4.1580567 -4.1634517 -4.1723113 -4.1706371 -4.1596842 -4.1447487 -4.1360259 -4.1466789 -4.1537375 -4.1464572][-4.2417469 -4.2196512 -4.20079 -4.1921377 -4.1883869 -4.1889768 -4.198452 -4.2119813 -4.2098231 -4.1969228 -4.17767 -4.1633043 -4.1677656 -4.1730385 -4.1702847][-4.254787 -4.2354355 -4.2228141 -4.2174459 -4.2156544 -4.2176132 -4.2269316 -4.2397885 -4.2383561 -4.2269788 -4.2099214 -4.1962776 -4.1973825 -4.1984057 -4.1958194][-4.2688513 -4.2526784 -4.2444868 -4.2457938 -4.2483535 -4.2525854 -4.2595849 -4.2677774 -4.2660713 -4.25871 -4.2464404 -4.2376828 -4.2383571 -4.2361455 -4.2299318]]...]
INFO - root - 2017-12-07 21:23:16.098899: step 55610, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 53h:30m:13s remains)
INFO - root - 2017-12-07 21:23:22.871633: step 55620, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 50h:22m:36s remains)
INFO - root - 2017-12-07 21:23:29.729598: step 55630, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 50h:04m:55s remains)
INFO - root - 2017-12-07 21:23:36.596090: step 55640, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 53h:51m:57s remains)
INFO - root - 2017-12-07 21:23:43.433411: step 55650, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 53h:53m:22s remains)
INFO - root - 2017-12-07 21:23:50.196626: step 55660, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 51h:56m:46s remains)
INFO - root - 2017-12-07 21:23:56.964790: step 55670, loss = 2.03, batch loss = 1.97 (12.5 examples/sec; 0.641 sec/batch; 49h:18m:43s remains)
INFO - root - 2017-12-07 21:24:03.633177: step 55680, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 49h:44m:50s remains)
INFO - root - 2017-12-07 21:24:10.400576: step 55690, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 54h:00m:31s remains)
INFO - root - 2017-12-07 21:24:17.249712: step 55700, loss = 2.03, batch loss = 1.97 (11.3 examples/sec; 0.706 sec/batch; 54h:16m:46s remains)
2017-12-07 21:24:17.957677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.200418 -4.1697822 -4.1576123 -4.1624918 -4.1811833 -4.2086453 -4.2334538 -4.2557182 -4.2660089 -4.2528663 -4.2230597 -4.176405 -4.125783 -4.1011777 -4.11763][-4.2253332 -4.2042661 -4.1988621 -4.2018108 -4.2050605 -4.2152433 -4.2220678 -4.2323942 -4.2391243 -4.225316 -4.1988015 -4.1545963 -4.101512 -4.0766287 -4.0936456][-4.2368317 -4.2194085 -4.2168255 -4.2182207 -4.2114086 -4.2067385 -4.2032046 -4.2076159 -4.2155414 -4.204865 -4.1827483 -4.1436057 -4.0884395 -4.0614872 -4.0747743][-4.2163491 -4.2011867 -4.200932 -4.2063069 -4.1982427 -4.1811132 -4.17123 -4.1728415 -4.1823916 -4.1799936 -4.1623583 -4.1290545 -4.0765495 -4.0539751 -4.0687146][-4.1839347 -4.1764007 -4.1797142 -4.1862531 -4.1763005 -4.1487579 -4.1303229 -4.1281991 -4.1394529 -4.1468859 -4.1398983 -4.1177959 -4.072927 -4.0574322 -4.0736866][-4.1618714 -4.1570773 -4.159493 -4.1609716 -4.1438975 -4.1053171 -4.0754018 -4.070508 -4.08753 -4.111269 -4.1244822 -4.1205244 -4.0857549 -4.0745296 -4.0893655][-4.1611505 -4.1471038 -4.1417952 -4.1368794 -4.1104512 -4.0563769 -4.0049849 -3.9873407 -4.0056829 -4.0489616 -4.0898185 -4.1119757 -4.0976729 -4.1002731 -4.1207452][-4.1734824 -4.1469145 -4.1350369 -4.1281705 -4.0987759 -4.0347872 -3.9648755 -3.9232202 -3.9278612 -3.9803436 -4.0423088 -4.0825996 -4.0853343 -4.1036043 -4.1369615][-4.19688 -4.1683807 -4.1586771 -4.1589665 -4.1394205 -4.0917625 -4.0374546 -3.9967082 -3.9861553 -4.0159693 -4.0585251 -4.08691 -4.0879917 -4.1096396 -4.1485829][-4.2280726 -4.2032866 -4.2016296 -4.2085414 -4.1973886 -4.1645784 -4.1305013 -4.10719 -4.1003084 -4.1145539 -4.1341243 -4.141118 -4.129684 -4.1393156 -4.1695452][-4.2483182 -4.227809 -4.2315578 -4.2414107 -4.2337008 -4.2067852 -4.1820693 -4.1714315 -4.1732321 -4.180521 -4.1848216 -4.1731391 -4.1465464 -4.1441641 -4.1679192][-4.2408338 -4.2247438 -4.2300797 -4.2423496 -4.2418385 -4.22223 -4.20069 -4.1907625 -4.1886697 -4.1883712 -4.1838603 -4.1591935 -4.1196017 -4.108748 -4.1341019][-4.2073355 -4.1880593 -4.1905675 -4.2014055 -4.2051687 -4.1942863 -4.178463 -4.1661491 -4.1554761 -4.1477571 -4.139255 -4.1060638 -4.0563841 -4.0444403 -4.0825377][-4.1788292 -4.1594667 -4.1608987 -4.1699491 -4.1715612 -4.1627574 -4.1513171 -4.1396074 -4.1248164 -4.1120806 -4.099123 -4.0605745 -4.0043836 -3.9909897 -4.0364337][-4.1800284 -4.166965 -4.1717978 -4.1808176 -4.1814494 -4.1761718 -4.1716671 -4.1641631 -4.150526 -4.134656 -4.1206717 -4.0864544 -4.0366483 -4.0234408 -4.0625219]]...]
INFO - root - 2017-12-07 21:24:24.593318: step 55710, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 49h:10m:27s remains)
INFO - root - 2017-12-07 21:24:31.413363: step 55720, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 52h:03m:52s remains)
INFO - root - 2017-12-07 21:24:38.174513: step 55730, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 51h:57m:01s remains)
INFO - root - 2017-12-07 21:24:44.883864: step 55740, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 53h:02m:00s remains)
INFO - root - 2017-12-07 21:24:51.461286: step 55750, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.644 sec/batch; 49h:28m:59s remains)
INFO - root - 2017-12-07 21:24:58.303998: step 55760, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 51h:28m:40s remains)
INFO - root - 2017-12-07 21:25:05.133678: step 55770, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 55h:07m:28s remains)
INFO - root - 2017-12-07 21:25:11.985804: step 55780, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 52h:20m:08s remains)
INFO - root - 2017-12-07 21:25:18.835738: step 55790, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 49h:00m:15s remains)
INFO - root - 2017-12-07 21:25:25.650107: step 55800, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.652 sec/batch; 50h:05m:02s remains)
2017-12-07 21:25:26.303704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.200768 -4.1845012 -4.1649079 -4.1568804 -4.1619515 -4.1757336 -4.1918325 -4.1964226 -4.19288 -4.1916871 -4.1955376 -4.1993423 -4.2034545 -4.2079744 -4.2115912][-4.1812778 -4.1663628 -4.1504641 -4.1443677 -4.1482034 -4.1586533 -4.1719913 -4.178535 -4.1782212 -4.1780539 -4.1812367 -4.18275 -4.1831746 -4.1839657 -4.1828132][-4.186873 -4.1801953 -4.1715961 -4.1707458 -4.1739917 -4.1780167 -4.1840158 -4.187983 -4.1868582 -4.1863694 -4.189065 -4.1901178 -4.1875143 -4.1830235 -4.1734147][-4.1944227 -4.1989794 -4.2024565 -4.211132 -4.2138495 -4.2121997 -4.2101374 -4.2075691 -4.2055531 -4.2067719 -4.2111039 -4.2152829 -4.2130036 -4.2069421 -4.1925411][-4.1744223 -4.1872177 -4.20218 -4.2176347 -4.2172875 -4.209445 -4.1991224 -4.1846237 -4.1809897 -4.1887517 -4.2010527 -4.2133536 -4.2187328 -4.2178993 -4.2100234][-4.1275983 -4.1461411 -4.1671147 -4.1839848 -4.1755848 -4.159132 -4.1374807 -4.1043081 -4.0968943 -4.1126814 -4.1368437 -4.1603208 -4.17645 -4.1868277 -4.1972208][-4.0840507 -4.0956283 -4.1092753 -4.1136646 -4.0877991 -4.0578647 -4.0237164 -3.9723635 -3.9626279 -3.9916251 -4.0281749 -4.0635772 -4.0882988 -4.1098366 -4.1407871][-4.0855188 -4.08627 -4.08764 -4.0740952 -4.0325408 -3.9964736 -3.9570334 -3.8969927 -3.8844309 -3.9191704 -3.9563618 -3.9925473 -4.0165644 -4.0376077 -4.0768638][-4.1626291 -4.1614184 -4.1568904 -4.1373572 -4.0996485 -4.0740876 -4.0484152 -4.00666 -3.9962378 -4.0142021 -4.0270376 -4.0421715 -4.0520287 -4.0643077 -4.0945778][-4.2509546 -4.2557464 -4.2544227 -4.2410789 -4.2158747 -4.2013025 -4.1876292 -4.164978 -4.1602716 -4.1685996 -4.1697731 -4.1703458 -4.1696777 -4.172842 -4.1885781][-4.302567 -4.309691 -4.3125677 -4.3073111 -4.2925487 -4.2836151 -4.2789884 -4.2722406 -4.2735014 -4.2792892 -4.28048 -4.2777443 -4.2718005 -4.2643008 -4.2625222][-4.3238907 -4.3255968 -4.3273578 -4.3255959 -4.3176327 -4.31104 -4.308341 -4.3061194 -4.3082113 -4.31231 -4.3148031 -4.310369 -4.2991166 -4.2836018 -4.2713852][-4.3313918 -4.3267212 -4.3233538 -4.3209381 -4.3158083 -4.3087845 -4.3032455 -4.2978139 -4.2960482 -4.2982268 -4.29943 -4.2900143 -4.2728386 -4.2532258 -4.2392449][-4.3327723 -4.32286 -4.3126922 -4.3060331 -4.2992482 -4.2874351 -4.2749271 -4.2645564 -4.2614317 -4.2674594 -4.2702465 -4.255619 -4.2306523 -4.20525 -4.1883655][-4.32185 -4.3071442 -4.2918978 -4.2812147 -4.2694058 -4.2480721 -4.2257075 -4.2125587 -4.2126 -4.2258844 -4.2315869 -4.2097182 -4.17319 -4.1384549 -4.1174979]]...]
INFO - root - 2017-12-07 21:25:32.926128: step 55810, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 52h:22m:48s remains)
INFO - root - 2017-12-07 21:25:39.643425: step 55820, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 52h:04m:55s remains)
INFO - root - 2017-12-07 21:25:46.367381: step 55830, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 49h:25m:22s remains)
INFO - root - 2017-12-07 21:25:53.071877: step 55840, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 50h:29m:40s remains)
INFO - root - 2017-12-07 21:25:59.843058: step 55850, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 54h:30m:11s remains)
INFO - root - 2017-12-07 21:26:06.715543: step 55860, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.740 sec/batch; 56h:50m:31s remains)
INFO - root - 2017-12-07 21:26:13.515707: step 55870, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 51h:17m:33s remains)
INFO - root - 2017-12-07 21:26:20.283361: step 55880, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 49h:30m:08s remains)
INFO - root - 2017-12-07 21:26:27.077005: step 55890, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 50h:49m:21s remains)
INFO - root - 2017-12-07 21:26:33.848311: step 55900, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 53h:09m:59s remains)
2017-12-07 21:26:34.648650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1181083 -4.1824579 -4.2478814 -4.2982235 -4.3244748 -4.3335552 -4.33478 -4.3278341 -4.3202806 -4.3140335 -4.3077455 -4.2993894 -4.2871323 -4.2704916 -4.2518263][-4.223175 -4.2599244 -4.2975154 -4.3220468 -4.3301053 -4.3285842 -4.325922 -4.3227963 -4.3214145 -4.3202372 -4.3187957 -4.31498 -4.3072467 -4.2962608 -4.2829456][-4.3012896 -4.3171244 -4.3314624 -4.3342075 -4.3253474 -4.3123846 -4.303422 -4.3027005 -4.3088388 -4.3164425 -4.321898 -4.3232274 -4.318706 -4.311255 -4.3030019][-4.3428116 -4.3453507 -4.3426218 -4.3275642 -4.301415 -4.2756171 -4.263093 -4.2690926 -4.2867413 -4.3060775 -4.3199348 -4.3255329 -4.3216124 -4.3148069 -4.308578][-4.3510795 -4.3468585 -4.3334885 -4.3014278 -4.2568917 -4.216774 -4.2018309 -4.2177997 -4.2518072 -4.2877584 -4.3136387 -4.32462 -4.3210406 -4.3128829 -4.3070455][-4.3408542 -4.3296752 -4.3055363 -4.2582579 -4.1951532 -4.1359949 -4.1109257 -4.1350212 -4.1905551 -4.2499557 -4.2933021 -4.316555 -4.3193626 -4.3127384 -4.3068285][-4.3299618 -4.3115335 -4.2730412 -4.2098937 -4.1288528 -4.0440383 -3.9895594 -4.0130014 -4.0991235 -4.1906323 -4.2559409 -4.292995 -4.3063722 -4.3078585 -4.3060355][-4.3310318 -4.3121643 -4.2671547 -4.19352 -4.0969787 -3.9805164 -3.877032 -3.87823 -3.9928861 -4.1185155 -4.2072334 -4.2590933 -4.2817154 -4.2895484 -4.29255][-4.3394461 -4.3279204 -4.2930212 -4.2251959 -4.1308289 -4.009068 -3.8835239 -3.8458943 -3.9422009 -4.0685124 -4.1642838 -4.2231631 -4.2505856 -4.2602477 -4.2630863][-4.346632 -4.343173 -4.3250842 -4.2778988 -4.2033682 -4.1040878 -4.002552 -3.9584336 -4.0038977 -4.0878768 -4.1610465 -4.2097025 -4.2305121 -4.233283 -4.2314334][-4.3509617 -4.3525972 -4.34857 -4.3262057 -4.2801652 -4.2104659 -4.1391234 -4.1012912 -4.1174374 -4.1650071 -4.2110648 -4.2385731 -4.242835 -4.2297215 -4.2156053][-4.3522358 -4.3565278 -4.3606129 -4.356976 -4.3377533 -4.2976975 -4.2520442 -4.2236385 -4.2277451 -4.2537069 -4.2809114 -4.2905722 -4.2801228 -4.2537994 -4.2279053][-4.35057 -4.3564253 -4.36429 -4.3712645 -4.369626 -4.3527045 -4.3265715 -4.307868 -4.3073978 -4.3199916 -4.3318706 -4.3289361 -4.3082657 -4.2774239 -4.2500439][-4.3472786 -4.3527985 -4.3595662 -4.36731 -4.3712492 -4.367877 -4.3579507 -4.3500662 -4.3493638 -4.3532615 -4.3543968 -4.34033 -4.310955 -4.2770948 -4.2542586][-4.3437495 -4.3476896 -4.3522425 -4.3569636 -4.3602405 -4.36067 -4.3592739 -4.358664 -4.3598547 -4.3601012 -4.35362 -4.3278871 -4.2878227 -4.2495108 -4.2326517]]...]
INFO - root - 2017-12-07 21:26:41.192406: step 55910, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.629 sec/batch; 48h:17m:38s remains)
INFO - root - 2017-12-07 21:26:48.099591: step 55920, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 50h:43m:37s remains)
INFO - root - 2017-12-07 21:26:54.971211: step 55930, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 52h:59m:57s remains)
INFO - root - 2017-12-07 21:27:01.774970: step 55940, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 52h:58m:35s remains)
INFO - root - 2017-12-07 21:27:08.609629: step 55950, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 51h:44m:39s remains)
INFO - root - 2017-12-07 21:27:15.351404: step 55960, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 50h:33m:17s remains)
INFO - root - 2017-12-07 21:27:22.138742: step 55970, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 50h:46m:24s remains)
INFO - root - 2017-12-07 21:27:29.093162: step 55980, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 54h:54m:21s remains)
INFO - root - 2017-12-07 21:27:35.915970: step 55990, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 53h:53m:57s remains)
INFO - root - 2017-12-07 21:27:42.697818: step 56000, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 51h:59m:54s remains)
2017-12-07 21:27:43.396936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1968293 -4.1860332 -4.2039752 -4.2317958 -4.2511096 -4.2668524 -4.2756972 -4.2761016 -4.2679234 -4.2549019 -4.2461176 -4.2474442 -4.25839 -4.2755251 -4.2827864][-4.2086368 -4.1958475 -4.2113876 -4.2331643 -4.2476554 -4.264636 -4.2812853 -4.2845325 -4.2786455 -4.2679639 -4.2591496 -4.2551861 -4.2595963 -4.2691765 -4.27267][-4.2132945 -4.2030616 -4.2210813 -4.238966 -4.244216 -4.2553225 -4.2704086 -4.2768064 -4.27698 -4.2734966 -4.2707214 -4.2670865 -4.2648969 -4.2671828 -4.2658362][-4.1998177 -4.1918359 -4.2138758 -4.228642 -4.2241435 -4.2239895 -4.2341008 -4.2492628 -4.2640381 -4.2725797 -4.2750874 -4.2722406 -4.2670975 -4.2656584 -4.2602873][-4.1760778 -4.1693287 -4.1922131 -4.2008872 -4.1871505 -4.1775184 -4.18147 -4.2047663 -4.2371697 -4.2608452 -4.267931 -4.2666121 -4.2611456 -4.2578878 -4.2491508][-4.1645346 -4.1548204 -4.1729159 -4.1726341 -4.1467247 -4.1259561 -4.1253867 -4.1565733 -4.2054763 -4.2435608 -4.2585883 -4.2607245 -4.2521563 -4.24323 -4.2306242][-4.1742072 -4.16008 -4.1720085 -4.1625633 -4.1255713 -4.0944967 -4.088407 -4.1229196 -4.1810079 -4.226532 -4.2484541 -4.2546091 -4.2461677 -4.2353048 -4.2194414][-4.1951556 -4.1787734 -4.1885018 -4.1737323 -4.1281428 -4.0908093 -4.0823736 -4.1154385 -4.1738162 -4.223207 -4.2480536 -4.2571955 -4.2527757 -4.2466011 -4.2317867][-4.2100358 -4.1961818 -4.2064428 -4.1908822 -4.1427531 -4.1047072 -4.0986767 -4.1287265 -4.1782565 -4.2196746 -4.2411895 -4.2497444 -4.2500677 -4.2544494 -4.248847][-4.2128458 -4.2016459 -4.2167549 -4.2079086 -4.1629682 -4.121737 -4.1126776 -4.1358747 -4.1732607 -4.2040539 -4.2222009 -4.2292242 -4.2346644 -4.2491217 -4.2563114][-4.2091079 -4.1972427 -4.2162323 -4.2190018 -4.1831975 -4.138772 -4.1217966 -4.1367793 -4.1647849 -4.1908875 -4.2072849 -4.2178669 -4.22818 -4.2462597 -4.25841][-4.193862 -4.1800938 -4.1998782 -4.214292 -4.193162 -4.155405 -4.1383038 -4.1459937 -4.1648 -4.1870661 -4.2004542 -4.2115126 -4.2234449 -4.2410398 -4.2513537][-4.1830225 -4.1722407 -4.1974277 -4.2217069 -4.2146478 -4.1888757 -4.1789522 -4.1836081 -4.1952147 -4.212709 -4.2218418 -4.2270885 -4.2341175 -4.2437062 -4.2469535][-4.210196 -4.20556 -4.2300811 -4.2555351 -4.2562275 -4.2396398 -4.2331548 -4.23635 -4.2442613 -4.2564521 -4.2649622 -4.2688913 -4.2711616 -4.2709723 -4.2650704][-4.2624507 -4.2631788 -4.2804775 -4.2971253 -4.2989225 -4.2871981 -4.2819195 -4.2820058 -4.2853136 -4.293654 -4.3007236 -4.3048482 -4.3061457 -4.3045731 -4.2968535]]...]
INFO - root - 2017-12-07 21:27:50.085381: step 56010, loss = 2.06, batch loss = 2.01 (10.7 examples/sec; 0.744 sec/batch; 57h:10m:19s remains)
INFO - root - 2017-12-07 21:27:56.944058: step 56020, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 53h:00m:33s remains)
INFO - root - 2017-12-07 21:28:03.683879: step 56030, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 49h:53m:42s remains)
INFO - root - 2017-12-07 21:28:10.490157: step 56040, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 49h:16m:37s remains)
INFO - root - 2017-12-07 21:28:17.335839: step 56050, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 54h:17m:11s remains)
INFO - root - 2017-12-07 21:28:23.962358: step 56060, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 52h:41m:14s remains)
INFO - root - 2017-12-07 21:28:30.818766: step 56070, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 53h:18m:56s remains)
INFO - root - 2017-12-07 21:28:37.552651: step 56080, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.644 sec/batch; 49h:27m:54s remains)
INFO - root - 2017-12-07 21:28:44.385515: step 56090, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 53h:31m:39s remains)
INFO - root - 2017-12-07 21:28:51.192443: step 56100, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.736 sec/batch; 56h:31m:13s remains)
2017-12-07 21:28:51.893562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.198751 -4.1932116 -4.1995497 -4.2112951 -4.2200775 -4.2089782 -4.1943893 -4.1924644 -4.1963124 -4.192956 -4.1840448 -4.184217 -4.1926613 -4.1959295 -4.189023][-4.234622 -4.2437563 -4.2562728 -4.2680578 -4.2718573 -4.2531195 -4.2318654 -4.2292833 -4.2376828 -4.2412271 -4.2395182 -4.2439413 -4.2531261 -4.257431 -4.2513337][-4.2661791 -4.2767048 -4.285008 -4.2894588 -4.2832117 -4.2596159 -4.2350607 -4.2317052 -4.2468963 -4.2598252 -4.2686367 -4.2805042 -4.2925739 -4.2959623 -4.2874622][-4.3013196 -4.3052411 -4.3048434 -4.2977357 -4.280014 -4.2481866 -4.2159758 -4.2099314 -4.2292514 -4.24817 -4.26535 -4.2861047 -4.3057952 -4.3117146 -4.3020535][-4.321384 -4.31384 -4.3011222 -4.2827148 -4.2533622 -4.2099276 -4.161171 -4.1456432 -4.1677847 -4.1931715 -4.2173772 -4.2471919 -4.2781386 -4.2907929 -4.2806673][-4.3089671 -4.2929387 -4.2711406 -4.2426634 -4.2009521 -4.1427951 -4.0748663 -4.0405622 -4.0641365 -4.1018515 -4.1367068 -4.1753349 -4.2154927 -4.2371 -4.2322173][-4.2604189 -4.2391276 -4.2130322 -4.1763892 -4.119091 -4.0448565 -3.9538159 -3.8904085 -3.9123645 -3.9677756 -4.0188022 -4.0751595 -4.1347671 -4.1750402 -4.1901116][-4.2001452 -4.1824393 -4.1596918 -4.1218572 -4.0576525 -3.9736447 -3.86415 -3.7736778 -3.7901263 -3.8608041 -3.9297664 -4.0046172 -4.0825462 -4.143383 -4.1827307][-4.190515 -4.1851077 -4.1743283 -4.1457949 -4.0922451 -4.0210433 -3.9277825 -3.838625 -3.8386164 -3.897989 -3.9637814 -4.0329208 -4.1049919 -4.1655293 -4.2106042][-4.2420635 -4.2453308 -4.242527 -4.2260933 -4.1902142 -4.1397481 -4.0755558 -4.0111504 -3.9961035 -4.0271659 -4.0696545 -4.1150417 -4.1659412 -4.2116194 -4.24747][-4.2993035 -4.3036828 -4.3018103 -4.2918587 -4.2674565 -4.2306428 -4.1888566 -4.1485043 -4.1277275 -4.1390357 -4.163619 -4.1913142 -4.2247553 -4.257328 -4.2808671][-4.3230577 -4.3217382 -4.3176146 -4.3107386 -4.2950554 -4.2701778 -4.243804 -4.2178178 -4.1957479 -4.1983376 -4.2195725 -4.2428632 -4.2664061 -4.2899694 -4.3017154][-4.3158064 -4.309042 -4.3025479 -4.2945209 -4.2848954 -4.2697968 -4.2532949 -4.2352614 -4.2165093 -4.219564 -4.2440448 -4.2685924 -4.2886939 -4.3058982 -4.310914][-4.2994442 -4.2889986 -4.2791185 -4.2704635 -4.2643661 -4.2570357 -4.2482009 -4.2361517 -4.22424 -4.2297316 -4.2537622 -4.2786074 -4.29643 -4.3104167 -4.313911][-4.2847424 -4.2741575 -4.2636561 -4.2550564 -4.2523527 -4.2513161 -4.2476535 -4.2399535 -4.2342529 -4.2404618 -4.2596035 -4.2800879 -4.29494 -4.3049583 -4.3084059]]...]
INFO - root - 2017-12-07 21:28:58.465185: step 56110, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 50h:37m:16s remains)
INFO - root - 2017-12-07 21:29:05.321071: step 56120, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 54h:56m:42s remains)
INFO - root - 2017-12-07 21:29:12.064758: step 56130, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 53h:07m:56s remains)
INFO - root - 2017-12-07 21:29:18.925454: step 56140, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 53h:17m:28s remains)
INFO - root - 2017-12-07 21:29:25.694900: step 56150, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 48h:51m:08s remains)
INFO - root - 2017-12-07 21:29:32.598048: step 56160, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 49h:34m:37s remains)
INFO - root - 2017-12-07 21:29:39.412466: step 56170, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 53h:06m:03s remains)
INFO - root - 2017-12-07 21:29:46.375160: step 56180, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 56h:47m:41s remains)
INFO - root - 2017-12-07 21:29:53.228092: step 56190, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 53h:03m:04s remains)
INFO - root - 2017-12-07 21:30:00.036080: step 56200, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.657 sec/batch; 50h:26m:43s remains)
2017-12-07 21:30:00.769127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2569714 -4.2500491 -4.2433977 -4.2327671 -4.2205091 -4.2127185 -4.2128377 -4.215096 -4.2068243 -4.1920924 -4.1802697 -4.1895132 -4.2135668 -4.235714 -4.2455678][-4.25679 -4.2507496 -4.2455654 -4.2351689 -4.2230315 -4.2172022 -4.2189789 -4.2187338 -4.2038207 -4.1813903 -4.1670532 -4.1753492 -4.2003989 -4.2256341 -4.241992][-4.2421923 -4.2363629 -4.2330232 -4.2223516 -4.2104254 -4.2075181 -4.2082338 -4.2071924 -4.1936436 -4.1709003 -4.1591377 -4.1642275 -4.185236 -4.2089415 -4.2297592][-4.2127233 -4.2060575 -4.2042556 -4.1935577 -4.1787171 -4.1736031 -4.17425 -4.1769829 -4.1740694 -4.1624427 -4.1563492 -4.1603317 -4.1768851 -4.1983647 -4.2219787][-4.1900473 -4.1859703 -4.1871104 -4.1743417 -4.1504736 -4.1344848 -4.12709 -4.1344929 -4.1507235 -4.161305 -4.1676097 -4.1720147 -4.181819 -4.1966834 -4.2148433][-4.1788483 -4.1723113 -4.169621 -4.150629 -4.1101017 -4.0690866 -4.0410037 -4.0539188 -4.1016445 -4.1449018 -4.1705704 -4.1776562 -4.1832891 -4.1902962 -4.1992126][-4.168951 -4.1533475 -4.1416631 -4.1129632 -4.0497847 -3.97149 -3.9076362 -3.9262161 -4.0133376 -4.0932026 -4.138237 -4.151619 -4.1575279 -4.1637268 -4.1707067][-4.168808 -4.1474771 -4.1290708 -4.09268 -4.0117326 -3.90655 -3.8217175 -3.8467155 -3.9591894 -4.0583992 -4.1113005 -4.12726 -4.1317554 -4.1399055 -4.1520081][-4.1944952 -4.1749587 -4.1568246 -4.1262913 -4.0551443 -3.9654641 -3.9008856 -3.9211907 -4.0138826 -4.0980153 -4.1383748 -4.1446209 -4.1381674 -4.1442223 -4.15893][-4.2301483 -4.2158413 -4.2038 -4.1855927 -4.1384635 -4.0810113 -4.0434422 -4.0540252 -4.1108952 -4.1682725 -4.1928096 -4.1884465 -4.1722746 -4.1764 -4.1919985][-4.2508869 -4.2450228 -4.2410517 -4.2345934 -4.2098408 -4.1793842 -4.158659 -4.1618514 -4.1935272 -4.2286377 -4.2414865 -4.2352452 -4.2211437 -4.2252374 -4.2405672][-4.2478108 -4.2527823 -4.2586427 -4.2613931 -4.2503242 -4.2362442 -4.2292671 -4.2310414 -4.2463841 -4.2656074 -4.273478 -4.2735543 -4.2675853 -4.2734461 -4.2852325][-4.215817 -4.2325459 -4.250679 -4.2631769 -4.26425 -4.2613955 -4.2599287 -4.2600503 -4.2654877 -4.2754049 -4.283802 -4.2910419 -4.2931027 -4.298665 -4.3031311][-4.1672678 -4.1935329 -4.2231317 -4.2450347 -4.254343 -4.2559462 -4.2542419 -4.2528367 -4.2557707 -4.265348 -4.278573 -4.2896924 -4.2941561 -4.2967305 -4.2935643][-4.1286068 -4.160439 -4.1984305 -4.2290649 -4.2447524 -4.2480078 -4.24469 -4.2414694 -4.2427526 -4.2516942 -4.2667937 -4.27889 -4.2835288 -4.2840471 -4.2785864]]...]
INFO - root - 2017-12-07 21:30:07.463333: step 56210, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 54h:06m:26s remains)
INFO - root - 2017-12-07 21:30:14.305732: step 56220, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 50h:37m:41s remains)
INFO - root - 2017-12-07 21:30:21.104346: step 56230, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 49h:16m:21s remains)
INFO - root - 2017-12-07 21:30:27.918305: step 56240, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 52h:42m:29s remains)
INFO - root - 2017-12-07 21:30:34.751575: step 56250, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.733 sec/batch; 56h:16m:15s remains)
INFO - root - 2017-12-07 21:30:41.506444: step 56260, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 50h:48m:35s remains)
INFO - root - 2017-12-07 21:30:48.339749: step 56270, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 50h:52m:27s remains)
INFO - root - 2017-12-07 21:30:55.223290: step 56280, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 49h:22m:30s remains)
INFO - root - 2017-12-07 21:31:02.023516: step 56290, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 56h:04m:12s remains)
INFO - root - 2017-12-07 21:31:08.892702: step 56300, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 55h:21m:53s remains)
2017-12-07 21:31:09.654651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2881589 -4.2794709 -4.2711964 -4.27182 -4.2808886 -4.2916393 -4.2961845 -4.29399 -4.2923775 -4.2896686 -4.2877159 -4.2882743 -4.292088 -4.2928858 -4.2914071][-4.2641187 -4.246325 -4.2315483 -4.2357111 -4.2546277 -4.2709804 -4.2744346 -4.2698569 -4.267086 -4.2644777 -4.2628603 -4.2631216 -4.2664247 -4.2641425 -4.2598124][-4.2521567 -4.2239618 -4.19797 -4.2024617 -4.2270474 -4.2436166 -4.2410946 -4.2291327 -4.2266555 -4.230412 -4.2327862 -4.2346845 -4.2404242 -4.2363343 -4.2288222][-4.2396631 -4.2046552 -4.1733212 -4.1763296 -4.2005491 -4.2100606 -4.195447 -4.1709337 -4.1711149 -4.1914825 -4.2118158 -4.2238545 -4.2347054 -4.2309427 -4.2224522][-4.2203288 -4.1821647 -4.1462021 -4.1474476 -4.1632166 -4.1542497 -4.12014 -4.0844917 -4.09449 -4.1384525 -4.1806836 -4.2076755 -4.226522 -4.2252645 -4.218441][-4.2020683 -4.1622033 -4.1194506 -4.1155343 -4.1155291 -4.0821218 -4.02466 -3.9748909 -4.0032477 -4.0740423 -4.1424122 -4.1912446 -4.2234731 -4.2274308 -4.2207932][-4.2033515 -4.1609945 -4.1101437 -4.09017 -4.0609074 -3.9947596 -3.9065104 -3.8404098 -3.9009714 -4.01188 -4.1067472 -4.1734815 -4.2191963 -4.230957 -4.2261395][-4.2126679 -4.1716881 -4.1226158 -4.0974956 -4.0505314 -3.9744072 -3.881542 -3.8207479 -3.8984077 -4.0203862 -4.1112761 -4.16811 -4.2065406 -4.2173924 -4.2145777][-4.2078943 -4.1718745 -4.1335111 -4.1209273 -4.0878124 -4.0358648 -3.984133 -3.96195 -4.0282059 -4.1110277 -4.1633635 -4.188664 -4.2036948 -4.2021766 -4.1969218][-4.1950226 -4.1712241 -4.1464152 -4.1493168 -4.1362305 -4.1092019 -4.0845265 -4.0799875 -4.123579 -4.172123 -4.1952505 -4.1947193 -4.1887679 -4.1767921 -4.1687255][-4.1951981 -4.1801848 -4.1672254 -4.1796136 -4.1803422 -4.1705627 -4.1595039 -4.1522651 -4.1793861 -4.2100992 -4.219378 -4.2046328 -4.1835709 -4.1655574 -4.1605535][-4.217689 -4.2025452 -4.1916571 -4.2079716 -4.2190938 -4.2189302 -4.217052 -4.2091823 -4.2228079 -4.2417393 -4.2450557 -4.2313776 -4.2115855 -4.1913428 -4.1880622][-4.245595 -4.2301917 -4.2187958 -4.2342691 -4.2491894 -4.2514844 -4.2548876 -4.2500777 -4.2580857 -4.2693162 -4.271852 -4.2631788 -4.2479858 -4.2290688 -4.2245464][-4.2739916 -4.2612629 -4.2482996 -4.254509 -4.2618871 -4.260232 -4.2670674 -4.2680335 -4.2770281 -4.2858148 -4.2888045 -4.2854309 -4.2772355 -4.2648273 -4.2613678][-4.2918687 -4.282402 -4.2716751 -4.2710028 -4.2707133 -4.2650385 -4.2696366 -4.2745476 -4.28467 -4.2924256 -4.2973313 -4.29821 -4.2934361 -4.2859144 -4.284863]]...]
INFO - root - 2017-12-07 21:31:16.297064: step 56310, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.725 sec/batch; 55h:39m:28s remains)
INFO - root - 2017-12-07 21:31:23.225895: step 56320, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 55h:30m:26s remains)
INFO - root - 2017-12-07 21:31:30.041721: step 56330, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 51h:31m:04s remains)
INFO - root - 2017-12-07 21:31:36.753739: step 56340, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 49h:01m:18s remains)
INFO - root - 2017-12-07 21:31:43.596116: step 56350, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.636 sec/batch; 48h:45m:30s remains)
INFO - root - 2017-12-07 21:31:50.424199: step 56360, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 53h:43m:58s remains)
INFO - root - 2017-12-07 21:31:57.058356: step 56370, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 52h:22m:30s remains)
INFO - root - 2017-12-07 21:32:03.885954: step 56380, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 50h:17m:31s remains)
INFO - root - 2017-12-07 21:32:10.677502: step 56390, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 49h:02m:06s remains)
INFO - root - 2017-12-07 21:32:17.432063: step 56400, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.667 sec/batch; 51h:08m:43s remains)
2017-12-07 21:32:18.258824: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27102 -4.2652969 -4.2651129 -4.2688971 -4.2696977 -4.266448 -4.2625051 -4.2592554 -4.2644157 -4.2729888 -4.2768188 -4.2727261 -4.2704844 -4.2725415 -4.2779436][-4.2592149 -4.2495489 -4.2472749 -4.2505941 -4.2504954 -4.2457147 -4.2425404 -4.2440281 -4.2556844 -4.2710967 -4.2813921 -4.2799592 -4.2757735 -4.2735562 -4.2725987][-4.2520123 -4.2419243 -4.240016 -4.2414742 -4.2342582 -4.2222748 -4.2176514 -4.2219954 -4.2408857 -4.2652225 -4.2813759 -4.2871575 -4.2861557 -4.2796235 -4.2719646][-4.2455091 -4.239954 -4.2403736 -4.2381096 -4.2201262 -4.1999512 -4.1910086 -4.1943231 -4.2209854 -4.2540221 -4.2749891 -4.2886925 -4.2953482 -4.2872744 -4.2751966][-4.2396932 -4.2402987 -4.2430816 -4.2367239 -4.2068076 -4.1716738 -4.1460371 -4.1394858 -4.1759081 -4.2260423 -4.2593288 -4.2831535 -4.2952304 -4.2897987 -4.2764387][-4.2414331 -4.2457094 -4.2490773 -4.2371931 -4.1943808 -4.1395745 -4.0855565 -4.0561786 -4.0944333 -4.166904 -4.2223196 -4.2589564 -4.278307 -4.2768054 -4.2658706][-4.2459607 -4.24864 -4.2502131 -4.2350912 -4.187304 -4.1186018 -4.0340929 -3.9663539 -3.986486 -4.07551 -4.1609492 -4.2188654 -4.248178 -4.2541103 -4.2485642][-4.2515926 -4.2559943 -4.2575712 -4.2443571 -4.201334 -4.1308618 -4.0319924 -3.9329402 -3.9210353 -4.0053697 -4.1040072 -4.177938 -4.21723 -4.230134 -4.2323704][-4.24435 -4.2524972 -4.2619824 -4.2607965 -4.2345476 -4.18065 -4.0981259 -4.008801 -3.9864261 -4.0402765 -4.1127119 -4.1719136 -4.2059937 -4.2226453 -4.232048][-4.2213111 -4.228559 -4.2463775 -4.2633519 -4.2602777 -4.227622 -4.1723938 -4.1142921 -4.0996227 -4.1268973 -4.16355 -4.1961331 -4.2184167 -4.2343326 -4.2443657][-4.206275 -4.2124534 -4.2313433 -4.251534 -4.2599072 -4.244 -4.2132244 -4.1839185 -4.1790257 -4.1941619 -4.213016 -4.2270536 -4.23561 -4.2448564 -4.2482967][-4.1974177 -4.1989322 -4.211535 -4.2278862 -4.2351232 -4.2280211 -4.2171831 -4.2072563 -4.2071161 -4.2192645 -4.2349725 -4.2432036 -4.245913 -4.2514844 -4.244216][-4.1970458 -4.1905756 -4.1949058 -4.200861 -4.1969528 -4.1895738 -4.18673 -4.1807847 -4.1792269 -4.1935325 -4.2181821 -4.2299957 -4.2332368 -4.242342 -4.2349][-4.2090964 -4.1995406 -4.1952629 -4.1878839 -4.1682529 -4.1502581 -4.1388206 -4.1218758 -4.1160789 -4.1420078 -4.1824827 -4.2057834 -4.21748 -4.230895 -4.2291265][-4.2090192 -4.2076454 -4.2039361 -4.19114 -4.15914 -4.1266866 -4.0981083 -4.0608172 -4.0441146 -4.0771046 -4.1340723 -4.17605 -4.2044377 -4.2261219 -4.2357254]]...]
INFO - root - 2017-12-07 21:32:24.829109: step 56410, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 51h:48m:20s remains)
INFO - root - 2017-12-07 21:32:31.646472: step 56420, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 49h:09m:21s remains)
INFO - root - 2017-12-07 21:32:38.532632: step 56430, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 51h:51m:55s remains)
INFO - root - 2017-12-07 21:32:45.375718: step 56440, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 53h:35m:41s remains)
INFO - root - 2017-12-07 21:32:52.140530: step 56450, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 53h:48m:36s remains)
INFO - root - 2017-12-07 21:32:58.926301: step 56460, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 50h:38m:34s remains)
INFO - root - 2017-12-07 21:33:05.657610: step 56470, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 50h:04m:26s remains)
INFO - root - 2017-12-07 21:33:12.436016: step 56480, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 48h:58m:21s remains)
INFO - root - 2017-12-07 21:33:19.255985: step 56490, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 55h:20m:36s remains)
INFO - root - 2017-12-07 21:33:26.073929: step 56500, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 53h:15m:32s remains)
2017-12-07 21:33:26.792384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2483287 -4.2489128 -4.2534971 -4.25691 -4.2602305 -4.2639542 -4.2676048 -4.2694073 -4.2681923 -4.262197 -4.2520704 -4.2412963 -4.2336349 -4.2301474 -4.2297134][-4.2450519 -4.2423396 -4.244266 -4.2447052 -4.2451181 -4.24497 -4.2446704 -4.2433748 -4.2404323 -4.2341428 -4.2256026 -4.218214 -4.2139049 -4.2132144 -4.2144551][-4.2352872 -4.2290635 -4.2267509 -4.2223048 -4.2177091 -4.2117009 -4.2062635 -4.2023239 -4.1996627 -4.19626 -4.1925135 -4.1912222 -4.1932611 -4.1975856 -4.2024965][-4.2284374 -4.2197595 -4.2104058 -4.1973886 -4.1856389 -4.1728339 -4.1610389 -4.1551275 -4.1557112 -4.1599078 -4.1659451 -4.1747894 -4.1860833 -4.1969013 -4.206408][-4.2302284 -4.2188721 -4.19908 -4.174674 -4.1548781 -4.136405 -4.1201935 -4.1156683 -4.1253119 -4.1400166 -4.155549 -4.1723981 -4.19086 -4.2062864 -4.2179418][-4.231214 -4.2170415 -4.1866274 -4.1480379 -4.1148081 -4.0875578 -4.0692549 -4.0730824 -4.0996828 -4.1300564 -4.1555538 -4.1777382 -4.1988525 -4.2156696 -4.2283154][-4.2236514 -4.2050791 -4.1652875 -4.1137967 -4.0667567 -4.029963 -4.0113716 -4.0273795 -4.0737629 -4.1207581 -4.1551175 -4.1803923 -4.2026777 -4.2209449 -4.2353811][-4.2192035 -4.1987648 -4.1559668 -4.1000896 -4.0473127 -4.0053296 -3.9874654 -4.0102096 -4.0649934 -4.1178341 -4.1544657 -4.179924 -4.2022524 -4.2218266 -4.2387156][-4.2246623 -4.2072735 -4.1697373 -4.1218562 -4.0739789 -4.0316062 -4.0113659 -4.0287638 -4.0752015 -4.120204 -4.1514015 -4.1748223 -4.197341 -4.2187786 -4.2381926][-4.2305541 -4.2205081 -4.1926613 -4.1574564 -4.1209126 -4.08289 -4.0589023 -4.0622478 -4.0895753 -4.1188884 -4.1413822 -4.1624732 -4.1860328 -4.2096791 -4.2311206][-4.2242146 -4.2235537 -4.2070045 -4.1835151 -4.1572757 -4.1252351 -4.1002393 -4.0923672 -4.1016421 -4.1154346 -4.1297507 -4.1485214 -4.1727619 -4.1973314 -4.218823][-4.1968932 -4.2037563 -4.1969113 -4.1833 -4.1665015 -4.142797 -4.122292 -4.1120048 -4.1114025 -4.114974 -4.122983 -4.1383419 -4.1607194 -4.1833992 -4.2022781][-4.1558857 -4.16767 -4.1688123 -4.1637549 -4.1556096 -4.1418495 -4.1302977 -4.1239319 -4.1202059 -4.1188335 -4.1232924 -4.13604 -4.155519 -4.1744885 -4.1892591][-4.1350746 -4.1470032 -4.1521034 -4.1523533 -4.1512203 -4.1465087 -4.143364 -4.1422329 -4.1404333 -4.1394334 -4.1427426 -4.152463 -4.1670785 -4.1807761 -4.1912241][-4.1527414 -4.1605535 -4.1628447 -4.1625853 -4.1642361 -4.1644573 -4.16602 -4.1683164 -4.1694527 -4.1705685 -4.1738491 -4.1805496 -4.1899438 -4.1988044 -4.2062883]]...]
INFO - root - 2017-12-07 21:33:33.421603: step 56510, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 48h:35m:15s remains)
INFO - root - 2017-12-07 21:33:40.200814: step 56520, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 55h:11m:13s remains)
INFO - root - 2017-12-07 21:33:47.030052: step 56530, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 53h:58m:25s remains)
INFO - root - 2017-12-07 21:33:53.793346: step 56540, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 51h:21m:25s remains)
INFO - root - 2017-12-07 21:34:00.600892: step 56550, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.669 sec/batch; 51h:19m:04s remains)
INFO - root - 2017-12-07 21:34:07.407217: step 56560, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 49h:39m:25s remains)
INFO - root - 2017-12-07 21:34:14.211221: step 56570, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 54h:56m:43s remains)
INFO - root - 2017-12-07 21:34:21.086801: step 56580, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 54h:47m:17s remains)
INFO - root - 2017-12-07 21:34:27.890728: step 56590, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 51h:58m:02s remains)
INFO - root - 2017-12-07 21:34:34.646292: step 56600, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 52h:10m:53s remains)
2017-12-07 21:34:35.413025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2756505 -4.2790322 -4.2765269 -4.2710228 -4.2662983 -4.260541 -4.2501607 -4.2367673 -4.2354107 -4.2470245 -4.2561436 -4.2642341 -4.2669158 -4.2627378 -4.2556615][-4.258285 -4.2668247 -4.2686138 -4.2635684 -4.2569284 -4.2433534 -4.2185221 -4.19044 -4.1890421 -4.209857 -4.2272964 -4.2449913 -4.2536139 -4.25794 -4.2572317][-4.2541261 -4.2629604 -4.2646627 -4.2564845 -4.2446303 -4.2192473 -4.1748848 -4.1275487 -4.1268563 -4.1631546 -4.192194 -4.2227139 -4.23966 -4.254086 -4.2620144][-4.2586122 -4.2628832 -4.2642813 -4.2528915 -4.2344952 -4.1979704 -4.13185 -4.0602317 -4.06147 -4.1163507 -4.1575603 -4.1969743 -4.2230382 -4.2475762 -4.2626009][-4.2594457 -4.2569308 -4.2562528 -4.2409382 -4.2161055 -4.1670489 -4.0745187 -3.9678676 -3.9778035 -4.0628529 -4.1221437 -4.17203 -4.2108121 -4.2457056 -4.2678251][-4.2632451 -4.252676 -4.245842 -4.2236214 -4.1866078 -4.119379 -3.9907923 -3.8347466 -3.8628902 -3.9965148 -4.0886278 -4.1553259 -4.2085156 -4.2526832 -4.2807012][-4.2752872 -4.2565231 -4.2413273 -4.2130566 -4.1602983 -4.068759 -3.90372 -3.6981287 -3.7435234 -3.9325497 -4.0625987 -4.145576 -4.2068911 -4.2548089 -4.2877879][-4.286684 -4.2615824 -4.2426767 -4.2175355 -4.1648459 -4.0724354 -3.9104621 -3.7106886 -3.7552884 -3.9461803 -4.0794268 -4.1573029 -4.2104263 -4.2524624 -4.2871671][-4.2984853 -4.2733116 -4.2569337 -4.2422724 -4.20649 -4.1356134 -4.0166535 -3.8741562 -3.9009004 -4.0339274 -4.1348615 -4.1964221 -4.2319541 -4.2641568 -4.295054][-4.308917 -4.2875624 -4.2773542 -4.2707853 -4.2460341 -4.1932044 -4.1150179 -4.024827 -4.0384526 -4.11852 -4.18661 -4.2298918 -4.2515588 -4.2753925 -4.299233][-4.3146815 -4.2958527 -4.289207 -4.2874818 -4.2673664 -4.227993 -4.181253 -4.1301093 -4.13935 -4.1872783 -4.2304044 -4.254282 -4.2627435 -4.2803969 -4.2981162][-4.3171978 -4.2986794 -4.2897711 -4.2913008 -4.2782993 -4.2546616 -4.2333841 -4.2087088 -4.2141185 -4.2403212 -4.2647476 -4.2741327 -4.275526 -4.2878833 -4.3013859][-4.3169627 -4.2982259 -4.2868338 -4.2897844 -4.2896075 -4.2824407 -4.2797179 -4.2722883 -4.2749162 -4.2850165 -4.294518 -4.2926445 -4.2924685 -4.3007126 -4.3090372][-4.3216357 -4.3054743 -4.2950487 -4.2983637 -4.3056693 -4.3112903 -4.3162818 -4.3165665 -4.3155708 -4.3155904 -4.3125567 -4.3055286 -4.3052006 -4.3110366 -4.3168082][-4.3248873 -4.3136449 -4.3043847 -4.3059287 -4.31378 -4.3264046 -4.3338532 -4.3351922 -4.3315048 -4.3267875 -4.3204055 -4.3131237 -4.313849 -4.3190613 -4.3235078]]...]
INFO - root - 2017-12-07 21:34:42.080445: step 56610, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 53h:26m:16s remains)
INFO - root - 2017-12-07 21:34:48.891290: step 56620, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 52h:24m:50s remains)
INFO - root - 2017-12-07 21:34:55.687385: step 56630, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 49h:59m:25s remains)
INFO - root - 2017-12-07 21:35:02.408582: step 56640, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 48h:10m:43s remains)
INFO - root - 2017-12-07 21:35:09.198770: step 56650, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 52h:39m:50s remains)
INFO - root - 2017-12-07 21:35:16.124588: step 56660, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 54h:08m:52s remains)
INFO - root - 2017-12-07 21:35:22.823265: step 56670, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.692 sec/batch; 53h:01m:14s remains)
INFO - root - 2017-12-07 21:35:29.520512: step 56680, loss = 2.09, batch loss = 2.03 (14.3 examples/sec; 0.559 sec/batch; 42h:49m:17s remains)
INFO - root - 2017-12-07 21:35:36.235943: step 56690, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 53h:11m:01s remains)
INFO - root - 2017-12-07 21:35:43.021668: step 56700, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 53h:26m:30s remains)
2017-12-07 21:35:43.753181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17943 -4.1813159 -4.1889558 -4.188643 -4.1899967 -4.1897478 -4.1822362 -4.1724515 -4.1729827 -4.1835938 -4.19613 -4.2027287 -4.2120404 -4.2181211 -4.220511][-4.1623077 -4.1622648 -4.1700587 -4.1747661 -4.1784272 -4.1785283 -4.1736526 -4.1673036 -4.1682315 -4.1787577 -4.1894274 -4.1925993 -4.196125 -4.1977243 -4.2019835][-4.1430984 -4.1388412 -4.1394939 -4.1456094 -4.1500225 -4.1487803 -4.1451907 -4.142777 -4.1462603 -4.1573329 -4.165637 -4.1675959 -4.1702046 -4.173564 -4.1825094][-4.1504426 -4.1407375 -4.1275835 -4.1269622 -4.1277785 -4.1220279 -4.1140909 -4.1090269 -4.1136007 -4.127666 -4.1336708 -4.1293411 -4.1276336 -4.1316485 -4.1431684][-4.1776738 -4.1686587 -4.1499863 -4.1406345 -4.1350718 -4.1259117 -4.1133003 -4.1055636 -4.112473 -4.1292887 -4.134563 -4.1240773 -4.1135521 -4.1067729 -4.1110592][-4.1733322 -4.1731806 -4.1602049 -4.1479874 -4.1354742 -4.1172991 -4.0949779 -4.0856276 -4.0996485 -4.1289568 -4.1426673 -4.1358938 -4.1248808 -4.1085687 -4.1002736][-4.1068478 -4.1173697 -4.1101427 -4.0959826 -4.0738611 -4.0360808 -3.992902 -3.9780478 -4.0092978 -4.06733 -4.107471 -4.1201024 -4.120018 -4.1071534 -4.1000237][-4.0031576 -4.0222859 -4.0157642 -4.0022392 -3.9734879 -3.9135363 -3.8408451 -3.8092895 -3.859724 -3.9528384 -4.02613 -4.0631056 -4.0775814 -4.075964 -4.084434][-3.9449704 -3.9735098 -3.9732111 -3.9669614 -3.9452245 -3.8858676 -3.8040581 -3.7564111 -3.7989988 -3.8883772 -3.9652622 -4.0112448 -4.0299239 -4.03228 -4.05143][-4.0127363 -4.0403662 -4.0420928 -4.0440273 -4.0399489 -4.0079041 -3.9534805 -3.9115477 -3.9213808 -3.9644482 -4.0102954 -4.0418534 -4.0517359 -4.0463586 -4.0625782][-4.1390438 -4.157218 -4.158771 -4.1619568 -4.1656651 -4.1549091 -4.1264935 -4.0950313 -4.0857663 -4.0950623 -4.1131949 -4.1280274 -4.128809 -4.1162124 -4.1231403][-4.2358952 -4.2488794 -4.2496643 -4.2498231 -4.2515416 -4.2470131 -4.2309761 -4.2088532 -4.1937809 -4.1876931 -4.1928096 -4.1978149 -4.1926308 -4.1780539 -4.1794629][-4.2870507 -4.2963729 -4.2960973 -4.2959166 -4.2954664 -4.2924085 -4.2838435 -4.2677164 -4.2481027 -4.2316222 -4.2292209 -4.225687 -4.2136607 -4.198019 -4.197691][-4.3046017 -4.3114457 -4.313127 -4.31519 -4.3143687 -4.3115883 -4.3054595 -4.2902908 -4.2646465 -4.2381039 -4.2274013 -4.2158976 -4.1957617 -4.1789117 -4.1810613][-4.3011379 -4.3075442 -4.312499 -4.3173523 -4.3189664 -4.3181114 -4.3117089 -4.2940416 -4.2642651 -4.2315345 -4.2132149 -4.1954913 -4.1730928 -4.1592417 -4.1652069]]...]
INFO - root - 2017-12-07 21:35:50.247246: step 56710, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 47h:54m:05s remains)
INFO - root - 2017-12-07 21:35:56.963616: step 56720, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.675 sec/batch; 51h:43m:01s remains)
INFO - root - 2017-12-07 21:36:03.819179: step 56730, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 53h:08m:49s remains)
INFO - root - 2017-12-07 21:36:10.657034: step 56740, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 52h:19m:52s remains)
INFO - root - 2017-12-07 21:36:17.355304: step 56750, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 48h:42m:08s remains)
INFO - root - 2017-12-07 21:36:24.066845: step 56760, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 48h:37m:54s remains)
INFO - root - 2017-12-07 21:36:30.800699: step 56770, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 50h:44m:15s remains)
INFO - root - 2017-12-07 21:36:37.560588: step 56780, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 49h:36m:00s remains)
INFO - root - 2017-12-07 21:36:44.242724: step 56790, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.691 sec/batch; 52h:57m:04s remains)
INFO - root - 2017-12-07 21:36:51.030465: step 56800, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 52h:09m:18s remains)
2017-12-07 21:36:51.851155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2393289 -4.2418022 -4.2428689 -4.2298088 -4.2307839 -4.2331171 -4.23709 -4.2539086 -4.2697268 -4.28027 -4.2857203 -4.2930636 -4.2907357 -4.2788844 -4.2671146][-4.2642984 -4.2601581 -4.250895 -4.2317772 -4.2314405 -4.2380128 -4.2461724 -4.2668638 -4.2824082 -4.2911949 -4.2927389 -4.2927089 -4.27831 -4.2572803 -4.2412267][-4.2790031 -4.2664709 -4.2435594 -4.2144394 -4.2066126 -4.2100787 -4.2196803 -4.2471976 -4.2686758 -4.2832847 -4.28811 -4.2864051 -4.2599564 -4.2186589 -4.188982][-4.283474 -4.2599721 -4.22099 -4.1773119 -4.1571269 -4.1541104 -4.162776 -4.1927886 -4.2214036 -4.246057 -4.2621841 -4.2676697 -4.2379179 -4.180903 -4.1341038][-4.2829061 -4.2524834 -4.1988564 -4.1392074 -4.10408 -4.0939465 -4.0996413 -4.1272855 -4.1605654 -4.1974616 -4.2324338 -4.2499852 -4.2231522 -4.1558676 -4.0931325][-4.2820792 -4.2472692 -4.1795635 -4.1026058 -4.0506878 -4.0242443 -4.0111217 -4.0261593 -4.0619388 -4.1214256 -4.1853757 -4.22709 -4.2231007 -4.1682534 -4.1006646][-4.2863622 -4.2539082 -4.1823812 -4.0912981 -4.0199833 -3.962558 -3.9060163 -3.8839788 -3.9110575 -4.0029364 -4.1075654 -4.1803856 -4.213263 -4.1977458 -4.1544437][-4.296237 -4.2763081 -4.2198949 -4.1391582 -4.0676403 -3.9871237 -3.8827996 -3.8042932 -3.7916079 -3.8923655 -4.0274072 -4.1272707 -4.1879792 -4.2065215 -4.1967988][-4.3036976 -4.2959766 -4.2609305 -4.2058449 -4.1554041 -4.0846157 -3.9846389 -3.8990712 -3.8602879 -3.9171817 -4.0201468 -4.1079855 -4.17299 -4.2074747 -4.2212968][-4.3039 -4.3046489 -4.2885656 -4.2582617 -4.2317638 -4.1833291 -4.1118369 -4.0520782 -4.02156 -4.045691 -4.0981541 -4.1507444 -4.1970167 -4.22713 -4.2469063][-4.2988725 -4.3020372 -4.2989731 -4.2882504 -4.2818542 -4.2576642 -4.2149448 -4.1815777 -4.1662765 -4.177639 -4.1973262 -4.2211237 -4.2489605 -4.269228 -4.2809486][-4.2958446 -4.2979198 -4.2985435 -4.2975883 -4.3035278 -4.2970815 -4.2797923 -4.2691412 -4.2651887 -4.2719979 -4.2784829 -4.2845573 -4.2950706 -4.3052545 -4.3103204][-4.2946281 -4.2953248 -4.2960782 -4.2966857 -4.3077087 -4.3112445 -4.3092265 -4.3147631 -4.3195186 -4.326508 -4.3281655 -4.3277583 -4.3275094 -4.326427 -4.3243618][-4.2913642 -4.2902188 -4.2894015 -4.289237 -4.3015623 -4.3091512 -4.3132343 -4.3265648 -4.3364429 -4.3447495 -4.3446918 -4.3399367 -4.3340569 -4.3271437 -4.3201694][-4.2883921 -4.2860942 -4.283401 -4.2804303 -4.2916284 -4.2982445 -4.3027167 -4.3162994 -4.3260312 -4.3331952 -4.3323121 -4.3268042 -4.3198266 -4.3122244 -4.3061547]]...]
INFO - root - 2017-12-07 21:36:58.357417: step 56810, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 51h:07m:51s remains)
INFO - root - 2017-12-07 21:37:05.116207: step 56820, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.677 sec/batch; 51h:52m:26s remains)
INFO - root - 2017-12-07 21:37:11.866366: step 56830, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 53h:10m:36s remains)
INFO - root - 2017-12-07 21:37:18.569926: step 56840, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 49h:42m:46s remains)
INFO - root - 2017-12-07 21:37:25.425117: step 56850, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 54h:54m:08s remains)
INFO - root - 2017-12-07 21:37:32.183648: step 56860, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 52h:01m:52s remains)
INFO - root - 2017-12-07 21:37:38.942387: step 56870, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 51h:56m:31s remains)
INFO - root - 2017-12-07 21:37:45.707008: step 56880, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 51h:53m:11s remains)
INFO - root - 2017-12-07 21:37:52.395963: step 56890, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 49h:23m:50s remains)
INFO - root - 2017-12-07 21:37:59.238863: step 56900, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 50h:36m:05s remains)
2017-12-07 21:38:00.016763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33457 -4.3263941 -4.2991133 -4.2771511 -4.2770181 -4.2881413 -4.2977109 -4.3002415 -4.2923431 -4.2825823 -4.2844167 -4.2962151 -4.3176894 -4.3375349 -4.3474226][-4.329257 -4.3231783 -4.3004618 -4.2820568 -4.2809815 -4.284461 -4.2782631 -4.265686 -4.2481937 -4.2357817 -4.2430568 -4.2642069 -4.296102 -4.3245516 -4.3405933][-4.3262668 -4.3258991 -4.3081245 -4.2912383 -4.2840328 -4.2734966 -4.2475414 -4.21613 -4.1864324 -4.1701031 -4.1846352 -4.217907 -4.2622619 -4.30211 -4.3272114][-4.3097405 -4.3198729 -4.3114042 -4.2980394 -4.2845025 -4.2582726 -4.2102208 -4.1550832 -4.1090083 -4.0908642 -4.1167789 -4.1643705 -4.2222118 -4.2749286 -4.3110189][-4.2671747 -4.2912245 -4.2957058 -4.2882013 -4.2725215 -4.2366266 -4.1709123 -4.0921445 -4.028172 -4.0103588 -4.049016 -4.1104193 -4.1823711 -4.2494292 -4.2955275][-4.194129 -4.2341785 -4.2519836 -4.2541294 -4.2395 -4.198143 -4.1218572 -4.0251961 -3.9457564 -3.9311702 -3.9876447 -4.0672274 -4.1548152 -4.2318363 -4.2829623][-4.0996561 -4.1595097 -4.1941433 -4.2104073 -4.2026238 -4.1563392 -4.0690613 -3.9541025 -3.8615 -3.8601503 -3.9460769 -4.0477405 -4.1478777 -4.2307978 -4.2810483][-4.0146246 -4.0967903 -4.1526389 -4.1836104 -4.1822891 -4.1273789 -4.0267439 -3.8928881 -3.7955337 -3.8162651 -3.9317245 -4.0538268 -4.162591 -4.2469759 -4.292676][-3.9911685 -4.0803027 -4.1447134 -4.1801844 -4.1802273 -4.1243343 -4.0242324 -3.8992839 -3.8196542 -3.8522134 -3.9684653 -4.089396 -4.195838 -4.2749424 -4.3133383][-4.0478611 -4.1191297 -4.17362 -4.20158 -4.1999545 -4.1554813 -4.0771508 -3.9835689 -3.9247096 -3.9452844 -4.0326014 -4.1320333 -4.2254214 -4.2946982 -4.3270512][-4.1199045 -4.16561 -4.2019458 -4.2174783 -4.2113395 -4.1806278 -4.1263275 -4.0589037 -4.0078435 -4.008234 -4.0649867 -4.1440287 -4.2265625 -4.2910686 -4.3239202][-4.164763 -4.1869779 -4.2058392 -4.2089067 -4.1994052 -4.1823816 -4.148262 -4.0949888 -4.0409136 -4.0255265 -4.0623364 -4.1275916 -4.2033391 -4.2681851 -4.3074102][-4.1806145 -4.1817026 -4.1875186 -4.1878362 -4.1822381 -4.177989 -4.160027 -4.1161971 -4.0616522 -4.042572 -4.071352 -4.1278749 -4.1952238 -4.2561431 -4.2968383][-4.187367 -4.1800728 -4.1817303 -4.182858 -4.1821313 -4.1873369 -4.1811891 -4.1478343 -4.0982313 -4.081008 -4.1053872 -4.1539445 -4.2113309 -4.2645788 -4.3021779][-4.2218714 -4.2150269 -4.2149029 -4.2163196 -4.2169375 -4.2224612 -4.2205486 -4.1972508 -4.1600475 -4.1487641 -4.1710863 -4.2095428 -4.2533216 -4.2935557 -4.3211942]]...]
INFO - root - 2017-12-07 21:38:06.673249: step 56910, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 51h:25m:37s remains)
INFO - root - 2017-12-07 21:38:13.367177: step 56920, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 51h:08m:23s remains)
INFO - root - 2017-12-07 21:38:20.235569: step 56930, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 50h:48m:33s remains)
INFO - root - 2017-12-07 21:38:26.988478: step 56940, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 51h:27m:24s remains)
INFO - root - 2017-12-07 21:38:33.801903: step 56950, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 56h:16m:23s remains)
INFO - root - 2017-12-07 21:38:40.532803: step 56960, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 51h:39m:15s remains)
INFO - root - 2017-12-07 21:38:47.265487: step 56970, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.625 sec/batch; 47h:49m:07s remains)
INFO - root - 2017-12-07 21:38:54.026267: step 56980, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 49h:37m:53s remains)
INFO - root - 2017-12-07 21:39:00.813347: step 56990, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 53h:51m:13s remains)
INFO - root - 2017-12-07 21:39:07.408278: step 57000, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 52h:34m:03s remains)
2017-12-07 21:39:08.144194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2602372 -4.2593679 -4.2524486 -4.2400718 -4.2234359 -4.2080159 -4.2066145 -4.2201486 -4.2376847 -4.2484922 -4.248395 -4.2403522 -4.2334657 -4.2325315 -4.24039][-4.2743039 -4.2688317 -4.2549257 -4.2331829 -4.2075186 -4.1819544 -4.1746216 -4.1951952 -4.22661 -4.2515097 -4.2631149 -4.2628026 -4.2575016 -4.2539434 -4.2601528][-4.286963 -4.2770729 -4.2544179 -4.2218671 -4.1873922 -4.1504846 -4.131938 -4.1543121 -4.1995916 -4.240407 -4.266057 -4.2786837 -4.2798042 -4.2751584 -4.2774372][-4.3021274 -4.2926006 -4.2630663 -4.2190294 -4.1717591 -4.117568 -4.0808816 -4.0975723 -4.1589947 -4.2179756 -4.255126 -4.2772379 -4.28442 -4.282753 -4.283752][-4.3103776 -4.3058567 -4.2775764 -4.2278318 -4.1698055 -4.0951295 -4.031 -4.0348349 -4.1133628 -4.1906233 -4.2366838 -4.2629466 -4.2732453 -4.2742558 -4.2753053][-4.308239 -4.3096294 -4.288291 -4.2403932 -4.1765208 -4.0880518 -4.005187 -4.0026464 -4.0947547 -4.1799312 -4.2266145 -4.2518749 -4.2638626 -4.2657156 -4.2671876][-4.2982078 -4.3052874 -4.2915444 -4.2521825 -4.1944642 -4.11372 -4.0335879 -4.0296984 -4.1155462 -4.1896048 -4.2301378 -4.2516088 -4.2646108 -4.2673736 -4.2685][-4.285924 -4.29539 -4.2888174 -4.2606082 -4.2166367 -4.1547279 -4.088707 -4.0813284 -4.1472945 -4.2026677 -4.2339292 -4.2496634 -4.2592778 -4.2608151 -4.2601213][-4.273519 -4.2825217 -4.2779417 -4.257123 -4.2230892 -4.1728311 -4.1197028 -4.1164436 -4.1700339 -4.2110534 -4.2322469 -4.2409754 -4.2435021 -4.2403893 -4.2382216][-4.2591543 -4.264657 -4.2592425 -4.2398658 -4.2088065 -4.1628237 -4.1217036 -4.1317115 -4.1804643 -4.2119751 -4.2256875 -4.2305894 -4.2279143 -4.2223568 -4.2211394][-4.2499933 -4.249403 -4.2418 -4.2189865 -4.1856012 -4.1419797 -4.1157427 -4.1447692 -4.195282 -4.2230639 -4.2337484 -4.2381144 -4.23308 -4.2262316 -4.2252436][-4.2468405 -4.241086 -4.2268744 -4.1997423 -4.1618719 -4.1210504 -4.1137419 -4.1598549 -4.21102 -4.2361746 -4.2444868 -4.2462964 -4.2405825 -4.234952 -4.2355828][-4.2499294 -4.2373176 -4.2115803 -4.1818771 -4.1433263 -4.1115379 -4.12265 -4.1739955 -4.2192388 -4.2388873 -4.2454081 -4.2458506 -4.2415128 -4.2385883 -4.2409005][-4.2627378 -4.2403331 -4.2037253 -4.1724586 -4.1354713 -4.1140785 -4.1350651 -4.1825085 -4.2166834 -4.2282405 -4.23127 -4.2315593 -4.2300572 -4.2308054 -4.2357607][-4.2737746 -4.2426252 -4.19924 -4.1696 -4.140316 -4.1317143 -4.154952 -4.1943178 -4.2181616 -4.2224774 -4.22077 -4.220274 -4.2224655 -4.22708 -4.23589]]...]
INFO - root - 2017-12-07 21:39:14.773597: step 57010, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 54h:42m:10s remains)
INFO - root - 2017-12-07 21:39:21.634542: step 57020, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 54h:07m:20s remains)
INFO - root - 2017-12-07 21:39:28.368862: step 57030, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 51h:51m:00s remains)
INFO - root - 2017-12-07 21:39:35.057301: step 57040, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 48h:36m:59s remains)
INFO - root - 2017-12-07 21:39:41.815689: step 57050, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 49h:40m:27s remains)
INFO - root - 2017-12-07 21:39:48.542507: step 57060, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.683 sec/batch; 52h:16m:55s remains)
INFO - root - 2017-12-07 21:39:55.306774: step 57070, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 53h:53m:33s remains)
INFO - root - 2017-12-07 21:40:02.106821: step 57080, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 51h:56m:23s remains)
INFO - root - 2017-12-07 21:40:08.876492: step 57090, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 49h:47m:58s remains)
INFO - root - 2017-12-07 21:40:15.668492: step 57100, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 51h:43m:17s remains)
2017-12-07 21:40:16.439026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.334166 -4.3460097 -4.3473849 -4.3318963 -4.3018069 -4.26138 -4.2250233 -4.1881347 -4.1528015 -4.14176 -4.1654253 -4.1947885 -4.2135324 -4.2267652 -4.2337208][-4.3355174 -4.3475032 -4.3479848 -4.3298635 -4.2965059 -4.2511396 -4.2073193 -4.1610513 -4.116714 -4.1038933 -4.1350441 -4.1700606 -4.189404 -4.2041669 -4.212739][-4.3340149 -4.3444519 -4.3440185 -4.3238215 -4.2875342 -4.2378588 -4.1854081 -4.1313739 -4.08261 -4.0730934 -4.1141853 -4.1532254 -4.1698475 -4.1800365 -4.1868663][-4.3329258 -4.3415327 -4.3379498 -4.3120403 -4.2689357 -4.2114582 -4.1465125 -4.085463 -4.0392504 -4.0412335 -4.0968952 -4.1447248 -4.1642647 -4.1728377 -4.1776614][-4.3332334 -4.3402758 -4.3331819 -4.3011336 -4.2473531 -4.1769619 -4.1005511 -4.040349 -4.00551 -4.0256929 -4.0989828 -4.1581111 -4.1794639 -4.1844907 -4.1885605][-4.3338423 -4.3396883 -4.3309464 -4.29598 -4.2354865 -4.1561112 -4.0791388 -4.0321689 -4.0188746 -4.0565863 -4.1345997 -4.191565 -4.2072682 -4.2066545 -4.2087522][-4.3335967 -4.337657 -4.3282857 -4.2938972 -4.2337337 -4.1590052 -4.0976853 -4.0724006 -4.0783825 -4.1195283 -4.1861649 -4.2321105 -4.240427 -4.2329874 -4.23063][-4.3318381 -4.333847 -4.3244457 -4.2936225 -4.2394066 -4.1771922 -4.1375928 -4.1323218 -4.1471057 -4.1811419 -4.2292156 -4.2619715 -4.2656465 -4.256412 -4.251812][-4.3293362 -4.3297076 -4.3191967 -4.2916427 -4.2485867 -4.2019954 -4.1798749 -4.1853323 -4.2013903 -4.2251458 -4.254508 -4.2741346 -4.274786 -4.2684779 -4.2676468][-4.3272886 -4.3259039 -4.3138442 -4.2900963 -4.2597213 -4.2287641 -4.217535 -4.2258162 -4.2408891 -4.2545466 -4.2680535 -4.2753305 -4.2741337 -4.2727504 -4.2763486][-4.3245759 -4.3210568 -4.3077812 -4.2876821 -4.2678046 -4.2514367 -4.2507315 -4.2606487 -4.2712393 -4.2751174 -4.2762775 -4.2738628 -4.2705884 -4.2720408 -4.2766151][-4.321784 -4.3171535 -4.3049321 -4.2895255 -4.2777052 -4.2734714 -4.2799063 -4.2878394 -4.2916956 -4.2870317 -4.2798409 -4.2721205 -4.2687497 -4.2707853 -4.2728271][-4.3203545 -4.3164353 -4.3076596 -4.2978473 -4.2922754 -4.293674 -4.2994103 -4.3010111 -4.29828 -4.2895622 -4.2790976 -4.2691512 -4.2638693 -4.2626467 -4.2605839][-4.3207316 -4.3190451 -4.3137093 -4.308506 -4.3061352 -4.3069172 -4.3071327 -4.3034811 -4.2988987 -4.2909594 -4.2794981 -4.2679815 -4.2592154 -4.2523556 -4.2454462][-4.32202 -4.3228688 -4.3203974 -4.3170757 -4.314188 -4.3110456 -4.3060908 -4.3007145 -4.2985516 -4.2941594 -4.28437 -4.2730188 -4.2620606 -4.2506886 -4.2382045]]...]
INFO - root - 2017-12-07 21:40:23.133432: step 57110, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 50h:52m:43s remains)
INFO - root - 2017-12-07 21:40:29.799879: step 57120, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 48h:31m:48s remains)
INFO - root - 2017-12-07 21:40:36.441546: step 57130, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.629 sec/batch; 48h:07m:27s remains)
INFO - root - 2017-12-07 21:40:43.214630: step 57140, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 54h:58m:49s remains)
INFO - root - 2017-12-07 21:40:49.971712: step 57150, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.678 sec/batch; 51h:49m:18s remains)
INFO - root - 2017-12-07 21:40:56.711093: step 57160, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 54h:22m:51s remains)
INFO - root - 2017-12-07 21:41:03.472424: step 57170, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.669 sec/batch; 51h:12m:02s remains)
INFO - root - 2017-12-07 21:41:10.195470: step 57180, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 48h:24m:41s remains)
INFO - root - 2017-12-07 21:41:16.957258: step 57190, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 51h:45m:44s remains)
INFO - root - 2017-12-07 21:41:23.716731: step 57200, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.704 sec/batch; 53h:47m:53s remains)
2017-12-07 21:41:24.494507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3245192 -4.3213792 -4.3216519 -4.324007 -4.3226433 -4.3208733 -4.3215933 -4.3266697 -4.3330264 -4.338273 -4.3431225 -4.3473959 -4.3546262 -4.36297 -4.3703861][-4.3011341 -4.2931051 -4.2932444 -4.2986269 -4.2973933 -4.2933159 -4.2945905 -4.3016758 -4.3099203 -4.3167696 -4.321672 -4.3266115 -4.3392434 -4.3541446 -4.3650737][-4.2754374 -4.2635369 -4.2656503 -4.2726235 -4.2667317 -4.2578564 -4.2602911 -4.2669663 -4.2742081 -4.2819824 -4.2878561 -4.2963367 -4.3165903 -4.337296 -4.3511696][-4.2411032 -4.2294612 -4.2342176 -4.2387586 -4.2266741 -4.2137966 -4.2171474 -4.2212 -4.2256942 -4.2328682 -4.2411962 -4.2574792 -4.2864532 -4.3131218 -4.332581][-4.1915469 -4.1804814 -4.1855087 -4.1880031 -4.1732492 -4.1588407 -4.1667042 -4.169116 -4.1696696 -4.1767654 -4.1881857 -4.2126217 -4.24946 -4.2844591 -4.313499][-4.1430254 -4.1284146 -4.1305232 -4.130332 -4.1137853 -4.0999603 -4.1091695 -4.1117897 -4.1109986 -4.1232529 -4.1427565 -4.1743388 -4.2175984 -4.2596588 -4.2965126][-4.1157732 -4.0962353 -4.0938649 -4.0948296 -4.0790539 -4.062737 -4.0667424 -4.06753 -4.0694528 -4.0892034 -4.11622 -4.1525946 -4.2010188 -4.2476134 -4.28982][-4.1224933 -4.1018481 -4.0989718 -4.1033974 -4.0904183 -4.0709825 -4.0632195 -4.0582933 -4.0543747 -4.0683732 -4.0954914 -4.1355195 -4.1907992 -4.241879 -4.2878981][-4.1680455 -4.1525221 -4.147933 -4.1502733 -4.1395369 -4.122057 -4.1045475 -4.0868931 -4.0692258 -4.0691395 -4.0887189 -4.1282287 -4.1869817 -4.242239 -4.2896962][-4.2211752 -4.2076435 -4.2022123 -4.2010541 -4.1930122 -4.181119 -4.1632013 -4.1416049 -4.115231 -4.10061 -4.1078715 -4.1447163 -4.2036695 -4.25603 -4.2977409][-4.2500749 -4.2342854 -4.2246571 -4.2219462 -4.2215819 -4.2191763 -4.2093372 -4.1933346 -4.1715159 -4.155035 -4.1563711 -4.1882882 -4.2383404 -4.2790236 -4.3073397][-4.2658405 -4.2377481 -4.2122836 -4.2012248 -4.2064576 -4.2164593 -4.2223206 -4.2215452 -4.2180209 -4.2147975 -4.2149796 -4.2362709 -4.2697034 -4.2942677 -4.309772][-4.2868333 -4.2443876 -4.1983323 -4.1726623 -4.1753592 -4.1946716 -4.218976 -4.2352743 -4.2492023 -4.2570953 -4.254262 -4.2632904 -4.2817869 -4.2941718 -4.3018885][-4.30789 -4.2636261 -4.20508 -4.1624818 -4.1514335 -4.1672344 -4.199893 -4.2315035 -4.2583823 -4.271976 -4.2666268 -4.26759 -4.2769337 -4.2813993 -4.286356][-4.3182659 -4.2888703 -4.2372274 -4.1878076 -4.1591015 -4.1579618 -4.1850286 -4.223392 -4.2584119 -4.2753654 -4.2707 -4.2673607 -4.2698531 -4.2675104 -4.2703447]]...]
INFO - root - 2017-12-07 21:41:31.031754: step 57210, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.657 sec/batch; 50h:15m:30s remains)
INFO - root - 2017-12-07 21:41:37.803216: step 57220, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 53h:14m:32s remains)
INFO - root - 2017-12-07 21:41:44.579263: step 57230, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 54h:55m:44s remains)
INFO - root - 2017-12-07 21:41:51.323704: step 57240, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 52h:10m:48s remains)
INFO - root - 2017-12-07 21:41:58.095087: step 57250, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 49h:54m:27s remains)
INFO - root - 2017-12-07 21:42:04.789091: step 57260, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 49h:48m:26s remains)
INFO - root - 2017-12-07 21:42:11.785050: step 57270, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 53h:32m:50s remains)
INFO - root - 2017-12-07 21:42:18.550616: step 57280, loss = 2.05, batch loss = 2.00 (10.7 examples/sec; 0.746 sec/batch; 57h:03m:56s remains)
INFO - root - 2017-12-07 21:42:25.370237: step 57290, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 53h:33m:51s remains)
INFO - root - 2017-12-07 21:42:32.094987: step 57300, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 50h:14m:10s remains)
2017-12-07 21:42:32.815067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1489639 -4.1571975 -4.1671896 -4.1771502 -4.1913562 -4.1834807 -4.1598763 -4.1381617 -4.1396055 -4.156847 -4.1951776 -4.2388439 -4.2663035 -4.2693853 -4.2771254][-4.1728182 -4.1803837 -4.1884732 -4.1958294 -4.2009277 -4.1821003 -4.1477108 -4.1132531 -4.1045132 -4.122427 -4.1683431 -4.2232876 -4.2601929 -4.2686424 -4.280108][-4.1782913 -4.1791244 -4.1865816 -4.1945133 -4.19462 -4.1705313 -4.1278625 -4.0823307 -4.0718603 -4.0958662 -4.1479826 -4.2096782 -4.2524986 -4.2653227 -4.277966][-4.191534 -4.1844282 -4.1903691 -4.1919408 -4.1795721 -4.1414714 -4.0819631 -4.0354319 -4.0414414 -4.0788393 -4.1396317 -4.2045627 -4.24767 -4.2626514 -4.2748504][-4.211328 -4.2025213 -4.2006545 -4.1881704 -4.1596184 -4.1050954 -4.0257125 -3.9764462 -4.0020819 -4.0634866 -4.1342611 -4.2028942 -4.2440834 -4.2602191 -4.272223][-4.2370315 -4.2267637 -4.2143826 -4.1806712 -4.122232 -4.0383945 -3.9342864 -3.8797474 -3.9279618 -4.0255594 -4.1194563 -4.1954932 -4.2391768 -4.2573977 -4.269237][-4.2701654 -4.2600031 -4.237659 -4.1865091 -4.101161 -3.9828918 -3.8466892 -3.7713079 -3.8231189 -3.9480658 -4.0701075 -4.1615834 -4.2163534 -4.2441845 -4.2628341][-4.2985005 -4.2898893 -4.2592282 -4.1975408 -4.1055827 -3.9867456 -3.858891 -3.7812068 -3.8138204 -3.926384 -4.0465369 -4.1372852 -4.1928773 -4.2268744 -4.2530046][-4.3078914 -4.3034854 -4.2806573 -4.2279773 -4.1512384 -4.0584178 -3.9633245 -3.8976777 -3.9076073 -3.984808 -4.07986 -4.153491 -4.1958957 -4.2247086 -4.2512326][-4.2984047 -4.3017368 -4.2973557 -4.26817 -4.2186503 -4.1560884 -4.0859628 -4.0263319 -4.0195212 -4.0659423 -4.1340389 -4.1906147 -4.2214961 -4.2425175 -4.2632723][-4.2788105 -4.2861853 -4.2960835 -4.2885666 -4.2618842 -4.22368 -4.1757016 -4.1277118 -4.1177292 -4.1465974 -4.1925864 -4.2346535 -4.2555537 -4.2699032 -4.282918][-4.2700648 -4.2779469 -4.2915726 -4.29368 -4.2815127 -4.2596784 -4.2298117 -4.1965647 -4.1915646 -4.2114849 -4.240922 -4.2709866 -4.287178 -4.2965193 -4.3025966][-4.2714195 -4.2790918 -4.2899485 -4.2940369 -4.28795 -4.2753639 -4.25668 -4.235178 -4.2340822 -4.2501054 -4.271275 -4.2936859 -4.3097906 -4.3185954 -4.3210597][-4.2769113 -4.2832503 -4.2905607 -4.2946138 -4.2931085 -4.2860947 -4.2719488 -4.2556653 -4.2547994 -4.2667174 -4.2827663 -4.3003354 -4.317327 -4.3287883 -4.33203][-4.2876077 -4.29221 -4.2951212 -4.296648 -4.2963309 -4.2921638 -4.2826643 -4.2726107 -4.2720442 -4.2803025 -4.2912903 -4.3042593 -4.318759 -4.3302064 -4.3348475]]...]
INFO - root - 2017-12-07 21:42:39.238672: step 57310, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.688 sec/batch; 52h:34m:14s remains)
INFO - root - 2017-12-07 21:42:46.103547: step 57320, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 53h:01m:45s remains)
INFO - root - 2017-12-07 21:42:52.819221: step 57330, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.650 sec/batch; 49h:42m:20s remains)
INFO - root - 2017-12-07 21:42:59.580833: step 57340, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 49h:09m:45s remains)
INFO - root - 2017-12-07 21:43:06.397314: step 57350, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 54h:27m:52s remains)
INFO - root - 2017-12-07 21:43:13.255248: step 57360, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 56h:35m:35s remains)
INFO - root - 2017-12-07 21:43:20.027855: step 57370, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 50h:24m:24s remains)
INFO - root - 2017-12-07 21:43:26.848278: step 57380, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 50h:43m:44s remains)
INFO - root - 2017-12-07 21:43:33.610586: step 57390, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 50h:11m:56s remains)
INFO - root - 2017-12-07 21:43:40.455884: step 57400, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 53h:10m:56s remains)
2017-12-07 21:43:41.092789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2495203 -4.2460537 -4.2517796 -4.2500868 -4.2358756 -4.2227874 -4.2177844 -4.211267 -4.1939692 -4.1695318 -4.1528425 -4.1504211 -4.1529555 -4.1609015 -4.1772785][-4.2246523 -4.2287908 -4.2485213 -4.2571249 -4.2432809 -4.2183533 -4.2008233 -4.1897731 -4.1737475 -4.1484308 -4.1308336 -4.1331849 -4.1368818 -4.1421542 -4.1590571][-4.1965942 -4.2087822 -4.2380085 -4.257277 -4.2447443 -4.2082434 -4.1765847 -4.1663156 -4.1632833 -4.1504512 -4.1423039 -4.1507325 -4.1542287 -4.1539254 -4.1643114][-4.1620889 -4.1774106 -4.213418 -4.2401943 -4.2324476 -4.1884217 -4.1444831 -4.1391869 -4.1585956 -4.1669617 -4.1689377 -4.1796403 -4.179575 -4.1716547 -4.1716046][-4.1327095 -4.1451941 -4.1782012 -4.2034392 -4.1977758 -4.14645 -4.087976 -4.0879087 -4.1342621 -4.1669602 -4.1795282 -4.1909142 -4.1870871 -4.1734443 -4.1645722][-4.1134348 -4.1225104 -4.1498036 -4.1678038 -4.153832 -4.0838666 -3.9979296 -3.9935675 -4.072041 -4.1349964 -4.1669507 -4.1857133 -4.1868019 -4.1713967 -4.1568575][-4.1098561 -4.1110468 -4.1278653 -4.1339879 -4.1018505 -4.0059233 -3.8785462 -3.8566792 -3.9766619 -4.0824862 -4.1389189 -4.1693392 -4.176641 -4.1592226 -4.13892][-4.1193213 -4.1062551 -4.1076517 -4.1037893 -4.0630913 -3.9570792 -3.8079386 -3.770916 -3.9142897 -4.0495229 -4.1267338 -4.1691365 -4.1830382 -4.1624022 -4.1331792][-4.1387005 -4.1153631 -4.109787 -4.1058288 -4.0764661 -4.0001 -3.890079 -3.8596888 -3.967732 -4.0809569 -4.1513524 -4.1924376 -4.2061715 -4.181417 -4.1444588][-4.1664071 -4.1464543 -4.1450129 -4.1443281 -4.1312332 -4.090167 -4.0251527 -4.0054483 -4.0675917 -4.1401143 -4.1870322 -4.2149844 -4.2220521 -4.1993871 -4.1634126][-4.1959848 -4.1837444 -4.1874628 -4.1906 -4.1875877 -4.1686749 -4.1337714 -4.1187849 -4.1486721 -4.189003 -4.21752 -4.2349792 -4.2361183 -4.213984 -4.1840682][-4.2128243 -4.208364 -4.2175703 -4.2269483 -4.2315531 -4.2219396 -4.1990705 -4.1855297 -4.2015533 -4.2266169 -4.24292 -4.2497311 -4.2443218 -4.2227321 -4.202127][-4.2149887 -4.2164421 -4.2271714 -4.2397509 -4.2491879 -4.2468548 -4.2327204 -4.2233338 -4.2316346 -4.2487 -4.2573676 -4.2584767 -4.2509079 -4.2351665 -4.2231936][-4.2192631 -4.2229586 -4.2306237 -4.2396417 -4.2494416 -4.2536244 -4.2494984 -4.2438741 -4.2474031 -4.2579174 -4.2639289 -4.2623329 -4.2539048 -4.244668 -4.241127][-4.2301612 -4.2349892 -4.2409248 -4.2461104 -4.2526178 -4.2554865 -4.252564 -4.246911 -4.2445765 -4.2484059 -4.2527771 -4.2499743 -4.2408695 -4.2349968 -4.2363758]]...]
INFO - root - 2017-12-07 21:43:47.723584: step 57410, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 50h:39m:48s remains)
INFO - root - 2017-12-07 21:43:54.595335: step 57420, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 51h:56m:35s remains)
INFO - root - 2017-12-07 21:44:01.541379: step 57430, loss = 2.04, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 56h:18m:16s remains)
INFO - root - 2017-12-07 21:44:08.318685: step 57440, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.731 sec/batch; 55h:52m:54s remains)
INFO - root - 2017-12-07 21:44:15.170696: step 57450, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 49h:31m:02s remains)
INFO - root - 2017-12-07 21:44:22.060015: step 57460, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 49h:48m:51s remains)
INFO - root - 2017-12-07 21:44:28.834721: step 57470, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 53h:26m:24s remains)
INFO - root - 2017-12-07 21:44:35.611224: step 57480, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 54h:47m:40s remains)
INFO - root - 2017-12-07 21:44:42.337173: step 57490, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 54h:07m:40s remains)
INFO - root - 2017-12-07 21:44:49.123616: step 57500, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 49h:14m:54s remains)
2017-12-07 21:44:49.891025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3570886 -4.3799348 -4.3809881 -4.3592653 -4.3172755 -4.2660737 -4.2081952 -4.1601787 -4.13407 -4.1476622 -4.190764 -4.23482 -4.2713051 -4.2977324 -4.312851][-4.3645282 -4.3920207 -4.3905945 -4.361702 -4.3108563 -4.2488694 -4.1840138 -4.1416478 -4.1238351 -4.1449218 -4.19387 -4.2379327 -4.27139 -4.2962251 -4.3123264][-4.37 -4.3970728 -4.3914466 -4.3579721 -4.3029575 -4.2350564 -4.1691203 -4.1403565 -4.1362953 -4.1587367 -4.1970181 -4.2333136 -4.2643733 -4.2889786 -4.3099265][-4.3792167 -4.4005451 -4.3883061 -4.3470583 -4.286159 -4.2126775 -4.1464911 -4.126121 -4.1353827 -4.1611023 -4.1905856 -4.2214608 -4.2525363 -4.2811155 -4.3103547][-4.383534 -4.3962636 -4.3715549 -4.3151979 -4.2440577 -4.1631737 -4.0877371 -4.0627756 -4.0835366 -4.1202564 -4.1495624 -4.1872735 -4.2299118 -4.269206 -4.3045754][-4.3720903 -4.3722978 -4.3307719 -4.2582674 -4.173461 -4.0828328 -3.9964786 -3.9616153 -3.9879956 -4.03755 -4.0798874 -4.1353078 -4.200552 -4.25113 -4.2894392][-4.3440938 -4.3303781 -4.2743373 -4.1885595 -4.0944781 -4.003231 -3.9229167 -3.8858502 -3.9097729 -3.9634094 -4.02096 -4.0938244 -4.1712818 -4.2257743 -4.2623277][-4.3160958 -4.29577 -4.2387247 -4.15488 -4.0668726 -3.9903655 -3.9284148 -3.8970933 -3.9115052 -3.9599724 -4.0223641 -4.0976362 -4.165247 -4.2064118 -4.228426][-4.3080735 -4.2933145 -4.2539263 -4.192637 -4.1245794 -4.0717044 -4.0355234 -4.0170283 -4.0173059 -4.0506067 -4.1049309 -4.1572628 -4.1870842 -4.1988564 -4.1976194][-4.3195167 -4.3196707 -4.3078055 -4.2785954 -4.2348027 -4.2001953 -4.1791248 -4.1662588 -4.1551909 -4.1673255 -4.2041225 -4.2268682 -4.2202473 -4.1988645 -4.1651983][-4.3243842 -4.3400154 -4.3497076 -4.34088 -4.3109922 -4.2832336 -4.2624536 -4.247179 -4.2372565 -4.2450132 -4.2716751 -4.2752156 -4.2476292 -4.1993561 -4.141315][-4.3033776 -4.3291774 -4.3518806 -4.3497009 -4.3265052 -4.304503 -4.2842703 -4.2736292 -4.27389 -4.2880497 -4.3122435 -4.308682 -4.2797289 -4.2226262 -4.1569719][-4.2627211 -4.2914758 -4.31507 -4.3107219 -4.2870474 -4.2678065 -4.2444348 -4.2377315 -4.2518234 -4.2804379 -4.3156137 -4.3283234 -4.315536 -4.2684321 -4.2100267][-4.2246933 -4.2475419 -4.2610526 -4.2512417 -4.2218542 -4.1964769 -4.1635571 -4.15573 -4.1845117 -4.2362289 -4.2938004 -4.3338876 -4.3435159 -4.3149557 -4.2700696][-4.1857276 -4.1980491 -4.20268 -4.1905394 -4.1578531 -4.129324 -4.0960569 -4.086154 -4.1231818 -4.1899309 -4.2583036 -4.3121686 -4.3344874 -4.3155794 -4.2782621]]...]
INFO - root - 2017-12-07 21:44:56.352112: step 57510, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 54h:02m:20s remains)
INFO - root - 2017-12-07 21:45:03.143918: step 57520, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 51h:55m:32s remains)
INFO - root - 2017-12-07 21:45:10.023348: step 57530, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 50h:40m:15s remains)
INFO - root - 2017-12-07 21:45:16.818069: step 57540, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 52h:13m:33s remains)
INFO - root - 2017-12-07 21:45:23.677998: step 57550, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 53h:47m:44s remains)
INFO - root - 2017-12-07 21:45:30.410204: step 57560, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 53h:20m:42s remains)
INFO - root - 2017-12-07 21:45:37.235916: step 57570, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 48h:52m:39s remains)
INFO - root - 2017-12-07 21:45:44.012589: step 57580, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.630 sec/batch; 48h:07m:39s remains)
INFO - root - 2017-12-07 21:45:50.840829: step 57590, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 53h:27m:02s remains)
INFO - root - 2017-12-07 21:45:57.582306: step 57600, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 51h:05m:00s remains)
2017-12-07 21:45:58.352542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2864056 -4.2701745 -4.2585249 -4.2460957 -4.2247972 -4.2058611 -4.2015195 -4.214571 -4.2318029 -4.2409415 -4.2367797 -4.236846 -4.2450285 -4.2602611 -4.2687507][-4.2887573 -4.271493 -4.2609677 -4.2478952 -4.2265611 -4.2039461 -4.1937637 -4.2075505 -4.2291131 -4.2381668 -4.2322836 -4.2287836 -4.2344112 -4.2486868 -4.2595711][-4.2939563 -4.2713513 -4.2571564 -4.2444448 -4.2234139 -4.1951561 -4.1769962 -4.1911049 -4.2218866 -4.23675 -4.2314377 -4.224771 -4.2259116 -4.2367682 -4.2497258][-4.2980509 -4.2691483 -4.250515 -4.241024 -4.22603 -4.1928554 -4.1659365 -4.1809292 -4.2156053 -4.2365222 -4.2375326 -4.2321424 -4.2281647 -4.2345462 -4.2491841][-4.3052082 -4.270854 -4.2443514 -4.23405 -4.2179923 -4.174356 -4.1324558 -4.147419 -4.1855521 -4.2169194 -4.23089 -4.2267566 -4.21683 -4.219511 -4.2314305][-4.317059 -4.2838435 -4.2469873 -4.2230272 -4.1896777 -4.1195807 -4.0457721 -4.0628729 -4.12093 -4.168335 -4.1927447 -4.1868072 -4.1713848 -4.1725364 -4.1803703][-4.3181891 -4.2868662 -4.2348061 -4.184145 -4.1144986 -3.9958017 -3.8681264 -3.8995852 -4.0020485 -4.0806527 -4.1179342 -4.1168637 -4.103282 -4.1102419 -4.1178331][-4.3071394 -4.2740388 -4.2096286 -4.1341038 -4.0254784 -3.8586171 -3.6744051 -3.7228005 -3.8727686 -3.9863539 -4.0388331 -4.0516434 -4.0561562 -4.0748062 -4.0867596][-4.2985921 -4.2747793 -4.2238173 -4.1588769 -4.0581312 -3.9080393 -3.7438288 -3.7745166 -3.9009144 -4.0012479 -4.0464144 -4.0597563 -4.0738916 -4.103405 -4.1223025][-4.292388 -4.2853665 -4.2624931 -4.2309513 -4.1738167 -4.0873871 -3.9895427 -3.9887762 -4.0442128 -4.0932126 -4.1074548 -4.10518 -4.121881 -4.1599379 -4.1794229][-4.2986131 -4.3025646 -4.2946482 -4.2786727 -4.2489395 -4.20744 -4.1503363 -4.1308985 -4.1435122 -4.1644788 -4.1640267 -4.1532159 -4.1707015 -4.2090778 -4.2274437][-4.3080316 -4.3161869 -4.3162251 -4.3047252 -4.2861395 -4.2648711 -4.2316933 -4.2101097 -4.2072816 -4.2139916 -4.2026405 -4.1851707 -4.2033429 -4.2399359 -4.258152][-4.3048038 -4.3075919 -4.3082657 -4.2985992 -4.2853112 -4.2779236 -4.2623129 -4.2469707 -4.2341022 -4.2237606 -4.198195 -4.1781473 -4.1928673 -4.2228804 -4.2364316][-4.3017969 -4.2858019 -4.2732263 -4.2613816 -4.2519641 -4.2510214 -4.2462392 -4.2414241 -4.2290978 -4.2097588 -4.1792064 -4.1610284 -4.1664867 -4.1811752 -4.1822171][-4.3054395 -4.2724915 -4.2419009 -4.2215519 -4.2108517 -4.2140536 -4.2174859 -4.2240648 -4.2250485 -4.2115045 -4.189589 -4.1803555 -4.1815977 -4.1790285 -4.1587372]]...]
INFO - root - 2017-12-07 21:46:04.984997: step 57610, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 50h:29m:00s remains)
INFO - root - 2017-12-07 21:46:11.667933: step 57620, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 54h:36m:28s remains)
INFO - root - 2017-12-07 21:46:18.505572: step 57630, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 55h:24m:49s remains)
INFO - root - 2017-12-07 21:46:25.357701: step 57640, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 50h:25m:43s remains)
INFO - root - 2017-12-07 21:46:32.129008: step 57650, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 49h:17m:37s remains)
INFO - root - 2017-12-07 21:46:38.925190: step 57660, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.693 sec/batch; 52h:52m:12s remains)
INFO - root - 2017-12-07 21:46:45.832163: step 57670, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 53h:27m:13s remains)
INFO - root - 2017-12-07 21:46:52.602604: step 57680, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 51h:47m:02s remains)
INFO - root - 2017-12-07 21:46:59.370084: step 57690, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 48h:10m:12s remains)
INFO - root - 2017-12-07 21:47:06.225824: step 57700, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:28m:05s remains)
2017-12-07 21:47:07.078080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104105 -4.3123636 -4.3098559 -4.3036218 -4.2964115 -4.2804546 -4.2498217 -4.2102852 -4.1714978 -4.1528435 -4.156321 -4.1734209 -4.1982317 -4.2120223 -4.2042117][-4.31818 -4.3161359 -4.3093252 -4.2988772 -4.283195 -4.253768 -4.2095003 -4.1534481 -4.1027765 -4.0833387 -4.0955057 -4.1236081 -4.1603785 -4.1818085 -4.1698451][-4.3089409 -4.3015242 -4.2902179 -4.2761779 -4.2503953 -4.2061629 -4.1471653 -4.0771441 -4.0200229 -4.0063491 -4.0358577 -4.0825043 -4.1336727 -4.1552534 -4.1323805][-4.2941289 -4.27804 -4.2665529 -4.2565241 -4.2305794 -4.1795144 -4.1104627 -4.0357943 -3.9896092 -3.9967458 -4.0446754 -4.1004348 -4.14707 -4.1564212 -4.124393][-4.2724657 -4.2500148 -4.2429547 -4.2434397 -4.2228789 -4.1708403 -4.0990963 -4.03118 -4.0088849 -4.0370703 -4.0899477 -4.1386337 -4.1703529 -4.1626582 -4.1184063][-4.2514248 -4.22557 -4.2222266 -4.2325468 -4.2214012 -4.17219 -4.1034646 -4.0468831 -4.0417137 -4.0789061 -4.1293225 -4.1671429 -4.1837521 -4.1610541 -4.1098871][-4.2355032 -4.2087812 -4.2093391 -4.2267923 -4.22196 -4.1705728 -4.0997143 -4.0479827 -4.048367 -4.0904374 -4.1435719 -4.1748052 -4.1783705 -4.1418343 -4.0883765][-4.2179356 -4.1991897 -4.2105088 -4.2351956 -4.2307639 -4.1727905 -4.095078 -4.037581 -4.0398803 -4.0906677 -4.1491542 -4.1783128 -4.1718683 -4.1270838 -4.0800285][-4.1996069 -4.1985269 -4.2246122 -4.2558589 -4.2527885 -4.1878695 -4.0996065 -4.0383883 -4.0475068 -4.10663 -4.163168 -4.1885481 -4.1762538 -4.1354628 -4.0981379][-4.1893339 -4.2042985 -4.2416754 -4.2790904 -4.277421 -4.2102261 -4.1205897 -4.0669937 -4.0835824 -4.140007 -4.1884866 -4.2102294 -4.1930685 -4.1581054 -4.124341][-4.1960721 -4.2160759 -4.2556868 -4.2946267 -4.2954984 -4.23761 -4.1610384 -4.120266 -4.1350207 -4.179409 -4.2200046 -4.2374229 -4.2190132 -4.188868 -4.1580362][-4.2225542 -4.2371788 -4.2682314 -4.303576 -4.308229 -4.2688365 -4.2131791 -4.1814637 -4.1887403 -4.221458 -4.2562532 -4.2700429 -4.2556667 -4.2305889 -4.2029147][-4.2651215 -4.2697806 -4.2852421 -4.3125343 -4.3215914 -4.2985811 -4.261662 -4.2349706 -4.2359519 -4.2604914 -4.290194 -4.3040276 -4.2973714 -4.279243 -4.2557044][-4.3014345 -4.3013072 -4.3068762 -4.3267546 -4.3380179 -4.3251281 -4.2996621 -4.276443 -4.2727289 -4.2896233 -4.3132472 -4.32766 -4.3296523 -4.3195848 -4.3004184][-4.3155675 -4.3194804 -4.323957 -4.3379016 -4.3482056 -4.3427539 -4.3245893 -4.3021231 -4.2926435 -4.3013306 -4.3172379 -4.3308048 -4.338088 -4.3354282 -4.3243542]]...]
INFO - root - 2017-12-07 21:47:13.670512: step 57710, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 49h:26m:33s remains)
INFO - root - 2017-12-07 21:47:20.333279: step 57720, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 49h:23m:42s remains)
INFO - root - 2017-12-07 21:47:27.047578: step 57730, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 49h:03m:03s remains)
INFO - root - 2017-12-07 21:47:33.821572: step 57740, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 55h:09m:12s remains)
INFO - root - 2017-12-07 21:47:40.568229: step 57750, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 52h:34m:28s remains)
INFO - root - 2017-12-07 21:47:47.297931: step 57760, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 50h:02m:28s remains)
INFO - root - 2017-12-07 21:47:54.013627: step 57770, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 51h:13m:20s remains)
INFO - root - 2017-12-07 21:48:00.915816: step 57780, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 52h:55m:06s remains)
INFO - root - 2017-12-07 21:48:07.751925: step 57790, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 54h:12m:21s remains)
INFO - root - 2017-12-07 21:48:14.543016: step 57800, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 53h:39m:07s remains)
2017-12-07 21:48:15.242877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1946864 -4.19356 -4.214694 -4.2399015 -4.2579103 -4.2651396 -4.2658291 -4.2645507 -4.2628088 -4.2603569 -4.2524447 -4.2505469 -4.2562757 -4.2667284 -4.2742314][-4.1743836 -4.1714244 -4.1922894 -4.2281928 -4.2609744 -4.2764721 -4.2805176 -4.2760286 -4.2675328 -4.2605991 -4.2552738 -4.259304 -4.2665339 -4.2756357 -4.2822185][-4.13969 -4.141911 -4.1671076 -4.2065558 -4.2508979 -4.2724137 -4.2798729 -4.2732215 -4.255765 -4.2445121 -4.2455153 -4.2562618 -4.2669048 -4.2781854 -4.2872458][-4.129024 -4.1351705 -4.162498 -4.1961665 -4.2402267 -4.2635775 -4.2667809 -4.253437 -4.2295957 -4.2172809 -4.2228184 -4.2379851 -4.2532339 -4.2703524 -4.2832003][-4.1410875 -4.1468644 -4.1705217 -4.1959267 -4.2320695 -4.2497478 -4.2456479 -4.2235446 -4.19835 -4.1928945 -4.2006478 -4.2136726 -4.2288246 -4.2520342 -4.2724614][-4.1607294 -4.1680045 -4.1883521 -4.2080717 -4.2341151 -4.2426667 -4.2260566 -4.1973 -4.1745968 -4.1785512 -4.1959567 -4.2115526 -4.2232447 -4.2457681 -4.26868][-4.1745276 -4.183044 -4.2042379 -4.2268562 -4.2436485 -4.2384086 -4.2100625 -4.1762629 -4.1563969 -4.170804 -4.2042565 -4.2291937 -4.2408323 -4.2585258 -4.2765741][-4.1866503 -4.1926761 -4.2141023 -4.2459192 -4.2575636 -4.2385945 -4.1970038 -4.1577659 -4.1407461 -4.1628695 -4.2097158 -4.246635 -4.2620988 -4.275816 -4.2873459][-4.1858 -4.1877131 -4.2111058 -4.2512684 -4.2654042 -4.2391567 -4.1883535 -4.1439662 -4.1313658 -4.1604877 -4.20964 -4.2513018 -4.2737474 -4.2843194 -4.2898016][-4.18731 -4.1807518 -4.200881 -4.24598 -4.2675285 -4.2438183 -4.1914163 -4.1425557 -4.1274772 -4.156827 -4.2055964 -4.2472544 -4.2736979 -4.2852244 -4.2879057][-4.2073579 -4.1926732 -4.2056994 -4.2461033 -4.2710247 -4.257 -4.2134123 -4.1674113 -4.1437855 -4.16498 -4.2104907 -4.2497907 -4.2760878 -4.2889152 -4.2921257][-4.2340813 -4.2160649 -4.2207346 -4.2509742 -4.2745781 -4.2705355 -4.2400146 -4.203815 -4.1761088 -4.1879816 -4.2283077 -4.2651649 -4.2885742 -4.2995682 -4.302886][-4.2696185 -4.2538023 -4.2529449 -4.2731481 -4.291153 -4.2892156 -4.2694778 -4.2437148 -4.2179413 -4.2216806 -4.2541528 -4.2864084 -4.3055148 -4.3157969 -4.3184466][-4.2981081 -4.2863092 -4.2829375 -4.2966456 -4.3105659 -4.3102107 -4.3004131 -4.2819061 -4.259676 -4.2572622 -4.2796297 -4.3020792 -4.3145227 -4.3240743 -4.3286972][-4.31324 -4.3065519 -4.3019276 -4.3072882 -4.3164687 -4.3192687 -4.3161178 -4.3052545 -4.2878933 -4.282867 -4.297071 -4.3125052 -4.3201847 -4.3275843 -4.3331804]]...]
INFO - root - 2017-12-07 21:48:21.925940: step 57810, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 53h:51m:40s remains)
INFO - root - 2017-12-07 21:48:28.712838: step 57820, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 52h:55m:27s remains)
INFO - root - 2017-12-07 21:48:35.501260: step 57830, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 52h:28m:45s remains)
INFO - root - 2017-12-07 21:48:42.274703: step 57840, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.675 sec/batch; 51h:31m:41s remains)
INFO - root - 2017-12-07 21:48:48.936188: step 57850, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 48h:10m:27s remains)
INFO - root - 2017-12-07 21:48:55.689529: step 57860, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.685 sec/batch; 52h:14m:32s remains)
INFO - root - 2017-12-07 21:49:02.427354: step 57870, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.702 sec/batch; 53h:33m:19s remains)
INFO - root - 2017-12-07 21:49:09.275806: step 57880, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 53h:07m:00s remains)
INFO - root - 2017-12-07 21:49:16.066842: step 57890, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 50h:51m:37s remains)
INFO - root - 2017-12-07 21:49:22.791875: step 57900, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 52h:07m:12s remains)
2017-12-07 21:49:23.547995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2499309 -4.2557683 -4.2610354 -4.2720695 -4.2792888 -4.2806568 -4.275198 -4.2665544 -4.259541 -4.2538171 -4.2518706 -4.2568674 -4.2623363 -4.2656693 -4.267036][-4.2269139 -4.2375436 -4.2452788 -4.2528329 -4.2531581 -4.2448735 -4.2352252 -4.2259078 -4.2223048 -4.2236929 -4.2256289 -4.236536 -4.2473793 -4.2502561 -4.2491293][-4.2140832 -4.2264967 -4.2318316 -4.2295074 -4.213212 -4.1832547 -4.1518011 -4.1370068 -4.1511436 -4.1756167 -4.1950507 -4.2180233 -4.2354522 -4.2381763 -4.2332053][-4.2133985 -4.2225256 -4.2214713 -4.2100639 -4.16951 -4.0978961 -4.0264344 -4.0068493 -4.0553651 -4.1199722 -4.1664314 -4.2050843 -4.2274065 -4.2306094 -4.2215042][-4.2230706 -4.2306037 -4.224051 -4.2034154 -4.1375914 -4.0212564 -3.9061832 -3.8866308 -3.972954 -4.0776014 -4.1499996 -4.1992664 -4.2190013 -4.2159076 -4.1999025][-4.2463007 -4.2521644 -4.2421641 -4.21312 -4.1306515 -3.989553 -3.8635483 -3.8589973 -3.966795 -4.0863376 -4.1618776 -4.19937 -4.2093315 -4.1921792 -4.1674838][-4.2589517 -4.2672715 -4.257781 -4.2244806 -4.1446595 -4.01503 -3.9141912 -3.929229 -4.0320849 -4.137403 -4.1965342 -4.2110257 -4.204844 -4.1732178 -4.1395063][-4.246901 -4.2580762 -4.2559304 -4.2298307 -4.1709924 -4.0783134 -4.0113711 -4.0314512 -4.1110368 -4.1842608 -4.2219243 -4.2206507 -4.2017584 -4.1637335 -4.1278653][-4.2017159 -4.2142515 -4.2194405 -4.2039309 -4.1681652 -4.1190734 -4.0916619 -4.1130872 -4.1656818 -4.2037587 -4.218864 -4.2037449 -4.1797042 -4.1464143 -4.1208849][-4.142612 -4.1568985 -4.1680751 -4.1620336 -4.140604 -4.1223478 -4.1256652 -4.14916 -4.1824584 -4.1959496 -4.1936569 -4.173017 -4.1524 -4.1306677 -4.1219034][-4.1239486 -4.140183 -4.1502848 -4.1449418 -4.1276569 -4.1243186 -4.1389308 -4.1617503 -4.1889582 -4.191885 -4.1800251 -4.1613665 -4.1488056 -4.137248 -4.1388512][-4.1617365 -4.1702476 -4.1721182 -4.1621113 -4.1494756 -4.1550035 -4.1682682 -4.1820726 -4.2020969 -4.2033072 -4.1918488 -4.1811838 -4.1737328 -4.1677904 -4.1739902][-4.2246289 -4.2198682 -4.211864 -4.201498 -4.1966572 -4.2075896 -4.2182465 -4.2238159 -4.2350044 -4.2334394 -4.2199969 -4.2132945 -4.2121253 -4.2102141 -4.2149696][-4.2888932 -4.2773957 -4.2636914 -4.2537532 -4.253623 -4.2675157 -4.2766404 -4.2786865 -4.2836738 -4.277998 -4.2614665 -4.2540746 -4.254065 -4.2507811 -4.2524757][-4.3250289 -4.3170667 -4.30476 -4.2988205 -4.3026595 -4.3153958 -4.32093 -4.3208704 -4.3205719 -4.3131905 -4.3007441 -4.2959991 -4.2953558 -4.2895179 -4.287776]]...]
INFO - root - 2017-12-07 21:49:30.214834: step 57910, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.705 sec/batch; 53h:44m:36s remains)
INFO - root - 2017-12-07 21:49:36.900599: step 57920, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 49h:10m:10s remains)
INFO - root - 2017-12-07 21:49:43.613983: step 57930, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 53h:21m:16s remains)
INFO - root - 2017-12-07 21:49:50.470863: step 57940, loss = 2.06, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 55h:36m:38s remains)
INFO - root - 2017-12-07 21:49:57.172322: step 57950, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 53h:01m:40s remains)
INFO - root - 2017-12-07 21:50:03.912784: step 57960, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 50h:49m:41s remains)
INFO - root - 2017-12-07 21:50:10.762673: step 57970, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 48h:54m:01s remains)
INFO - root - 2017-12-07 21:50:17.532429: step 57980, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 54h:49m:00s remains)
INFO - root - 2017-12-07 21:50:24.264973: step 57990, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 52h:12m:06s remains)
INFO - root - 2017-12-07 21:50:30.997542: step 58000, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 50h:28m:17s remains)
2017-12-07 21:50:31.804578: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3329916 -4.3263931 -4.3247571 -4.3277168 -4.3320565 -4.3343387 -4.3380027 -4.3453636 -4.3517256 -4.3557281 -4.3598275 -4.3637328 -4.3675365 -4.3708396 -4.3735709][-4.3098197 -4.3000703 -4.3000197 -4.3062043 -4.3116608 -4.3116961 -4.3151412 -4.326263 -4.33628 -4.342237 -4.3466663 -4.3514643 -4.359097 -4.3657813 -4.3707838][-4.273026 -4.2603855 -4.2640853 -4.2789583 -4.2896814 -4.2897053 -4.2935328 -4.3082767 -4.3194132 -4.3244476 -4.3287005 -4.3346038 -4.3453979 -4.3555088 -4.3640351][-4.2280703 -4.2147579 -4.221559 -4.2414618 -4.2576523 -4.2627664 -4.270401 -4.287251 -4.2950158 -4.29784 -4.3031325 -4.313602 -4.3293734 -4.342629 -4.3554411][-4.1780491 -4.1684866 -4.1728244 -4.1879687 -4.2053623 -4.2205415 -4.2401295 -4.2601147 -4.2647324 -4.2665811 -4.2706995 -4.2823029 -4.3015761 -4.3185916 -4.3364897][-4.1172314 -4.1117096 -4.116272 -4.1239958 -4.1375232 -4.1560326 -4.1833014 -4.2130165 -4.2244463 -4.2289114 -4.2283368 -4.2391543 -4.2633152 -4.28588 -4.31072][-4.0622973 -4.0514417 -4.0445557 -4.0424838 -4.0521355 -4.0725594 -4.1062212 -4.1455879 -4.1681795 -4.1778612 -4.1780677 -4.192636 -4.223381 -4.2517171 -4.2841105][-4.0359211 -4.0187297 -3.9999616 -3.9878311 -3.9941022 -4.010592 -4.0475817 -4.0951438 -4.1221271 -4.1352067 -4.1399889 -4.1602306 -4.1980309 -4.2319808 -4.2702231][-4.04961 -4.0409808 -4.0225592 -4.0019293 -3.9983218 -4.0028892 -4.032639 -4.0839424 -4.1098347 -4.1223454 -4.1344242 -4.1603637 -4.202394 -4.2358108 -4.2742939][-4.0757194 -4.0888095 -4.0816164 -4.0591383 -4.0416012 -4.033772 -4.0502815 -4.0935006 -4.1137733 -4.1282997 -4.1446638 -4.1721559 -4.2157989 -4.2509823 -4.2865472][-4.06683 -4.0974555 -4.1081362 -4.097755 -4.0796814 -4.0646877 -4.0710969 -4.09862 -4.1119757 -4.1320262 -4.1542673 -4.1827879 -4.2251015 -4.26067 -4.2942924][-4.01493 -4.047545 -4.074779 -4.08177 -4.0728579 -4.0613379 -4.0690994 -4.0896845 -4.0951304 -4.1195045 -4.147543 -4.178792 -4.2232614 -4.2617497 -4.2964392][-3.9672973 -3.986702 -4.0213151 -4.0419097 -4.0490937 -4.0484629 -4.0686746 -4.0971112 -4.1025581 -4.1195931 -4.1410775 -4.1717706 -4.2190638 -4.2627 -4.3014355][-3.971458 -3.9730773 -4.0002713 -4.0241227 -4.0393777 -4.0478544 -4.0802727 -4.1205988 -4.1362505 -4.1443739 -4.1488614 -4.1719346 -4.2147889 -4.2601528 -4.3041353][-4.0280409 -4.0183024 -4.0269618 -4.0403385 -4.0533977 -4.06739 -4.1042404 -4.1496782 -4.174027 -4.1777186 -4.1713147 -4.1874666 -4.2228937 -4.2656631 -4.3089571]]...]
INFO - root - 2017-12-07 21:50:38.387615: step 58010, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 49h:47m:55s remains)
INFO - root - 2017-12-07 21:50:45.075792: step 58020, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.693 sec/batch; 52h:50m:15s remains)
INFO - root - 2017-12-07 21:50:51.873857: step 58030, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 51h:01m:28s remains)
INFO - root - 2017-12-07 21:50:58.517129: step 58040, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.632 sec/batch; 48h:12m:05s remains)
INFO - root - 2017-12-07 21:51:05.281801: step 58050, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 49h:45m:26s remains)
INFO - root - 2017-12-07 21:51:12.099261: step 58060, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 52h:06m:28s remains)
INFO - root - 2017-12-07 21:51:18.870537: step 58070, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 51h:35m:12s remains)
INFO - root - 2017-12-07 21:51:25.654867: step 58080, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.683 sec/batch; 52h:03m:14s remains)
INFO - root - 2017-12-07 21:51:32.434236: step 58090, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 49h:15m:35s remains)
INFO - root - 2017-12-07 21:51:39.254473: step 58100, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 50h:16m:49s remains)
2017-12-07 21:51:39.999032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2718086 -4.3105989 -4.3278775 -4.3071733 -4.2516141 -4.1902809 -4.1430235 -4.1294432 -4.1494088 -4.1792831 -4.2030058 -4.1866231 -4.1388736 -4.0738254 -4.0295815][-4.2471333 -4.2903538 -4.3060508 -4.2817822 -4.2305551 -4.1781931 -4.1446939 -4.1317272 -4.1411529 -4.1649008 -4.1939254 -4.1829267 -4.1342859 -4.0589676 -4.0063057][-4.2233734 -4.2555127 -4.2592087 -4.2294755 -4.1860433 -4.1514859 -4.1351271 -4.1282463 -4.1337581 -4.1555495 -4.1852326 -4.1768441 -4.12461 -4.0434041 -3.9889789][-4.2019525 -4.2230797 -4.2207279 -4.1850615 -4.1454878 -4.1269383 -4.1230507 -4.1218958 -4.1290169 -4.1542034 -4.18729 -4.1801095 -4.122252 -4.0405893 -3.9901936][-4.171946 -4.1917882 -4.1940203 -4.1564617 -4.1184163 -4.10981 -4.1145158 -4.1201396 -4.1336164 -4.1618977 -4.1965408 -4.1897049 -4.1277394 -4.0532284 -4.0110993][-4.1298666 -4.1517782 -4.1686916 -4.1394324 -4.1024346 -4.0960135 -4.1019487 -4.1072712 -4.1239276 -4.1524906 -4.1834326 -4.1767263 -4.1165323 -4.0541511 -4.0245633][-4.098856 -4.1174622 -4.1455722 -4.1261759 -4.0916634 -4.0787721 -4.0759354 -4.0745487 -4.0950556 -4.1268177 -4.1582422 -4.1519232 -4.0974464 -4.0445585 -4.0266943][-4.0697145 -4.0875516 -4.1135693 -4.102891 -4.0773349 -4.0565476 -4.0396318 -4.0329161 -4.0580997 -4.0974288 -4.1268892 -4.1187754 -4.0662932 -4.017755 -4.0078263][-4.0675983 -4.0847492 -4.1021791 -4.0949655 -4.0774069 -4.0549011 -4.0339003 -4.0255265 -4.0525784 -4.096046 -4.1190844 -4.104001 -4.0512643 -4.0086942 -4.0075617][-4.114727 -4.1321478 -4.1478319 -4.1441607 -4.1276088 -4.1069951 -4.0832896 -4.0718122 -4.0952463 -4.1351423 -4.1513042 -4.1292906 -4.0858312 -4.0548458 -4.0580587][-4.172195 -4.1901479 -4.2073107 -4.2071881 -4.1972919 -4.1847029 -4.1653671 -4.1552105 -4.1697774 -4.1930985 -4.1990929 -4.1804314 -4.1509089 -4.1298337 -4.1328921][-4.2314181 -4.2457876 -4.2595811 -4.2594795 -4.2546134 -4.2489772 -4.2368164 -4.2343836 -4.2464595 -4.2590661 -4.258604 -4.2434092 -4.2221589 -4.2095819 -4.2112393][-4.2777715 -4.2904229 -4.2997861 -4.2964468 -4.2916818 -4.2894378 -4.2846251 -4.2865233 -4.2950816 -4.3024249 -4.3034539 -4.2920027 -4.2759781 -4.27018 -4.2699251][-4.3052025 -4.3151188 -4.3211784 -4.318644 -4.3158116 -4.3130283 -4.3098354 -4.3134036 -4.3187342 -4.3230991 -4.3244834 -4.316823 -4.3039141 -4.2975116 -4.2961717][-4.311399 -4.3185353 -4.3220129 -4.3216949 -4.3211489 -4.3197956 -4.3185649 -4.3210073 -4.3226662 -4.3229685 -4.3225918 -4.3179369 -4.3105216 -4.3054166 -4.3048868]]...]
INFO - root - 2017-12-07 21:51:46.601491: step 58110, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 51h:25m:11s remains)
INFO - root - 2017-12-07 21:51:53.270782: step 58120, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 49h:13m:02s remains)
INFO - root - 2017-12-07 21:52:00.153238: step 58130, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 49h:57m:32s remains)
INFO - root - 2017-12-07 21:52:06.956079: step 58140, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.705 sec/batch; 53h:43m:52s remains)
INFO - root - 2017-12-07 21:52:13.791332: step 58150, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.760 sec/batch; 57h:56m:51s remains)
INFO - root - 2017-12-07 21:52:20.715073: step 58160, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 51h:39m:53s remains)
INFO - root - 2017-12-07 21:52:27.456903: step 58170, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 49h:08m:30s remains)
INFO - root - 2017-12-07 21:52:34.237112: step 58180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 53h:00m:06s remains)
INFO - root - 2017-12-07 21:52:40.983578: step 58190, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 52h:22m:00s remains)
INFO - root - 2017-12-07 21:52:47.720351: step 58200, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.725 sec/batch; 55h:16m:09s remains)
2017-12-07 21:52:48.421871: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3332496 -4.3318548 -4.3268857 -4.3181067 -4.3057852 -4.2940221 -4.2879925 -4.2879019 -4.2871594 -4.2894173 -4.2962861 -4.3087268 -4.3184743 -4.3193712 -4.3094683][-4.3363819 -4.3334656 -4.3246703 -4.3101344 -4.2933702 -4.2795272 -4.2727594 -4.2697716 -4.2655149 -4.2660732 -4.2771287 -4.2966714 -4.3118157 -4.3144326 -4.302578][-4.3357544 -4.332212 -4.3212333 -4.3014092 -4.2758694 -4.257997 -4.2502937 -4.2437768 -4.2371459 -4.2339911 -4.2476339 -4.2747679 -4.2965894 -4.3005939 -4.2857819][-4.3310428 -4.3262773 -4.3140507 -4.2929215 -4.26177 -4.2393608 -4.2300515 -4.2239628 -4.2158742 -4.2061357 -4.2150826 -4.2458463 -4.2738223 -4.283072 -4.2662387][-4.3176103 -4.3090148 -4.2915349 -4.266243 -4.2364879 -4.2144451 -4.2055688 -4.2031922 -4.197547 -4.1841221 -4.1856694 -4.2144256 -4.2454677 -4.2589822 -4.2426805][-4.2978363 -4.2827778 -4.25682 -4.22532 -4.1979446 -4.1791725 -4.1681118 -4.1657629 -4.1650987 -4.1579227 -4.1595397 -4.18137 -4.2110057 -4.2284102 -4.2201362][-4.2915158 -4.2672496 -4.2279086 -4.1865864 -4.1521044 -4.1280007 -4.1126256 -4.106617 -4.1101685 -4.1112127 -4.1216145 -4.1436467 -4.17196 -4.1965475 -4.2007651][-4.2868576 -4.2606096 -4.2137437 -4.1640878 -4.1248083 -4.099165 -4.0878606 -4.0843649 -4.0897284 -4.0935364 -4.1068926 -4.1284695 -4.1570973 -4.1859903 -4.2003241][-4.2768288 -4.2598925 -4.2219315 -4.1706185 -4.1243725 -4.098434 -4.0994592 -4.1068606 -4.1109514 -4.1112475 -4.1195283 -4.1343303 -4.1601744 -4.1918888 -4.2173476][-4.2663407 -4.2564163 -4.2318583 -4.1893439 -4.145246 -4.1157484 -4.11471 -4.1236782 -4.1255479 -4.1246305 -4.1290064 -4.1399894 -4.1603441 -4.1895967 -4.2220993][-4.2455716 -4.2421126 -4.2313442 -4.2078528 -4.1791487 -4.1533542 -4.1447105 -4.1477046 -4.1509423 -4.1511707 -4.1511273 -4.1580839 -4.1710238 -4.1941528 -4.2268939][-4.2363787 -4.2405925 -4.24046 -4.2320089 -4.2201886 -4.2048931 -4.1938753 -4.1940217 -4.1940374 -4.1916614 -4.1848097 -4.1847539 -4.1941714 -4.2117753 -4.2414756][-4.2440786 -4.2534108 -4.2612753 -4.2629519 -4.2627926 -4.2599459 -4.2547693 -4.2527957 -4.2504897 -4.2447743 -4.2317877 -4.2218533 -4.223752 -4.2361441 -4.2598634][-4.2553711 -4.2661409 -4.27854 -4.289741 -4.295001 -4.2979703 -4.2981863 -4.2967496 -4.29578 -4.2930808 -4.2804642 -4.2651043 -4.2572703 -4.2613449 -4.2785206][-4.2660775 -4.2755556 -4.2883072 -4.3009734 -4.3079615 -4.3111424 -4.313252 -4.314929 -4.3184662 -4.3209143 -4.3138404 -4.2998471 -4.2891641 -4.2856655 -4.2948661]]...]
INFO - root - 2017-12-07 21:52:55.013386: step 58210, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 50h:52m:17s remains)
INFO - root - 2017-12-07 21:53:01.861430: step 58220, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 52h:32m:12s remains)
INFO - root - 2017-12-07 21:53:08.663905: step 58230, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 53h:37m:47s remains)
INFO - root - 2017-12-07 21:53:15.328350: step 58240, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 48h:57m:17s remains)
INFO - root - 2017-12-07 21:53:22.086155: step 58250, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 53h:12m:03s remains)
INFO - root - 2017-12-07 21:53:28.922092: step 58260, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.759 sec/batch; 57h:48m:49s remains)
INFO - root - 2017-12-07 21:53:35.597326: step 58270, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 53h:46m:06s remains)
INFO - root - 2017-12-07 21:53:42.495017: step 58280, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.730 sec/batch; 55h:36m:48s remains)
INFO - root - 2017-12-07 21:53:49.315645: step 58290, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 50h:10m:30s remains)
INFO - root - 2017-12-07 21:53:56.171386: step 58300, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 53h:26m:07s remains)
2017-12-07 21:53:56.871269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2257786 -4.2180982 -4.2124085 -4.2035456 -4.1909533 -4.1953268 -4.2265067 -4.2479253 -4.2465496 -4.2374039 -4.2247128 -4.2151413 -4.2205844 -4.2351961 -4.2421632][-4.2194467 -4.2119942 -4.2077556 -4.1985669 -4.1850719 -4.1893749 -4.2213993 -4.2445407 -4.2424822 -4.2343507 -4.2241421 -4.2171459 -4.2249451 -4.2382517 -4.2391095][-4.2100215 -4.2052293 -4.2015204 -4.1885996 -4.1734705 -4.174 -4.1995573 -4.22106 -4.220397 -4.214417 -4.2116795 -4.2101154 -4.2198792 -4.2321787 -4.2312789][-4.2059131 -4.2006888 -4.1920438 -4.1738238 -4.1561151 -4.1520114 -4.169404 -4.1869078 -4.1887026 -4.1850224 -4.1896338 -4.1931782 -4.2042952 -4.2172208 -4.2190428][-4.2085762 -4.2049594 -4.1879392 -4.1573529 -4.1311045 -4.1177115 -4.1229835 -4.1325293 -4.1376319 -4.1393871 -4.1535735 -4.1681719 -4.1857944 -4.2002668 -4.2048364][-4.21233 -4.2057118 -4.1756263 -4.1277385 -4.0845137 -4.0547709 -4.0433679 -4.0431814 -4.0548325 -4.0716858 -4.1033034 -4.134244 -4.1617479 -4.1816525 -4.1880722][-4.2057738 -4.1907663 -4.1479435 -4.0836029 -4.0232162 -3.9786777 -3.9550397 -3.9484758 -3.967777 -4.0062566 -4.0582542 -4.1034904 -4.1378746 -4.1630383 -4.1719542][-4.1849747 -4.1556854 -4.1048388 -4.0337148 -3.9651155 -3.9204097 -3.901314 -3.9022963 -3.9355648 -3.9879179 -4.0410104 -4.0848942 -4.1213112 -4.1500072 -4.1622429][-4.1760106 -4.1358452 -4.0845237 -4.0202284 -3.9607654 -3.932852 -3.9332187 -3.9486792 -3.9791265 -4.0225334 -4.0607729 -4.0941091 -4.1274843 -4.1571708 -4.1713209][-4.1913738 -4.1554055 -4.1172614 -4.0683975 -4.0238271 -4.0099654 -4.0195317 -4.0344386 -4.0492678 -4.0756907 -4.1002307 -4.1271982 -4.1586585 -4.1876183 -4.2011628][-4.2254448 -4.2053494 -4.1841664 -4.1548619 -4.1264777 -4.1178508 -4.1217833 -4.1232634 -4.1223469 -4.1354189 -4.15137 -4.1766763 -4.2065005 -4.2322164 -4.2430491][-4.2556696 -4.2532573 -4.246798 -4.2317328 -4.2155137 -4.2067795 -4.2006912 -4.1909609 -4.1832147 -4.190959 -4.2034154 -4.224061 -4.2460575 -4.2639613 -4.270793][-4.2736778 -4.2780843 -4.2793221 -4.2741747 -4.2671371 -4.2617993 -4.252687 -4.2408233 -4.2323627 -4.2369323 -4.2456646 -4.2598624 -4.2734013 -4.2834945 -4.2877569][-4.2903447 -4.2932763 -4.2944212 -4.2948475 -4.2956204 -4.2967935 -4.2925916 -4.2857242 -4.2802682 -4.2813458 -4.2845435 -4.2903066 -4.2965446 -4.3009038 -4.30363][-4.3092508 -4.3114209 -4.3126507 -4.3138437 -4.3153863 -4.3183622 -4.3191576 -4.3163347 -4.3129153 -4.3125553 -4.3128614 -4.3137136 -4.3150864 -4.31612 -4.3171625]]...]
INFO - root - 2017-12-07 21:54:03.469457: step 58310, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 53h:04m:51s remains)
INFO - root - 2017-12-07 21:54:10.321417: step 58320, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 49h:56m:45s remains)
INFO - root - 2017-12-07 21:54:17.093881: step 58330, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:54m:36s remains)
INFO - root - 2017-12-07 21:54:23.909479: step 58340, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 56h:01m:18s remains)
INFO - root - 2017-12-07 21:54:30.755197: step 58350, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 53h:09m:25s remains)
INFO - root - 2017-12-07 21:54:37.491212: step 58360, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 50h:12m:13s remains)
INFO - root - 2017-12-07 21:54:44.431764: step 58370, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 52h:11m:44s remains)
INFO - root - 2017-12-07 21:54:51.366973: step 58380, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 54h:51m:29s remains)
INFO - root - 2017-12-07 21:54:58.173116: step 58390, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:18m:45s remains)
INFO - root - 2017-12-07 21:55:05.077426: step 58400, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 53h:48m:44s remains)
2017-12-07 21:55:05.772192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.305541 -4.3104844 -4.3154526 -4.3167644 -4.3134046 -4.3065896 -4.30034 -4.2962489 -4.2918243 -4.28516 -4.2782178 -4.2702942 -4.2621326 -4.2551618 -4.2493958][-4.3085728 -4.3124995 -4.3159904 -4.31318 -4.3037004 -4.2910981 -4.2804956 -4.2736053 -4.2661352 -4.25701 -4.2490959 -4.2411528 -4.2330666 -4.2260609 -4.2213635][-4.3100982 -4.31361 -4.3138561 -4.304493 -4.2865529 -4.2666836 -4.2505441 -4.2391067 -4.2290406 -4.2197938 -4.2122831 -4.2048655 -4.1988597 -4.1946759 -4.19392][-4.3103166 -4.310564 -4.3022718 -4.28065 -4.2510395 -4.2236323 -4.2029862 -4.1888962 -4.1790218 -4.173202 -4.1699171 -4.1663351 -4.1659222 -4.1699576 -4.1788321][-4.3055811 -4.297081 -4.2742848 -4.2373428 -4.197998 -4.1680455 -4.14866 -4.1387477 -4.137105 -4.1385875 -4.1397548 -4.1437435 -4.1524673 -4.1675034 -4.1865273][-4.2954154 -4.2747269 -4.2372355 -4.1896291 -4.1490865 -4.1253242 -4.1155448 -4.1160374 -4.1225967 -4.1276221 -4.1321945 -4.1444459 -4.1642566 -4.1900306 -4.2154789][-4.284308 -4.2526321 -4.2064548 -4.1579347 -4.12369 -4.1092529 -4.1072545 -4.1129913 -4.1173863 -4.1156597 -4.1202726 -4.1413116 -4.1732411 -4.2101455 -4.2418275][-4.2604022 -4.2224422 -4.1758208 -4.1367235 -4.1165757 -4.1134582 -4.1168008 -4.11892 -4.1130247 -4.10353 -4.1101842 -4.1400948 -4.183351 -4.2266822 -4.257863][-4.2275939 -4.1883087 -4.1487193 -4.1262636 -4.1255569 -4.1353335 -4.1444163 -4.1417723 -4.1278024 -4.11476 -4.1215506 -4.1564379 -4.2037692 -4.2448883 -4.2695889][-4.1980529 -4.1624446 -4.136097 -4.1306667 -4.1424284 -4.160748 -4.1751947 -4.1724777 -4.160634 -4.1515679 -4.16097 -4.1961923 -4.2401781 -4.2740397 -4.2871919][-4.1846185 -4.1561584 -4.142705 -4.1486468 -4.1653547 -4.1881866 -4.2058473 -4.204947 -4.1992645 -4.1981969 -4.2132916 -4.2472415 -4.2848315 -4.3065186 -4.305059][-4.189528 -4.1696558 -4.1659203 -4.174984 -4.1906352 -4.2130842 -4.228333 -4.2289205 -4.2296429 -4.2346163 -4.2521257 -4.2827625 -4.3103223 -4.3176522 -4.3036776][-4.2179174 -4.2059741 -4.2057848 -4.2125306 -4.225543 -4.243526 -4.2543488 -4.2549734 -4.2547612 -4.2582278 -4.2755356 -4.3004489 -4.3149977 -4.3085861 -4.287756][-4.2582278 -4.2492027 -4.2456431 -4.2479134 -4.257184 -4.2688942 -4.2737036 -4.270082 -4.2639394 -4.2659006 -4.2833424 -4.3010564 -4.3049469 -4.2917542 -4.2716904][-4.2896137 -4.2780266 -4.2674069 -4.2652407 -4.2698412 -4.27626 -4.275773 -4.2684808 -4.2619562 -4.2683935 -4.287394 -4.3004723 -4.299686 -4.2870226 -4.2705383]]...]
INFO - root - 2017-12-07 21:55:12.372935: step 58410, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 53h:19m:32s remains)
INFO - root - 2017-12-07 21:55:19.234035: step 58420, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 54h:55m:57s remains)
INFO - root - 2017-12-07 21:55:26.111320: step 58430, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 48h:04m:29s remains)
INFO - root - 2017-12-07 21:55:32.926121: step 58440, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 51h:01m:35s remains)
INFO - root - 2017-12-07 21:55:39.746569: step 58450, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 54h:00m:15s remains)
INFO - root - 2017-12-07 21:55:46.599870: step 58460, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 55h:38m:54s remains)
INFO - root - 2017-12-07 21:55:53.322299: step 58470, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.693 sec/batch; 52h:43m:06s remains)
INFO - root - 2017-12-07 21:56:00.103133: step 58480, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.636 sec/batch; 48h:26m:12s remains)
INFO - root - 2017-12-07 21:56:06.875726: step 58490, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 51h:28m:02s remains)
INFO - root - 2017-12-07 21:56:13.649143: step 58500, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 54h:43m:17s remains)
2017-12-07 21:56:14.394302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.273591 -4.2679009 -4.2573714 -4.2507696 -4.25176 -4.2562141 -4.2536335 -4.23918 -4.2210474 -4.2250047 -4.2406464 -4.261632 -4.2774692 -4.2869115 -4.2892423][-4.2536616 -4.24066 -4.219398 -4.2033195 -4.1994305 -4.2046204 -4.2094007 -4.2097611 -4.2093129 -4.2259 -4.2495208 -4.2734871 -4.29026 -4.2972941 -4.297534][-4.2201715 -4.2014513 -4.1691203 -4.1416082 -4.130374 -4.1348066 -4.1450715 -4.1585116 -4.1757736 -4.2057786 -4.2400804 -4.2707386 -4.293376 -4.3022475 -4.3023572][-4.1909237 -4.1672516 -4.1251397 -4.0871792 -4.0680766 -4.0706091 -4.0826168 -4.1031761 -4.132905 -4.1728129 -4.2138085 -4.2494907 -4.2796845 -4.2954092 -4.3002481][-4.180789 -4.1539063 -4.1059217 -4.0593705 -4.032443 -4.0306072 -4.0415297 -4.0633049 -4.0986032 -4.1434364 -4.1879721 -4.2248893 -4.2595115 -4.2819071 -4.2930007][-4.1934986 -4.1650782 -4.1156483 -4.0619164 -4.0259991 -4.0178137 -4.023901 -4.0423074 -4.0758424 -4.1214828 -4.167357 -4.2061477 -4.2431722 -4.2697406 -4.2858796][-4.2223439 -4.1963496 -4.1501489 -4.0934525 -4.0457964 -4.0251722 -4.02155 -4.0316529 -4.058146 -4.102531 -4.1497817 -4.19121 -4.2318883 -4.2620568 -4.2812567][-4.2520881 -4.234179 -4.1986222 -4.1488562 -4.0960617 -4.0589857 -4.0355072 -4.0298538 -4.046124 -4.0910907 -4.1401916 -4.1831722 -4.2263827 -4.2597933 -4.2802744][-4.271481 -4.2632256 -4.2418823 -4.2079897 -4.1635365 -4.118834 -4.0770707 -4.0498457 -4.0508442 -4.0930281 -4.1428609 -4.1850786 -4.22731 -4.2615347 -4.2816591][-4.2633438 -4.2652 -4.2598472 -4.245575 -4.2195845 -4.1832671 -4.1397772 -4.1017866 -4.0899067 -4.1245885 -4.1693792 -4.2066631 -4.2423167 -4.26952 -4.2844548][-4.2256093 -4.2347145 -4.2445159 -4.249918 -4.24544 -4.2269063 -4.1967525 -4.1641746 -4.147747 -4.172349 -4.2085238 -4.2383995 -4.2653751 -4.2822866 -4.28841][-4.1781077 -4.1871252 -4.205565 -4.2255888 -4.2388568 -4.238606 -4.2281194 -4.2100663 -4.1983886 -4.2161875 -4.2423558 -4.2634635 -4.2822123 -4.2901859 -4.2884603][-4.1470623 -4.1512504 -4.1698437 -4.1939864 -4.2154613 -4.2283983 -4.2371974 -4.2380123 -4.237195 -4.2500467 -4.2653222 -4.2758422 -4.2877846 -4.2900982 -4.2856092][-4.1533542 -4.1524868 -4.1660337 -4.1851535 -4.2039008 -4.2199526 -4.2395782 -4.2557497 -4.2676907 -4.2795658 -4.2861748 -4.2862105 -4.2891603 -4.2861543 -4.2813468][-4.1866603 -4.1843262 -4.1922541 -4.20394 -4.2139874 -4.2240925 -4.24292 -4.2661862 -4.2866945 -4.2994418 -4.3015103 -4.2960124 -4.2914395 -4.2827921 -4.2771831]]...]
INFO - root - 2017-12-07 21:56:20.894385: step 58510, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 49h:09m:09s remains)
INFO - root - 2017-12-07 21:56:27.802356: step 58520, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 52h:03m:20s remains)
INFO - root - 2017-12-07 21:56:34.685344: step 58530, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 53h:44m:01s remains)
INFO - root - 2017-12-07 21:56:41.511721: step 58540, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 50h:57m:35s remains)
INFO - root - 2017-12-07 21:56:48.090062: step 58550, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 48h:41m:32s remains)
INFO - root - 2017-12-07 21:56:54.889426: step 58560, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 50h:44m:47s remains)
INFO - root - 2017-12-07 21:57:01.760231: step 58570, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 54h:18m:10s remains)
INFO - root - 2017-12-07 21:57:08.573945: step 58580, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 53h:48m:35s remains)
INFO - root - 2017-12-07 21:57:15.464370: step 58590, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 49h:41m:20s remains)
INFO - root - 2017-12-07 21:57:22.199045: step 58600, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 50h:16m:04s remains)
2017-12-07 21:57:23.063046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2215958 -4.2346563 -4.2410054 -4.2292652 -4.2260656 -4.2287035 -4.2135406 -4.1948419 -4.1915674 -4.2059569 -4.2073941 -4.1930976 -4.1888123 -4.2068324 -4.2388372][-4.2123556 -4.2309122 -4.2381196 -4.2235427 -4.2219143 -4.2275429 -4.2105937 -4.1845026 -4.1800942 -4.2007008 -4.2044396 -4.1904125 -4.1812081 -4.19583 -4.2275925][-4.2050257 -4.2254019 -4.2316475 -4.2146683 -4.2138033 -4.2147818 -4.1897078 -4.1547585 -4.1514659 -4.1825714 -4.1887388 -4.1758 -4.1674809 -4.1847157 -4.2178845][-4.2029433 -4.219605 -4.2224393 -4.2019258 -4.1961012 -4.1784887 -4.1302347 -4.0785804 -4.0881605 -4.1407838 -4.1562319 -4.1494126 -4.1513138 -4.173728 -4.2083168][-4.2112942 -4.2194276 -4.215322 -4.1853271 -4.1628723 -4.1180973 -4.0363512 -3.9592164 -3.9920845 -4.0780206 -4.1137228 -4.1251106 -4.1442528 -4.1729436 -4.2082043][-4.2258968 -4.2214174 -4.2061639 -4.1682506 -4.1292381 -4.05497 -3.9353015 -3.8329871 -3.9004488 -4.0261536 -4.0857596 -4.1169882 -4.1498218 -4.1858535 -4.2244706][-4.246676 -4.2336473 -4.2072434 -4.1630411 -4.1075191 -4.0153036 -3.8754573 -3.7755 -3.8753273 -4.0162115 -4.0833139 -4.1245179 -4.1628752 -4.2001219 -4.2407269][-4.2488184 -4.2368174 -4.2097769 -4.1644316 -4.1046438 -4.0182667 -3.8939531 -3.8279195 -3.9322577 -4.0476012 -4.1018715 -4.1374683 -4.171844 -4.2047243 -4.2436972][-4.2256179 -4.2158527 -4.1966939 -4.1612639 -4.1137991 -4.0477781 -3.9573331 -3.9180124 -3.996799 -4.077939 -4.1149077 -4.1407881 -4.1703415 -4.1998048 -4.2370963][-4.2217956 -4.2097659 -4.1925831 -4.1614919 -4.1233749 -4.0760069 -4.0110173 -3.9866328 -4.0427532 -4.1017675 -4.1291637 -4.1500654 -4.1772814 -4.2055144 -4.2381568][-4.2422576 -4.2277327 -4.2079277 -4.1743617 -4.1354337 -4.0942731 -4.04346 -4.0270042 -4.0696359 -4.1145797 -4.141964 -4.167665 -4.1962409 -4.2240362 -4.2510862][-4.2628489 -4.2459745 -4.2218819 -4.1823921 -4.1355424 -4.0949111 -4.0560889 -4.0478992 -4.0868883 -4.1314526 -4.1654706 -4.195642 -4.2242661 -4.2480121 -4.2678862][-4.2844896 -4.2674575 -4.2427197 -4.201685 -4.1526031 -4.1179862 -4.0996542 -4.1045341 -4.138629 -4.1796675 -4.2123437 -4.2404785 -4.2638631 -4.2798061 -4.2915759][-4.30313 -4.2910705 -4.2700453 -4.2378144 -4.2002606 -4.1738472 -4.1692924 -4.1837621 -4.2127767 -4.2426214 -4.2639537 -4.2820044 -4.2984552 -4.3086829 -4.3140116][-4.3156347 -4.3117609 -4.2970271 -4.2758231 -4.2506661 -4.2342114 -4.2353582 -4.2489133 -4.2677484 -4.2874 -4.3002439 -4.31029 -4.3206773 -4.3265014 -4.3262024]]...]
INFO - root - 2017-12-07 21:57:29.576676: step 58610, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.699 sec/batch; 53h:09m:13s remains)
INFO - root - 2017-12-07 21:57:36.350373: step 58620, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 49h:31m:06s remains)
INFO - root - 2017-12-07 21:57:43.082215: step 58630, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 48h:00m:25s remains)
INFO - root - 2017-12-07 21:57:49.864144: step 58640, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.689 sec/batch; 52h:25m:05s remains)
INFO - root - 2017-12-07 21:57:56.735105: step 58650, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.738 sec/batch; 56h:06m:52s remains)
INFO - root - 2017-12-07 21:58:03.445682: step 58660, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 53h:23m:22s remains)
INFO - root - 2017-12-07 21:58:10.238862: step 58670, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 48h:47m:03s remains)
INFO - root - 2017-12-07 21:58:17.040301: step 58680, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.641 sec/batch; 48h:45m:08s remains)
INFO - root - 2017-12-07 21:58:23.840156: step 58690, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 53h:15m:06s remains)
INFO - root - 2017-12-07 21:58:30.641879: step 58700, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 52h:05m:08s remains)
2017-12-07 21:58:31.378849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3033481 -4.30406 -4.3070273 -4.307497 -4.301383 -4.2887821 -4.2749658 -4.262301 -4.2568541 -4.2647114 -4.2798181 -4.2953963 -4.3134942 -4.3297744 -4.3379226][-4.2762318 -4.2746792 -4.2755613 -4.2729521 -4.26336 -4.2473826 -4.2339697 -4.2214341 -4.2188425 -4.2352638 -4.2616463 -4.2841325 -4.3017383 -4.3190184 -4.3314605][-4.2465072 -4.2464209 -4.2475481 -4.2397504 -4.2240419 -4.2004824 -4.1852341 -4.1745009 -4.1775579 -4.2015548 -4.2359152 -4.2656832 -4.2848644 -4.3030066 -4.3198948][-4.2144961 -4.22282 -4.2311544 -4.2200904 -4.1946568 -4.1567512 -4.1303263 -4.1207695 -4.1319966 -4.16193 -4.1989355 -4.2355814 -4.2628841 -4.2855 -4.3066697][-4.1798558 -4.1998758 -4.2169662 -4.206079 -4.17175 -4.1135907 -4.0632911 -4.0541081 -4.0847816 -4.1231232 -4.1602221 -4.2000585 -4.2373757 -4.268014 -4.2949352][-4.1490951 -4.1752648 -4.1920595 -4.1758056 -4.1318159 -4.0529995 -3.970386 -3.9639108 -4.0337911 -4.0946426 -4.1321654 -4.1701927 -4.2155957 -4.2548871 -4.28656][-4.1338406 -4.1559296 -4.1572247 -4.1276894 -4.0690742 -3.9635661 -3.8368297 -3.8256662 -3.945857 -4.0428653 -4.0890131 -4.1308589 -4.1862254 -4.2383208 -4.2764659][-4.1481051 -4.152894 -4.1312237 -4.0895419 -4.0207472 -3.9008598 -3.7461514 -3.729748 -3.8836906 -4.0066886 -4.066463 -4.1145968 -4.1728554 -4.2278247 -4.2668509][-4.181447 -4.16856 -4.1314607 -4.0910883 -4.0382252 -3.9445555 -3.8283858 -3.8145871 -3.9365125 -4.0453057 -4.1025772 -4.1461306 -4.1974735 -4.2420425 -4.2728124][-4.2122722 -4.18967 -4.148962 -4.1206403 -4.0896235 -4.0380754 -3.9770784 -3.971981 -4.0448852 -4.117919 -4.1635127 -4.1996017 -4.241178 -4.2729406 -4.294127][-4.2240081 -4.1958656 -4.1656375 -4.158555 -4.1503944 -4.1290026 -4.1029091 -4.109129 -4.1511059 -4.1934066 -4.22474 -4.2530684 -4.2845349 -4.3053737 -4.3182955][-4.2204442 -4.1946688 -4.1854892 -4.2026515 -4.2157321 -4.2131853 -4.2036619 -4.2130914 -4.2373786 -4.2578425 -4.2734122 -4.2908664 -4.3109941 -4.325706 -4.3341274][-4.2079306 -4.1971531 -4.208456 -4.24165 -4.2681055 -4.2784867 -4.2775264 -4.2817097 -4.2935996 -4.3017888 -4.3077579 -4.3175859 -4.3290491 -4.3395691 -4.3450108][-4.1924648 -4.2036014 -4.2323666 -4.2705617 -4.2990055 -4.3133554 -4.3153138 -4.313602 -4.3142462 -4.3155193 -4.319901 -4.328866 -4.3391228 -4.3482409 -4.3525448][-4.1773062 -4.2061386 -4.2442217 -4.2781749 -4.3010249 -4.3137226 -4.3162808 -4.3121586 -4.3096061 -4.3102646 -4.31652 -4.3274431 -4.3390708 -4.3487225 -4.3539481]]...]
INFO - root - 2017-12-07 21:58:38.003459: step 58710, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 49h:28m:24s remains)
INFO - root - 2017-12-07 21:58:44.814086: step 58720, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 51h:22m:38s remains)
INFO - root - 2017-12-07 21:58:51.593482: step 58730, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.699 sec/batch; 53h:07m:47s remains)
INFO - root - 2017-12-07 21:58:58.449085: step 58740, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 49h:12m:43s remains)
INFO - root - 2017-12-07 21:59:05.187396: step 58750, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 48h:47m:02s remains)
INFO - root - 2017-12-07 21:59:11.904957: step 58760, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.668 sec/batch; 50h:46m:09s remains)
INFO - root - 2017-12-07 21:59:18.731443: step 58770, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 53h:35m:45s remains)
INFO - root - 2017-12-07 21:59:25.461768: step 58780, loss = 2.02, batch loss = 1.96 (11.4 examples/sec; 0.701 sec/batch; 53h:18m:01s remains)
INFO - root - 2017-12-07 21:59:32.276483: step 58790, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 52h:20m:26s remains)
INFO - root - 2017-12-07 21:59:38.949461: step 58800, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 49h:00m:42s remains)
2017-12-07 21:59:39.654825: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2485538 -4.2496638 -4.2542791 -4.2571507 -4.2579851 -4.2577734 -4.2615461 -4.2622991 -4.2491083 -4.2279239 -4.2092733 -4.2018924 -4.1959019 -4.1965303 -4.2013741][-4.2375445 -4.2380643 -4.2394896 -4.2374907 -4.234417 -4.229907 -4.2309322 -4.2329006 -4.2229919 -4.2055793 -4.1914916 -4.1925793 -4.191637 -4.1988091 -4.208982][-4.2316709 -4.2323709 -4.2318358 -4.2236729 -4.2145095 -4.2059069 -4.203546 -4.2086415 -4.2037668 -4.1908212 -4.17818 -4.1820936 -4.1886945 -4.2038856 -4.2209597][-4.1953726 -4.1902447 -4.1853642 -4.1735916 -4.1600804 -4.1427937 -4.1310091 -4.1380649 -4.1468949 -4.1497021 -4.1456957 -4.1499047 -4.1635618 -4.1888185 -4.2164879][-4.1548328 -4.1407609 -4.1295347 -4.1132188 -4.0898771 -4.0509 -4.013916 -4.0234838 -4.0633492 -4.0947642 -4.105732 -4.11115 -4.1276183 -4.1619716 -4.199594][-4.1104903 -4.0976744 -4.0833254 -4.0645909 -4.0237694 -3.9471691 -3.8714697 -3.8873875 -3.9773393 -4.0540447 -4.0839896 -4.0895734 -4.1042948 -4.14218 -4.184073][-4.0502405 -4.0442142 -4.0280609 -4.0055938 -3.9453232 -3.8293035 -3.7098422 -3.7357545 -3.879035 -3.9949882 -4.0424886 -4.055491 -4.0760446 -4.1185346 -4.1670046][-4.0477028 -4.0460253 -4.0340075 -4.0139756 -3.9553981 -3.8434381 -3.7233858 -3.7431278 -3.8795929 -3.9888027 -4.0364375 -4.0516434 -4.07585 -4.1178613 -4.1656966][-4.0853524 -4.0890965 -4.0839105 -4.0756721 -4.0423231 -3.9743574 -3.8948188 -3.9003267 -3.9909015 -4.0685663 -4.1077943 -4.1193991 -4.1380491 -4.1670394 -4.1990061][-4.1272941 -4.1322432 -4.1322017 -4.1332812 -4.1236515 -4.0946422 -4.0449471 -4.03607 -4.0843883 -4.1359591 -4.1712289 -4.1828303 -4.190753 -4.2041254 -4.2192035][-4.171031 -4.1756821 -4.180933 -4.1906786 -4.195189 -4.1912565 -4.1612482 -4.143187 -4.1578674 -4.1856122 -4.21384 -4.2241888 -4.2221613 -4.2200074 -4.2223468][-4.2037864 -4.2085052 -4.2203312 -4.2348976 -4.244154 -4.248889 -4.2282343 -4.2052994 -4.1995158 -4.2063069 -4.2242074 -4.2348957 -4.2298646 -4.2247305 -4.2261267][-4.2309146 -4.2355881 -4.251173 -4.2677827 -4.2777481 -4.2827907 -4.2675667 -4.2446566 -4.2268496 -4.2188554 -4.2265067 -4.2378621 -4.2369089 -4.2388434 -4.2463694][-4.2630043 -4.2603436 -4.26998 -4.2831912 -4.2913189 -4.2969666 -4.2901812 -4.2731791 -4.2524433 -4.2366161 -4.2350216 -4.2459006 -4.2528629 -4.2654972 -4.2786074][-4.2980328 -4.2877979 -4.2861977 -4.2916574 -4.2966123 -4.3004818 -4.2990484 -4.2877851 -4.2701616 -4.2544913 -4.248446 -4.258347 -4.2712064 -4.2902327 -4.3049]]...]
INFO - root - 2017-12-07 21:59:46.336082: step 58810, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 55h:32m:57s remains)
INFO - root - 2017-12-07 21:59:53.297764: step 58820, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 50h:58m:32s remains)
INFO - root - 2017-12-07 22:00:00.098997: step 58830, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 52h:11m:11s remains)
INFO - root - 2017-12-07 22:00:06.979682: step 58840, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 54h:30m:17s remains)
INFO - root - 2017-12-07 22:00:13.838536: step 58850, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 53h:21m:15s remains)
INFO - root - 2017-12-07 22:00:20.399745: step 58860, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 47h:48m:42s remains)
INFO - root - 2017-12-07 22:00:27.150100: step 58870, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 49h:01m:17s remains)
INFO - root - 2017-12-07 22:00:33.924250: step 58880, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 53h:47m:52s remains)
INFO - root - 2017-12-07 22:00:40.852723: step 58890, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 52h:25m:00s remains)
INFO - root - 2017-12-07 22:00:47.704099: step 58900, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 52h:26m:03s remains)
2017-12-07 22:00:48.390306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3341508 -4.3344207 -4.3302093 -4.3268332 -4.3255167 -4.3277493 -4.3313479 -4.3338718 -4.3351355 -4.3359122 -4.3372836 -4.3389153 -4.3398619 -4.3406324 -4.3419394][-4.3322906 -4.3277678 -4.3174567 -4.309484 -4.3062005 -4.308661 -4.3145947 -4.3212824 -4.3257694 -4.3289986 -4.3338008 -4.3384171 -4.3390903 -4.3380876 -4.3390861][-4.3197317 -4.305202 -4.2834606 -4.2668366 -4.2604709 -4.2635503 -4.2704668 -4.2812071 -4.2901292 -4.2991343 -4.3102622 -4.3193 -4.321414 -4.320889 -4.3250718][-4.2930326 -4.2667294 -4.2313385 -4.200448 -4.1859231 -4.1863656 -4.1950793 -4.2123184 -4.2301393 -4.2488842 -4.2707033 -4.2884068 -4.2930412 -4.2947927 -4.3032212][-4.2486639 -4.2061696 -4.1563635 -4.1140161 -4.0917659 -4.0865207 -4.0956759 -4.1236482 -4.1534591 -4.1803503 -4.2105532 -4.2372303 -4.2474284 -4.255188 -4.2718186][-4.1917515 -4.1330943 -4.0691538 -4.0168676 -3.9856396 -3.971761 -3.9780431 -4.0202127 -4.0676861 -4.1017852 -4.1392407 -4.1728334 -4.1902685 -4.206615 -4.234076][-4.1456585 -4.0752292 -4.0018454 -3.9423037 -3.9071755 -3.8888359 -3.8969026 -3.9606471 -4.0270686 -4.0606089 -4.0973325 -4.1345854 -4.1581206 -4.1792736 -4.2132058][-4.1210661 -4.0479069 -3.9754975 -3.9269333 -3.9036653 -3.8902664 -3.9026747 -3.983501 -4.0610085 -4.0895953 -4.1215219 -4.15788 -4.1821055 -4.2010775 -4.2310681][-4.1346064 -4.0748515 -4.0194292 -3.9907272 -3.9784799 -3.9700198 -3.9838016 -4.0615282 -4.1360474 -4.1602683 -4.1873245 -4.2177696 -4.2375145 -4.2496157 -4.2684388][-4.1846313 -4.1501846 -4.1210952 -4.1141191 -4.1100025 -4.102787 -4.1110964 -4.1691785 -4.2234316 -4.2377586 -4.2538214 -4.2731056 -4.2857566 -4.2911034 -4.2992396][-4.2449813 -4.2333722 -4.2261844 -4.2322774 -4.2317529 -4.2232537 -4.2259431 -4.26298 -4.2950778 -4.3011541 -4.3069859 -4.315125 -4.3205967 -4.3223338 -4.3252029][-4.2876077 -4.2879953 -4.2922773 -4.3009357 -4.3008857 -4.292542 -4.2912788 -4.3104949 -4.3252745 -4.327939 -4.3316207 -4.3372278 -4.3405175 -4.3405209 -4.3405414][-4.3071089 -4.31307 -4.3213067 -4.3296847 -4.3308892 -4.3242612 -4.3206353 -4.3279533 -4.3332105 -4.3331113 -4.3356228 -4.3404427 -4.3443861 -4.3455696 -4.3454189][-4.3212652 -4.3266883 -4.3331237 -4.339179 -4.3407578 -4.3372831 -4.3346438 -4.337183 -4.3383765 -4.3380404 -4.3397651 -4.3426762 -4.3450565 -4.3458061 -4.3459597][-4.33292 -4.33666 -4.3398061 -4.3425741 -4.3436351 -4.3427715 -4.342504 -4.3441062 -4.3448868 -4.3454652 -4.3461795 -4.3470335 -4.3480568 -4.3482194 -4.3480735]]...]
INFO - root - 2017-12-07 22:00:55.019473: step 58910, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 51h:34m:36s remains)
INFO - root - 2017-12-07 22:01:01.771961: step 58920, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 50h:49m:23s remains)
INFO - root - 2017-12-07 22:01:08.500212: step 58930, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 51h:14m:08s remains)
INFO - root - 2017-12-07 22:01:15.180733: step 58940, loss = 2.04, batch loss = 1.98 (12.8 examples/sec; 0.627 sec/batch; 47h:40m:26s remains)
INFO - root - 2017-12-07 22:01:21.949851: step 58950, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.642 sec/batch; 48h:47m:57s remains)
INFO - root - 2017-12-07 22:01:28.646362: step 58960, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 50h:04m:32s remains)
INFO - root - 2017-12-07 22:01:35.435990: step 58970, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 51h:53m:09s remains)
INFO - root - 2017-12-07 22:01:42.212824: step 58980, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 53h:22m:17s remains)
INFO - root - 2017-12-07 22:01:48.975382: step 58990, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 50h:59m:06s remains)
INFO - root - 2017-12-07 22:01:55.642633: step 59000, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 47h:41m:01s remains)
2017-12-07 22:01:56.408416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1507435 -4.181663 -4.1852741 -4.1642814 -4.146 -4.1338072 -4.1282458 -4.1244988 -4.14172 -4.1679807 -4.1864815 -4.1814675 -4.155302 -4.138104 -4.1411352][-4.1866417 -4.2031426 -4.2017312 -4.1836104 -4.1629705 -4.1514168 -4.1440978 -4.1320887 -4.140317 -4.158813 -4.1806512 -4.1911745 -4.1755137 -4.1592455 -4.1581011][-4.2171693 -4.22552 -4.2207422 -4.2043796 -4.1883 -4.1829534 -4.180954 -4.1698675 -4.1652255 -4.1601353 -4.1597838 -4.161725 -4.1541548 -4.1478114 -4.152277][-4.2301664 -4.2301679 -4.2216635 -4.2100978 -4.2035508 -4.2057467 -4.2113533 -4.2086339 -4.1954651 -4.1663938 -4.1385536 -4.1259065 -4.1278887 -4.1328311 -4.1371779][-4.2138104 -4.2021251 -4.1898456 -4.1787109 -4.1740665 -4.1764922 -4.1938939 -4.209641 -4.2009349 -4.1690197 -4.1328864 -4.1162796 -4.1235852 -4.1332946 -4.1336617][-4.1773667 -4.1649933 -4.1520486 -4.135416 -4.1105666 -4.0908079 -4.1194916 -4.1644177 -4.1809211 -4.1718354 -4.1485128 -4.1305313 -4.1273031 -4.1347094 -4.136518][-4.1485691 -4.1477828 -4.1320968 -4.0984063 -4.0289989 -3.9619451 -4.0008216 -4.0900164 -4.1419511 -4.161912 -4.1555748 -4.1338954 -4.1212525 -4.1338916 -4.1488171][-4.1504579 -4.1611896 -4.1436806 -4.0952621 -3.994375 -3.8934934 -3.9389822 -4.0432072 -4.1057663 -4.138032 -4.1399632 -4.1193213 -4.1081104 -4.1285157 -4.1518235][-4.1724234 -4.1928697 -4.1813602 -4.145618 -4.0653648 -3.9926407 -4.0124931 -4.0624886 -4.0883579 -4.1098342 -4.11986 -4.1125803 -4.1078768 -4.1191444 -4.1288466][-4.2003422 -4.2149978 -4.2052574 -4.188673 -4.1449647 -4.1036959 -4.1009679 -4.1009336 -4.0939121 -4.1049781 -4.1243186 -4.1315246 -4.1234293 -4.10821 -4.0918937][-4.2157931 -4.2080407 -4.1956315 -4.1997395 -4.1831288 -4.1590433 -4.1493192 -4.13512 -4.1206932 -4.1327753 -4.1596985 -4.1653581 -4.1410012 -4.1087546 -4.08434][-4.222847 -4.1939087 -4.1777282 -4.1934881 -4.1934333 -4.1825328 -4.1815238 -4.1694584 -4.1519456 -4.1577187 -4.17514 -4.1721449 -4.1453671 -4.1228151 -4.1142964][-4.2226419 -4.18621 -4.1669884 -4.181046 -4.1884222 -4.1903987 -4.1985087 -4.1864061 -4.1595669 -4.1504989 -4.157619 -4.1544404 -4.1426234 -4.1390491 -4.1382003][-4.2194996 -4.1854734 -4.1640453 -4.1695662 -4.180665 -4.1914096 -4.2010283 -4.1833014 -4.1502843 -4.1371555 -4.1433287 -4.1481204 -4.1463947 -4.1441522 -4.1363459][-4.2302508 -4.2031126 -4.1849127 -4.1870613 -4.1971622 -4.2046046 -4.2017612 -4.1778126 -4.1461382 -4.1382556 -4.1510763 -4.1616316 -4.1532378 -4.1335034 -4.1154742]]...]
INFO - root - 2017-12-07 22:02:02.997739: step 59010, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.705 sec/batch; 53h:33m:14s remains)
INFO - root - 2017-12-07 22:02:09.689361: step 59020, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 52h:28m:03s remains)
INFO - root - 2017-12-07 22:02:16.443959: step 59030, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 50h:56m:40s remains)
INFO - root - 2017-12-07 22:02:23.296845: step 59040, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 51h:58m:22s remains)
INFO - root - 2017-12-07 22:02:30.029877: step 59050, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 53h:13m:43s remains)
INFO - root - 2017-12-07 22:02:36.823479: step 59060, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 50h:59m:15s remains)
INFO - root - 2017-12-07 22:02:43.605692: step 59070, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 48h:57m:15s remains)
INFO - root - 2017-12-07 22:02:50.345945: step 59080, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 50h:59m:46s remains)
INFO - root - 2017-12-07 22:02:57.092193: step 59090, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 49h:31m:49s remains)
INFO - root - 2017-12-07 22:03:03.876376: step 59100, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 54h:22m:21s remains)
2017-12-07 22:03:04.646700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2695441 -4.277844 -4.2833552 -4.2754936 -4.2519732 -4.2199273 -4.1953239 -4.1852627 -4.1910439 -4.2042575 -4.2118182 -4.2043748 -4.2010379 -4.2150469 -4.2348447][-4.2952762 -4.3007016 -4.3024898 -4.2873659 -4.2563405 -4.2205377 -4.1937447 -4.1837559 -4.1917806 -4.2088737 -4.2221756 -4.2136555 -4.2025495 -4.208065 -4.2239404][-4.301137 -4.3028669 -4.3024282 -4.2861938 -4.2576051 -4.22595 -4.2007928 -4.1867733 -4.1894293 -4.2039638 -4.2197256 -4.2160096 -4.2011595 -4.2000408 -4.210341][-4.2899261 -4.2894154 -4.2851629 -4.2719874 -4.2486567 -4.221045 -4.1904135 -4.16518 -4.1591544 -4.1730256 -4.1920338 -4.1982584 -4.1962919 -4.2016616 -4.2112222][-4.2687197 -4.2647209 -4.2592769 -4.2482405 -4.226974 -4.1945519 -4.1531167 -4.1173306 -4.103765 -4.122014 -4.1475425 -4.1673608 -4.1833329 -4.1989546 -4.2097487][-4.2437057 -4.2368689 -4.2326007 -4.2217488 -4.1975813 -4.1529026 -4.0962 -4.0469127 -4.0308938 -4.0647116 -4.1004047 -4.1298833 -4.1582489 -4.179213 -4.1889081][-4.2201653 -4.2127509 -4.2080607 -4.1949005 -4.1660833 -4.1138487 -4.0498548 -3.9948277 -3.9782674 -4.02001 -4.0612679 -4.0942879 -4.1243615 -4.1410089 -4.147934][-4.1952767 -4.1826367 -4.1774487 -4.16475 -4.136694 -4.0921292 -4.0403752 -3.9941797 -3.9741337 -4.0049548 -4.0428753 -4.0708342 -4.0899692 -4.0944614 -4.0954909][-4.1834741 -4.1642132 -4.157712 -4.1491361 -4.13023 -4.0997367 -4.0649495 -4.0331421 -4.0149517 -4.0308928 -4.0564127 -4.0718217 -4.0709229 -4.0582113 -4.0486846][-4.1993728 -4.175498 -4.1673441 -4.1633849 -4.1542673 -4.1382875 -4.1184635 -4.0968127 -4.08121 -4.08543 -4.0962996 -4.0943937 -4.0736928 -4.0487356 -4.0393066][-4.2415047 -4.2166338 -4.2066784 -4.2056632 -4.2050433 -4.199646 -4.1912055 -4.1781383 -4.1654406 -4.162251 -4.1624928 -4.1473918 -4.1186914 -4.0959167 -4.0958381][-4.2920041 -4.2728114 -4.263885 -4.2639832 -4.2638683 -4.26139 -4.2557788 -4.2453117 -4.2361307 -4.2325773 -4.23057 -4.2166224 -4.1943712 -4.1836314 -4.1913619][-4.3330169 -4.3202634 -4.3112106 -4.3082876 -4.304791 -4.3014665 -4.2982197 -4.2916775 -4.2864146 -4.2859192 -4.2864022 -4.2789092 -4.2668304 -4.26367 -4.2714753][-4.3552847 -4.347096 -4.3395014 -4.3329062 -4.3267808 -4.3232307 -4.3241644 -4.3243003 -4.3235331 -4.3240814 -4.3241267 -4.3201809 -4.3135662 -4.3125443 -4.3175211][-4.3624749 -4.3564334 -4.3500137 -4.3424625 -4.3364654 -4.3346486 -4.3379045 -4.3420038 -4.34406 -4.3450422 -4.3440905 -4.3405056 -4.3358655 -4.3353276 -4.3387785]]...]
INFO - root - 2017-12-07 22:03:11.244960: step 59110, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.615 sec/batch; 46h:43m:55s remains)
INFO - root - 2017-12-07 22:03:17.985499: step 59120, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 48h:43m:02s remains)
INFO - root - 2017-12-07 22:03:24.839688: step 59130, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.740 sec/batch; 56h:10m:44s remains)
INFO - root - 2017-12-07 22:03:31.576555: step 59140, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 53h:30m:48s remains)
INFO - root - 2017-12-07 22:03:38.313193: step 59150, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 52h:23m:01s remains)
INFO - root - 2017-12-07 22:03:45.096971: step 59160, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.642 sec/batch; 48h:44m:23s remains)
INFO - root - 2017-12-07 22:03:51.597296: step 59170, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 52h:24m:15s remains)
INFO - root - 2017-12-07 22:03:58.312721: step 59180, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 52h:24m:55s remains)
INFO - root - 2017-12-07 22:04:05.075714: step 59190, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 52h:20m:56s remains)
INFO - root - 2017-12-07 22:04:11.807316: step 59200, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 52h:24m:37s remains)
2017-12-07 22:04:12.592596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3040051 -4.2882833 -4.2724595 -4.2555666 -4.2410221 -4.2434907 -4.263227 -4.2844663 -4.2979689 -4.3058448 -4.3033228 -4.2878528 -4.2717524 -4.2562766 -4.2435017][-4.2967796 -4.2778668 -4.2616205 -4.243216 -4.226614 -4.2293887 -4.2506886 -4.2719545 -4.2859588 -4.2929735 -4.2867837 -4.2676473 -4.2474737 -4.2259364 -4.2065907][-4.2902122 -4.277122 -4.2682891 -4.2518306 -4.2332611 -4.2297831 -4.2427111 -4.2571349 -4.2724786 -4.2830076 -4.2763052 -4.2568674 -4.2355628 -4.2067952 -4.1780453][-4.2846093 -4.2860017 -4.2873349 -4.2718844 -4.2459307 -4.228981 -4.22574 -4.2304168 -4.2506313 -4.2705474 -4.2688088 -4.2544193 -4.2359796 -4.20379 -4.1692014][-4.2803125 -4.2965617 -4.3040013 -4.284564 -4.2482829 -4.2143054 -4.1920938 -4.1881266 -4.21645 -4.2497725 -4.2582703 -4.2542658 -4.2423224 -4.2130513 -4.179152][-4.2696033 -4.2945118 -4.3021183 -4.2760353 -4.2246537 -4.1690531 -4.1268115 -4.1210852 -4.1677861 -4.2202277 -4.243021 -4.2529478 -4.2499957 -4.2267742 -4.198297][-4.2484503 -4.2732458 -4.2761068 -4.2418365 -4.1770172 -4.1008735 -4.0395141 -4.039031 -4.1121144 -4.1879468 -4.2289348 -4.2530131 -4.2603755 -4.2434611 -4.2202544][-4.2297044 -4.2490926 -4.2443461 -4.2001143 -4.1236806 -4.0347562 -3.9669547 -3.9814095 -4.0810409 -4.1761637 -4.2314339 -4.263586 -4.275311 -4.2616887 -4.2397728][-4.2374134 -4.2488766 -4.2357092 -4.1874313 -4.1140471 -4.0398445 -3.9940348 -4.0240159 -4.1171141 -4.2008963 -4.2489929 -4.2761273 -4.28511 -4.27197 -4.2509475][-4.2593169 -4.2652349 -4.2507491 -4.2087245 -4.1514764 -4.1048918 -4.0871716 -4.1165328 -4.1789565 -4.2336078 -4.2640848 -4.281085 -4.284656 -4.2713175 -4.2512846][-4.2798057 -4.2864275 -4.2761121 -4.2442212 -4.20086 -4.1726952 -4.1693773 -4.1908264 -4.2242465 -4.2515626 -4.2642 -4.2690353 -4.2663221 -4.2517247 -4.2336187][-4.2967176 -4.3058629 -4.2996922 -4.2762132 -4.2440009 -4.2243371 -4.2201529 -4.2264261 -4.2351923 -4.2421784 -4.2432747 -4.2405314 -4.2349935 -4.2212281 -4.2051024][-4.3103819 -4.3167887 -4.309906 -4.2906718 -4.2637773 -4.2427855 -4.2282224 -4.218935 -4.2095227 -4.2037086 -4.2021818 -4.2003117 -4.1994467 -4.1916852 -4.1807704][-4.3169045 -4.3158331 -4.3025002 -4.2805595 -4.2529712 -4.2269912 -4.2023425 -4.1825967 -4.1643882 -4.15685 -4.1608849 -4.1674533 -4.1752024 -4.1763968 -4.1719217][-4.3077931 -4.2993846 -4.2811093 -4.25546 -4.2242837 -4.1947122 -4.1658607 -4.1446991 -4.1301017 -4.1317182 -4.1459255 -4.1619282 -4.1751022 -4.1793394 -4.175293]]...]
INFO - root - 2017-12-07 22:04:19.222144: step 59210, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 53h:15m:18s remains)
INFO - root - 2017-12-07 22:04:25.971577: step 59220, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 53h:45m:59s remains)
INFO - root - 2017-12-07 22:04:32.688295: step 59230, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 48h:42m:43s remains)
INFO - root - 2017-12-07 22:04:39.417508: step 59240, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 49h:34m:05s remains)
INFO - root - 2017-12-07 22:04:46.210232: step 59250, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.703 sec/batch; 53h:19m:36s remains)
INFO - root - 2017-12-07 22:04:53.066208: step 59260, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 54h:03m:08s remains)
INFO - root - 2017-12-07 22:04:59.796566: step 59270, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 53h:30m:03s remains)
INFO - root - 2017-12-07 22:05:06.604966: step 59280, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 50h:13m:35s remains)
INFO - root - 2017-12-07 22:05:13.498328: step 59290, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 50h:39m:12s remains)
INFO - root - 2017-12-07 22:05:20.310066: step 59300, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.693 sec/batch; 52h:33m:38s remains)
2017-12-07 22:05:20.987797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3060427 -4.3138056 -4.3061819 -4.2981081 -4.2866693 -4.2693567 -4.2576637 -4.25503 -4.2635365 -4.2758722 -4.2889891 -4.2993479 -4.3038292 -4.3039856 -4.3049793][-4.3185649 -4.3220172 -4.3073225 -4.2916632 -4.2736778 -4.250351 -4.2338376 -4.2274685 -4.2373509 -4.2542243 -4.2740793 -4.2935514 -4.3066936 -4.3100905 -4.3132796][-4.3278356 -4.3247476 -4.3035378 -4.2806292 -4.2523861 -4.2182913 -4.1915684 -4.1771774 -4.1867018 -4.2118907 -4.2430038 -4.2744851 -4.2974415 -4.3063226 -4.3134322][-4.3275018 -4.3204517 -4.2955079 -4.263155 -4.2206306 -4.1717439 -4.1292143 -4.1037083 -4.1167078 -4.1539249 -4.1966534 -4.2409859 -4.2742877 -4.2936187 -4.307663][-4.3194513 -4.3116884 -4.28444 -4.2415972 -4.1818995 -4.1111517 -4.0466671 -4.0077462 -4.0317965 -4.0862832 -4.1400805 -4.1914582 -4.2345076 -4.2671642 -4.2928929][-4.3072987 -4.3003645 -4.2690477 -4.2125168 -4.1324735 -4.0323553 -3.9311094 -3.8638773 -3.9056807 -3.9916537 -4.0673161 -4.127182 -4.1799812 -4.2276621 -4.2680798][-4.2945323 -4.2883348 -4.2520561 -4.1872149 -4.0966697 -3.9767516 -3.8374522 -3.7286773 -3.7775748 -3.8997984 -4.0019526 -4.0798678 -4.1491613 -4.2089968 -4.253263][-4.2856078 -4.2785969 -4.2435241 -4.1824541 -4.1014576 -3.9916849 -3.860013 -3.7509966 -3.7804024 -3.8934639 -3.9953403 -4.0854473 -4.1636505 -4.2211614 -4.2586474][-4.2830691 -4.2742038 -4.2445927 -4.1948304 -4.1317387 -4.0531816 -3.9700303 -3.9034917 -3.9181225 -3.987715 -4.0641294 -4.1448011 -4.2117028 -4.2541523 -4.2778931][-4.2890444 -4.2785378 -4.252974 -4.2148981 -4.1689415 -4.1178565 -4.0740666 -4.0466847 -4.0626717 -4.1040678 -4.1562304 -4.21769 -4.2651291 -4.2910218 -4.3016348][-4.3011909 -4.2922735 -4.270412 -4.2455921 -4.2150135 -4.1812878 -4.1595016 -4.152194 -4.1662221 -4.19461 -4.2314219 -4.2741346 -4.3056474 -4.3211708 -4.3222075][-4.3131046 -4.3082275 -4.2925425 -4.2802157 -4.266355 -4.2472277 -4.2339973 -4.2323585 -4.2416568 -4.2591529 -4.28163 -4.3088517 -4.3292513 -4.3360915 -4.3320594][-4.3175335 -4.3179145 -4.3086905 -4.3058162 -4.3028593 -4.2942977 -4.2859774 -4.2845726 -4.2925181 -4.3014865 -4.3109322 -4.3232021 -4.331892 -4.3311858 -4.3263249][-4.3098183 -4.3148108 -4.3096781 -4.309968 -4.3100686 -4.3053403 -4.3003612 -4.2999845 -4.3078818 -4.3141212 -4.3156152 -4.3177347 -4.3174286 -4.3136578 -4.3113809][-4.3016224 -4.3094025 -4.3085847 -4.3112273 -4.315012 -4.3118777 -4.3073778 -4.3063517 -4.3101344 -4.3124003 -4.3097081 -4.3076472 -4.3048759 -4.3022914 -4.3017974]]...]
INFO - root - 2017-12-07 22:05:27.607591: step 59310, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 49h:45m:38s remains)
INFO - root - 2017-12-07 22:05:34.402690: step 59320, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 48h:34m:11s remains)
INFO - root - 2017-12-07 22:05:41.257599: step 59330, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 55h:23m:12s remains)
INFO - root - 2017-12-07 22:05:48.263980: step 59340, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 53h:12m:21s remains)
INFO - root - 2017-12-07 22:05:55.056592: step 59350, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 51h:23m:55s remains)
INFO - root - 2017-12-07 22:06:01.831746: step 59360, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.623 sec/batch; 47h:15m:12s remains)
INFO - root - 2017-12-07 22:06:08.673849: step 59370, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 49h:23m:19s remains)
INFO - root - 2017-12-07 22:06:15.513710: step 59380, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 53h:37m:15s remains)
INFO - root - 2017-12-07 22:06:22.240641: step 59390, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 51h:53m:41s remains)
INFO - root - 2017-12-07 22:06:28.976652: step 59400, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 49h:40m:03s remains)
2017-12-07 22:06:29.665524: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2586608 -4.2557831 -4.2678237 -4.2816243 -4.2800035 -4.2626739 -4.2454047 -4.2314954 -4.213666 -4.2022038 -4.2101121 -4.2218018 -4.2169232 -4.2060905 -4.2044024][-4.2511821 -4.248806 -4.2642326 -4.2777963 -4.2716889 -4.2550287 -4.2375321 -4.2149744 -4.1858959 -4.1746335 -4.197331 -4.2243638 -4.2198715 -4.1969709 -4.183476][-4.2450171 -4.24777 -4.267427 -4.2836342 -4.2739129 -4.2545719 -4.2303343 -4.1885695 -4.14479 -4.1423063 -4.1880479 -4.2273469 -4.2236471 -4.1928811 -4.1707773][-4.2388506 -4.2475867 -4.2756286 -4.2954335 -4.2791553 -4.2501926 -4.2052774 -4.1377711 -4.0870247 -4.1102934 -4.1834016 -4.2262087 -4.2180123 -4.181036 -4.1574035][-4.2365012 -4.2519875 -4.2861938 -4.3028855 -4.2777939 -4.2317777 -4.1555977 -4.0606513 -4.025516 -4.0956025 -4.1867447 -4.2229657 -4.2073436 -4.1679897 -4.1477427][-4.2392282 -4.2553463 -4.2849488 -4.29329 -4.2657204 -4.2017069 -4.0889487 -3.9674258 -3.971267 -4.096993 -4.1952233 -4.2197213 -4.1948614 -4.154531 -4.1385865][-4.2296677 -4.2376928 -4.2565589 -4.2606 -4.233552 -4.1509657 -4.0023909 -3.8653419 -3.9239459 -4.0886426 -4.1838226 -4.1987419 -4.1681557 -4.128499 -4.1206961][-4.2151961 -4.2157822 -4.2245827 -4.2267556 -4.200254 -4.1070013 -3.9495325 -3.8421514 -3.9426503 -4.0975108 -4.1651139 -4.1633749 -4.1309462 -4.0999861 -4.1091166][-4.2019935 -4.1966205 -4.1982207 -4.1990633 -4.17278 -4.0796919 -3.9473312 -3.9030938 -4.0162926 -4.1268787 -4.1546893 -4.1332045 -4.1031885 -4.0888057 -4.1152124][-4.1940966 -4.1832542 -4.1788449 -4.1762009 -4.1472373 -4.0576057 -3.9554458 -3.9619861 -4.071959 -4.1439734 -4.1434169 -4.112813 -4.0901155 -4.0912366 -4.1281381][-4.1976933 -4.1823173 -4.1766863 -4.1708155 -4.1398644 -4.0580745 -3.9856098 -4.0165167 -4.1084208 -4.1543126 -4.144721 -4.1200752 -4.1087418 -4.12097 -4.155705][-4.2337108 -4.2171807 -4.2111425 -4.2028604 -4.1724982 -4.1054072 -4.0581307 -4.0904503 -4.1543617 -4.180109 -4.1712713 -4.1618757 -4.1652813 -4.1806078 -4.208662][-4.2837925 -4.2691264 -4.2617946 -4.2532868 -4.2292857 -4.1835403 -4.1541562 -4.1755538 -4.2131796 -4.2279453 -4.2251282 -4.2318387 -4.2450032 -4.2564392 -4.2748823][-4.3298059 -4.3216567 -4.3147769 -4.3046041 -4.287704 -4.2602696 -4.2426629 -4.2548103 -4.2763319 -4.2838912 -4.2838149 -4.2981873 -4.3140597 -4.3201971 -4.3274417][-4.3572917 -4.3543024 -4.349215 -4.3414984 -4.3309832 -4.3158903 -4.3065915 -4.3141518 -4.3272333 -4.3301916 -4.3308387 -4.3433809 -4.3545341 -4.3551044 -4.3547096]]...]
INFO - root - 2017-12-07 22:06:36.289227: step 59410, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.711 sec/batch; 53h:54m:21s remains)
INFO - root - 2017-12-07 22:06:43.065703: step 59420, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 52h:44m:04s remains)
INFO - root - 2017-12-07 22:06:49.875142: step 59430, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 51h:03m:55s remains)
INFO - root - 2017-12-07 22:06:56.603694: step 59440, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 50h:48m:41s remains)
INFO - root - 2017-12-07 22:07:03.394564: step 59450, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 53h:56m:47s remains)
INFO - root - 2017-12-07 22:07:10.289023: step 59460, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 53h:42m:08s remains)
INFO - root - 2017-12-07 22:07:16.998966: step 59470, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:43m:01s remains)
INFO - root - 2017-12-07 22:07:23.496616: step 59480, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.624 sec/batch; 47h:17m:50s remains)
INFO - root - 2017-12-07 22:07:30.326967: step 59490, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 54h:03m:50s remains)
INFO - root - 2017-12-07 22:07:37.183346: step 59500, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 53h:03m:11s remains)
2017-12-07 22:07:37.916572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3430037 -4.3375173 -4.3225884 -4.3062544 -4.2955112 -4.294992 -4.2917504 -4.2856712 -4.2840667 -4.2912607 -4.3010721 -4.2962418 -4.2821503 -4.2716131 -4.275435][-4.3361735 -4.3263659 -4.3029485 -4.2728844 -4.2532787 -4.2509556 -4.2520704 -4.2527437 -4.2570333 -4.2690306 -4.28417 -4.2771459 -4.2619677 -4.2506104 -4.2519226][-4.327868 -4.3126612 -4.2797475 -4.2319288 -4.1946297 -4.181675 -4.1845236 -4.1936674 -4.213089 -4.2380233 -4.2611275 -4.2573128 -4.245019 -4.2369823 -4.2397][-4.3233814 -4.3045211 -4.2652149 -4.20661 -4.1506405 -4.1190505 -4.1161871 -4.1346011 -4.1718316 -4.2089682 -4.2396326 -4.2422519 -4.235455 -4.231566 -4.2330694][-4.3275857 -4.3079944 -4.268106 -4.2095475 -4.1467934 -4.0993729 -4.0876756 -4.1099319 -4.1529059 -4.1896744 -4.2198067 -4.2298417 -4.2304926 -4.2323747 -4.2334485][-4.3335114 -4.3153605 -4.2793722 -4.2308383 -4.180397 -4.1338186 -4.1124287 -4.129457 -4.1670647 -4.1969838 -4.2173338 -4.224544 -4.2229166 -4.2262478 -4.2273374][-4.3335013 -4.3147044 -4.2794232 -4.2365222 -4.1987424 -4.1666117 -4.1456194 -4.1561346 -4.1877604 -4.2162528 -4.2333326 -4.2372403 -4.2308469 -4.2307196 -4.2292705][-4.3308291 -4.3097839 -4.2748432 -4.2335768 -4.2005138 -4.1777134 -4.1561036 -4.1583619 -4.1807547 -4.2125559 -4.2350926 -4.2434535 -4.2411165 -4.2389116 -4.2348619][-4.3270721 -4.3048129 -4.2723351 -4.2340522 -4.2024574 -4.1809511 -4.1562233 -4.1497822 -4.164125 -4.1950269 -4.2204714 -4.2320795 -4.2359667 -4.2390404 -4.2346244][-4.3233647 -4.2995725 -4.2674007 -4.2313552 -4.2024827 -4.1841712 -4.1619482 -4.1542306 -4.1636691 -4.1900387 -4.217803 -4.2273059 -4.2315373 -4.2384887 -4.2373738][-4.3176312 -4.2935748 -4.2636528 -4.2292581 -4.2063742 -4.1945581 -4.1782908 -4.1726923 -4.1796255 -4.2001 -4.2220416 -4.2265644 -4.2294292 -4.2384515 -4.24101][-4.3140292 -4.2920418 -4.2665882 -4.2356806 -4.2191954 -4.2119246 -4.1957493 -4.1902952 -4.1949196 -4.2105241 -4.2276134 -4.2323189 -4.2350082 -4.2414355 -4.2440009][-4.31525 -4.2954216 -4.2738347 -4.2478971 -4.2338395 -4.2265868 -4.2112064 -4.2072845 -4.2125106 -4.2241435 -4.2354546 -4.2386117 -4.2398386 -4.2446766 -4.2479696][-4.32102 -4.3011732 -4.2810969 -4.2595053 -4.2472968 -4.2397413 -4.2301478 -4.2330966 -4.2404747 -4.2470565 -4.2499986 -4.2474895 -4.2467909 -4.2503929 -4.2541814][-4.3276348 -4.3062935 -4.2867947 -4.2702932 -4.26209 -4.258142 -4.25554 -4.2616196 -4.2683306 -4.271667 -4.2717013 -4.2680016 -4.2673635 -4.2718635 -4.2785954]]...]
INFO - root - 2017-12-07 22:07:44.460823: step 59510, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.654 sec/batch; 49h:34m:08s remains)
INFO - root - 2017-12-07 22:07:51.277248: step 59520, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.697 sec/batch; 52h:50m:22s remains)
INFO - root - 2017-12-07 22:07:58.142707: step 59530, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 53h:31m:14s remains)
INFO - root - 2017-12-07 22:08:04.986322: step 59540, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 51h:25m:30s remains)
INFO - root - 2017-12-07 22:08:11.808781: step 59550, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 49h:16m:04s remains)
INFO - root - 2017-12-07 22:08:18.630974: step 59560, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 49h:21m:32s remains)
INFO - root - 2017-12-07 22:08:25.506518: step 59570, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.687 sec/batch; 52h:03m:19s remains)
INFO - root - 2017-12-07 22:08:32.291532: step 59580, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.693 sec/batch; 52h:30m:14s remains)
INFO - root - 2017-12-07 22:08:39.223686: step 59590, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 49h:30m:09s remains)
INFO - root - 2017-12-07 22:08:45.906040: step 59600, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 48h:28m:36s remains)
2017-12-07 22:08:46.706026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3151965 -4.3132482 -4.3097363 -4.3054504 -4.3054752 -4.3090382 -4.3109035 -4.3091135 -4.3094153 -4.3098888 -4.3088183 -4.30777 -4.3051324 -4.306098 -4.3099003][-4.3053627 -4.2995286 -4.2930379 -4.2856731 -4.2855368 -4.2923784 -4.2961192 -4.2947869 -4.2975693 -4.2987833 -4.2941875 -4.2885442 -4.2821851 -4.2829103 -4.2895937][-4.2840791 -4.2728777 -4.2634215 -4.2548003 -4.2560039 -4.2671142 -4.273304 -4.2716675 -4.2761612 -4.2770495 -4.2693124 -4.2611885 -4.2541237 -4.2563376 -4.2654843][-4.25575 -4.2383647 -4.2291851 -4.2249579 -4.2328973 -4.2495356 -4.2534151 -4.2474813 -4.2509956 -4.2483706 -4.2366581 -4.2294583 -4.2297955 -4.2384238 -4.2504826][-4.2163997 -4.19296 -4.1862822 -4.1950345 -4.21182 -4.2283111 -4.2258883 -4.2130179 -4.2193589 -4.2186604 -4.2047439 -4.197865 -4.2075963 -4.2272329 -4.2447176][-4.1730371 -4.1427083 -4.1373215 -4.1592655 -4.1802111 -4.1857705 -4.1675563 -4.14357 -4.1573067 -4.1688752 -4.1611643 -4.159358 -4.1820035 -4.2136431 -4.2384362][-4.1451569 -4.1078548 -4.097146 -4.1173964 -4.13134 -4.1137686 -4.0596414 -4.0157394 -4.0487852 -4.092751 -4.1096272 -4.1236238 -4.1589484 -4.1991482 -4.2304368][-4.152792 -4.1139603 -4.0905204 -4.0918174 -4.0829749 -4.0366573 -3.9354751 -3.8530691 -3.9145141 -4.0009961 -4.05227 -4.0875187 -4.1350212 -4.1838408 -4.221931][-4.2093191 -4.177794 -4.1407137 -4.1201372 -4.093771 -4.0334263 -3.9169221 -3.8233724 -3.8995974 -4.0027833 -4.0668874 -4.1071353 -4.1492004 -4.1912174 -4.226336][-4.262764 -4.2488089 -4.2164969 -4.1903415 -4.1649294 -4.1177258 -4.0407524 -3.9829323 -4.0365458 -4.1097336 -4.151916 -4.172368 -4.1929426 -4.2180047 -4.2406745][-4.283145 -4.281682 -4.2589941 -4.2346492 -4.2150326 -4.1852331 -4.1422148 -4.11408 -4.1486492 -4.1912665 -4.2127981 -4.2220488 -4.229506 -4.2429771 -4.2547455][-4.2948155 -4.2919521 -4.2741609 -4.2544975 -4.2388439 -4.2187738 -4.1962719 -4.1861277 -4.2109704 -4.231555 -4.239388 -4.243206 -4.2480955 -4.2601008 -4.2673287][-4.3142629 -4.3088155 -4.2951908 -4.2828932 -4.2726893 -4.258985 -4.245368 -4.2426496 -4.2595949 -4.2675877 -4.2678337 -4.2682405 -4.2706203 -4.2794971 -4.2835312][-4.3245687 -4.3180733 -4.3061919 -4.298768 -4.293921 -4.2849855 -4.27503 -4.2713284 -4.2815933 -4.2875204 -4.2892475 -4.2905812 -4.2932158 -4.2993979 -4.3004036][-4.3236566 -4.3167481 -4.3061481 -4.3001256 -4.2980433 -4.2945657 -4.2877359 -4.28399 -4.2891412 -4.2939014 -4.2995491 -4.3055434 -4.3104916 -4.3145442 -4.3148589]]...]
INFO - root - 2017-12-07 22:08:53.332320: step 59610, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 49h:04m:41s remains)
INFO - root - 2017-12-07 22:09:00.266778: step 59620, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 48h:52m:19s remains)
INFO - root - 2017-12-07 22:09:06.967488: step 59630, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 49h:22m:51s remains)
INFO - root - 2017-12-07 22:09:13.793010: step 59640, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 54h:19m:34s remains)
INFO - root - 2017-12-07 22:09:20.705075: step 59650, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.726 sec/batch; 55h:01m:20s remains)
INFO - root - 2017-12-07 22:09:27.445478: step 59660, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 55h:23m:10s remains)
INFO - root - 2017-12-07 22:09:34.372280: step 59670, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 50h:56m:41s remains)
INFO - root - 2017-12-07 22:09:41.180781: step 59680, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 50h:03m:40s remains)
INFO - root - 2017-12-07 22:09:47.998645: step 59690, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 53h:44m:02s remains)
INFO - root - 2017-12-07 22:09:54.840720: step 59700, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 54h:47m:35s remains)
2017-12-07 22:09:55.609773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2850909 -4.2805772 -4.2728581 -4.2622118 -4.2582278 -4.2620115 -4.2716856 -4.2790885 -4.2867756 -4.2980123 -4.3072681 -4.3091593 -4.3045249 -4.3031578 -4.312201][-4.2465148 -4.2327886 -4.2165518 -4.1995678 -4.1950774 -4.1986318 -4.2088022 -4.2189579 -4.2329736 -4.2547507 -4.2741117 -4.2806606 -4.2762871 -4.2756262 -4.2905607][-4.213912 -4.18696 -4.161952 -4.1425858 -4.1441207 -4.1508074 -4.1572394 -4.1633282 -4.1798153 -4.2094264 -4.2373929 -4.2492313 -4.2464614 -4.2467551 -4.2673883][-4.1975365 -4.1628637 -4.1370564 -4.1225066 -4.1308317 -4.138134 -4.1330204 -4.127367 -4.1431756 -4.1775641 -4.210772 -4.2290378 -4.2295628 -4.2322083 -4.255517][-4.1935453 -4.1546159 -4.1277075 -4.1156616 -4.126153 -4.1319427 -4.1179514 -4.100533 -4.112576 -4.1463308 -4.1814332 -4.2048516 -4.2133665 -4.2224984 -4.2486906][-4.1927962 -4.1543112 -4.1278052 -4.1140542 -4.1209059 -4.1233029 -4.1010408 -4.0684543 -4.069118 -4.1001344 -4.1416378 -4.1720381 -4.1902342 -4.2080741 -4.2394314][-4.1955457 -4.1612835 -4.1351643 -4.1195054 -4.12037 -4.1139112 -4.0782437 -4.0268273 -4.0114827 -4.0397592 -4.0914946 -4.1367579 -4.1680083 -4.1941423 -4.2319961][-4.2015567 -4.170907 -4.1429782 -4.1234269 -4.11561 -4.1013021 -4.0562358 -3.9957764 -3.9729371 -4.0014844 -4.061439 -4.1191044 -4.1605825 -4.1920171 -4.2313771][-4.2198172 -4.19108 -4.1591597 -4.1317635 -4.1117783 -4.0869522 -4.0357723 -3.9781897 -3.9620445 -3.9993262 -4.06755 -4.1302481 -4.1707578 -4.1979504 -4.2326326][-4.24809 -4.2240372 -4.1913815 -4.155766 -4.1252093 -4.0866446 -4.0273852 -3.9730365 -3.9652469 -4.0127497 -4.0882993 -4.1490674 -4.1823869 -4.2038822 -4.2313547][-4.2731752 -4.2547116 -4.2251167 -4.1888509 -4.1566043 -4.1142621 -4.0552335 -4.003737 -3.9950008 -4.0405211 -4.1141458 -4.1664438 -4.1905532 -4.2046285 -4.225976][-4.2887492 -4.2755957 -4.2532315 -4.2215557 -4.1927629 -4.1571684 -4.1073055 -4.0654907 -4.0604658 -4.0935407 -4.1495008 -4.1864991 -4.203289 -4.212193 -4.227272][-4.2968826 -4.2888885 -4.2746711 -4.2516131 -4.2298141 -4.2042689 -4.167172 -4.1383448 -4.1387324 -4.1582417 -4.1911483 -4.2146478 -4.2273426 -4.2334824 -4.2418771][-4.3015032 -4.2957721 -4.2861676 -4.2721262 -4.2588921 -4.2419467 -4.2170091 -4.1984434 -4.2002568 -4.2110691 -4.22865 -4.2447371 -4.2551303 -4.2579536 -4.26129][-4.3041291 -4.2989287 -4.2915978 -4.28355 -4.2759566 -4.2651668 -4.2491865 -4.2356477 -4.235137 -4.2426953 -4.2555838 -4.2689805 -4.2773819 -4.2786717 -4.2811642]]...]
INFO - root - 2017-12-07 22:10:02.361348: step 59710, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 51h:05m:23s remains)
INFO - root - 2017-12-07 22:10:09.182736: step 59720, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 52h:03m:16s remains)
INFO - root - 2017-12-07 22:10:16.025318: step 59730, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 55h:10m:28s remains)
INFO - root - 2017-12-07 22:10:22.829325: step 59740, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 54h:46m:50s remains)
INFO - root - 2017-12-07 22:10:29.593682: step 59750, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 48h:23m:01s remains)
INFO - root - 2017-12-07 22:10:36.407589: step 59760, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.632 sec/batch; 47h:54m:52s remains)
INFO - root - 2017-12-07 22:10:43.390991: step 59770, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.729 sec/batch; 55h:13m:52s remains)
INFO - root - 2017-12-07 22:10:50.253071: step 59780, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 56h:13m:27s remains)
INFO - root - 2017-12-07 22:10:56.803344: step 59790, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 48h:20m:29s remains)
INFO - root - 2017-12-07 22:11:03.597170: step 59800, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 50h:55m:19s remains)
2017-12-07 22:11:04.445167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3315449 -4.33057 -4.3288856 -4.3263984 -4.3238697 -4.3237696 -4.3235254 -4.3240662 -4.3268929 -4.3297172 -4.33216 -4.3326449 -4.3320279 -4.3341656 -4.3380122][-4.31123 -4.3095818 -4.307497 -4.3040285 -4.3018007 -4.301446 -4.2981973 -4.2975545 -4.3014264 -4.3027873 -4.3022981 -4.3007941 -4.2984171 -4.2993145 -4.3054624][-4.2946877 -4.2903419 -4.2859416 -4.2812576 -4.2780714 -4.27655 -4.268961 -4.268621 -4.2757339 -4.2770653 -4.2713103 -4.2664804 -4.2594266 -4.2585292 -4.2666669][-4.2791433 -4.2661209 -4.2557268 -4.2456021 -4.237289 -4.2297463 -4.2155185 -4.2126846 -4.2256265 -4.2332382 -4.2266083 -4.2182717 -4.2075958 -4.2073812 -4.2210569][-4.2693195 -4.2470164 -4.2270455 -4.2056336 -4.1876335 -4.1697431 -4.1408329 -4.1240454 -4.1430607 -4.1718597 -4.1798692 -4.1746669 -4.1658831 -4.1694503 -4.188735][-4.2566147 -4.2222939 -4.1889873 -4.1529422 -4.1208291 -4.0822821 -4.0208 -3.9719009 -4.004838 -4.0800457 -4.1268239 -4.1368246 -4.1352282 -4.1439166 -4.1670265][-4.2454896 -4.2026806 -4.155992 -4.1067495 -4.0548973 -3.9852304 -3.876457 -3.7862206 -3.8417876 -3.97868 -4.0711794 -4.0993347 -4.1031718 -4.1133366 -4.1401176][-4.2427926 -4.1984277 -4.1459079 -4.0871873 -4.020545 -3.936326 -3.815551 -3.7119129 -3.7814443 -3.9439223 -4.0531526 -4.0828753 -4.0796866 -4.0881386 -4.1190166][-4.252243 -4.2170119 -4.1711988 -4.113719 -4.0532579 -3.9876304 -3.9096973 -3.850749 -3.9005578 -4.01499 -4.0923553 -4.0989075 -4.0781426 -4.0803494 -4.1103497][-4.2627821 -4.2373719 -4.2026219 -4.1540074 -4.1079736 -4.0671048 -4.03213 -4.0062971 -4.0278158 -4.0856676 -4.1250863 -4.1127219 -4.0825362 -4.085475 -4.1177306][-4.2685566 -4.2471962 -4.2199011 -4.1862521 -4.1528053 -4.1297331 -4.1219153 -4.11509 -4.1188393 -4.1400275 -4.1509485 -4.1273336 -4.0985875 -4.1080446 -4.1454992][-4.276391 -4.2600141 -4.24054 -4.2197218 -4.1990714 -4.1876378 -4.19542 -4.20242 -4.2035542 -4.2043204 -4.1985884 -4.1774936 -4.158639 -4.1703491 -4.2036066][-4.2860703 -4.277544 -4.2664909 -4.2573843 -4.2488313 -4.24626 -4.2565136 -4.2653723 -4.26686 -4.2627516 -4.2556353 -4.2434464 -4.2355537 -4.2451143 -4.2677112][-4.30054 -4.2980847 -4.2950897 -4.2947855 -4.2934957 -4.2929611 -4.2967734 -4.2991672 -4.2968373 -4.2926188 -4.2878342 -4.2836761 -4.2857456 -4.2954888 -4.3114276][-4.3185091 -4.319231 -4.319427 -4.3205366 -4.3197508 -4.3181243 -4.3175311 -4.3162694 -4.3128452 -4.3095479 -4.3078446 -4.3086519 -4.3142142 -4.322783 -4.33313]]...]
INFO - root - 2017-12-07 22:11:11.108993: step 59810, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 50h:17m:51s remains)
INFO - root - 2017-12-07 22:11:17.867886: step 59820, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 50h:07m:11s remains)
INFO - root - 2017-12-07 22:11:24.618982: step 59830, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 51h:27m:31s remains)
INFO - root - 2017-12-07 22:11:31.465273: step 59840, loss = 2.02, batch loss = 1.96 (11.5 examples/sec; 0.695 sec/batch; 52h:38m:54s remains)
INFO - root - 2017-12-07 22:11:38.214057: step 59850, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 52h:27m:52s remains)
INFO - root - 2017-12-07 22:11:45.035216: step 59860, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 52h:21m:03s remains)
INFO - root - 2017-12-07 22:11:51.776065: step 59870, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 51h:42m:56s remains)
INFO - root - 2017-12-07 22:11:58.688387: step 59880, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 51h:47m:36s remains)
INFO - root - 2017-12-07 22:12:05.468644: step 59890, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 52h:21m:43s remains)
INFO - root - 2017-12-07 22:12:12.260442: step 59900, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.716 sec/batch; 54h:14m:20s remains)
2017-12-07 22:12:13.027507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2087855 -4.20955 -4.2117615 -4.222826 -4.2442231 -4.2614884 -4.2711167 -4.2741823 -4.273078 -4.2753544 -4.2866411 -4.297327 -4.305727 -4.3091974 -4.3018017][-4.2270761 -4.2266345 -4.2250614 -4.2295046 -4.242743 -4.2550817 -4.2605 -4.259551 -4.2546282 -4.2546821 -4.2672887 -4.27951 -4.290442 -4.2987471 -4.2946992][-4.241221 -4.240819 -4.2365136 -4.2329526 -4.2322807 -4.2322307 -4.2296534 -4.2259908 -4.2213678 -4.2224975 -4.2333779 -4.2417641 -4.2503409 -4.2591357 -4.2593584][-4.2334046 -4.2351217 -4.230125 -4.2217846 -4.2083421 -4.1942549 -4.18292 -4.1747217 -4.1691012 -4.168169 -4.1723771 -4.1713352 -4.1744747 -4.1846371 -4.1952448][-4.2166257 -4.2190261 -4.2126112 -4.1979065 -4.1724949 -4.1441722 -4.1192656 -4.1012058 -4.0941939 -4.0966334 -4.1001077 -4.0929317 -4.0916409 -4.106463 -4.1319757][-4.2089758 -4.2078657 -4.193294 -4.1640081 -4.1223297 -4.0775766 -4.0349655 -4.0046911 -4.0043926 -4.0260892 -4.043303 -4.0431547 -4.046535 -4.0688643 -4.1055355][-4.2054539 -4.1952558 -4.167202 -4.1200514 -4.0635037 -4.0062008 -3.9509168 -3.9170818 -3.9353127 -3.9829307 -4.0216446 -4.0422139 -4.0615211 -4.090436 -4.126164][-4.2112336 -4.1876326 -4.1447697 -4.0857949 -4.0196476 -3.95935 -3.9093316 -3.8909156 -3.9297113 -3.9906077 -4.0431542 -4.0814462 -4.1126904 -4.1397576 -4.1643286][-4.2303338 -4.194221 -4.1418276 -4.0763774 -4.0099053 -3.9558761 -3.9242978 -3.9290009 -3.9758296 -4.0364108 -4.0911155 -4.1368194 -4.1720676 -4.1951842 -4.21058][-4.2578239 -4.2206454 -4.17255 -4.1139312 -4.0564637 -4.0101027 -3.9909756 -4.0044618 -4.0461812 -4.099791 -4.1535997 -4.2024813 -4.2368789 -4.254705 -4.2626858][-4.270854 -4.2386665 -4.198267 -4.1516376 -4.1080675 -4.0730338 -4.0616641 -4.0741162 -4.1073747 -4.151906 -4.2003632 -4.2453351 -4.2726774 -4.2831845 -4.2855225][-4.2676835 -4.2426581 -4.2104154 -4.1761241 -4.1464195 -4.1239157 -4.1181316 -4.1288619 -4.1552849 -4.189631 -4.2273793 -4.2614813 -4.2755914 -4.27475 -4.270009][-4.2468843 -4.231658 -4.2086344 -4.1842604 -4.1666708 -4.1556506 -4.1556978 -4.1673059 -4.1889534 -4.2137275 -4.2385378 -4.2557874 -4.2516828 -4.2366729 -4.2229338][-4.2163744 -4.2114353 -4.1989551 -4.18341 -4.1739216 -4.1675806 -4.1682334 -4.177907 -4.1943026 -4.2112923 -4.2249846 -4.2277379 -4.2089415 -4.1827869 -4.1601505][-4.1887856 -4.193953 -4.1940513 -4.1882491 -4.1823015 -4.176456 -4.1728187 -4.175529 -4.1830411 -4.1909347 -4.1956744 -4.190227 -4.1652465 -4.1339979 -4.1057439]]...]
INFO - root - 2017-12-07 22:12:19.634728: step 59910, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 51h:12m:41s remains)
INFO - root - 2017-12-07 22:12:26.505106: step 59920, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 51h:43m:12s remains)
INFO - root - 2017-12-07 22:12:33.271842: step 59930, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 54h:03m:17s remains)
INFO - root - 2017-12-07 22:12:40.131877: step 59940, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.700 sec/batch; 53h:02m:02s remains)
INFO - root - 2017-12-07 22:12:46.885048: step 59950, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 51h:05m:53s remains)
INFO - root - 2017-12-07 22:12:53.740737: step 59960, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:38m:23s remains)
INFO - root - 2017-12-07 22:13:00.633450: step 59970, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 53h:09m:42s remains)
INFO - root - 2017-12-07 22:13:07.436202: step 59980, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 54h:35m:32s remains)
INFO - root - 2017-12-07 22:13:14.279172: step 59990, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 50h:28m:38s remains)
INFO - root - 2017-12-07 22:13:21.046391: step 60000, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 49h:05m:37s remains)
2017-12-07 22:13:21.823741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3284059 -4.328052 -4.33057 -4.3267732 -4.3158641 -4.3057947 -4.3021836 -4.3044672 -4.3117037 -4.3159027 -4.3123741 -4.3060036 -4.3062215 -4.313324 -4.3199272][-4.3218064 -4.321279 -4.3198204 -4.3085766 -4.288909 -4.2730017 -4.2695293 -4.2749972 -4.2867746 -4.2960362 -4.2959914 -4.2892308 -4.2899971 -4.3003793 -4.3070745][-4.3009048 -4.29994 -4.2922673 -4.2718892 -4.2445145 -4.2242513 -4.2202568 -4.2264442 -4.2418985 -4.2584677 -4.2654514 -4.2606912 -4.2626548 -4.2766662 -4.2840214][-4.277741 -4.2751822 -4.2583747 -4.2234068 -4.1848569 -4.1644554 -4.1646352 -4.1717253 -4.1885796 -4.2131357 -4.2298965 -4.2329288 -4.236412 -4.2488303 -4.2555571][-4.2618675 -4.2545953 -4.2263613 -4.1725035 -4.1210861 -4.1027327 -4.1134806 -4.12584 -4.1433206 -4.17174 -4.1942353 -4.2038431 -4.209672 -4.22045 -4.2266545][-4.2534947 -4.2350311 -4.1948671 -4.1267805 -4.0655184 -4.0518551 -4.0800247 -4.1026282 -4.1203952 -4.1469111 -4.1674037 -4.1728573 -4.177166 -4.1907668 -4.1982279][-4.2475481 -4.2142859 -4.1667948 -4.1008787 -4.0444179 -4.0361271 -4.0702834 -4.0958533 -4.1120763 -4.1338673 -4.15313 -4.1568909 -4.1554389 -4.1693897 -4.1792502][-4.2348957 -4.1880469 -4.1450977 -4.1006923 -4.0634208 -4.0532403 -4.0726104 -4.0893536 -4.1000624 -4.1188145 -4.1418967 -4.1527967 -4.1511989 -4.1595597 -4.1666136][-4.2140222 -4.163713 -4.1368771 -4.123035 -4.1039658 -4.0875049 -4.0872331 -4.09127 -4.0959463 -4.111002 -4.1347723 -4.1489244 -4.1473174 -4.1514997 -4.1586876][-4.1913848 -4.1488957 -4.1383061 -4.1445165 -4.1416087 -4.1274509 -4.1207933 -4.1176796 -4.1180959 -4.13202 -4.1519818 -4.1593332 -4.154222 -4.1576991 -4.1659083][-4.1838384 -4.1494532 -4.1438789 -4.1584387 -4.1665115 -4.1581354 -4.1503468 -4.1457272 -4.1466069 -4.1625681 -4.1780205 -4.1773682 -4.1707253 -4.1765909 -4.1861162][-4.1992793 -4.1697965 -4.1620612 -4.1767673 -4.1878948 -4.1824055 -4.17613 -4.1775026 -4.1820049 -4.1937895 -4.2012882 -4.1948714 -4.1893616 -4.1960974 -4.2061663][-4.2179122 -4.1936417 -4.1852965 -4.1983438 -4.2103152 -4.2098527 -4.2102265 -4.2178149 -4.2242165 -4.2289858 -4.2267118 -4.2144661 -4.2065277 -4.2114224 -4.2194376][-4.2403183 -4.2222786 -4.2144847 -4.2233715 -4.2348185 -4.2398844 -4.2464304 -4.2555337 -4.2601004 -4.2598066 -4.2540402 -4.2415786 -4.2342548 -4.2374897 -4.2437015][-4.2681818 -4.2564435 -4.2509336 -4.2560267 -4.2653904 -4.2721219 -4.2802997 -4.2874579 -4.2895517 -4.2877622 -4.2836475 -4.2769594 -4.2732172 -4.2760959 -4.2804728]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-07 22:13:29.458935: step 60010, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 51h:52m:59s remains)
INFO - root - 2017-12-07 22:13:36.326256: step 60020, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 53h:39m:06s remains)
INFO - root - 2017-12-07 22:13:43.120722: step 60030, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 52h:53m:49s remains)
INFO - root - 2017-12-07 22:13:49.747221: step 60040, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 48h:04m:48s remains)
INFO - root - 2017-12-07 22:13:56.585657: step 60050, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 49h:55m:21s remains)
INFO - root - 2017-12-07 22:14:03.475956: step 60060, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 54h:40m:24s remains)
INFO - root - 2017-12-07 22:14:10.338049: step 60070, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.679 sec/batch; 51h:22m:04s remains)
INFO - root - 2017-12-07 22:14:17.060312: step 60080, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 50h:13m:44s remains)
INFO - root - 2017-12-07 22:14:23.814841: step 60090, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 48h:55m:26s remains)
INFO - root - 2017-12-07 22:14:30.502328: step 60100, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.711 sec/batch; 53h:50m:07s remains)
2017-12-07 22:14:31.213766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2112346 -4.21452 -4.2188725 -4.2141962 -4.2070637 -4.2164488 -4.2302885 -4.2349906 -4.2294769 -4.2239594 -4.2231603 -4.2264123 -4.2293072 -4.2304516 -4.2288456][-4.2258539 -4.2323656 -4.2384362 -4.23648 -4.2347293 -4.2481503 -4.2626367 -4.2672071 -4.2586331 -4.250071 -4.2486563 -4.2512264 -4.2547984 -4.2574553 -4.2584386][-4.2312222 -4.2391691 -4.2498684 -4.2530904 -4.2585583 -4.2746668 -4.2861767 -4.2847095 -4.2723246 -4.2614946 -4.2618995 -4.2675238 -4.2755671 -4.2828245 -4.2890515][-4.2315454 -4.2393532 -4.2508173 -4.2575545 -4.2665982 -4.2787852 -4.2813315 -4.2694426 -4.2540922 -4.2460575 -4.2512541 -4.2618651 -4.274858 -4.2881517 -4.2991734][-4.2038274 -4.2103477 -4.21958 -4.225821 -4.2326741 -4.2383757 -4.2298903 -4.2082295 -4.1949067 -4.1993814 -4.2166047 -4.2349396 -4.2525225 -4.2692695 -4.2823224][-4.1352978 -4.1338525 -4.1383996 -4.14295 -4.1474986 -4.1473927 -4.1246367 -4.0906487 -4.0818577 -4.1093369 -4.1488533 -4.1800261 -4.2025476 -4.2196851 -4.2312403][-4.0546236 -4.0439234 -4.0464249 -4.052403 -4.0546632 -4.0461493 -4.0044527 -3.9518096 -3.9473097 -4.0035772 -4.0683107 -4.1118736 -4.1372247 -4.1523361 -4.1608586][-4.0536785 -4.0339174 -4.0304174 -4.0321321 -4.029542 -4.0141659 -3.9623544 -3.9011729 -3.9009509 -3.9700329 -4.0405345 -4.0807419 -4.0972548 -4.1027451 -4.10417][-4.1422987 -4.1247153 -4.1155534 -4.1108203 -4.1032968 -4.0900378 -4.054626 -4.0137625 -4.0164318 -4.0658121 -4.1112752 -4.1274219 -4.1234775 -4.112216 -4.1021934][-4.219089 -4.209229 -4.2029648 -4.1991777 -4.194149 -4.1891165 -4.1742148 -4.1563549 -4.161221 -4.188086 -4.2076616 -4.2050047 -4.187964 -4.1686263 -4.1543036][-4.2507057 -4.2488 -4.2482252 -4.2475548 -4.2460995 -4.2461834 -4.239213 -4.2287574 -4.2291532 -4.24172 -4.2520633 -4.2489662 -4.2369876 -4.2251992 -4.2186036][-4.2338266 -4.2419109 -4.2498765 -4.2534533 -4.2542558 -4.2559328 -4.2487292 -4.2348094 -4.2273812 -4.2324181 -4.2432904 -4.24912 -4.25038 -4.2523494 -4.2576213][-4.1948996 -4.2083974 -4.2208357 -4.2231464 -4.2226405 -4.2237086 -4.2167172 -4.2011442 -4.19124 -4.1952891 -4.2106056 -4.2259073 -4.2391253 -4.2522197 -4.2627249][-4.1737051 -4.1822877 -4.1861234 -4.1761909 -4.1690154 -4.1704965 -4.1671467 -4.156117 -4.1502976 -4.15896 -4.1788082 -4.1976786 -4.2148581 -4.2289038 -4.2350359][-4.177866 -4.1767612 -4.1668439 -4.144877 -4.1297932 -4.1295857 -4.1295424 -4.1256113 -4.1283107 -4.1406784 -4.1609941 -4.1783452 -4.1898665 -4.1970773 -4.1944041]]...]
INFO - root - 2017-12-07 22:14:37.912976: step 60110, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 50h:10m:52s remains)
INFO - root - 2017-12-07 22:14:44.681183: step 60120, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 52h:34m:41s remains)
INFO - root - 2017-12-07 22:14:51.507118: step 60130, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.703 sec/batch; 53h:10m:54s remains)
INFO - root - 2017-12-07 22:14:58.344092: step 60140, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.744 sec/batch; 56h:16m:31s remains)
INFO - root - 2017-12-07 22:15:05.135625: step 60150, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.675 sec/batch; 51h:05m:48s remains)
INFO - root - 2017-12-07 22:15:11.963463: step 60160, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.672 sec/batch; 50h:50m:11s remains)
INFO - root - 2017-12-07 22:15:18.784821: step 60170, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 51h:44m:20s remains)
INFO - root - 2017-12-07 22:15:25.537984: step 60180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 52h:50m:35s remains)
INFO - root - 2017-12-07 22:15:32.393346: step 60190, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 52h:45m:16s remains)
INFO - root - 2017-12-07 22:15:39.186607: step 60200, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 54h:03m:46s remains)
2017-12-07 22:15:39.932045: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2830787 -4.27292 -4.2761 -4.28359 -4.2835298 -4.2730637 -4.2612309 -4.262198 -4.2769165 -4.2873359 -4.288734 -4.2784514 -4.2533274 -4.2237892 -4.2015333][-4.2667913 -4.2660241 -4.2788725 -4.2897663 -4.2867675 -4.2709432 -4.2574954 -4.2665205 -4.2885771 -4.301013 -4.2976842 -4.2786508 -4.2424498 -4.201139 -4.1752582][-4.218019 -4.2320085 -4.2600074 -4.2738638 -4.2626524 -4.2372608 -4.2202811 -4.240777 -4.2810507 -4.3037815 -4.2989073 -4.2725492 -4.2219906 -4.1653619 -4.1336188][-4.1667628 -4.1931729 -4.2352591 -4.2482533 -4.2190037 -4.1693711 -4.1451583 -4.1820812 -4.2458816 -4.2842641 -4.2835226 -4.2532706 -4.1964755 -4.1336031 -4.1022835][-4.1384349 -4.1733174 -4.219768 -4.22376 -4.1655903 -4.078599 -4.0448365 -4.1064014 -4.197329 -4.2559843 -4.2666044 -4.235827 -4.1745043 -4.1099629 -4.0856977][-4.1251297 -4.1678767 -4.2119069 -4.2008166 -4.1099691 -3.9779956 -3.9258022 -4.0107827 -4.1325989 -4.2175169 -4.2420282 -4.2114344 -4.1469855 -4.0817838 -4.0604911][-4.1185904 -4.164228 -4.197084 -4.1683378 -4.050004 -3.8814669 -3.8083744 -3.9044363 -4.0519481 -4.1617126 -4.2007294 -4.172924 -4.109035 -4.0487518 -4.0331068][-4.1247125 -4.1622324 -4.1822977 -4.1439633 -4.0205922 -3.8506114 -3.7740941 -3.8577785 -4.00206 -4.119401 -4.1601672 -4.132082 -4.0688524 -4.0140662 -4.0069418][-4.1341963 -4.1619945 -4.1749148 -4.1400275 -4.0354156 -3.9012146 -3.8430362 -3.898632 -4.0117588 -4.1058559 -4.1355958 -4.1027813 -4.037025 -3.9852917 -3.9826412][-4.1334181 -4.1560903 -4.1694851 -4.1445093 -4.0664005 -3.9749362 -3.9400129 -3.9690046 -4.0399609 -4.0989575 -4.1160278 -4.0862164 -4.0208797 -3.9693122 -3.9645143][-4.1338115 -4.1499782 -4.1619797 -4.1445208 -4.0913548 -4.038413 -4.022398 -4.0323324 -4.066288 -4.098238 -4.1074181 -4.0846534 -4.0280161 -3.9810271 -3.9732504][-4.1514792 -4.1615262 -4.1676636 -4.1521339 -4.1171861 -4.0918331 -4.0863895 -4.0852423 -4.0945616 -4.1122446 -4.1239071 -4.1121378 -4.0679646 -4.0240469 -4.0090294][-4.186801 -4.192627 -4.1941295 -4.1812043 -4.1588731 -4.147018 -4.1448441 -4.1390839 -4.137722 -4.1482573 -4.1602573 -4.1575952 -4.1261449 -4.0905428 -4.0727344][-4.2332821 -4.2350469 -4.2357607 -4.2286606 -4.2164989 -4.2100182 -4.2079692 -4.1998992 -4.193964 -4.1974282 -4.2038336 -4.2041388 -4.1845169 -4.1619883 -4.1506233][-4.2823339 -4.2820597 -4.2819242 -4.277585 -4.2717795 -4.2687559 -4.2674441 -4.2625294 -4.2555881 -4.2518716 -4.2526307 -4.2526402 -4.2439165 -4.2356615 -4.2348785]]...]
INFO - root - 2017-12-07 22:15:46.578445: step 60210, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 52h:03m:16s remains)
INFO - root - 2017-12-07 22:15:53.337135: step 60220, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 50h:22m:35s remains)
INFO - root - 2017-12-07 22:16:00.069835: step 60230, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 47h:40m:48s remains)
INFO - root - 2017-12-07 22:16:06.712440: step 60240, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.630 sec/batch; 47h:40m:49s remains)
INFO - root - 2017-12-07 22:16:13.637064: step 60250, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 53h:40m:35s remains)
INFO - root - 2017-12-07 22:16:20.540259: step 60260, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 54h:12m:36s remains)
INFO - root - 2017-12-07 22:16:27.329714: step 60270, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 50h:38m:33s remains)
INFO - root - 2017-12-07 22:16:34.137497: step 60280, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 47h:33m:05s remains)
INFO - root - 2017-12-07 22:16:40.965316: step 60290, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.691 sec/batch; 52h:15m:52s remains)
INFO - root - 2017-12-07 22:16:47.691034: step 60300, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 51h:54m:34s remains)
2017-12-07 22:16:48.412480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2210121 -4.2288232 -4.231452 -4.2374125 -4.2353396 -4.2088518 -4.1545386 -4.1098957 -4.1317868 -4.1768942 -4.1858048 -4.1840067 -4.1954784 -4.1978183 -4.1898289][-4.232759 -4.2444196 -4.2506461 -4.2549415 -4.2451119 -4.2141166 -4.1595087 -4.1147661 -4.1379485 -4.18191 -4.1932335 -4.1925578 -4.198699 -4.1990681 -4.1894517][-4.2335806 -4.2499022 -4.2603159 -4.2656097 -4.2568517 -4.2288642 -4.1720896 -4.1230774 -4.1420946 -4.1824241 -4.193511 -4.1960058 -4.2002769 -4.2011185 -4.1888747][-4.2216415 -4.2389717 -4.2504821 -4.2569666 -4.2531271 -4.2256584 -4.1621242 -4.1086392 -4.1326289 -4.1782451 -4.1922264 -4.2017226 -4.2055783 -4.2071638 -4.1979752][-4.1859159 -4.2014012 -4.2179713 -4.2309341 -4.2295718 -4.2002869 -4.1206388 -4.0621963 -4.1068287 -4.1667047 -4.1865883 -4.1981316 -4.2001963 -4.2027812 -4.1973886][-4.13363 -4.1448159 -4.1740551 -4.1977806 -4.1971693 -4.162015 -4.0532904 -3.9831619 -4.0625963 -4.1434159 -4.1693034 -4.1829987 -4.1869173 -4.1904192 -4.1818209][-4.0784721 -4.0836644 -4.1272807 -4.1596632 -4.1576562 -4.1028194 -3.9464712 -3.8538067 -3.9870918 -4.0980043 -4.1341224 -4.1546612 -4.1630087 -4.1712394 -4.1662908][-4.0540338 -4.0527811 -4.1067452 -4.1474681 -4.1432881 -4.0752072 -3.8963642 -3.7978747 -3.9598069 -4.07959 -4.1131821 -4.1366382 -4.1557984 -4.1711173 -4.1745625][-4.0797 -4.0818386 -4.1376204 -4.1793532 -4.1814475 -4.1252608 -3.979475 -3.9059618 -4.0344563 -4.125524 -4.1427746 -4.1625276 -4.1832523 -4.1954117 -4.1974387][-4.1132956 -4.1191344 -4.1728144 -4.2120752 -4.2188935 -4.179172 -4.0744228 -4.0213962 -4.1132154 -4.174737 -4.1822042 -4.1987181 -4.2095046 -4.2103667 -4.2077169][-4.1327357 -4.1382332 -4.1870685 -4.2249 -4.2358813 -4.2085032 -4.1345396 -4.0948963 -4.1570845 -4.196424 -4.1987567 -4.2181745 -4.2225394 -4.2167277 -4.214046][-4.1638236 -4.16471 -4.1982055 -4.2293334 -4.240962 -4.2178054 -4.1596928 -4.130794 -4.17617 -4.2016644 -4.19995 -4.2193694 -4.2248874 -4.2190967 -4.2159271][-4.1897159 -4.1863275 -4.2057672 -4.2320175 -4.2432404 -4.2174296 -4.1683397 -4.1490541 -4.1797204 -4.1991959 -4.1983814 -4.2107821 -4.2153425 -4.2118468 -4.2076082][-4.185123 -4.185123 -4.2000051 -4.222559 -4.2362461 -4.20365 -4.1573205 -4.1481738 -4.1771288 -4.1913438 -4.1886711 -4.1956005 -4.1993856 -4.1981616 -4.1901121][-4.1694989 -4.165843 -4.1760178 -4.1908245 -4.2083759 -4.17268 -4.1301432 -4.1274428 -4.1549215 -4.1671486 -4.1614103 -4.1690359 -4.1807442 -4.1825666 -4.1702843]]...]
INFO - root - 2017-12-07 22:16:54.566367: step 60310, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 50h:11m:28s remains)
INFO - root - 2017-12-07 22:17:01.259114: step 60320, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.659 sec/batch; 49h:47m:28s remains)
INFO - root - 2017-12-07 22:17:07.983473: step 60330, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 50h:09m:45s remains)
INFO - root - 2017-12-07 22:17:14.687158: step 60340, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 52h:28m:22s remains)
INFO - root - 2017-12-07 22:17:21.446935: step 60350, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 53h:05m:58s remains)
INFO - root - 2017-12-07 22:17:28.247072: step 60360, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 52h:01m:28s remains)
INFO - root - 2017-12-07 22:17:35.012845: step 60370, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:47m:04s remains)
INFO - root - 2017-12-07 22:17:41.775566: step 60380, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 49h:05m:55s remains)
INFO - root - 2017-12-07 22:17:48.691657: step 60390, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 53h:41m:07s remains)
INFO - root - 2017-12-07 22:17:55.450285: step 60400, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 52h:27m:38s remains)
2017-12-07 22:17:56.201614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1999121 -4.1806355 -4.1832547 -4.2003098 -4.2216406 -4.2302027 -4.220612 -4.2036452 -4.1793923 -4.1564813 -4.1411543 -4.1457982 -4.1648407 -4.1643414 -4.1749353][-4.235796 -4.2199841 -4.2188892 -4.2318506 -4.2469034 -4.2516541 -4.238647 -4.2117834 -4.1828365 -4.1683135 -4.1651635 -4.170783 -4.1811419 -4.17606 -4.1910429][-4.2377486 -4.2249079 -4.2237186 -4.2318048 -4.2409811 -4.2410445 -4.226223 -4.1989775 -4.174592 -4.1740966 -4.1890178 -4.2026558 -4.2066088 -4.195869 -4.2093005][-4.2078662 -4.193625 -4.1944208 -4.1989794 -4.2011251 -4.192997 -4.1750417 -4.1533489 -4.1435752 -4.1617341 -4.1943994 -4.2189956 -4.2205625 -4.2071528 -4.2146506][-4.1762171 -4.1587739 -4.1540723 -4.1504817 -4.1386952 -4.1188469 -4.09433 -4.0745521 -4.0875082 -4.1362691 -4.1841125 -4.2188396 -4.22602 -4.2141066 -4.2131519][-4.1643648 -4.1418304 -4.1260738 -4.1073194 -4.074594 -4.0362067 -3.9975047 -3.9666612 -4.0038061 -4.0948429 -4.1634278 -4.2089772 -4.2237024 -4.2132378 -4.205904][-4.1626973 -4.13814 -4.1142745 -4.0826306 -4.0346713 -3.9822974 -3.9199996 -3.8585625 -3.912219 -4.0493183 -4.1435957 -4.1999693 -4.2203207 -4.2090049 -4.1978674][-4.147861 -4.1182232 -4.0968614 -4.0683227 -4.0245013 -3.9762015 -3.9131207 -3.8497992 -3.9024606 -4.0449238 -4.1404181 -4.19745 -4.2189136 -4.2045975 -4.1890025][-4.1245847 -4.0817981 -4.063148 -4.0587468 -4.0404387 -4.0119481 -3.9761982 -3.9509189 -3.9939451 -4.0904317 -4.1603284 -4.203855 -4.2178483 -4.199378 -4.1786571][-4.1116843 -4.0638537 -4.0540414 -4.0743327 -4.0835505 -4.0686812 -4.0531912 -4.05469 -4.0859547 -4.1397657 -4.1823821 -4.2105317 -4.2186222 -4.1973863 -4.1749892][-4.118176 -4.078589 -4.0813942 -4.1132479 -4.1324091 -4.1272683 -4.1234236 -4.1392488 -4.1576738 -4.1802564 -4.1996112 -4.2165618 -4.2219443 -4.2025194 -4.18243][-4.1334286 -4.1038575 -4.1178503 -4.1499677 -4.1704879 -4.1722054 -4.1748896 -4.1978641 -4.2093334 -4.210731 -4.2105508 -4.2144117 -4.217061 -4.2035046 -4.1856117][-4.1548309 -4.1273322 -4.1420155 -4.1709571 -4.1919327 -4.1977353 -4.2060256 -4.2308207 -4.2387533 -4.2277155 -4.211678 -4.1981554 -4.195436 -4.19003 -4.1823606][-4.1828485 -4.1505337 -4.1567259 -4.1761293 -4.1942086 -4.2052855 -4.2181792 -4.2389855 -4.2416606 -4.2234559 -4.1969337 -4.1722951 -4.1656585 -4.16967 -4.1796269][-4.2077727 -4.1681013 -4.1595626 -4.1674323 -4.1847196 -4.2035742 -4.2213831 -4.2375145 -4.2301192 -4.2012229 -4.1697288 -4.1474953 -4.1467295 -4.1633487 -4.1913166]]...]
INFO - root - 2017-12-07 22:18:02.523323: step 60410, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 52h:42m:48s remains)
INFO - root - 2017-12-07 22:18:09.429963: step 60420, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 55h:49m:37s remains)
INFO - root - 2017-12-07 22:18:16.355844: step 60430, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:31m:20s remains)
INFO - root - 2017-12-07 22:18:23.227086: step 60440, loss = 2.11, batch loss = 2.05 (12.5 examples/sec; 0.639 sec/batch; 48h:18m:21s remains)
INFO - root - 2017-12-07 22:18:30.066516: step 60450, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 56h:24m:23s remains)
INFO - root - 2017-12-07 22:18:36.933113: step 60460, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.721 sec/batch; 54h:26m:52s remains)
INFO - root - 2017-12-07 22:18:43.761114: step 60470, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 50h:41m:06s remains)
INFO - root - 2017-12-07 22:18:50.518123: step 60480, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 48h:17m:09s remains)
INFO - root - 2017-12-07 22:18:57.397630: step 60490, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 50h:02m:52s remains)
INFO - root - 2017-12-07 22:19:04.277655: step 60500, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 53h:32m:00s remains)
2017-12-07 22:19:05.006050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2937484 -4.2708464 -4.2423973 -4.2179217 -4.1985 -4.1834407 -4.1530771 -4.1158681 -4.0890775 -4.0687013 -4.0461931 -4.0386353 -4.0429125 -4.0608478 -4.0804391][-4.2952485 -4.2742944 -4.2472992 -4.2218056 -4.1998692 -4.1806564 -4.145227 -4.1126733 -4.1012659 -4.088109 -4.0664549 -4.0576773 -4.0654278 -4.0824986 -4.0982184][-4.29766 -4.27901 -4.2540717 -4.2275615 -4.2005892 -4.1757207 -4.1389408 -4.1139517 -4.1146231 -4.1120558 -4.0898871 -4.0733075 -4.0754957 -4.0785604 -4.0884862][-4.3003273 -4.2817106 -4.2574172 -4.2281451 -4.1983523 -4.1735582 -4.1403704 -4.1176629 -4.1239514 -4.1339273 -4.119844 -4.1020069 -4.1009932 -4.0894785 -4.0888138][-4.3034997 -4.2847981 -4.2597003 -4.2264652 -4.1949921 -4.1693759 -4.1378555 -4.115941 -4.1246991 -4.1435623 -4.1447 -4.1413622 -4.14459 -4.1276288 -4.1197958][-4.3063984 -4.2881131 -4.2631788 -4.2293386 -4.1956897 -4.1641908 -4.1268835 -4.1051035 -4.1154761 -4.1379819 -4.1545067 -4.1653094 -4.1808844 -4.1738057 -4.1710787][-4.308877 -4.290853 -4.2679143 -4.2371273 -4.2049732 -4.1712952 -4.1281881 -4.1011047 -4.1051745 -4.1254873 -4.1488814 -4.173315 -4.2081661 -4.2218642 -4.2250657][-4.3098946 -4.2919536 -4.2704864 -4.2445745 -4.216002 -4.1856313 -4.1424189 -4.1098318 -4.1048274 -4.1175256 -4.1405244 -4.1749153 -4.2182159 -4.24485 -4.2546587][-4.3105779 -4.2917895 -4.2698169 -4.2459011 -4.2170196 -4.1892486 -4.1499782 -4.1202745 -4.1137695 -4.1199222 -4.13741 -4.1700735 -4.21209 -4.2437167 -4.2576671][-4.3105125 -4.2903256 -4.264636 -4.2367034 -4.2047687 -4.178401 -4.1444783 -4.1227727 -4.1176872 -4.1221871 -4.1364417 -4.1623926 -4.1957455 -4.2253537 -4.2426991][-4.3095269 -4.2876468 -4.2590814 -4.2257266 -4.1868663 -4.1577444 -4.1289592 -4.1140594 -4.112813 -4.117908 -4.1277671 -4.1501665 -4.1789894 -4.2019587 -4.2133303][-4.3080525 -4.2845745 -4.2559662 -4.2221694 -4.17992 -4.1460042 -4.1173396 -4.0993419 -4.0939116 -4.0940762 -4.1013074 -4.1232872 -4.15623 -4.1781907 -4.1824083][-4.3072629 -4.2843728 -4.2578521 -4.2274218 -4.1885538 -4.155704 -4.1220489 -4.0933728 -4.0754018 -4.0691171 -4.0775366 -4.1057935 -4.14503 -4.1669931 -4.1708441][-4.3063622 -4.2847929 -4.2610278 -4.2348084 -4.2003675 -4.1686325 -4.1297603 -4.0950818 -4.0760674 -4.0727811 -4.0848842 -4.1166048 -4.1582685 -4.1794419 -4.1845078][-4.3049417 -4.2824736 -4.2589817 -4.2357817 -4.2054482 -4.1768312 -4.1365128 -4.1066289 -4.1012745 -4.1082606 -4.1248379 -4.15519 -4.1927738 -4.20806 -4.2121463]]...]
INFO - root - 2017-12-07 22:19:11.642232: step 60510, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.666 sec/batch; 50h:20m:32s remains)
INFO - root - 2017-12-07 22:19:18.498646: step 60520, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 51h:03m:42s remains)
INFO - root - 2017-12-07 22:19:25.401912: step 60530, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 53h:14m:58s remains)
INFO - root - 2017-12-07 22:19:32.137718: step 60540, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 54h:14m:00s remains)
INFO - root - 2017-12-07 22:19:38.984773: step 60550, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 52h:35m:17s remains)
INFO - root - 2017-12-07 22:19:45.653166: step 60560, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 49h:07m:10s remains)
INFO - root - 2017-12-07 22:19:52.292002: step 60570, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 48h:13m:20s remains)
INFO - root - 2017-12-07 22:19:59.066179: step 60580, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 52h:37m:13s remains)
INFO - root - 2017-12-07 22:20:05.867212: step 60590, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 54h:16m:32s remains)
INFO - root - 2017-12-07 22:20:12.598109: step 60600, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 52h:37m:32s remains)
2017-12-07 22:20:13.292226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3124433 -4.3137784 -4.3181105 -4.3238025 -4.3255768 -4.319406 -4.3076544 -4.2918186 -4.2736411 -4.2676144 -4.2693191 -4.269031 -4.2578936 -4.2381768 -4.2235527][-4.3046536 -4.3024964 -4.3053889 -4.3106852 -4.3120265 -4.3059535 -4.2921848 -4.2729678 -4.2532005 -4.2478447 -4.2518067 -4.2558875 -4.2483268 -4.2310228 -4.2161736][-4.2860508 -4.2798209 -4.2840919 -4.2952023 -4.2986541 -4.2912579 -4.2711062 -4.2454987 -4.2249565 -4.2229009 -4.2292762 -4.2323074 -4.2232661 -4.2045169 -4.1864743][-4.2456927 -4.2359319 -4.24584 -4.2683206 -4.2768173 -4.2662525 -4.2342062 -4.1962876 -4.175139 -4.1806579 -4.1929536 -4.1967916 -4.1870446 -4.1692543 -4.1513691][-4.1858873 -4.1716051 -4.1879907 -4.227592 -4.2441344 -4.2262387 -4.179852 -4.1273141 -4.1063323 -4.1212215 -4.1454377 -4.1558032 -4.1479845 -4.1346397 -4.1258106][-4.1209068 -4.0989094 -4.1197782 -4.1728692 -4.1928267 -4.1676073 -4.1123362 -4.0518532 -4.0395918 -4.0703039 -4.1087813 -4.1263518 -4.1187563 -4.1059461 -4.1061854][-4.0835795 -4.0563211 -4.071702 -4.1196055 -4.1289978 -4.088377 -4.0187893 -3.9534898 -3.9610963 -4.015903 -4.0676613 -4.0918112 -4.0862193 -4.0718942 -4.0753345][-4.1090651 -4.0833244 -4.0807128 -4.0992832 -4.0821152 -4.0121322 -3.91131 -3.8332076 -3.8637049 -3.9430821 -4.0068512 -4.0388966 -4.0407248 -4.0290494 -4.03595][-4.1599054 -4.1416073 -4.1261063 -4.1180348 -4.0785532 -3.9836633 -3.860378 -3.7770367 -3.8216267 -3.9095757 -3.9723952 -4.0069814 -4.0141792 -4.0087004 -4.0194798][-4.1933 -4.1926274 -4.1866288 -4.1762071 -4.1365786 -4.0479188 -3.9432206 -3.8801324 -3.9136155 -3.9738312 -4.0168529 -4.0433807 -4.047955 -4.0430679 -4.0521169][-4.2046046 -4.2196465 -4.2304144 -4.2320766 -4.2051992 -4.1429777 -4.0735607 -4.0354848 -4.0535917 -4.0838137 -4.1053796 -4.1228571 -4.125524 -4.1196432 -4.1227489][-4.2058859 -4.222374 -4.2387772 -4.250174 -4.2424245 -4.2134218 -4.1754928 -4.1554494 -4.1624641 -4.1732545 -4.1817846 -4.1953478 -4.1993494 -4.1949797 -4.1940789][-4.2011514 -4.2112327 -4.22337 -4.2377687 -4.248518 -4.2470169 -4.2336445 -4.2238793 -4.2227116 -4.22376 -4.2270679 -4.2386603 -4.2450747 -4.24433 -4.2427239][-4.1805363 -4.1876278 -4.1954603 -4.210495 -4.2307343 -4.2477059 -4.2538567 -4.25399 -4.2527323 -4.2532191 -4.2572064 -4.2675037 -4.273809 -4.273077 -4.2694092][-4.1456251 -4.1544394 -4.159133 -4.1733527 -4.1966314 -4.2227955 -4.2444644 -4.2584953 -4.2655978 -4.272367 -4.2811284 -4.2911034 -4.29604 -4.2937684 -4.2862449]]...]
INFO - root - 2017-12-07 22:20:19.802819: step 60610, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 51h:01m:10s remains)
INFO - root - 2017-12-07 22:20:26.671030: step 60620, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.718 sec/batch; 54h:14m:25s remains)
INFO - root - 2017-12-07 22:20:33.557632: step 60630, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 50h:16m:14s remains)
INFO - root - 2017-12-07 22:20:40.316122: step 60640, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.637 sec/batch; 48h:04m:20s remains)
INFO - root - 2017-12-07 22:20:47.277759: step 60650, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 50h:02m:03s remains)
INFO - root - 2017-12-07 22:20:54.103949: step 60660, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 52h:20m:37s remains)
INFO - root - 2017-12-07 22:21:00.943799: step 60670, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 54h:54m:06s remains)
INFO - root - 2017-12-07 22:21:07.760226: step 60680, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 51h:46m:19s remains)
INFO - root - 2017-12-07 22:21:14.555203: step 60690, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 50h:04m:04s remains)
INFO - root - 2017-12-07 22:21:21.305165: step 60700, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.631 sec/batch; 47h:37m:57s remains)
2017-12-07 22:21:22.065562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3099217 -4.3098965 -4.3109822 -4.31137 -4.3122287 -4.3128037 -4.312521 -4.3118215 -4.311954 -4.3132305 -4.3146677 -4.314507 -4.3130236 -4.3113852 -4.309783][-4.2857676 -4.2862649 -4.2889128 -4.2903 -4.2917318 -4.2920694 -4.2912269 -4.28988 -4.2899303 -4.2923188 -4.2946663 -4.2941 -4.2919879 -4.2901855 -4.288425][-4.2381635 -4.239996 -4.2449794 -4.24762 -4.2496347 -4.24938 -4.2469778 -4.2437778 -4.2432923 -4.2468758 -4.2501478 -4.2493339 -4.2465153 -4.24421 -4.2426395][-4.1789479 -4.1821446 -4.1888428 -4.192193 -4.1937709 -4.192379 -4.1882319 -4.1829557 -4.1819487 -4.1863065 -4.1900945 -4.1890988 -4.1859474 -4.1831846 -4.182858][-4.1328845 -4.1396246 -4.1461015 -4.148921 -4.1480865 -4.1444612 -4.1381874 -4.1315532 -4.1304588 -4.1331968 -4.1351085 -4.13231 -4.1290565 -4.1277871 -4.1315727][-4.1160994 -4.1269517 -4.1310468 -4.1302762 -4.1248045 -4.117857 -4.1098 -4.1031637 -4.1018567 -4.1017013 -4.1000075 -4.0969329 -4.0982337 -4.1037316 -4.1150427][-4.1210132 -4.13305 -4.1312766 -4.1238742 -4.1133947 -4.1046519 -4.1000819 -4.0987434 -4.100256 -4.0989413 -4.0953784 -4.0952663 -4.1055379 -4.1204805 -4.13872][-4.143805 -4.1529431 -4.1434131 -4.1291146 -4.1164341 -4.1100445 -4.1130872 -4.1217375 -4.130424 -4.13204 -4.1312647 -4.1370115 -4.1541805 -4.1733122 -4.1923161][-4.1745043 -4.1800647 -4.1661825 -4.1490574 -4.1350503 -4.1314082 -4.1417904 -4.1596465 -4.1757941 -4.1823692 -4.1861219 -4.1958222 -4.2137189 -4.2309151 -4.2455449][-4.1996107 -4.2053709 -4.1926804 -4.1756148 -4.1599579 -4.1564875 -4.1694822 -4.1920261 -4.2138667 -4.224946 -4.2316489 -4.2415996 -4.2561278 -4.26909 -4.2793412][-4.2204494 -4.2292514 -4.2182994 -4.1986432 -4.1792474 -4.1741438 -4.1876092 -4.2123485 -4.2375417 -4.2519846 -4.2600083 -4.26883 -4.2798157 -4.2899122 -4.2980576][-4.231782 -4.2425127 -4.2320495 -4.2095079 -4.1895103 -4.1856003 -4.2009377 -4.2263613 -4.2516365 -4.26682 -4.2751656 -4.2820983 -4.2894554 -4.2969046 -4.3035679][-4.2389846 -4.2462249 -4.2324114 -4.2086129 -4.1926956 -4.1955304 -4.2154026 -4.2400217 -4.2620244 -4.275085 -4.28223 -4.2870731 -4.2916455 -4.2970562 -4.3028655][-4.2445173 -4.2427421 -4.2236438 -4.2022519 -4.1953063 -4.2081242 -4.2325754 -4.255661 -4.273118 -4.2831511 -4.2881351 -4.2909107 -4.2930388 -4.2965617 -4.3012834][-4.2426114 -4.2328396 -4.2152019 -4.2032871 -4.2063446 -4.2251692 -4.2506108 -4.2702556 -4.2830157 -4.2901773 -4.2933154 -4.2948747 -4.2953863 -4.2971449 -4.3000851]]...]
INFO - root - 2017-12-07 22:21:28.684301: step 60710, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 54h:24m:34s remains)
INFO - root - 2017-12-07 22:21:35.363560: step 60720, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 50h:07m:20s remains)
INFO - root - 2017-12-07 22:21:42.145495: step 60730, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 50h:17m:27s remains)
INFO - root - 2017-12-07 22:21:48.955868: step 60740, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 54h:17m:14s remains)
INFO - root - 2017-12-07 22:21:55.795240: step 60750, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.677 sec/batch; 51h:08m:15s remains)
INFO - root - 2017-12-07 22:22:02.560740: step 60760, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 50h:30m:46s remains)
INFO - root - 2017-12-07 22:22:09.227687: step 60770, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 49h:15m:28s remains)
INFO - root - 2017-12-07 22:22:15.969147: step 60780, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 50h:00m:28s remains)
INFO - root - 2017-12-07 22:22:22.696735: step 60790, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 50h:46m:59s remains)
INFO - root - 2017-12-07 22:22:29.467255: step 60800, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 52h:15m:27s remains)
2017-12-07 22:22:30.231250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26668 -4.278161 -4.2759371 -4.268364 -4.2627087 -4.2636418 -4.2632356 -4.262404 -4.2721629 -4.2882614 -4.3004322 -4.3011971 -4.2937393 -4.2865272 -4.2844253][-4.2268338 -4.2439632 -4.2431211 -4.2377591 -4.2348385 -4.2407389 -4.2426438 -4.2370543 -4.2436275 -4.2619457 -4.2767849 -4.2783875 -4.2713642 -4.2631664 -4.2586174][-4.1868615 -4.2081795 -4.214787 -4.2177105 -4.218904 -4.2262049 -4.223691 -4.2119789 -4.2172604 -4.238132 -4.254612 -4.2551169 -4.2473426 -4.2392063 -4.2308936][-4.1589341 -4.1790328 -4.1922626 -4.2015686 -4.2041044 -4.2081065 -4.1986747 -4.1842995 -4.1946726 -4.2202387 -4.2353525 -4.2349405 -4.2270303 -4.2210321 -4.2081003][-4.1573639 -4.1689038 -4.1755295 -4.1813545 -4.17736 -4.173542 -4.1576452 -4.1434975 -4.1670828 -4.2047973 -4.2229953 -4.22279 -4.2121339 -4.2050071 -4.18809][-4.1596727 -4.162828 -4.1605988 -4.1499829 -4.1222949 -4.0963049 -4.0603614 -4.0398312 -4.0914435 -4.1597276 -4.1939716 -4.1999307 -4.1895514 -4.1816072 -4.1621008][-4.1479688 -4.1483188 -4.1401672 -4.1081195 -4.0442739 -3.9754553 -3.8875084 -3.8393645 -3.9383597 -4.06758 -4.1414309 -4.1642594 -4.154572 -4.1454611 -4.127986][-4.1506362 -4.149838 -4.1355824 -4.0935645 -4.003377 -3.8813157 -3.7214036 -3.6266537 -3.7745697 -3.9750121 -4.0973625 -4.1432648 -4.1358705 -4.1242137 -4.106513][-4.1833663 -4.1850986 -4.17101 -4.1351662 -4.0534453 -3.9346247 -3.7754254 -3.6768403 -3.803555 -3.9914651 -4.1123824 -4.1629519 -4.15309 -4.1363573 -4.1137075][-4.2214184 -4.2274442 -4.2159629 -4.1905875 -4.1325278 -4.0508051 -3.9418645 -3.8721986 -3.9484286 -4.0740323 -4.1666417 -4.2112818 -4.1999106 -4.1777658 -4.15018][-4.2447491 -4.2488337 -4.2426729 -4.2280564 -4.1917562 -4.1421885 -4.074985 -4.0328012 -4.0803766 -4.1625214 -4.2266617 -4.2620807 -4.2532287 -4.2285957 -4.1987042][-4.2564497 -4.2594628 -4.2568045 -4.25117 -4.2335415 -4.2049356 -4.163868 -4.1363611 -4.1655154 -4.2197666 -4.2671547 -4.2968168 -4.2927308 -4.2704711 -4.2414317][-4.2730594 -4.2787986 -4.2775745 -4.2740507 -4.26766 -4.2495089 -4.2203245 -4.2037539 -4.2230415 -4.2611556 -4.2960868 -4.3207345 -4.3225303 -4.3045382 -4.2754159][-4.2838874 -4.29076 -4.2914009 -4.288137 -4.2866569 -4.2784986 -4.2613764 -4.2536693 -4.264534 -4.2874379 -4.3115058 -4.3290043 -4.3344855 -4.3219018 -4.2981229][-4.2817988 -4.2863927 -4.2873559 -4.2876763 -4.29009 -4.2881241 -4.281755 -4.2791529 -4.2835746 -4.2949104 -4.3078294 -4.3187571 -4.3243289 -4.3188519 -4.3057814]]...]
INFO - root - 2017-12-07 22:22:36.861313: step 60810, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.735 sec/batch; 55h:25m:59s remains)
INFO - root - 2017-12-07 22:22:43.700801: step 60820, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.738 sec/batch; 55h:42m:56s remains)
INFO - root - 2017-12-07 22:22:50.495349: step 60830, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 50h:50m:26s remains)
INFO - root - 2017-12-07 22:22:57.244869: step 60840, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 48h:10m:06s remains)
INFO - root - 2017-12-07 22:23:04.018057: step 60850, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 48h:43m:55s remains)
INFO - root - 2017-12-07 22:23:10.844801: step 60860, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 54h:06m:11s remains)
INFO - root - 2017-12-07 22:23:17.604150: step 60870, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.737 sec/batch; 55h:37m:17s remains)
INFO - root - 2017-12-07 22:23:24.411145: step 60880, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 53h:35m:29s remains)
INFO - root - 2017-12-07 22:23:31.104287: step 60890, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 47h:05m:38s remains)
INFO - root - 2017-12-07 22:23:37.961404: step 60900, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 52h:36m:19s remains)
2017-12-07 22:23:38.799652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1983776 -4.2256188 -4.2508569 -4.2757788 -4.2972941 -4.3058867 -4.2991586 -4.2839952 -4.2727633 -4.2621713 -4.2558813 -4.2535529 -4.2521191 -4.2458611 -4.2372417][-4.2101264 -4.2328062 -4.2504382 -4.2656717 -4.2735314 -4.2679343 -4.2488904 -4.2278152 -4.2228136 -4.2271266 -4.234446 -4.2383108 -4.2361765 -4.2260923 -4.2195621][-4.2390275 -4.2541823 -4.2576084 -4.2507606 -4.2353783 -4.2030821 -4.1604581 -4.1300325 -4.13797 -4.1693487 -4.1979346 -4.2134819 -4.2078238 -4.1849432 -4.17157][-4.276938 -4.2844205 -4.27049 -4.2372627 -4.1917844 -4.126545 -4.0486479 -4.003499 -4.0312743 -4.1003389 -4.1602745 -4.1953082 -4.1899981 -4.1535187 -4.1272678][-4.3039937 -4.3032184 -4.2735944 -4.2202773 -4.1473408 -4.0432167 -3.9210052 -3.8567233 -3.9115334 -4.0252886 -4.1232481 -4.1837463 -4.190589 -4.1542358 -4.1243949][-4.31238 -4.3053641 -4.2671647 -4.2024689 -4.1100416 -3.9611247 -3.7821765 -3.6992071 -3.7953227 -3.964138 -4.0974789 -4.1817441 -4.20591 -4.1786361 -4.1532841][-4.3057218 -4.2981243 -4.261 -4.1918907 -4.0838633 -3.8979187 -3.6741936 -3.574652 -3.7020659 -3.9137974 -4.074482 -4.1730447 -4.2098789 -4.1907873 -4.1692286][-4.2956829 -4.2889509 -4.2607431 -4.19922 -4.0981812 -3.9205983 -3.7167974 -3.6287925 -3.7503786 -3.9447694 -4.0875425 -4.17461 -4.20822 -4.1882515 -4.1672244][-4.2878571 -4.2875414 -4.2694316 -4.2186151 -4.1382017 -4.004602 -3.8679242 -3.8173332 -3.9094963 -4.0435777 -4.137238 -4.195034 -4.2149763 -4.1906457 -4.1647043][-4.2823291 -4.2888627 -4.2810993 -4.2432656 -4.1864834 -4.1006188 -4.0248775 -4.000711 -4.0625982 -4.13903 -4.1892691 -4.2189016 -4.2220511 -4.1927156 -4.1663556][-4.2704277 -4.2807164 -4.2816005 -4.2582393 -4.227303 -4.1820488 -4.1422949 -4.1280785 -4.1633673 -4.19999 -4.2218595 -4.229002 -4.2201667 -4.1902409 -4.1684108][-4.2475095 -4.2572155 -4.2593651 -4.2473035 -4.2389646 -4.2255759 -4.2103419 -4.1999297 -4.2134514 -4.2260447 -4.233068 -4.2237334 -4.205533 -4.1776619 -4.1629767][-4.2293434 -4.2343974 -4.2337942 -4.2301044 -4.2358403 -4.2410169 -4.2415385 -4.2383213 -4.2398467 -4.2382884 -4.23387 -4.21891 -4.1998639 -4.17956 -4.17069][-4.2370744 -4.2400804 -4.239563 -4.2408662 -4.248033 -4.2555842 -4.2594781 -4.2603145 -4.2591891 -4.252769 -4.245151 -4.23322 -4.2204356 -4.2081785 -4.2019558][-4.2644181 -4.2637911 -4.2616072 -4.2622581 -4.2660017 -4.26842 -4.269969 -4.2733135 -4.27549 -4.2731261 -4.2687941 -4.264019 -4.2579069 -4.2509451 -4.2454567]]...]
INFO - root - 2017-12-07 22:23:45.346130: step 60910, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 50h:23m:48s remains)
INFO - root - 2017-12-07 22:23:52.049986: step 60920, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 48h:50m:01s remains)
INFO - root - 2017-12-07 22:23:58.779507: step 60930, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 49h:02m:36s remains)
INFO - root - 2017-12-07 22:24:05.597445: step 60940, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 51h:00m:17s remains)
INFO - root - 2017-12-07 22:24:12.405018: step 60950, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 52h:58m:46s remains)
INFO - root - 2017-12-07 22:24:19.088869: step 60960, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 49h:42m:53s remains)
INFO - root - 2017-12-07 22:24:25.805347: step 60970, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 48h:38m:51s remains)
INFO - root - 2017-12-07 22:24:32.574529: step 60980, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.669 sec/batch; 50h:29m:12s remains)
INFO - root - 2017-12-07 22:24:39.419556: step 60990, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 53h:21m:52s remains)
INFO - root - 2017-12-07 22:24:46.176867: step 61000, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 53h:52m:06s remains)
2017-12-07 22:24:46.982775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2426186 -4.2645826 -4.2923303 -4.3048067 -4.2982159 -4.2726278 -4.2272644 -4.1756768 -4.1313944 -4.1074319 -4.114058 -4.1261597 -4.1320496 -4.1434207 -4.1674142][-4.2364292 -4.2572351 -4.2838354 -4.2911291 -4.2769527 -4.2422409 -4.18713 -4.1232438 -4.0788832 -4.0629454 -4.0796895 -4.1039629 -4.1118145 -4.1220555 -4.1421695][-4.1991429 -4.2170334 -4.2415128 -4.2482967 -4.2351031 -4.2073 -4.15841 -4.0963707 -4.0554504 -4.04295 -4.0566535 -4.0797229 -4.0860181 -4.0925231 -4.1086807][-4.1406674 -4.1565938 -4.1799493 -4.1888366 -4.1862988 -4.1735525 -4.141983 -4.091701 -4.0552325 -4.0441227 -4.0506668 -4.0645428 -4.0640783 -4.0616622 -4.0683956][-4.0961046 -4.1137395 -4.1369262 -4.1471648 -4.14473 -4.136981 -4.1122565 -4.0722609 -4.0440164 -4.0405498 -4.0466866 -4.0567465 -4.0534716 -4.0395775 -4.0284839][-4.0648155 -4.0849738 -4.1124883 -4.1178021 -4.1018667 -4.0890837 -4.0686016 -4.0395241 -4.02598 -4.0340695 -4.0440392 -4.0555463 -4.0536904 -4.0294876 -4.0013576][-4.0278072 -4.052989 -4.0873489 -4.0878229 -4.0526891 -4.0250869 -4.00355 -3.9871371 -3.9942033 -4.017457 -4.0339222 -4.0484104 -4.0485797 -4.0171695 -3.9881418][-3.9744587 -4.0124393 -4.0558014 -4.0625081 -4.0194168 -3.9717524 -3.9361753 -3.9221845 -3.9477339 -3.9913559 -4.0182462 -4.036293 -4.0383039 -4.01004 -3.9899621][-3.993221 -4.0280967 -4.0681119 -4.0831 -4.0434093 -3.9845736 -3.9338369 -3.9054356 -3.9184051 -3.9607358 -3.9871447 -3.999567 -3.9994648 -3.9796708 -3.9776454][-4.0454764 -4.0687804 -4.1011686 -4.1182532 -4.0916815 -4.0426927 -3.9951634 -3.9625771 -3.9616859 -3.9921937 -4.0099225 -4.010901 -4.00632 -3.9921758 -3.9944098][-4.0971417 -4.1105461 -4.1332808 -4.1499152 -4.133924 -4.0984669 -4.0666432 -4.049036 -4.0496526 -4.0790272 -4.0945683 -4.0916719 -4.0859771 -4.0777488 -4.0773721][-4.1271715 -4.140656 -4.1570354 -4.1721458 -4.1673985 -4.1437707 -4.1217685 -4.1106396 -4.1140394 -4.1386328 -4.1512818 -4.1537757 -4.1583333 -4.1587653 -4.1595383][-4.1239743 -4.1342125 -4.1483297 -4.1649489 -4.1745472 -4.1654782 -4.1485147 -4.1389174 -4.1435523 -4.1629348 -4.1725955 -4.1855631 -4.2032342 -4.2137322 -4.2205472][-4.1009011 -4.104599 -4.1159639 -4.1350527 -4.1515322 -4.1526618 -4.1402721 -4.1297989 -4.1308341 -4.1380954 -4.1423712 -4.1631312 -4.1952744 -4.2187891 -4.2328591][-4.0822287 -4.0822763 -4.09259 -4.113452 -4.1342454 -4.1429591 -4.1358986 -4.1202149 -4.1067343 -4.0972643 -4.09313 -4.1168356 -4.1581731 -4.1898375 -4.2070508]]...]
INFO - root - 2017-12-07 22:24:53.558481: step 61010, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 51h:29m:33s remains)
INFO - root - 2017-12-07 22:25:00.420791: step 61020, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 53h:31m:24s remains)
INFO - root - 2017-12-07 22:25:06.977522: step 61030, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 50h:25m:28s remains)
INFO - root - 2017-12-07 22:25:13.773572: step 61040, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:39m:21s remains)
INFO - root - 2017-12-07 22:25:20.560877: step 61050, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 50h:19m:58s remains)
INFO - root - 2017-12-07 22:25:27.462089: step 61060, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 53h:13m:50s remains)
INFO - root - 2017-12-07 22:25:34.262827: step 61070, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 52h:32m:20s remains)
INFO - root - 2017-12-07 22:25:41.103307: step 61080, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 52h:32m:55s remains)
INFO - root - 2017-12-07 22:25:47.801671: step 61090, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 50h:45m:06s remains)
INFO - root - 2017-12-07 22:25:54.651262: step 61100, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 49h:43m:03s remains)
2017-12-07 22:25:55.457857: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3220854 -4.3207746 -4.3179421 -4.3135996 -4.31 -4.30867 -4.3089728 -4.3101258 -4.3104515 -4.3096433 -4.3086505 -4.30654 -4.3012314 -4.2925148 -4.2798033][-4.3248978 -4.3218722 -4.316525 -4.3097639 -4.3045735 -4.3038363 -4.3063383 -4.3102994 -4.3129764 -4.3132253 -4.3125114 -4.3105707 -4.3058681 -4.298028 -4.2860374][-4.3202286 -4.3146091 -4.3041558 -4.2926068 -4.2839608 -4.2829847 -4.2900009 -4.3009081 -4.3105145 -4.3154788 -4.3166652 -4.3153324 -4.312079 -4.3067417 -4.297914][-4.3092546 -4.2979403 -4.2787223 -4.2580605 -4.2422237 -4.2401929 -4.2544417 -4.2754374 -4.2962985 -4.3109908 -4.3173985 -4.3184953 -4.3171453 -4.3147573 -4.309659][-4.2882371 -4.2676048 -4.233696 -4.1979809 -4.1730003 -4.170156 -4.1912427 -4.2236276 -4.2602553 -4.2892275 -4.3046436 -4.3107352 -4.3138165 -4.3155074 -4.3133941][-4.2509589 -4.2224841 -4.1755481 -4.1256962 -4.0945897 -4.0899467 -4.1136208 -4.1503372 -4.1953464 -4.236568 -4.2621984 -4.2761221 -4.2872729 -4.2950659 -4.2960305][-4.199914 -4.1708841 -4.1188259 -4.0602355 -4.0221224 -4.0106487 -4.0302753 -4.0644422 -4.1110268 -4.1587963 -4.1922803 -4.2155533 -4.2364078 -4.2492828 -4.2502203][-4.1471443 -4.127974 -4.0808768 -4.0213623 -3.9767914 -3.953866 -3.9600053 -3.985136 -4.0286994 -4.0772147 -4.115088 -4.144125 -4.1673193 -4.1764464 -4.1697497][-4.1220775 -4.1191092 -4.0861421 -4.0371828 -3.9912579 -3.9528477 -3.936625 -3.9467068 -3.9814136 -4.0264707 -4.0660467 -4.0953431 -4.1110163 -4.1037579 -4.07937][-4.1398797 -4.1515303 -4.1374803 -4.1037059 -4.0600119 -4.0148253 -3.9868472 -3.9894357 -4.0153432 -4.0506926 -4.0829821 -4.1061096 -4.1111741 -4.0866013 -4.0489607][-4.1998668 -4.213769 -4.2112818 -4.1923985 -4.1619287 -4.1283813 -4.1082373 -4.1108012 -4.1270056 -4.1471858 -4.1651306 -4.1765761 -4.1720519 -4.1412244 -4.1025743][-4.2657709 -4.2774196 -4.2791486 -4.2700515 -4.2518125 -4.231132 -4.2180409 -4.2186785 -4.2276092 -4.239254 -4.2488408 -4.2536178 -4.2483039 -4.2245822 -4.19658][-4.3077817 -4.3157067 -4.3173141 -4.3111649 -4.2995028 -4.2848024 -4.2732091 -4.2681522 -4.2699442 -4.27775 -4.28791 -4.2946396 -4.2948942 -4.2830887 -4.267797][-4.32725 -4.3311939 -4.330719 -4.3210006 -4.3050022 -4.2853293 -4.2641459 -4.2454343 -4.2397804 -4.251833 -4.2749052 -4.2955308 -4.3072267 -4.3069048 -4.3002024][-4.3312349 -4.3324504 -4.327333 -4.3062668 -4.272768 -4.23443 -4.1928477 -4.1563349 -4.1445475 -4.1671824 -4.2118545 -4.2544584 -4.282568 -4.2942448 -4.2920208]]...]
INFO - root - 2017-12-07 22:26:02.043272: step 61110, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 51h:13m:31s remains)
INFO - root - 2017-12-07 22:26:08.846530: step 61120, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 51h:28m:20s remains)
INFO - root - 2017-12-07 22:26:15.712713: step 61130, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 49h:52m:09s remains)
INFO - root - 2017-12-07 22:26:22.505855: step 61140, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 53h:41m:48s remains)
INFO - root - 2017-12-07 22:26:29.421719: step 61150, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 52h:57m:54s remains)
INFO - root - 2017-12-07 22:26:36.165958: step 61160, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.669 sec/batch; 50h:24m:47s remains)
INFO - root - 2017-12-07 22:26:42.896750: step 61170, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 46h:40m:22s remains)
INFO - root - 2017-12-07 22:26:49.687492: step 61180, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 48h:35m:56s remains)
INFO - root - 2017-12-07 22:26:56.525329: step 61190, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 52h:49m:43s remains)
INFO - root - 2017-12-07 22:27:03.331970: step 61200, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 54h:18m:55s remains)
2017-12-07 22:27:04.037482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.210453 -4.2467694 -4.2679639 -4.2506914 -4.1905131 -4.1154723 -4.0463324 -4.0484328 -4.0924597 -4.145946 -4.202898 -4.2659111 -4.2966208 -4.3029637 -4.3110247][-4.1860838 -4.2194996 -4.2371345 -4.2105246 -4.1402054 -4.05358 -3.9734964 -3.9827709 -4.0398779 -4.1015072 -4.1758976 -4.2554789 -4.2928104 -4.300034 -4.3097034][-4.1636539 -4.1905227 -4.2015505 -4.1682568 -4.0926037 -3.9958205 -3.907793 -3.9274135 -3.9972801 -4.0690331 -4.1574364 -4.2455888 -4.2882137 -4.2975063 -4.3074884][-4.1573906 -4.1757507 -4.1779642 -4.1455469 -4.0752339 -3.9806755 -3.8959985 -3.9266579 -3.9973018 -4.0665617 -4.152638 -4.2384319 -4.2830019 -4.2955785 -4.304821][-4.155653 -4.1692619 -4.1673841 -4.1396389 -4.0774055 -3.9923794 -3.9184361 -3.9548783 -4.0189729 -4.0790792 -4.1571484 -4.2360382 -4.278605 -4.2925148 -4.3023458][-4.1557984 -4.1651645 -4.1622281 -4.1424375 -4.0842824 -3.9988909 -3.9276738 -3.9671705 -4.0234056 -4.0800891 -4.1577559 -4.2347727 -4.276165 -4.2910376 -4.3012838][-4.140769 -4.1503339 -4.14777 -4.128736 -4.065186 -3.972636 -3.8918548 -3.9326425 -3.9869847 -4.0514574 -4.1401744 -4.22464 -4.2701449 -4.2891951 -4.3012476][-4.1202831 -4.1301608 -4.1328173 -4.1194086 -4.0565491 -3.953578 -3.8580484 -3.8837516 -3.9319456 -4.0146465 -4.1211853 -4.2140737 -4.264523 -4.2867918 -4.30083][-4.1166644 -4.1218796 -4.125608 -4.1248951 -4.0740061 -3.9700556 -3.876687 -3.8929961 -3.9302337 -4.0125456 -4.1182556 -4.2107677 -4.2630372 -4.2857008 -4.3002782][-4.1221271 -4.1252894 -4.1330419 -4.1399479 -4.0975986 -4.0006042 -3.9187863 -3.9372003 -3.9679246 -4.0362873 -4.1253228 -4.2106085 -4.2614069 -4.2845721 -4.2993822][-4.1409988 -4.1444221 -4.1514549 -4.1535182 -4.1082931 -4.0145612 -3.9439213 -3.9709735 -3.9989028 -4.0565553 -4.1321058 -4.21076 -4.2600513 -4.2830396 -4.2991395][-4.153964 -4.1629205 -4.1688914 -4.1637435 -4.1033711 -4.0011067 -3.9376192 -3.9797802 -4.01761 -4.0744939 -4.1413965 -4.2155323 -4.2617679 -4.2833152 -4.2995625][-4.168282 -4.1796069 -4.1860461 -4.1751451 -4.101737 -3.9893558 -3.9273884 -3.9727514 -4.0191922 -4.0836124 -4.1535287 -4.22606 -4.2673864 -4.2861514 -4.3004489][-4.1770897 -4.1872606 -4.1905618 -4.1747675 -4.1034989 -3.9954982 -3.939467 -3.9793165 -4.02591 -4.0939779 -4.1651425 -4.2360835 -4.2740097 -4.28972 -4.3017488][-4.1865468 -4.1953545 -4.1947312 -4.1783209 -4.1189489 -4.0272765 -3.9812455 -4.0110636 -4.0498891 -4.1118679 -4.1766186 -4.2429433 -4.2799706 -4.2934384 -4.3029518]]...]
INFO - root - 2017-12-07 22:27:10.597382: step 61210, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 50h:33m:29s remains)
INFO - root - 2017-12-07 22:27:17.338032: step 61220, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 53h:37m:28s remains)
INFO - root - 2017-12-07 22:27:24.150836: step 61230, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 54h:03m:31s remains)
INFO - root - 2017-12-07 22:27:30.867727: step 61240, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 51h:25m:47s remains)
INFO - root - 2017-12-07 22:27:37.697474: step 61250, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 49h:41m:28s remains)
INFO - root - 2017-12-07 22:27:44.402121: step 61260, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 49h:49m:50s remains)
INFO - root - 2017-12-07 22:27:51.234895: step 61270, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 53h:22m:48s remains)
INFO - root - 2017-12-07 22:27:57.965835: step 61280, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 49h:17m:50s remains)
INFO - root - 2017-12-07 22:28:04.791866: step 61290, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 50h:29m:30s remains)
INFO - root - 2017-12-07 22:28:11.468217: step 61300, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 50h:16m:59s remains)
2017-12-07 22:28:12.215280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1936846 -4.2051587 -4.217464 -4.2217512 -4.2216196 -4.2184439 -4.2151566 -4.2154174 -4.2202187 -4.2275724 -4.2326126 -4.2391405 -4.2495008 -4.2623649 -4.2857275][-4.2007642 -4.2101841 -4.2189922 -4.2206235 -4.2182665 -4.2130151 -4.2063684 -4.203908 -4.2075715 -4.2127128 -4.2145805 -4.2198691 -4.2303152 -4.24474 -4.2730289][-4.23381 -4.2438712 -4.2499471 -4.2470889 -4.2387218 -4.2267084 -4.2137876 -4.2088618 -4.2127371 -4.2165 -4.2161303 -4.2187223 -4.2270255 -4.2401485 -4.2681336][-4.2545991 -4.2590919 -4.2551441 -4.2425866 -4.2233248 -4.20246 -4.1847172 -4.1787009 -4.1895905 -4.2018552 -4.207521 -4.2109003 -4.2170477 -4.2291555 -4.2569165][-4.2408619 -4.2400417 -4.2267323 -4.1995659 -4.1628909 -4.1319141 -4.1151648 -4.1149869 -4.1400528 -4.1720161 -4.1942058 -4.2017851 -4.2061167 -4.2186179 -4.2483358][-4.2347884 -4.233048 -4.2157211 -4.1762338 -4.1218195 -4.076941 -4.05613 -4.0580153 -4.0939035 -4.1457443 -4.1852336 -4.2016687 -4.2047739 -4.2139897 -4.2420883][-4.2248383 -4.2214794 -4.1982656 -4.1485543 -4.082068 -4.02638 -3.9994278 -3.9991717 -4.0361943 -4.0974684 -4.1478257 -4.1712365 -4.1754966 -4.1834669 -4.21465][-4.16942 -4.1653624 -4.1427364 -4.09453 -4.0318375 -3.9798281 -3.9543858 -3.9535871 -3.9887044 -4.0522804 -4.1072731 -4.1325488 -4.1363173 -4.1448612 -4.1835][-4.1367092 -4.14054 -4.1331849 -4.1076355 -4.0699687 -4.0375133 -4.0171232 -4.0155568 -4.0438743 -4.0929971 -4.1345749 -4.1481228 -4.1475048 -4.1547689 -4.1917071][-4.1661129 -4.1795382 -4.188015 -4.1836753 -4.1660209 -4.1447363 -4.1232028 -4.1164851 -4.1355014 -4.1660266 -4.1884894 -4.1892414 -4.184556 -4.1887164 -4.2175593][-4.2097983 -4.2235441 -4.2358055 -4.2363634 -4.2227583 -4.202105 -4.181375 -4.1758633 -4.1940627 -4.2183475 -4.2312331 -4.2278252 -4.2220526 -4.2256441 -4.2473025][-4.2449522 -4.2568583 -4.2682123 -4.2686868 -4.2566371 -4.2390828 -4.2225518 -4.2208633 -4.2383065 -4.2599568 -4.2704191 -4.268971 -4.2648358 -4.2675643 -4.2817659][-4.2757144 -4.2862711 -4.2970572 -4.2980175 -4.2878022 -4.2732558 -4.2582679 -4.2569952 -4.2721825 -4.2923212 -4.304317 -4.3053613 -4.3032994 -4.3034096 -4.3115358][-4.2982583 -4.30689 -4.3168492 -4.318181 -4.31134 -4.3021379 -4.2925558 -4.291657 -4.303968 -4.3221817 -4.3352284 -4.3375406 -4.336123 -4.3336563 -4.33572][-4.3139939 -4.3190117 -4.3257995 -4.3257828 -4.3209958 -4.3171396 -4.3142347 -4.316587 -4.3277807 -4.3435907 -4.3552847 -4.3579316 -4.3567047 -4.3529625 -4.3502903]]...]
INFO - root - 2017-12-07 22:28:18.912396: step 61310, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 52h:09m:22s remains)
INFO - root - 2017-12-07 22:28:25.669251: step 61320, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 49h:07m:46s remains)
INFO - root - 2017-12-07 22:28:32.331314: step 61330, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 52h:08m:01s remains)
INFO - root - 2017-12-07 22:28:38.836436: step 61340, loss = 2.04, batch loss = 1.98 (16.7 examples/sec; 0.480 sec/batch; 36h:09m:42s remains)
INFO - root - 2017-12-07 22:28:45.561398: step 61350, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 52h:44m:39s remains)
INFO - root - 2017-12-07 22:28:52.259782: step 61360, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.687 sec/batch; 51h:42m:26s remains)
INFO - root - 2017-12-07 22:28:58.991339: step 61370, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 51h:14m:33s remains)
INFO - root - 2017-12-07 22:29:05.782980: step 61380, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 48h:58m:51s remains)
INFO - root - 2017-12-07 22:29:12.565834: step 61390, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.648 sec/batch; 48h:46m:53s remains)
INFO - root - 2017-12-07 22:29:19.350544: step 61400, loss = 2.09, batch loss = 2.04 (11.3 examples/sec; 0.707 sec/batch; 53h:16m:29s remains)
2017-12-07 22:29:20.113153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568088 -4.2468939 -4.244978 -4.2477865 -4.2507238 -4.2503796 -4.2556648 -4.2630219 -4.2667384 -4.2687039 -4.2702909 -4.2700772 -4.2677956 -4.2690415 -4.2738004][-4.2432027 -4.2332754 -4.2309532 -4.2340441 -4.2382231 -4.2411809 -4.2506227 -4.260941 -4.2667751 -4.2697892 -4.2713737 -4.27077 -4.2678657 -4.2668538 -4.26898][-4.2115359 -4.1999507 -4.1966248 -4.1999173 -4.2069378 -4.21834 -4.2373905 -4.2536311 -4.2632322 -4.2693191 -4.2730889 -4.2748551 -4.2740235 -4.2724366 -4.2714634][-4.1645522 -4.1529164 -4.1494966 -4.1516128 -4.1614895 -4.1832485 -4.2147741 -4.23818 -4.249342 -4.2548642 -4.2571883 -4.2592983 -4.2620063 -4.2624207 -4.260951][-4.1183062 -4.1100039 -4.1076565 -4.1047645 -4.1111646 -4.1370454 -4.1763258 -4.2046733 -4.2148991 -4.2170229 -4.2151904 -4.2157459 -4.2219691 -4.2250748 -4.2257318][-4.0886931 -4.0839052 -4.0810423 -4.0672092 -4.0587921 -4.0761323 -4.1113796 -4.1388593 -4.1505818 -4.154676 -4.1522074 -4.15088 -4.1586137 -4.1651678 -4.1701927][-4.0841875 -4.0750532 -4.0652056 -4.0366435 -4.0066552 -4.003562 -4.0233316 -4.0484834 -4.0683784 -4.0812736 -4.0831337 -4.0829234 -4.0903478 -4.0964422 -4.103539][-4.1143894 -4.0949316 -4.0735965 -4.0329356 -3.9849844 -3.9559588 -3.9467077 -3.961396 -3.9930198 -4.0188947 -4.0309815 -4.0347004 -4.039959 -4.0403843 -4.0395451][-4.1681638 -4.1440816 -4.1182375 -4.0768471 -4.0268731 -3.9852958 -3.9521878 -3.9473629 -3.9725339 -3.9961326 -4.0103478 -4.016717 -4.0180254 -4.0083485 -3.9933062][-4.2113266 -4.191649 -4.1724539 -4.1419029 -4.1038733 -4.0692787 -4.035943 -4.0185156 -4.0235229 -4.0300417 -4.035316 -4.0381927 -4.0349889 -4.0186357 -3.9924662][-4.223002 -4.2116032 -4.2020335 -4.1872888 -4.1663995 -4.1444645 -4.1189008 -4.0974069 -4.0898123 -4.0876985 -4.0879374 -4.0877018 -4.0831447 -4.0663939 -4.0391417][-4.2126765 -4.2054434 -4.2018914 -4.1973033 -4.1896672 -4.1803446 -4.164362 -4.1453738 -4.1334605 -4.1318011 -4.1334009 -4.133152 -4.1307487 -4.1183262 -4.0969129][-4.1968174 -4.1898479 -4.1895027 -4.1891775 -4.1878939 -4.186306 -4.1791415 -4.1660118 -4.1556187 -4.1565828 -4.162014 -4.1637044 -4.1635709 -4.1571269 -4.1454787][-4.1931663 -4.1843705 -4.1852608 -4.1867266 -4.1871915 -4.1884837 -4.1881161 -4.1812534 -4.1737123 -4.1764369 -4.1830721 -4.1853533 -4.1857529 -4.1836619 -4.1798525][-4.21038 -4.2019148 -4.2036386 -4.2061977 -4.20756 -4.2099524 -4.2133107 -4.2107973 -4.2059793 -4.2079735 -4.2117763 -4.2115803 -4.2096167 -4.2075844 -4.2059989]]...]
INFO - root - 2017-12-07 22:29:26.746979: step 61410, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 49h:40m:10s remains)
INFO - root - 2017-12-07 22:29:33.594335: step 61420, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 49h:28m:16s remains)
INFO - root - 2017-12-07 22:29:40.479234: step 61430, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 51h:52m:36s remains)
INFO - root - 2017-12-07 22:29:47.471672: step 61440, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.718 sec/batch; 54h:05m:15s remains)
INFO - root - 2017-12-07 22:29:54.313826: step 61450, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 49h:30m:52s remains)
INFO - root - 2017-12-07 22:30:01.217608: step 61460, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 50h:01m:10s remains)
INFO - root - 2017-12-07 22:30:07.944593: step 61470, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.706 sec/batch; 53h:08m:35s remains)
INFO - root - 2017-12-07 22:30:14.731730: step 61480, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 54h:27m:41s remains)
INFO - root - 2017-12-07 22:30:21.441439: step 61490, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 52h:57m:15s remains)
INFO - root - 2017-12-07 22:30:28.342374: step 61500, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 50h:04m:26s remains)
2017-12-07 22:30:29.089071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3006778 -4.2918382 -4.2859759 -4.2809467 -4.2809696 -4.2837567 -4.2876616 -4.2983313 -4.3080478 -4.3137035 -4.3145866 -4.3087444 -4.3028665 -4.3011951 -4.30556][-4.2823863 -4.27325 -4.2677832 -4.2615013 -4.2595463 -4.2604876 -4.2630644 -4.2776175 -4.2933087 -4.3019433 -4.3014522 -4.2919688 -4.2829518 -4.2812939 -4.2870455][-4.2627649 -4.2461023 -4.2360082 -4.2263308 -4.2203183 -4.216845 -4.2193985 -4.2410522 -4.2675934 -4.2796812 -4.2765512 -4.2648811 -4.2545691 -4.2546253 -4.2602062][-4.2417178 -4.2112951 -4.1926079 -4.1759782 -4.1600018 -4.1477551 -4.1525416 -4.185894 -4.2217827 -4.2340355 -4.2270732 -4.2147317 -4.2074909 -4.2100177 -4.2127581][-4.2150903 -4.1675453 -4.137042 -4.1166854 -4.0948763 -4.0644526 -4.0556135 -4.0924153 -4.1398926 -4.1564817 -4.1493115 -4.1418428 -4.1428685 -4.1492257 -4.1510892][-4.1835003 -4.1237326 -4.0808792 -4.051599 -4.0184941 -3.9637911 -3.9288361 -3.959172 -4.0238385 -4.0571146 -4.06429 -4.0666904 -4.0761123 -4.0877061 -4.0961757][-4.1682687 -4.1043549 -4.0522051 -4.0113497 -3.9566734 -3.8666995 -3.7837791 -3.7929566 -3.8863256 -3.9566872 -3.9911299 -4.0058007 -4.0225029 -4.0374374 -4.0507088][-4.1716456 -4.1127572 -4.0597138 -4.015501 -3.9495947 -3.8389044 -3.7023959 -3.6863523 -3.8157525 -3.9189868 -3.9708643 -3.9929168 -4.0093374 -4.0222831 -4.0352917][-4.1976604 -4.15459 -4.1141834 -4.0811844 -4.0267344 -3.9335375 -3.8161771 -3.8047705 -3.910979 -3.9938068 -4.0359182 -4.051496 -4.0567274 -4.0567217 -4.063446][-4.2300262 -4.2038264 -4.1774573 -4.1557488 -4.1148024 -4.0567508 -3.995486 -3.99903 -4.0591459 -4.1018062 -4.1242881 -4.1341257 -4.1320577 -4.1186538 -4.1174226][-4.259573 -4.2441621 -4.2277284 -4.2127614 -4.186604 -4.1591315 -4.1358767 -4.1415 -4.1731677 -4.194221 -4.202457 -4.2064528 -4.2041407 -4.1906362 -4.1883774][-4.2825994 -4.2713475 -4.2625952 -4.2567892 -4.2464027 -4.2371016 -4.2318873 -4.2381883 -4.2523589 -4.261981 -4.2656226 -4.2645655 -4.2645445 -4.2560077 -4.256423][-4.2973971 -4.2878108 -4.284584 -4.2892804 -4.2908788 -4.2915058 -4.2908444 -4.29394 -4.297967 -4.3006454 -4.3002024 -4.2956939 -4.2932477 -4.2893534 -4.2930231][-4.3077521 -4.3018322 -4.3002419 -4.3055873 -4.3093877 -4.3114729 -4.3132 -4.31423 -4.3128314 -4.311307 -4.308814 -4.3048053 -4.3006883 -4.2974958 -4.3009148][-4.3180809 -4.3144722 -4.3134823 -4.3138561 -4.3131938 -4.3132844 -4.3146052 -4.3156643 -4.3148766 -4.3149047 -4.3140984 -4.312717 -4.3103542 -4.3077106 -4.3080826]]...]
INFO - root - 2017-12-07 22:30:35.674139: step 61510, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 53h:03m:18s remains)
INFO - root - 2017-12-07 22:30:42.530859: step 61520, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 50h:51m:11s remains)
INFO - root - 2017-12-07 22:30:49.438312: step 61530, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 50h:25m:46s remains)
INFO - root - 2017-12-07 22:30:56.252288: step 61540, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 49h:55m:18s remains)
INFO - root - 2017-12-07 22:31:03.043530: step 61550, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 51h:23m:32s remains)
INFO - root - 2017-12-07 22:31:10.029505: step 61560, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 55h:14m:26s remains)
INFO - root - 2017-12-07 22:31:16.869353: step 61570, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 51h:08m:03s remains)
INFO - root - 2017-12-07 22:31:23.594113: step 61580, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:59m:45s remains)
INFO - root - 2017-12-07 22:31:30.476429: step 61590, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 51h:07m:12s remains)
INFO - root - 2017-12-07 22:31:37.214101: step 61600, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 51h:29m:02s remains)
2017-12-07 22:31:37.953509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2395134 -4.2276478 -4.2299476 -4.25754 -4.2986131 -4.3389792 -4.3610325 -4.3420572 -4.2737651 -4.1696334 -4.068687 -4.0300474 -4.07887 -4.168642 -4.2409511][-4.3147492 -4.3188958 -4.33182 -4.3541784 -4.3748384 -4.3884082 -4.3847966 -4.3409886 -4.2554364 -4.1388664 -4.0356183 -4.0120335 -4.0792589 -4.1775661 -4.2591329][-4.360786 -4.3773589 -4.3992519 -4.4166312 -4.4190865 -4.4082861 -4.382494 -4.3223739 -4.2308888 -4.1240516 -4.0449576 -4.0482054 -4.1260638 -4.2181764 -4.2928238][-4.3833046 -4.40876 -4.4315095 -4.4420061 -4.4306793 -4.40349 -4.3599057 -4.2891173 -4.1986618 -4.114964 -4.0707135 -4.0997519 -4.1835513 -4.2669649 -4.3295979][-4.3993559 -4.4279513 -4.4441962 -4.4423928 -4.4126096 -4.3670692 -4.3055973 -4.2246757 -4.1427059 -4.0950661 -4.0931644 -4.14069 -4.2198195 -4.2933626 -4.3454165][-4.3997774 -4.4268651 -4.4358363 -4.4186397 -4.3629594 -4.2875848 -4.1984348 -4.0983729 -4.0253391 -4.0247445 -4.075192 -4.1457996 -4.2235088 -4.2909784 -4.3392706][-4.3857646 -4.4099894 -4.4100142 -4.3689623 -4.2778783 -4.1657586 -4.0345416 -3.8918095 -3.817843 -3.8820636 -4.0011578 -4.1122813 -4.2072868 -4.2813973 -4.3325][-4.3558655 -4.3756981 -4.368793 -4.3079343 -4.1887445 -4.0364094 -3.8484757 -3.6530201 -3.5895042 -3.727252 -3.9093971 -4.0618687 -4.1814871 -4.2705655 -4.3271813][-4.3292894 -4.3492465 -4.3413916 -4.2754154 -4.1484313 -3.9758844 -3.7591646 -3.5550325 -3.5307565 -3.7031007 -3.89844 -4.0594039 -4.1845837 -4.2736239 -4.3300676][-4.294086 -4.3246222 -4.3302245 -4.2821636 -4.178246 -4.0275912 -3.8488994 -3.7100573 -3.7149673 -3.8470945 -3.9961643 -4.1281347 -4.2326384 -4.3022637 -4.3469868][-4.2557955 -4.3045664 -4.3287969 -4.3064165 -4.235971 -4.1265879 -4.0088463 -3.9344583 -3.9466081 -4.030725 -4.1294918 -4.2230897 -4.2954717 -4.3405008 -4.3710032][-4.2220368 -4.2899995 -4.329114 -4.323873 -4.2804947 -4.2093358 -4.143621 -4.1135273 -4.1279764 -4.1800923 -4.2431331 -4.2992878 -4.3423657 -4.3696513 -4.3876619][-4.2193866 -4.2948189 -4.3356495 -4.3354845 -4.3058286 -4.2629967 -4.2348275 -4.232306 -4.2499404 -4.28145 -4.3142595 -4.3407674 -4.363306 -4.3801641 -4.3911185][-4.2570834 -4.3202782 -4.3470106 -4.338047 -4.3080034 -4.2789631 -4.2714477 -4.2859483 -4.3090711 -4.329073 -4.3422966 -4.3511586 -4.3633742 -4.3759074 -4.3853602][-4.3057933 -4.3482513 -4.3547111 -4.3287358 -4.2871881 -4.2596049 -4.2628927 -4.29187 -4.3230729 -4.3412442 -4.3454585 -4.3462477 -4.3538218 -4.3661566 -4.3774462]]...]
INFO - root - 2017-12-07 22:31:44.466675: step 61610, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 46h:54m:00s remains)
INFO - root - 2017-12-07 22:31:51.349703: step 61620, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 50h:07m:40s remains)
INFO - root - 2017-12-07 22:31:58.117961: step 61630, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 51h:47m:07s remains)
INFO - root - 2017-12-07 22:32:04.995902: step 61640, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.748 sec/batch; 56h:16m:35s remains)
INFO - root - 2017-12-07 22:32:11.826226: step 61650, loss = 2.06, batch loss = 2.00 (14.2 examples/sec; 0.562 sec/batch; 42h:15m:54s remains)
INFO - root - 2017-12-07 22:32:18.655492: step 61660, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 52h:43m:06s remains)
INFO - root - 2017-12-07 22:32:25.552834: step 61670, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 52h:23m:52s remains)
INFO - root - 2017-12-07 22:32:32.372366: step 61680, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 51h:52m:52s remains)
INFO - root - 2017-12-07 22:32:39.162026: step 61690, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.637 sec/batch; 47h:53m:50s remains)
INFO - root - 2017-12-07 22:32:45.971001: step 61700, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 49h:26m:48s remains)
2017-12-07 22:32:46.724158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1369944 -4.0670085 -4.0220957 -4.0135574 -4.006813 -3.9950411 -4.0087633 -4.0387583 -4.05494 -4.0605493 -4.0737329 -4.0986137 -4.1197586 -4.1235943 -4.1153827][-4.1257062 -4.06478 -4.0335965 -4.0297909 -4.0218892 -3.9968867 -3.9982247 -4.0207582 -4.0301871 -4.0381384 -4.0591564 -4.0942698 -4.1200304 -4.1183133 -4.0983887][-4.1185369 -4.08201 -4.06997 -4.0648584 -4.0431757 -4.0006075 -3.9948649 -4.0160451 -4.0290637 -4.0437036 -4.0706334 -4.1073084 -4.1281929 -4.1111684 -4.0744104][-4.1207266 -4.0989809 -4.0905628 -4.077642 -4.0453959 -3.9938648 -3.983305 -4.0074234 -4.0356441 -4.0651903 -4.0953093 -4.1282449 -4.1455569 -4.1257362 -4.0824285][-4.1176009 -4.0981908 -4.0859571 -4.0681286 -4.0374002 -3.9934843 -3.981786 -4.01205 -4.0568323 -4.0967531 -4.1289067 -4.162158 -4.180048 -4.1689715 -4.1349325][-4.120544 -4.0985942 -4.081212 -4.068615 -4.0477285 -4.0134859 -3.9930418 -4.0160775 -4.0648665 -4.1121716 -4.1511712 -4.1864028 -4.208477 -4.2102289 -4.1921282][-4.1354671 -4.1187768 -4.1080503 -4.1087918 -4.096808 -4.0640883 -4.0254745 -4.0273004 -4.0674648 -4.1132326 -4.1575742 -4.1941 -4.2211146 -4.2356896 -4.2326975][-4.1534839 -4.1464853 -4.1493468 -4.1607628 -4.1556106 -4.1246309 -4.0764694 -4.0600166 -4.0872307 -4.1229491 -4.1613388 -4.1939845 -4.2252345 -4.2472763 -4.2525544][-4.1710153 -4.1699815 -4.1789808 -4.1954432 -4.197607 -4.1765857 -4.1349726 -4.1076689 -4.1172109 -4.1416526 -4.1696315 -4.193759 -4.2221327 -4.2457566 -4.2551856][-4.1871934 -4.1901145 -4.1994882 -4.2136612 -4.2183361 -4.2063241 -4.1737933 -4.1383677 -4.1340575 -4.1532311 -4.1748447 -4.1929522 -4.2136893 -4.2329144 -4.2436366][-4.1927872 -4.1983533 -4.2036791 -4.2104673 -4.2130251 -4.2022815 -4.172514 -4.135582 -4.130641 -4.1515679 -4.1753297 -4.1940331 -4.2092338 -4.2218575 -4.22998][-4.1799269 -4.1847062 -4.1843171 -4.1846409 -4.181015 -4.16346 -4.1329746 -4.1046829 -4.1084571 -4.1376724 -4.1684418 -4.1930847 -4.2057319 -4.2081285 -4.2116456][-4.1802254 -4.1834793 -4.1797557 -4.1777077 -4.171205 -4.1466708 -4.1137609 -4.090291 -4.1004643 -4.1356335 -4.1751676 -4.2039704 -4.212307 -4.2065268 -4.2031965][-4.2015071 -4.2003274 -4.193532 -4.1892672 -4.180851 -4.1514316 -4.1164432 -4.0969467 -4.1109037 -4.146575 -4.188189 -4.215415 -4.2198572 -4.2099552 -4.2051358][-4.2379408 -4.2308416 -4.2202773 -4.2154822 -4.2063651 -4.18053 -4.1524763 -4.14254 -4.157094 -4.1843824 -4.2145581 -4.2332854 -4.234 -4.2230344 -4.2168031]]...]
INFO - root - 2017-12-07 22:32:53.354198: step 61710, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 52h:34m:24s remains)
INFO - root - 2017-12-07 22:33:00.275731: step 61720, loss = 2.02, batch loss = 1.96 (12.1 examples/sec; 0.663 sec/batch; 49h:52m:49s remains)
INFO - root - 2017-12-07 22:33:07.130442: step 61730, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 48h:34m:16s remains)
INFO - root - 2017-12-07 22:33:14.010775: step 61740, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 53h:16m:13s remains)
INFO - root - 2017-12-07 22:33:20.761623: step 61750, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 55h:24m:46s remains)
INFO - root - 2017-12-07 22:33:27.632648: step 61760, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 50h:33m:10s remains)
INFO - root - 2017-12-07 22:33:34.384043: step 61770, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 47h:50m:31s remains)
INFO - root - 2017-12-07 22:33:41.105063: step 61780, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 48h:36m:16s remains)
INFO - root - 2017-12-07 22:33:47.901099: step 61790, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 49h:51m:03s remains)
INFO - root - 2017-12-07 22:33:54.729494: step 61800, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 52h:55m:33s remains)
2017-12-07 22:33:55.528522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2866263 -4.2913418 -4.299706 -4.3067231 -4.3098631 -4.3110037 -4.3118811 -4.31223 -4.3123608 -4.3131351 -4.3146348 -4.315589 -4.314568 -4.31246 -4.3105388][-4.3092332 -4.3133063 -4.3197794 -4.3246474 -4.3259377 -4.3254876 -4.3243389 -4.322444 -4.3216577 -4.3236022 -4.3276229 -4.3316417 -4.3338876 -4.334034 -4.3322568][-4.3211823 -4.3246341 -4.3278165 -4.3284349 -4.3254457 -4.320086 -4.3132958 -4.3068619 -4.3050733 -4.3100314 -4.319592 -4.32988 -4.3375382 -4.34085 -4.3403959][-4.3123326 -4.3138957 -4.3138542 -4.30994 -4.3017673 -4.29071 -4.2770576 -4.2641621 -4.25889 -4.2657843 -4.2820797 -4.30181 -4.3191056 -4.3295074 -4.3333364][-4.2908235 -4.2893763 -4.2861042 -4.27734 -4.2631412 -4.2447853 -4.2221122 -4.1984925 -4.183969 -4.1889782 -4.2116151 -4.2435393 -4.2755742 -4.2983027 -4.3116479][-4.2653975 -4.2617683 -4.2561393 -4.2435541 -4.2254591 -4.2028918 -4.1758218 -4.1439328 -4.1197462 -4.117878 -4.1396017 -4.1772847 -4.2179451 -4.2503076 -4.2739444][-4.2452803 -4.2404356 -4.2347994 -4.2215562 -4.2044783 -4.1855564 -4.1646352 -4.1371489 -4.1113892 -4.1002741 -4.1110282 -4.1397333 -4.1744256 -4.2067156 -4.2349372][-4.2393789 -4.23638 -4.233067 -4.2209783 -4.2062135 -4.1933351 -4.1819019 -4.165225 -4.1466765 -4.1333961 -4.1335564 -4.1477113 -4.1690178 -4.192275 -4.2168927][-4.2600851 -4.2599754 -4.2578769 -4.2451835 -4.2279763 -4.2140651 -4.20604 -4.1986766 -4.1916256 -4.1875939 -4.1897588 -4.1967139 -4.2064977 -4.2170739 -4.2316937][-4.2856655 -4.285676 -4.2823839 -4.2685947 -4.2495508 -4.2348285 -4.22897 -4.2279181 -4.22997 -4.2356472 -4.243618 -4.2488184 -4.251533 -4.2534037 -4.2589917][-4.298049 -4.2973471 -4.2933927 -4.2813978 -4.2655406 -4.2545495 -4.2524705 -4.2555141 -4.2616558 -4.2703919 -4.2795725 -4.2838273 -4.2833977 -4.2815433 -4.2814617][-4.3058825 -4.3057775 -4.3025007 -4.2941628 -4.2836089 -4.2770014 -4.2768593 -4.2797108 -4.2837682 -4.2895103 -4.295825 -4.2992516 -4.2982774 -4.2953858 -4.2913785][-4.3128452 -4.3132911 -4.3105507 -4.3044186 -4.2966576 -4.2923732 -4.2924418 -4.2923741 -4.291676 -4.2921782 -4.2941837 -4.2953334 -4.2935581 -4.290132 -4.2835169][-4.3207335 -4.3208652 -4.3182769 -4.313808 -4.3091373 -4.3071561 -4.3073373 -4.3046608 -4.2985983 -4.2925696 -4.2876363 -4.2832589 -4.2781944 -4.271852 -4.2620811][-4.3302622 -4.3293619 -4.326118 -4.3222532 -4.31982 -4.3195806 -4.31915 -4.3144603 -4.3049145 -4.2949166 -4.2858944 -4.2780838 -4.2714682 -4.2643986 -4.2534003]]...]
INFO - root - 2017-12-07 22:34:02.139979: step 61810, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 48h:36m:57s remains)
INFO - root - 2017-12-07 22:34:09.106215: step 61820, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 53h:36m:57s remains)
INFO - root - 2017-12-07 22:34:15.887752: step 61830, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 53h:58m:08s remains)
INFO - root - 2017-12-07 22:34:22.720312: step 61840, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 51h:06m:00s remains)
INFO - root - 2017-12-07 22:34:29.568693: step 61850, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.669 sec/batch; 50h:17m:38s remains)
INFO - root - 2017-12-07 22:34:36.427356: step 61860, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 51h:34m:34s remains)
INFO - root - 2017-12-07 22:34:43.226537: step 61870, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 52h:33m:07s remains)
INFO - root - 2017-12-07 22:34:50.048125: step 61880, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 51h:56m:34s remains)
INFO - root - 2017-12-07 22:34:56.851364: step 61890, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 51h:06m:41s remains)
INFO - root - 2017-12-07 22:35:03.573407: step 61900, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 51h:05m:16s remains)
2017-12-07 22:35:04.317741: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2896962 -4.3005643 -4.296032 -4.2611351 -4.2066135 -4.1393447 -4.0972638 -4.1047626 -4.1470232 -4.1978674 -4.2431712 -4.2768354 -4.29533 -4.3043408 -4.3112583][-4.2897539 -4.2995434 -4.2955794 -4.2575617 -4.1929226 -4.1153398 -4.0669665 -4.0794654 -4.130548 -4.1893911 -4.2416544 -4.278409 -4.2991128 -4.3098154 -4.3162942][-4.2871332 -4.2926059 -4.2874546 -4.2519555 -4.1859541 -4.0999384 -4.0384645 -4.0427556 -4.0941367 -4.1628284 -4.2267861 -4.2711835 -4.2969942 -4.310874 -4.3178315][-4.2808833 -4.2827449 -4.2755165 -4.2426105 -4.180613 -4.0913944 -4.0156841 -4.0024862 -4.0491834 -4.12869 -4.206645 -4.2597308 -4.2914472 -4.3096471 -4.3173103][-4.2714276 -4.2683072 -4.2588191 -4.2292933 -4.171289 -4.0828414 -3.9985704 -3.9714472 -4.0163121 -4.1055326 -4.1941171 -4.2530794 -4.2877026 -4.3078403 -4.315968][-4.2596717 -4.2522659 -4.2421913 -4.2170782 -4.1638184 -4.0801344 -3.998477 -3.9679587 -4.0162878 -4.112299 -4.2027159 -4.2590389 -4.2909 -4.3096223 -4.3164744][-4.2391963 -4.229866 -4.222724 -4.2039514 -4.157311 -4.081439 -4.0089159 -3.9846263 -4.0387859 -4.1339984 -4.2173138 -4.2687163 -4.29834 -4.3147249 -4.31907][-4.1990747 -4.1890435 -4.1889238 -4.1801152 -4.1451941 -4.0845366 -4.0284815 -4.016593 -4.0715814 -4.1570349 -4.2292647 -4.2763786 -4.30443 -4.3193846 -4.3214297][-4.1431179 -4.1334419 -4.1453352 -4.1490755 -4.1285529 -4.0872512 -4.0509996 -4.051908 -4.1016707 -4.1730618 -4.2363992 -4.282804 -4.3111324 -4.3236375 -4.3230271][-4.1165276 -4.1047568 -4.1221814 -4.1326675 -4.1223617 -4.0934868 -4.0704465 -4.078423 -4.1209292 -4.1825867 -4.2416148 -4.2886014 -4.31647 -4.326088 -4.323802][-4.1428909 -4.1281543 -4.1404467 -4.1495972 -4.13987 -4.11095 -4.0853171 -4.0905905 -4.1281519 -4.1868253 -4.2473092 -4.2933273 -4.3197622 -4.327445 -4.3240032][-4.1793265 -4.1612859 -4.1670184 -4.1737309 -4.1608343 -4.1262817 -4.0912285 -4.089663 -4.12278 -4.182374 -4.2464142 -4.292263 -4.3176975 -4.3260336 -4.32365][-4.2124968 -4.1941328 -4.1962037 -4.2030311 -4.1924653 -4.1558723 -4.1087356 -4.0943675 -4.1237779 -4.1835847 -4.2466707 -4.2907538 -4.3149395 -4.3243413 -4.3236737][-4.236877 -4.2202897 -4.2191563 -4.2251167 -4.2157283 -4.1781673 -4.1234632 -4.1005363 -4.1286373 -4.1868439 -4.2471423 -4.28934 -4.3120618 -4.3230858 -4.3244543][-4.2484083 -4.2347388 -4.2295785 -4.2309723 -4.2177935 -4.1756034 -4.1174779 -4.0931573 -4.1215858 -4.1784158 -4.2386074 -4.2830524 -4.3078356 -4.3217964 -4.3246217]]...]
INFO - root - 2017-12-07 22:35:10.816887: step 61910, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 53h:18m:37s remains)
INFO - root - 2017-12-07 22:35:17.769517: step 61920, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 51h:02m:53s remains)
INFO - root - 2017-12-07 22:35:24.621850: step 61930, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 48h:13m:38s remains)
INFO - root - 2017-12-07 22:35:31.482411: step 61940, loss = 2.10, batch loss = 2.04 (11.3 examples/sec; 0.710 sec/batch; 53h:21m:08s remains)
INFO - root - 2017-12-07 22:35:38.351461: step 61950, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 54h:01m:16s remains)
INFO - root - 2017-12-07 22:35:45.105988: step 61960, loss = 2.07, batch loss = 2.01 (13.4 examples/sec; 0.598 sec/batch; 44h:56m:38s remains)
INFO - root - 2017-12-07 22:35:51.736527: step 61970, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.620 sec/batch; 46h:36m:35s remains)
INFO - root - 2017-12-07 22:35:58.581586: step 61980, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 51h:24m:46s remains)
INFO - root - 2017-12-07 22:36:05.414129: step 61990, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 55h:31m:37s remains)
INFO - root - 2017-12-07 22:36:12.277781: step 62000, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 50h:06m:54s remains)
2017-12-07 22:36:13.059431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0837479 -4.0831313 -4.1031952 -4.128623 -4.1475978 -4.1428947 -4.1008091 -4.0354037 -3.9734547 -3.976665 -4.0470934 -4.1356235 -4.2155924 -4.2855263 -4.3311176][-4.094286 -4.0952334 -4.1155963 -4.1350675 -4.1415854 -4.1278057 -4.0764589 -4.0028687 -3.9369283 -3.9441509 -4.0224075 -4.1184664 -4.2014208 -4.2738447 -4.3228807][-4.1022806 -4.1076479 -4.1296854 -4.1433191 -4.1402845 -4.1216311 -4.0601759 -3.9744802 -3.9017708 -3.9096816 -3.9965997 -4.1047244 -4.1935515 -4.2676516 -4.3183174][-4.0962114 -4.1071711 -4.1313629 -4.146102 -4.1458535 -4.1250448 -4.0534897 -3.9543657 -3.871305 -3.8795807 -3.9776039 -4.0997219 -4.1959181 -4.2696824 -4.3190913][-4.0744185 -4.0877204 -4.1130376 -4.1347919 -4.1435471 -4.1236782 -4.0398817 -3.9289222 -3.8415878 -3.8552017 -3.9677856 -4.1029506 -4.2046447 -4.2762122 -4.3226781][-4.065197 -4.0696387 -4.0864015 -4.1102772 -4.1211524 -4.0961661 -4.0038605 -3.8972509 -3.8204539 -3.8430653 -3.9665136 -4.1063151 -4.21031 -4.2829595 -4.3281584][-4.073246 -4.06837 -4.071929 -4.0848675 -4.0833716 -4.0413017 -3.9475112 -3.85884 -3.8022103 -3.8358591 -3.9635797 -4.1029086 -4.2091 -4.2866435 -4.3320909][-4.0897541 -4.0819211 -4.0788107 -4.0774364 -4.0551481 -3.9926994 -3.8959355 -3.8244717 -3.7861233 -3.824264 -3.9537673 -4.0924368 -4.2027111 -4.2856612 -4.3335776][-4.1019607 -4.1010275 -4.1022077 -4.098588 -4.0710793 -4.0006933 -3.9020302 -3.8297515 -3.791265 -3.8244073 -3.9486508 -4.0856915 -4.1986852 -4.2844806 -4.3341141][-4.1063867 -4.1127243 -4.1259656 -4.1315384 -4.1116433 -4.04701 -3.9563103 -3.8768566 -3.8274128 -3.8490503 -3.9587226 -4.0870161 -4.2005219 -4.2855749 -4.3343616][-4.1095638 -4.1126056 -4.1381483 -4.1527562 -4.1413708 -4.086051 -4.00456 -3.9266262 -3.8707974 -3.8783715 -3.9739873 -4.0936785 -4.2036881 -4.2875166 -4.3346944][-4.1136732 -4.1089888 -4.1390848 -4.1584 -4.1506472 -4.1020718 -4.0327787 -3.9656403 -3.9078276 -3.9022489 -3.9874597 -4.1016917 -4.2062669 -4.2891865 -4.3349419][-4.119441 -4.1096129 -4.1427121 -4.1675959 -4.1582308 -4.1109209 -4.0489283 -3.9874346 -3.92888 -3.9162626 -3.9986038 -4.1103435 -4.2098217 -4.2905455 -4.3358464][-4.1284142 -4.117516 -4.1562557 -4.1888776 -4.1711555 -4.1236091 -4.0623269 -3.9941406 -3.9325051 -3.9243975 -4.0081153 -4.1190453 -4.2146764 -4.2940192 -4.3381333][-4.1411142 -4.1320052 -4.169754 -4.1987529 -4.176188 -4.137413 -4.0789094 -4.0029111 -3.9387345 -3.9386349 -4.0220237 -4.1279526 -4.2204432 -4.2991395 -4.3414941]]...]
INFO - root - 2017-12-07 22:36:19.704716: step 62010, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 53h:10m:27s remains)
INFO - root - 2017-12-07 22:36:26.552077: step 62020, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 51h:30m:44s remains)
INFO - root - 2017-12-07 22:36:33.316042: step 62030, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 48h:23m:38s remains)
INFO - root - 2017-12-07 22:36:40.159373: step 62040, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 50h:28m:59s remains)
INFO - root - 2017-12-07 22:36:46.948592: step 62050, loss = 2.05, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 52h:26m:02s remains)
INFO - root - 2017-12-07 22:36:53.833108: step 62060, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 53h:21m:05s remains)
INFO - root - 2017-12-07 22:37:00.696117: step 62070, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 52h:17m:18s remains)
INFO - root - 2017-12-07 22:37:07.423313: step 62080, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 48h:06m:07s remains)
INFO - root - 2017-12-07 22:37:14.298893: step 62090, loss = 2.10, batch loss = 2.04 (12.8 examples/sec; 0.626 sec/batch; 47h:02m:56s remains)
INFO - root - 2017-12-07 22:37:21.188761: step 62100, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 53h:48m:41s remains)
2017-12-07 22:37:21.896153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3503146 -4.3508091 -4.3508053 -4.3511848 -4.3517284 -4.3513021 -4.3503847 -4.350296 -4.3498669 -4.3470297 -4.3429537 -4.3414078 -4.3421583 -4.3448009 -4.34794][-4.3298244 -4.32897 -4.3279634 -4.3288827 -4.3317366 -4.33257 -4.3294082 -4.32747 -4.3269572 -4.32321 -4.3169904 -4.3144975 -4.3164797 -4.3207879 -4.3259149][-4.296134 -4.2910037 -4.2879896 -4.2898192 -4.296144 -4.2991519 -4.293252 -4.2891321 -4.288507 -4.2853985 -4.2771106 -4.272799 -4.2754946 -4.2808924 -4.2889624][-4.2541957 -4.240459 -4.2337089 -4.2362809 -4.2452617 -4.2499475 -4.2439895 -4.2398129 -4.2374678 -4.2337136 -4.2247348 -4.2193108 -4.2214308 -4.2264237 -4.2371778][-4.2197924 -4.1960268 -4.1823449 -4.1810174 -4.1886587 -4.1935396 -4.19062 -4.1878676 -4.183033 -4.1796541 -4.1755147 -4.1722474 -4.1725454 -4.171545 -4.1810703][-4.2026443 -4.1700473 -4.1466951 -4.1384377 -4.1408734 -4.1446738 -4.1482396 -4.1501851 -4.1415286 -4.139667 -4.144464 -4.1459231 -4.1445193 -4.1368537 -4.140738][-4.1921377 -4.153039 -4.1198931 -4.1044912 -4.104507 -4.1104231 -4.1227465 -4.1297827 -4.1168489 -4.114408 -4.1278644 -4.1371746 -4.1398139 -4.1318192 -4.1332][-4.1878481 -4.1448889 -4.1058259 -4.0893936 -4.091435 -4.0999622 -4.1179938 -4.1291809 -4.1146207 -4.1098709 -4.1271148 -4.1432471 -4.1549926 -4.1537905 -4.1562328][-4.1911182 -4.1485329 -4.1137204 -4.1016078 -4.1098132 -4.1223679 -4.14205 -4.1538634 -4.1392307 -4.1314707 -4.1477084 -4.1676769 -4.1889257 -4.1982555 -4.2029462][-4.2060423 -4.1689305 -4.1419539 -4.1358924 -4.1476951 -4.1607027 -4.1791668 -4.1910415 -4.1798263 -4.1671429 -4.1774259 -4.1971765 -4.220881 -4.2378922 -4.249022][-4.2285104 -4.1982551 -4.1790838 -4.1787219 -4.1902266 -4.1990523 -4.2113981 -4.2212529 -4.2140613 -4.199821 -4.2010379 -4.2143884 -4.2356367 -4.2557273 -4.271873][-4.2560387 -4.2295365 -4.2136178 -4.2166224 -4.2248507 -4.2280831 -4.2329082 -4.2383728 -4.23515 -4.2260184 -4.2234092 -4.2265563 -4.2389469 -4.2539663 -4.268661][-4.2693992 -4.24705 -4.23484 -4.2384071 -4.2406831 -4.2360835 -4.2342024 -4.2390881 -4.244916 -4.2489371 -4.2490654 -4.2468767 -4.2491374 -4.254734 -4.2614708][-4.2568169 -4.2418823 -4.2363329 -4.239306 -4.2354379 -4.2243838 -4.2170668 -4.2209368 -4.23687 -4.2575617 -4.2692332 -4.2706814 -4.2701931 -4.268713 -4.2662387][-4.2408061 -4.2331953 -4.2328539 -4.2343211 -4.2243266 -4.2068243 -4.1925383 -4.1915288 -4.21093 -4.242692 -4.2666965 -4.2772245 -4.2799582 -4.2763991 -4.2693081]]...]
INFO - root - 2017-12-07 22:37:28.432649: step 62110, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 47h:50m:14s remains)
INFO - root - 2017-12-07 22:37:35.268610: step 62120, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 49h:11m:37s remains)
INFO - root - 2017-12-07 22:37:42.251455: step 62130, loss = 2.02, batch loss = 1.96 (11.5 examples/sec; 0.695 sec/batch; 52h:10m:31s remains)
INFO - root - 2017-12-07 22:37:49.148138: step 62140, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 52h:41m:21s remains)
INFO - root - 2017-12-07 22:37:55.940999: step 62150, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 50h:00m:20s remains)
INFO - root - 2017-12-07 22:38:02.678487: step 62160, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 48h:45m:54s remains)
INFO - root - 2017-12-07 22:38:09.455727: step 62170, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 50h:20m:58s remains)
INFO - root - 2017-12-07 22:38:16.345453: step 62180, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 50h:50m:57s remains)
INFO - root - 2017-12-07 22:38:23.258892: step 62190, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.719 sec/batch; 53h:57m:34s remains)
INFO - root - 2017-12-07 22:38:30.051456: step 62200, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 50h:22m:25s remains)
2017-12-07 22:38:30.775584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2358189 -4.2381058 -4.2462139 -4.2568688 -4.2620425 -4.2636685 -4.2618275 -4.2603736 -4.2611642 -4.2632585 -4.2683625 -4.2739739 -4.2782149 -4.2791858 -4.278286][-4.2215767 -4.2294307 -4.2406626 -4.2484517 -4.2470789 -4.2440529 -4.2454915 -4.2498193 -4.2522507 -4.2534227 -4.2560949 -4.2578983 -4.2564092 -4.2490149 -4.2401805][-4.1914086 -4.2012019 -4.2145782 -4.2225428 -4.21984 -4.2191195 -4.2279239 -4.2379975 -4.2414479 -4.2423716 -4.2433753 -4.2438626 -4.2373805 -4.2223215 -4.2080731][-4.1713691 -4.1826029 -4.1977415 -4.205195 -4.2005363 -4.1980271 -4.2069273 -4.2178693 -4.2197952 -4.2206025 -4.2227592 -4.2265024 -4.221683 -4.2078662 -4.1962161][-4.1608925 -4.1747408 -4.1882472 -4.1883826 -4.1727448 -4.1604786 -4.1594167 -4.1638784 -4.1659865 -4.175632 -4.1894541 -4.2023993 -4.2035446 -4.1958032 -4.1905446][-4.1643329 -4.1782141 -4.182518 -4.1705985 -4.140749 -4.111093 -4.0890474 -4.0753627 -4.0715032 -4.095542 -4.1291127 -4.1584692 -4.1715465 -4.1781611 -4.1878176][-4.171752 -4.1766534 -4.165442 -4.1358905 -4.0901594 -4.0429616 -4.0010858 -3.9726746 -3.9687669 -4.0082507 -4.0611782 -4.1069317 -4.1362429 -4.1645255 -4.1945181][-4.1930509 -4.1860905 -4.1640525 -4.1256895 -4.0684614 -4.0075116 -3.9573731 -3.928833 -3.9309628 -3.9751692 -4.0312333 -4.0803161 -4.1172404 -4.161459 -4.2056141][-4.2205148 -4.2046165 -4.180326 -4.1476231 -4.0955119 -4.0393209 -3.9978752 -3.9831996 -3.9937015 -4.0325193 -4.0800815 -4.1197758 -4.1503072 -4.1894183 -4.2292509][-4.2442369 -4.2258439 -4.2085447 -4.1878505 -4.1507683 -4.1126757 -4.0896387 -4.0866594 -4.1001158 -4.1284533 -4.1626039 -4.1907897 -4.2093434 -4.2304029 -4.25192][-4.2722063 -4.2534523 -4.2428737 -4.2305522 -4.2073612 -4.1867442 -4.1812263 -4.1869845 -4.20049 -4.2188954 -4.2413259 -4.2556334 -4.258779 -4.2595286 -4.2596035][-4.3005409 -4.2852306 -4.27409 -4.2615638 -4.243782 -4.2319021 -4.2330976 -4.2431908 -4.2572117 -4.2710586 -4.2855344 -4.2862587 -4.2726626 -4.2563868 -4.2434182][-4.3287759 -4.3195124 -4.3073468 -4.2925134 -4.2766232 -4.2676978 -4.2676611 -4.2771297 -4.289063 -4.2976618 -4.3024969 -4.2879629 -4.2583575 -4.2291641 -4.2115288][-4.3425107 -4.3375149 -4.3266945 -4.3121166 -4.2997556 -4.2936063 -4.2897172 -4.2951784 -4.3061171 -4.3104091 -4.3045974 -4.2771235 -4.2377052 -4.2013917 -4.1804438][-4.3405747 -4.3345695 -4.3227286 -4.3087754 -4.3016396 -4.2991471 -4.2928109 -4.2929287 -4.30102 -4.3018603 -4.2862291 -4.2522793 -4.2164116 -4.1868792 -4.1686435]]...]
INFO - root - 2017-12-07 22:38:37.527412: step 62210, loss = 2.09, batch loss = 2.03 (10.9 examples/sec; 0.736 sec/batch; 55h:14m:22s remains)
INFO - root - 2017-12-07 22:38:44.371369: step 62220, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 50h:50m:14s remains)
INFO - root - 2017-12-07 22:38:51.150083: step 62230, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 46h:40m:58s remains)
INFO - root - 2017-12-07 22:38:57.849207: step 62240, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 46h:35m:44s remains)
INFO - root - 2017-12-07 22:39:04.662928: step 62250, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 53h:41m:22s remains)
INFO - root - 2017-12-07 22:39:11.476747: step 62260, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 51h:04m:33s remains)
INFO - root - 2017-12-07 22:39:18.239788: step 62270, loss = 2.07, batch loss = 2.02 (13.6 examples/sec; 0.590 sec/batch; 44h:18m:15s remains)
INFO - root - 2017-12-07 22:39:24.898773: step 62280, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 49h:08m:06s remains)
INFO - root - 2017-12-07 22:39:31.697825: step 62290, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 52h:39m:14s remains)
INFO - root - 2017-12-07 22:39:38.511822: step 62300, loss = 2.11, batch loss = 2.05 (11.3 examples/sec; 0.705 sec/batch; 52h:56m:06s remains)
2017-12-07 22:39:39.229118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2426634 -4.2575564 -4.2758474 -4.2787776 -4.267241 -4.2497063 -4.2312775 -4.2198391 -4.2224183 -4.2295408 -4.2359514 -4.2408671 -4.24266 -4.246069 -4.2601452][-4.2718029 -4.2817492 -4.2902722 -4.2780066 -4.2467508 -4.2136841 -4.186543 -4.1790571 -4.1963654 -4.21665 -4.2318616 -4.2437053 -4.2447219 -4.2424464 -4.2498736][-4.3019514 -4.3042612 -4.3006792 -4.2716603 -4.2194061 -4.1676588 -4.1291795 -4.1253858 -4.1579771 -4.1941938 -4.2220063 -4.2444487 -4.2498474 -4.2479205 -4.2519126][-4.3148513 -4.31322 -4.304739 -4.269063 -4.2027268 -4.1333117 -4.084002 -4.0810413 -4.1221657 -4.1702814 -4.2089539 -4.2423949 -4.2569003 -4.2598147 -4.2661219][-4.3078365 -4.3058081 -4.2985635 -4.2645793 -4.19564 -4.1161609 -4.0576329 -4.0516081 -4.0951428 -4.1537781 -4.2026124 -4.2415442 -4.2628646 -4.2718439 -4.2830052][-4.2902322 -4.2918544 -4.2894583 -4.2593651 -4.1926494 -4.1112709 -4.0442176 -4.0278034 -4.0650616 -4.1341157 -4.1966329 -4.243227 -4.2725158 -4.287096 -4.3011217][-4.282382 -4.2878661 -4.288126 -4.2590222 -4.1934576 -4.1118488 -4.0358529 -3.9992692 -4.0191045 -4.0938129 -4.1734486 -4.234426 -4.2764997 -4.3012524 -4.3191013][-4.2899151 -4.2999659 -4.300036 -4.2690034 -4.199307 -4.1121373 -4.0277686 -3.972069 -3.9731998 -4.0459986 -4.1395378 -4.2171841 -4.2732925 -4.3091712 -4.3329511][-4.3012133 -4.3135262 -4.3095174 -4.2706 -4.1921129 -4.0961065 -4.0041385 -3.9420075 -3.9389129 -4.0098448 -4.1105046 -4.2010117 -4.2699103 -4.3143883 -4.3410316][-4.2988353 -4.31091 -4.3006835 -4.2494931 -4.1611228 -4.0544295 -3.958401 -3.9032254 -3.9123311 -3.9930389 -4.1002975 -4.1997333 -4.2763796 -4.3227792 -4.346725][-4.2881184 -4.2992349 -4.2827082 -4.2231922 -4.1299677 -4.0237765 -3.9353516 -3.8988166 -3.9275599 -4.0166645 -4.1224742 -4.2205276 -4.2942009 -4.3344626 -4.3509254][-4.2687583 -4.2773023 -4.258141 -4.2019529 -4.1193986 -4.029933 -3.9658406 -3.9553254 -3.9992595 -4.0828109 -4.1740031 -4.2573133 -4.3169436 -4.3448968 -4.3522072][-4.2563448 -4.2649164 -4.2517085 -4.2078104 -4.144875 -4.0793481 -4.0425978 -4.0530062 -4.0998626 -4.1679173 -4.2349505 -4.2937465 -4.3342571 -4.3485751 -4.3475862][-4.2642794 -4.277843 -4.276289 -4.2494764 -4.2074471 -4.1628852 -4.1424232 -4.1566362 -4.1945658 -4.2425718 -4.2837181 -4.3200612 -4.344595 -4.3488951 -4.3411727][-4.2918525 -4.3074675 -4.3112955 -4.2947931 -4.2679081 -4.2403502 -4.229917 -4.2398915 -4.2619185 -4.2892938 -4.3111019 -4.3323126 -4.3481884 -4.3476734 -4.3348575]]...]
INFO - root - 2017-12-07 22:39:45.839233: step 62310, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 48h:51m:22s remains)
INFO - root - 2017-12-07 22:39:52.585168: step 62320, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.682 sec/batch; 51h:10m:16s remains)
INFO - root - 2017-12-07 22:39:59.407231: step 62330, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 52h:43m:31s remains)
INFO - root - 2017-12-07 22:40:06.236624: step 62340, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 51h:39m:25s remains)
INFO - root - 2017-12-07 22:40:13.027545: step 62350, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.646 sec/batch; 48h:28m:19s remains)
INFO - root - 2017-12-07 22:40:20.028442: step 62360, loss = 2.03, batch loss = 1.97 (11.1 examples/sec; 0.719 sec/batch; 53h:56m:44s remains)
INFO - root - 2017-12-07 22:40:26.942231: step 62370, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 52h:51m:09s remains)
INFO - root - 2017-12-07 22:40:33.791704: step 62380, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 52h:39m:48s remains)
INFO - root - 2017-12-07 22:40:40.642519: step 62390, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 51h:22m:44s remains)
INFO - root - 2017-12-07 22:40:47.357770: step 62400, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 49h:12m:55s remains)
2017-12-07 22:40:48.179560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0795679 -4.0245171 -4.0269485 -4.0512457 -4.0643473 -4.057507 -4.0223479 -3.9966216 -4.0175347 -4.0736089 -4.1362023 -4.1723342 -4.1865611 -4.2066946 -4.2339792][-4.0582728 -4.0121164 -4.0208082 -4.050849 -4.0685282 -4.0600557 -4.0218287 -3.9910145 -4.0113797 -4.0764565 -4.148777 -4.1866617 -4.196547 -4.21386 -4.2390976][-4.0635052 -4.0275211 -4.0399137 -4.070879 -4.08436 -4.0667028 -4.01938 -3.9834239 -4.0087342 -4.0831704 -4.1618171 -4.2008042 -4.2079096 -4.2226467 -4.2443581][-4.077733 -4.0512209 -4.06856 -4.0931177 -4.08745 -4.0524812 -3.9929585 -3.9450052 -3.9769382 -4.0690975 -4.1605868 -4.206912 -4.2159634 -4.230514 -4.2507458][-4.0899754 -4.0737185 -4.0989208 -4.1176858 -4.0820408 -4.016818 -3.9388342 -3.8774707 -3.9205427 -4.0396309 -4.1488018 -4.2050257 -4.2203822 -4.2370191 -4.2577658][-4.1048112 -4.0972157 -4.1229362 -4.1309204 -4.06911 -3.9755616 -3.8765883 -3.8098223 -3.8787909 -4.0233603 -4.1419611 -4.2024927 -4.2240577 -4.2433739 -4.264792][-4.1219649 -4.118958 -4.14354 -4.1414742 -4.0605488 -3.9400175 -3.8138273 -3.7470469 -3.8475213 -4.0118523 -4.1365166 -4.2009592 -4.2271328 -4.2483692 -4.2703648][-4.1328769 -4.133708 -4.1603594 -4.1575918 -4.0704241 -3.9397392 -3.8016212 -3.7375073 -3.8465087 -4.011939 -4.1374183 -4.2028403 -4.2312884 -4.2526383 -4.27484][-4.124887 -4.1294861 -4.1609931 -4.1681385 -4.0952878 -3.9810572 -3.8607907 -3.8057463 -3.896729 -4.040379 -4.1516957 -4.2096095 -4.2345128 -4.2555132 -4.2779946][-4.0973005 -4.10766 -4.1445022 -4.1603355 -4.1050839 -4.0147023 -3.9212246 -3.8797212 -3.9538333 -4.074265 -4.1670742 -4.2130852 -4.2337914 -4.2549629 -4.277566][-4.0891976 -4.0978136 -4.1302238 -4.1454725 -4.1014566 -4.0254378 -3.9453256 -3.9140384 -3.9789021 -4.0859771 -4.1652145 -4.2046242 -4.2239723 -4.2455182 -4.2698727][-4.1128268 -4.1162858 -4.1405196 -4.1529694 -4.1127548 -4.0403004 -3.9675612 -3.9400628 -3.9934397 -4.0813107 -4.1478157 -4.1857529 -4.2074671 -4.2319551 -4.259038][-4.1420546 -4.1425724 -4.1601009 -4.1700711 -4.1317382 -4.0671678 -4.0096865 -3.9874561 -4.0202432 -4.0779381 -4.1285844 -4.16491 -4.192205 -4.2216239 -4.2496104][-4.1659794 -4.160316 -4.1696167 -4.1718311 -4.1339927 -4.0859818 -4.0517282 -4.0382762 -4.0539579 -4.0844545 -4.1169276 -4.1460118 -4.1778369 -4.2107615 -4.2388091][-4.1871824 -4.1741986 -4.1732016 -4.1672692 -4.1312203 -4.0976043 -4.0801773 -4.0754437 -4.0845914 -4.1011891 -4.1197348 -4.1399117 -4.1699591 -4.202888 -4.2322221]]...]
INFO - root - 2017-12-07 22:40:54.804668: step 62410, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 50h:37m:21s remains)
INFO - root - 2017-12-07 22:41:01.482142: step 62420, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 47h:11m:14s remains)
INFO - root - 2017-12-07 22:41:08.180715: step 62430, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 48h:05m:19s remains)
INFO - root - 2017-12-07 22:41:15.017398: step 62440, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 51h:51m:48s remains)
INFO - root - 2017-12-07 22:41:21.965782: step 62450, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.739 sec/batch; 55h:25m:32s remains)
INFO - root - 2017-12-07 22:41:28.778036: step 62460, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 51h:21m:48s remains)
INFO - root - 2017-12-07 22:41:35.641403: step 62470, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.660 sec/batch; 49h:30m:23s remains)
INFO - root - 2017-12-07 22:41:42.431462: step 62480, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.654 sec/batch; 49h:04m:56s remains)
INFO - root - 2017-12-07 22:41:49.158733: step 62490, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 51h:07m:06s remains)
INFO - root - 2017-12-07 22:41:56.002223: step 62500, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 53h:25m:42s remains)
2017-12-07 22:41:56.714559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3134184 -4.2726068 -4.213192 -4.14475 -4.1041608 -4.1139908 -4.1555023 -4.2140355 -4.2676158 -4.297461 -4.2983861 -4.2753282 -4.2445245 -4.2184157 -4.1893392][-4.2798357 -4.2476716 -4.2085419 -4.1617732 -4.1355414 -4.1388845 -4.1609626 -4.2019496 -4.2459707 -4.2724366 -4.2712932 -4.2456665 -4.2175193 -4.1991773 -4.1812649][-4.2316909 -4.2193694 -4.2088213 -4.1896276 -4.1770349 -4.1744413 -4.1759496 -4.1926303 -4.224288 -4.2481513 -4.2476673 -4.2239609 -4.1987815 -4.1838775 -4.1751442][-4.1740613 -4.1883926 -4.2115602 -4.2174687 -4.2121029 -4.2018032 -4.1827974 -4.1752791 -4.19669 -4.2264519 -4.2355285 -4.2226057 -4.2004 -4.1798773 -4.1658931][-4.1443877 -4.1822 -4.2277775 -4.2429543 -4.2290587 -4.1985288 -4.1559372 -4.1265569 -4.143909 -4.1885128 -4.2203217 -4.2312889 -4.2192097 -4.1925306 -4.17078][-4.1650343 -4.2135506 -4.2580876 -4.2632327 -4.2277846 -4.1655407 -4.0946169 -4.0396056 -4.0576286 -4.1315403 -4.1952386 -4.2312274 -4.2360063 -4.21448 -4.1914477][-4.1972 -4.2429862 -4.2742066 -4.2616959 -4.2031221 -4.1088595 -3.991689 -3.8934162 -3.9150932 -4.039227 -4.1512465 -4.2150922 -4.2373528 -4.229053 -4.2135162][-4.2039108 -4.245378 -4.2685308 -4.2462492 -4.1737204 -4.052217 -3.8926141 -3.7560933 -3.7852426 -3.9609349 -4.1149898 -4.1987967 -4.2346277 -4.233295 -4.2179971][-4.1810746 -4.2130051 -4.239398 -4.2250466 -4.16323 -4.0582471 -3.9285722 -3.8274322 -3.8565483 -4.0090561 -4.1421409 -4.2123423 -4.2440825 -4.2385154 -4.2107916][-4.1522484 -4.1721621 -4.2038417 -4.2083225 -4.1771288 -4.11822 -4.0463982 -3.9958603 -4.0103469 -4.1064968 -4.1951632 -4.24215 -4.2596912 -4.2453728 -4.2058582][-4.1631746 -4.1729703 -4.2021408 -4.219974 -4.2121916 -4.1900935 -4.1608133 -4.1372886 -4.1381874 -4.191555 -4.2480149 -4.2735853 -4.27609 -4.2519469 -4.2053123][-4.2010608 -4.2059259 -4.2273731 -4.246624 -4.2466688 -4.2392612 -4.2335377 -4.226706 -4.2246881 -4.2523375 -4.2852063 -4.2934518 -4.2909932 -4.2694731 -4.2304873][-4.2397051 -4.2407532 -4.2532034 -4.2624254 -4.2582474 -4.2528009 -4.25607 -4.2599297 -4.2590351 -4.270813 -4.2870502 -4.292243 -4.2966094 -4.2892609 -4.2652144][-4.2607245 -4.2577786 -4.2627454 -4.2649751 -4.2602744 -4.2530775 -4.2554374 -4.2624497 -4.2640996 -4.2667727 -4.2738156 -4.2827239 -4.2951283 -4.2998981 -4.2918043][-4.259922 -4.2581377 -4.2625947 -4.2648177 -4.2606111 -4.2525206 -4.251966 -4.2598314 -4.2632718 -4.261713 -4.2643504 -4.2748871 -4.2901297 -4.3017735 -4.3055711]]...]
INFO - root - 2017-12-07 22:42:03.261635: step 62510, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 47h:13m:56s remains)
INFO - root - 2017-12-07 22:42:10.108377: step 62520, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 53h:58m:45s remains)
INFO - root - 2017-12-07 22:42:16.942186: step 62530, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 51h:32m:19s remains)
INFO - root - 2017-12-07 22:42:23.765995: step 62540, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 53h:06m:52s remains)
INFO - root - 2017-12-07 22:42:30.569792: step 62550, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 47h:14m:11s remains)
INFO - root - 2017-12-07 22:42:37.375522: step 62560, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.671 sec/batch; 50h:19m:44s remains)
INFO - root - 2017-12-07 22:42:44.204997: step 62570, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 53h:23m:16s remains)
INFO - root - 2017-12-07 22:42:51.055762: step 62580, loss = 2.06, batch loss = 2.01 (10.6 examples/sec; 0.754 sec/batch; 56h:31m:37s remains)
INFO - root - 2017-12-07 22:42:57.570508: step 62590, loss = 2.10, batch loss = 2.05 (12.6 examples/sec; 0.633 sec/batch; 47h:28m:51s remains)
INFO - root - 2017-12-07 22:43:04.472337: step 62600, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 49h:55m:23s remains)
2017-12-07 22:43:05.326980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19398 -4.1850476 -4.190618 -4.190546 -4.1761293 -4.1636467 -4.1656022 -4.1736526 -4.1958432 -4.2170792 -4.2260976 -4.2331839 -4.2332482 -4.2162027 -4.172575][-4.1856132 -4.1680293 -4.176053 -4.1798654 -4.1674371 -4.1503353 -4.1450276 -4.1531081 -4.1762185 -4.1969137 -4.204155 -4.2148929 -4.2243195 -4.2235155 -4.1966481][-4.199708 -4.1815257 -4.1838069 -4.1812797 -4.1660576 -4.139781 -4.1218448 -4.1271319 -4.1475153 -4.17159 -4.1839995 -4.2039328 -4.2245183 -4.2358418 -4.2239108][-4.2384624 -4.2215033 -4.21636 -4.2012782 -4.1719275 -4.1258655 -4.0859957 -4.0893779 -4.1151004 -4.148057 -4.1731019 -4.1966763 -4.2206717 -4.2394414 -4.2402577][-4.2867417 -4.2706447 -4.257412 -4.2270508 -4.174521 -4.1012468 -4.04666 -4.0547404 -4.0947647 -4.1366873 -4.168787 -4.1942258 -4.2199349 -4.2486668 -4.2628546][-4.3207631 -4.3060584 -4.283318 -4.2367 -4.1618862 -4.0720253 -4.01597 -4.0315366 -4.0817261 -4.1293855 -4.1702204 -4.2042394 -4.2350926 -4.2631731 -4.2829061][-4.3314786 -4.3160615 -4.2871728 -4.2322488 -4.1485944 -4.0563111 -3.9978886 -4.0139422 -4.0676584 -4.1240973 -4.1777172 -4.2233944 -4.26076 -4.2842445 -4.3006253][-4.32877 -4.3144298 -4.2819533 -4.2263713 -4.1476145 -4.057785 -3.9930253 -4.0055132 -4.0622349 -4.1265941 -4.1931186 -4.2454858 -4.2828922 -4.303062 -4.3152862][-4.3230286 -4.3125448 -4.2837882 -4.233901 -4.1630197 -4.07461 -4.0019169 -4.0086155 -4.0648003 -4.1332345 -4.2079439 -4.2609329 -4.2981906 -4.3179679 -4.32898][-4.3162723 -4.3054652 -4.2819824 -4.2432318 -4.184629 -4.1011438 -4.033546 -4.043139 -4.0992341 -4.1625981 -4.2303176 -4.2756276 -4.3087363 -4.3281283 -4.337059][-4.3042459 -4.2899237 -4.2722492 -4.2480249 -4.2084656 -4.1435161 -4.0899587 -4.1045246 -4.1570373 -4.2090025 -4.2584534 -4.2879333 -4.3107429 -4.3292618 -4.3391566][-4.2885942 -4.2718716 -4.2612305 -4.2490954 -4.2275972 -4.1831388 -4.1444964 -4.1598687 -4.2032185 -4.2415247 -4.2757115 -4.2935605 -4.3075013 -4.3263965 -4.3396978][-4.2707729 -4.2567344 -4.2526174 -4.2530246 -4.2505136 -4.2268496 -4.1985421 -4.2024746 -4.2289948 -4.2560811 -4.2814264 -4.2929544 -4.3032551 -4.3241191 -4.3404813][-4.259881 -4.2494307 -4.2477055 -4.25629 -4.2674379 -4.2603459 -4.2408876 -4.2328849 -4.2421045 -4.2594357 -4.2812357 -4.2928982 -4.3047638 -4.3249497 -4.3408704][-4.2544308 -4.24781 -4.2511873 -4.2612338 -4.2767129 -4.2779098 -4.266798 -4.2546086 -4.2550955 -4.2677875 -4.2853117 -4.2974882 -4.309967 -4.3279386 -4.3399444]]...]
INFO - root - 2017-12-07 22:43:11.997301: step 62610, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.633 sec/batch; 47h:29m:05s remains)
INFO - root - 2017-12-07 22:43:18.675727: step 62620, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:41m:46s remains)
INFO - root - 2017-12-07 22:43:25.380039: step 62630, loss = 2.03, batch loss = 1.97 (11.8 examples/sec; 0.677 sec/batch; 50h:46m:14s remains)
INFO - root - 2017-12-07 22:43:32.206292: step 62640, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.710 sec/batch; 53h:15m:15s remains)
INFO - root - 2017-12-07 22:43:39.018506: step 62650, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 53h:52m:04s remains)
INFO - root - 2017-12-07 22:43:45.767239: step 62660, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:42m:51s remains)
INFO - root - 2017-12-07 22:43:52.619349: step 62670, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 51h:22m:54s remains)
INFO - root - 2017-12-07 22:43:59.560010: step 62680, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.683 sec/batch; 51h:10m:59s remains)
INFO - root - 2017-12-07 22:44:06.399887: step 62690, loss = 2.08, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 52h:45m:26s remains)
INFO - root - 2017-12-07 22:44:13.228496: step 62700, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 49h:33m:53s remains)
2017-12-07 22:44:13.954421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1498747 -4.10997 -4.0927658 -4.1080785 -4.1425261 -4.1620026 -4.1597738 -4.1503921 -4.138998 -4.1382551 -4.1313133 -4.1127481 -4.0891762 -4.0709743 -4.0684547][-4.1457605 -4.1111059 -4.095562 -4.1085625 -4.1379218 -4.158226 -4.1607337 -4.1601019 -4.1588216 -4.1649413 -4.1596375 -4.1280904 -4.0891466 -4.0641422 -4.0591197][-4.1488938 -4.1209073 -4.1128893 -4.1267052 -4.1504359 -4.1679144 -4.1748695 -4.1813326 -4.1830697 -4.1880245 -4.1757526 -4.1300383 -4.0828466 -4.0528545 -4.0412245][-4.1612449 -4.1412606 -4.1369114 -4.1430416 -4.1521111 -4.155241 -4.1581144 -4.1670022 -4.1710653 -4.1757493 -4.1635346 -4.1167011 -4.0705347 -4.0366035 -4.0187922][-4.1676707 -4.1510911 -4.1401305 -4.1232767 -4.0998049 -4.0727334 -4.0693521 -4.0908442 -4.1114554 -4.1274233 -4.1288471 -4.1001568 -4.0681806 -4.0375447 -4.0249658][-4.150178 -4.12817 -4.1024408 -4.0522118 -3.9841948 -3.9270637 -3.9234567 -3.9671123 -4.0166364 -4.0534453 -4.0804114 -4.0824323 -4.0734262 -4.0632091 -4.0698957][-4.1258473 -4.097394 -4.053761 -3.9738731 -3.8717282 -3.8069057 -3.8147969 -3.8722496 -3.939213 -3.9968679 -4.0502357 -4.0828938 -4.0964704 -4.1102262 -4.1364241][-4.10877 -4.0820947 -4.0365191 -3.962538 -3.8768728 -3.8314519 -3.8486981 -3.901052 -3.9631073 -4.0243826 -4.0819092 -4.1243067 -4.1489353 -4.1695428 -4.1955442][-4.1043253 -4.08172 -4.0477018 -4.0044751 -3.9588504 -3.9377561 -3.9570589 -3.9966295 -4.0429873 -4.0923667 -4.1374855 -4.1751685 -4.2021613 -4.2191939 -4.2339592][-4.1224928 -4.1076622 -4.0875111 -4.0684948 -4.0493641 -4.0443039 -4.0637465 -4.0946212 -4.1279554 -4.1640859 -4.1932917 -4.2198315 -4.2409778 -4.2493229 -4.2496762][-4.1386733 -4.1309643 -4.1261091 -4.1229005 -4.1179371 -4.1199608 -4.1398716 -4.1668711 -4.1914396 -4.2133141 -4.224648 -4.2331238 -4.2398539 -4.2372131 -4.2299423][-4.1446624 -4.145905 -4.1504025 -4.1524158 -4.1502318 -4.1538606 -4.1727948 -4.195888 -4.2109976 -4.2208652 -4.2214622 -4.2181053 -4.2150993 -4.2079206 -4.1998868][-4.1569118 -4.1597295 -4.1653113 -4.1682587 -4.1683922 -4.1716504 -4.1852589 -4.200563 -4.20814 -4.2111254 -4.2094836 -4.2068639 -4.2043829 -4.2007527 -4.1969433][-4.1642547 -4.1596909 -4.1596308 -4.1596503 -4.159987 -4.1625061 -4.1696906 -4.1766634 -4.1798511 -4.1810222 -4.1828671 -4.1861644 -4.1895981 -4.1922827 -4.1941133][-4.1697941 -4.1604238 -4.1572909 -4.1547456 -4.153738 -4.1547375 -4.1585603 -4.162394 -4.1646967 -4.1669765 -4.1707697 -4.176209 -4.1812792 -4.1851106 -4.1872077]]...]
INFO - root - 2017-12-07 22:44:20.497648: step 62710, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 50h:15m:46s remains)
INFO - root - 2017-12-07 22:44:27.335235: step 62720, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 52h:31m:42s remains)
INFO - root - 2017-12-07 22:44:34.094663: step 62730, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 49h:53m:36s remains)
INFO - root - 2017-12-07 22:44:40.868228: step 62740, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:41m:34s remains)
INFO - root - 2017-12-07 22:44:47.584436: step 62750, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 49h:59m:22s remains)
INFO - root - 2017-12-07 22:44:54.332384: step 62760, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 51h:23m:18s remains)
INFO - root - 2017-12-07 22:45:01.131800: step 62770, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.729 sec/batch; 54h:38m:51s remains)
INFO - root - 2017-12-07 22:45:07.971145: step 62780, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 51h:28m:52s remains)
INFO - root - 2017-12-07 22:45:14.814732: step 62790, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 49h:58m:40s remains)
INFO - root - 2017-12-07 22:45:21.590918: step 62800, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 52h:11m:12s remains)
2017-12-07 22:45:22.362936: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3437428 -4.3380508 -4.3346577 -4.33475 -4.3376293 -4.3410411 -4.3435493 -4.3468227 -4.3489227 -4.3497343 -4.3495269 -4.34824 -4.3486109 -4.3508835 -4.353024][-4.336525 -4.3277206 -4.3206615 -4.3161621 -4.3135705 -4.3158917 -4.3209682 -4.3288145 -4.33395 -4.3364043 -4.3368974 -4.3359756 -4.3374424 -4.3420372 -4.3476353][-4.3258791 -4.3128181 -4.3006129 -4.28565 -4.2726164 -4.271039 -4.2793741 -4.2930112 -4.3038211 -4.3102641 -4.3125572 -4.3129959 -4.3172708 -4.3268805 -4.3386416][-4.3173604 -4.2968574 -4.276473 -4.2480941 -4.2191868 -4.21057 -4.2244139 -4.2452536 -4.26346 -4.2763548 -4.2812991 -4.2827263 -4.2897892 -4.3064289 -4.326457][-4.3136039 -4.2854481 -4.2528195 -4.2081141 -4.1612911 -4.1441989 -4.1639576 -4.1923881 -4.2175074 -4.2365751 -4.24463 -4.2477679 -4.2574177 -4.2830744 -4.3132257][-4.3119817 -4.2786951 -4.232038 -4.1694508 -4.106741 -4.0843081 -4.1094017 -4.1407914 -4.1671438 -4.1877403 -4.1985168 -4.2052608 -4.2189307 -4.254993 -4.2967][-4.3120265 -4.276752 -4.2205148 -4.1438951 -4.069181 -4.0393891 -4.0650163 -4.0966406 -4.1180544 -4.1341739 -4.1452751 -4.1543345 -4.17333 -4.2206039 -4.2754745][-4.3046012 -4.2659016 -4.2076788 -4.1282878 -4.0455008 -4.0054445 -4.0257182 -4.055028 -4.0727129 -4.0874882 -4.1035023 -4.1190758 -4.1418986 -4.1936936 -4.256001][-4.2998371 -4.2648191 -4.2154713 -4.1464252 -4.0660105 -4.0188251 -4.0285611 -4.0479612 -4.0616851 -4.0803981 -4.1039214 -4.123961 -4.1466093 -4.1915154 -4.2492061][-4.3058629 -4.2814474 -4.2482209 -4.19784 -4.1284676 -4.0801935 -4.0749068 -4.0792708 -4.0859318 -4.1060724 -4.1323895 -4.1519752 -4.1692276 -4.2015853 -4.2489271][-4.3152628 -4.30276 -4.2858877 -4.2573295 -4.2040215 -4.157856 -4.1366754 -4.1208811 -4.1178689 -4.1389718 -4.1663694 -4.1860452 -4.2003527 -4.22183 -4.2579703][-4.3232303 -4.3178978 -4.3123512 -4.3000321 -4.2649331 -4.2255912 -4.1949949 -4.1680603 -4.1608582 -4.180223 -4.2075567 -4.2268419 -4.2386594 -4.2536054 -4.2792196][-4.3279052 -4.3266125 -4.328115 -4.3271122 -4.3110185 -4.2857056 -4.2578506 -4.2295222 -4.2198706 -4.2327361 -4.2531819 -4.2680979 -4.2764792 -4.2871604 -4.3039169][-4.3286777 -4.3298154 -4.3344278 -4.3397193 -4.337894 -4.3274031 -4.3100934 -4.2893691 -4.2799397 -4.2833166 -4.2906 -4.2968144 -4.3020129 -4.3109827 -4.3231421][-4.3325014 -4.3333941 -4.3368268 -4.34146 -4.3430057 -4.339642 -4.331738 -4.3214869 -4.3158669 -4.3153286 -4.3156333 -4.3155012 -4.3181138 -4.3258505 -4.3347859]]...]
INFO - root - 2017-12-07 22:45:28.973344: step 62810, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:40m:07s remains)
INFO - root - 2017-12-07 22:45:35.712418: step 62820, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 48h:24m:43s remains)
INFO - root - 2017-12-07 22:45:42.449702: step 62830, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 51h:28m:38s remains)
INFO - root - 2017-12-07 22:45:49.133323: step 62840, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 49h:18m:05s remains)
INFO - root - 2017-12-07 22:45:55.880894: step 62850, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.688 sec/batch; 51h:33m:14s remains)
INFO - root - 2017-12-07 22:46:02.617841: step 62860, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 51h:53m:08s remains)
INFO - root - 2017-12-07 22:46:09.427424: step 62870, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 49h:32m:21s remains)
INFO - root - 2017-12-07 22:46:16.216829: step 62880, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 49h:39m:04s remains)
INFO - root - 2017-12-07 22:46:23.055128: step 62890, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 53h:21m:20s remains)
INFO - root - 2017-12-07 22:46:29.696800: step 62900, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 49h:58m:24s remains)
2017-12-07 22:46:30.443597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2104144 -4.2160683 -4.2183452 -4.2126966 -4.2074003 -4.2058425 -4.184629 -4.1538672 -4.14535 -4.1721945 -4.2163062 -4.2652659 -4.3047876 -4.32866 -4.3380013][-4.2245646 -4.2299504 -4.2314668 -4.2224784 -4.2131433 -4.2073035 -4.1799622 -4.1488504 -4.143157 -4.1691628 -4.2138557 -4.2637653 -4.3035774 -4.3262339 -4.3349128][-4.2300773 -4.2338314 -4.2333179 -4.223249 -4.2091413 -4.1958656 -4.1600718 -4.1254563 -4.1248593 -4.1563473 -4.204946 -4.2586918 -4.2996778 -4.3221564 -4.3301191][-4.2318745 -4.2283816 -4.2191167 -4.2025042 -4.1832485 -4.1634 -4.1219511 -4.0881176 -4.0984473 -4.1425724 -4.1934643 -4.2489038 -4.2891545 -4.3123951 -4.3212051][-4.2292151 -4.2156053 -4.1935177 -4.1640911 -4.1386614 -4.1140208 -4.0685582 -4.0404553 -4.0636082 -4.115705 -4.1668653 -4.2252345 -4.2713566 -4.2984962 -4.3094363][-4.2307916 -4.2125449 -4.180604 -4.1365719 -4.0959215 -4.057816 -4.0075469 -3.9832036 -4.0136733 -4.06884 -4.1203604 -4.1867852 -4.245327 -4.2818747 -4.2978849][-4.2309241 -4.2159142 -4.182024 -4.1267557 -4.065413 -4.0076742 -3.9522183 -3.9295542 -3.9649448 -4.0228934 -4.0780959 -4.1522775 -4.2229519 -4.2678485 -4.2891655][-4.2409239 -4.2340422 -4.2016568 -4.136189 -4.0537872 -3.9775047 -3.9204254 -3.9065497 -3.9506907 -4.0139761 -4.0753965 -4.1495185 -4.2200108 -4.265162 -4.2881088][-4.2689009 -4.2697597 -4.2345142 -4.1562843 -4.0548 -3.9657507 -3.9121373 -3.9130664 -3.966989 -4.0389366 -4.1082654 -4.1789641 -4.239665 -4.2763619 -4.2982664][-4.3022757 -4.30519 -4.269279 -4.1878171 -4.079504 -3.9867892 -3.9342012 -3.9374704 -3.9947565 -4.074964 -4.1531763 -4.2190509 -4.2692842 -4.298584 -4.3173976][-4.3250227 -4.3216972 -4.2822433 -4.2022667 -4.0966597 -4.0054588 -3.9572628 -3.9619279 -4.0222764 -4.1118774 -4.1985683 -4.2596507 -4.3004146 -4.3242888 -4.3377972][-4.3318849 -4.3149195 -4.2645922 -4.178628 -4.0747666 -3.9872594 -3.9432535 -3.9542491 -4.0232015 -4.1273623 -4.2237377 -4.2833385 -4.3201165 -4.3406239 -4.3506427][-4.321 -4.2884536 -4.2241368 -4.1292052 -4.0277634 -3.9515 -3.9180026 -3.9386036 -4.0165744 -4.1302118 -4.229888 -4.2894363 -4.3253989 -4.3450966 -4.3551521][-4.3002372 -4.2585359 -4.1879187 -4.0944619 -4.0085096 -3.9539878 -3.9363055 -3.9636297 -4.0398083 -4.1447229 -4.2345185 -4.2889876 -4.3231931 -4.3427982 -4.3536863][-4.2835655 -4.24285 -4.1771975 -4.1004257 -4.0399518 -4.0087752 -4.0055285 -4.034091 -4.0967112 -4.1789651 -4.2488647 -4.2917027 -4.3206129 -4.3388467 -4.3507595]]...]
INFO - root - 2017-12-07 22:46:37.106895: step 62910, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 54h:22m:04s remains)
INFO - root - 2017-12-07 22:46:43.867588: step 62920, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 51h:32m:06s remains)
INFO - root - 2017-12-07 22:46:50.683505: step 62930, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 47h:27m:37s remains)
INFO - root - 2017-12-07 22:46:57.584904: step 62940, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:20m:16s remains)
INFO - root - 2017-12-07 22:47:04.411520: step 62950, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 50h:35m:07s remains)
INFO - root - 2017-12-07 22:47:11.088842: step 62960, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 51h:18m:02s remains)
INFO - root - 2017-12-07 22:47:17.766525: step 62970, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 51h:12m:31s remains)
INFO - root - 2017-12-07 22:47:24.470601: step 62980, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 46h:48m:02s remains)
INFO - root - 2017-12-07 22:47:31.244449: step 62990, loss = 2.11, batch loss = 2.05 (12.6 examples/sec; 0.632 sec/batch; 47h:20m:45s remains)
INFO - root - 2017-12-07 22:47:38.125447: step 63000, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 51h:17m:56s remains)
2017-12-07 22:47:38.925128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1763744 -4.1614037 -4.1745682 -4.1970663 -4.1922493 -4.1754413 -4.1671839 -4.1755137 -4.1793871 -4.1706877 -4.1704636 -4.1875887 -4.1857114 -4.1724653 -4.1702991][-4.164372 -4.1614685 -4.1791244 -4.1953783 -4.1848888 -4.1658244 -4.1542692 -4.1616182 -4.1678424 -4.1622405 -4.1612883 -4.1796188 -4.1777372 -4.1634593 -4.1647015][-4.1688423 -4.1747637 -4.190237 -4.1992741 -4.1860557 -4.1661334 -4.1433825 -4.1429224 -4.1554575 -4.164959 -4.1710496 -4.1879368 -4.1841497 -4.1665349 -4.1616368][-4.1559792 -4.1636248 -4.172657 -4.1793709 -4.1726055 -4.1541424 -4.1227665 -4.1087976 -4.1233206 -4.1519194 -4.1736379 -4.1906762 -4.1820831 -4.1559949 -4.1382036][-4.1355581 -4.1426682 -4.1482911 -4.1600413 -4.1611543 -4.1399317 -4.0941129 -4.0589838 -4.0745993 -4.124629 -4.1638827 -4.1797895 -4.156713 -4.1164436 -4.0895004][-4.1243892 -4.1235523 -4.1246781 -4.1429605 -4.1488423 -4.116066 -4.0466962 -3.9802284 -3.98952 -4.0603924 -4.115077 -4.1280646 -4.0938315 -4.0550122 -4.0428348][-4.1380768 -4.1256924 -4.1185222 -4.1305504 -4.1260309 -4.0741954 -3.9724238 -3.8665528 -3.8697071 -3.966363 -4.0375719 -4.0552492 -4.0332689 -4.0211105 -4.0386281][-4.1805916 -4.1584358 -4.1395311 -4.1376371 -4.1171103 -4.0525031 -3.9380784 -3.8280339 -3.8353219 -3.9361506 -4.0069904 -4.0264382 -4.0244927 -4.0413065 -4.08411][-4.2237759 -4.2008948 -4.1819105 -4.1745729 -4.1534696 -4.0969753 -4.0093546 -3.938554 -3.9416103 -4.0023918 -4.0413971 -4.0481739 -4.0556154 -4.0896192 -4.1469603][-4.2304363 -4.2137055 -4.2038336 -4.1980424 -4.1826057 -4.1432571 -4.0862632 -4.0461149 -4.0438633 -4.0677776 -4.0735044 -4.0670471 -4.0802178 -4.1256547 -4.1828117][-4.2091131 -4.1940646 -4.191153 -4.191411 -4.1813273 -4.1515822 -4.114521 -4.0989027 -4.099925 -4.1035519 -4.0922074 -4.0832858 -4.1012354 -4.1426268 -4.1814041][-4.18828 -4.1693635 -4.1687946 -4.1770554 -4.171051 -4.1491332 -4.1312685 -4.1370482 -4.1455178 -4.1404414 -4.1230979 -4.1153393 -4.1294346 -4.1566949 -4.171773][-4.1875582 -4.1689425 -4.16973 -4.1803312 -4.1766872 -4.1645551 -4.1620159 -4.1745963 -4.1833568 -4.1739483 -4.158483 -4.154273 -4.1637416 -4.1777992 -4.1793346][-4.2002888 -4.1849489 -4.1842589 -4.1936722 -4.192986 -4.1888938 -4.1915627 -4.2009068 -4.2059941 -4.1993494 -4.1889758 -4.1885338 -4.1931639 -4.1999674 -4.1982064][-4.2305226 -4.218534 -4.215363 -4.2196946 -4.2197123 -4.2200785 -4.2240195 -4.2294226 -4.2328038 -4.230617 -4.2269382 -4.2272434 -4.2286572 -4.2313046 -4.2299304]]...]
INFO - root - 2017-12-07 22:47:45.469572: step 63010, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 47h:45m:52s remains)
INFO - root - 2017-12-07 22:47:52.314216: step 63020, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.712 sec/batch; 53h:19m:29s remains)
INFO - root - 2017-12-07 22:47:59.138962: step 63030, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 53h:59m:51s remains)
INFO - root - 2017-12-07 22:48:06.005237: step 63040, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 50h:52m:19s remains)
INFO - root - 2017-12-07 22:48:12.767649: step 63050, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 47h:21m:01s remains)
INFO - root - 2017-12-07 22:48:19.607493: step 63060, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.644 sec/batch; 48h:11m:47s remains)
INFO - root - 2017-12-07 22:48:26.470167: step 63070, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.679 sec/batch; 50h:48m:42s remains)
INFO - root - 2017-12-07 22:48:33.246601: step 63080, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 52h:24m:04s remains)
INFO - root - 2017-12-07 22:48:39.995096: step 63090, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 52h:14m:07s remains)
INFO - root - 2017-12-07 22:48:46.770838: step 63100, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 49h:16m:48s remains)
2017-12-07 22:48:47.424565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2198749 -4.21729 -4.2210712 -4.2146969 -4.2236047 -4.2266903 -4.213026 -4.1920085 -4.1839414 -4.189002 -4.1938996 -4.2178812 -4.25609 -4.2885356 -4.3098726][-4.1789389 -4.1831446 -4.1952848 -4.1921849 -4.1987591 -4.1961775 -4.1841516 -4.1658859 -4.1603179 -4.1711326 -4.1787596 -4.2061048 -4.2446566 -4.2787337 -4.300189][-4.1456208 -4.1590095 -4.1813731 -4.1865778 -4.1892815 -4.1827254 -4.1689639 -4.1492019 -4.1448007 -4.156364 -4.1642842 -4.1938567 -4.2316461 -4.2674112 -4.2907891][-4.1408949 -4.1671906 -4.1977849 -4.2071333 -4.2017884 -4.1873722 -4.1756768 -4.1565866 -4.1510081 -4.159461 -4.1695509 -4.1983151 -4.2301159 -4.2623415 -4.2870693][-4.1855559 -4.213624 -4.2388792 -4.2448821 -4.231678 -4.2039638 -4.1846871 -4.1690531 -4.1676335 -4.1753139 -4.1883903 -4.2120776 -4.2388835 -4.2679772 -4.2912817][-4.2401838 -4.2607679 -4.2699828 -4.2555938 -4.2180824 -4.1650214 -4.1282921 -4.1220222 -4.1494212 -4.1752262 -4.19711 -4.2215466 -4.2511082 -4.2777271 -4.2974963][-4.2627282 -4.2660117 -4.2506328 -4.2092028 -4.1377459 -4.0467854 -3.9857371 -4.0004544 -4.0809736 -4.1487179 -4.1936417 -4.2269316 -4.2614818 -4.2853756 -4.3008728][-4.25517 -4.2440391 -4.2035761 -4.13301 -4.0279951 -3.8923807 -3.7999294 -3.8423567 -3.9840913 -4.1035147 -4.1748486 -4.2205105 -4.2558074 -4.2793045 -4.29855][-4.2251019 -4.2081952 -4.1659317 -4.0950122 -3.9960334 -3.8636093 -3.7699749 -3.8173854 -3.9691749 -4.099968 -4.1767278 -4.2236285 -4.2573314 -4.2777596 -4.2966337][-4.1994328 -4.1858234 -4.1591845 -4.1179376 -4.06489 -3.9936728 -3.9422927 -3.9701061 -4.0677381 -4.1593719 -4.2141523 -4.2476687 -4.273828 -4.290205 -4.302093][-4.2182336 -4.2103553 -4.2015405 -4.1839185 -4.1638088 -4.136097 -4.1129589 -4.1217508 -4.1696925 -4.2207808 -4.2536397 -4.272429 -4.2881904 -4.3005481 -4.3075795][-4.23462 -4.2318487 -4.2358842 -4.2355757 -4.2352543 -4.22896 -4.2184587 -4.2156272 -4.2334752 -4.2574253 -4.2732983 -4.2848382 -4.2944374 -4.3045444 -4.3084488][-4.2295461 -4.2311454 -4.2436681 -4.2513475 -4.2626872 -4.2680707 -4.2657986 -4.2628546 -4.2701902 -4.2798095 -4.28594 -4.2920203 -4.2967076 -4.3051462 -4.3087707][-4.214478 -4.2206793 -4.2367077 -4.2484574 -4.2622366 -4.2708745 -4.2736311 -4.2745652 -4.2798295 -4.2860937 -4.2900891 -4.2940121 -4.2966223 -4.30285 -4.3077435][-4.21391 -4.2183337 -4.2324691 -4.2426405 -4.2538066 -4.2599626 -4.2631292 -4.2647529 -4.2700047 -4.2778721 -4.2851973 -4.2923703 -4.2965708 -4.302855 -4.3098869]]...]
INFO - root - 2017-12-07 22:48:53.923147: step 63110, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 51h:56m:41s remains)
INFO - root - 2017-12-07 22:49:00.622816: step 63120, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 51h:09m:13s remains)
INFO - root - 2017-12-07 22:49:07.620950: step 63130, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 48h:51m:40s remains)
INFO - root - 2017-12-07 22:49:14.536844: step 63140, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 49h:15m:39s remains)
INFO - root - 2017-12-07 22:49:21.380749: step 63150, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 54h:29m:06s remains)
INFO - root - 2017-12-07 22:49:28.183542: step 63160, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 52h:05m:58s remains)
INFO - root - 2017-12-07 22:49:35.083334: step 63170, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 51h:06m:15s remains)
INFO - root - 2017-12-07 22:49:41.852398: step 63180, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 48h:54m:53s remains)
INFO - root - 2017-12-07 22:49:48.776553: step 63190, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 53h:29m:17s remains)
INFO - root - 2017-12-07 22:49:55.644948: step 63200, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.717 sec/batch; 53h:38m:22s remains)
2017-12-07 22:49:56.304929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3289 -4.3211861 -4.3227758 -4.3185019 -4.3107862 -4.3174782 -4.3267961 -4.3363924 -4.343 -4.3411489 -4.3357444 -4.3296475 -4.3237205 -4.3131094 -4.284399][-4.32545 -4.3083739 -4.3016181 -4.2946935 -4.2865171 -4.2932458 -4.3028693 -4.3162055 -4.3293772 -4.3296289 -4.3236046 -4.3153238 -4.3078547 -4.2958465 -4.2614231][-4.3109837 -4.2854834 -4.26728 -4.2499681 -4.2399774 -4.248817 -4.2582006 -4.2718654 -4.2886052 -4.2928171 -4.2875042 -4.2827229 -4.2780981 -4.2670393 -4.2330022][-4.2964926 -4.2694745 -4.2427568 -4.2091436 -4.1909828 -4.1971354 -4.2011266 -4.2087703 -4.2279735 -4.2396526 -4.2401 -4.2432351 -4.247457 -4.2442937 -4.2203054][-4.2985926 -4.2734342 -4.2406545 -4.1979942 -4.1719456 -4.1676016 -4.1566772 -4.1509819 -4.1689448 -4.1912894 -4.2022963 -4.2164221 -4.2341123 -4.2438831 -4.2355509][-4.2911539 -4.2585478 -4.2183232 -4.1726789 -4.1493526 -4.1368833 -4.1067209 -4.0816107 -4.0966272 -4.140708 -4.1736879 -4.2021012 -4.233521 -4.2551546 -4.2610316][-4.2723308 -4.2247243 -4.1714845 -4.1205926 -4.09281 -4.0631461 -4.0026011 -3.9403479 -3.951138 -4.0382805 -4.1120586 -4.1640997 -4.2156 -4.2522817 -4.2719989][-4.261034 -4.1998405 -4.1255703 -4.0561371 -4.0092173 -3.958545 -3.8648038 -3.7534232 -3.7581418 -3.900136 -4.0248132 -4.1059623 -4.1768312 -4.2279563 -4.2600522][-4.2750983 -4.2236919 -4.15653 -4.082664 -4.0194445 -3.9604621 -3.8676167 -3.7571011 -3.7525434 -3.883739 -4.0099063 -4.0914736 -4.16599 -4.2189603 -4.2519722][-4.2909894 -4.2599216 -4.2216206 -4.1680455 -4.1082091 -4.0557547 -3.9915183 -3.9212077 -3.9144058 -3.9914997 -4.06894 -4.1275415 -4.192574 -4.2387533 -4.2637134][-4.2993402 -4.2839413 -4.2655058 -4.2341123 -4.1906562 -4.1517706 -4.1120515 -4.0739374 -4.07294 -4.1111531 -4.1476083 -4.1880136 -4.2413969 -4.2798023 -4.2965221][-4.3061047 -4.2993946 -4.2931371 -4.27684 -4.2478323 -4.22241 -4.1975026 -4.1735487 -4.1713958 -4.1888709 -4.2105517 -4.244853 -4.2884927 -4.3177743 -4.3281279][-4.3153381 -4.31351 -4.3147383 -4.3087177 -4.2938085 -4.2796617 -4.2634482 -4.2485962 -4.2436934 -4.2471447 -4.258019 -4.281981 -4.3118024 -4.3319817 -4.3394728][-4.3208041 -4.3209591 -4.3245864 -4.3243465 -4.3211122 -4.3158622 -4.30499 -4.2974172 -4.2944279 -4.2938519 -4.2959414 -4.3048687 -4.3191409 -4.329752 -4.3338647][-4.3191729 -4.3191819 -4.3213558 -4.3218117 -4.3233151 -4.3243723 -4.3190079 -4.3170424 -4.3183284 -4.3207755 -4.3203688 -4.320045 -4.3234305 -4.3262129 -4.3270011]]...]
INFO - root - 2017-12-07 22:50:02.928744: step 63210, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 52h:39m:28s remains)
INFO - root - 2017-12-07 22:50:09.776353: step 63220, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.741 sec/batch; 55h:26m:07s remains)
INFO - root - 2017-12-07 22:50:16.688421: step 63230, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 52h:49m:24s remains)
INFO - root - 2017-12-07 22:50:23.491544: step 63240, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:44m:00s remains)
INFO - root - 2017-12-07 22:50:30.375361: step 63250, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 51h:15m:38s remains)
INFO - root - 2017-12-07 22:50:37.153603: step 63260, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 51h:08m:58s remains)
INFO - root - 2017-12-07 22:50:43.928914: step 63270, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 50h:12m:49s remains)
INFO - root - 2017-12-07 22:50:50.721432: step 63280, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 50h:27m:43s remains)
INFO - root - 2017-12-07 22:50:57.471108: step 63290, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.629 sec/batch; 47h:04m:23s remains)
INFO - root - 2017-12-07 22:51:04.296293: step 63300, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.640 sec/batch; 47h:53m:34s remains)
2017-12-07 22:51:05.061396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.305697 -4.3070803 -4.3087573 -4.3098869 -4.310143 -4.3094754 -4.3077388 -4.3062677 -4.3060403 -4.3056664 -4.3039827 -4.3013387 -4.2987227 -4.296855 -4.2960892][-4.3080025 -4.3090711 -4.3108592 -4.311975 -4.3126416 -4.3127828 -4.3119955 -4.3103566 -4.3092189 -4.3084373 -4.3072762 -4.3057 -4.3043127 -4.3034859 -4.3031921][-4.3076668 -4.3077412 -4.3066 -4.3044438 -4.302556 -4.3026743 -4.3040891 -4.305305 -4.3060832 -4.3052492 -4.3030515 -4.3004508 -4.2991247 -4.300251 -4.3024173][-4.2983127 -4.3009658 -4.2944336 -4.2839017 -4.275815 -4.2734451 -4.276319 -4.2830181 -4.2911196 -4.2951512 -4.2917533 -4.2855687 -4.2805929 -4.2813325 -4.2865591][-4.2710776 -4.2807961 -4.2696419 -4.2457728 -4.2246914 -4.217288 -4.2218804 -4.2367396 -4.2565575 -4.2718315 -4.2699189 -4.2568746 -4.2435207 -4.2405419 -4.2470613][-4.2244954 -4.2436237 -4.2287846 -4.1873302 -4.1442537 -4.1258597 -4.1343369 -4.1632595 -4.1999269 -4.2301664 -4.2321596 -4.2088394 -4.1803212 -4.1687679 -4.1774192][-4.1710486 -4.2017179 -4.1871066 -4.1283636 -4.0556593 -4.0128736 -4.0214639 -4.0699186 -4.1305213 -4.1792617 -4.1880045 -4.1540151 -4.1021004 -4.06922 -4.0772843][-4.1312304 -4.1752467 -4.1701937 -4.1084986 -4.0177541 -3.946243 -3.9378855 -3.9920585 -4.0739136 -4.1424685 -4.1614556 -4.123776 -4.0506182 -3.9849181 -3.9736211][-4.124279 -4.17096 -4.1786652 -4.1301727 -4.0469775 -3.9716256 -3.9451251 -3.9834991 -4.0650764 -4.1423292 -4.1734285 -4.1448545 -4.0699625 -3.9835269 -3.9392271][-4.1477852 -4.18009 -4.18989 -4.1604433 -4.10087 -4.0450082 -4.0231605 -4.046793 -4.1032839 -4.1665173 -4.1985946 -4.1808987 -4.1210141 -4.0444174 -3.9931192][-4.1917243 -4.2029076 -4.2042761 -4.1868944 -4.1506724 -4.1167321 -4.1076026 -4.1277561 -4.163693 -4.2020531 -4.2196426 -4.2055244 -4.1650338 -4.1118584 -4.0758262][-4.2253113 -4.2224717 -4.2171969 -4.2053142 -4.1844034 -4.1678371 -4.1688375 -4.1864648 -4.2097397 -4.2279897 -4.2291951 -4.2129951 -4.188457 -4.1601472 -4.1421924][-4.2405543 -4.2296944 -4.223156 -4.2150869 -4.2044125 -4.2025552 -4.2129259 -4.2283831 -4.236804 -4.2375512 -4.2286777 -4.213841 -4.2013531 -4.1924653 -4.1898832][-4.2519512 -4.2371783 -4.2287841 -4.2225976 -4.2214227 -4.2305937 -4.247076 -4.2564669 -4.24842 -4.2317362 -4.214817 -4.2043543 -4.2067122 -4.2155213 -4.2247376][-4.2682118 -4.2506361 -4.2377267 -4.2296515 -4.2331758 -4.2462897 -4.2637596 -4.2654505 -4.2451158 -4.2166781 -4.1940823 -4.1903086 -4.20688 -4.2301679 -4.248898]]...]
INFO - root - 2017-12-07 22:51:11.831683: step 63310, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 54h:11m:19s remains)
INFO - root - 2017-12-07 22:51:18.512287: step 63320, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 47h:10m:16s remains)
INFO - root - 2017-12-07 22:51:25.349705: step 63330, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 47h:57m:26s remains)
INFO - root - 2017-12-07 22:51:32.080125: step 63340, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 52h:29m:28s remains)
INFO - root - 2017-12-07 22:51:38.922437: step 63350, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 52h:51m:28s remains)
INFO - root - 2017-12-07 22:51:45.719737: step 63360, loss = 2.03, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 51h:21m:25s remains)
INFO - root - 2017-12-07 22:51:52.599926: step 63370, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 48h:30m:20s remains)
INFO - root - 2017-12-07 22:51:59.477041: step 63380, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 52h:06m:41s remains)
INFO - root - 2017-12-07 22:52:06.303269: step 63390, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 53h:26m:40s remains)
INFO - root - 2017-12-07 22:52:13.043795: step 63400, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 53h:41m:10s remains)
2017-12-07 22:52:13.875647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2394891 -4.2846737 -4.3084812 -4.3207831 -4.3213859 -4.3099966 -4.2897987 -4.2645531 -4.2518024 -4.2527452 -4.2618666 -4.2795219 -4.3001151 -4.313798 -4.3204718][-4.264257 -4.3020883 -4.3218288 -4.3329945 -4.3328476 -4.3267193 -4.3154793 -4.3028817 -4.2977061 -4.2990422 -4.3034148 -4.3122787 -4.3212042 -4.32666 -4.3306127][-4.2864285 -4.312161 -4.3220892 -4.3245993 -4.32016 -4.3129897 -4.3034248 -4.2950225 -4.2920456 -4.295001 -4.2999406 -4.3058443 -4.3108191 -4.3148403 -4.3203692][-4.3082533 -4.3201723 -4.31707 -4.3060837 -4.2929587 -4.2767797 -4.2563825 -4.2380643 -4.2283158 -4.2302537 -4.2426047 -4.2585731 -4.2757378 -4.2900763 -4.3018937][-4.331759 -4.3261428 -4.3017635 -4.2672286 -4.2339792 -4.19462 -4.1479769 -4.10972 -4.0905924 -4.0987139 -4.1305203 -4.17336 -4.2167659 -4.2500768 -4.2728753][-4.3403406 -4.3193011 -4.2734604 -4.2141862 -4.1564388 -4.0870495 -4.0048981 -3.9399924 -3.9138603 -3.9408669 -4.005971 -4.0775223 -4.14868 -4.2049737 -4.2400413][-4.3261766 -4.2947426 -4.2287211 -4.1424437 -4.0564303 -3.9615383 -3.8428307 -3.7397342 -3.704699 -3.7671351 -3.8793993 -3.9728632 -4.0644836 -4.1473322 -4.2035584][-4.2958655 -4.25745 -4.1753297 -4.061162 -3.9507885 -3.8390841 -3.6893084 -3.5449989 -3.4994226 -3.610337 -3.7814972 -3.9025021 -4.0097632 -4.1150589 -4.1882205][-4.2536836 -4.2155838 -4.1328068 -4.0135555 -3.9100037 -3.8149245 -3.6849866 -3.5598161 -3.5329261 -3.66683 -3.8479903 -3.9652629 -4.0580134 -4.1472197 -4.2042623][-4.2120986 -4.1883707 -4.1317363 -4.0442996 -3.9767354 -3.9194064 -3.8418713 -3.776387 -3.7819862 -3.8863606 -4.0113096 -4.0898361 -4.1505003 -4.2037315 -4.2313285][-4.1901388 -4.1808624 -4.1487012 -4.0977941 -4.0627456 -4.0332837 -3.9973967 -3.9725506 -3.991859 -4.0570207 -4.1262445 -4.1696777 -4.2096295 -4.2392917 -4.2472739][-4.2028728 -4.2012191 -4.1807189 -4.1521525 -4.1380267 -4.123343 -4.1057987 -4.099194 -4.119565 -4.1596084 -4.1931334 -4.2122245 -4.2367163 -4.2538571 -4.2567911][-4.2227974 -4.2215018 -4.2058673 -4.1947775 -4.1975904 -4.1953306 -4.1846724 -4.1796675 -4.1884828 -4.2059941 -4.2169542 -4.2198329 -4.2351484 -4.249311 -4.2560163][-4.2388577 -4.2378764 -4.2268076 -4.225841 -4.2356782 -4.2398949 -4.2340751 -4.2275023 -4.2270274 -4.2277441 -4.2239628 -4.2208748 -4.2305946 -4.2440333 -4.2532392][-4.2629809 -4.2629895 -4.2558432 -4.256525 -4.2653842 -4.2698822 -4.2681437 -4.2642903 -4.2604012 -4.2560987 -4.2476945 -4.2454472 -4.2542958 -4.26262 -4.2659287]]...]
INFO - root - 2017-12-07 22:52:20.524360: step 63410, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 53h:40m:06s remains)
INFO - root - 2017-12-07 22:52:27.430511: step 63420, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 53h:17m:30s remains)
INFO - root - 2017-12-07 22:52:34.314579: step 63430, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 50h:32m:41s remains)
INFO - root - 2017-12-07 22:52:41.090114: step 63440, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 49h:10m:57s remains)
INFO - root - 2017-12-07 22:52:47.900080: step 63450, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 48h:40m:27s remains)
INFO - root - 2017-12-07 22:52:54.778987: step 63460, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 51h:34m:49s remains)
INFO - root - 2017-12-07 22:53:01.544240: step 63470, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 54h:01m:54s remains)
INFO - root - 2017-12-07 22:53:08.375974: step 63480, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 53h:14m:48s remains)
INFO - root - 2017-12-07 22:53:15.141448: step 63490, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 47h:39m:57s remains)
INFO - root - 2017-12-07 22:53:21.939852: step 63500, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.662 sec/batch; 49h:29m:41s remains)
2017-12-07 22:53:22.903245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0975747 -4.1094422 -4.1293 -4.1305065 -4.1099505 -4.0724406 -4.0662112 -4.0909429 -4.1295271 -4.1663609 -4.2045965 -4.2496977 -4.2855988 -4.3062625 -4.3199477][-4.0994954 -4.1037064 -4.1125941 -4.1001778 -4.0676312 -4.0290356 -4.0388823 -4.081625 -4.1313696 -4.1782928 -4.22102 -4.2640305 -4.2949581 -4.3122635 -4.323535][-4.1058941 -4.1064157 -4.1106143 -4.09307 -4.0588384 -4.0268574 -4.048389 -4.0989704 -4.1532331 -4.2046595 -4.2468266 -4.2844748 -4.30836 -4.3208075 -4.3277345][-4.1054788 -4.10062 -4.1027856 -4.0852447 -4.051652 -4.0216951 -4.0472894 -4.1038966 -4.1649122 -4.2232304 -4.2657933 -4.3003321 -4.319787 -4.3276882 -4.33126][-4.1202312 -4.1108866 -4.11063 -4.0939479 -4.06265 -4.0338783 -4.0573645 -4.116221 -4.1799154 -4.2386279 -4.2799721 -4.3114371 -4.3277507 -4.332149 -4.33436][-4.1675606 -4.1560807 -4.1533084 -4.1364927 -4.1059413 -4.0763779 -4.0902367 -4.1403737 -4.1963477 -4.2504435 -4.290525 -4.3221822 -4.3359942 -4.3363018 -4.3369813][-4.2304077 -4.216229 -4.2086306 -4.1880736 -4.1521788 -4.1162281 -4.1186295 -4.1569481 -4.2018104 -4.2490664 -4.2914696 -4.3263354 -4.3404374 -4.3387389 -4.3384295][-4.2634225 -4.2531309 -4.2489939 -4.2272224 -4.1798043 -4.1314611 -4.1264443 -4.1565275 -4.19006 -4.2321205 -4.28067 -4.3215823 -4.3381987 -4.3365369 -4.3361382][-4.241816 -4.2396522 -4.2416334 -4.2186904 -4.1582737 -4.0906329 -4.0773239 -4.1060762 -4.1435409 -4.1913543 -4.2516575 -4.3008652 -4.3221531 -4.324883 -4.3283176][-4.1969929 -4.1975918 -4.1991143 -4.1713877 -4.0991035 -4.0171013 -4.0050268 -4.0434566 -4.0967493 -4.1567197 -4.2247553 -4.2818508 -4.3088226 -4.3148651 -4.3212552][-4.1797986 -4.1783032 -4.1782808 -4.14755 -4.0749412 -3.9955857 -3.9930844 -4.0390663 -4.0945177 -4.1532869 -4.2204623 -4.2802539 -4.30852 -4.3135943 -4.3193421][-4.2057076 -4.2006297 -4.1991224 -4.172441 -4.1109009 -4.045969 -4.050487 -4.091589 -4.1370697 -4.18699 -4.2472472 -4.3004742 -4.3217759 -4.3208938 -4.321229][-4.2381482 -4.234488 -4.2329664 -4.2122765 -4.1646175 -4.1157651 -4.1230578 -4.154902 -4.1929922 -4.23624 -4.2853074 -4.3253632 -4.3364673 -4.328733 -4.3255186][-4.2709036 -4.2721391 -4.2746949 -4.2602186 -4.2248912 -4.1890655 -4.192493 -4.2123322 -4.2430134 -4.2811017 -4.3159428 -4.3418522 -4.3467512 -4.338665 -4.3336139][-4.3108573 -4.3128743 -4.3146138 -4.3045373 -4.2826834 -4.2614846 -4.2610755 -4.2715812 -4.2914796 -4.3194718 -4.3410592 -4.3552113 -4.3561711 -4.3489332 -4.3435717]]...]
INFO - root - 2017-12-07 22:53:29.289729: step 63510, loss = 2.08, batch loss = 2.03 (17.5 examples/sec; 0.458 sec/batch; 34h:11m:36s remains)
INFO - root - 2017-12-07 22:53:36.061150: step 63520, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 53h:11m:52s remains)
INFO - root - 2017-12-07 22:53:42.769960: step 63530, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 52h:59m:48s remains)
INFO - root - 2017-12-07 22:53:49.521389: step 63540, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 52h:14m:42s remains)
INFO - root - 2017-12-07 22:53:56.326875: step 63550, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.687 sec/batch; 51h:17m:45s remains)
INFO - root - 2017-12-07 22:54:03.059972: step 63560, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 47h:47m:29s remains)
INFO - root - 2017-12-07 22:54:09.768723: step 63570, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 48h:57m:02s remains)
INFO - root - 2017-12-07 22:54:16.493394: step 63580, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 51h:46m:06s remains)
INFO - root - 2017-12-07 22:54:23.256668: step 63590, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 53h:02m:38s remains)
INFO - root - 2017-12-07 22:54:30.003274: step 63600, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 52h:27m:24s remains)
2017-12-07 22:54:30.702489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1180916 -4.1388478 -4.1692963 -4.1882319 -4.1991291 -4.2068162 -4.2070827 -4.2060866 -4.214179 -4.22862 -4.2386665 -4.2313461 -4.2113566 -4.2038918 -4.2158561][-4.1686459 -4.1853905 -4.2027593 -4.211122 -4.2104764 -4.20755 -4.2042112 -4.20757 -4.2214947 -4.2398982 -4.2493911 -4.2418957 -4.2272635 -4.2245331 -4.2341795][-4.2144794 -4.2283936 -4.2358255 -4.232718 -4.2188654 -4.2023263 -4.1956768 -4.2041578 -4.2249718 -4.247746 -4.256218 -4.2475729 -4.2369394 -4.2362347 -4.2394004][-4.2311254 -4.2373867 -4.239687 -4.2284479 -4.2032604 -4.1741338 -4.1685548 -4.1856775 -4.2143426 -4.244277 -4.2520566 -4.2377558 -4.2226119 -4.2156639 -4.2121577][-4.1840234 -4.1829581 -4.186357 -4.1742392 -4.1408086 -4.10003 -4.0919085 -4.1225729 -4.1667261 -4.2046113 -4.213109 -4.1925521 -4.1671805 -4.145668 -4.1370816][-4.1033249 -4.0957608 -4.1029763 -4.090744 -4.0499697 -3.9959803 -3.9716613 -4.0104346 -4.0724139 -4.1203623 -4.13293 -4.1085129 -4.0823421 -4.0652065 -4.0677042][-4.0673828 -4.0449233 -4.0390615 -4.0120831 -3.96219 -3.8892884 -3.8318603 -3.8609846 -3.9460413 -4.01653 -4.04253 -4.027482 -4.0157037 -4.0252905 -4.0552669][-4.1007137 -4.0623302 -4.0328441 -3.9838214 -3.9192019 -3.8236513 -3.7262793 -3.7242329 -3.818953 -3.9173727 -3.9667633 -3.9761055 -3.9957578 -4.041729 -4.0965624][-4.1742487 -4.12562 -4.08632 -4.0338879 -3.9761629 -3.8932803 -3.805052 -3.7799737 -3.8382282 -3.9227059 -3.972878 -4.0025334 -4.048296 -4.1102524 -4.168375][-4.2375736 -4.1941481 -4.1591029 -4.1139588 -4.0709276 -4.0125051 -3.9530571 -3.9313879 -3.9517393 -4.0003371 -4.0356178 -4.0706525 -4.1275969 -4.1887431 -4.2393422][-4.2607412 -4.2337627 -4.2093697 -4.1775236 -4.152307 -4.1172271 -4.0825319 -4.0742068 -4.0812078 -4.1044974 -4.1234512 -4.1472206 -4.1934366 -4.2389154 -4.275424][-4.2466431 -4.235085 -4.2253809 -4.2134047 -4.2095394 -4.1993947 -4.1808271 -4.1753964 -4.176538 -4.1902456 -4.2014279 -4.2151537 -4.2400146 -4.2642303 -4.2890253][-4.2266855 -4.2285962 -4.2347817 -4.23974 -4.2520294 -4.2588897 -4.2533822 -4.2470174 -4.2444959 -4.2517109 -4.2586894 -4.2668762 -4.2776089 -4.2887816 -4.3038635][-4.187398 -4.2034616 -4.225472 -4.2379003 -4.2511244 -4.2626905 -4.2662883 -4.2648468 -4.2657781 -4.2694893 -4.2743163 -4.280694 -4.2875414 -4.2951713 -4.3036852][-4.1409106 -4.1686006 -4.2084122 -4.2311039 -4.2430434 -4.2538404 -4.2602792 -4.2629004 -4.2645335 -4.2666626 -4.2692385 -4.2738814 -4.27843 -4.2823548 -4.2848949]]...]
INFO - root - 2017-12-07 22:54:37.333275: step 63610, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.700 sec/batch; 52h:18m:30s remains)
INFO - root - 2017-12-07 22:54:44.322279: step 63620, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 53h:52m:32s remains)
INFO - root - 2017-12-07 22:54:51.113014: step 63630, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 52h:56m:02s remains)
INFO - root - 2017-12-07 22:54:57.938341: step 63640, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.648 sec/batch; 48h:24m:28s remains)
INFO - root - 2017-12-07 22:55:04.772461: step 63650, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 51h:04m:31s remains)
INFO - root - 2017-12-07 22:55:11.629300: step 63660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 51h:36m:46s remains)
INFO - root - 2017-12-07 22:55:18.513366: step 63670, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 48h:58m:34s remains)
INFO - root - 2017-12-07 22:55:25.359492: step 63680, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 47h:59m:34s remains)
INFO - root - 2017-12-07 22:55:32.225596: step 63690, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 48h:59m:09s remains)
INFO - root - 2017-12-07 22:55:39.091802: step 63700, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.711 sec/batch; 53h:07m:20s remains)
2017-12-07 22:55:39.808138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.270133 -4.2677059 -4.2713122 -4.26859 -4.2585731 -4.2502842 -4.25833 -4.280252 -4.2942281 -4.2931471 -4.2792182 -4.251873 -4.2257843 -4.2075381 -4.1890407][-4.2774663 -4.2819705 -4.2833719 -4.2801027 -4.2726727 -4.2685761 -4.2727346 -4.2868867 -4.2913918 -4.2844133 -4.2649393 -4.2301111 -4.1946945 -4.1727743 -4.1539297][-4.2763653 -4.2868538 -4.2890143 -4.2835293 -4.2750454 -4.2648125 -4.2596769 -4.2624912 -4.2556396 -4.2412658 -4.2189507 -4.1840715 -4.1583447 -4.1523142 -4.1464043][-4.2672553 -4.2830758 -4.289638 -4.2826385 -4.2652574 -4.2431059 -4.2296543 -4.2300491 -4.227169 -4.2136049 -4.1915874 -4.1621137 -4.149869 -4.16777 -4.1805234][-4.2525511 -4.272706 -4.2846193 -4.2776461 -4.2499828 -4.2084851 -4.1881423 -4.1958532 -4.2134075 -4.217586 -4.2018437 -4.1789837 -4.1771173 -4.2058773 -4.2288728][-4.2370315 -4.2604141 -4.2772317 -4.2678814 -4.2264881 -4.1629529 -4.1285281 -4.1451235 -4.189713 -4.2174792 -4.2177811 -4.2142715 -4.2233448 -4.246901 -4.2664266][-4.2236872 -4.2455826 -4.2576027 -4.2343659 -4.1682453 -4.0758619 -4.0209684 -4.0497174 -4.1302028 -4.1978078 -4.2325683 -4.2515292 -4.2655067 -4.2755551 -4.2793412][-4.2177696 -4.231935 -4.2306962 -4.1960616 -4.114223 -4.0047951 -3.9364522 -3.97572 -4.0851336 -4.1892796 -4.250298 -4.27668 -4.2817869 -4.2740016 -4.2600126][-4.2220716 -4.2326431 -4.2300167 -4.2002916 -4.1323385 -4.0431385 -3.9897094 -4.0339117 -4.1347055 -4.2276778 -4.2755671 -4.2872739 -4.2782564 -4.2604966 -4.2421269][-4.2243853 -4.2332597 -4.2402048 -4.2315397 -4.2006679 -4.1558237 -4.12666 -4.1532273 -4.2159529 -4.2724104 -4.2928615 -4.2887149 -4.2712464 -4.2551541 -4.24135][-4.2291865 -4.2389731 -4.2629952 -4.2737432 -4.2647305 -4.2426634 -4.2237597 -4.23058 -4.2610683 -4.2908778 -4.2952533 -4.2837243 -4.2648764 -4.2506695 -4.2395468][-4.238513 -4.2536325 -4.2848625 -4.3011432 -4.296247 -4.2783985 -4.2614317 -4.2553387 -4.263804 -4.2834969 -4.2920966 -4.288126 -4.2744646 -4.2602129 -4.2455764][-4.2513256 -4.2692347 -4.2923741 -4.3009095 -4.2931929 -4.2783647 -4.26197 -4.2481103 -4.2435975 -4.2592716 -4.2765608 -4.2853746 -4.2831073 -4.2727537 -4.2561049][-4.2547894 -4.2730727 -4.2905226 -4.2948217 -4.2892079 -4.2777476 -4.2601571 -4.2391667 -4.2250481 -4.2345548 -4.2555118 -4.272676 -4.2832012 -4.2795925 -4.2655497][-4.2444868 -4.2605548 -4.2749586 -4.2790275 -4.2774429 -4.2698174 -4.2510705 -4.2246494 -4.2064281 -4.2124815 -4.2363777 -4.2590294 -4.2747064 -4.2762918 -4.2689619]]...]
INFO - root - 2017-12-07 22:55:46.360278: step 63710, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 48h:58m:37s remains)
INFO - root - 2017-12-07 22:55:53.251094: step 63720, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 49h:52m:02s remains)
INFO - root - 2017-12-07 22:56:00.168737: step 63730, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 52h:54m:18s remains)
INFO - root - 2017-12-07 22:56:07.015890: step 63740, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 52h:44m:11s remains)
INFO - root - 2017-12-07 22:56:13.818230: step 63750, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 48h:35m:46s remains)
INFO - root - 2017-12-07 22:56:20.539703: step 63760, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 48h:16m:30s remains)
INFO - root - 2017-12-07 22:56:27.360584: step 63770, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 53h:15m:06s remains)
INFO - root - 2017-12-07 22:56:34.214906: step 63780, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 53h:32m:40s remains)
INFO - root - 2017-12-07 22:56:41.037962: step 63790, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 50h:33m:12s remains)
INFO - root - 2017-12-07 22:56:47.766085: step 63800, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 48h:07m:51s remains)
2017-12-07 22:56:48.504092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2012362 -4.2062125 -4.2063508 -4.209599 -4.2216387 -4.2291851 -4.228447 -4.2277346 -4.2274809 -4.2270803 -4.2281084 -4.2289839 -4.2288795 -4.2281356 -4.2274075][-4.1829619 -4.1927834 -4.1945386 -4.1973367 -4.2134314 -4.2255549 -4.2269559 -4.2287211 -4.2304091 -4.2304525 -4.2308631 -4.2304006 -4.228189 -4.2251806 -4.2223778][-4.1651592 -4.1704946 -4.1692 -4.1720023 -4.1947546 -4.2126942 -4.2171483 -4.2235484 -4.2306447 -4.2306852 -4.2302861 -4.2308021 -4.2254629 -4.2174292 -4.212501][-4.1673212 -4.1639624 -4.1556845 -4.154592 -4.1778245 -4.1984782 -4.2056632 -4.2188835 -4.2362638 -4.2390761 -4.2383881 -4.2421532 -4.2372179 -4.2229338 -4.2143159][-4.1753511 -4.1624618 -4.1488619 -4.143188 -4.1628013 -4.1770425 -4.1753578 -4.1852612 -4.214159 -4.223269 -4.2254186 -4.2355919 -4.2351713 -4.2205873 -4.2092862][-4.16531 -4.141952 -4.1224957 -4.11306 -4.1317124 -4.136353 -4.116487 -4.1162782 -4.1604586 -4.1835203 -4.1929007 -4.2088957 -4.2142243 -4.1992235 -4.1838274][-4.1437268 -4.1070294 -4.0709224 -4.047205 -4.0593972 -4.0513749 -4.0076928 -3.9890893 -4.0631785 -4.1154008 -4.1419606 -4.168498 -4.1792703 -4.1594276 -4.1332784][-4.1443968 -4.1037059 -4.0524259 -4.0053968 -3.9909906 -3.9526529 -3.8664885 -3.8132751 -3.9300194 -4.0310054 -4.0831585 -4.126616 -4.1475463 -4.128552 -4.0993447][-4.1765342 -4.1499429 -4.1055217 -4.0537229 -4.019073 -3.9609056 -3.8459308 -3.7542379 -3.876451 -4.0009537 -4.0637341 -4.1119022 -4.141912 -4.1349792 -4.1213675][-4.2131376 -4.2029042 -4.1777096 -4.1416111 -4.1110134 -4.0672436 -3.984323 -3.9114828 -3.9818163 -4.0691919 -4.1114588 -4.1443915 -4.1695151 -4.169281 -4.1678381][-4.2452812 -4.2418709 -4.2292442 -4.2089505 -4.1894374 -4.1670942 -4.1255274 -4.08605 -4.1174259 -4.1656656 -4.1894155 -4.2051005 -4.2191429 -4.21766 -4.2175374][-4.2708993 -4.2697744 -4.263031 -4.2507544 -4.2387414 -4.2320976 -4.2167063 -4.1968093 -4.2083006 -4.2326441 -4.2469926 -4.2535472 -4.2592516 -4.25717 -4.2541652][-4.2784672 -4.2770476 -4.2733269 -4.2653575 -4.2554283 -4.255343 -4.2515454 -4.2413259 -4.2472181 -4.2636042 -4.2769704 -4.2807469 -4.2806044 -4.2740378 -4.2705026][-4.2750506 -4.2717628 -4.2700243 -4.2672505 -4.2616706 -4.2630444 -4.2622042 -4.2580776 -4.2631073 -4.2765393 -4.2878141 -4.2901092 -4.2866569 -4.2800179 -4.276485][-4.269805 -4.2657661 -4.2655988 -4.2675462 -4.2678356 -4.2703767 -4.270318 -4.2682004 -4.2706923 -4.2780609 -4.2838459 -4.2845268 -4.281558 -4.2770605 -4.2730689]]...]
INFO - root - 2017-12-07 22:56:55.125718: step 63810, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 50h:34m:50s remains)
INFO - root - 2017-12-07 22:57:01.796626: step 63820, loss = 2.07, batch loss = 2.01 (15.5 examples/sec; 0.515 sec/batch; 38h:24m:39s remains)
INFO - root - 2017-12-07 22:57:08.571749: step 63830, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 53h:39m:39s remains)
INFO - root - 2017-12-07 22:57:15.452287: step 63840, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.735 sec/batch; 54h:49m:53s remains)
INFO - root - 2017-12-07 22:57:22.295883: step 63850, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 51h:38m:09s remains)
INFO - root - 2017-12-07 22:57:29.101304: step 63860, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.624 sec/batch; 46h:34m:10s remains)
INFO - root - 2017-12-07 22:57:35.962604: step 63870, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 52h:17m:24s remains)
INFO - root - 2017-12-07 22:57:42.784312: step 63880, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 52h:06m:59s remains)
INFO - root - 2017-12-07 22:57:49.546861: step 63890, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 49h:55m:44s remains)
INFO - root - 2017-12-07 22:57:56.285599: step 63900, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 49h:19m:19s remains)
2017-12-07 22:57:56.975009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1278553 -4.0339708 -4.0045938 -4.0438485 -4.0948277 -4.1314406 -4.15714 -4.1854281 -4.1909966 -4.1884165 -4.1740379 -4.1616092 -4.1633124 -4.165175 -4.1631808][-4.1239872 -4.0144553 -3.9719095 -4.0032449 -4.0508366 -4.0971661 -4.1238041 -4.1583567 -4.177321 -4.1823239 -4.1723595 -4.148334 -4.1330032 -4.1345677 -4.1390924][-4.1132264 -3.987077 -3.9210875 -3.9456003 -3.9977646 -4.0546546 -4.0873022 -4.1239243 -4.1554637 -4.1696043 -4.1684079 -4.1427693 -4.11342 -4.1056795 -4.1109171][-4.1129217 -3.9908783 -3.9086838 -3.9242778 -3.9721527 -4.0284643 -4.0622663 -4.097652 -4.1322603 -4.1539912 -4.1639357 -4.1488504 -4.1189308 -4.1028876 -4.0980344][-4.1264586 -4.0224447 -3.9393055 -3.945425 -3.970818 -4.012733 -4.0443621 -4.0817237 -4.1191578 -4.1493144 -4.1734667 -4.1658158 -4.1342864 -4.1104016 -4.0946088][-4.1299539 -4.0435257 -3.972647 -3.9746888 -3.9789541 -4.00297 -4.0321074 -4.0730805 -4.1151032 -4.155015 -4.181684 -4.1759796 -4.143743 -4.1196027 -4.0987549][-4.1227412 -4.0448933 -3.987438 -3.9974811 -3.9961629 -4.0056677 -4.0249147 -4.063014 -4.109684 -4.1526217 -4.1713529 -4.1643414 -4.1371884 -4.1176362 -4.098484][-4.110867 -4.0341783 -3.9923067 -4.0101929 -4.0116215 -4.00993 -4.0170403 -4.0530248 -4.1021161 -4.1388955 -4.1407828 -4.128933 -4.1090865 -4.0975518 -4.0878382][-4.1036377 -4.0300493 -3.9975579 -4.0210662 -4.0351171 -4.0351257 -4.0419173 -4.0686078 -4.1070409 -4.1264281 -4.1124821 -4.0934772 -4.0797195 -4.0730128 -4.0773716][-4.1172853 -4.0443468 -4.0131207 -4.0390248 -4.0609369 -4.0715556 -4.0852385 -4.1036968 -4.1257973 -4.1274924 -4.0981965 -4.0699048 -4.058301 -4.0609803 -4.0772672][-4.1398578 -4.0671034 -4.037 -4.0650749 -4.0875034 -4.1013141 -4.1149063 -4.1298108 -4.1432514 -4.1392155 -4.1090975 -4.0759687 -4.0645332 -4.0767226 -4.1021252][-4.1488314 -4.0777206 -4.0560231 -4.0910759 -4.112052 -4.1216574 -4.1305904 -4.1434555 -4.1602664 -4.1588416 -4.1330132 -4.0984159 -4.0904369 -4.1060758 -4.1323261][-4.1404943 -4.0730739 -4.061141 -4.0991573 -4.1183171 -4.1230564 -4.12899 -4.1364732 -4.149673 -4.1565394 -4.140965 -4.1143761 -4.1080918 -4.123157 -4.1416783][-4.14235 -4.0755758 -4.0652685 -4.0978203 -4.1060958 -4.1109371 -4.1205683 -4.1247177 -4.134727 -4.1431103 -4.1325583 -4.1120915 -4.1055822 -4.1168661 -4.1267776][-4.1651134 -4.0953379 -4.0769539 -4.1008453 -4.1006122 -4.1063075 -4.1223435 -4.1312208 -4.1433578 -4.1497197 -4.1386495 -4.1185441 -4.1055026 -4.1039376 -4.105526]]...]
INFO - root - 2017-12-07 22:58:03.591747: step 63910, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 51h:44m:11s remains)
INFO - root - 2017-12-07 22:58:10.437831: step 63920, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 50h:42m:45s remains)
INFO - root - 2017-12-07 22:58:17.204677: step 63930, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 49h:36m:18s remains)
INFO - root - 2017-12-07 22:58:24.015653: step 63940, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 48h:49m:13s remains)
INFO - root - 2017-12-07 22:58:30.831273: step 63950, loss = 2.02, batch loss = 1.97 (11.6 examples/sec; 0.688 sec/batch; 51h:19m:44s remains)
INFO - root - 2017-12-07 22:58:37.581273: step 63960, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 51h:45m:58s remains)
INFO - root - 2017-12-07 22:58:44.405993: step 63970, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 51h:42m:39s remains)
INFO - root - 2017-12-07 22:58:51.223317: step 63980, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 48h:27m:39s remains)
INFO - root - 2017-12-07 22:58:58.016953: step 63990, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 49h:13m:44s remains)
INFO - root - 2017-12-07 22:59:04.959903: step 64000, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 52h:45m:35s remains)
2017-12-07 22:59:05.683318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1917543 -4.206192 -4.2138238 -4.2171741 -4.2292008 -4.2458215 -4.2597713 -4.2703147 -4.2755785 -4.2755108 -4.274806 -4.2690926 -4.2553396 -4.2410669 -4.2307572][-4.183 -4.200366 -4.2096505 -4.212214 -4.2233791 -4.239162 -4.2583694 -4.2754874 -4.2847629 -4.2843757 -4.278162 -4.2657418 -4.2485237 -4.2335863 -4.2251687][-4.175899 -4.19764 -4.2097573 -4.2129488 -4.2208672 -4.2353878 -4.2578254 -4.2816005 -4.2964063 -4.3001943 -4.2930036 -4.2772303 -4.2585874 -4.2442436 -4.2351885][-4.1633687 -4.1888514 -4.2059593 -4.2129283 -4.2178416 -4.2300224 -4.253469 -4.2807536 -4.3005276 -4.3104792 -4.3076353 -4.2948685 -4.2783551 -4.26389 -4.2536607][-4.1547322 -4.1787453 -4.1990271 -4.20971 -4.2094026 -4.213954 -4.231678 -4.2569284 -4.2791739 -4.2958426 -4.3028445 -4.2976871 -4.2839708 -4.2644143 -4.2479372][-4.1624026 -4.1795712 -4.1984911 -4.2079959 -4.1978235 -4.1838064 -4.1832705 -4.1996441 -4.2275438 -4.2553616 -4.2759447 -4.2815189 -4.2699752 -4.2424903 -4.2140765][-4.1895247 -4.1989374 -4.2121029 -4.217145 -4.1958957 -4.1533165 -4.1140246 -4.106884 -4.1424046 -4.1961327 -4.2406106 -4.2619252 -4.255383 -4.222466 -4.1852093][-4.2264724 -4.2297993 -4.2348571 -4.2354908 -4.2096529 -4.1478691 -4.0663724 -4.0141916 -4.0449004 -4.1258793 -4.2007527 -4.2421341 -4.2466745 -4.2195921 -4.1836634][-4.2498636 -4.2505016 -4.252327 -4.2527633 -4.2344074 -4.1806 -4.09248 -4.0108247 -4.0060935 -4.0774164 -4.16169 -4.218883 -4.2389989 -4.2275677 -4.2012115][-4.2504611 -4.2542663 -4.2601075 -4.2657919 -4.2615881 -4.231802 -4.1694226 -4.0952897 -4.0581436 -4.0797687 -4.1382742 -4.1954994 -4.228085 -4.2346191 -4.2228651][-4.2413464 -4.2491689 -4.2602453 -4.2713957 -4.2778745 -4.268065 -4.2338295 -4.1815958 -4.1357331 -4.117907 -4.1374149 -4.176496 -4.2113338 -4.2296829 -4.2294035][-4.2319641 -4.2428694 -4.2562785 -4.2676759 -4.2770014 -4.2763066 -4.2616062 -4.23236 -4.1942797 -4.1596951 -4.1488109 -4.160697 -4.184288 -4.2066112 -4.2135859][-4.2219243 -4.233808 -4.2460356 -4.2550578 -4.2627749 -4.2643209 -4.2608089 -4.2490177 -4.2259145 -4.1944318 -4.1701074 -4.1584115 -4.1624327 -4.1784649 -4.1883373][-4.2107182 -4.2209806 -4.2275395 -4.2307558 -4.2348332 -4.2360029 -4.2381353 -4.23964 -4.2323041 -4.2149816 -4.1934719 -4.1715555 -4.1620584 -4.1690025 -4.178266][-4.2071939 -4.2127738 -4.2112265 -4.2051792 -4.2013083 -4.1993113 -4.2044878 -4.2142172 -4.2197189 -4.2179747 -4.2088757 -4.1908617 -4.1806197 -4.1873484 -4.1973615]]...]
INFO - root - 2017-12-07 22:59:12.222167: step 64010, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 50h:49m:21s remains)
INFO - root - 2017-12-07 22:59:19.053604: step 64020, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.658 sec/batch; 49h:06m:21s remains)
INFO - root - 2017-12-07 22:59:25.767781: step 64030, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.681 sec/batch; 50h:46m:33s remains)
INFO - root - 2017-12-07 22:59:32.480124: step 64040, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 51h:22m:56s remains)
INFO - root - 2017-12-07 22:59:39.335418: step 64050, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 51h:24m:00s remains)
INFO - root - 2017-12-07 22:59:46.102413: step 64060, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 47h:18m:10s remains)
INFO - root - 2017-12-07 22:59:52.883359: step 64070, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 49h:30m:56s remains)
INFO - root - 2017-12-07 22:59:59.638863: step 64080, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 51h:57m:38s remains)
INFO - root - 2017-12-07 23:00:06.495285: step 64090, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 53h:11m:58s remains)
INFO - root - 2017-12-07 23:00:13.336963: step 64100, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 51h:44m:44s remains)
2017-12-07 23:00:14.021283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3338504 -4.3258719 -4.320178 -4.316978 -4.3155341 -4.3156137 -4.31819 -4.3216825 -4.3245816 -4.3260641 -4.3266511 -4.3232107 -4.3171554 -4.312355 -4.3111291][-4.30965 -4.300437 -4.2937694 -4.291688 -4.2933321 -4.2971616 -4.3025246 -4.306735 -4.3088727 -4.3089585 -4.3084273 -4.3025889 -4.29384 -4.2901897 -4.2899117][-4.2876453 -4.2806144 -4.2747874 -4.275043 -4.2804775 -4.2874351 -4.2926917 -4.2951713 -4.2945919 -4.2936139 -4.2943091 -4.2914634 -4.286212 -4.2850156 -4.2838764][-4.2802267 -4.275403 -4.268949 -4.2679677 -4.2696896 -4.2710152 -4.2683311 -4.2651138 -4.2625031 -4.2647586 -4.2726593 -4.2787962 -4.2806349 -4.2836261 -4.2824321][-4.2764807 -4.2664251 -4.2525206 -4.2439866 -4.2351589 -4.2225494 -4.2073197 -4.2007985 -4.205071 -4.2183332 -4.2363758 -4.2497721 -4.2582912 -4.268126 -4.275835][-4.2476516 -4.2255712 -4.1988864 -4.1829929 -4.1669383 -4.1413312 -4.110467 -4.0981793 -4.1134443 -4.1420827 -4.169971 -4.1903729 -4.2056723 -4.2255688 -4.2501426][-4.19873 -4.1664762 -4.1271925 -4.1034718 -4.0803094 -4.0402327 -3.9852207 -3.9535089 -3.9794621 -4.035368 -4.0846987 -4.1176753 -4.14145 -4.1713195 -4.2094035][-4.1599083 -4.1248727 -4.0757465 -4.0421324 -4.0106993 -3.9614923 -3.8846769 -3.8321447 -3.8737359 -3.9654241 -4.0395288 -4.079895 -4.1020026 -4.1312537 -4.171936][-4.1484165 -4.1228666 -4.0788732 -4.0455403 -4.0187564 -3.983012 -3.9271805 -3.8944962 -3.9391177 -4.0208378 -4.0815911 -4.1073041 -4.1108122 -4.1228938 -4.1500559][-4.1632395 -4.1513863 -4.1226249 -4.10157 -4.0890965 -4.0745125 -4.0507975 -4.0399666 -4.0684538 -4.1144342 -4.142787 -4.1441884 -4.1273956 -4.1208792 -4.1323433][-4.1958141 -4.1946893 -4.177001 -4.1655674 -4.16053 -4.1545496 -4.1450415 -4.1375303 -4.1431222 -4.15436 -4.1561751 -4.1414971 -4.1150832 -4.0987253 -4.1039386][-4.21198 -4.2182603 -4.2065611 -4.2000575 -4.1994467 -4.1972628 -4.1907487 -4.1780591 -4.166151 -4.156508 -4.147552 -4.1328878 -4.1109319 -4.0952435 -4.1010623][-4.2140021 -4.225162 -4.217639 -4.2135053 -4.2145929 -4.2121162 -4.2049584 -4.1883841 -4.1674967 -4.15136 -4.14399 -4.1405492 -4.1312633 -4.120635 -4.1260653][-4.2183867 -4.2321663 -4.228941 -4.2262383 -4.2260933 -4.2214108 -4.210557 -4.1892514 -4.1658826 -4.1525555 -4.1524367 -4.1598277 -4.1600208 -4.1546082 -4.1616335][-4.24286 -4.2597232 -4.2610006 -4.2587113 -4.2561541 -4.2482991 -4.234797 -4.2130713 -4.1943336 -4.1896739 -4.19683 -4.2080612 -4.2134714 -4.2144871 -4.2216167]]...]
INFO - root - 2017-12-07 23:00:20.614443: step 64110, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 54h:11m:02s remains)
INFO - root - 2017-12-07 23:00:27.405256: step 64120, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 52h:26m:39s remains)
INFO - root - 2017-12-07 23:00:34.136111: step 64130, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:33m:39s remains)
INFO - root - 2017-12-07 23:00:40.772551: step 64140, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 47h:08m:19s remains)
INFO - root - 2017-12-07 23:00:47.455947: step 64150, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 48h:31m:07s remains)
INFO - root - 2017-12-07 23:00:54.320173: step 64160, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 50h:48m:28s remains)
INFO - root - 2017-12-07 23:01:01.195119: step 64170, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 51h:47m:10s remains)
INFO - root - 2017-12-07 23:01:07.929546: step 64180, loss = 2.07, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 49h:58m:42s remains)
INFO - root - 2017-12-07 23:01:14.692707: step 64190, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 48h:19m:08s remains)
INFO - root - 2017-12-07 23:01:21.480178: step 64200, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 51h:17m:36s remains)
2017-12-07 23:01:22.207213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29912 -4.2875261 -4.2713571 -4.2413964 -4.206635 -4.1837616 -4.1790543 -4.1971421 -4.2482548 -4.30349 -4.3192339 -4.304491 -4.277864 -4.2480936 -4.2213192][-4.2974744 -4.2797194 -4.2600985 -4.2349834 -4.2025018 -4.17341 -4.1630526 -4.1813016 -4.2296228 -4.2790003 -4.2903576 -4.2701983 -4.2372971 -4.20354 -4.1729994][-4.2941427 -4.2751913 -4.2582521 -4.2421541 -4.2130141 -4.1767612 -4.1573167 -4.1717434 -4.2143779 -4.2522807 -4.2556314 -4.23126 -4.1962442 -4.1607285 -4.1255336][-4.2899504 -4.2702012 -4.2549958 -4.2444167 -4.2195835 -4.1809273 -4.1541591 -4.156261 -4.1784458 -4.1983557 -4.1961732 -4.1771803 -4.1493549 -4.1208158 -4.0887513][-4.2826529 -4.2578897 -4.2372947 -4.2266221 -4.2096791 -4.173173 -4.13609 -4.1114774 -4.0976944 -4.0942192 -4.0985847 -4.0983682 -4.0959616 -4.0919437 -4.0787234][-4.2732043 -4.2431474 -4.2115011 -4.193356 -4.1748695 -4.1315217 -4.0773625 -4.0226946 -3.9689219 -3.9482598 -3.9689708 -4.0063663 -4.0508251 -4.0827322 -4.0914855][-4.2684288 -4.2370033 -4.1961994 -4.1651859 -4.13829 -4.0858016 -4.0181427 -3.9488618 -3.8805485 -3.8588614 -3.8971539 -3.9615343 -4.0367975 -4.0894947 -4.1140904][-4.2731156 -4.2407432 -4.1905231 -4.1403351 -4.0998163 -4.0463405 -3.9853449 -3.9295845 -3.8844438 -3.8773017 -3.9170911 -3.9760392 -4.0470309 -4.0988359 -4.1282554][-4.2722306 -4.2355137 -4.1775179 -4.1153245 -4.0699649 -4.0307617 -3.9905872 -3.9577861 -3.9438803 -3.9426782 -3.9696493 -4.0115824 -4.0616035 -4.0931268 -4.1120296][-4.266005 -4.2288942 -4.1755924 -4.1197305 -4.0827847 -4.0632949 -4.0437341 -4.0258708 -4.0253291 -4.0195661 -4.0230265 -4.0442696 -4.0724807 -4.0843539 -4.0915737][-4.2653146 -4.2324057 -4.1928067 -4.1551504 -4.1314464 -4.12398 -4.1150007 -4.1016831 -4.098434 -4.0880289 -4.0744243 -4.0794282 -4.0936594 -4.0971313 -4.0966115][-4.2736259 -4.2461748 -4.2193408 -4.1967845 -4.18324 -4.181354 -4.1772628 -4.1648149 -4.1616168 -4.1573772 -4.1414943 -4.1369591 -4.1404305 -4.1384311 -4.1336384][-4.2892981 -4.266109 -4.2469983 -4.2330394 -4.2273374 -4.2309709 -4.2331576 -4.2253942 -4.2263365 -4.2300415 -4.2185612 -4.2098546 -4.2041407 -4.1988759 -4.1925983][-4.3070507 -4.2877712 -4.2736435 -4.26523 -4.2642841 -4.2709641 -4.2780633 -4.2762275 -4.2779865 -4.281405 -4.2745852 -4.2665935 -4.2587814 -4.253953 -4.2527137][-4.3195629 -4.303298 -4.2914691 -4.2848716 -4.2832637 -4.2870398 -4.2928729 -4.2923656 -4.290915 -4.2906857 -4.2865191 -4.2830472 -4.2806396 -4.2814307 -4.287869]]...]
INFO - root - 2017-12-07 23:01:28.823954: step 64210, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 48h:27m:44s remains)
INFO - root - 2017-12-07 23:01:35.752296: step 64220, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 51h:48m:06s remains)
INFO - root - 2017-12-07 23:01:42.612280: step 64230, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 50h:44m:52s remains)
INFO - root - 2017-12-07 23:01:49.445639: step 64240, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 52h:30m:07s remains)
INFO - root - 2017-12-07 23:01:56.224992: step 64250, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 48h:10m:13s remains)
INFO - root - 2017-12-07 23:02:03.013522: step 64260, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 49h:51m:18s remains)
INFO - root - 2017-12-07 23:02:09.890454: step 64270, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.704 sec/batch; 52h:28m:52s remains)
INFO - root - 2017-12-07 23:02:16.639794: step 64280, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 51h:58m:46s remains)
INFO - root - 2017-12-07 23:02:23.441632: step 64290, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 50h:52m:36s remains)
INFO - root - 2017-12-07 23:02:30.273460: step 64300, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 49h:33m:20s remains)
2017-12-07 23:02:30.985131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2509751 -4.2451954 -4.2583508 -4.2792029 -4.2939239 -4.2991643 -4.3025961 -4.3108096 -4.3179512 -4.3170271 -4.3107862 -4.3033166 -4.2992392 -4.3046227 -4.312892][-4.20903 -4.2018065 -4.223784 -4.2586656 -4.2843561 -4.29585 -4.3001194 -4.3079371 -4.3139172 -4.3129859 -4.3072491 -4.2980704 -4.2882476 -4.2892203 -4.2964234][-4.163662 -4.1589231 -4.1915984 -4.2392879 -4.2726431 -4.2834325 -4.2794166 -4.2785096 -4.2814088 -4.2814574 -4.2789512 -4.2764945 -4.2692218 -4.2682343 -4.2706981][-4.1706033 -4.1709886 -4.206357 -4.2510304 -4.275455 -4.2717915 -4.2485294 -4.2344604 -4.2309489 -4.2308307 -4.2361588 -4.2494488 -4.2570648 -4.2622323 -4.2629418][-4.2315168 -4.2303839 -4.2514281 -4.2742958 -4.2761469 -4.2467628 -4.1962938 -4.1721473 -4.1730232 -4.1803083 -4.1981063 -4.2263365 -4.2477503 -4.2629528 -4.2675829][-4.2813878 -4.2708831 -4.271173 -4.2682362 -4.2443814 -4.1859827 -4.1043663 -4.0711994 -4.0892115 -4.1167922 -4.1529908 -4.1981091 -4.23081 -4.2548351 -4.2651153][-4.2918153 -4.2705808 -4.2507505 -4.2253842 -4.178885 -4.0915041 -3.9746773 -3.9221373 -3.9613111 -4.0253 -4.0929861 -4.1602492 -4.2065616 -4.2382641 -4.2503395][-4.2753773 -4.2424803 -4.2032976 -4.1578426 -4.0933938 -3.9877481 -3.845701 -3.76709 -3.8172469 -3.92268 -4.0270886 -4.1164284 -4.1752191 -4.2104888 -4.2231207][-4.2519913 -4.2075343 -4.1525455 -4.0942597 -4.0291991 -3.9401464 -3.8181736 -3.7313924 -3.77085 -3.8901982 -4.0060291 -4.0951581 -4.1542721 -4.1853542 -4.1909819][-4.2529712 -4.2089477 -4.1543689 -4.10078 -4.0519605 -4.0002956 -3.9320788 -3.8677626 -3.8842587 -3.9750957 -4.0652895 -4.1265855 -4.1657705 -4.18209 -4.1748452][-4.2789979 -4.2485986 -4.2113523 -4.175838 -4.1464105 -4.123126 -4.0965896 -4.0624709 -4.0646639 -4.1141405 -4.1660652 -4.1967826 -4.213686 -4.2151771 -4.1962881][-4.3085089 -4.293148 -4.2747793 -4.2564492 -4.2411675 -4.23276 -4.2264318 -4.2122164 -4.209939 -4.2314081 -4.254427 -4.2675567 -4.2716651 -4.2671041 -4.2475128][-4.3257217 -4.3197951 -4.314024 -4.3073707 -4.3008142 -4.29901 -4.2985148 -4.2934442 -4.292408 -4.3023324 -4.312335 -4.3143382 -4.31153 -4.3052735 -4.2930908][-4.3272557 -4.327 -4.3294535 -4.3310971 -4.3311939 -4.3331218 -4.3350787 -4.3342543 -4.3342814 -4.3385878 -4.3419557 -4.3378181 -4.3312979 -4.325882 -4.3207569][-4.3153057 -4.319653 -4.328527 -4.3351936 -4.3373094 -4.3394523 -4.3428149 -4.3462596 -4.3491306 -4.351243 -4.350709 -4.3445449 -4.3367639 -4.3310461 -4.3284988]]...]
INFO - root - 2017-12-07 23:02:37.569036: step 64310, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 50h:41m:32s remains)
INFO - root - 2017-12-07 23:02:44.471569: step 64320, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 48h:18m:23s remains)
INFO - root - 2017-12-07 23:02:51.376508: step 64330, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 51h:39m:21s remains)
INFO - root - 2017-12-07 23:02:58.158311: step 64340, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 51h:09m:44s remains)
INFO - root - 2017-12-07 23:03:04.998945: step 64350, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 53h:33m:53s remains)
INFO - root - 2017-12-07 23:03:11.864705: step 64360, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 50h:56m:10s remains)
INFO - root - 2017-12-07 23:03:18.587413: step 64370, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 48h:20m:40s remains)
INFO - root - 2017-12-07 23:03:25.382945: step 64380, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 49h:04m:05s remains)
INFO - root - 2017-12-07 23:03:32.242037: step 64390, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 52h:34m:33s remains)
INFO - root - 2017-12-07 23:03:39.088844: step 64400, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 51h:58m:19s remains)
2017-12-07 23:03:39.913805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3088779 -4.3081708 -4.3050756 -4.304389 -4.3011265 -4.2974839 -4.2953143 -4.2945638 -4.295332 -4.297008 -4.29769 -4.2951622 -4.2909474 -4.2826266 -4.2738891][-4.2878881 -4.2862792 -4.2855778 -4.2904205 -4.2924547 -4.2931523 -4.2936826 -4.2947021 -4.2969918 -4.2996397 -4.2995028 -4.2963204 -4.2904992 -4.2785969 -4.2650232][-4.2717791 -4.2705641 -4.2737083 -4.2825994 -4.2867351 -4.2870731 -4.2853489 -4.2838106 -4.2860608 -4.2899685 -4.2917342 -4.290545 -4.285933 -4.2733426 -4.2580762][-4.2608919 -4.2573848 -4.2575626 -4.2642412 -4.2649927 -4.2571964 -4.2462358 -4.2377625 -4.2386122 -4.24415 -4.2490635 -4.2521896 -4.2528143 -4.2444782 -4.2328329][-4.2567124 -4.2490153 -4.2421479 -4.2409897 -4.2324634 -4.2114673 -4.1872272 -4.1708407 -4.1720939 -4.1817422 -4.1904306 -4.1974845 -4.2019954 -4.1980948 -4.1931591][-4.2391233 -4.2206221 -4.201921 -4.1875024 -4.1630821 -4.1251917 -4.0869541 -4.0635548 -4.0701118 -4.0910115 -4.1125274 -4.1317639 -4.1463838 -4.1515231 -4.1556582][-4.178978 -4.1424675 -4.1066003 -4.0750742 -4.0321703 -3.9757547 -3.9206548 -3.8897736 -3.9076769 -3.9479442 -3.9891865 -4.02772 -4.0575466 -4.0738459 -4.0890985][-4.10641 -4.0552955 -4.0082207 -3.9694998 -3.9171283 -3.8468411 -3.7770195 -3.7364302 -3.7642016 -3.8226213 -3.8820956 -3.9383819 -3.9815018 -4.0043912 -4.0242391][-4.06227 -4.0173855 -3.9855542 -3.9694979 -3.9424229 -3.8950009 -3.8480673 -3.8203135 -3.8414702 -3.8910561 -3.9411275 -3.9847014 -4.0151296 -4.0249987 -4.0322671][-4.0497088 -4.023778 -4.0180407 -4.0302725 -4.0332522 -4.0167503 -3.9998989 -3.9900606 -4.0049839 -4.0372853 -4.06585 -4.0850587 -4.093884 -4.0865517 -4.0785627][-4.0594406 -4.05118 -4.0652204 -4.0936027 -4.1117373 -4.109817 -4.1056471 -4.1028743 -4.1117558 -4.1292877 -4.1431317 -4.1507096 -4.1505442 -4.1379333 -4.125762][-4.1120768 -4.1139741 -4.1346068 -4.1651788 -4.185051 -4.1859846 -4.1843195 -4.1833868 -4.1882806 -4.19728 -4.2053471 -4.2108836 -4.2106848 -4.1998754 -4.19069][-4.2100286 -4.2177258 -4.2360835 -4.2588034 -4.2731495 -4.2741418 -4.273706 -4.2733064 -4.275918 -4.2801309 -4.2833428 -4.2868714 -4.288599 -4.2817512 -4.2771683][-4.2910395 -4.3009429 -4.3136439 -4.3262134 -4.3332267 -4.3326511 -4.3314395 -4.3305449 -4.3316073 -4.3329945 -4.3339095 -4.3367944 -4.3407869 -4.3395863 -4.339088][-4.3367577 -4.3461328 -4.3541536 -4.3602343 -4.3626842 -4.3615403 -4.3602386 -4.359292 -4.3591871 -4.3591132 -4.359334 -4.3615079 -4.3648996 -4.3664069 -4.3674493]]...]
INFO - root - 2017-12-07 23:03:46.554366: step 64410, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 51h:44m:55s remains)
INFO - root - 2017-12-07 23:03:53.301862: step 64420, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 48h:49m:51s remains)
INFO - root - 2017-12-07 23:04:00.054600: step 64430, loss = 2.07, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 48h:55m:59s remains)
INFO - root - 2017-12-07 23:04:06.727605: step 64440, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 47h:00m:56s remains)
INFO - root - 2017-12-07 23:04:13.586423: step 64450, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 51h:34m:51s remains)
INFO - root - 2017-12-07 23:04:20.331092: step 64460, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 49h:57m:31s remains)
INFO - root - 2017-12-07 23:04:27.131432: step 64470, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.660 sec/batch; 49h:07m:28s remains)
INFO - root - 2017-12-07 23:04:33.964705: step 64480, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 50h:23m:04s remains)
INFO - root - 2017-12-07 23:04:40.749009: step 64490, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 48h:53m:41s remains)
INFO - root - 2017-12-07 23:04:47.516572: step 64500, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 53h:33m:47s remains)
2017-12-07 23:04:48.238638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2074523 -4.2022996 -4.1903491 -4.1832213 -4.174654 -4.1588931 -4.1420112 -4.1296768 -4.1313124 -4.146841 -4.1708918 -4.1938596 -4.2052116 -4.1866894 -4.1548872][-4.1879511 -4.1915369 -4.1871166 -4.1823945 -4.1733661 -4.1561584 -4.1359015 -4.1174469 -4.1135836 -4.1259565 -4.1491842 -4.173368 -4.1913614 -4.1832042 -4.1565013][-4.2013478 -4.2158527 -4.2193961 -4.2179585 -4.2112494 -4.195169 -4.1754112 -4.1569328 -4.1512666 -4.1601729 -4.1788177 -4.1994877 -4.2163248 -4.2126746 -4.187511][-4.2327342 -4.247849 -4.2520323 -4.2481761 -4.24249 -4.2308135 -4.2160583 -4.2023211 -4.1971655 -4.204205 -4.2205138 -4.2378321 -4.252667 -4.2551847 -4.2330995][-4.2500548 -4.2574153 -4.2590766 -4.2523146 -4.2479196 -4.2397389 -4.223166 -4.1994796 -4.1903625 -4.2075086 -4.2342834 -4.2549553 -4.2712779 -4.28103 -4.2689939][-4.2338009 -4.227232 -4.2210269 -4.2089992 -4.2016568 -4.1900373 -4.1606746 -4.115963 -4.103363 -4.1400056 -4.1862512 -4.2163835 -4.24068 -4.2643423 -4.2706928][-4.1948671 -4.1750851 -4.159502 -4.1398153 -4.1250811 -4.0982676 -4.0387077 -3.9573553 -3.9393933 -4.0058327 -4.0811968 -4.129674 -4.1712942 -4.2141714 -4.240458][-4.1707697 -4.1467943 -4.1295428 -4.1085629 -4.0865216 -4.0372262 -3.9385757 -3.8144665 -3.7903497 -3.8843842 -3.9850237 -4.0493393 -4.106595 -4.1661716 -4.2074103][-4.1974797 -4.1771617 -4.1657596 -4.1542282 -4.1383204 -4.0887427 -3.9853833 -3.8626232 -3.83706 -3.9131691 -3.9945626 -4.0452037 -4.0963902 -4.1568847 -4.2047434][-4.251729 -4.236876 -4.2332377 -4.237236 -4.2383661 -4.2063527 -4.1308827 -4.0444546 -4.021934 -4.05923 -4.1037889 -4.1317873 -4.1659155 -4.2098465 -4.2481833][-4.2891693 -4.2748175 -4.2741466 -4.2888227 -4.2999744 -4.2863054 -4.2452159 -4.202075 -4.1922293 -4.209188 -4.2326283 -4.2487159 -4.2665005 -4.2874689 -4.3069863][-4.2930875 -4.27198 -4.267827 -4.2833362 -4.29871 -4.2984452 -4.2841668 -4.2723083 -4.2758145 -4.2918 -4.3122163 -4.3250394 -4.3333664 -4.3407741 -4.3455105][-4.2729855 -4.2378173 -4.2264132 -4.2396917 -4.2552 -4.2628827 -4.2617526 -4.2647948 -4.2779489 -4.3007808 -4.3256674 -4.33922 -4.3449945 -4.3488712 -4.3508434][-4.2514257 -4.2056251 -4.18996 -4.2016177 -4.2177815 -4.2289677 -4.2328691 -4.2423382 -4.2601953 -4.2865677 -4.3136497 -4.3282132 -4.3337049 -4.3385754 -4.3425765][-4.2557316 -4.2158184 -4.2028871 -4.2121 -4.223979 -4.2320781 -4.2374434 -4.2514553 -4.2692571 -4.2902756 -4.3123722 -4.3251815 -4.3297315 -4.3335557 -4.3364053]]...]
INFO - root - 2017-12-07 23:04:54.856718: step 64510, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 49h:55m:05s remains)
INFO - root - 2017-12-07 23:05:01.655655: step 64520, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 49h:19m:01s remains)
INFO - root - 2017-12-07 23:05:08.355797: step 64530, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 53h:03m:45s remains)
INFO - root - 2017-12-07 23:05:15.172166: step 64540, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.697 sec/batch; 51h:52m:41s remains)
INFO - root - 2017-12-07 23:05:21.915694: step 64550, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 46h:56m:43s remains)
INFO - root - 2017-12-07 23:05:28.631013: step 64560, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 48h:23m:24s remains)
INFO - root - 2017-12-07 23:05:35.432205: step 64570, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 52h:14m:25s remains)
INFO - root - 2017-12-07 23:05:42.257489: step 64580, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.736 sec/batch; 54h:46m:06s remains)
INFO - root - 2017-12-07 23:05:49.063557: step 64590, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 51h:45m:45s remains)
INFO - root - 2017-12-07 23:05:55.805499: step 64600, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:28m:26s remains)
2017-12-07 23:05:56.510876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2412534 -4.1962214 -4.1716542 -4.1562891 -4.1491637 -4.1249814 -4.079566 -4.08385 -4.130734 -4.1742244 -4.1874652 -4.1839108 -4.1846724 -4.1973538 -4.2037678][-4.2420239 -4.20957 -4.198133 -4.1909409 -4.1844654 -4.149632 -4.097146 -4.1007948 -4.1401658 -4.17301 -4.1821289 -4.1814175 -4.1856737 -4.195199 -4.1952744][-4.2453051 -4.2184658 -4.2062817 -4.1981554 -4.1906161 -4.1491117 -4.0982571 -4.1067357 -4.1409326 -4.167016 -4.1746483 -4.1795106 -4.1860909 -4.1916165 -4.1796761][-4.2475605 -4.2150373 -4.1943235 -4.1752625 -4.1576719 -4.1062927 -4.0542555 -4.0645561 -4.1052194 -4.1357284 -4.1520262 -4.16369 -4.1699448 -4.1676507 -4.1398797][-4.2478924 -4.2046895 -4.171597 -4.137876 -4.0997648 -4.0282626 -3.9652672 -3.993283 -4.0554962 -4.0980873 -4.1239247 -4.1416187 -4.1468744 -4.1321096 -4.08533][-4.2442431 -4.1891913 -4.1409521 -4.0913782 -4.0273504 -3.9129572 -3.803561 -3.8565524 -3.970257 -4.0460358 -4.0906496 -4.116693 -4.126102 -4.1067052 -4.0483084][-4.2372675 -4.1761684 -4.1138053 -4.0442328 -3.9453511 -3.7703743 -3.5923738 -3.6960938 -3.8985257 -4.0132537 -4.071878 -4.1066136 -4.1226997 -4.1032777 -4.0339017][-4.2320852 -4.167407 -4.1023 -4.026895 -3.9299467 -3.7688801 -3.6101902 -3.7343411 -3.9452269 -4.0500731 -4.097548 -4.1243372 -4.1338019 -4.1068172 -4.0349412][-4.2339783 -4.1724119 -4.1158113 -4.0607424 -4.0021849 -3.9218929 -3.8426559 -3.921329 -4.0552459 -4.1179171 -4.1370277 -4.139308 -4.1383653 -4.1160975 -4.0656967][-4.2465954 -4.1969523 -4.1528382 -4.115757 -4.0857296 -4.0526891 -4.0103188 -4.0515022 -4.131341 -4.164742 -4.1641054 -4.1459594 -4.1384878 -4.1290226 -4.1056213][-4.2620764 -4.2194252 -4.1824708 -4.1534247 -4.1388588 -4.1206741 -4.08459 -4.0987568 -4.1514993 -4.1690025 -4.1564426 -4.1360722 -4.1313605 -4.1320996 -4.1370521][-4.264143 -4.2193975 -4.1834588 -4.1555142 -4.1430249 -4.1245446 -4.0804033 -4.0842748 -4.1334038 -4.1505237 -4.1393752 -4.1286831 -4.1396961 -4.1552858 -4.1732349][-4.2617431 -4.2124977 -4.168251 -4.1370854 -4.1253877 -4.1075673 -4.0599375 -4.0669727 -4.128541 -4.1555271 -4.1533914 -4.1555586 -4.175159 -4.1906571 -4.2033215][-4.2637978 -4.2156739 -4.1736474 -4.1469116 -4.142673 -4.1248879 -4.079989 -4.0841928 -4.1483235 -4.1873655 -4.1981668 -4.2029629 -4.2155428 -4.2246833 -4.2293615][-4.2695241 -4.230514 -4.1964107 -4.1760039 -4.1787105 -4.1719365 -4.1392059 -4.1387897 -4.1924052 -4.2283387 -4.2404976 -4.244699 -4.252224 -4.2569437 -4.2586622]]...]
INFO - root - 2017-12-07 23:06:03.207226: step 64610, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 51h:39m:08s remains)
INFO - root - 2017-12-07 23:06:10.116183: step 64620, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 49h:48m:05s remains)
INFO - root - 2017-12-07 23:06:16.968467: step 64630, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 48h:17m:21s remains)
INFO - root - 2017-12-07 23:06:23.935734: step 64640, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 51h:44m:27s remains)
INFO - root - 2017-12-07 23:06:30.793732: step 64650, loss = 2.08, batch loss = 2.03 (11.1 examples/sec; 0.723 sec/batch; 53h:48m:16s remains)
INFO - root - 2017-12-07 23:06:37.580580: step 64660, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 50h:08m:41s remains)
INFO - root - 2017-12-07 23:06:44.346578: step 64670, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 49h:30m:46s remains)
INFO - root - 2017-12-07 23:06:51.060114: step 64680, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.653 sec/batch; 48h:34m:35s remains)
INFO - root - 2017-12-07 23:06:57.986492: step 64690, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 52h:53m:59s remains)
INFO - root - 2017-12-07 23:07:04.771272: step 64700, loss = 2.03, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 52h:57m:25s remains)
2017-12-07 23:07:05.480284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438517 -4.2276621 -4.2019453 -4.1744952 -4.1564503 -4.1580811 -4.1770368 -4.2073226 -4.2329593 -4.2414393 -4.2428441 -4.2489061 -4.2420068 -4.2287211 -4.2334437][-4.2441092 -4.2206979 -4.1861072 -4.1530151 -4.1346178 -4.1365476 -4.1565475 -4.18943 -4.2175813 -4.2269559 -4.2267771 -4.2298436 -4.2255654 -4.2188044 -4.2286072][-4.2491183 -4.2194734 -4.1787672 -4.1374645 -4.112205 -4.108551 -4.1234856 -4.1604476 -4.1973767 -4.2161927 -4.2179308 -4.2156973 -4.2096834 -4.2043271 -4.2154222][-4.258688 -4.2317066 -4.1909661 -4.1422362 -4.1050296 -4.0852232 -4.0843682 -4.1184349 -4.167367 -4.2033472 -4.2157326 -4.2157235 -4.2101645 -4.2050185 -4.212141][-4.268899 -4.2547884 -4.2226763 -4.1701865 -4.1130824 -4.066659 -4.041357 -4.0624342 -4.1210623 -4.177628 -4.207994 -4.2176919 -4.2191839 -4.217011 -4.2174125][-4.2779789 -4.2778535 -4.2605305 -4.21165 -4.13759 -4.0594745 -4.0004573 -3.9942696 -4.0540247 -4.1316023 -4.1838169 -4.20996 -4.2225041 -4.2246137 -4.2208452][-4.2800226 -4.2907858 -4.28927 -4.25351 -4.1798677 -4.0799465 -3.983829 -3.9349871 -3.9743814 -4.0627422 -4.1337204 -4.1740561 -4.1954451 -4.2040887 -4.2058043][-4.2770791 -4.2954125 -4.3060703 -4.2870007 -4.2264237 -4.1223378 -4.0016136 -3.9090796 -3.9076481 -3.9943271 -4.0802436 -4.1298523 -4.1549373 -4.1699033 -4.180922][-4.2699194 -4.2928157 -4.3127704 -4.3091211 -4.2661171 -4.1752639 -4.0536041 -3.9398594 -3.9009495 -3.9741614 -4.0651994 -4.1166143 -4.1377082 -4.149776 -4.1640906][-4.2621665 -4.28252 -4.3063107 -4.3137369 -4.2881289 -4.2204151 -4.1177125 -4.0099726 -3.9579847 -4.0083208 -4.0881481 -4.1284161 -4.1356378 -4.1399322 -4.1535435][-4.2565269 -4.269166 -4.2911663 -4.30525 -4.2949615 -4.2506719 -4.1733284 -4.0842481 -4.0303335 -4.059196 -4.1206632 -4.1453872 -4.1350126 -4.130115 -4.14373][-4.2572417 -4.2598591 -4.2766609 -4.2941155 -4.2973094 -4.2742467 -4.2184949 -4.1467204 -4.0952635 -4.1057186 -4.147727 -4.1587186 -4.1375523 -4.1293507 -4.1416039][-4.2627707 -4.2582188 -4.2668247 -4.2848182 -4.2958813 -4.2881436 -4.2496643 -4.1936984 -4.1484842 -4.1482539 -4.1732788 -4.1765943 -4.1548996 -4.1442289 -4.1477218][-4.265903 -4.258534 -4.2591581 -4.2732081 -4.285708 -4.2871513 -4.2633061 -4.2214894 -4.183084 -4.174365 -4.1866379 -4.1905942 -4.1755867 -4.1627645 -4.15798][-4.2678523 -4.2606297 -4.2549086 -4.2615089 -4.2713685 -4.2748241 -4.2585239 -4.2243438 -4.1915607 -4.1774421 -4.1788673 -4.1885223 -4.1864529 -4.1786547 -4.1721306]]...]
INFO - root - 2017-12-07 23:07:12.143873: step 64710, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 49h:18m:01s remains)
INFO - root - 2017-12-07 23:07:18.923248: step 64720, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 51h:36m:02s remains)
INFO - root - 2017-12-07 23:07:25.668850: step 64730, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 51h:21m:31s remains)
INFO - root - 2017-12-07 23:07:32.426519: step 64740, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 50h:54m:44s remains)
INFO - root - 2017-12-07 23:07:38.965667: step 64750, loss = 2.03, batch loss = 1.97 (16.2 examples/sec; 0.493 sec/batch; 36h:41m:52s remains)
INFO - root - 2017-12-07 23:07:45.789585: step 64760, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 52h:44m:51s remains)
INFO - root - 2017-12-07 23:07:52.594498: step 64770, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 55h:25m:48s remains)
INFO - root - 2017-12-07 23:07:59.427018: step 64780, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 53h:39m:44s remains)
INFO - root - 2017-12-07 23:08:06.209962: step 64790, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 47h:11m:16s remains)
INFO - root - 2017-12-07 23:08:13.041965: step 64800, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 49h:59m:44s remains)
2017-12-07 23:08:13.807881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2862744 -4.2728019 -4.2721367 -4.283277 -4.2931943 -4.2946835 -4.2958455 -4.2998252 -4.2997923 -4.280138 -4.2404447 -4.190505 -4.1517048 -4.1329265 -4.1715007][-4.2734814 -4.2600617 -4.2537065 -4.2559872 -4.257966 -4.2556915 -4.2600446 -4.2738504 -4.2860746 -4.2805996 -4.249207 -4.1999226 -4.1582093 -4.1393652 -4.1757259][-4.265821 -4.2557783 -4.2460551 -4.2392278 -4.2314057 -4.2205448 -4.22212 -4.2387681 -4.2610922 -4.2681966 -4.2432652 -4.1936407 -4.1505718 -4.1361213 -4.1749229][-4.2534933 -4.2490835 -4.2369781 -4.2233086 -4.2049642 -4.1865473 -4.179955 -4.191246 -4.2173424 -4.2363687 -4.220911 -4.1753507 -4.1381388 -4.130837 -4.1734085][-4.2265058 -4.2319732 -4.223371 -4.2049565 -4.1789947 -4.1523762 -4.1293378 -4.1258769 -4.1523304 -4.1843944 -4.1873465 -4.1534781 -4.1260281 -4.1289635 -4.1758475][-4.1991048 -4.2128811 -4.2096553 -4.1910925 -4.1591611 -4.12434 -4.0871186 -4.06826 -4.08829 -4.1261711 -4.141386 -4.1207094 -4.1026049 -4.115108 -4.1709104][-4.186451 -4.2093134 -4.2124109 -4.197681 -4.1687279 -4.1293621 -4.08356 -4.0558038 -4.0652928 -4.0965033 -4.1103935 -4.0947089 -4.0791178 -4.0960531 -4.160789][-4.1999273 -4.2283163 -4.2394009 -4.2298956 -4.2076941 -4.1732073 -4.1320992 -4.10544 -4.1052079 -4.12411 -4.1302748 -4.113174 -4.0917635 -4.1035228 -4.1658907][-4.230948 -4.26459 -4.2810445 -4.2733669 -4.2573628 -4.233902 -4.2063241 -4.1847858 -4.1783595 -4.18567 -4.1827922 -4.1640034 -4.1399612 -4.1442008 -4.1946526][-4.2657652 -4.2987752 -4.3169079 -4.3103294 -4.2965 -4.2822084 -4.2689981 -4.2562337 -4.2476087 -4.2486196 -4.241508 -4.2210016 -4.199091 -4.1977172 -4.2329288][-4.2965364 -4.3224072 -4.3396029 -4.3345432 -4.3217797 -4.3125691 -4.3087258 -4.3019762 -4.2937846 -4.2937484 -4.2886162 -4.2717094 -4.2544255 -4.2505374 -4.2719455][-4.3145046 -4.3322659 -4.3462644 -4.3436704 -4.3345494 -4.328021 -4.327466 -4.3237863 -4.3208318 -4.3218446 -4.3170605 -4.3052144 -4.2911978 -4.2854733 -4.2985206][-4.3149757 -4.327395 -4.3389907 -4.3387141 -4.3338037 -4.3304143 -4.332376 -4.33117 -4.3335114 -4.3366218 -4.3344784 -4.327775 -4.3166156 -4.3089066 -4.3143148][-4.3032928 -4.3142948 -4.3230462 -4.3231 -4.3206673 -4.3207564 -4.3226514 -4.3227267 -4.3274 -4.3312182 -4.3318481 -4.3311176 -4.3246326 -4.3161969 -4.3168135][-4.2878628 -4.2971811 -4.3023391 -4.3005495 -4.2982168 -4.3002853 -4.3022432 -4.3033137 -4.3070283 -4.3094139 -4.3111019 -4.3139234 -4.3118162 -4.3077049 -4.3094516]]...]
INFO - root - 2017-12-07 23:08:20.426766: step 64810, loss = 2.11, batch loss = 2.05 (11.6 examples/sec; 0.688 sec/batch; 51h:08m:05s remains)
INFO - root - 2017-12-07 23:08:27.309505: step 64820, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 50h:41m:47s remains)
INFO - root - 2017-12-07 23:08:34.099904: step 64830, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 49h:49m:45s remains)
INFO - root - 2017-12-07 23:08:41.059653: step 64840, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 52h:09m:08s remains)
INFO - root - 2017-12-07 23:08:47.891858: step 64850, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 49h:46m:51s remains)
INFO - root - 2017-12-07 23:08:54.675091: step 64860, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 49h:33m:20s remains)
INFO - root - 2017-12-07 23:09:01.445555: step 64870, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 52h:16m:10s remains)
INFO - root - 2017-12-07 23:09:08.205905: step 64880, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 53h:39m:03s remains)
INFO - root - 2017-12-07 23:09:14.926356: step 64890, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 52h:57m:09s remains)
INFO - root - 2017-12-07 23:09:21.762377: step 64900, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 49h:55m:57s remains)
2017-12-07 23:09:22.530737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.320076 -4.322135 -4.3024721 -4.2765427 -4.2521653 -4.2220755 -4.1952386 -4.1707969 -4.148541 -4.1320348 -4.1316113 -4.1538424 -4.191153 -4.2273965 -4.2462683][-4.3280563 -4.3307467 -4.3107233 -4.2850575 -4.2617416 -4.2332458 -4.2107816 -4.1923041 -4.1801224 -4.1765342 -4.1874008 -4.2108383 -4.2361097 -4.2605743 -4.2640166][-4.3184824 -4.3181362 -4.298481 -4.2693706 -4.2416067 -4.2097287 -4.18551 -4.17444 -4.1754327 -4.1840634 -4.2029696 -4.2215629 -4.2393823 -4.2540889 -4.2565947][-4.3029552 -4.2958407 -4.271266 -4.2343106 -4.1933808 -4.1497188 -4.1163673 -4.1071959 -4.1214209 -4.1431494 -4.1647162 -4.1830883 -4.2038474 -4.2214508 -4.2368422][-4.284903 -4.2698088 -4.2384448 -4.1897497 -4.1332288 -4.0804768 -4.0381603 -4.01974 -4.0353718 -4.0621862 -4.090054 -4.1224403 -4.1662087 -4.2028646 -4.2357507][-4.2602496 -4.2385869 -4.2006254 -4.1438775 -4.0799179 -4.0187783 -3.9621077 -3.9304507 -3.9397538 -3.959228 -3.9905877 -4.0480738 -4.126296 -4.1914577 -4.2449665][-4.2364545 -4.2111068 -4.1763844 -4.1220293 -4.0657406 -4.0051837 -3.9446063 -3.9045877 -3.8947396 -3.8905032 -3.9185534 -3.9978426 -4.09994 -4.184021 -4.2498217][-4.2287931 -4.2032371 -4.1774807 -4.1387081 -4.10137 -4.0555816 -4.0064015 -3.965992 -3.9375384 -3.916697 -3.9398527 -4.0209832 -4.1196952 -4.1987867 -4.2598867][-4.2316775 -4.2062941 -4.188168 -4.1690493 -4.1565027 -4.1281981 -4.0906568 -4.058145 -4.0273924 -4.0063462 -4.0307784 -4.1027617 -4.1802745 -4.2387781 -4.2822714][-4.2408156 -4.221684 -4.214098 -4.2134094 -4.2175446 -4.2008691 -4.1709585 -4.1428781 -4.1124334 -4.0941963 -4.1160378 -4.1733742 -4.2303915 -4.2717037 -4.3006167][-4.2632389 -4.2534523 -4.2539625 -4.2653713 -4.276041 -4.265162 -4.2408485 -4.2179866 -4.193223 -4.1784482 -4.1949644 -4.234211 -4.2709923 -4.2977657 -4.3172665][-4.29171 -4.2882442 -4.2954388 -4.3112774 -4.32228 -4.3126807 -4.2894444 -4.2659326 -4.2445283 -4.2354693 -4.2534833 -4.2825418 -4.3070149 -4.3241048 -4.3353553][-4.3101277 -4.3088503 -4.3164892 -4.33164 -4.3423939 -4.3324037 -4.3115988 -4.29193 -4.2769179 -4.2738104 -4.2930264 -4.3178606 -4.3345122 -4.3444595 -4.3475394][-4.30334 -4.3034582 -4.3111596 -4.3240228 -4.3295374 -4.3185573 -4.3019514 -4.28915 -4.2809787 -4.2823224 -4.3010497 -4.3247514 -4.3388519 -4.3466635 -4.3464031][-4.2863045 -4.2912731 -4.3010077 -4.3102231 -4.3098264 -4.2978063 -4.2830458 -4.2735291 -4.2688241 -4.2717638 -4.2883244 -4.3110776 -4.3267083 -4.3360615 -4.3381958]]...]
INFO - root - 2017-12-07 23:09:29.100648: step 64910, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 52h:34m:56s remains)
INFO - root - 2017-12-07 23:09:35.879770: step 64920, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 49h:46m:57s remains)
INFO - root - 2017-12-07 23:09:42.754223: step 64930, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.659 sec/batch; 48h:57m:08s remains)
INFO - root - 2017-12-07 23:09:49.539153: step 64940, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 50h:14m:24s remains)
INFO - root - 2017-12-07 23:09:56.373567: step 64950, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 53h:20m:39s remains)
INFO - root - 2017-12-07 23:10:03.232193: step 64960, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 53h:34m:02s remains)
INFO - root - 2017-12-07 23:10:10.113907: step 64970, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 50h:23m:35s remains)
INFO - root - 2017-12-07 23:10:16.840534: step 64980, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.643 sec/batch; 47h:47m:33s remains)
INFO - root - 2017-12-07 23:10:23.551671: step 64990, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 47h:44m:55s remains)
INFO - root - 2017-12-07 23:10:30.391334: step 65000, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 53h:03m:11s remains)
2017-12-07 23:10:31.064867: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2611008 -4.2649016 -4.2649293 -4.2635665 -4.2618527 -4.253356 -4.2477956 -4.2493477 -4.2574935 -4.257812 -4.2415276 -4.224925 -4.2268763 -4.2380929 -4.2458348][-4.2567544 -4.2639961 -4.2616897 -4.2557034 -4.2517729 -4.2392778 -4.2312307 -4.2335877 -4.2464709 -4.251421 -4.2341995 -4.2080655 -4.2013826 -4.2092342 -4.218854][-4.2547665 -4.2624626 -4.2582479 -4.2467093 -4.233201 -4.2107229 -4.1959286 -4.2034435 -4.228682 -4.2458167 -4.2334824 -4.2033296 -4.1852722 -4.1831703 -4.1893764][-4.249361 -4.2548933 -4.2462621 -4.2259836 -4.1965537 -4.1543045 -4.1284046 -4.1524577 -4.2053704 -4.2387218 -4.2291331 -4.1958642 -4.1734114 -4.1624866 -4.1601815][-4.2313719 -4.2286067 -4.21047 -4.1796823 -4.1300116 -4.0575781 -4.0093937 -4.0446496 -4.1349545 -4.2033405 -4.2086835 -4.1749516 -4.1534061 -4.1387568 -4.1255207][-4.2049394 -4.1973467 -4.1750331 -4.1334038 -4.0611448 -3.9476249 -3.857187 -3.8917332 -4.0216556 -4.1336288 -4.1690226 -4.1511555 -4.1376696 -4.1244979 -4.1038184][-4.1900296 -4.1837249 -4.1596785 -4.1092134 -4.0175805 -3.8751571 -3.7516332 -3.7742019 -3.9340496 -4.0755672 -4.1361103 -4.1413665 -4.1429553 -4.1401691 -4.1172628][-4.2028785 -4.2013144 -4.1771407 -4.1286573 -4.0410247 -3.9151845 -3.8118246 -3.8340702 -3.9730518 -4.0958781 -4.1486754 -4.1616068 -4.1713514 -4.1744061 -4.1521115][-4.231554 -4.2380171 -4.222815 -4.1895142 -4.1322074 -4.0514603 -3.9870822 -4.001483 -4.087007 -4.1559367 -4.1830521 -4.1877613 -4.19346 -4.1898441 -4.1599603][-4.2491832 -4.2556367 -4.2496929 -4.2353706 -4.2113428 -4.1613274 -4.1049604 -4.1003551 -4.1548991 -4.1951766 -4.2087903 -4.2112813 -4.2086029 -4.18959 -4.14633][-4.2535391 -4.2507982 -4.2436543 -4.2367353 -4.2304072 -4.2009249 -4.1480832 -4.1351166 -4.1824303 -4.2167187 -4.2260494 -4.2278962 -4.2217073 -4.1909552 -4.1387687][-4.241972 -4.2276974 -4.2152376 -4.2134037 -4.2190232 -4.2061005 -4.1656709 -4.1564727 -4.2036133 -4.2336416 -4.2347345 -4.2376218 -4.2305641 -4.19695 -4.14488][-4.2347441 -4.2096305 -4.1964622 -4.2032061 -4.2183814 -4.2183018 -4.1939435 -4.1897044 -4.224618 -4.2471256 -4.2431731 -4.241241 -4.2321739 -4.2052231 -4.1639853][-4.2514968 -4.2215328 -4.2062178 -4.2150507 -4.2336721 -4.238668 -4.2278872 -4.2283044 -4.248477 -4.2648168 -4.261447 -4.2549982 -4.2450919 -4.2286644 -4.2031116][-4.2774868 -4.2542839 -4.2401609 -4.24469 -4.2577829 -4.2612391 -4.2549653 -4.2550025 -4.2633958 -4.2753315 -4.278245 -4.2757297 -4.2706637 -4.2652168 -4.2549758]]...]
INFO - root - 2017-12-07 23:10:37.696159: step 65010, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 48h:34m:59s remains)
INFO - root - 2017-12-07 23:10:44.513932: step 65020, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 49h:52m:18s remains)
INFO - root - 2017-12-07 23:10:51.312784: step 65030, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 51h:50m:59s remains)
INFO - root - 2017-12-07 23:10:58.177675: step 65040, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 55h:11m:33s remains)
INFO - root - 2017-12-07 23:11:05.068832: step 65050, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 46h:52m:57s remains)
INFO - root - 2017-12-07 23:11:11.768184: step 65060, loss = 2.06, batch loss = 2.00 (13.9 examples/sec; 0.575 sec/batch; 42h:42m:49s remains)
INFO - root - 2017-12-07 23:11:18.513610: step 65070, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 52h:17m:45s remains)
INFO - root - 2017-12-07 23:11:25.351676: step 65080, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 51h:32m:53s remains)
INFO - root - 2017-12-07 23:11:32.190463: step 65090, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 48h:54m:04s remains)
INFO - root - 2017-12-07 23:11:39.016458: step 65100, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 48h:19m:10s remains)
2017-12-07 23:11:39.865297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3279305 -4.3282161 -4.3329892 -4.3395977 -4.3388829 -4.326334 -4.3055449 -4.2856679 -4.2748237 -4.2732534 -4.2755146 -4.2849422 -4.2848239 -4.2780304 -4.2794986][-4.3259215 -4.3240538 -4.3259535 -4.3273525 -4.3195028 -4.2996945 -4.2700839 -4.239615 -4.2245426 -4.2221003 -4.2242613 -4.2407904 -4.2488647 -4.2434196 -4.2404661][-4.3271475 -4.3222642 -4.317945 -4.31371 -4.2955608 -4.2697563 -4.2287 -4.1818614 -4.1624141 -4.1605964 -4.1634479 -4.18904 -4.2120457 -4.2106457 -4.1989651][-4.32473 -4.3187456 -4.3067479 -4.2919755 -4.2632189 -4.2302866 -4.1765523 -4.1118369 -4.0886612 -4.0926709 -4.0968595 -4.1314034 -4.1690187 -4.173707 -4.1624942][-4.3202443 -4.3159685 -4.3001771 -4.27915 -4.2430458 -4.2015057 -4.1376214 -4.0536318 -4.020165 -4.0300393 -4.0381722 -4.0774126 -4.1303406 -4.1456752 -4.134047][-4.3166313 -4.3186555 -4.3054686 -4.2821159 -4.2439489 -4.1897469 -4.1131616 -4.0149245 -3.9688382 -3.9787414 -3.9894657 -4.0397549 -4.110785 -4.1358151 -4.123179][-4.3134513 -4.320508 -4.311204 -4.2878246 -4.2417006 -4.1677532 -4.0799274 -3.9683475 -3.9119503 -3.9233503 -3.9428878 -4.0142956 -4.1014585 -4.131063 -4.115458][-4.3085551 -4.3149571 -4.3037009 -4.2856808 -4.2348289 -4.141583 -4.0360284 -3.9111071 -3.8541279 -3.881808 -3.9216347 -4.0146422 -4.1103373 -4.1486077 -4.1410685][-4.300487 -4.3020105 -4.2893691 -4.2811842 -4.2331729 -4.1346979 -4.0241246 -3.9063768 -3.8652644 -3.9152036 -3.9745197 -4.0661626 -4.1480947 -4.1867604 -4.1914334][-4.2965169 -4.2969422 -4.2899065 -4.2830205 -4.2402654 -4.1551542 -4.0649076 -3.9812927 -3.9613628 -4.015306 -4.075088 -4.1425252 -4.1933017 -4.220118 -4.2306843][-4.2974091 -4.3006158 -4.2978826 -4.2890978 -4.2555366 -4.1986127 -4.1425037 -4.0895281 -4.0772643 -4.1235261 -4.1680579 -4.2067814 -4.2321076 -4.2465539 -4.2542825][-4.3000169 -4.3006287 -4.2989788 -4.2896304 -4.2682405 -4.2380347 -4.2075663 -4.1755567 -4.1682754 -4.2020164 -4.2308245 -4.2542257 -4.2687721 -4.2762823 -4.2781672][-4.3073273 -4.3069878 -4.3058758 -4.299963 -4.2888341 -4.276516 -4.2595253 -4.2377596 -4.2320724 -4.254539 -4.2711916 -4.2857065 -4.2954774 -4.2997508 -4.2983661][-4.3138113 -4.3143964 -4.31477 -4.3123689 -4.3074579 -4.3032579 -4.2953892 -4.2831974 -4.2783904 -4.2890143 -4.29526 -4.3035593 -4.3109446 -4.3138285 -4.3146276][-4.3187752 -4.3185339 -4.3197231 -4.3193412 -4.3167872 -4.3155966 -4.3119764 -4.3051915 -4.3026128 -4.3043938 -4.301425 -4.3041763 -4.3101072 -4.3149734 -4.3191938]]...]
INFO - root - 2017-12-07 23:11:46.500595: step 65110, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 52h:32m:42s remains)
INFO - root - 2017-12-07 23:11:53.256188: step 65120, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.645 sec/batch; 47h:56m:18s remains)
INFO - root - 2017-12-07 23:12:00.119068: step 65130, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 50h:07m:30s remains)
INFO - root - 2017-12-07 23:12:06.883925: step 65140, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 53h:02m:42s remains)
INFO - root - 2017-12-07 23:12:13.730807: step 65150, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 50h:39m:28s remains)
INFO - root - 2017-12-07 23:12:20.540424: step 65160, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 50h:12m:44s remains)
INFO - root - 2017-12-07 23:12:27.274299: step 65170, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 50h:23m:15s remains)
INFO - root - 2017-12-07 23:12:34.076601: step 65180, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 47h:33m:07s remains)
INFO - root - 2017-12-07 23:12:40.879569: step 65190, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 50h:36m:42s remains)
INFO - root - 2017-12-07 23:12:47.668924: step 65200, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 53h:59m:30s remains)
2017-12-07 23:12:48.433855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.229465 -4.2244167 -4.2130809 -4.2080116 -4.2127075 -4.2159643 -4.2156267 -4.2118545 -4.2140989 -4.2227511 -4.2294726 -4.2340941 -4.2391949 -4.248013 -4.256722][-4.2344627 -4.228085 -4.216681 -4.2111154 -4.2121625 -4.2118478 -4.2098517 -4.2055268 -4.2056036 -4.2104783 -4.2132411 -4.2148213 -4.2189612 -4.2307096 -4.24305][-4.236239 -4.2278357 -4.2159691 -4.2081208 -4.2071204 -4.2069936 -4.2072835 -4.20592 -4.2054162 -4.2043781 -4.1984005 -4.1925941 -4.1900067 -4.1971765 -4.2086058][-4.2299433 -4.2175169 -4.2028475 -4.1884685 -4.1833496 -4.1827264 -4.1906581 -4.2006965 -4.2047138 -4.2012429 -4.188446 -4.1735072 -4.1616879 -4.158844 -4.1652808][-4.2114973 -4.1918821 -4.1705966 -4.149168 -4.1343312 -4.1278391 -4.1436672 -4.1721811 -4.1906657 -4.192678 -4.1785793 -4.1588464 -4.142962 -4.135416 -4.1386046][-4.1858821 -4.1539049 -4.1200218 -4.0851908 -4.0479369 -4.0186639 -4.0335279 -4.086926 -4.132225 -4.1543069 -4.1541305 -4.14362 -4.1308784 -4.1239338 -4.1242962][-4.1548882 -4.1080475 -4.0592332 -4.0078254 -3.9480653 -3.884227 -3.8834391 -3.9593821 -4.0347886 -4.0813351 -4.1033449 -4.1126013 -4.1116905 -4.1086893 -4.1078515][-4.1349669 -4.0791388 -4.0241637 -3.9721313 -3.90794 -3.8263416 -3.8065577 -3.8780375 -3.9594228 -4.0193167 -4.0603313 -4.0886559 -4.1027923 -4.108284 -4.1119776][-4.1539969 -4.1046867 -4.0626769 -4.033464 -3.9947832 -3.9415264 -3.9269381 -3.9661629 -4.018868 -4.066051 -4.1038647 -4.1313138 -4.1474667 -4.1546431 -4.1605344][-4.196363 -4.1623774 -4.1414437 -4.1341043 -4.1197357 -4.0959597 -4.0921645 -4.1091695 -4.1352549 -4.1655631 -4.1907082 -4.2057037 -4.213419 -4.21714 -4.221765][-4.2326956 -4.21462 -4.2094288 -4.2162819 -4.2169733 -4.2109451 -4.2132831 -4.2200089 -4.2292743 -4.2457066 -4.257937 -4.2622552 -4.2627788 -4.2622166 -4.2637787][-4.2426338 -4.2381415 -4.2421584 -4.2567496 -4.267736 -4.273139 -4.2769475 -4.2782116 -4.2793574 -4.2861085 -4.2897024 -4.2874446 -4.2860112 -4.2844019 -4.2816963][-4.2393622 -4.2399697 -4.2462263 -4.2612586 -4.2767291 -4.2888765 -4.2955923 -4.2959385 -4.2934513 -4.2947135 -4.294879 -4.2909226 -4.2891293 -4.2878442 -4.2829294][-4.2465711 -4.2491961 -4.2558069 -4.2685895 -4.2835937 -4.2958236 -4.3033133 -4.3045011 -4.302084 -4.3018646 -4.3010736 -4.2987542 -4.29884 -4.2983336 -4.2927175][-4.26296 -4.2673717 -4.2725987 -4.2817063 -4.2911363 -4.29738 -4.3011122 -4.3016996 -4.3016744 -4.3034172 -4.305572 -4.3071127 -4.3096237 -4.3109589 -4.3073621]]...]
INFO - root - 2017-12-07 23:12:55.111998: step 65210, loss = 2.04, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 49h:38m:48s remains)
INFO - root - 2017-12-07 23:13:01.844443: step 65220, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 50h:50m:39s remains)
INFO - root - 2017-12-07 23:13:08.559636: step 65230, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 50h:56m:02s remains)
INFO - root - 2017-12-07 23:13:15.282599: step 65240, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 49h:32m:54s remains)
INFO - root - 2017-12-07 23:13:22.012848: step 65250, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 50h:27m:41s remains)
INFO - root - 2017-12-07 23:13:28.642627: step 65260, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 48h:16m:22s remains)
INFO - root - 2017-12-07 23:13:35.359041: step 65270, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 48h:50m:22s remains)
INFO - root - 2017-12-07 23:13:42.082904: step 65280, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.702 sec/batch; 52h:04m:45s remains)
INFO - root - 2017-12-07 23:13:48.864712: step 65290, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 52h:24m:14s remains)
INFO - root - 2017-12-07 23:13:55.594705: step 65300, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 50h:50m:10s remains)
2017-12-07 23:13:56.354581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054214 -4.3191533 -4.3276749 -4.3309097 -4.3339849 -4.3384242 -4.3406348 -4.33348 -4.3194609 -4.2972927 -4.2643876 -4.2359118 -4.2212429 -4.2176385 -4.2178974][-4.3296852 -4.3333669 -4.3272319 -4.3138409 -4.3036685 -4.3009429 -4.3056483 -4.3089461 -4.30867 -4.2979412 -4.2717719 -4.248786 -4.2371945 -4.2378769 -4.2463555][-4.3351254 -4.331203 -4.3117485 -4.2819829 -4.2559948 -4.2410069 -4.2470469 -4.2661309 -4.2790852 -4.2805495 -4.2689581 -4.2544069 -4.2438393 -4.2456632 -4.2597189][-4.3268356 -4.3202314 -4.2921209 -4.2493815 -4.2055764 -4.1684346 -4.1648192 -4.1983733 -4.2320628 -4.2509017 -4.2577128 -4.2567239 -4.2517123 -4.2572527 -4.2733865][-4.3114729 -4.3007159 -4.267725 -4.2179646 -4.1542187 -4.0809536 -4.0477452 -4.0892882 -4.1587415 -4.2106271 -4.2409043 -4.2562084 -4.2624497 -4.2740774 -4.2886057][-4.305738 -4.2917833 -4.2567959 -4.2033815 -4.1231585 -4.0077014 -3.9174073 -3.9467795 -4.0587854 -4.15786 -4.2175083 -4.2481956 -4.2667251 -4.285121 -4.2981319][-4.3002558 -4.2881918 -4.2582741 -4.2120867 -4.1274257 -3.9897053 -3.8365352 -3.8159654 -3.9524829 -4.0922155 -4.1794987 -4.2230959 -4.2497096 -4.2739587 -4.2906122][-4.3063822 -4.297956 -4.2749667 -4.2370596 -4.1680641 -4.0491543 -3.8862031 -3.8023262 -3.9035952 -4.0479884 -4.1447768 -4.1965547 -4.2275777 -4.2576647 -4.282949][-4.3146472 -4.3125634 -4.3001275 -4.2749848 -4.2305608 -4.1515465 -4.027091 -3.9293931 -3.9563148 -4.057107 -4.1427994 -4.1934037 -4.2244954 -4.2550278 -4.2839851][-4.3019528 -4.3046441 -4.3034425 -4.2930527 -4.2704229 -4.2274289 -4.1505456 -4.0708055 -4.0511975 -4.0949678 -4.1531029 -4.1946478 -4.2219334 -4.2489491 -4.2735624][-4.2794256 -4.2837296 -4.2871904 -4.287302 -4.2817144 -4.2640238 -4.2216144 -4.1698279 -4.1393895 -4.1477675 -4.176403 -4.203486 -4.2241793 -4.2437248 -4.259923][-4.2587533 -4.2618885 -4.2668376 -4.27312 -4.2765579 -4.2723355 -4.2518349 -4.221168 -4.1978555 -4.1964059 -4.2076507 -4.2197566 -4.2312479 -4.2432394 -4.2505474][-4.2548122 -4.2549667 -4.2573667 -4.26053 -4.26432 -4.26386 -4.2557421 -4.24338 -4.2340026 -4.2351341 -4.2397242 -4.2418971 -4.2457819 -4.2520819 -4.2555652][-4.2549 -4.252934 -4.2524886 -4.2534995 -4.257607 -4.2587624 -4.25413 -4.2504539 -4.2502117 -4.2533092 -4.2557826 -4.2546706 -4.2543759 -4.2564569 -4.2582703][-4.25842 -4.2554107 -4.2549992 -4.2560678 -4.2605948 -4.2647357 -4.2637978 -4.2619042 -4.262876 -4.2651219 -4.2668171 -4.2650843 -4.2633486 -4.264112 -4.2648745]]...]
INFO - root - 2017-12-07 23:14:03.004357: step 65310, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.722 sec/batch; 53h:33m:05s remains)
INFO - root - 2017-12-07 23:14:09.874885: step 65320, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 50h:33m:51s remains)
INFO - root - 2017-12-07 23:14:16.700567: step 65330, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 49h:11m:34s remains)
INFO - root - 2017-12-07 23:14:23.497074: step 65340, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 49h:00m:31s remains)
INFO - root - 2017-12-07 23:14:30.360429: step 65350, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 54h:01m:49s remains)
INFO - root - 2017-12-07 23:14:37.131232: step 65360, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 51h:36m:31s remains)
INFO - root - 2017-12-07 23:14:43.899850: step 65370, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.642 sec/batch; 47h:37m:42s remains)
INFO - root - 2017-12-07 23:14:50.470228: step 65380, loss = 2.03, batch loss = 1.98 (12.3 examples/sec; 0.650 sec/batch; 48h:12m:13s remains)
INFO - root - 2017-12-07 23:14:57.167830: step 65390, loss = 2.04, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 50h:20m:59s remains)
INFO - root - 2017-12-07 23:15:04.053398: step 65400, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 54h:16m:00s remains)
2017-12-07 23:15:04.837752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2991381 -4.2959404 -4.2968535 -4.2977686 -4.2977533 -4.2968144 -4.296103 -4.2962227 -4.2967753 -4.2978106 -4.3002129 -4.3033538 -4.3057537 -4.3056803 -4.3034525][-4.3049812 -4.3033252 -4.3057308 -4.3061323 -4.3040872 -4.3014693 -4.3004432 -4.30211 -4.3043385 -4.3075848 -4.31196 -4.3170424 -4.320755 -4.3206077 -4.3170576][-4.3017664 -4.3043485 -4.3093972 -4.3067 -4.2979808 -4.2903271 -4.2887678 -4.2937889 -4.29957 -4.3072057 -4.3146467 -4.3228788 -4.3283505 -4.3283539 -4.3236027][-4.3061719 -4.310689 -4.3127875 -4.3024287 -4.2836666 -4.2684326 -4.2628064 -4.2685513 -4.2794428 -4.2917261 -4.3023252 -4.3150978 -4.3259816 -4.3325524 -4.33299][-4.3131895 -4.3174686 -4.3117828 -4.2880569 -4.2522111 -4.220643 -4.2051578 -4.2122192 -4.2355313 -4.2575812 -4.2738261 -4.2938795 -4.31354 -4.329596 -4.3377929][-4.2918787 -4.2929912 -4.2786074 -4.2388721 -4.1804047 -4.1231985 -4.0932755 -4.1085582 -4.1533575 -4.189517 -4.2161946 -4.2479286 -4.2793875 -4.3059459 -4.322288][-4.2283063 -4.21877 -4.1920791 -4.133985 -4.0490708 -3.9602723 -3.91731 -3.9578118 -4.0362725 -4.0889435 -4.1248941 -4.16926 -4.2162042 -4.2541175 -4.2783513][-4.1640034 -4.14443 -4.1076159 -4.0349669 -3.9280086 -3.8058624 -3.7462149 -3.8205037 -3.9321628 -3.9998345 -4.0443311 -4.0993209 -4.1613421 -4.2084355 -4.2384553][-4.1651993 -4.1421814 -4.101491 -4.0286088 -3.9268146 -3.804842 -3.7459896 -3.8314862 -3.947319 -4.0137916 -4.0577745 -4.1104789 -4.1700897 -4.2143211 -4.2423186][-4.2223372 -4.2039971 -4.1708207 -4.1161919 -4.0432982 -3.9538915 -3.9175971 -3.9867682 -4.0775757 -4.1260853 -4.1587114 -4.1983409 -4.2410483 -4.2693939 -4.2840848][-4.2819252 -4.2755375 -4.256197 -4.2226763 -4.1770577 -4.1180792 -4.0982933 -4.1428661 -4.2017741 -4.2325072 -4.2542768 -4.2805781 -4.3035674 -4.3132443 -4.3089161][-4.3030162 -4.3074841 -4.3014894 -4.2850122 -4.2583523 -4.2237554 -4.2169795 -4.2434497 -4.2774467 -4.2953796 -4.3083568 -4.3244514 -4.3329029 -4.3308654 -4.3143778][-4.2873158 -4.2978597 -4.3026667 -4.3001547 -4.2899189 -4.2745275 -4.2753806 -4.2900743 -4.3073807 -4.3142567 -4.3195243 -4.328866 -4.3305912 -4.3238988 -4.3053522][-4.2614527 -4.2677045 -4.2703919 -4.2692018 -4.2664928 -4.2637491 -4.269681 -4.2797651 -4.2909 -4.2926154 -4.2953053 -4.3018823 -4.3016915 -4.2939148 -4.2775178][-4.2389855 -4.238637 -4.2331853 -4.2241182 -4.2173004 -4.215476 -4.2226963 -4.2328711 -4.2421675 -4.2422695 -4.24576 -4.2545829 -4.2585883 -4.2559128 -4.2464175]]...]
INFO - root - 2017-12-07 23:15:11.395601: step 65410, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 48h:09m:40s remains)
INFO - root - 2017-12-07 23:15:18.377180: step 65420, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 51h:41m:40s remains)
INFO - root - 2017-12-07 23:15:25.179205: step 65430, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 51h:41m:34s remains)
INFO - root - 2017-12-07 23:15:32.055735: step 65440, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.682 sec/batch; 50h:34m:19s remains)
INFO - root - 2017-12-07 23:15:38.886501: step 65450, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 49h:27m:14s remains)
INFO - root - 2017-12-07 23:15:45.665204: step 65460, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 52h:50m:46s remains)
INFO - root - 2017-12-07 23:15:52.472192: step 65470, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 51h:38m:38s remains)
INFO - root - 2017-12-07 23:15:59.203258: step 65480, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 50h:34m:06s remains)
INFO - root - 2017-12-07 23:16:05.918943: step 65490, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 47h:05m:18s remains)
INFO - root - 2017-12-07 23:16:12.705264: step 65500, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 47h:27m:14s remains)
2017-12-07 23:16:13.463080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438903 -4.2504992 -4.2499366 -4.2406292 -4.2291131 -4.2135968 -4.1995234 -4.1963716 -4.2051773 -4.2190123 -4.2195807 -4.2186422 -4.2226934 -4.2253122 -4.2290888][-4.2556071 -4.2632437 -4.2676368 -4.2606506 -4.2485242 -4.23326 -4.2166162 -4.2106166 -4.2183247 -4.2249355 -4.2114353 -4.1953497 -4.1896157 -4.1932173 -4.2052507][-4.2618222 -4.2637811 -4.2637072 -4.2518058 -4.2392659 -4.2292166 -4.2201762 -4.2220516 -4.2337732 -4.2363973 -4.2136211 -4.1822948 -4.161212 -4.158453 -4.1706238][-4.2671237 -4.2641187 -4.2536025 -4.233098 -4.218442 -4.2123337 -4.2145972 -4.2287412 -4.2444324 -4.2486987 -4.2240114 -4.1857767 -4.1566553 -4.1466904 -4.1496415][-4.261765 -4.2546921 -4.2330413 -4.206882 -4.1863365 -4.1812997 -4.1950827 -4.2173181 -4.2359829 -4.2428107 -4.2252421 -4.1948915 -4.1697359 -4.1595826 -4.1537137][-4.2483768 -4.2382336 -4.2118831 -4.1815643 -4.1567783 -4.1497478 -4.164782 -4.1908307 -4.2162266 -4.2262888 -4.2187366 -4.2032132 -4.1892781 -4.1821837 -4.1727133][-4.2259045 -4.2167206 -4.1946473 -4.1715322 -4.1528006 -4.1445212 -4.1470346 -4.1639481 -4.1853161 -4.1947827 -4.1992459 -4.1996369 -4.198339 -4.194726 -4.1845131][-4.196104 -4.1968403 -4.1874533 -4.179493 -4.1758461 -4.1629591 -4.1398945 -4.1330938 -4.1353083 -4.1369333 -4.1573725 -4.1770711 -4.188283 -4.190722 -4.17946][-4.1713405 -4.1772161 -4.1812816 -4.1911564 -4.2015052 -4.1876516 -4.1405764 -4.1010966 -4.0766449 -4.0712185 -4.1108217 -4.1486073 -4.1691666 -4.1719909 -4.1533489][-4.1598616 -4.1642051 -4.1729116 -4.1947904 -4.2117352 -4.2024293 -4.1528735 -4.099268 -4.0544047 -4.0428667 -4.0890708 -4.1286936 -4.1483378 -4.1503077 -4.1296711][-4.1543217 -4.1474276 -4.1533251 -4.181282 -4.2087049 -4.2084265 -4.169888 -4.1229048 -4.0792308 -4.0641747 -4.1018238 -4.128336 -4.1415572 -4.1486683 -4.1363144][-4.151154 -4.1398654 -4.1437297 -4.1796141 -4.2186413 -4.2217174 -4.1927609 -4.1594868 -4.1256251 -4.1135325 -4.1397305 -4.1498108 -4.1526604 -4.1644564 -4.168128][-4.161634 -4.1508107 -4.1568074 -4.1982245 -4.2361584 -4.2380586 -4.2131729 -4.185967 -4.1581659 -4.1486583 -4.1665912 -4.1672897 -4.1659789 -4.1797156 -4.195446][-4.1736922 -4.1653967 -4.1703138 -4.2096214 -4.24264 -4.2445507 -4.2218685 -4.1951208 -4.1696339 -4.1645675 -4.1803336 -4.1765089 -4.1713681 -4.185761 -4.2078691][-4.1790371 -4.170701 -4.1709256 -4.2018037 -4.2314887 -4.2358575 -4.2171655 -4.1941862 -4.1718121 -4.1718192 -4.1882892 -4.1822586 -4.1712632 -4.1809292 -4.2048125]]...]
INFO - root - 2017-12-07 23:16:20.069313: step 65510, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.695 sec/batch; 51h:34m:30s remains)
INFO - root - 2017-12-07 23:16:26.854662: step 65520, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 46h:46m:00s remains)
INFO - root - 2017-12-07 23:16:33.572920: step 65530, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.649 sec/batch; 48h:08m:22s remains)
INFO - root - 2017-12-07 23:16:40.336470: step 65540, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 50h:56m:12s remains)
INFO - root - 2017-12-07 23:16:47.044839: step 65550, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 49h:17m:07s remains)
INFO - root - 2017-12-07 23:16:53.836944: step 65560, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.692 sec/batch; 51h:17m:23s remains)
INFO - root - 2017-12-07 23:17:00.644839: step 65570, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 48h:37m:20s remains)
INFO - root - 2017-12-07 23:17:07.402576: step 65580, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 48h:03m:00s remains)
INFO - root - 2017-12-07 23:17:14.183558: step 65590, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 48h:20m:28s remains)
INFO - root - 2017-12-07 23:17:20.976325: step 65600, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 52h:31m:49s remains)
2017-12-07 23:17:21.698379: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.158699 -4.1403918 -4.1400709 -4.1477785 -4.1484108 -4.1454058 -4.1457758 -4.1511054 -4.1793923 -4.2156124 -4.2129197 -4.1799512 -4.1402187 -4.1079955 -4.0942383][-4.148478 -4.1327882 -4.1354632 -4.14181 -4.1409459 -4.1362405 -4.1331997 -4.1382294 -4.1697326 -4.2081928 -4.2097063 -4.1775589 -4.1362228 -4.0978355 -4.0818191][-4.1348715 -4.119699 -4.1195889 -4.1226244 -4.1180649 -4.1061926 -4.0966477 -4.1014276 -4.138732 -4.1827164 -4.1921606 -4.1648889 -4.1308808 -4.0978045 -4.0827675][-4.1170607 -4.1001325 -4.0966496 -4.0947132 -4.0783672 -4.0526748 -4.0341864 -4.041647 -4.0910306 -4.1465364 -4.167604 -4.1511841 -4.1251945 -4.0999932 -4.0888276][-4.1062317 -4.0898452 -4.0844007 -4.0704203 -4.0335584 -3.9853559 -3.9560738 -3.9740591 -4.0418749 -4.1136632 -4.1485295 -4.14292 -4.1196375 -4.0945172 -4.0826716][-4.1055789 -4.0936074 -4.08371 -4.05404 -3.9940915 -3.9210651 -3.8837981 -3.920114 -4.0083461 -4.0930533 -4.1389251 -4.14354 -4.119925 -4.0855885 -4.0683031][-4.1079354 -4.1080008 -4.0928264 -4.0501242 -3.9748394 -3.8913984 -3.8570983 -3.9071941 -4.0039864 -4.0874038 -4.135798 -4.1484537 -4.1263289 -4.08238 -4.0598631][-4.118988 -4.1277318 -4.1126113 -4.0678077 -3.9969764 -3.9273868 -3.9055619 -3.952733 -4.0320287 -4.09578 -4.1343327 -4.1475749 -4.1279922 -4.0849891 -4.0618834][-4.1324282 -4.1450653 -4.1371479 -4.1040297 -4.0517163 -4.0056915 -3.9935405 -4.0237546 -4.074296 -4.1106997 -4.1348925 -4.1455078 -4.1303716 -4.0906463 -4.0663004][-4.1393123 -4.1499782 -4.1496091 -4.1310148 -4.0977116 -4.0720992 -4.0674996 -4.0837259 -4.1075668 -4.1227636 -4.137197 -4.1454759 -4.1316023 -4.0940008 -4.0654058][-4.138761 -4.1423244 -4.1469593 -4.1411037 -4.1240172 -4.1124868 -4.1143107 -4.123364 -4.1316009 -4.1330686 -4.1391516 -4.1433506 -4.1281195 -4.0951939 -4.0679584][-4.132164 -4.133287 -4.1430368 -4.1470571 -4.1389036 -4.13193 -4.1347857 -4.1436577 -4.1488757 -4.1441894 -4.1454768 -4.14636 -4.1305165 -4.1032805 -4.0824437][-4.12948 -4.1315694 -4.1436968 -4.151762 -4.1469507 -4.1399908 -4.1398163 -4.1479378 -4.1558738 -4.1555977 -4.1552372 -4.1513524 -4.135613 -4.1138158 -4.1051445][-4.136703 -4.1386132 -4.1455493 -4.1519642 -4.1490569 -4.1427431 -4.1398644 -4.1476774 -4.1605506 -4.1658654 -4.1645355 -4.1573792 -4.1402736 -4.1242504 -4.12488][-4.1473055 -4.1463995 -4.1465735 -4.1505857 -4.1485028 -4.1438127 -4.1418028 -4.1492033 -4.16039 -4.1673651 -4.1642418 -4.155077 -4.1394229 -4.1283875 -4.1362119]]...]
INFO - root - 2017-12-07 23:17:28.318507: step 65610, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 49h:01m:11s remains)
INFO - root - 2017-12-07 23:17:35.078654: step 65620, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.696 sec/batch; 51h:33m:52s remains)
INFO - root - 2017-12-07 23:17:41.870157: step 65630, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 53h:53m:15s remains)
INFO - root - 2017-12-07 23:17:48.669363: step 65640, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.708 sec/batch; 52h:30m:07s remains)
INFO - root - 2017-12-07 23:17:55.354109: step 65650, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 48h:13m:53s remains)
INFO - root - 2017-12-07 23:18:02.054807: step 65660, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.624 sec/batch; 46h:16m:12s remains)
INFO - root - 2017-12-07 23:18:08.908804: step 65670, loss = 2.06, batch loss = 2.01 (10.9 examples/sec; 0.731 sec/batch; 54h:12m:49s remains)
INFO - root - 2017-12-07 23:18:15.693753: step 65680, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 52h:37m:09s remains)
INFO - root - 2017-12-07 23:18:22.311390: step 65690, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 48h:38m:30s remains)
INFO - root - 2017-12-07 23:18:29.062566: step 65700, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 50h:07m:07s remains)
2017-12-07 23:18:29.854377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.323256 -4.3142891 -4.3009386 -4.2918224 -4.2902036 -4.2958531 -4.3027096 -4.3097587 -4.3124189 -4.2999635 -4.2792344 -4.2640929 -4.2607551 -4.2631431 -4.266737][-4.300385 -4.2881365 -4.270402 -4.2575889 -4.25272 -4.2591372 -4.2713571 -4.2824368 -4.2872739 -4.2731133 -4.2504764 -4.2338 -4.2333765 -4.24221 -4.2510881][-4.2545433 -4.2422867 -4.2232728 -4.2064519 -4.1958752 -4.2025776 -4.2219186 -4.2360005 -4.2383013 -4.219264 -4.197722 -4.18637 -4.1918845 -4.2102127 -4.2294436][-4.1983929 -4.1821303 -4.1593909 -4.1334724 -4.1145639 -4.129477 -4.16473 -4.184689 -4.182971 -4.1574731 -4.1362367 -4.13354 -4.146946 -4.1779666 -4.20952][-4.1794753 -4.1568189 -4.1295218 -4.0949793 -4.0675321 -4.0847096 -4.1317339 -4.1553078 -4.1496944 -4.1195807 -4.10136 -4.108778 -4.1325555 -4.1730084 -4.2117095][-4.1852722 -4.1690555 -4.1493821 -4.1154828 -4.0845633 -4.0946393 -4.1363373 -4.1518955 -4.1349707 -4.0896335 -4.06952 -4.0921993 -4.1313262 -4.1808367 -4.220273][-4.1874962 -4.1771474 -4.1680007 -4.1402569 -4.1040282 -4.1030164 -4.1376114 -4.14907 -4.1280584 -4.0765376 -4.0519447 -4.082653 -4.13201 -4.1836104 -4.2249937][-4.1972895 -4.1837683 -4.1750455 -4.1470389 -4.1059842 -4.0981112 -4.1309395 -4.1416469 -4.1197491 -4.0715261 -4.0466833 -4.07633 -4.1242881 -4.1736245 -4.2209883][-4.2289028 -4.210186 -4.1953573 -4.16784 -4.1241093 -4.1132154 -4.1501865 -4.1619444 -4.13951 -4.0963588 -4.0671234 -4.0896873 -4.1342983 -4.1822371 -4.232254][-4.2450714 -4.233243 -4.2229395 -4.20512 -4.1746478 -4.1693792 -4.2101 -4.2231231 -4.203042 -4.1673279 -4.140729 -4.1614203 -4.2005882 -4.2397017 -4.2799454][-4.2463369 -4.2466974 -4.2495546 -4.2480073 -4.2447577 -4.25219 -4.2836862 -4.2885065 -4.2638173 -4.2307868 -4.2059832 -4.2194419 -4.2482619 -4.2775393 -4.3085794][-4.25439 -4.2610922 -4.269424 -4.2720609 -4.2799554 -4.293623 -4.315042 -4.3154473 -4.2913117 -4.2599034 -4.2330637 -4.2372074 -4.2552605 -4.2789993 -4.3078966][-4.2776856 -4.2816434 -4.28494 -4.2855611 -4.2973161 -4.3159189 -4.3339224 -4.3359027 -4.3175578 -4.2890692 -4.2612681 -4.2572851 -4.2690053 -4.2904196 -4.3171687][-4.2972937 -4.2944655 -4.29204 -4.2946672 -4.3087807 -4.32958 -4.3462725 -4.3488812 -4.3333712 -4.3046136 -4.2797184 -4.2772408 -4.2894225 -4.3102775 -4.3318858][-4.2984247 -4.2930946 -4.2898755 -4.2952285 -4.3121691 -4.334043 -4.3484054 -4.3484426 -4.3329287 -4.3033404 -4.2786255 -4.2754965 -4.2889848 -4.3104558 -4.3308258]]...]
INFO - root - 2017-12-07 23:18:36.506641: step 65710, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 51h:30m:28s remains)
INFO - root - 2017-12-07 23:18:43.266415: step 65720, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 47h:21m:11s remains)
INFO - root - 2017-12-07 23:18:50.032182: step 65730, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 51h:46m:45s remains)
INFO - root - 2017-12-07 23:18:56.794566: step 65740, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.702 sec/batch; 51h:59m:56s remains)
INFO - root - 2017-12-07 23:19:03.506686: step 65750, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 50h:16m:46s remains)
INFO - root - 2017-12-07 23:19:10.325119: step 65760, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 53h:22m:51s remains)
INFO - root - 2017-12-07 23:19:17.077684: step 65770, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.651 sec/batch; 48h:14m:07s remains)
INFO - root - 2017-12-07 23:19:23.881671: step 65780, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 49h:52m:52s remains)
INFO - root - 2017-12-07 23:19:30.619817: step 65790, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 52h:32m:16s remains)
INFO - root - 2017-12-07 23:19:37.418516: step 65800, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 53h:18m:15s remains)
2017-12-07 23:19:38.077191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2525229 -4.250762 -4.2487726 -4.2477536 -4.2481565 -4.2493715 -4.2505121 -4.2515106 -4.2512221 -4.2474594 -4.2391434 -4.2291522 -4.2222843 -4.2231984 -4.2325306][-4.2471523 -4.2448254 -4.2417808 -4.2404704 -4.2406893 -4.2415996 -4.241766 -4.2415109 -4.2380772 -4.2273374 -4.2119036 -4.1974192 -4.1882548 -4.1910892 -4.2054167][-4.245533 -4.241343 -4.2368269 -4.235065 -4.2350812 -4.2357955 -4.2351365 -4.2309957 -4.219996 -4.1983318 -4.1737757 -4.1548295 -4.1468868 -4.1546664 -4.1768646][-4.2407055 -4.2335582 -4.2262578 -4.2214427 -4.2178822 -4.2177348 -4.2170458 -4.2109957 -4.1927471 -4.1595211 -4.1242847 -4.1002293 -4.0950413 -4.1083956 -4.1375513][-4.2256532 -4.21573 -4.2054663 -4.1959777 -4.1869864 -4.1843743 -4.1838427 -4.1775508 -4.1593518 -4.1230617 -4.0815654 -4.0533581 -4.0473118 -4.0620036 -4.0914011][-4.1944356 -4.1792011 -4.1627531 -4.1441779 -4.1281137 -4.1257534 -4.1322427 -4.1359072 -4.128149 -4.0981474 -4.0567145 -4.02585 -4.0139627 -4.0203276 -4.0442004][-4.1636329 -4.1395969 -4.1125574 -4.079246 -4.050097 -4.044035 -4.0609832 -4.0836849 -4.092474 -4.0731153 -4.03611 -4.0036793 -3.9851542 -3.9827147 -4.0022054][-4.1617541 -4.1372991 -4.1059475 -4.063714 -4.0249596 -4.0094223 -4.0282736 -4.0606904 -4.0802469 -4.0705624 -4.0416169 -4.009769 -3.9855042 -3.974638 -3.9929419][-4.1871095 -4.171361 -4.1464043 -4.1096048 -4.0724635 -4.0523176 -4.0656476 -4.0940175 -4.1147237 -4.112803 -4.0909467 -4.0620136 -4.0374689 -4.0188441 -4.0331221][-4.2141118 -4.2056084 -4.1889219 -4.1638532 -4.1358333 -4.1200709 -4.1300225 -4.1507907 -4.1705656 -4.1769028 -4.1647973 -4.144424 -4.1244016 -4.1003737 -4.1025395][-4.2288618 -4.2229137 -4.210907 -4.1942816 -4.1744652 -4.1615806 -4.16548 -4.17966 -4.19985 -4.2122564 -4.2123809 -4.2046862 -4.1911149 -4.168756 -4.1628165][-4.2416053 -4.2391577 -4.2329164 -4.2224908 -4.2074151 -4.1956005 -4.1941018 -4.2038922 -4.2214141 -4.2311244 -4.2349291 -4.2309184 -4.220377 -4.2033978 -4.1959777][-4.2512445 -4.254127 -4.2555051 -4.2520161 -4.24197 -4.233171 -4.2302537 -4.2338214 -4.2413611 -4.2415943 -4.2406621 -4.2351522 -4.223 -4.2097907 -4.204998][-4.2495275 -4.2566285 -4.2619696 -4.2610078 -4.2546086 -4.249362 -4.2474208 -4.2479796 -4.2463655 -4.2381353 -4.2307134 -4.2220626 -4.2124395 -4.2047491 -4.20467][-4.232976 -4.2427568 -4.2508221 -4.2523522 -4.2503076 -4.2492323 -4.248981 -4.2489195 -4.2443686 -4.2319889 -4.2199526 -4.2109718 -4.2052708 -4.203639 -4.206214]]...]
INFO - root - 2017-12-07 23:19:44.676815: step 65810, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 49h:19m:07s remains)
INFO - root - 2017-12-07 23:19:51.499512: step 65820, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 53h:28m:02s remains)
INFO - root - 2017-12-07 23:19:58.262785: step 65830, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 50h:31m:04s remains)
INFO - root - 2017-12-07 23:20:05.076263: step 65840, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 50h:46m:11s remains)
INFO - root - 2017-12-07 23:20:11.971022: step 65850, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 48h:57m:38s remains)
INFO - root - 2017-12-07 23:20:18.742369: step 65860, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 47h:26m:45s remains)
INFO - root - 2017-12-07 23:20:25.451196: step 65870, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.691 sec/batch; 51h:09m:38s remains)
INFO - root - 2017-12-07 23:20:32.205717: step 65880, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 51h:53m:58s remains)
INFO - root - 2017-12-07 23:20:38.933559: step 65890, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 50h:25m:12s remains)
INFO - root - 2017-12-07 23:20:45.550877: step 65900, loss = 2.04, batch loss = 1.98 (12.2 examples/sec; 0.654 sec/batch; 48h:23m:58s remains)
2017-12-07 23:20:46.261198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3784709 -4.38206 -4.3833642 -4.383853 -4.3832092 -4.3818078 -4.3805552 -4.3800368 -4.3811183 -4.3823018 -4.3825793 -4.381722 -4.3802152 -4.3779345 -4.3769255][-4.3593526 -4.366725 -4.3707414 -4.3714457 -4.3690166 -4.3636971 -4.3596573 -4.3594131 -4.3634505 -4.3678226 -4.3690872 -4.366673 -4.3619981 -4.3566537 -4.3545804][-4.3247008 -4.3369355 -4.3464255 -4.350173 -4.3472571 -4.3383532 -4.3303094 -4.3301854 -4.3383603 -4.346952 -4.34766 -4.342031 -4.332952 -4.3219547 -4.3151693][-4.284534 -4.3012824 -4.3195424 -4.3292503 -4.3280163 -4.3148079 -4.2986021 -4.2946258 -4.3062582 -4.3197756 -4.3222289 -4.3156962 -4.3025017 -4.2816963 -4.2646627][-4.2394276 -4.2572465 -4.2829471 -4.2970705 -4.2952018 -4.2758784 -4.2494869 -4.2453279 -4.2675571 -4.2926784 -4.3016334 -4.2973747 -4.2798972 -4.2466092 -4.2145019][-4.1906404 -4.2081761 -4.2389312 -4.255198 -4.2512231 -4.22226 -4.1864338 -4.1873446 -4.2244368 -4.261961 -4.2780161 -4.2780666 -4.2591095 -4.2160025 -4.1700077][-4.13793 -4.15183 -4.1865239 -4.2048092 -4.1963315 -4.1553226 -4.1131182 -4.1244149 -4.1806693 -4.2283478 -4.2501078 -4.2521968 -4.2329421 -4.1876526 -4.1372032][-4.0867434 -4.0916533 -4.1274538 -4.1497107 -4.1325483 -4.0744133 -4.0201559 -4.0425277 -4.1196427 -4.1811628 -4.2130618 -4.2219715 -4.2073088 -4.1661143 -4.119051][-4.0684013 -4.0629482 -4.09476 -4.1136842 -4.0839429 -4.0014987 -3.9269204 -3.9515073 -4.0443678 -4.1221662 -4.1642904 -4.1813064 -4.1759467 -4.1504221 -4.1218338][-4.0988832 -4.0874734 -4.1120534 -4.1213441 -4.0822968 -3.9950943 -3.9183841 -3.9382625 -4.0250564 -4.1003995 -4.1406631 -4.156981 -4.1576858 -4.1502852 -4.1425047][-4.1574087 -4.1442404 -4.16269 -4.1673174 -4.1298428 -4.0605946 -4.001122 -4.0123467 -4.0775485 -4.1349263 -4.16018 -4.1670275 -4.1672893 -4.1708708 -4.1782188][-4.2239265 -4.2128897 -4.2244854 -4.2249155 -4.1930876 -4.1429658 -4.1046066 -4.1143074 -4.1591244 -4.197886 -4.2098246 -4.2056341 -4.1993771 -4.2050362 -4.2229075][-4.2766013 -4.2693448 -4.2713575 -4.2647643 -4.2420154 -4.2104917 -4.1898332 -4.2001619 -4.22773 -4.2516022 -4.2560177 -4.2467132 -4.2394509 -4.2472734 -4.2673154][-4.2970786 -4.2945271 -4.2887983 -4.2743373 -4.2573123 -4.2415338 -4.2370486 -4.2518263 -4.2705607 -4.2857466 -4.2861781 -4.2768483 -4.2702937 -4.2736568 -4.2881441][-4.2927008 -4.2916183 -4.2805657 -4.263268 -4.2510333 -4.2446246 -4.2499294 -4.2706184 -4.2894197 -4.3018441 -4.2979679 -4.286005 -4.2750988 -4.2703681 -4.2761831]]...]
INFO - root - 2017-12-07 23:20:52.879661: step 65910, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 50h:53m:50s remains)
INFO - root - 2017-12-07 23:20:59.736512: step 65920, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 52h:12m:42s remains)
INFO - root - 2017-12-07 23:21:06.492668: step 65930, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 47h:00m:01s remains)
INFO - root - 2017-12-07 23:21:13.341728: step 65940, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 49h:08m:56s remains)
INFO - root - 2017-12-07 23:21:20.148683: step 65950, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 51h:13m:56s remains)
INFO - root - 2017-12-07 23:21:26.999534: step 65960, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.715 sec/batch; 52h:57m:28s remains)
INFO - root - 2017-12-07 23:21:33.767004: step 65970, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 52h:21m:34s remains)
INFO - root - 2017-12-07 23:21:40.463230: step 65980, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.639 sec/batch; 47h:16m:32s remains)
INFO - root - 2017-12-07 23:21:47.259988: step 65990, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.653 sec/batch; 48h:20m:49s remains)
INFO - root - 2017-12-07 23:21:53.893334: step 66000, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 54h:00m:04s remains)
2017-12-07 23:21:54.529802: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2523012 -4.2263937 -4.1923423 -4.1826878 -4.1935492 -4.2097983 -4.2185364 -4.2305484 -4.24785 -4.2502 -4.2437773 -4.2477508 -4.250587 -4.249651 -4.2432766][-4.23606 -4.2212391 -4.203125 -4.20639 -4.2254529 -4.23935 -4.2351966 -4.23468 -4.2408514 -4.2358441 -4.2217932 -4.2221026 -4.2242837 -4.2240996 -4.2164559][-4.198946 -4.1856732 -4.1769061 -4.1875777 -4.2141232 -4.2265167 -4.2152424 -4.2150335 -4.2227945 -4.2188773 -4.2067337 -4.2075567 -4.2153373 -4.2126269 -4.196743][-4.1594405 -4.1431317 -4.1446509 -4.163693 -4.18995 -4.1930127 -4.1707249 -4.1795521 -4.1967487 -4.2009034 -4.1963944 -4.1992702 -4.2101641 -4.1998553 -4.1728988][-4.1097703 -4.0919318 -4.1019592 -4.1324511 -4.1555033 -4.1427989 -4.1006002 -4.1226134 -4.1592073 -4.1810694 -4.1874447 -4.1954551 -4.209343 -4.1866069 -4.1469741][-4.0418158 -4.0348687 -4.0520267 -4.0818329 -4.0926151 -4.053998 -3.9850667 -4.0235877 -4.0927596 -4.1455574 -4.1703777 -4.1902404 -4.2071252 -4.1807075 -4.132679][-3.9518733 -3.9646206 -3.9926071 -4.0144267 -4.0026255 -3.9230764 -3.8176012 -3.8798442 -3.9945319 -4.0922337 -4.1483269 -4.1826415 -4.20878 -4.184051 -4.1314631][-3.8934517 -3.921649 -3.947227 -3.9567318 -3.9258115 -3.811342 -3.682651 -3.7680821 -3.9198215 -4.0497665 -4.1256685 -4.1655545 -4.1958613 -4.1749191 -4.123939][-3.9257619 -3.9473605 -3.9535904 -3.9544246 -3.9257345 -3.8277397 -3.7302384 -3.8197627 -3.9582081 -4.0724854 -4.1366935 -4.1640716 -4.1873841 -4.173923 -4.1372361][-3.9900994 -4.0016966 -3.9924917 -3.9883678 -3.9688704 -3.9096894 -3.8635778 -3.947782 -4.0536971 -4.1313491 -4.1660633 -4.1752515 -4.1902595 -4.1816926 -4.16039][-4.0548854 -4.0584865 -4.0420375 -4.0369935 -4.0288658 -4.000833 -3.9906185 -4.05684 -4.1272573 -4.1701612 -4.18056 -4.1794291 -4.1888957 -4.1839042 -4.1756487][-4.137856 -4.13911 -4.123075 -4.1183133 -4.1196175 -4.1096096 -4.1083937 -4.146625 -4.1783328 -4.1866493 -4.1789818 -4.1741815 -4.1810927 -4.1809707 -4.1846809][-4.2168941 -4.2199855 -4.2064476 -4.2001643 -4.2019362 -4.1978264 -4.1896744 -4.2025557 -4.2070713 -4.185751 -4.1530204 -4.1386595 -4.1413412 -4.1481991 -4.1612105][-4.2677722 -4.2719054 -4.262888 -4.2587228 -4.2586436 -4.2524323 -4.2383871 -4.23412 -4.2235885 -4.18485 -4.1348972 -4.1081243 -4.0997963 -4.1045241 -4.1232452][-4.2957344 -4.2982855 -4.2937007 -4.2908649 -4.2874894 -4.2768192 -4.2614074 -4.2541108 -4.2457929 -4.2114096 -4.1584582 -4.1168013 -4.0856566 -4.0772696 -4.0927906]]...]
INFO - root - 2017-12-07 23:22:01.138649: step 66010, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 48h:21m:04s remains)
INFO - root - 2017-12-07 23:22:07.965532: step 66020, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 52h:49m:47s remains)
INFO - root - 2017-12-07 23:22:14.769716: step 66030, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 53h:37m:02s remains)
INFO - root - 2017-12-07 23:22:21.615834: step 66040, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 51h:04m:38s remains)
INFO - root - 2017-12-07 23:22:28.378489: step 66050, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 46h:17m:25s remains)
INFO - root - 2017-12-07 23:22:35.241294: step 66060, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 49h:53m:21s remains)
INFO - root - 2017-12-07 23:22:42.149880: step 66070, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 51h:29m:28s remains)
INFO - root - 2017-12-07 23:22:48.957567: step 66080, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 52h:54m:41s remains)
INFO - root - 2017-12-07 23:22:55.777203: step 66090, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 48h:25m:59s remains)
INFO - root - 2017-12-07 23:23:02.575190: step 66100, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 47h:54m:02s remains)
2017-12-07 23:23:03.365111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3081884 -4.2924571 -4.2834182 -4.2846236 -4.2888618 -4.2906952 -4.2840242 -4.2744155 -4.2646856 -4.2580447 -4.2571816 -4.2662134 -4.2840447 -4.2999573 -4.3122425][-4.2991219 -4.2793345 -4.2680354 -4.2739248 -4.2822714 -4.2810574 -4.2673807 -4.254158 -4.2427187 -4.235126 -4.2348843 -4.2457066 -4.2670736 -4.2863297 -4.3019629][-4.2917886 -4.266923 -4.2510862 -4.252286 -4.2614579 -4.2598667 -4.240572 -4.2249403 -4.2161527 -4.2123694 -4.2149467 -4.2288284 -4.2530332 -4.2741671 -4.2935519][-4.2913 -4.2613521 -4.23774 -4.2309175 -4.2323165 -4.2269006 -4.2025752 -4.1896329 -4.1899076 -4.1941028 -4.2039471 -4.2188921 -4.243094 -4.2658958 -4.28722][-4.2572408 -4.2222428 -4.1934705 -4.182126 -4.1767731 -4.1654577 -4.13866 -4.1357307 -4.1520524 -4.169343 -4.191834 -4.2106085 -4.2368779 -4.2609334 -4.2826362][-4.1908932 -4.1537981 -4.1272793 -4.11203 -4.0941882 -4.0691218 -4.03109 -4.035975 -4.0745316 -4.1107893 -4.1525116 -4.1851091 -4.2207375 -4.2521276 -4.2773557][-4.1110597 -4.0718994 -4.0509448 -4.0326152 -4.00023 -3.9439015 -3.8735056 -3.8768821 -3.942642 -4.0080409 -4.0786471 -4.1390924 -4.1933923 -4.238204 -4.271162][-4.0663481 -4.02309 -4.0059566 -3.9922602 -3.9495695 -3.8633673 -3.7569942 -3.7460005 -3.8311129 -3.9235127 -4.0174575 -4.1027946 -4.1753216 -4.2322955 -4.2712307][-4.115027 -4.077414 -4.0668349 -4.0555439 -4.0173178 -3.941 -3.848006 -3.8259277 -3.885366 -3.9611695 -4.0395122 -4.1190605 -4.1905231 -4.2448697 -4.2812214][-4.1882105 -4.1640263 -4.1572804 -4.1476579 -4.1221895 -4.0730233 -4.0152144 -3.9952745 -4.0208392 -4.0660181 -4.1168742 -4.1737227 -4.228128 -4.2703156 -4.2967367][-4.2439537 -4.2306805 -4.2270265 -4.2167621 -4.2034745 -4.1736836 -4.1438513 -4.134551 -4.14581 -4.1732421 -4.2038546 -4.2389183 -4.2725506 -4.2995391 -4.3126249][-4.2698536 -4.2628684 -4.2624516 -4.2534375 -4.2473392 -4.2353044 -4.2192883 -4.22007 -4.2301903 -4.2493587 -4.27084 -4.2904906 -4.3098235 -4.3245845 -4.3267632][-4.2963061 -4.2915082 -4.2900305 -4.2805295 -4.2790089 -4.27874 -4.2749157 -4.2783332 -4.28766 -4.2994113 -4.3122678 -4.3220835 -4.33173 -4.3390942 -4.3357191][-4.3139486 -4.31109 -4.3094025 -4.3015847 -4.2997 -4.3022013 -4.3011823 -4.3051472 -4.31333 -4.3215256 -4.3304019 -4.3364882 -4.3418312 -4.3445735 -4.3387651][-4.314362 -4.3130803 -4.3112082 -4.3054376 -4.301888 -4.3034148 -4.3041453 -4.3087368 -4.315639 -4.3227167 -4.3289118 -4.3338633 -4.3384771 -4.3405542 -4.3365622]]...]
INFO - root - 2017-12-07 23:23:10.079427: step 66110, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 51h:39m:30s remains)
INFO - root - 2017-12-07 23:23:16.846715: step 66120, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.675 sec/batch; 49h:55m:14s remains)
INFO - root - 2017-12-07 23:23:23.622559: step 66130, loss = 2.05, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 50h:12m:52s remains)
INFO - root - 2017-12-07 23:23:30.428654: step 66140, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.706 sec/batch; 52h:13m:02s remains)
INFO - root - 2017-12-07 23:23:37.390486: step 66150, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 53h:54m:13s remains)
INFO - root - 2017-12-07 23:23:44.242816: step 66160, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 50h:52m:15s remains)
INFO - root - 2017-12-07 23:23:51.032324: step 66170, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.648 sec/batch; 47h:54m:55s remains)
INFO - root - 2017-12-07 23:23:57.863892: step 66180, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 50h:42m:36s remains)
INFO - root - 2017-12-07 23:24:04.783508: step 66190, loss = 2.10, batch loss = 2.04 (10.8 examples/sec; 0.738 sec/batch; 54h:35m:09s remains)
INFO - root - 2017-12-07 23:24:11.582101: step 66200, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 53h:10m:16s remains)
2017-12-07 23:24:12.377279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1487341 -4.1698928 -4.1967578 -4.2201376 -4.2239451 -4.209537 -4.194356 -4.1867218 -4.1846752 -4.1939254 -4.2153888 -4.2372274 -4.2463284 -4.2429447 -4.2394133][-4.1798897 -4.2042484 -4.2326736 -4.2522116 -4.2476883 -4.2219763 -4.1943722 -4.1778226 -4.17372 -4.1823192 -4.198812 -4.2144947 -4.2250047 -4.22958 -4.2341428][-4.2149224 -4.2419944 -4.2684436 -4.2801385 -4.2644482 -4.2289052 -4.1893463 -4.160284 -4.1503515 -4.1576829 -4.1711459 -4.1758242 -4.1860218 -4.199739 -4.2141519][-4.2419176 -4.2621055 -4.2802129 -4.2813005 -4.2557592 -4.2110004 -4.1629958 -4.123673 -4.1091723 -4.1204338 -4.1393776 -4.1495094 -4.1648088 -4.1842909 -4.2048492][-4.2682967 -4.2770443 -4.2826686 -4.2705207 -4.2342696 -4.1793218 -4.11157 -4.053793 -4.0323014 -4.0594177 -4.1066809 -4.1434083 -4.1712809 -4.1944542 -4.2173548][-4.285284 -4.28511 -4.2770014 -4.2547464 -4.2041416 -4.1274171 -4.028502 -3.94336 -3.9118068 -3.9577188 -4.0429049 -4.1151114 -4.1606894 -4.1901746 -4.2182217][-4.2852731 -4.2763715 -4.2591805 -4.2282052 -4.1642442 -4.068429 -3.9366527 -3.8185921 -3.7812674 -3.8543103 -3.9754245 -4.0801516 -4.1429906 -4.1820922 -4.2162185][-4.2721391 -4.2607651 -4.2429986 -4.2115583 -4.1445293 -4.0414248 -3.8987896 -3.7663805 -3.7347643 -3.8308444 -3.9662075 -4.0806165 -4.1551256 -4.2006803 -4.2331181][-4.2693219 -4.2606115 -4.2483559 -4.22654 -4.1743364 -4.0925756 -3.9811926 -3.8768747 -3.8507226 -3.927732 -4.0327797 -4.12266 -4.1878147 -4.2254171 -4.2481189][-4.2723017 -4.2728577 -4.2664242 -4.2555165 -4.220943 -4.1655412 -4.0956326 -4.0282316 -4.0024242 -4.0421305 -4.1043429 -4.1576819 -4.2014794 -4.2221322 -4.232832][-4.2676105 -4.2769427 -4.2737641 -4.2665768 -4.2385139 -4.1987462 -4.1554503 -4.1128054 -4.0954809 -4.1138206 -4.1486578 -4.1752706 -4.1908069 -4.1901832 -4.1842155][-4.254858 -4.2643747 -4.259366 -4.2517281 -4.2272239 -4.1970272 -4.1703968 -4.1465483 -4.1428266 -4.1575384 -4.1806726 -4.188345 -4.17721 -4.1541052 -4.13279][-4.2387352 -4.2458935 -4.24271 -4.2341089 -4.2093673 -4.1831264 -4.162436 -4.1506929 -4.1596036 -4.174685 -4.19199 -4.1876764 -4.1594176 -4.130312 -4.1102128][-4.223062 -4.2312517 -4.2319527 -4.2230949 -4.1963739 -4.1705065 -4.151515 -4.1428976 -4.1537509 -4.1724329 -4.1895175 -4.1788645 -4.148808 -4.133348 -4.1318164][-4.2110591 -4.2179203 -4.2179003 -4.2070889 -4.1801958 -4.1573458 -4.1422696 -4.13514 -4.14323 -4.1636457 -4.1844435 -4.1827307 -4.1673431 -4.1715631 -4.1886864]]...]
INFO - root - 2017-12-07 23:24:18.958660: step 66210, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 49h:20m:18s remains)
INFO - root - 2017-12-07 23:24:25.708219: step 66220, loss = 2.10, batch loss = 2.04 (10.9 examples/sec; 0.733 sec/batch; 54h:12m:22s remains)
INFO - root - 2017-12-07 23:24:32.516948: step 66230, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 45h:45m:37s remains)
INFO - root - 2017-12-07 23:24:39.284983: step 66240, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 47h:28m:12s remains)
INFO - root - 2017-12-07 23:24:46.176201: step 66250, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.697 sec/batch; 51h:35m:00s remains)
INFO - root - 2017-12-07 23:24:53.119739: step 66260, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 52h:16m:39s remains)
INFO - root - 2017-12-07 23:24:59.803733: step 66270, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 50h:00m:03s remains)
INFO - root - 2017-12-07 23:25:06.505131: step 66280, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 48h:12m:50s remains)
INFO - root - 2017-12-07 23:25:13.267626: step 66290, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 47h:58m:48s remains)
INFO - root - 2017-12-07 23:25:20.024319: step 66300, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 49h:34m:44s remains)
2017-12-07 23:25:20.766280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2737646 -4.2739029 -4.2757969 -4.2779865 -4.2799592 -4.2784443 -4.2740211 -4.2649746 -4.2588191 -4.25842 -4.2605081 -4.268959 -4.2791615 -4.2872868 -4.2927341][-4.2688503 -4.272306 -4.276628 -4.2799931 -4.2816949 -4.2749639 -4.260838 -4.2391806 -4.2218022 -4.2163253 -4.2184854 -4.2347617 -4.2558794 -4.2716341 -4.2813864][-4.2664738 -4.2726235 -4.2781549 -4.2821159 -4.2821922 -4.2660136 -4.2352967 -4.192595 -4.1632442 -4.1546912 -4.1609325 -4.191227 -4.2270989 -4.2552652 -4.27152][-4.2574005 -4.2669053 -4.2737875 -4.2769365 -4.2742686 -4.2481375 -4.198493 -4.1301656 -4.09061 -4.0849137 -4.0986 -4.1438231 -4.1961269 -4.2382274 -4.2606978][-4.224493 -4.2384634 -4.2456789 -4.2485843 -4.2438197 -4.2088947 -4.1425705 -4.0501208 -4.0081773 -4.0183287 -4.0475755 -4.1082406 -4.1756582 -4.22635 -4.2509956][-4.1915808 -4.2093029 -4.212369 -4.2110105 -4.2003546 -4.1523304 -4.062695 -3.9439123 -3.912838 -3.9539702 -4.0039797 -4.076508 -4.1581459 -4.2161355 -4.2415514][-4.1586213 -4.1719937 -4.1628366 -4.1505661 -4.1266718 -4.0579805 -3.9339211 -3.7843082 -3.7863169 -3.8802955 -3.958667 -4.0417213 -4.1342998 -4.2008419 -4.2300558][-4.1191516 -4.1167006 -4.0922122 -4.0658116 -4.0263481 -3.936281 -3.7732892 -3.5963445 -3.6500263 -3.8081329 -3.9172254 -4.0123811 -4.1132116 -4.1890721 -4.2238193][-4.0930934 -4.0714664 -4.0318294 -3.9959042 -3.9586282 -3.8794088 -3.720633 -3.5547771 -3.64042 -3.8214197 -3.9359696 -4.0246878 -4.1184964 -4.1940289 -4.228651][-4.1174221 -4.0868459 -4.0404654 -4.0087824 -3.9932034 -3.9572239 -3.8476934 -3.7225151 -3.7849755 -3.9225054 -4.0113931 -4.0805039 -4.1584678 -4.2248125 -4.2510891][-4.1704793 -4.136487 -4.0932913 -4.0723243 -4.0746174 -4.0722513 -4.0084338 -3.922241 -3.95291 -4.03905 -4.10299 -4.1552329 -4.2163057 -4.2681093 -4.2832985][-4.2221317 -4.1852565 -4.1473327 -4.1375961 -4.154078 -4.171855 -4.14159 -4.0866017 -4.09635 -4.1448822 -4.1873231 -4.2231164 -4.2660966 -4.3020678 -4.3092346][-4.2665133 -4.2340617 -4.2031436 -4.1995916 -4.2198906 -4.2440953 -4.239728 -4.2118721 -4.211206 -4.2359147 -4.2600932 -4.2820458 -4.3068628 -4.3274941 -4.32889][-4.2992291 -4.27394 -4.2516074 -4.2487078 -4.263062 -4.2836537 -4.2905097 -4.2801232 -4.2763777 -4.2877293 -4.3004065 -4.3124661 -4.3272333 -4.3398275 -4.33752][-4.3181229 -4.3025908 -4.2896209 -4.288507 -4.295958 -4.3076057 -4.3130879 -4.3099751 -4.306426 -4.3111868 -4.3188176 -4.3263259 -4.3352151 -4.3411584 -4.3376741]]...]
INFO - root - 2017-12-07 23:25:27.206708: step 66310, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 47h:12m:28s remains)
INFO - root - 2017-12-07 23:25:34.003428: step 66320, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 49h:01m:33s remains)
INFO - root - 2017-12-07 23:25:40.902265: step 66330, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 52h:59m:03s remains)
INFO - root - 2017-12-07 23:25:47.693969: step 66340, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 49h:12m:33s remains)
INFO - root - 2017-12-07 23:25:54.477997: step 66350, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 49h:31m:16s remains)
INFO - root - 2017-12-07 23:26:01.275992: step 66360, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 49h:25m:10s remains)
INFO - root - 2017-12-07 23:26:08.089242: step 66370, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.671 sec/batch; 49h:36m:53s remains)
INFO - root - 2017-12-07 23:26:14.849914: step 66380, loss = 2.09, batch loss = 2.04 (11.6 examples/sec; 0.690 sec/batch; 51h:00m:40s remains)
INFO - root - 2017-12-07 23:26:21.498661: step 66390, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 50h:54m:10s remains)
INFO - root - 2017-12-07 23:26:28.338659: step 66400, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 48h:09m:25s remains)
2017-12-07 23:26:29.056488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1788759 -4.1755676 -4.1877503 -4.2044067 -4.2138929 -4.2107916 -4.2019048 -4.2003875 -4.204154 -4.2124043 -4.2188168 -4.2230611 -4.2219524 -4.2166982 -4.2088261][-4.1542025 -4.1497607 -4.1658549 -4.1846685 -4.1970119 -4.1981158 -4.1915946 -4.1923361 -4.1979074 -4.2077169 -4.2134509 -4.2171803 -4.2187362 -4.219645 -4.2164083][-4.1251717 -4.1200991 -4.1404471 -4.161674 -4.1776195 -4.1842909 -4.1826296 -4.1874652 -4.1970835 -4.2097154 -4.2171597 -4.2232065 -4.228652 -4.2337837 -4.2351389][-4.0905294 -4.0797157 -4.1037602 -4.1320949 -4.1563048 -4.170619 -4.1756682 -4.1822786 -4.1929984 -4.2074556 -4.2158866 -4.2209115 -4.2263379 -4.2348609 -4.2425475][-4.0502787 -4.03519 -4.0661268 -4.1052928 -4.1432567 -4.1661334 -4.1742516 -4.1788158 -4.1809874 -4.1880684 -4.1946087 -4.2016854 -4.2103786 -4.2233005 -4.2369871][-4.0323954 -4.0093994 -4.0439057 -4.0948386 -4.1399269 -4.1669292 -4.1707172 -4.1674986 -4.1575546 -4.1522017 -4.1531906 -4.1639757 -4.1819625 -4.1990762 -4.2154922][-4.0612631 -4.0264587 -4.051168 -4.1028585 -4.1464119 -4.1683035 -4.1688852 -4.1598029 -4.1402626 -4.1214905 -4.112493 -4.1196947 -4.1423306 -4.1680021 -4.1919909][-4.1063023 -4.0658441 -4.0765181 -4.1222763 -4.1623755 -4.1774173 -4.17833 -4.1675596 -4.1440778 -4.1182437 -4.1036596 -4.1072235 -4.1268024 -4.152761 -4.1829119][-4.1401205 -4.0980268 -4.1000142 -4.1381969 -4.1744685 -4.187161 -4.1909037 -4.1833205 -4.1651754 -4.143621 -4.1306162 -4.1323037 -4.1452637 -4.1591663 -4.1792917][-4.1666207 -4.1184921 -4.109787 -4.1390576 -4.1738796 -4.1946054 -4.2116323 -4.21503 -4.2073407 -4.1941729 -4.1854053 -4.182826 -4.1865344 -4.1882062 -4.190907][-4.2006397 -4.1458769 -4.1230884 -4.1421571 -4.1773658 -4.2076354 -4.2354655 -4.2521329 -4.2562585 -4.2481575 -4.2361856 -4.2277021 -4.2265644 -4.2222967 -4.215569][-4.222023 -4.1669812 -4.1371112 -4.1464996 -4.1754093 -4.2056608 -4.2400279 -4.266089 -4.2794843 -4.2792659 -4.2691541 -4.2570949 -4.2528687 -4.2463422 -4.2381287][-4.2270913 -4.1791754 -4.1468134 -4.1492395 -4.169837 -4.1918979 -4.2179585 -4.2435932 -4.2640438 -4.274435 -4.2728548 -4.2664614 -4.2624068 -4.2556791 -4.2485676][-4.22898 -4.1900644 -4.1581478 -4.1557913 -4.1667123 -4.1776805 -4.1889157 -4.2069416 -4.2254114 -4.2401762 -4.247251 -4.2471948 -4.2475796 -4.2456884 -4.2437825][-4.2372208 -4.2054505 -4.1760831 -4.1712093 -4.1753917 -4.1747746 -4.1702118 -4.1749911 -4.1850209 -4.1969647 -4.211247 -4.2200146 -4.2258506 -4.2283754 -4.2330952]]...]
INFO - root - 2017-12-07 23:26:35.748527: step 66410, loss = 2.04, batch loss = 1.98 (10.8 examples/sec; 0.741 sec/batch; 54h:44m:37s remains)
INFO - root - 2017-12-07 23:26:42.560482: step 66420, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.682 sec/batch; 50h:26m:11s remains)
INFO - root - 2017-12-07 23:26:49.266813: step 66430, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 47h:26m:13s remains)
INFO - root - 2017-12-07 23:26:56.166989: step 66440, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 49h:48m:31s remains)
INFO - root - 2017-12-07 23:27:03.080316: step 66450, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 52h:04m:33s remains)
INFO - root - 2017-12-07 23:27:09.837001: step 66460, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 51h:00m:29s remains)
INFO - root - 2017-12-07 23:27:16.623580: step 66470, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 47h:55m:32s remains)
INFO - root - 2017-12-07 23:27:23.376298: step 66480, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 47h:49m:48s remains)
INFO - root - 2017-12-07 23:27:30.174086: step 66490, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 49h:03m:57s remains)
INFO - root - 2017-12-07 23:27:37.013604: step 66500, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 52h:21m:28s remains)
2017-12-07 23:27:37.825564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32787 -4.3316164 -4.3337951 -4.3316841 -4.3183346 -4.2837029 -4.2237535 -4.147131 -4.0536227 -3.9376707 -3.8184893 -3.7580538 -3.8007369 -3.897007 -3.9925749][-4.3340273 -4.3396516 -4.3438144 -4.3442078 -4.3348818 -4.306706 -4.2583466 -4.1993318 -4.1284451 -4.0383863 -3.9486547 -3.9027648 -3.9188938 -3.9725595 -4.0363908][-4.3406839 -4.3480654 -4.3542471 -4.3569427 -4.350636 -4.3315415 -4.3005996 -4.2664437 -4.2294679 -4.1783066 -4.1228957 -4.088541 -4.0823536 -4.0955982 -4.1214361][-4.3453908 -4.3532338 -4.3597536 -4.3618207 -4.3546429 -4.3384352 -4.3164415 -4.2989178 -4.2881861 -4.2709994 -4.24653 -4.226119 -4.2133079 -4.2070985 -4.2078075][-4.3480515 -4.3543816 -4.3575807 -4.353466 -4.3359904 -4.3094454 -4.2804089 -4.2629843 -4.2648082 -4.2759509 -4.2843633 -4.2854981 -4.2816381 -4.2757797 -4.2696714][-4.348958 -4.3529143 -4.3490171 -4.3317556 -4.2932172 -4.2446432 -4.1971159 -4.16964 -4.1772184 -4.2150989 -4.2586555 -4.2892189 -4.3038421 -4.3066931 -4.3013282][-4.3502722 -4.3525271 -4.3401861 -4.305943 -4.2405872 -4.1586876 -4.076107 -4.02263 -4.0324006 -4.0983033 -4.1818042 -4.2491055 -4.2907729 -4.3098669 -4.312376][-4.3511992 -4.3524356 -4.3345475 -4.2901473 -4.20809 -4.0954704 -3.9682367 -3.8745222 -3.8816228 -3.9790888 -4.101759 -4.205071 -4.2760925 -4.3136258 -4.3275757][-4.3558745 -4.357996 -4.3396626 -4.296319 -4.2147794 -4.0916142 -3.9463959 -3.8324838 -3.8377604 -3.9473665 -4.0804682 -4.1954341 -4.2765675 -4.3211985 -4.340961][-4.3657227 -4.3694396 -4.3565011 -4.3255577 -4.2642069 -4.1652141 -4.0481477 -3.9603651 -3.9575174 -4.0267448 -4.1196079 -4.2072558 -4.2724061 -4.309783 -4.3269482][-4.3623662 -4.3627462 -4.3544273 -4.3361764 -4.2973561 -4.2310696 -4.1537514 -4.0956316 -4.0812178 -4.1034122 -4.1488886 -4.1985178 -4.2377305 -4.2624011 -4.27457][-4.338398 -4.3294749 -4.3202024 -4.3089418 -4.2885542 -4.2529469 -4.2083516 -4.1688938 -4.147923 -4.1450272 -4.1590858 -4.1780252 -4.198379 -4.211544 -4.21346][-4.3026171 -4.2828479 -4.2701097 -4.2625122 -4.2570238 -4.2440553 -4.2200694 -4.1891365 -4.1636772 -4.148375 -4.1452856 -4.146874 -4.15644 -4.1622505 -4.1521468][-4.2570343 -4.2247224 -4.2065024 -4.2005463 -4.2042646 -4.2059693 -4.1948166 -4.1739087 -4.1535697 -4.1373358 -4.1271219 -4.1237836 -4.1339116 -4.1368566 -4.1153865][-4.2155633 -4.1749654 -4.1532164 -4.1492114 -4.1600547 -4.1714578 -4.1710005 -4.1645017 -4.1562772 -4.1447115 -4.1320682 -4.126967 -4.1361933 -4.1363807 -4.1101794]]...]
INFO - root - 2017-12-07 23:27:44.377385: step 66510, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 48h:13m:00s remains)
INFO - root - 2017-12-07 23:27:51.078313: step 66520, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 52h:42m:32s remains)
INFO - root - 2017-12-07 23:27:57.802224: step 66530, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 52h:20m:08s remains)
INFO - root - 2017-12-07 23:28:04.615375: step 66540, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 50h:08m:04s remains)
INFO - root - 2017-12-07 23:28:11.368733: step 66550, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 48h:10m:14s remains)
INFO - root - 2017-12-07 23:28:18.123412: step 66560, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 46h:48m:44s remains)
INFO - root - 2017-12-07 23:28:25.027618: step 66570, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 51h:41m:51s remains)
INFO - root - 2017-12-07 23:28:31.894256: step 66580, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 53h:21m:34s remains)
INFO - root - 2017-12-07 23:28:38.599699: step 66590, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 50h:48m:36s remains)
INFO - root - 2017-12-07 23:28:45.423916: step 66600, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 48h:55m:29s remains)
2017-12-07 23:28:46.239187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1635919 -4.1391621 -4.1576133 -4.2018437 -4.236546 -4.2512569 -4.2497773 -4.2466693 -4.2502017 -4.25726 -4.2572269 -4.2351079 -4.200871 -4.1772552 -4.1858721][-4.1562886 -4.1262627 -4.148788 -4.2039938 -4.2463241 -4.2587285 -4.2460194 -4.2312098 -4.2290287 -4.2398005 -4.2470603 -4.2329197 -4.2039747 -4.1809168 -4.1897221][-4.1519065 -4.12188 -4.1502051 -4.2111087 -4.2554727 -4.2581739 -4.22479 -4.1901746 -4.1839738 -4.209446 -4.2379813 -4.2409286 -4.2167263 -4.1898232 -4.1907897][-4.1535249 -4.1278505 -4.1619129 -4.22331 -4.2629433 -4.2457838 -4.1842432 -4.129251 -4.1256485 -4.174541 -4.2293386 -4.2525148 -4.2321148 -4.1980729 -4.1870837][-4.1659522 -4.1439228 -4.1759996 -4.2324162 -4.2625346 -4.2243838 -4.1393461 -4.0714812 -4.0782433 -4.1492066 -4.2228346 -4.2606525 -4.2441969 -4.2056336 -4.1842136][-4.1732721 -4.1523595 -4.1780763 -4.2276907 -4.2509604 -4.2016568 -4.1040311 -4.0291767 -4.0495925 -4.13111 -4.2086735 -4.2524338 -4.2408056 -4.200665 -4.1711173][-4.1771774 -4.1526513 -4.1682711 -4.2118158 -4.2338977 -4.1818819 -4.0788021 -3.99825 -4.0258141 -4.1100836 -4.1830845 -4.2254462 -4.2171617 -4.1769729 -4.141592][-4.1849442 -4.15341 -4.1582136 -4.1976132 -4.2212462 -4.1721029 -4.0663271 -3.9787042 -4.0050926 -4.0877843 -4.1530933 -4.1891294 -4.1831632 -4.1481309 -4.1114879][-4.1944513 -4.1546459 -4.1499848 -4.1881943 -4.2161021 -4.1750689 -4.0741687 -3.9863002 -4.0028458 -4.0729256 -4.1258907 -4.1530228 -4.1489367 -4.1243873 -4.098855][-4.1995907 -4.1548367 -4.14531 -4.1833515 -4.2144737 -4.1822491 -4.0932684 -4.0074716 -4.0047674 -4.06348 -4.1150637 -4.1420326 -4.1431975 -4.1328239 -4.1259737][-4.2001262 -4.155045 -4.1448197 -4.1831589 -4.2177138 -4.1958747 -4.1252766 -4.0484715 -4.0350127 -4.0882587 -4.1427464 -4.1695352 -4.1722059 -4.1714773 -4.1768723][-4.19628 -4.1536908 -4.1457152 -4.1844473 -4.223516 -4.2119203 -4.1581464 -4.09362 -4.0801735 -4.1317749 -4.1861835 -4.21187 -4.2156205 -4.2180786 -4.2255459][-4.1939721 -4.1533117 -4.1465616 -4.1833544 -4.2244897 -4.2186608 -4.1742511 -4.1217532 -4.112062 -4.1618924 -4.2142634 -4.2422395 -4.2506151 -4.2539177 -4.2588263][-4.1948652 -4.1530757 -4.142386 -4.17352 -4.21403 -4.2144122 -4.1787653 -4.1385069 -4.1348763 -4.1798196 -4.2312436 -4.260253 -4.2696953 -4.2721038 -4.2747135][-4.2010493 -4.1558242 -4.1373839 -4.1599879 -4.2013988 -4.2119336 -4.1871929 -4.1567097 -4.1545658 -4.1901731 -4.2374892 -4.26535 -4.2731442 -4.2752833 -4.2779374]]...]
INFO - root - 2017-12-07 23:28:52.891890: step 66610, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 51h:22m:41s remains)
INFO - root - 2017-12-07 23:28:59.391851: step 66620, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 47h:20m:18s remains)
INFO - root - 2017-12-07 23:29:06.109882: step 66630, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 47h:27m:50s remains)
INFO - root - 2017-12-07 23:29:12.939390: step 66640, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 51h:25m:50s remains)
INFO - root - 2017-12-07 23:29:19.786968: step 66650, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 54h:17m:07s remains)
INFO - root - 2017-12-07 23:29:26.530807: step 66660, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 51h:30m:41s remains)
INFO - root - 2017-12-07 23:29:33.258879: step 66670, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 47h:54m:29s remains)
INFO - root - 2017-12-07 23:29:39.964029: step 66680, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.676 sec/batch; 49h:53m:27s remains)
INFO - root - 2017-12-07 23:29:46.707228: step 66690, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 50h:14m:01s remains)
INFO - root - 2017-12-07 23:29:53.524804: step 66700, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.723 sec/batch; 53h:24m:12s remains)
2017-12-07 23:29:54.227557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1977057 -4.2059617 -4.2215991 -4.2353787 -4.2410779 -4.227397 -4.1944337 -4.1508255 -4.12238 -4.1162047 -4.1330361 -4.1572394 -4.1631846 -4.159287 -4.1581945][-4.2377195 -4.2449393 -4.2547331 -4.259964 -4.2560024 -4.24179 -4.2099438 -4.1671672 -4.1421061 -4.1442595 -4.1666427 -4.1834292 -4.1811523 -4.1720486 -4.171998][-4.2660227 -4.2751508 -4.2811012 -4.2747602 -4.2568088 -4.237556 -4.2075057 -4.1770267 -4.1661348 -4.1769543 -4.1989994 -4.2065868 -4.1935143 -4.1785579 -4.1897831][-4.2667928 -4.2762904 -4.2784185 -4.2629056 -4.238656 -4.22063 -4.1965284 -4.1762185 -4.1716304 -4.1817837 -4.1957936 -4.1929049 -4.1710167 -4.15355 -4.1780391][-4.2351003 -4.2426457 -4.2461276 -4.2317009 -4.211813 -4.1974454 -4.1796937 -4.167244 -4.1674757 -4.17663 -4.1852541 -4.1736646 -4.145659 -4.1289525 -4.1534419][-4.1959987 -4.1996098 -4.2101355 -4.1987486 -4.1812377 -4.1702094 -4.1553764 -4.1442266 -4.1474175 -4.1618662 -4.17599 -4.169064 -4.1428833 -4.1237431 -4.1384659][-4.1722617 -4.1760015 -4.1889138 -4.1799417 -4.1632 -4.154705 -4.1464372 -4.1372976 -4.1399364 -4.1569123 -4.1739759 -4.1744132 -4.154799 -4.1364622 -4.1393332][-4.1720119 -4.1757894 -4.1882119 -4.1808076 -4.1653938 -4.1611652 -4.1618257 -4.1592565 -4.1577144 -4.1623363 -4.1735091 -4.1776462 -4.1653557 -4.148046 -4.1426845][-4.2027822 -4.197711 -4.2034512 -4.1952858 -4.1821628 -4.1813264 -4.1896625 -4.1940265 -4.1887927 -4.180696 -4.1792808 -4.183732 -4.1775713 -4.1609478 -4.1456866][-4.2535596 -4.238482 -4.2319264 -4.2214622 -4.2125812 -4.2141247 -4.2256651 -4.2317982 -4.225553 -4.2163057 -4.2074661 -4.2071843 -4.2015467 -4.1815753 -4.1567535][-4.2817187 -4.2624421 -4.2501416 -4.244596 -4.2416992 -4.2450094 -4.2586551 -4.264915 -4.262207 -4.2542796 -4.2422528 -4.2335973 -4.22054 -4.1966696 -4.1704264][-4.2737775 -4.2564826 -4.2481294 -4.2513747 -4.2551 -4.2602172 -4.2762213 -4.2834277 -4.2842832 -4.2743073 -4.2598515 -4.2443361 -4.2249312 -4.2029409 -4.1864791][-4.2380233 -4.2247238 -4.2231 -4.2349844 -4.2472 -4.257833 -4.2782803 -4.2834377 -4.2813969 -4.2696242 -4.253861 -4.2365355 -4.2181058 -4.2046447 -4.2018447][-4.1838312 -4.1676483 -4.1714597 -4.192143 -4.2156248 -4.2352324 -4.2564445 -4.2587543 -4.2543221 -4.243464 -4.2307911 -4.2170639 -4.2060423 -4.2034903 -4.2133689][-4.1316543 -4.1080785 -4.1092105 -4.1322527 -4.1660032 -4.1956582 -4.2160616 -4.2181535 -4.2201095 -4.2186351 -4.215261 -4.2108145 -4.2049236 -4.2068043 -4.2199178]]...]
INFO - root - 2017-12-07 23:30:00.867354: step 66710, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 50h:23m:22s remains)
INFO - root - 2017-12-07 23:30:07.754277: step 66720, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.738 sec/batch; 54h:27m:02s remains)
INFO - root - 2017-12-07 23:30:14.557832: step 66730, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 52h:29m:41s remains)
INFO - root - 2017-12-07 23:30:21.299780: step 66740, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.681 sec/batch; 50h:18m:02s remains)
INFO - root - 2017-12-07 23:30:28.078752: step 66750, loss = 2.10, batch loss = 2.04 (12.6 examples/sec; 0.635 sec/batch; 46h:52m:21s remains)
INFO - root - 2017-12-07 23:30:34.830185: step 66760, loss = 2.09, batch loss = 2.03 (12.9 examples/sec; 0.622 sec/batch; 45h:56m:31s remains)
INFO - root - 2017-12-07 23:30:41.705087: step 66770, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 52h:35m:26s remains)
INFO - root - 2017-12-07 23:30:48.592076: step 66780, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.721 sec/batch; 53h:14m:48s remains)
INFO - root - 2017-12-07 23:30:55.498538: step 66790, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 50h:09m:17s remains)
INFO - root - 2017-12-07 23:31:02.232090: step 66800, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 47h:16m:12s remains)
2017-12-07 23:31:02.950604: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3466029 -4.3428907 -4.3359146 -4.3249373 -4.3211441 -4.3252859 -4.3190579 -4.3110085 -4.3010988 -4.2966275 -4.2959304 -4.2948895 -4.291338 -4.2804518 -4.275538][-4.3427043 -4.3421283 -4.33726 -4.328258 -4.3209395 -4.321454 -4.3149652 -4.308146 -4.297543 -4.2921162 -4.2899289 -4.2877865 -4.2848921 -4.2725139 -4.2653217][-4.328795 -4.3281417 -4.3211961 -4.311173 -4.2989964 -4.2919149 -4.286787 -4.2868252 -4.2821012 -4.2807961 -4.2798295 -4.27705 -4.2755141 -4.2661886 -4.2605624][-4.3100438 -4.3055038 -4.2976966 -4.2859926 -4.266726 -4.2493238 -4.24361 -4.2532558 -4.2621784 -4.2726641 -4.2760534 -4.2715073 -4.2719316 -4.2668314 -4.2628193][-4.29284 -4.2846656 -4.271843 -4.2551246 -4.2329612 -4.2082138 -4.1989326 -4.2133312 -4.2400289 -4.2647161 -4.2733283 -4.2678313 -4.26917 -4.2665863 -4.2643094][-4.2694492 -4.2584171 -4.242146 -4.2232604 -4.1967325 -4.1610017 -4.1339254 -4.1398726 -4.1862812 -4.2309542 -4.2492309 -4.2473745 -4.2521057 -4.25532 -4.259232][-4.2311935 -4.2128897 -4.1893373 -4.1649394 -4.1337085 -4.0840545 -4.0315781 -4.0205884 -4.0849323 -4.154839 -4.1900725 -4.2007461 -4.215497 -4.2324939 -4.248395][-4.1891 -4.159081 -4.1256356 -4.0944004 -4.0597873 -3.9988341 -3.9187667 -3.8861947 -3.967525 -4.0617509 -4.116663 -4.1445808 -4.1750193 -4.2082443 -4.2362776][-4.1669035 -4.1329393 -4.0984282 -4.0675898 -4.0368991 -3.9831367 -3.9078691 -3.8719492 -3.9455571 -4.0369368 -4.0958829 -4.1313276 -4.1681705 -4.2053456 -4.2359948][-4.1855111 -4.1559238 -4.1279006 -4.1051655 -4.08324 -4.0473843 -3.9958951 -3.9686971 -4.0164461 -4.083065 -4.1326451 -4.1668277 -4.1997328 -4.2304726 -4.254323][-4.2222142 -4.205092 -4.1878171 -4.1740475 -4.160223 -4.1390557 -4.104641 -4.081305 -4.1075315 -4.1523137 -4.1910443 -4.219193 -4.2446613 -4.2665596 -4.2826624][-4.2503891 -4.243082 -4.2355204 -4.2279634 -4.2195988 -4.2085371 -4.188807 -4.1734338 -4.1867828 -4.214458 -4.2423086 -4.2631941 -4.2819738 -4.29753 -4.3082042][-4.2575159 -4.2548418 -4.2529912 -4.2524376 -4.2526431 -4.2511225 -4.2430058 -4.2339711 -4.2388778 -4.2553658 -4.2747684 -4.2896805 -4.3041949 -4.3169289 -4.3248334][-4.2611513 -4.2602205 -4.2638664 -4.2702003 -4.2779632 -4.2832346 -4.2824616 -4.2784576 -4.2786183 -4.2865362 -4.2986588 -4.3093691 -4.320714 -4.3307076 -4.3366637][-4.2814813 -4.2828937 -4.2891045 -4.2965107 -4.3043 -4.3095741 -4.311008 -4.309937 -4.3084979 -4.3105721 -4.3164768 -4.3236785 -4.3321314 -4.3396177 -4.3442531]]...]
INFO - root - 2017-12-07 23:31:09.475863: step 66810, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 52h:42m:07s remains)
INFO - root - 2017-12-07 23:31:16.287990: step 66820, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 51h:34m:26s remains)
INFO - root - 2017-12-07 23:31:23.093546: step 66830, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 49h:07m:19s remains)
INFO - root - 2017-12-07 23:31:29.925020: step 66840, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 52h:26m:38s remains)
INFO - root - 2017-12-07 23:31:36.888934: step 66850, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 53h:09m:33s remains)
INFO - root - 2017-12-07 23:31:43.759802: step 66860, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 50h:49m:34s remains)
INFO - root - 2017-12-07 23:31:50.488070: step 66870, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.639 sec/batch; 47h:07m:20s remains)
INFO - root - 2017-12-07 23:31:57.300584: step 66880, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 48h:51m:08s remains)
INFO - root - 2017-12-07 23:32:04.058380: step 66890, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 49h:03m:31s remains)
INFO - root - 2017-12-07 23:32:10.857051: step 66900, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 50h:25m:37s remains)
2017-12-07 23:32:11.519975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2049465 -4.2031021 -4.1825137 -4.1671934 -4.1920052 -4.2258663 -4.2367873 -4.2324553 -4.2383151 -4.2363853 -4.21403 -4.2030826 -4.2142429 -4.23779 -4.2549224][-4.1960359 -4.20298 -4.183558 -4.1673927 -4.1856542 -4.2133512 -4.2178612 -4.2107291 -4.2249708 -4.2304349 -4.20755 -4.1931381 -4.2022562 -4.2289777 -4.247654][-4.19196 -4.2053638 -4.1921988 -4.1779637 -4.18627 -4.1985765 -4.1923451 -4.1845317 -4.2085443 -4.223371 -4.2068462 -4.1892281 -4.1973739 -4.2267585 -4.2460256][-4.1963358 -4.2226586 -4.2190728 -4.2075295 -4.2050185 -4.1974368 -4.1775632 -4.1705995 -4.2021713 -4.2248716 -4.2172542 -4.2000551 -4.2077103 -4.2322516 -4.2464967][-4.2051044 -4.2341747 -4.2348504 -4.2262783 -4.2155495 -4.1881552 -4.151237 -4.1436324 -4.1828184 -4.2150311 -4.215837 -4.2033391 -4.2106681 -4.2327023 -4.243309][-4.2077413 -4.2335906 -4.2341318 -4.228889 -4.2117252 -4.1635647 -4.0970616 -4.0835943 -4.1460614 -4.1990023 -4.2149816 -4.2086873 -4.2147851 -4.2350473 -4.2426491][-4.2001553 -4.2243161 -4.2235436 -4.2197309 -4.1935077 -4.1200423 -4.0126977 -3.9796116 -4.082859 -4.1788883 -4.2148485 -4.2189593 -4.223166 -4.2396321 -4.243968][-4.2015443 -4.2199707 -4.2169266 -4.2091069 -4.1722264 -4.0721292 -3.9119866 -3.8268209 -3.9730649 -4.1217904 -4.18786 -4.2087936 -4.21689 -4.22977 -4.23332][-4.2136407 -4.22415 -4.2178893 -4.210381 -4.1786833 -4.078867 -3.9079385 -3.7870598 -3.9356933 -4.1017017 -4.1813831 -4.2112703 -4.2191143 -4.2250357 -4.2217994][-4.230608 -4.2365465 -4.234446 -4.2362862 -4.2180877 -4.1467495 -4.024034 -3.9287212 -4.0108523 -4.1271191 -4.1906481 -4.2213688 -4.2310863 -4.2303319 -4.2219362][-4.2458725 -4.2477779 -4.2510262 -4.2582755 -4.2512217 -4.2065673 -4.1255484 -4.056284 -4.0946932 -4.1642661 -4.2107096 -4.2398434 -4.2504821 -4.2453141 -4.23188][-4.2584033 -4.2522764 -4.253087 -4.25852 -4.2604756 -4.2419467 -4.1975322 -4.1569867 -4.1808457 -4.2183051 -4.2427998 -4.2649479 -4.2741847 -4.2668967 -4.2485061][-4.2684212 -4.2564888 -4.2496405 -4.2474842 -4.2539921 -4.2530479 -4.2334943 -4.2131567 -4.2297783 -4.2508316 -4.2626958 -4.2769256 -4.2836103 -4.2739806 -4.2557921][-4.2711658 -4.2581539 -4.2467632 -4.2401361 -4.2442923 -4.2469988 -4.2389779 -4.2286034 -4.2397289 -4.2523842 -4.2550926 -4.2647309 -4.2697577 -4.2614155 -4.2523422][-4.2613664 -4.2508597 -4.2415042 -4.2360525 -4.2362103 -4.237483 -4.2351766 -4.2308631 -4.2354236 -4.2393217 -4.2353444 -4.2393227 -4.2448683 -4.24474 -4.2476034]]...]
INFO - root - 2017-12-07 23:32:18.276898: step 66910, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 50h:11m:07s remains)
INFO - root - 2017-12-07 23:32:25.095064: step 66920, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.719 sec/batch; 53h:01m:40s remains)
INFO - root - 2017-12-07 23:32:31.742837: step 66930, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 47h:00m:38s remains)
INFO - root - 2017-12-07 23:32:38.432535: step 66940, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 47h:02m:39s remains)
INFO - root - 2017-12-07 23:32:45.295756: step 66950, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 50h:55m:49s remains)
INFO - root - 2017-12-07 23:32:51.980026: step 66960, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 48h:41m:58s remains)
INFO - root - 2017-12-07 23:32:58.687451: step 66970, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 52h:44m:05s remains)
INFO - root - 2017-12-07 23:33:05.457306: step 66980, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.672 sec/batch; 49h:32m:39s remains)
INFO - root - 2017-12-07 23:33:12.380385: step 66990, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 48h:43m:57s remains)
INFO - root - 2017-12-07 23:33:19.199730: step 67000, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.668 sec/batch; 49h:15m:21s remains)
2017-12-07 23:33:19.984434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3099713 -4.2700362 -4.2084918 -4.1344986 -4.0814152 -4.0898609 -4.1330514 -4.1713605 -4.1914992 -4.2015295 -4.2049026 -4.2097778 -4.2144418 -4.2075338 -4.1903968][-4.303371 -4.2628431 -4.2005844 -4.12578 -4.07179 -4.0770245 -4.1150813 -4.1524954 -4.173 -4.1872072 -4.1962428 -4.1966391 -4.1960621 -4.1826153 -4.1552143][-4.3051615 -4.2644081 -4.2028871 -4.1327949 -4.0792828 -4.0752687 -4.1063519 -4.1412334 -4.1592255 -4.1704268 -4.1848154 -4.1851439 -4.1797762 -4.1610103 -4.1273775][-4.306375 -4.2628689 -4.1995955 -4.136621 -4.0856228 -4.0748138 -4.1042538 -4.1381092 -4.14662 -4.1469851 -4.1630206 -4.1699905 -4.1655931 -4.1439972 -4.1091185][-4.3050737 -4.2579827 -4.1870108 -4.1150603 -4.0571651 -4.0442147 -4.07687 -4.1119003 -4.1166544 -4.1059704 -4.1248212 -4.1461353 -4.1531591 -4.1319737 -4.0953636][-4.299077 -4.2478714 -4.1641331 -4.0753407 -4.0056896 -3.9908652 -4.0303812 -4.0710855 -4.0788026 -4.066102 -4.0813475 -4.1085315 -4.133152 -4.1255746 -4.0923247][-4.2947073 -4.24147 -4.1505013 -4.0492339 -3.966882 -3.9527884 -4.0041342 -4.0525036 -4.068985 -4.0662227 -4.0769405 -4.0965877 -4.1270514 -4.1324224 -4.103127][-4.2977104 -4.2525392 -4.1698046 -4.0724325 -3.9845374 -3.9663305 -4.0162473 -4.0606985 -4.0784655 -4.0906286 -4.1032481 -4.1185732 -4.1467786 -4.1625633 -4.1394935][-4.3037462 -4.2714186 -4.20627 -4.1267743 -4.0447431 -4.0215678 -4.0596848 -4.0904603 -4.1031876 -4.127532 -4.1558533 -4.1693292 -4.1857166 -4.202137 -4.187284][-4.3145914 -4.2958689 -4.2492113 -4.18979 -4.1217771 -4.0957508 -4.1203003 -4.1406541 -4.1487384 -4.1728525 -4.210525 -4.2291312 -4.2387466 -4.2477722 -4.2379794][-4.3277216 -4.3235569 -4.2942548 -4.2517381 -4.1965971 -4.168982 -4.182147 -4.1961565 -4.2022815 -4.2203488 -4.2548494 -4.2729306 -4.2800431 -4.2848458 -4.2764568][-4.331099 -4.3385549 -4.3254571 -4.2969208 -4.2533894 -4.2263851 -4.2341089 -4.2466817 -4.2516847 -4.2647815 -4.2910843 -4.3029914 -4.3082695 -4.3096986 -4.3047967][-4.3255377 -4.3349471 -4.3335876 -4.3186111 -4.2888718 -4.2680798 -4.2743855 -4.2875443 -4.2910676 -4.2976475 -4.3192797 -4.3286653 -4.3322082 -4.3316369 -4.3270593][-4.321094 -4.32554 -4.327055 -4.3206239 -4.3044472 -4.292685 -4.2989607 -4.3115945 -4.3147807 -4.3196659 -4.3370037 -4.3443975 -4.3442459 -4.3413773 -4.3353481][-4.3193021 -4.3193574 -4.3194942 -4.3181472 -4.3110929 -4.3076115 -4.315083 -4.326159 -4.3313503 -4.3346157 -4.3456092 -4.350368 -4.3469357 -4.3409438 -4.3335319]]...]
INFO - root - 2017-12-07 23:33:26.434738: step 67010, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 49h:13m:20s remains)
INFO - root - 2017-12-07 23:33:33.263541: step 67020, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 48h:06m:43s remains)
INFO - root - 2017-12-07 23:33:39.913039: step 67030, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 49h:37m:12s remains)
INFO - root - 2017-12-07 23:33:46.682824: step 67040, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.680 sec/batch; 50h:09m:53s remains)
INFO - root - 2017-12-07 23:33:53.454632: step 67050, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 51h:04m:47s remains)
INFO - root - 2017-12-07 23:34:00.090700: step 67060, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 48h:47m:29s remains)
INFO - root - 2017-12-07 23:34:06.882416: step 67070, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 46h:42m:24s remains)
INFO - root - 2017-12-07 23:34:13.750935: step 67080, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 51h:14m:24s remains)
INFO - root - 2017-12-07 23:34:20.583750: step 67090, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.716 sec/batch; 52h:46m:40s remains)
INFO - root - 2017-12-07 23:34:27.417356: step 67100, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 49h:08m:21s remains)
2017-12-07 23:34:28.121614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2064462 -4.1750131 -4.1533351 -4.1266723 -4.1053576 -4.1077871 -4.1324005 -4.1657166 -4.1901603 -4.1965151 -4.1858568 -4.1496687 -4.1013284 -4.0674839 -4.0521626][-4.213891 -4.1817822 -4.1553416 -4.1271014 -4.1086783 -4.1119323 -4.13904 -4.1741395 -4.19952 -4.2081103 -4.2056141 -4.1749449 -4.12313 -4.0783587 -4.0471449][-4.2178793 -4.19515 -4.1687889 -4.1447783 -4.131763 -4.1274061 -4.1389842 -4.1644254 -4.18659 -4.1986513 -4.206605 -4.1908994 -4.1490955 -4.1046605 -4.0613194][-4.2132735 -4.2011819 -4.1799541 -4.1593204 -4.1418376 -4.1211119 -4.1098518 -4.1224914 -4.146183 -4.1672306 -4.187088 -4.1921177 -4.1705823 -4.1392875 -4.1013269][-4.1986952 -4.1917067 -4.174366 -4.1521697 -4.1280828 -4.0951071 -4.0669436 -4.0676055 -4.0910697 -4.1204481 -4.1533337 -4.1737056 -4.1702151 -4.1591635 -4.142962][-4.1781578 -4.176446 -4.1659436 -4.1463737 -4.1164985 -4.0771708 -4.034936 -4.0148025 -4.0252047 -4.05586 -4.0987759 -4.1336341 -4.1497359 -4.1579361 -4.1620584][-4.141521 -4.1489677 -4.1518931 -4.1404195 -4.1103773 -4.0692148 -4.016655 -3.9721522 -3.9561231 -3.9783878 -4.0310378 -4.0861697 -4.1227221 -4.1456847 -4.1628671][-4.092289 -4.1107059 -4.132225 -4.134006 -4.1113262 -4.07639 -4.0223789 -3.959569 -3.9151874 -3.9211276 -3.9772491 -4.0544758 -4.10982 -4.1379285 -4.1579604][-4.0575776 -4.0823689 -4.1202865 -4.136785 -4.1233468 -4.0948296 -4.0475249 -3.9846239 -3.9290614 -3.9225271 -3.97222 -4.0543957 -4.1162663 -4.1401448 -4.1527276][-4.0561481 -4.0800428 -4.1209884 -4.1434951 -4.1411223 -4.1205168 -4.0834188 -4.0353465 -3.9935949 -3.9895744 -4.0255461 -4.087779 -4.1343369 -4.1486831 -4.1562486][-4.0900784 -4.1092582 -4.1416984 -4.1619105 -4.1673875 -4.1551704 -4.1281061 -4.0950837 -4.0730696 -4.074533 -4.0986609 -4.137897 -4.1658993 -4.1724458 -4.1766949][-4.138351 -4.1536236 -4.175858 -4.1878986 -4.1949391 -4.1901293 -4.1716566 -4.15017 -4.1384254 -4.1420383 -4.1605859 -4.1855912 -4.2008696 -4.2033587 -4.2068148][-4.1887617 -4.1998072 -4.2127891 -4.2171192 -4.2204542 -4.2188187 -4.2095118 -4.1993637 -4.1938386 -4.1967645 -4.2098541 -4.2260003 -4.2354074 -4.2391262 -4.2455974][-4.2328105 -4.2383471 -4.2466135 -4.2502227 -4.2517519 -4.248775 -4.2436948 -4.2413163 -4.2409081 -4.2423291 -4.2483888 -4.2578459 -4.2671 -4.2741876 -4.2827125][-4.2690587 -4.271071 -4.2769938 -4.2817125 -4.283062 -4.2804055 -4.2765422 -4.27504 -4.2752743 -4.2759757 -4.2791977 -4.2866082 -4.2958312 -4.3030214 -4.309217]]...]
INFO - root - 2017-12-07 23:34:34.833114: step 67110, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 50h:53m:14s remains)
INFO - root - 2017-12-07 23:34:41.696845: step 67120, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.698 sec/batch; 51h:26m:59s remains)
INFO - root - 2017-12-07 23:34:48.523770: step 67130, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 48h:45m:50s remains)
INFO - root - 2017-12-07 23:34:55.239333: step 67140, loss = 2.04, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 47h:04m:27s remains)
INFO - root - 2017-12-07 23:35:02.043752: step 67150, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 48h:23m:03s remains)
INFO - root - 2017-12-07 23:35:08.982476: step 67160, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 51h:07m:25s remains)
INFO - root - 2017-12-07 23:35:15.814824: step 67170, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 50h:46m:44s remains)
INFO - root - 2017-12-07 23:35:22.584262: step 67180, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 47h:32m:00s remains)
INFO - root - 2017-12-07 23:35:29.352807: step 67190, loss = 2.05, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 48h:41m:11s remains)
INFO - root - 2017-12-07 23:35:36.210481: step 67200, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 52h:05m:00s remains)
2017-12-07 23:35:36.861843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2333837 -4.2321568 -4.2316432 -4.2326522 -4.2351923 -4.2370214 -4.2335496 -4.2210531 -4.2033181 -4.1882181 -4.1828303 -4.1901913 -4.2071257 -4.2259421 -4.2402511][-4.2342939 -4.2296395 -4.2271161 -4.2264395 -4.2263603 -4.2229 -4.2115927 -4.1882529 -4.1606317 -4.1413255 -4.1385775 -4.1566458 -4.1893835 -4.2214918 -4.2433434][-4.2409449 -4.231843 -4.2269144 -4.223979 -4.2193904 -4.2067308 -4.1808696 -4.1404905 -4.0986876 -4.0724134 -4.07528 -4.1121798 -4.1665931 -4.2146363 -4.2457414][-4.2490368 -4.23824 -4.2308464 -4.2235365 -4.2093534 -4.1800585 -4.1332846 -4.0697279 -4.0079541 -3.972152 -3.9879804 -4.0540781 -4.1342788 -4.1975012 -4.2370372][-4.2502761 -4.2430916 -4.2357583 -4.2213674 -4.1915436 -4.1420708 -4.0728393 -3.9833329 -3.8971782 -3.8525724 -3.8889151 -3.9905088 -4.0973959 -4.1730018 -4.2186174][-4.239193 -4.2404432 -4.2328229 -4.2096066 -4.1658511 -4.1027308 -4.0207057 -3.9171712 -3.8202314 -3.775842 -3.8302653 -3.954478 -4.0738463 -4.151475 -4.1979146][-4.2177539 -4.225091 -4.2139807 -4.182405 -4.1322861 -4.0708861 -3.9956033 -3.9023898 -3.8232419 -3.7948976 -3.8517754 -3.9645667 -4.0682435 -4.1326537 -4.1726241][-4.1841307 -4.1926785 -4.1789203 -4.1460376 -4.1029358 -4.0602508 -4.0098114 -3.946136 -3.8988197 -3.8825498 -3.9256516 -4.0094657 -4.083879 -4.12866 -4.159595][-4.1525612 -4.1556468 -4.1427197 -4.1200194 -4.0978093 -4.0835567 -4.0641928 -4.0307822 -4.0050383 -3.9927032 -4.0205741 -4.0792508 -4.1311436 -4.1615195 -4.1830883][-4.1552663 -4.1536808 -4.1454234 -4.1386509 -4.1394339 -4.145515 -4.1432066 -4.1271677 -4.1111546 -4.1006117 -4.1158643 -4.1536355 -4.1888337 -4.2110538 -4.2259912][-4.1942458 -4.1900511 -4.18575 -4.1896963 -4.2021794 -4.2131748 -4.2146425 -4.2085829 -4.2013454 -4.192359 -4.1944318 -4.2117953 -4.2334924 -4.250905 -4.262857][-4.2166595 -4.2096591 -4.2083883 -4.2192249 -4.2386923 -4.2539272 -4.2615647 -4.2657013 -4.26498 -4.2558379 -4.2477832 -4.2507358 -4.2612834 -4.2737684 -4.28233][-4.2114949 -4.2014589 -4.2017088 -4.2171674 -4.2449903 -4.270606 -4.28715 -4.2980947 -4.2980986 -4.2857971 -4.2704096 -4.2636485 -4.26473 -4.2708778 -4.2745924][-4.2057319 -4.1918716 -4.1900954 -4.2074814 -4.2393093 -4.2696171 -4.2883 -4.2972789 -4.2908397 -4.27423 -4.2574959 -4.2483382 -4.2451406 -4.243475 -4.2363644][-4.2033982 -4.1875882 -4.1811209 -4.196713 -4.2276926 -4.2550344 -4.2659087 -4.2634125 -4.2469153 -4.2282596 -4.2169838 -4.2145205 -4.2140794 -4.205925 -4.186255]]...]
INFO - root - 2017-12-07 23:35:43.452540: step 67210, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 47h:30m:12s remains)
INFO - root - 2017-12-07 23:35:50.320692: step 67220, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 48h:54m:35s remains)
INFO - root - 2017-12-07 23:35:57.032419: step 67230, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 50h:34m:27s remains)
INFO - root - 2017-12-07 23:36:03.595748: step 67240, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 49h:26m:45s remains)
INFO - root - 2017-12-07 23:36:10.517330: step 67250, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 48h:48m:57s remains)
INFO - root - 2017-12-07 23:36:17.460960: step 67260, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 50h:54m:39s remains)
INFO - root - 2017-12-07 23:36:24.348955: step 67270, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 51h:48m:34s remains)
INFO - root - 2017-12-07 23:36:31.078649: step 67280, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 50h:33m:52s remains)
INFO - root - 2017-12-07 23:36:37.833834: step 67290, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 48h:44m:47s remains)
INFO - root - 2017-12-07 23:36:44.650391: step 67300, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 49h:29m:26s remains)
2017-12-07 23:36:45.377530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2333264 -4.2332082 -4.2437725 -4.263257 -4.2776971 -4.2842712 -4.2875457 -4.282083 -4.2797756 -4.2849536 -4.2889934 -4.2847071 -4.2749076 -4.2725224 -4.292594][-4.2334127 -4.2295012 -4.2390342 -4.2629623 -4.2795515 -4.2765961 -4.2638631 -4.2469082 -4.2410917 -4.2490911 -4.2629766 -4.2711625 -4.2678189 -4.2688484 -4.2917352][-4.2413487 -4.2351036 -4.2422857 -4.2634306 -4.2724643 -4.2518587 -4.2181816 -4.190289 -4.1861782 -4.2049589 -4.2384181 -4.2637854 -4.2680354 -4.2720675 -4.2941246][-4.253509 -4.2495551 -4.2564263 -4.26985 -4.2623277 -4.2169843 -4.1581435 -4.1184044 -4.120244 -4.155673 -4.2114229 -4.2507138 -4.2638941 -4.2730212 -4.2960148][-4.2705655 -4.2685251 -4.27282 -4.2745409 -4.2489038 -4.1771169 -4.0896153 -4.0343628 -4.03701 -4.0895648 -4.1691537 -4.2236772 -4.2486978 -4.2659621 -4.293705][-4.2808957 -4.2774382 -4.2760038 -4.2679796 -4.2285662 -4.1325779 -4.0127373 -3.9380658 -3.9408817 -4.0086455 -4.114789 -4.1908011 -4.2307606 -4.2577972 -4.2906647][-4.2693033 -4.2675939 -4.2640376 -4.2484121 -4.1994877 -4.0889931 -3.9497137 -3.8654861 -3.8741307 -3.9563129 -4.0815392 -4.1749568 -4.2275691 -4.2598295 -4.2926812][-4.24147 -4.2464175 -4.2483878 -4.2319536 -4.1766887 -4.0644712 -3.9268446 -3.849329 -3.8664057 -3.9532256 -4.083209 -4.1846452 -4.2397161 -4.2698264 -4.298306][-4.2156968 -4.2279143 -4.2387238 -4.2273788 -4.1734171 -4.07488 -3.9543114 -3.8894749 -3.9047678 -3.9830832 -4.1057096 -4.2028732 -4.2549076 -4.2802095 -4.3027744][-4.1941347 -4.2112541 -4.2301965 -4.2285075 -4.1891546 -4.1128941 -4.0179486 -3.9661605 -3.9737327 -4.0392051 -4.144588 -4.2262974 -4.2714381 -4.29074 -4.3063183][-4.1842532 -4.1959925 -4.2184634 -4.227108 -4.2082396 -4.16202 -4.0959129 -4.0649209 -4.0758567 -4.1249008 -4.1977978 -4.2558455 -4.2909012 -4.302423 -4.3117456][-4.1751013 -4.1749597 -4.1959825 -4.210587 -4.2079568 -4.1897321 -4.1560035 -4.1491928 -4.1690927 -4.2072563 -4.251554 -4.2888474 -4.311552 -4.3149858 -4.3196592][-4.155231 -4.1375685 -4.14802 -4.1644936 -4.1749606 -4.1829309 -4.1821427 -4.1965637 -4.2255616 -4.2627707 -4.2947187 -4.3186407 -4.3275747 -4.3240819 -4.3261104][-4.1293612 -4.0938277 -4.0939317 -4.1129818 -4.1363373 -4.1650233 -4.1909986 -4.2211084 -4.2570271 -4.2959905 -4.3235583 -4.3367629 -4.3341494 -4.3273382 -4.3292694][-4.1244874 -4.0843825 -4.0789185 -4.09697 -4.1290908 -4.1708322 -4.2071509 -4.24153 -4.2776823 -4.3130956 -4.3354454 -4.3392634 -4.3312979 -4.322803 -4.3268232]]...]
INFO - root - 2017-12-07 23:36:51.953588: step 67310, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 50h:28m:37s remains)
INFO - root - 2017-12-07 23:36:58.785373: step 67320, loss = 2.02, batch loss = 1.97 (12.7 examples/sec; 0.630 sec/batch; 46h:25m:42s remains)
INFO - root - 2017-12-07 23:37:05.482066: step 67330, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 47h:01m:06s remains)
INFO - root - 2017-12-07 23:37:12.294333: step 67340, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 51h:54m:13s remains)
INFO - root - 2017-12-07 23:37:19.035401: step 67350, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 51h:19m:46s remains)
INFO - root - 2017-12-07 23:37:25.825869: step 67360, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 52h:05m:07s remains)
INFO - root - 2017-12-07 23:37:32.648553: step 67370, loss = 2.12, batch loss = 2.06 (12.4 examples/sec; 0.644 sec/batch; 47h:27m:49s remains)
INFO - root - 2017-12-07 23:37:39.478792: step 67380, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 46h:47m:22s remains)
INFO - root - 2017-12-07 23:37:46.296563: step 67390, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 51h:41m:59s remains)
INFO - root - 2017-12-07 23:37:53.120323: step 67400, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 52h:22m:32s remains)
2017-12-07 23:37:53.805001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1391978 -4.1318183 -4.1326308 -4.1379461 -4.1499786 -4.158989 -4.1574945 -4.1480422 -4.1456113 -4.144136 -4.1398516 -4.1366591 -4.1367269 -4.1373835 -4.1383619][-4.1463652 -4.1384239 -4.1380367 -4.1418633 -4.1488652 -4.1528039 -4.1479664 -4.1394968 -4.1405997 -4.1394234 -4.1319509 -4.1275892 -4.1280966 -4.127419 -4.1265154][-4.1617928 -4.1610718 -4.1646957 -4.1690512 -4.1682067 -4.1606894 -4.1481533 -4.141767 -4.143785 -4.1398244 -4.1268578 -4.1182103 -4.1136041 -4.1097293 -4.108736][-4.1609559 -4.1687932 -4.1787653 -4.1868296 -4.1815877 -4.1626239 -4.1419315 -4.1356263 -4.1364093 -4.1305842 -4.1151371 -4.1020966 -4.0912943 -4.0865312 -4.0896435][-4.1355648 -4.148993 -4.1664982 -4.17626 -4.1673536 -4.1400361 -4.1134863 -4.1084766 -4.1104875 -4.1027021 -4.0856051 -4.067728 -4.0542336 -4.0540843 -4.0669651][-4.104948 -4.1217194 -4.1405082 -4.1434674 -4.1251163 -4.0865808 -4.0543575 -4.0522223 -4.0601339 -4.0537038 -4.0331216 -4.0085673 -3.9993203 -4.0100226 -4.0314379][-4.0645733 -4.0825639 -4.0944576 -4.07714 -4.0329914 -3.9739945 -3.9386668 -3.9482718 -3.9755621 -3.9840865 -3.9694068 -3.9405682 -3.9390318 -3.9560571 -3.9795184][-4.0239949 -4.0396543 -4.0439286 -4.0062428 -3.9339888 -3.8532593 -3.8190842 -3.8498487 -3.9080727 -3.9427381 -3.9380724 -3.9090858 -3.9112415 -3.9272652 -3.9434474][-4.0013733 -4.0147257 -4.01946 -3.9845831 -3.9173326 -3.8455911 -3.8294477 -3.8738029 -3.9409251 -3.9841313 -3.9834111 -3.9603343 -3.9595308 -3.9640641 -3.9671433][-4.0109358 -4.0252953 -4.0425925 -4.0318127 -3.9988298 -3.958962 -3.9510891 -3.9770029 -4.0238776 -4.0576324 -4.0581846 -4.0425305 -4.0412765 -4.0422482 -4.0442243][-4.0534091 -4.0602903 -4.0812759 -4.0863166 -4.0793853 -4.0648537 -4.0637875 -4.0762072 -4.1048131 -4.1269641 -4.1271667 -4.1159463 -4.1119537 -4.1120458 -4.1150022][-4.09744 -4.0908513 -4.1032357 -4.1136932 -4.1205759 -4.124125 -4.1322122 -4.1429663 -4.1635513 -4.1791258 -4.1810966 -4.1742735 -4.1692872 -4.16584 -4.1614423][-4.1209316 -4.1052938 -4.1081963 -4.1156435 -4.1260853 -4.1379018 -4.1503 -4.159585 -4.1741529 -4.1853871 -4.1886 -4.185 -4.180263 -4.1746716 -4.1674805][-4.1473742 -4.1338325 -4.1323276 -4.1354427 -4.1410046 -4.1504483 -4.162292 -4.1710215 -4.1810575 -4.1893497 -4.1919551 -4.1880884 -4.1832032 -4.1770139 -4.1711712][-4.1833744 -4.1726775 -4.1703467 -4.1712465 -4.1734505 -4.1769872 -4.1829281 -4.186657 -4.1904531 -4.1931911 -4.1933761 -4.19168 -4.1908445 -4.1872749 -4.1839418]]...]
INFO - root - 2017-12-07 23:38:00.389780: step 67410, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 49h:45m:56s remains)
INFO - root - 2017-12-07 23:38:07.258793: step 67420, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 51h:48m:17s remains)
INFO - root - 2017-12-07 23:38:14.193823: step 67430, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 53h:26m:52s remains)
INFO - root - 2017-12-07 23:38:21.049963: step 67440, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.635 sec/batch; 46h:43m:49s remains)
INFO - root - 2017-12-07 23:38:27.887789: step 67450, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.667 sec/batch; 49h:05m:25s remains)
INFO - root - 2017-12-07 23:38:34.757712: step 67460, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 51h:35m:10s remains)
INFO - root - 2017-12-07 23:38:41.525736: step 67470, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 50h:16m:48s remains)
INFO - root - 2017-12-07 23:38:48.394666: step 67480, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 50h:33m:27s remains)
INFO - root - 2017-12-07 23:38:55.119540: step 67490, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.649 sec/batch; 47h:44m:29s remains)
INFO - root - 2017-12-07 23:39:01.980142: step 67500, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 51h:15m:53s remains)
2017-12-07 23:39:02.744065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3073926 -4.3062644 -4.2983904 -4.2832651 -4.267509 -4.2630968 -4.2751784 -4.2953415 -4.3110547 -4.3184214 -4.317071 -4.3103495 -4.3037105 -4.3015742 -4.3011866][-4.3144221 -4.3091974 -4.2920227 -4.2637153 -4.2358265 -4.2255039 -4.2418518 -4.2749248 -4.3051558 -4.324966 -4.3312359 -4.3265986 -4.318028 -4.3133321 -4.3108616][-4.3115368 -4.29753 -4.2661295 -4.222116 -4.1809378 -4.1623874 -4.1804581 -4.2248421 -4.2704816 -4.307281 -4.3272924 -4.3310275 -4.3244929 -4.31949 -4.31572][-4.3048067 -4.2819161 -4.2367964 -4.1782708 -4.1247306 -4.0979385 -4.1149688 -4.1665306 -4.2222075 -4.2716236 -4.3057766 -4.321384 -4.3222051 -4.320343 -4.3179803][-4.2921267 -4.2629952 -4.2081547 -4.1368608 -4.074378 -4.043159 -4.057703 -4.1118364 -4.1735039 -4.2276249 -4.2705617 -4.2984824 -4.3091574 -4.3130813 -4.3140416][-4.28102 -4.2473736 -4.1833057 -4.100903 -4.0298386 -3.9945941 -4.0025973 -4.0529661 -4.1182818 -4.1751246 -4.2258658 -4.2679052 -4.2906017 -4.3032842 -4.3092813][-4.2777338 -4.2443008 -4.177186 -4.0836177 -4.0009041 -3.9520922 -3.9407322 -3.9777806 -4.0459671 -4.1114097 -4.1762414 -4.2364693 -4.2732229 -4.2928457 -4.3039656][-4.2860236 -4.2611618 -4.2015209 -4.1060457 -4.009861 -3.9380329 -3.898829 -3.9170597 -3.9890089 -4.071322 -4.1520014 -4.2247195 -4.2689357 -4.2878928 -4.2996354][-4.3001409 -4.28774 -4.2476759 -4.1701751 -4.0754457 -3.98455 -3.9271345 -3.934643 -4.005435 -4.0894966 -4.1678319 -4.2346044 -4.2716565 -4.2826881 -4.2905464][-4.3067555 -4.3085861 -4.2934146 -4.2461109 -4.1711493 -4.0772424 -4.0116982 -4.0178638 -4.0797749 -4.1501727 -4.2085133 -4.2513566 -4.267992 -4.2661805 -4.2718339][-4.2955818 -4.3154588 -4.3240967 -4.3066983 -4.257762 -4.1788836 -4.1163154 -4.1167889 -4.1620417 -4.2116971 -4.2456703 -4.2605247 -4.2527571 -4.2355108 -4.2380071][-4.2580934 -4.2968373 -4.3305941 -4.3415647 -4.3222604 -4.2725906 -4.2249427 -4.2171354 -4.2390246 -4.2616796 -4.2670026 -4.2524586 -4.220273 -4.1874914 -4.1866131][-4.1968789 -4.2503905 -4.3051658 -4.3405166 -4.3482771 -4.3295584 -4.3026013 -4.2950373 -4.3025508 -4.2994919 -4.2712073 -4.2241783 -4.1697254 -4.1266894 -4.1254106][-4.1294303 -4.1924453 -4.2590528 -4.3087082 -4.3350077 -4.3411207 -4.3332424 -4.3301373 -4.3311906 -4.3107033 -4.2569537 -4.1827221 -4.1095805 -4.0627041 -4.0636835][-4.0554419 -4.1214318 -4.1968393 -4.2551503 -4.2977166 -4.3238811 -4.3317537 -4.3341355 -4.3323092 -4.3036208 -4.2351065 -4.1401787 -4.0522633 -4.0036049 -4.0028458]]...]
INFO - root - 2017-12-07 23:39:09.332092: step 67510, loss = 2.08, batch loss = 2.03 (11.9 examples/sec; 0.670 sec/batch; 49h:19m:16s remains)
INFO - root - 2017-12-07 23:39:16.237365: step 67520, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 48h:51m:02s remains)
INFO - root - 2017-12-07 23:39:23.120257: step 67530, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 52h:58m:10s remains)
INFO - root - 2017-12-07 23:39:29.991193: step 67540, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 50h:24m:02s remains)
INFO - root - 2017-12-07 23:39:36.581855: step 67550, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 47h:35m:07s remains)
INFO - root - 2017-12-07 23:39:43.460031: step 67560, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 48h:29m:45s remains)
INFO - root - 2017-12-07 23:39:50.292928: step 67570, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 54h:51m:33s remains)
INFO - root - 2017-12-07 23:39:57.152697: step 67580, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.721 sec/batch; 53h:02m:24s remains)
INFO - root - 2017-12-07 23:40:04.029276: step 67590, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 50h:34m:32s remains)
INFO - root - 2017-12-07 23:40:10.844105: step 67600, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 47h:29m:12s remains)
2017-12-07 23:40:11.571762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2475443 -4.2106805 -4.1604342 -4.1214972 -4.1234093 -4.1597261 -4.1943836 -4.2152629 -4.2130775 -4.2043657 -4.2033529 -4.2132206 -4.2337761 -4.2529569 -4.2633295][-4.2573552 -4.2230887 -4.168107 -4.1206155 -4.1146274 -4.1490936 -4.1814742 -4.1936388 -4.1845655 -4.1788979 -4.1907125 -4.2114706 -4.2358623 -4.2560472 -4.2663116][-4.2582016 -4.2224612 -4.1621513 -4.1034646 -4.08399 -4.10826 -4.1307173 -4.1309805 -4.1229672 -4.1333818 -4.1639948 -4.1982193 -4.2289829 -4.249527 -4.25822][-4.2577987 -4.2237387 -4.1690774 -4.1123996 -4.0863104 -4.0953946 -4.0969329 -4.0732508 -4.0602441 -4.0928497 -4.1479974 -4.1955557 -4.2288384 -4.2459917 -4.2484736][-4.2664256 -4.2401958 -4.1981177 -4.1542797 -4.1298914 -4.1240249 -4.1010151 -4.0481081 -4.0203242 -4.0692167 -4.1441669 -4.2006588 -4.234561 -4.2501626 -4.2492676][-4.2745676 -4.2567511 -4.2219515 -4.1833577 -4.1548829 -4.13248 -4.0879912 -4.0048614 -3.9599404 -4.0243883 -4.112287 -4.1762896 -4.2146034 -4.2343764 -4.2407246][-4.2686892 -4.2536263 -4.222506 -4.1856093 -4.1463308 -4.1058521 -4.0430717 -3.9415462 -3.8897233 -3.97067 -4.0689211 -4.138411 -4.1835546 -4.2123351 -4.2274261][-4.2650423 -4.2488365 -4.223743 -4.192318 -4.1494317 -4.1006117 -4.0354915 -3.9386551 -3.8942423 -3.9733851 -4.0628386 -4.1267629 -4.1681776 -4.2000079 -4.2179523][-4.270092 -4.2563009 -4.238431 -4.2194214 -4.187428 -4.1445279 -4.0874243 -4.0060644 -3.9751658 -4.0363927 -4.1045666 -4.1537318 -4.1799903 -4.204195 -4.2182479][-4.2804747 -4.2725239 -4.25976 -4.2480378 -4.2288709 -4.1986151 -4.150806 -4.0810089 -4.0556536 -4.0976458 -4.1451211 -4.1844354 -4.20308 -4.2203174 -4.2256813][-4.2869997 -4.2844653 -4.2749805 -4.264101 -4.25296 -4.2318449 -4.1902847 -4.130775 -4.109601 -4.1372795 -4.1715503 -4.2090235 -4.2287745 -4.2418656 -4.2368979][-4.2860689 -4.2848186 -4.2775292 -4.26945 -4.2635727 -4.2441936 -4.2031527 -4.1509938 -4.1344662 -4.157444 -4.1891789 -4.2277279 -4.2495742 -4.2584519 -4.2455945][-4.2796741 -4.2773023 -4.2692742 -4.2647805 -4.2655225 -4.2500987 -4.2111034 -4.1647048 -4.1507277 -4.1675539 -4.1947236 -4.2328587 -4.2587733 -4.26746 -4.2546291][-4.2687869 -4.2647586 -4.2556496 -4.2527962 -4.25953 -4.25392 -4.2279315 -4.1935945 -4.1808076 -4.1888056 -4.201808 -4.2300782 -4.2565846 -4.2688651 -4.2631407][-4.2641983 -4.2582674 -4.2483797 -4.2440319 -4.2516613 -4.25566 -4.2458458 -4.2248678 -4.2110071 -4.208015 -4.2071786 -4.2221093 -4.2439218 -4.2587442 -4.2622924]]...]
INFO - root - 2017-12-07 23:40:18.186260: step 67610, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 48h:52m:29s remains)
INFO - root - 2017-12-07 23:40:24.946948: step 67620, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 46h:58m:14s remains)
INFO - root - 2017-12-07 23:40:31.749762: step 67630, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 49h:46m:46s remains)
INFO - root - 2017-12-07 23:40:38.579846: step 67640, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 51h:15m:28s remains)
INFO - root - 2017-12-07 23:40:45.311728: step 67650, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 52h:10m:39s remains)
INFO - root - 2017-12-07 23:40:52.070070: step 67660, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 50h:00m:42s remains)
INFO - root - 2017-12-07 23:40:58.765881: step 67670, loss = 2.04, batch loss = 1.99 (13.0 examples/sec; 0.615 sec/batch; 45h:12m:46s remains)
INFO - root - 2017-12-07 23:41:05.493645: step 67680, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 47h:33m:49s remains)
INFO - root - 2017-12-07 23:41:12.322370: step 67690, loss = 2.04, batch loss = 1.98 (10.9 examples/sec; 0.736 sec/batch; 54h:08m:57s remains)
INFO - root - 2017-12-07 23:41:19.155992: step 67700, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.738 sec/batch; 54h:14m:53s remains)
2017-12-07 23:41:19.844278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1934619 -4.2189822 -4.2531357 -4.2665219 -4.2652888 -4.2521 -4.2402558 -4.2365651 -4.2343316 -4.24457 -4.2649345 -4.2778826 -4.2850318 -4.2889118 -4.2879353][-4.169034 -4.1963477 -4.2345614 -4.2508488 -4.2513084 -4.2319484 -4.2101469 -4.1990008 -4.1983347 -4.2166958 -4.2431517 -4.2592893 -4.2708793 -4.2772241 -4.2762747][-4.1468558 -4.1692581 -4.2079024 -4.2281322 -4.23455 -4.2130718 -4.1799173 -4.1601272 -4.165061 -4.19285 -4.225543 -4.24491 -4.2552485 -4.2626262 -4.26146][-4.1042538 -4.1169791 -4.1582627 -4.1895428 -4.2052207 -4.1817393 -4.1376925 -4.1119828 -4.1247029 -4.1668983 -4.2095537 -4.2321119 -4.2391939 -4.2443562 -4.2423439][-4.0605817 -4.0656385 -4.1071486 -4.1499557 -4.17036 -4.1402869 -4.0827875 -4.0517521 -4.0774021 -4.1359181 -4.1933613 -4.2229385 -4.2297435 -4.2311177 -4.2260232][-4.0660286 -4.0609832 -4.0902767 -4.1298909 -4.1400609 -4.09334 -4.018384 -3.9788668 -4.0205283 -4.1012225 -4.1733479 -4.2133007 -4.2230506 -4.2237535 -4.215673][-4.1087241 -4.0856571 -4.0949979 -4.1158714 -4.1089497 -4.0450339 -3.9570136 -3.9090273 -3.9664342 -4.0730639 -4.1586795 -4.2057438 -4.2203112 -4.2237759 -4.2129564][-4.15433 -4.1223192 -4.1144066 -4.1190467 -4.1088281 -4.050921 -3.9666471 -3.9188681 -3.9730318 -4.079392 -4.1682873 -4.2141647 -4.2299981 -4.2375793 -4.2249713][-4.1894741 -4.1574154 -4.1440077 -4.1424966 -4.1384234 -4.1031365 -4.0366197 -3.9953356 -4.0324125 -4.114974 -4.1942973 -4.2349 -4.2495465 -4.2550535 -4.2420726][-4.21592 -4.1931577 -4.1866441 -4.1872063 -4.1875095 -4.1662555 -4.1183357 -4.0874753 -4.1096277 -4.1672091 -4.2324815 -4.2658572 -4.27297 -4.27086 -4.2579126][-4.2396836 -4.2338734 -4.2385049 -4.2412591 -4.2389755 -4.2248778 -4.1911516 -4.1684713 -4.1820645 -4.2248707 -4.2768865 -4.3009667 -4.2984438 -4.28887 -4.2751083][-4.2615271 -4.2639971 -4.2729959 -4.2769737 -4.276967 -4.2683749 -4.2434983 -4.2253547 -4.2332878 -4.2649956 -4.3062086 -4.3230243 -4.3171382 -4.3072805 -4.2924166][-4.2742815 -4.2797556 -4.28631 -4.2885056 -4.2892852 -4.2832084 -4.2628584 -4.2476034 -4.2558661 -4.2797713 -4.3108187 -4.3254528 -4.323308 -4.3155932 -4.3001504][-4.275435 -4.2826438 -4.288074 -4.2873688 -4.2865705 -4.2814865 -4.2650061 -4.253602 -4.2619567 -4.2794194 -4.298244 -4.3101234 -4.31174 -4.3088765 -4.2980933][-4.27941 -4.2876081 -4.2905178 -4.2874961 -4.2847581 -4.27923 -4.2661176 -4.2582731 -4.262598 -4.2742486 -4.2856731 -4.2960968 -4.3020043 -4.3036389 -4.2977467]]...]
INFO - root - 2017-12-07 23:41:26.534059: step 67710, loss = 2.03, batch loss = 1.98 (11.7 examples/sec; 0.684 sec/batch; 50h:20m:24s remains)
INFO - root - 2017-12-07 23:41:33.429525: step 67720, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 52h:58m:05s remains)
INFO - root - 2017-12-07 23:41:40.237975: step 67730, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 51h:46m:39s remains)
INFO - root - 2017-12-07 23:41:47.008585: step 67740, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 47h:54m:06s remains)
INFO - root - 2017-12-07 23:41:53.784995: step 67750, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 48h:09m:50s remains)
INFO - root - 2017-12-07 23:42:00.629717: step 67760, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 52h:13m:49s remains)
INFO - root - 2017-12-07 23:42:07.363944: step 67770, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 49h:10m:27s remains)
INFO - root - 2017-12-07 23:42:14.121008: step 67780, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 49h:26m:27s remains)
INFO - root - 2017-12-07 23:42:20.929745: step 67790, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 49h:28m:50s remains)
INFO - root - 2017-12-07 23:42:27.776465: step 67800, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.653 sec/batch; 47h:59m:14s remains)
2017-12-07 23:42:28.523246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2935357 -4.3062053 -4.3169351 -4.3107176 -4.2811484 -4.230701 -4.1607876 -4.10145 -4.0846257 -4.1237168 -4.1908078 -4.2444978 -4.2761273 -4.2928915 -4.3033385][-4.3043094 -4.3185496 -4.3236122 -4.3015108 -4.2539487 -4.1815572 -4.0752096 -3.9845784 -3.96501 -4.0344672 -4.1409426 -4.2194195 -4.2634597 -4.2884941 -4.3051696][-4.3152461 -4.3295846 -4.3252211 -4.2858996 -4.221415 -4.126174 -3.9874694 -3.8629687 -3.833931 -3.9376054 -4.0891914 -4.1943321 -4.2518997 -4.2878051 -4.3118567][-4.3423681 -4.3529482 -4.3359928 -4.2796693 -4.202251 -4.0936937 -3.9399347 -3.7966032 -3.7564576 -3.878294 -4.0578666 -4.1826191 -4.2494946 -4.2933121 -4.3231955][-4.3654537 -4.3666615 -4.3341265 -4.2619781 -4.1755576 -4.0661745 -3.9216819 -3.7902918 -3.7494483 -3.8655574 -4.0452852 -4.17581 -4.2491765 -4.29494 -4.3236909][-4.3795977 -4.3678188 -4.315227 -4.2275844 -4.1327486 -4.0282707 -3.9120278 -3.8168762 -3.7876182 -3.8844886 -4.0457363 -4.1718917 -4.2480621 -4.293117 -4.3197045][-4.3855915 -4.3632803 -4.2969294 -4.2030339 -4.1038542 -4.0048556 -3.9148378 -3.8541007 -3.8365564 -3.9111013 -4.0471683 -4.1665311 -4.2480397 -4.2948818 -4.3197718][-4.3803716 -4.3505015 -4.280714 -4.1915751 -4.10095 -4.0152068 -3.9511163 -3.9131794 -3.9023893 -3.9573202 -4.0678816 -4.1769562 -4.2598977 -4.3067355 -4.3279829][-4.3642282 -4.3351097 -4.2739611 -4.2010937 -4.13191 -4.0635676 -4.01346 -3.9887431 -3.9818883 -4.0205722 -4.1100569 -4.2067437 -4.281673 -4.3203373 -4.3342819][-4.3453135 -4.3243566 -4.2804403 -4.233232 -4.1924047 -4.1451588 -4.1024113 -4.078691 -4.0721474 -4.09775 -4.1675644 -4.2452183 -4.3002048 -4.3222017 -4.3253989][-4.3289561 -4.3215942 -4.2977986 -4.2743845 -4.2581072 -4.2325206 -4.1932125 -4.1622357 -4.1492586 -4.16565 -4.2185445 -4.2766557 -4.3103013 -4.3155274 -4.3087][-4.3129907 -4.3168974 -4.3089085 -4.3002906 -4.297493 -4.2872944 -4.2519336 -4.214963 -4.1938443 -4.2049575 -4.2473779 -4.2901196 -4.3073716 -4.303061 -4.2923837][-4.300323 -4.3081536 -4.3081827 -4.3061767 -4.3091278 -4.3078589 -4.280786 -4.24427 -4.2215128 -4.2302094 -4.2642403 -4.2952089 -4.3036246 -4.2973104 -4.288981][-4.2943711 -4.3003545 -4.3029542 -4.3046408 -4.3102288 -4.3136158 -4.2965717 -4.266984 -4.247982 -4.25383 -4.2790723 -4.29996 -4.3048511 -4.301013 -4.2967033][-4.2936816 -4.2962446 -4.298058 -4.3018904 -4.3085408 -4.316258 -4.3086209 -4.28808 -4.2740269 -4.2765064 -4.2920661 -4.3036976 -4.3067551 -4.3060155 -4.3061123]]...]
INFO - root - 2017-12-07 23:42:35.045316: step 67810, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 50h:56m:18s remains)
INFO - root - 2017-12-07 23:42:41.858441: step 67820, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.654 sec/batch; 48h:04m:36s remains)
INFO - root - 2017-12-07 23:42:48.729524: step 67830, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 48h:34m:35s remains)
INFO - root - 2017-12-07 23:42:55.511371: step 67840, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 51h:17m:16s remains)
INFO - root - 2017-12-07 23:43:02.263504: step 67850, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.719 sec/batch; 52h:50m:12s remains)
INFO - root - 2017-12-07 23:43:08.859799: step 67860, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 49h:53m:57s remains)
INFO - root - 2017-12-07 23:43:15.746943: step 67870, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 48h:08m:31s remains)
INFO - root - 2017-12-07 23:43:22.559475: step 67880, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 48h:39m:15s remains)
INFO - root - 2017-12-07 23:43:29.343976: step 67890, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 52h:31m:48s remains)
INFO - root - 2017-12-07 23:43:36.133591: step 67900, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 50h:49m:21s remains)
2017-12-07 23:43:36.897768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2734776 -4.2484961 -4.2361646 -4.235651 -4.2414393 -4.2592969 -4.2761407 -4.2873964 -4.2884941 -4.2843418 -4.2748618 -4.253171 -4.228559 -4.2090516 -4.208231][-4.2513556 -4.22596 -4.2139354 -4.2149072 -4.2169685 -4.2321534 -4.2516389 -4.263268 -4.269311 -4.26869 -4.2615232 -4.2384987 -4.2064295 -4.1840048 -4.1886826][-4.2220097 -4.19543 -4.1806874 -4.1830816 -4.1859961 -4.1962295 -4.2131743 -4.2250028 -4.2391224 -4.2470541 -4.2465239 -4.2280645 -4.1921492 -4.1632786 -4.1680245][-4.19323 -4.1549191 -4.1264057 -4.1229105 -4.1242952 -4.1285734 -4.141407 -4.1610394 -4.1924987 -4.2166171 -4.2228675 -4.2108326 -4.1776142 -4.1422644 -4.1393237][-4.1780791 -4.1211815 -4.0705085 -4.0564346 -4.0494986 -4.0378094 -4.0475698 -4.0850587 -4.141757 -4.1850266 -4.2063923 -4.2109737 -4.186666 -4.1483707 -4.1324334][-4.1834497 -4.1119351 -4.0449958 -4.0167141 -3.9929912 -3.9591405 -3.9686668 -4.0254793 -4.0996022 -4.1541243 -4.1844788 -4.1987538 -4.1894364 -4.1653233 -4.1413541][-4.19642 -4.1226988 -4.0516882 -4.0106773 -3.9736524 -3.9267356 -3.9404426 -4.010973 -4.091773 -4.1485147 -4.1744304 -4.1867075 -4.18554 -4.1805072 -4.1667886][-4.2003121 -4.1317072 -4.0627909 -4.0187874 -3.9830768 -3.9385898 -3.95293 -4.0351429 -4.119123 -4.1680822 -4.1828423 -4.1915107 -4.1986437 -4.2100005 -4.205349][-4.2053132 -4.1488209 -4.086751 -4.0465045 -4.0194297 -3.9848063 -4.00015 -4.0787044 -4.1560135 -4.196794 -4.2067413 -4.2188993 -4.2315412 -4.2497406 -4.2474365][-4.2220597 -4.1746888 -4.1155605 -4.0714335 -4.0463934 -4.0248547 -4.0405569 -4.1073165 -4.1707335 -4.2077475 -4.2221975 -4.2426119 -4.2561307 -4.2740068 -4.2773142][-4.2320423 -4.1890292 -4.1375289 -4.0911479 -4.0616288 -4.0434051 -4.0523863 -4.1026797 -4.1621838 -4.2022476 -4.2281117 -4.2519889 -4.2596569 -4.2683811 -4.2705584][-4.2430053 -4.2041397 -4.161943 -4.1161041 -4.0846291 -4.0645556 -4.0620451 -4.0959506 -4.1484504 -4.1964889 -4.2327843 -4.2563386 -4.2571416 -4.2493544 -4.2384782][-4.2513533 -4.2214832 -4.1894588 -4.1475372 -4.1148992 -4.0928922 -4.0834322 -4.103303 -4.1386585 -4.1850982 -4.2251449 -4.2463245 -4.243772 -4.2271457 -4.2091594][-4.2600079 -4.240222 -4.2217808 -4.1917391 -4.1623511 -4.1359296 -4.1201258 -4.1299615 -4.1542683 -4.1904173 -4.2216492 -4.236968 -4.2307591 -4.2138824 -4.1993446][-4.2727842 -4.260849 -4.2544641 -4.2366261 -4.2123733 -4.1877646 -4.1706581 -4.1737576 -4.1931019 -4.2179871 -4.2337852 -4.240449 -4.229197 -4.2107024 -4.2000732]]...]
INFO - root - 2017-12-07 23:43:43.556351: step 67910, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 51h:01m:45s remains)
INFO - root - 2017-12-07 23:43:50.298305: step 67920, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 50h:51m:20s remains)
INFO - root - 2017-12-07 23:43:56.961035: step 67930, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 46h:55m:46s remains)
INFO - root - 2017-12-07 23:44:03.693140: step 67940, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 47h:59m:38s remains)
INFO - root - 2017-12-07 23:44:10.423922: step 67950, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 51h:40m:28s remains)
INFO - root - 2017-12-07 23:44:17.246013: step 67960, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 50h:50m:59s remains)
INFO - root - 2017-12-07 23:44:24.004702: step 67970, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 52h:40m:34s remains)
INFO - root - 2017-12-07 23:44:30.845227: step 67980, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.626 sec/batch; 45h:58m:45s remains)
INFO - root - 2017-12-07 23:44:37.583028: step 67990, loss = 2.05, batch loss = 1.99 (12.9 examples/sec; 0.619 sec/batch; 45h:28m:15s remains)
INFO - root - 2017-12-07 23:44:44.409764: step 68000, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 50h:14m:54s remains)
2017-12-07 23:44:45.098752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1686931 -4.1420536 -4.1540618 -4.1851525 -4.2205896 -4.2431865 -4.2334762 -4.2051854 -4.1875019 -4.1685572 -4.130651 -4.0958014 -4.096951 -4.1324873 -4.1731582][-4.1978216 -4.1810575 -4.1842165 -4.1983876 -4.2187057 -4.2351727 -4.2321024 -4.2137561 -4.1946907 -4.1674089 -4.1204896 -4.0783639 -4.0775037 -4.1179824 -4.1613493][-4.2217231 -4.2178698 -4.2178 -4.210804 -4.210814 -4.2176881 -4.2184753 -4.2115889 -4.1993628 -4.1759124 -4.1394844 -4.1086183 -4.1110873 -4.1531529 -4.186233][-4.2413869 -4.2430673 -4.2383952 -4.2157741 -4.1953235 -4.1784739 -4.1656661 -4.1644444 -4.1722183 -4.1753573 -4.1663547 -4.1561089 -4.1660962 -4.197917 -4.2123632][-4.2474566 -4.2476563 -4.2365923 -4.2056422 -4.1657233 -4.1148548 -4.061224 -4.0497146 -4.0873842 -4.1355157 -4.1662841 -4.18074 -4.1990705 -4.2187529 -4.2133479][-4.2429404 -4.2402434 -4.2242455 -4.1908321 -4.1418953 -4.0627384 -3.9644194 -3.9314566 -3.999228 -4.0892568 -4.1520128 -4.1845965 -4.2054377 -4.213522 -4.1946049][-4.2355337 -4.2341156 -4.2230725 -4.1946373 -4.15305 -4.0795012 -3.9820633 -3.9402471 -4.0013595 -4.0862041 -4.1421137 -4.172605 -4.1872897 -4.1889639 -4.1700115][-4.2103834 -4.2092395 -4.2074666 -4.1913261 -4.1681242 -4.12328 -4.0639496 -4.0389967 -4.072804 -4.115303 -4.136394 -4.1446695 -4.1500182 -4.1553531 -4.1509638][-4.1787138 -4.1704555 -4.1682711 -4.1589446 -4.1508155 -4.1351247 -4.1118064 -4.1000805 -4.1091056 -4.1179509 -4.1107359 -4.0949368 -4.0885911 -4.0946302 -4.103662][-4.1512218 -4.1301675 -4.1168709 -4.1073389 -4.1049638 -4.105804 -4.1077251 -4.1012583 -4.0934086 -4.0828404 -4.0635867 -4.0387173 -4.0241084 -4.0275788 -4.0364213][-4.1639228 -4.1405616 -4.1171608 -4.0965371 -4.08227 -4.0826569 -4.0941305 -4.0936961 -4.0825424 -4.0728645 -4.0603857 -4.0464735 -4.0370965 -4.03642 -4.0357394][-4.2102532 -4.193408 -4.16771 -4.1316695 -4.1016498 -4.0965381 -4.107162 -4.1137877 -4.1075191 -4.0998807 -4.0951257 -4.0910578 -4.0880084 -4.0896444 -4.0849886][-4.2461305 -4.2339396 -4.2095556 -4.1661835 -4.1297436 -4.1245627 -4.1343985 -4.1484585 -4.1459637 -4.1364522 -4.1302624 -4.1297264 -4.1316237 -4.1353149 -4.13076][-4.2680969 -4.2604303 -4.2406812 -4.2013659 -4.1674047 -4.1599541 -4.1664414 -4.1817217 -4.18451 -4.1760073 -4.1652946 -4.162858 -4.1675105 -4.1719756 -4.168282][-4.2751274 -4.270885 -4.2567487 -4.2310643 -4.2051687 -4.1966443 -4.2004128 -4.2130523 -4.2181406 -4.2102523 -4.1972995 -4.189003 -4.1897979 -4.1940646 -4.1938562]]...]
INFO - root - 2017-12-07 23:44:51.742769: step 68010, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 48h:25m:04s remains)
INFO - root - 2017-12-07 23:44:58.583441: step 68020, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 48h:35m:28s remains)
INFO - root - 2017-12-07 23:45:05.435770: step 68030, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 50h:23m:57s remains)
INFO - root - 2017-12-07 23:45:12.248125: step 68040, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 50h:47m:30s remains)
INFO - root - 2017-12-07 23:45:18.989920: step 68050, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 47h:02m:26s remains)
INFO - root - 2017-12-07 23:45:25.765508: step 68060, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 48h:33m:57s remains)
INFO - root - 2017-12-07 23:45:32.572635: step 68070, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 49h:13m:11s remains)
INFO - root - 2017-12-07 23:45:39.489082: step 68080, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 52h:26m:12s remains)
INFO - root - 2017-12-07 23:45:46.278352: step 68090, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 52h:35m:11s remains)
INFO - root - 2017-12-07 23:45:53.126390: step 68100, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 50h:30m:20s remains)
2017-12-07 23:45:53.823669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3373818 -4.3138056 -4.2714586 -4.2193594 -4.1597309 -4.1158195 -4.0935059 -4.10865 -4.1421618 -4.1734495 -4.2007942 -4.2250986 -4.233645 -4.23361 -4.2331681][-4.338532 -4.3177629 -4.2787137 -4.2254767 -4.1628461 -4.1175127 -4.1034842 -4.1300669 -4.1693454 -4.1992269 -4.2223325 -4.2428594 -4.2484221 -4.2442102 -4.2391086][-4.3412261 -4.3235507 -4.2866635 -4.2309246 -4.1642709 -4.1143861 -4.099761 -4.1302333 -4.180409 -4.2171612 -4.2421374 -4.2610645 -4.2642193 -4.2548246 -4.2453928][-4.3442907 -4.3276491 -4.2924104 -4.2327948 -4.1547561 -4.0900116 -4.0615873 -4.087708 -4.1491122 -4.20577 -4.244658 -4.26795 -4.2687354 -4.2557573 -4.2446961][-4.3467584 -4.3282471 -4.2918782 -4.2282395 -4.1368666 -4.0481043 -3.9960811 -4.0115967 -4.0801458 -4.1547089 -4.2106237 -4.2453361 -4.252593 -4.24182 -4.2324033][-4.3501353 -4.3293476 -4.2903666 -4.2237964 -4.1242537 -4.0122995 -3.9313602 -3.92991 -4.0027957 -4.0918331 -4.1625957 -4.2096205 -4.2283812 -4.2248988 -4.2192082][-4.3558183 -4.3371639 -4.2989798 -4.2346621 -4.1368976 -4.0174465 -3.9196572 -3.9051535 -3.9782991 -4.0726132 -4.1459708 -4.1959329 -4.2201672 -4.2212186 -4.2154951][-4.3595653 -4.3458967 -4.3141766 -4.2603793 -4.1775069 -4.0732746 -3.9855418 -3.9690394 -4.0312696 -4.1180291 -4.1830745 -4.2242007 -4.2419014 -4.2373381 -4.2214351][-4.358984 -4.3475628 -4.3219862 -4.2776141 -4.2062464 -4.1164565 -4.0430508 -4.0298386 -4.0829816 -4.1612792 -4.2206731 -4.253058 -4.2616096 -4.2513361 -4.2248917][-4.3551993 -4.3420067 -4.3179588 -4.2788386 -4.2139268 -4.135016 -4.0749536 -4.070962 -4.1183271 -4.1863003 -4.2397656 -4.2663946 -4.268497 -4.2530112 -4.2212739][-4.3539381 -4.3386607 -4.3146138 -4.2790513 -4.2221632 -4.1561403 -4.1098547 -4.1128263 -4.154026 -4.2087812 -4.2551427 -4.2765627 -4.2732687 -4.2523584 -4.2195907][-4.358171 -4.3435769 -4.3181329 -4.2838149 -4.23541 -4.1848106 -4.1530857 -4.1600981 -4.1923056 -4.231987 -4.2698812 -4.2864113 -4.2803397 -4.2580338 -4.2260795][-4.3618283 -4.3480015 -4.3192778 -4.281642 -4.2365332 -4.1985044 -4.1776185 -4.1861649 -4.2133327 -4.2462287 -4.2795515 -4.2915854 -4.2830853 -4.2604351 -4.228878][-4.3592982 -4.3404207 -4.3042521 -4.2587557 -4.2118387 -4.1818042 -4.1734982 -4.1891732 -4.2178707 -4.2472153 -4.2748218 -4.2829013 -4.274096 -4.2537804 -4.225378][-4.3533044 -4.327836 -4.2822194 -4.2238984 -4.1657429 -4.1338997 -4.1363568 -4.1652 -4.2030163 -4.2325888 -4.2551951 -4.2609286 -4.254703 -4.2414432 -4.2216296]]...]
INFO - root - 2017-12-07 23:46:00.421313: step 68110, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 50h:33m:17s remains)
INFO - root - 2017-12-07 23:46:07.204277: step 68120, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 49h:22m:13s remains)
INFO - root - 2017-12-07 23:46:14.119729: step 68130, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 49h:03m:41s remains)
INFO - root - 2017-12-07 23:46:20.911111: step 68140, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 48h:30m:30s remains)
INFO - root - 2017-12-07 23:46:27.731943: step 68150, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 49h:56m:42s remains)
INFO - root - 2017-12-07 23:46:34.507040: step 68160, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 50h:44m:44s remains)
INFO - root - 2017-12-07 23:46:41.083889: step 68170, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 47h:40m:43s remains)
INFO - root - 2017-12-07 23:46:47.793049: step 68180, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 46h:57m:15s remains)
INFO - root - 2017-12-07 23:46:54.661916: step 68190, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 48h:49m:43s remains)
INFO - root - 2017-12-07 23:47:01.449062: step 68200, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 52h:34m:31s remains)
2017-12-07 23:47:02.353493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3442345 -4.330946 -4.3208427 -4.3150806 -4.3142633 -4.3176827 -4.3262863 -4.3324709 -4.3289318 -4.3214455 -4.3155107 -4.3130021 -4.3141832 -4.31396 -4.30373][-4.3325944 -4.3214426 -4.3126445 -4.30869 -4.3076015 -4.3065 -4.3035569 -4.2944531 -4.2788744 -4.2688522 -4.2686524 -4.27545 -4.2853951 -4.2879996 -4.2728839][-4.3137307 -4.311728 -4.3061638 -4.3019433 -4.2956743 -4.282866 -4.2585216 -4.2259 -4.196806 -4.1892586 -4.2008376 -4.2210703 -4.2407203 -4.2479186 -4.2336173][-4.29124 -4.2988391 -4.2940092 -4.2832355 -4.2646551 -4.2333941 -4.1852436 -4.131319 -4.0957909 -4.0989838 -4.1314154 -4.1708684 -4.2003384 -4.2095118 -4.1998377][-4.2533937 -4.2650404 -4.2580833 -4.2402391 -4.2103305 -4.1646347 -4.0980544 -4.0312066 -3.9970503 -4.0183949 -4.0790825 -4.1376228 -4.17262 -4.1817451 -4.1785183][-4.1995258 -4.2121015 -4.2053494 -4.1883807 -4.1545177 -4.1039991 -4.0334263 -3.9695587 -3.9507964 -3.9915304 -4.0699039 -4.1375823 -4.1674213 -4.1724925 -4.1737394][-4.1473165 -4.1582346 -4.1552491 -4.1485806 -4.1224723 -4.0825591 -4.0319834 -3.9975116 -4.0048966 -4.043313 -4.1050782 -4.1579728 -4.1751723 -4.1753378 -4.1789145][-4.1229835 -4.1273594 -4.1269011 -4.129427 -4.1194758 -4.1047473 -4.0902386 -4.0835447 -4.1003761 -4.119103 -4.1481557 -4.1758347 -4.1818566 -4.181756 -4.1900592][-4.1223521 -4.1229048 -4.126102 -4.137785 -4.1443052 -4.1497741 -4.1573453 -4.1598349 -4.1685133 -4.1696315 -4.1775217 -4.1918764 -4.1954937 -4.2000618 -4.214427][-4.1340175 -4.140811 -4.1530685 -4.1733804 -4.1875896 -4.1994638 -4.2095704 -4.2091179 -4.2081556 -4.2028885 -4.205966 -4.2156496 -4.2210479 -4.2304091 -4.2459693][-4.1570625 -4.1781087 -4.1989684 -4.2207065 -4.2336936 -4.2431974 -4.2436156 -4.2341237 -4.2285905 -4.2264667 -4.231091 -4.2396297 -4.2469888 -4.2568245 -4.2677765][-4.1864314 -4.2191153 -4.2424884 -4.2568688 -4.2609367 -4.2603679 -4.2507191 -4.2371254 -4.2350793 -4.2390413 -4.242763 -4.248611 -4.2566724 -4.2657466 -4.272553][-4.2195787 -4.2533169 -4.2737885 -4.2776651 -4.2671309 -4.254117 -4.2398038 -4.2314687 -4.236321 -4.239893 -4.2389894 -4.2394133 -4.2446074 -4.2527514 -4.2606173][-4.2449837 -4.27185 -4.2849169 -4.278038 -4.2581391 -4.2402306 -4.2304516 -4.2306376 -4.23599 -4.2341623 -4.2247839 -4.2169042 -4.2170959 -4.2265697 -4.2400017][-4.2581372 -4.2719173 -4.2738366 -4.2613292 -4.2417879 -4.2294607 -4.2300687 -4.2349105 -4.2348905 -4.2234607 -4.2036138 -4.1871405 -4.1835465 -4.1952357 -4.2156558]]...]
INFO - root - 2017-12-07 23:47:08.958051: step 68210, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 50h:18m:53s remains)
INFO - root - 2017-12-07 23:47:15.716487: step 68220, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 51h:20m:49s remains)
INFO - root - 2017-12-07 23:47:22.411385: step 68230, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 51h:14m:24s remains)
INFO - root - 2017-12-07 23:47:29.210751: step 68240, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.696 sec/batch; 51h:05m:44s remains)
INFO - root - 2017-12-07 23:47:35.894063: step 68250, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 47h:20m:52s remains)
INFO - root - 2017-12-07 23:47:42.601152: step 68260, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 48h:35m:03s remains)
INFO - root - 2017-12-07 23:47:49.443835: step 68270, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 51h:32m:02s remains)
INFO - root - 2017-12-07 23:47:56.212124: step 68280, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.721 sec/batch; 52h:52m:51s remains)
INFO - root - 2017-12-07 23:48:03.061682: step 68290, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.658 sec/batch; 48h:16m:40s remains)
INFO - root - 2017-12-07 23:48:09.797421: step 68300, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.653 sec/batch; 47h:57m:07s remains)
2017-12-07 23:48:10.508481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626266 -4.1956692 -4.1119328 -4.0647993 -4.0711007 -4.1023326 -4.1161103 -4.136158 -4.1837807 -4.2322369 -4.255496 -4.251184 -4.2321224 -4.2210655 -4.2450724][-4.2860985 -4.2172608 -4.1310253 -4.0669971 -4.0602036 -4.0889096 -4.11356 -4.1483254 -4.19985 -4.2392483 -4.2527471 -4.2456374 -4.2377849 -4.2496095 -4.2872314][-4.3045974 -4.2396173 -4.1559606 -4.0819106 -4.057744 -4.0724192 -4.0990496 -4.1407776 -4.193646 -4.2257915 -4.2371063 -4.2329869 -4.2386165 -4.2717519 -4.3174291][-4.3115115 -4.2524652 -4.1761522 -4.0986605 -4.0555968 -4.0486417 -4.0653191 -4.1125317 -4.1723304 -4.2042484 -4.2150254 -4.2198577 -4.2447524 -4.293097 -4.3376441][-4.3054361 -4.2540627 -4.1865053 -4.1118069 -4.0536804 -4.0251546 -4.0281725 -4.0774493 -4.1440678 -4.177206 -4.1891456 -4.2101526 -4.2585154 -4.3141894 -4.349577][-4.2957835 -4.2563767 -4.1985912 -4.1305637 -4.0677862 -4.03017 -4.0223861 -4.06493 -4.1243477 -4.1496611 -4.1653361 -4.2070403 -4.274394 -4.330574 -4.354116][-4.2922044 -4.2662005 -4.2203155 -4.1635361 -4.1013761 -4.0592551 -4.0444679 -4.0768971 -4.1185918 -4.1323409 -4.1540575 -4.2136464 -4.2884665 -4.338861 -4.3523145][-4.2984719 -4.2842717 -4.2506928 -4.2042565 -4.1447148 -4.0967321 -4.0727735 -4.089519 -4.1165466 -4.1296167 -4.1627851 -4.231204 -4.3033638 -4.3436966 -4.3487563][-4.3109927 -4.303874 -4.2790055 -4.2382741 -4.1814823 -4.1236224 -4.0835896 -4.0832763 -4.1055274 -4.1360607 -4.1852865 -4.2536883 -4.3151727 -4.3450265 -4.3442411][-4.3182011 -4.3116035 -4.2902293 -4.2538266 -4.20008 -4.1335087 -4.0777974 -4.0624347 -4.0876341 -4.1408629 -4.2049694 -4.2687526 -4.3184271 -4.33968 -4.3360953][-4.3187065 -4.3099713 -4.2896848 -4.2565751 -4.2067957 -4.1393142 -4.0746183 -4.0497823 -4.0814443 -4.15197 -4.2238464 -4.2806253 -4.3196354 -4.33607 -4.3317447][-4.3175883 -4.3076138 -4.2900038 -4.2605648 -4.2174807 -4.1578784 -4.0969172 -4.0710311 -4.1024814 -4.1745481 -4.2435894 -4.2927895 -4.3245344 -4.34015 -4.3370304][-4.318512 -4.3087549 -4.2933087 -4.2681222 -4.2343655 -4.1878119 -4.1415238 -4.1217914 -4.1451783 -4.2030191 -4.2618308 -4.3031912 -4.3309293 -4.3452706 -4.341989][-4.3242621 -4.3158665 -4.3042068 -4.2855396 -4.2611833 -4.2294636 -4.1986618 -4.1841617 -4.1990881 -4.2399092 -4.2849903 -4.3178883 -4.3405724 -4.3514771 -4.3471951][-4.3308854 -4.32573 -4.3186631 -4.307271 -4.2918086 -4.2726121 -4.2524872 -4.2407937 -4.24877 -4.274662 -4.3066072 -4.331439 -4.3481879 -4.355267 -4.3509789]]...]
INFO - root - 2017-12-07 23:48:17.202387: step 68310, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 52h:17m:33s remains)
INFO - root - 2017-12-07 23:48:24.140990: step 68320, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 49h:59m:36s remains)
INFO - root - 2017-12-07 23:48:30.863999: step 68330, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 47h:17m:48s remains)
INFO - root - 2017-12-07 23:48:37.659392: step 68340, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.703 sec/batch; 51h:34m:42s remains)
INFO - root - 2017-12-07 23:48:44.443594: step 68350, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 51h:21m:15s remains)
INFO - root - 2017-12-07 23:48:51.238341: step 68360, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 49h:42m:06s remains)
INFO - root - 2017-12-07 23:48:57.963774: step 68370, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 48h:18m:21s remains)
INFO - root - 2017-12-07 23:49:04.778607: step 68380, loss = 2.09, batch loss = 2.04 (12.3 examples/sec; 0.653 sec/batch; 47h:53m:22s remains)
INFO - root - 2017-12-07 23:49:11.637007: step 68390, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.698 sec/batch; 51h:14m:38s remains)
INFO - root - 2017-12-07 23:49:18.448410: step 68400, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 52h:17m:16s remains)
2017-12-07 23:49:19.278220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3523417 -4.3463979 -4.3407803 -4.3401313 -4.3428159 -4.3442407 -4.3378811 -4.3223367 -4.2965522 -4.2727056 -4.2637815 -4.2691689 -4.2782707 -4.2857337 -4.2933664][-4.3533034 -4.3412423 -4.3271933 -4.3214993 -4.3266582 -4.3348112 -4.3345671 -4.3240728 -4.3034577 -4.2857184 -4.2813382 -4.2884431 -4.295826 -4.2997236 -4.3013439][-4.3451028 -4.3261118 -4.3001728 -4.2849894 -4.2881894 -4.3032045 -4.3098593 -4.3046503 -4.29393 -4.286026 -4.2863846 -4.2940059 -4.2989287 -4.30171 -4.3018532][-4.3264475 -4.3019495 -4.2640429 -4.23453 -4.2275047 -4.2407694 -4.2540932 -4.2578635 -4.2657948 -4.2743459 -4.2827697 -4.293664 -4.2985549 -4.3015494 -4.3011045][-4.3000937 -4.2703681 -4.2213125 -4.1763582 -4.1512494 -4.1516018 -4.16394 -4.1749139 -4.2011356 -4.2267857 -4.248188 -4.2696939 -4.2838359 -4.29444 -4.2991929][-4.265389 -4.2288022 -4.1688681 -4.1017413 -4.04653 -4.0197353 -4.021244 -4.0400538 -4.0936856 -4.1465917 -4.1900363 -4.230154 -4.257802 -4.2785363 -4.2910347][-4.2277093 -4.1907139 -4.1230974 -4.0298738 -3.9243689 -3.848866 -3.8286228 -3.8563261 -3.9393785 -4.0227733 -4.091434 -4.1587315 -4.2111549 -4.2480922 -4.2744079][-4.2009239 -4.1771169 -4.1212249 -4.0187206 -3.8713226 -3.7368078 -3.680227 -3.7097208 -3.8059764 -3.9082065 -3.998394 -4.0923071 -4.1691055 -4.21867 -4.2546372][-4.1924524 -4.1910853 -4.1637096 -4.0904822 -3.9658816 -3.8356764 -3.7559359 -3.7553415 -3.8184483 -3.9001226 -3.9832411 -4.0782075 -4.1593084 -4.2097168 -4.245739][-4.2055874 -4.2180586 -4.2115669 -4.1732416 -4.1027808 -4.0315046 -3.9746974 -3.9485807 -3.9661567 -4.0070648 -4.0573645 -4.1272526 -4.1881 -4.2218275 -4.2472396][-4.2253351 -4.2368741 -4.2373195 -4.2224813 -4.1947083 -4.1726346 -4.1494842 -4.124558 -4.1177592 -4.1315856 -4.153347 -4.19261 -4.2302766 -4.2452054 -4.257103][-4.2451138 -4.2498879 -4.2555404 -4.2537856 -4.2475648 -4.2491407 -4.2426548 -4.2239704 -4.2108669 -4.2160015 -4.22697 -4.2433066 -4.2614608 -4.2671013 -4.2697563][-4.2517571 -4.2499609 -4.2564754 -4.2644453 -4.2712679 -4.2802458 -4.2779989 -4.260004 -4.2443147 -4.2493296 -4.2580276 -4.2652922 -4.2725897 -4.274425 -4.2709169][-4.2576976 -4.2535796 -4.2560039 -4.2649746 -4.2767849 -4.2866235 -4.2837172 -4.2645793 -4.2445807 -4.2445168 -4.2516603 -4.2567816 -4.2637019 -4.2670312 -4.2630429][-4.2634721 -4.2598605 -4.2594795 -4.2636452 -4.2715416 -4.2773876 -4.2728443 -4.2552848 -4.2340078 -4.2282391 -4.2346959 -4.2452855 -4.2568059 -4.2644119 -4.2617345]]...]
INFO - root - 2017-12-07 23:49:25.917079: step 68410, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 51h:30m:45s remains)
INFO - root - 2017-12-07 23:49:32.643506: step 68420, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 50h:53m:23s remains)
INFO - root - 2017-12-07 23:49:39.382179: step 68430, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 48h:15m:10s remains)
INFO - root - 2017-12-07 23:49:46.105490: step 68440, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 47h:59m:51s remains)
INFO - root - 2017-12-07 23:49:52.829017: step 68450, loss = 2.06, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 47h:29m:38s remains)
INFO - root - 2017-12-07 23:49:59.573526: step 68460, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.660 sec/batch; 48h:25m:21s remains)
INFO - root - 2017-12-07 23:50:06.283137: step 68470, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.705 sec/batch; 51h:40m:12s remains)
INFO - root - 2017-12-07 23:50:12.940368: step 68480, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 47h:33m:56s remains)
INFO - root - 2017-12-07 23:50:19.756154: step 68490, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.650 sec/batch; 47h:39m:55s remains)
INFO - root - 2017-12-07 23:50:26.551541: step 68500, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.686 sec/batch; 50h:19m:04s remains)
2017-12-07 23:50:27.332072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2632027 -4.2621012 -4.2543454 -4.246212 -4.2322483 -4.203763 -4.1638489 -4.1426821 -4.1531506 -4.180757 -4.2131038 -4.2386932 -4.2625341 -4.2866559 -4.3070431][-4.2737675 -4.2730751 -4.2648 -4.2557712 -4.2395043 -4.2069421 -4.1571589 -4.122529 -4.1338477 -4.1720772 -4.2177596 -4.2512736 -4.27719 -4.3020649 -4.3199048][-4.2542353 -4.252182 -4.2442493 -4.23596 -4.2233553 -4.1947637 -4.1417217 -4.1024804 -4.1143365 -4.1603355 -4.21312 -4.2495356 -4.2747722 -4.2944446 -4.307404][-4.2208939 -4.216845 -4.2096686 -4.2067547 -4.1968479 -4.168211 -4.1100912 -4.0674019 -4.0888543 -4.1472154 -4.2054906 -4.2402716 -4.259943 -4.2707119 -4.2780929][-4.1941128 -4.183589 -4.1700974 -4.1694803 -4.1641026 -4.1369061 -4.0711579 -4.0150576 -4.0439067 -4.1212339 -4.1879597 -4.2268052 -4.2479706 -4.2536116 -4.256547][-4.1790409 -4.1586771 -4.1292076 -4.123558 -4.1126494 -4.0827227 -4.0026655 -3.9173782 -3.946492 -4.0529351 -4.1392393 -4.1919789 -4.2259445 -4.2383862 -4.2383742][-4.1684041 -4.1499715 -4.116127 -4.1005197 -4.0754986 -4.0403156 -3.9558594 -3.8420048 -3.8608692 -3.9946938 -4.102674 -4.1654625 -4.2087331 -4.2282586 -4.2266092][-4.1572657 -4.1499143 -4.1254463 -4.1066442 -4.0809512 -4.0591092 -4.0012317 -3.9001918 -3.8879948 -4.0027175 -4.1089873 -4.1684508 -4.2105937 -4.2329574 -4.229166][-4.1352777 -4.140696 -4.1337209 -4.1190081 -4.1067348 -4.10779 -4.0802193 -4.004859 -3.9708638 -4.0461354 -4.1347928 -4.1894016 -4.2282596 -4.2472515 -4.2366681][-4.1193361 -4.1358724 -4.1464763 -4.1359959 -4.134551 -4.1510062 -4.138752 -4.0811357 -4.0412407 -4.0893683 -4.1630592 -4.2133193 -4.2440076 -4.2521582 -4.2265368][-4.1239128 -4.15062 -4.1718879 -4.1597147 -4.1512337 -4.1675248 -4.1592655 -4.1132836 -4.0756197 -4.1100149 -4.1768312 -4.2202497 -4.23895 -4.2330523 -4.1877952][-4.1167889 -4.1541648 -4.1750965 -4.1591377 -4.1468072 -4.1604743 -4.1565905 -4.1257868 -4.0937 -4.1189933 -4.1805873 -4.2171106 -4.2250085 -4.2076859 -4.1506324][-4.0906029 -4.124999 -4.1438689 -4.131928 -4.1268997 -4.14241 -4.1492605 -4.1387606 -4.115221 -4.1267686 -4.1763911 -4.2047324 -4.2065749 -4.1853223 -4.1290479][-4.0962334 -4.1148672 -4.1249976 -4.1126127 -4.106801 -4.1199389 -4.1465135 -4.1598821 -4.1510496 -4.1506314 -4.1812978 -4.194654 -4.1900434 -4.1710949 -4.1230125][-4.1353559 -4.1341696 -4.1291504 -4.1105042 -4.0972028 -4.1067352 -4.1473436 -4.1770773 -4.1790261 -4.1776433 -4.1954603 -4.1952705 -4.1817756 -4.1627479 -4.1291814]]...]
INFO - root - 2017-12-07 23:50:33.963016: step 68510, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 48h:19m:34s remains)
INFO - root - 2017-12-07 23:50:40.684226: step 68520, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.638 sec/batch; 46h:48m:30s remains)
INFO - root - 2017-12-07 23:50:47.474112: step 68530, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 52h:27m:55s remains)
INFO - root - 2017-12-07 23:50:54.254794: step 68540, loss = 2.07, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 50h:45m:15s remains)
INFO - root - 2017-12-07 23:51:00.991919: step 68550, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 48h:56m:46s remains)
INFO - root - 2017-12-07 23:51:07.701225: step 68560, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 46h:42m:49s remains)
INFO - root - 2017-12-07 23:51:14.446876: step 68570, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 47h:54m:43s remains)
INFO - root - 2017-12-07 23:51:21.221551: step 68580, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 51h:25m:00s remains)
INFO - root - 2017-12-07 23:51:28.044392: step 68590, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 51h:16m:42s remains)
INFO - root - 2017-12-07 23:51:34.873627: step 68600, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 51h:55m:31s remains)
2017-12-07 23:51:35.661963: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2836452 -4.2863903 -4.2965984 -4.3051605 -4.3072743 -4.3013148 -4.29176 -4.2856808 -4.2836819 -4.2881832 -4.303195 -4.3194342 -4.3288722 -4.32986 -4.3197894][-4.2906013 -4.291707 -4.3004575 -4.3061042 -4.3032494 -4.2918482 -4.2794714 -4.2715054 -4.2657108 -4.268064 -4.2875829 -4.3095651 -4.321043 -4.3200684 -4.307456][-4.292767 -4.2882695 -4.2907753 -4.2893839 -4.27846 -4.258997 -4.2446904 -4.2388945 -4.2340016 -4.2366381 -4.2587886 -4.2854271 -4.29868 -4.2980642 -4.287138][-4.2853985 -4.2741795 -4.2679591 -4.2573495 -4.2394352 -4.2188206 -4.2104487 -4.2122669 -4.2111893 -4.2121434 -4.2307816 -4.2559552 -4.2703428 -4.2735653 -4.2699056][-4.2657838 -4.2502632 -4.2403731 -4.2289381 -4.2118011 -4.1936021 -4.1871538 -4.1925592 -4.195405 -4.197135 -4.2112732 -4.2292838 -4.239862 -4.2445865 -4.2477779][-4.2409034 -4.2326908 -4.2251377 -4.2105894 -4.1910458 -4.168189 -4.1550026 -4.1616354 -4.1763363 -4.1880612 -4.200304 -4.2094355 -4.2128 -4.2163467 -4.2235985][-4.2235546 -4.2202744 -4.2129812 -4.1922665 -4.1622181 -4.1257086 -4.1013751 -4.1126018 -4.1476383 -4.1767774 -4.1886163 -4.1898403 -4.1885271 -4.191123 -4.2021666][-4.2094727 -4.2090096 -4.2004671 -4.1731668 -4.1307745 -4.0814552 -4.0513048 -4.0686431 -4.12232 -4.1676555 -4.1834455 -4.1823578 -4.1781831 -4.1807122 -4.1931176][-4.2032447 -4.2050161 -4.1963949 -4.1664128 -4.1166525 -4.0597482 -4.0256395 -4.0460882 -4.1076303 -4.1600056 -4.1796751 -4.1810527 -4.1784525 -4.1832247 -4.198092][-4.2130189 -4.2159963 -4.2072821 -4.176394 -4.1240411 -4.0658307 -4.0295405 -4.0460124 -4.1017823 -4.1495667 -4.1691651 -4.1760683 -4.1801653 -4.19028 -4.20495][-4.2327065 -4.2336287 -4.2248459 -4.1973739 -4.1505041 -4.0981088 -4.0631704 -4.0704913 -4.1087089 -4.1420903 -4.156652 -4.169548 -4.1835132 -4.1988873 -4.2121615][-4.2411213 -4.241889 -4.2370119 -4.2187638 -4.1846423 -4.1447821 -4.114635 -4.1088495 -4.1254144 -4.139544 -4.1411858 -4.153718 -4.178977 -4.201766 -4.219296][-4.2343121 -4.2346992 -4.2322845 -4.2211461 -4.1982808 -4.1692939 -4.1430006 -4.1303763 -4.132781 -4.1286292 -4.1170444 -4.1320634 -4.172864 -4.2063255 -4.2271709][-4.2021513 -4.1999931 -4.19855 -4.1958141 -4.1851821 -4.1670203 -4.1430221 -4.1260223 -4.1173697 -4.0971823 -4.0786467 -4.1009359 -4.156661 -4.2004781 -4.2254596][-4.1574974 -4.1567721 -4.1578407 -4.160172 -4.1573143 -4.146708 -4.1228929 -4.1008959 -4.0850444 -4.05495 -4.0341849 -4.0643568 -4.1306763 -4.1833091 -4.2146845]]...]
INFO - root - 2017-12-07 23:51:42.298547: step 68610, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.698 sec/batch; 51h:11m:29s remains)
INFO - root - 2017-12-07 23:51:49.055700: step 68620, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.661 sec/batch; 48h:28m:31s remains)
INFO - root - 2017-12-07 23:51:55.803420: step 68630, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 47h:21m:04s remains)
INFO - root - 2017-12-07 23:52:02.531954: step 68640, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 46h:24m:07s remains)
INFO - root - 2017-12-07 23:52:09.447958: step 68650, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 50h:16m:14s remains)
INFO - root - 2017-12-07 23:52:16.327754: step 68660, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 52h:26m:25s remains)
INFO - root - 2017-12-07 23:52:23.221330: step 68670, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.688 sec/batch; 50h:26m:51s remains)
INFO - root - 2017-12-07 23:52:30.057769: step 68680, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.693 sec/batch; 50h:45m:11s remains)
INFO - root - 2017-12-07 23:52:36.856159: step 68690, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 46h:23m:32s remains)
INFO - root - 2017-12-07 23:52:43.599350: step 68700, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 50h:17m:54s remains)
2017-12-07 23:52:44.312730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28795 -4.2950478 -4.3012557 -4.3004618 -4.2955594 -4.2838564 -4.2782059 -4.2780056 -4.2694459 -4.2651796 -4.2694345 -4.2670345 -4.2593579 -4.2547951 -4.2491217][-4.282063 -4.2871714 -4.2918024 -4.2878642 -4.2836914 -4.2770505 -4.2783475 -4.279747 -4.269969 -4.2635136 -4.2713327 -4.2733574 -4.26728 -4.2552462 -4.2372851][-4.2792044 -4.2796993 -4.2789011 -4.2685614 -4.2611308 -4.2562413 -4.2603917 -4.2621012 -4.258203 -4.2518044 -4.2630682 -4.2716942 -4.2677393 -4.2563186 -4.2280335][-4.2768288 -4.2717438 -4.2618384 -4.2394509 -4.223659 -4.2158475 -4.2156606 -4.2193613 -4.2271252 -4.2282796 -4.2439613 -4.2575011 -4.2544765 -4.2468829 -4.2183342][-4.2754273 -4.2689452 -4.2531376 -4.2214227 -4.1949582 -4.1756563 -4.1631746 -4.1639013 -4.1777267 -4.19072 -4.2157469 -4.2314415 -4.2247686 -4.2204266 -4.2012787][-4.2778149 -4.274775 -4.2626743 -4.2351232 -4.2070136 -4.1780972 -4.1492505 -4.1401024 -4.152823 -4.1682639 -4.1926212 -4.2047367 -4.195735 -4.1958537 -4.1924691][-4.2807832 -4.2820334 -4.2757354 -4.2569356 -4.23446 -4.2027864 -4.1623583 -4.1455464 -4.1601338 -4.1727605 -4.1895766 -4.1975451 -4.19068 -4.1939268 -4.196795][-4.2835345 -4.288455 -4.2849426 -4.2734756 -4.2597461 -4.2310958 -4.1910334 -4.1696491 -4.179966 -4.1906605 -4.2047911 -4.2126045 -4.2097344 -4.2148461 -4.2159185][-4.2849417 -4.2901258 -4.2873321 -4.279757 -4.2713008 -4.2479086 -4.2163997 -4.1975121 -4.2016158 -4.2063961 -4.2219748 -4.2339091 -4.2349296 -4.2378654 -4.2319822][-4.2849016 -4.2893929 -4.2856426 -4.27908 -4.2717395 -4.2517123 -4.2274351 -4.2096887 -4.2061486 -4.2031784 -4.2204022 -4.2385311 -4.2475586 -4.2538967 -4.2477221][-4.2841926 -4.2849035 -4.2765489 -4.2650642 -4.2558169 -4.2419233 -4.2250609 -4.2093844 -4.2041011 -4.1999083 -4.2177482 -4.237277 -4.2471395 -4.2550941 -4.2546258][-4.2827148 -4.278131 -4.263947 -4.2446756 -4.231679 -4.2214446 -4.2096581 -4.1985946 -4.1997943 -4.20307 -4.2270517 -4.2498851 -4.2579226 -4.2652493 -4.2721806][-4.2838092 -4.2788916 -4.2641711 -4.2431445 -4.2294497 -4.2186484 -4.2117372 -4.2069016 -4.212347 -4.2215338 -4.247952 -4.2693095 -4.274683 -4.2800369 -4.2918677][-4.2860193 -4.2827435 -4.2712703 -4.2534366 -4.2402091 -4.2275977 -4.2236977 -4.2224703 -4.2273412 -4.2362075 -4.2579436 -4.2758679 -4.2824421 -4.2876296 -4.2997618][-4.2862897 -4.2830081 -4.2749853 -4.2612572 -4.2471614 -4.2317467 -4.2253165 -4.224257 -4.2232389 -4.2256446 -4.2415214 -4.2553706 -4.2649083 -4.2740293 -4.286972]]...]
INFO - root - 2017-12-07 23:52:50.915376: step 68710, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 47h:41m:44s remains)
INFO - root - 2017-12-07 23:52:57.796918: step 68720, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 47h:29m:57s remains)
INFO - root - 2017-12-07 23:53:04.576365: step 68730, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.701 sec/batch; 51h:23m:41s remains)
INFO - root - 2017-12-07 23:53:11.398018: step 68740, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 51h:33m:46s remains)
INFO - root - 2017-12-07 23:53:18.186625: step 68750, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 48h:36m:56s remains)
INFO - root - 2017-12-07 23:53:24.878967: step 68760, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 47h:51m:50s remains)
INFO - root - 2017-12-07 23:53:31.661093: step 68770, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 48h:20m:18s remains)
INFO - root - 2017-12-07 23:53:38.456018: step 68780, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 51h:31m:13s remains)
INFO - root - 2017-12-07 23:53:45.005826: step 68790, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 51h:24m:25s remains)
INFO - root - 2017-12-07 23:53:51.711141: step 68800, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 47h:16m:16s remains)
2017-12-07 23:53:52.451996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2334304 -4.2163134 -4.2085094 -4.2008748 -4.1944642 -4.1966882 -4.1965861 -4.1919513 -4.1893973 -4.1864977 -4.1895576 -4.2008018 -4.2086668 -4.2263026 -4.2429018][-4.2303181 -4.2157764 -4.2054882 -4.1901755 -4.1789136 -4.1811919 -4.1837769 -4.1800094 -4.1740403 -4.1676083 -4.1696668 -4.1805644 -4.190897 -4.2157278 -4.2418776][-4.2434397 -4.2323174 -4.2151752 -4.1888151 -4.1681681 -4.1710882 -4.1841307 -4.1857777 -4.1753044 -4.161365 -4.1545119 -4.16224 -4.1757946 -4.2071352 -4.2383833][-4.2632608 -4.2510285 -4.223629 -4.1800852 -4.1416087 -4.143435 -4.1755633 -4.193831 -4.1861415 -4.1666546 -4.1499577 -4.1548352 -4.1728816 -4.2067156 -4.2367883][-4.2714915 -4.2577519 -4.2230835 -4.1595421 -4.0937529 -4.0861959 -4.143158 -4.1903305 -4.1972661 -4.1789985 -4.1581516 -4.1624489 -4.1832027 -4.2134008 -4.2352548][-4.2594051 -4.2464876 -4.2092786 -4.1270652 -4.025383 -3.9968455 -4.0780468 -4.1614285 -4.1971941 -4.1922197 -4.1707344 -4.1714725 -4.1885691 -4.2131767 -4.2297268][-4.2392974 -4.2319684 -4.1999917 -4.1074557 -3.9683714 -3.8997562 -3.9867835 -4.1051464 -4.1757655 -4.1940689 -4.1771111 -4.1716814 -4.1823831 -4.1982613 -4.2073345][-4.2059774 -4.2071218 -4.1916022 -4.1160822 -3.9791825 -3.8820877 -3.9312925 -4.0430975 -4.13336 -4.17696 -4.1737924 -4.1671228 -4.1677113 -4.17274 -4.1687565][-4.1826634 -4.1888433 -4.191226 -4.1457715 -4.0470953 -3.9620564 -3.9684083 -4.0358148 -4.1142554 -4.167222 -4.1807871 -4.18069 -4.1741581 -4.1699343 -4.1566267][-4.1849957 -4.1913233 -4.2042122 -4.1864891 -4.1306396 -4.0743003 -4.060534 -4.086031 -4.1365571 -4.1818495 -4.201364 -4.2040396 -4.1940608 -4.1839056 -4.1675754][-4.2029963 -4.2049794 -4.2172112 -4.2128725 -4.1859388 -4.154469 -4.1411119 -4.1477795 -4.175179 -4.2058864 -4.2209082 -4.2229686 -4.21429 -4.2078404 -4.1987443][-4.2247543 -4.2249765 -4.232913 -4.231122 -4.2200809 -4.2061582 -4.1979542 -4.19805 -4.2084217 -4.2225556 -4.2305927 -4.2316656 -4.2269578 -4.2266335 -4.2252064][-4.2426472 -4.2409892 -4.243371 -4.2423348 -4.2420936 -4.2437887 -4.2426314 -4.2408996 -4.2436619 -4.2477541 -4.2476225 -4.246181 -4.2434063 -4.2442021 -4.2456851][-4.2638116 -4.2620044 -4.2596421 -4.2568989 -4.2615595 -4.2713065 -4.2763872 -4.275425 -4.2759647 -4.2765088 -4.2728848 -4.2699084 -4.2674727 -4.2672281 -4.2681308][-4.2889085 -4.2876272 -4.283937 -4.2790141 -4.2826228 -4.2916923 -4.297122 -4.2980304 -4.2988467 -4.299798 -4.2967877 -4.2924914 -4.288825 -4.28715 -4.286025]]...]
INFO - root - 2017-12-07 23:53:58.915090: step 68810, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 49h:47m:10s remains)
INFO - root - 2017-12-07 23:54:05.729963: step 68820, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:55m:46s remains)
INFO - root - 2017-12-07 23:54:12.492964: step 68830, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.630 sec/batch; 46h:06m:31s remains)
INFO - root - 2017-12-07 23:54:19.324597: step 68840, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 49h:26m:04s remains)
INFO - root - 2017-12-07 23:54:26.234621: step 68850, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 51h:58m:20s remains)
INFO - root - 2017-12-07 23:54:33.051083: step 68860, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 48h:45m:05s remains)
INFO - root - 2017-12-07 23:54:39.839655: step 68870, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 47h:07m:55s remains)
INFO - root - 2017-12-07 23:54:46.554257: step 68880, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 47h:51m:54s remains)
INFO - root - 2017-12-07 23:54:53.354499: step 68890, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.689 sec/batch; 50h:27m:05s remains)
INFO - root - 2017-12-07 23:55:00.234451: step 68900, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 51h:28m:13s remains)
2017-12-07 23:55:00.911496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2913294 -4.2903075 -4.3000917 -4.3049021 -4.2969389 -4.2669096 -4.21204 -4.1537442 -4.124362 -4.1466637 -4.1972208 -4.2463646 -4.2700777 -4.2719688 -4.271132][-4.301856 -4.2985053 -4.3017569 -4.2980723 -4.2812371 -4.2449536 -4.1843686 -4.1261196 -4.1038256 -4.1355486 -4.1927013 -4.2472835 -4.2759333 -4.2758541 -4.268364][-4.3078818 -4.3031139 -4.2999663 -4.2879057 -4.2608981 -4.2195172 -4.1622109 -4.112587 -4.1000328 -4.1378813 -4.1971855 -4.2532911 -4.2835035 -4.2813787 -4.2684946][-4.311852 -4.3083272 -4.3024368 -4.2867703 -4.2535791 -4.2076273 -4.1523762 -4.1069045 -4.0989709 -4.1389537 -4.19931 -4.2579479 -4.2902827 -4.2912512 -4.2771835][-4.3165674 -4.3165379 -4.3083925 -4.2901254 -4.253983 -4.2012582 -4.140348 -4.0893927 -4.0762134 -4.1131849 -4.1789212 -4.2452903 -4.2869735 -4.2970362 -4.2840486][-4.3168459 -4.3203073 -4.3103237 -4.2910666 -4.2536469 -4.192637 -4.1195703 -4.0540361 -4.0305266 -4.0661077 -4.1390524 -4.2153864 -4.2707233 -4.2945995 -4.2866983][-4.3156815 -4.3223515 -4.313201 -4.2948427 -4.2585497 -4.1939931 -4.1108546 -4.0317497 -3.9961071 -4.0301309 -4.1080604 -4.1914468 -4.2583871 -4.2946482 -4.2965217][-4.3185592 -4.3303032 -4.3243771 -4.3090858 -4.2768669 -4.2153597 -4.1296444 -4.0412345 -3.9907143 -4.0130219 -4.0882058 -4.1733127 -4.2487783 -4.2990847 -4.3142543][-4.3154769 -4.3357134 -4.335784 -4.3259358 -4.3027935 -4.2504215 -4.16848 -4.0769591 -4.0117679 -4.013145 -4.07588 -4.1592298 -4.2397666 -4.2993436 -4.3260489][-4.2931204 -4.3253279 -4.3345733 -4.3321481 -4.3186884 -4.2785592 -4.2072959 -4.1222162 -4.05584 -4.0447755 -4.0923834 -4.1644788 -4.2372131 -4.2935328 -4.3236661][-4.2615242 -4.3023834 -4.3215623 -4.3263025 -4.3183079 -4.287065 -4.2282014 -4.1551561 -4.0985861 -4.0885973 -4.1250677 -4.1786456 -4.2318759 -4.2767477 -4.3082414][-4.2353382 -4.2811794 -4.3076425 -4.3154936 -4.3080668 -4.2793155 -4.2249808 -4.1577015 -4.1093063 -4.1035585 -4.1367812 -4.179873 -4.2174706 -4.2519031 -4.2853284][-4.2276564 -4.274816 -4.3019853 -4.307157 -4.2958031 -4.2647 -4.2120652 -4.1486068 -4.1065836 -4.1062779 -4.139514 -4.1763105 -4.2022495 -4.2260165 -4.257473][-4.2282376 -4.2732992 -4.2987304 -4.3009839 -4.2873435 -4.2576942 -4.2108932 -4.1571665 -4.125267 -4.1305776 -4.160038 -4.1857095 -4.1955423 -4.2033687 -4.2281785][-4.2333069 -4.275836 -4.2994971 -4.2977533 -4.2807078 -4.2524672 -4.2152596 -4.1746373 -4.1542397 -4.1617565 -4.185061 -4.200645 -4.1963835 -4.19007 -4.2069793]]...]
INFO - root - 2017-12-07 23:55:07.547839: step 68910, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 47h:59m:41s remains)
INFO - root - 2017-12-07 23:55:14.318104: step 68920, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.708 sec/batch; 51h:51m:18s remains)
INFO - root - 2017-12-07 23:55:21.056979: step 68930, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 51h:45m:59s remains)
INFO - root - 2017-12-07 23:55:27.906024: step 68940, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 48h:18m:03s remains)
INFO - root - 2017-12-07 23:55:34.575045: step 68950, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 48h:04m:55s remains)
INFO - root - 2017-12-07 23:55:41.411957: step 68960, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 50h:26m:57s remains)
INFO - root - 2017-12-07 23:55:48.233799: step 68970, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 51h:18m:14s remains)
INFO - root - 2017-12-07 23:55:55.102101: step 68980, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:54m:18s remains)
INFO - root - 2017-12-07 23:56:01.885570: step 68990, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 49h:12m:29s remains)
INFO - root - 2017-12-07 23:56:08.616656: step 69000, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.632 sec/batch; 46h:17m:34s remains)
2017-12-07 23:56:09.424471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21385 -4.1743627 -4.1430855 -4.1239142 -4.1126084 -4.1065855 -4.0997252 -4.0841217 -4.0579929 -4.0520625 -4.0759373 -4.1146989 -4.1591139 -4.20498 -4.2352095][-4.2198262 -4.1816778 -4.1497488 -4.1296172 -4.1150007 -4.1077585 -4.0989532 -4.0872083 -4.0683184 -4.0629897 -4.0832682 -4.12363 -4.171844 -4.2239432 -4.2640519][-4.2466388 -4.2125387 -4.1782742 -4.1509385 -4.1276479 -4.1121516 -4.0961804 -4.0868468 -4.0758476 -4.0777907 -4.1004453 -4.1409316 -4.1880894 -4.2395434 -4.2811618][-4.263958 -4.235127 -4.1975088 -4.1575685 -4.1244755 -4.1006842 -4.0761852 -4.0653286 -4.059876 -4.07141 -4.0998287 -4.1418314 -4.1918373 -4.2435308 -4.2830863][-4.2508454 -4.2198267 -4.1766119 -4.1237459 -4.07918 -4.0489726 -4.0222249 -4.0147781 -4.0199442 -4.0450134 -4.0824986 -4.1306486 -4.1862931 -4.2386904 -4.2736263][-4.2276726 -4.1929088 -4.1472673 -4.0851717 -4.0274487 -3.985218 -3.9535117 -3.9482203 -3.968349 -4.0104704 -4.0591483 -4.1160736 -4.1749425 -4.2242818 -4.2531719][-4.2217569 -4.1901908 -4.1499686 -4.0860806 -4.017355 -3.9596977 -3.9128432 -3.8979998 -3.9271243 -3.987606 -4.051672 -4.1147242 -4.1723232 -4.2132635 -4.2297153][-4.2358127 -4.2115517 -4.1796451 -4.1231871 -4.0580735 -3.9974427 -3.9387331 -3.9078221 -3.9344754 -4.0029421 -4.0735278 -4.1355042 -4.1852112 -4.2165828 -4.2213058][-4.25514 -4.2367492 -4.2137356 -4.1670427 -4.1088533 -4.0508838 -3.9909804 -3.9530401 -3.9717405 -4.0376158 -4.105422 -4.1621728 -4.2033577 -4.2276235 -4.2261038][-4.2749872 -4.2616234 -4.2465639 -4.2065959 -4.1524143 -4.0968404 -4.0431066 -4.0057755 -4.0165462 -4.0742331 -4.1342478 -4.1817737 -4.2123771 -4.2308583 -4.2272787][-4.2843165 -4.2755132 -4.2689567 -4.2412682 -4.1955256 -4.1419806 -4.0910358 -4.0530243 -4.0520415 -4.1004453 -4.1551433 -4.19565 -4.2168918 -4.2305036 -4.225306][-4.2778077 -4.272902 -4.2761912 -4.2666435 -4.2359653 -4.1911092 -4.1452255 -4.1060643 -4.09517 -4.1326261 -4.181942 -4.214798 -4.2271256 -4.231874 -4.2206049][-4.2737427 -4.2691584 -4.2752509 -4.2785487 -4.2637539 -4.2315145 -4.1941023 -4.1593704 -4.1488562 -4.1801453 -4.2244554 -4.2511744 -4.255332 -4.2509012 -4.2349486][-4.2859221 -4.2808022 -4.2843075 -4.2891541 -4.2821321 -4.2587156 -4.2271981 -4.1983719 -4.1929979 -4.2221026 -4.2617254 -4.2857461 -4.2875819 -4.2824197 -4.2690716][-4.3051987 -4.3009171 -4.3002133 -4.3016958 -4.2950025 -4.2737575 -4.2445893 -4.2196493 -4.2161136 -4.2396283 -4.274055 -4.2973056 -4.3040662 -4.3056207 -4.2996254]]...]
INFO - root - 2017-12-07 23:56:16.114927: step 69010, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.712 sec/batch; 52h:08m:54s remains)
INFO - root - 2017-12-07 23:56:22.993629: step 69020, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 49h:01m:34s remains)
INFO - root - 2017-12-07 23:56:29.718477: step 69030, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 46h:57m:17s remains)
INFO - root - 2017-12-07 23:56:36.448375: step 69040, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 50h:36m:05s remains)
INFO - root - 2017-12-07 23:56:43.309292: step 69050, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 52h:18m:01s remains)
INFO - root - 2017-12-07 23:56:50.071076: step 69060, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 49h:26m:10s remains)
INFO - root - 2017-12-07 23:56:56.814273: step 69070, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 47h:36m:21s remains)
INFO - root - 2017-12-07 23:57:03.654130: step 69080, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.642 sec/batch; 46h:57m:59s remains)
INFO - root - 2017-12-07 23:57:10.464104: step 69090, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 50h:41m:43s remains)
INFO - root - 2017-12-07 23:57:17.086424: step 69100, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 52h:13m:14s remains)
2017-12-07 23:57:17.920151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.235414 -4.2361231 -4.2352204 -4.2238712 -4.209867 -4.203877 -4.2017179 -4.1978431 -4.2046585 -4.2227497 -4.2335982 -4.2325668 -4.2316432 -4.2195926 -4.2051086][-4.2438183 -4.2469764 -4.2488651 -4.2415009 -4.2291436 -4.2206745 -4.2165203 -4.2145019 -4.2233968 -4.2422972 -4.256362 -4.2601953 -4.2603526 -4.2519846 -4.2414532][-4.2641225 -4.2717795 -4.2736053 -4.26557 -4.2539864 -4.2397494 -4.2282724 -4.2268519 -4.2391009 -4.2600155 -4.2783155 -4.2851954 -4.2847285 -4.2761383 -4.2670531][-4.277575 -4.2943788 -4.2999892 -4.2886219 -4.2693763 -4.2448807 -4.2236075 -4.2189846 -4.2320061 -4.2536416 -4.2779751 -4.2908492 -4.2877703 -4.2733378 -4.2621593][-4.2572141 -4.281816 -4.2922883 -4.2769346 -4.2453384 -4.2027259 -4.1655722 -4.1544533 -4.1770282 -4.21275 -4.2499423 -4.2753749 -4.2746687 -4.2555761 -4.2403588][-4.2045832 -4.2350426 -4.2474842 -4.2274179 -4.1804705 -4.1095715 -4.0424037 -4.021472 -4.0674067 -4.1360331 -4.1944246 -4.236876 -4.2510548 -4.2370749 -4.2221313][-4.1299753 -4.1689172 -4.1823764 -4.156889 -4.0927734 -3.9898345 -3.8844256 -3.856149 -3.9347038 -4.0437808 -4.127625 -4.1869249 -4.2191944 -4.2169237 -4.2066407][-4.052774 -4.1022429 -4.1122861 -4.0795469 -4.0098038 -3.8887088 -3.7543681 -3.7142062 -3.8123567 -3.9552598 -4.0658 -4.1413221 -4.188488 -4.1973319 -4.192533][-4.0503917 -4.1065164 -4.1147866 -4.0856781 -4.032321 -3.9324658 -3.8122313 -3.7592077 -3.8245597 -3.9540541 -4.0682936 -4.1462379 -4.1934357 -4.203371 -4.200839][-4.1351051 -4.1787233 -4.1799531 -4.1578269 -4.132165 -4.0784588 -4.0097895 -3.9648652 -3.9823551 -4.0610161 -4.1389108 -4.1877184 -4.2158055 -4.2164021 -4.2109303][-4.2055988 -4.2293148 -4.22584 -4.212306 -4.2039351 -4.18072 -4.1516695 -4.12287 -4.11676 -4.1549315 -4.1948423 -4.2104516 -4.21482 -4.2036071 -4.1954222][-4.2099776 -4.22599 -4.2266321 -4.2207074 -4.2176404 -4.2073889 -4.1958079 -4.177887 -4.1633825 -4.1824756 -4.2046852 -4.2023854 -4.1901484 -4.1653543 -4.1547093][-4.1812387 -4.194109 -4.1997361 -4.2010622 -4.2011313 -4.1962872 -4.1918435 -4.1809435 -4.1666956 -4.177639 -4.1936679 -4.1822257 -4.1598334 -4.1250362 -4.1050878][-4.1549578 -4.160253 -4.1695876 -4.1729555 -4.1702037 -4.1645441 -4.160512 -4.1571684 -4.1523576 -4.1638684 -4.179903 -4.1691904 -4.1462264 -4.1040878 -4.0703182][-4.159481 -4.1632152 -4.1729016 -4.172411 -4.1643362 -4.1548924 -4.1493087 -4.1483359 -4.1497331 -4.1614046 -4.177619 -4.1752806 -4.1599932 -4.1202621 -4.0811224]]...]
INFO - root - 2017-12-07 23:57:24.491972: step 69110, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 51h:11m:35s remains)
INFO - root - 2017-12-07 23:57:31.146303: step 69120, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 50h:44m:50s remains)
INFO - root - 2017-12-07 23:57:37.871799: step 69130, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 49h:39m:13s remains)
INFO - root - 2017-12-07 23:57:44.589630: step 69140, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 48h:12m:17s remains)
INFO - root - 2017-12-07 23:57:51.336513: step 69150, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 48h:52m:45s remains)
INFO - root - 2017-12-07 23:57:58.107522: step 69160, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.703 sec/batch; 51h:24m:18s remains)
INFO - root - 2017-12-07 23:58:04.821519: step 69170, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.651 sec/batch; 47h:35m:12s remains)
INFO - root - 2017-12-07 23:58:11.682116: step 69180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.693 sec/batch; 50h:40m:49s remains)
INFO - root - 2017-12-07 23:58:18.385110: step 69190, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 47h:10m:18s remains)
INFO - root - 2017-12-07 23:58:25.141149: step 69200, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 47h:52m:06s remains)
2017-12-07 23:58:25.901609: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.300756 -4.3025718 -4.3001962 -4.2948036 -4.2884974 -4.2816567 -4.2722054 -4.2636204 -4.2564392 -4.2479911 -4.2384019 -4.2338691 -4.2296648 -4.2278504 -4.2369132][-4.2749014 -4.2787523 -4.2773533 -4.26899 -4.2575579 -4.2453146 -4.2317767 -4.2216115 -4.2135482 -4.2034326 -4.1915946 -4.1864939 -4.1815238 -4.1786332 -4.193192][-4.2422295 -4.24893 -4.2480817 -4.2364707 -4.2197824 -4.2017808 -4.1868935 -4.1792126 -4.1761184 -4.1720519 -4.1666012 -4.1668472 -4.1621141 -4.1565146 -4.1699181][-4.2059469 -4.2157478 -4.2171645 -4.2046313 -4.1809087 -4.1540985 -4.1359124 -4.1311274 -4.1374726 -4.1477823 -4.1550217 -4.1637325 -4.1620035 -4.155303 -4.1645842][-4.1777 -4.1893153 -4.1941485 -4.1826262 -4.1514235 -4.1119251 -4.0807261 -4.0693755 -4.0838208 -4.1130681 -4.1412163 -4.1629515 -4.1643538 -4.1570997 -4.1614275][-4.1668105 -4.1767731 -4.1798396 -4.1677289 -4.1314831 -4.0780706 -4.0218067 -3.9872475 -4.0071774 -4.0597649 -4.1169691 -4.1603413 -4.1694884 -4.1643515 -4.1644864][-4.1683259 -4.1726847 -4.1681156 -4.1501441 -4.1075726 -4.0428 -3.9566557 -3.887428 -3.9088733 -3.9921181 -4.085423 -4.1543069 -4.1788535 -4.1801934 -4.18064][-4.1671329 -4.1672783 -4.1590838 -4.131392 -4.0799251 -4.0091391 -3.8987274 -3.7925413 -3.8108282 -3.9216449 -4.0339694 -4.1200643 -4.1669822 -4.1821213 -4.1861935][-4.1589274 -4.1590281 -4.1511526 -4.11678 -4.0635929 -3.9991908 -3.8951993 -3.7789335 -3.7949543 -3.9017668 -3.9973538 -4.0758662 -4.13277 -4.1577783 -4.166276][-4.1487532 -4.1506143 -4.1459293 -4.1174941 -4.0783987 -4.0425138 -3.9820457 -3.8989463 -3.8995292 -3.964206 -4.0194387 -4.06653 -4.111084 -4.1340585 -4.1476197][-4.1470585 -4.1466923 -4.1475139 -4.1315193 -4.1121426 -4.1035895 -4.0829229 -4.0386224 -4.0300274 -4.055707 -4.076364 -4.091835 -4.116538 -4.1340365 -4.1514821][-4.1526933 -4.1470022 -4.149694 -4.1443491 -4.1431832 -4.150619 -4.1524057 -4.1345763 -4.12823 -4.1377354 -4.1445289 -4.1490207 -4.1584587 -4.1647544 -4.1793809][-4.1774225 -4.168366 -4.1640706 -4.1560345 -4.1608043 -4.1779408 -4.1928639 -4.1908116 -4.1882052 -4.1903486 -4.1938653 -4.2034321 -4.2059631 -4.2044539 -4.2168393][-4.2028203 -4.1925383 -4.1805344 -4.16721 -4.1685061 -4.1863403 -4.2054071 -4.20745 -4.2058578 -4.2053738 -4.2113552 -4.2295375 -4.2365046 -4.236423 -4.2479711][-4.2146287 -4.2066617 -4.1961274 -4.1822667 -4.1802011 -4.1928592 -4.2095947 -4.2128315 -4.2123675 -4.2147565 -4.220634 -4.2354965 -4.2456555 -4.2512417 -4.2624254]]...]
INFO - root - 2017-12-07 23:58:32.523713: step 69210, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 51h:09m:35s remains)
INFO - root - 2017-12-07 23:58:39.316862: step 69220, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 48h:59m:24s remains)
INFO - root - 2017-12-07 23:58:46.047457: step 69230, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.653 sec/batch; 47h:44m:51s remains)
INFO - root - 2017-12-07 23:58:52.796849: step 69240, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.707 sec/batch; 51h:43m:42s remains)
INFO - root - 2017-12-07 23:58:59.538717: step 69250, loss = 2.03, batch loss = 1.98 (11.4 examples/sec; 0.705 sec/batch; 51h:31m:12s remains)
INFO - root - 2017-12-07 23:59:06.356602: step 69260, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 48h:16m:01s remains)
INFO - root - 2017-12-07 23:59:13.081935: step 69270, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 46h:06m:56s remains)
INFO - root - 2017-12-07 23:59:19.846021: step 69280, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 48h:42m:03s remains)
INFO - root - 2017-12-07 23:59:26.670123: step 69290, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 50h:25m:03s remains)
INFO - root - 2017-12-07 23:59:33.542229: step 69300, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 52h:53m:04s remains)
2017-12-07 23:59:34.266975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2083006 -4.1845951 -4.1599736 -4.12961 -4.1159482 -4.1251864 -4.1418438 -4.1705155 -4.2050223 -4.2297673 -4.244174 -4.2550759 -4.2512045 -4.2380857 -4.222383][-4.2411904 -4.2072892 -4.1708789 -4.1340365 -4.1185503 -4.1322422 -4.1552038 -4.1857791 -4.2184381 -4.2398233 -4.2531643 -4.2656145 -4.2697778 -4.2688875 -4.2594604][-4.2514944 -4.2109251 -4.1697965 -4.1399255 -4.1327233 -4.1489372 -4.173737 -4.2034006 -4.2331634 -4.2523489 -4.2657256 -4.2777162 -4.284936 -4.2896285 -4.282886][-4.2457423 -4.2035708 -4.1653166 -4.1427331 -4.1346846 -4.1393962 -4.1506281 -4.1701384 -4.1986408 -4.2273426 -4.2491679 -4.2632871 -4.2750044 -4.2859278 -4.2778354][-4.227066 -4.1913633 -4.1632252 -4.1501713 -4.1387272 -4.1232767 -4.1038232 -4.0999918 -4.1288595 -4.1660852 -4.1918287 -4.2053533 -4.2204285 -4.2373252 -4.2302432][-4.2142048 -4.19589 -4.1772614 -4.1689744 -4.149972 -4.1065631 -4.0356612 -3.9957743 -4.0395851 -4.097168 -4.12708 -4.1363859 -4.1454105 -4.1563497 -4.1512585][-4.208632 -4.2045984 -4.1886196 -4.1670537 -4.12399 -4.0384817 -3.9099884 -3.8409889 -3.9296081 -4.03056 -4.0774412 -4.0969672 -4.1065111 -4.1086903 -4.1035037][-4.2145171 -4.2173924 -4.2020063 -4.1727629 -4.1178555 -4.0186763 -3.8813038 -3.8101215 -3.9175267 -4.0284266 -4.0745358 -4.09521 -4.1058645 -4.1053033 -4.1025143][-4.2272954 -4.239882 -4.2299485 -4.2037263 -4.1601672 -4.0919266 -4.01077 -3.9649317 -4.0289559 -4.093257 -4.1097112 -4.1095524 -4.1081815 -4.1050019 -4.1120687][-4.2117076 -4.2356811 -4.2323985 -4.2075953 -4.1797895 -4.1447558 -4.1058178 -4.0730023 -4.1025715 -4.1339293 -4.1330829 -4.1231027 -4.1164837 -4.120358 -4.1404881][-4.1944714 -4.223083 -4.2267137 -4.2093992 -4.2018838 -4.1905179 -4.1701717 -4.1416707 -4.1481266 -4.1597381 -4.1513948 -4.1420565 -4.1412673 -4.1574116 -4.185854][-4.1996746 -4.2216125 -4.2352867 -4.2291679 -4.2382126 -4.2446589 -4.2360635 -4.2099628 -4.19998 -4.1936975 -4.1752272 -4.1663156 -4.173893 -4.1983089 -4.22999][-4.1974735 -4.206974 -4.2240615 -4.2265997 -4.2423415 -4.2547822 -4.25172 -4.2274466 -4.21048 -4.2008085 -4.1871753 -4.1858234 -4.1998034 -4.2251148 -4.2509241][-4.1891775 -4.1902719 -4.21167 -4.2202187 -4.2324762 -4.2410431 -4.2379341 -4.2144284 -4.1963773 -4.1883993 -4.1865168 -4.1954474 -4.2104368 -4.2332587 -4.2546277][-4.2082734 -4.2036543 -4.2241106 -4.2347665 -4.243331 -4.2464247 -4.2424231 -4.2217169 -4.2013645 -4.1892323 -4.1877985 -4.1973734 -4.2102776 -4.2309494 -4.2486653]]...]
INFO - root - 2017-12-07 23:59:40.913550: step 69310, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 50h:03m:57s remains)
INFO - root - 2017-12-07 23:59:47.859296: step 69320, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 49h:39m:45s remains)
INFO - root - 2017-12-07 23:59:54.611441: step 69330, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 49h:41m:01s remains)
INFO - root - 2017-12-08 00:00:01.480117: step 69340, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 47h:00m:27s remains)
INFO - root - 2017-12-08 00:00:08.264676: step 69350, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.651 sec/batch; 47h:36m:24s remains)
INFO - root - 2017-12-08 00:00:15.102711: step 69360, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.693 sec/batch; 50h:40m:17s remains)
INFO - root - 2017-12-08 00:00:21.941870: step 69370, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 51h:16m:27s remains)
INFO - root - 2017-12-08 00:00:28.726347: step 69380, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 50h:34m:35s remains)
INFO - root - 2017-12-08 00:00:35.544878: step 69390, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 51h:41m:52s remains)
INFO - root - 2017-12-08 00:00:42.338816: step 69400, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 47h:50m:19s remains)
2017-12-08 00:00:43.084610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2641516 -4.2726851 -4.2858038 -4.2978725 -4.3062081 -4.3027277 -4.2865982 -4.2609835 -4.2338462 -4.2109995 -4.1942158 -4.1870761 -4.1886497 -4.1950588 -4.1977324][-4.2486176 -4.25535 -4.2712536 -4.2827129 -4.2882609 -4.28132 -4.2586627 -4.2252436 -4.1936493 -4.1685095 -4.14987 -4.1369519 -4.1347575 -4.1443744 -4.14836][-4.232944 -4.2377486 -4.2574129 -4.2690458 -4.2680912 -4.2513986 -4.2216063 -4.1848845 -4.1495247 -4.1242337 -4.1075659 -4.0939145 -4.0935349 -4.1057248 -4.1096535][-4.2167683 -4.2222161 -4.2469392 -4.2585526 -4.2440729 -4.2072482 -4.16446 -4.1240158 -4.0862074 -4.0653424 -4.0609932 -4.0621238 -4.0746412 -4.0895047 -4.0983014][-4.2136011 -4.2180705 -4.2386789 -4.241827 -4.2127566 -4.1570354 -4.1017523 -4.0561056 -4.0244637 -4.0198717 -4.0396423 -4.0658278 -4.0927982 -4.1098042 -4.1223207][-4.2252178 -4.2280684 -4.2376943 -4.2258749 -4.1800728 -4.1068449 -4.0400095 -3.9941883 -3.9771521 -4.0056863 -4.0615969 -4.1110754 -4.1458306 -4.1671028 -4.1862669][-4.2398014 -4.2351351 -4.2294135 -4.2035189 -4.14126 -4.0521097 -3.9760547 -3.93656 -3.9455905 -4.0103588 -4.0952692 -4.1590471 -4.1945052 -4.2216816 -4.2472968][-4.2481513 -4.2377768 -4.2216477 -4.1903291 -4.1232572 -4.0353436 -3.9712176 -3.9557173 -3.9888523 -4.0639725 -4.145062 -4.2008104 -4.2319808 -4.2555346 -4.2779436][-4.2631259 -4.2481956 -4.2262955 -4.1966434 -4.1390266 -4.0685763 -4.0282693 -4.0335078 -4.0772252 -4.1431236 -4.2028375 -4.2384582 -4.2594309 -4.2742486 -4.2855353][-4.2748752 -4.254498 -4.2290897 -4.2062693 -4.1679363 -4.1211166 -4.0976515 -4.10939 -4.1470704 -4.1967845 -4.2359524 -4.2540216 -4.2660027 -4.2703662 -4.2688761][-4.2803965 -4.2575178 -4.2336459 -4.2197347 -4.2004652 -4.1761374 -4.1652818 -4.1754918 -4.2022109 -4.23195 -4.2490172 -4.2510929 -4.2505689 -4.2448978 -4.2352552][-4.2859154 -4.2633281 -4.2421489 -4.2349019 -4.2282448 -4.2200842 -4.2183275 -4.2293248 -4.2467623 -4.2594357 -4.2599564 -4.2496819 -4.2367244 -4.221468 -4.2076936][-4.2966657 -4.27675 -4.2602763 -4.2578931 -4.2594447 -4.2618041 -4.26299 -4.2712021 -4.2830076 -4.2875309 -4.2815723 -4.2659807 -4.244967 -4.2236032 -4.2058616][-4.3107839 -4.2952085 -4.2817974 -4.2797432 -4.2849941 -4.2918596 -4.2940726 -4.2983375 -4.3053203 -4.3040895 -4.2940383 -4.279048 -4.2603445 -4.2413697 -4.2237439][-4.3228326 -4.3133478 -4.3031039 -4.2990079 -4.3018293 -4.3073783 -4.3084888 -4.3098416 -4.3108144 -4.3064046 -4.296463 -4.28422 -4.272253 -4.2593927 -4.2465606]]...]
INFO - root - 2017-12-08 00:00:49.563470: step 69410, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 50h:40m:34s remains)
INFO - root - 2017-12-08 00:00:56.411183: step 69420, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 46h:33m:17s remains)
INFO - root - 2017-12-08 00:01:03.268035: step 69430, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.694 sec/batch; 50h:44m:35s remains)
INFO - root - 2017-12-08 00:01:10.107425: step 69440, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 51h:24m:43s remains)
INFO - root - 2017-12-08 00:01:16.909629: step 69450, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 51h:35m:44s remains)
INFO - root - 2017-12-08 00:01:23.729038: step 69460, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 46h:18m:06s remains)
INFO - root - 2017-12-08 00:01:30.666349: step 69470, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 48h:46m:29s remains)
INFO - root - 2017-12-08 00:01:37.496513: step 69480, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 51h:50m:32s remains)
INFO - root - 2017-12-08 00:01:44.301245: step 69490, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:49m:40s remains)
INFO - root - 2017-12-08 00:01:51.218774: step 69500, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.711 sec/batch; 51h:55m:06s remains)
2017-12-08 00:01:51.940686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2966118 -4.2858577 -4.2712188 -4.25979 -4.2548985 -4.2564259 -4.2669516 -4.2804756 -4.2927742 -4.3019357 -4.3075538 -4.3130312 -4.3193774 -4.3243337 -4.3237739][-4.2768216 -4.2524133 -4.2245131 -4.206593 -4.2019916 -4.2063632 -4.2239475 -4.24809 -4.2706246 -4.2871871 -4.296957 -4.3050656 -4.3143549 -4.3214374 -4.3220153][-4.2520146 -4.21276 -4.1700239 -4.14452 -4.1394033 -4.1449857 -4.1690178 -4.2044306 -4.2393756 -4.2643156 -4.280654 -4.2930765 -4.3050995 -4.3145776 -4.315836][-4.23138 -4.17946 -4.1238728 -4.0908914 -4.0831971 -4.0839 -4.1071296 -4.150311 -4.197556 -4.2307944 -4.2559466 -4.2741842 -4.2904162 -4.3035293 -4.3058147][-4.2219014 -4.1627488 -4.0974693 -4.0569797 -4.0444126 -4.0392671 -4.0564432 -4.0984025 -4.1506004 -4.1885142 -4.2220178 -4.2470317 -4.2675848 -4.2859883 -4.2925425][-4.2223516 -4.159061 -4.086247 -4.0397873 -4.0204282 -4.0089831 -4.0185709 -4.0519466 -4.0982013 -4.1365328 -4.1756363 -4.2106676 -4.2380061 -4.2641878 -4.2763438][-4.2273746 -4.1607189 -4.0844 -4.0369678 -4.0086083 -3.9879777 -3.9856822 -4.0036354 -4.0399537 -4.0807438 -4.1284609 -4.1716275 -4.2060404 -4.2381587 -4.2568789][-4.2367082 -4.1691904 -4.0939198 -4.0450654 -4.0104322 -3.9819739 -3.9647241 -3.9671416 -3.9967475 -4.0394173 -4.0947723 -4.1432505 -4.1805229 -4.2152524 -4.2381358][-4.24864 -4.1868076 -4.1182981 -4.0692234 -4.032939 -4.0025 -3.9776745 -3.9708042 -3.9967937 -4.0348506 -4.0839529 -4.13021 -4.1665888 -4.2005682 -4.2211328][-4.2633057 -4.2097597 -4.1520915 -4.1053948 -4.0736055 -4.0505085 -4.0277486 -4.0151124 -4.0331678 -4.0581808 -4.0897317 -4.1289325 -4.1595821 -4.1872444 -4.2040615][-4.2801075 -4.2384143 -4.1939168 -4.1553025 -4.1313581 -4.1204858 -4.1039481 -4.0879107 -4.0898266 -4.0941463 -4.1027603 -4.1280789 -4.1522145 -4.1734595 -4.1878524][-4.2975769 -4.2701726 -4.2394581 -4.2098694 -4.1944838 -4.1916742 -4.18197 -4.1685829 -4.1550417 -4.1395249 -4.1279655 -4.1360765 -4.1535473 -4.1677113 -4.1763563][-4.3109627 -4.2959666 -4.2773666 -4.2583709 -4.2481289 -4.2456565 -4.2378211 -4.2264438 -4.207334 -4.1820602 -4.159143 -4.1524224 -4.1583915 -4.1634703 -4.1659722][-4.3197894 -4.3123941 -4.3025031 -4.2931437 -4.2872577 -4.2848568 -4.2777104 -4.2658954 -4.2428746 -4.2106695 -4.1812973 -4.1637983 -4.1585927 -4.155273 -4.1545424][-4.3225493 -4.3197937 -4.3154049 -4.3114734 -4.3092527 -4.309257 -4.3050246 -4.2926931 -4.2685485 -4.2344651 -4.1992493 -4.1720486 -4.1580591 -4.1493073 -4.1464086]]...]
INFO - root - 2017-12-08 00:01:58.657756: step 69510, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.698 sec/batch; 51h:01m:30s remains)
INFO - root - 2017-12-08 00:02:05.502623: step 69520, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 53h:56m:51s remains)
INFO - root - 2017-12-08 00:02:12.347328: step 69530, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 50h:18m:34s remains)
INFO - root - 2017-12-08 00:02:19.202123: step 69540, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 48h:48m:24s remains)
INFO - root - 2017-12-08 00:02:25.984925: step 69550, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 50h:38m:48s remains)
INFO - root - 2017-12-08 00:02:32.857490: step 69560, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 51h:42m:28s remains)
INFO - root - 2017-12-08 00:02:39.701023: step 69570, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.675 sec/batch; 49h:16m:48s remains)
INFO - root - 2017-12-08 00:02:46.523847: step 69580, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 48h:29m:34s remains)
INFO - root - 2017-12-08 00:02:53.405631: step 69590, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 49h:21m:32s remains)
INFO - root - 2017-12-08 00:03:00.198904: step 69600, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 50h:11m:50s remains)
2017-12-08 00:03:00.967833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3116574 -4.3139787 -4.3163648 -4.3138986 -4.3062406 -4.2966805 -4.2928953 -4.2927418 -4.29665 -4.3012624 -4.3054781 -4.3093691 -4.30963 -4.2998343 -4.2811489][-4.3274689 -4.3295264 -4.3299737 -4.3241835 -4.312942 -4.2976475 -4.2868547 -4.282783 -4.2859821 -4.2918921 -4.29514 -4.2983551 -4.29805 -4.2869692 -4.2612567][-4.3428111 -4.3407025 -4.3351412 -4.3247304 -4.3097196 -4.2882481 -4.2693167 -4.259161 -4.2615304 -4.2696228 -4.2735996 -4.2762356 -4.2775521 -4.2641788 -4.2306161][-4.3497796 -4.340982 -4.3270764 -4.3088336 -4.2839093 -4.2525425 -4.2234745 -4.205584 -4.2076235 -4.2230668 -4.2365937 -4.2490616 -4.2550654 -4.2401805 -4.2020631][-4.3506823 -4.3345542 -4.310339 -4.2795286 -4.2416739 -4.1947441 -4.1500463 -4.1251559 -4.1305642 -4.1576738 -4.1878223 -4.2161984 -4.2308807 -4.2202339 -4.1869621][-4.3430672 -4.3208094 -4.2851095 -4.2419996 -4.1897025 -4.1284685 -4.0673547 -4.0305753 -4.0418649 -4.0819883 -4.126513 -4.1705127 -4.2009153 -4.20563 -4.1897268][-4.3276544 -4.2988219 -4.2506208 -4.1952529 -4.134665 -4.0687842 -4.003912 -3.9620993 -3.9725018 -4.0162363 -4.0701303 -4.1263657 -4.17378 -4.199821 -4.2086639][-4.3147168 -4.2800107 -4.2219095 -4.1580033 -4.0970631 -4.0399466 -3.9853177 -3.9513843 -3.9578383 -3.9937408 -4.0479808 -4.1122785 -4.1720743 -4.2134132 -4.2380819][-4.3191872 -4.2845278 -4.22438 -4.1609473 -4.1050916 -4.0622654 -4.0244789 -4.003469 -4.0064688 -4.031158 -4.0762768 -4.1356096 -4.1950345 -4.2402186 -4.2701616][-4.3327851 -4.3061647 -4.2586012 -4.2070913 -4.1637321 -4.134654 -4.1115975 -4.0988617 -4.1006808 -4.1169958 -4.1480722 -4.1916 -4.23946 -4.2776546 -4.3041754][-4.34548 -4.3298726 -4.2994928 -4.2672324 -4.2398858 -4.22423 -4.2109838 -4.2020068 -4.2023249 -4.2150354 -4.23337 -4.2597423 -4.2910123 -4.3159122 -4.3330493][-4.3560195 -4.3481894 -4.3324227 -4.3141966 -4.2982054 -4.2895136 -4.2802429 -4.2733092 -4.2754159 -4.284852 -4.2946014 -4.3077416 -4.3270254 -4.3419 -4.3516955][-4.3639717 -4.361053 -4.3546329 -4.3460259 -4.3370829 -4.3294163 -4.3200655 -4.3143053 -4.3153214 -4.31856 -4.3213196 -4.3284569 -4.3433056 -4.3548331 -4.3603921][-4.3658972 -4.3661404 -4.3653717 -4.3632975 -4.3579364 -4.3505359 -4.3434162 -4.3395038 -4.3393283 -4.3380871 -4.3368092 -4.34062 -4.3511562 -4.358027 -4.3602633][-4.360857 -4.3610725 -4.3617907 -4.3614855 -4.3580875 -4.3533878 -4.3501067 -4.3489103 -4.3489685 -4.347342 -4.3466516 -4.3494844 -4.3552074 -4.3569336 -4.3571715]]...]
INFO - root - 2017-12-08 00:03:07.663352: step 69610, loss = 2.08, batch loss = 2.03 (12.3 examples/sec; 0.649 sec/batch; 47h:23m:29s remains)
INFO - root - 2017-12-08 00:03:14.524584: step 69620, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 50h:39m:34s remains)
INFO - root - 2017-12-08 00:03:21.400841: step 69630, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 54h:23m:30s remains)
INFO - root - 2017-12-08 00:03:28.205981: step 69640, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.712 sec/batch; 51h:57m:51s remains)
INFO - root - 2017-12-08 00:03:34.977256: step 69650, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.631 sec/batch; 46h:05m:55s remains)
INFO - root - 2017-12-08 00:03:41.841934: step 69660, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 47h:10m:37s remains)
INFO - root - 2017-12-08 00:03:48.636609: step 69670, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 50h:32m:08s remains)
INFO - root - 2017-12-08 00:03:55.483117: step 69680, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 53h:33m:51s remains)
INFO - root - 2017-12-08 00:04:02.232245: step 69690, loss = 2.04, batch loss = 1.98 (12.1 examples/sec; 0.663 sec/batch; 48h:23m:49s remains)
INFO - root - 2017-12-08 00:04:08.922297: step 69700, loss = 2.07, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 46h:17m:50s remains)
2017-12-08 00:04:09.639188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1230907 -4.1157603 -4.1172967 -4.1137195 -4.1061683 -4.0975752 -4.0937634 -4.0982671 -4.1167021 -4.1403441 -4.1643996 -4.1809597 -4.1968908 -4.2087555 -4.2006264][-4.1094375 -4.1080737 -4.1205935 -4.1251535 -4.11762 -4.1024513 -4.0872135 -4.0817232 -4.0940604 -4.1149454 -4.1391969 -4.1569595 -4.1740389 -4.1936264 -4.1960015][-4.101913 -4.1060081 -4.1303658 -4.1467333 -4.1434436 -4.1225462 -4.0957575 -4.0789475 -4.0766096 -4.0838 -4.1013312 -4.1195407 -4.1377239 -4.16241 -4.1764617][-4.10433 -4.1096306 -4.1382189 -4.1614952 -4.1579151 -4.1280403 -4.0894475 -4.05998 -4.0440969 -4.0414209 -4.054873 -4.0797548 -4.1034665 -4.1308489 -4.15313][-4.1187282 -4.1189551 -4.1384411 -4.1572275 -4.1445036 -4.1049004 -4.057126 -4.0182619 -3.9977863 -4.0002217 -4.0209055 -4.05589 -4.0843563 -4.1086454 -4.1336484][-4.1291 -4.1209903 -4.1254177 -4.1343732 -4.1130738 -4.0663323 -4.0132241 -3.9711473 -3.9583235 -3.9801471 -4.0175247 -4.0623469 -4.0903072 -4.1064954 -4.1268024][-4.14389 -4.1327543 -4.1261139 -4.1234083 -4.0943885 -4.04659 -3.9928803 -3.9537253 -3.9537113 -3.9943218 -4.0414238 -4.0865388 -4.1097908 -4.1185932 -4.134666][-4.17214 -4.1594796 -4.145154 -4.1354046 -4.104948 -4.0653 -4.0196843 -3.987505 -3.9978089 -4.0455542 -4.0889435 -4.1205187 -4.1346488 -4.1400042 -4.1522436][-4.1978059 -4.1852865 -4.1704459 -4.1634588 -4.140554 -4.1088028 -4.0758286 -4.0545621 -4.0703616 -4.1155496 -4.1462975 -4.1548786 -4.1540565 -4.1524525 -4.1560321][-4.2062283 -4.1979222 -4.1933765 -4.1962814 -4.18292 -4.157505 -4.1361785 -4.1239643 -4.139473 -4.1782327 -4.1976395 -4.1871037 -4.1679006 -4.1504021 -4.1394215][-4.1974916 -4.1950989 -4.20398 -4.218914 -4.2128005 -4.1945128 -4.1846619 -4.179378 -4.1933107 -4.224525 -4.2331834 -4.2111983 -4.1762652 -4.1439495 -4.1207938][-4.1811938 -4.1789947 -4.1954341 -4.2157016 -4.2148461 -4.2048869 -4.2034583 -4.2042966 -4.2212906 -4.249486 -4.2535648 -4.2282834 -4.190486 -4.1525722 -4.1215072][-4.1567249 -4.1488976 -4.1648068 -4.1828547 -4.1872773 -4.1860962 -4.1928406 -4.2003264 -4.2197008 -4.24888 -4.2545986 -4.2338948 -4.1999068 -4.1614642 -4.1297464][-4.1408606 -4.1276932 -4.1357317 -4.1449537 -4.1488261 -4.154747 -4.1678619 -4.1822672 -4.2081275 -4.2395587 -4.2483225 -4.2350483 -4.2080412 -4.1731625 -4.1454997][-4.1321335 -4.1220474 -4.1239014 -4.1216927 -4.1200247 -4.1254115 -4.1410131 -4.1616058 -4.1943417 -4.2251573 -4.2385325 -4.2339935 -4.2170758 -4.1921091 -4.1710367]]...]
INFO - root - 2017-12-08 00:04:16.317880: step 69710, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.746 sec/batch; 54h:27m:32s remains)
INFO - root - 2017-12-08 00:04:23.003481: step 69720, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 47h:16m:08s remains)
INFO - root - 2017-12-08 00:04:29.880283: step 69730, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 48h:43m:03s remains)
INFO - root - 2017-12-08 00:04:36.741867: step 69740, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.726 sec/batch; 52h:57m:47s remains)
INFO - root - 2017-12-08 00:04:43.682725: step 69750, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 51h:16m:32s remains)
INFO - root - 2017-12-08 00:04:50.471703: step 69760, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 48h:08m:02s remains)
INFO - root - 2017-12-08 00:04:57.268120: step 69770, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.648 sec/batch; 47h:18m:24s remains)
INFO - root - 2017-12-08 00:05:04.007676: step 69780, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.699 sec/batch; 51h:02m:11s remains)
INFO - root - 2017-12-08 00:05:10.810542: step 69790, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.681 sec/batch; 49h:40m:59s remains)
INFO - root - 2017-12-08 00:05:17.561003: step 69800, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 49h:28m:36s remains)
2017-12-08 00:05:18.331790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3044639 -4.2972956 -4.281743 -4.2588305 -4.235405 -4.2164259 -4.2062917 -4.2251267 -4.2670093 -4.2934871 -4.2847981 -4.269691 -4.2583642 -4.2450671 -4.229969][-4.3130689 -4.303371 -4.2848983 -4.2598228 -4.2336607 -4.213378 -4.2016973 -4.2225332 -4.2630706 -4.2828236 -4.2674236 -4.2498269 -4.2385216 -4.2227421 -4.1990986][-4.3191686 -4.3088236 -4.2910986 -4.2658086 -4.2360835 -4.2161689 -4.205565 -4.223855 -4.2566681 -4.2664137 -4.2447863 -4.2242427 -4.2136703 -4.1977324 -4.1716123][-4.3193645 -4.3102665 -4.2944093 -4.270215 -4.2399759 -4.2178321 -4.2021933 -4.210856 -4.2318344 -4.2345743 -4.214695 -4.197782 -4.1918597 -4.1794972 -4.1506891][-4.3111348 -4.3007421 -4.2850914 -4.2617974 -4.2287164 -4.1969495 -4.1623883 -4.1487885 -4.1641555 -4.1795664 -4.178896 -4.1801291 -4.1824269 -4.1762457 -4.1446452][-4.2997985 -4.2856512 -4.2663665 -4.2381878 -4.1952686 -4.1439972 -4.0766163 -4.0312953 -4.0488071 -4.0925159 -4.1225815 -4.1503954 -4.166791 -4.1674027 -4.1371646][-4.2927518 -4.2736378 -4.2479382 -4.2076349 -4.1468797 -4.0703974 -3.971534 -3.8960767 -3.9246376 -4.0111942 -4.0773759 -4.122396 -4.1421647 -4.1412907 -4.1203475][-4.2829633 -4.2513747 -4.2154646 -4.1691165 -4.1019959 -4.0198779 -3.9265103 -3.864078 -3.9119329 -4.0148244 -4.0864844 -4.1238475 -4.1317739 -4.1197205 -4.1048026][-4.2648468 -4.2194171 -4.1733365 -4.13036 -4.07595 -4.0135732 -3.9559264 -3.9281459 -3.9798105 -4.0649381 -4.1140127 -4.1269522 -4.1197629 -4.0979705 -4.0864353][-4.2555246 -4.2070441 -4.1600981 -4.1259174 -4.0937357 -4.0608444 -4.0363607 -4.0335336 -4.0694413 -4.1185007 -4.1391931 -4.1352644 -4.1215677 -4.0974269 -4.0863867][-4.2587337 -4.2185049 -4.180171 -4.1586804 -4.1470242 -4.1389365 -4.1323853 -4.1341505 -4.1474733 -4.167212 -4.1691809 -4.1548166 -4.1375318 -4.1147628 -4.104167][-4.276722 -4.2477617 -4.2230229 -4.2150774 -4.2171512 -4.2219763 -4.2199435 -4.2153444 -4.2142792 -4.216548 -4.21179 -4.1962819 -4.1777172 -4.1589208 -4.154089][-4.2995181 -4.2806849 -4.2686405 -4.2694826 -4.2778454 -4.2867012 -4.28461 -4.2761426 -4.2690392 -4.2612915 -4.2528749 -4.2421789 -4.2309513 -4.2226071 -4.224299][-4.31828 -4.3063745 -4.3027158 -4.3071904 -4.31501 -4.3206644 -4.3170128 -4.3083787 -4.298914 -4.2902465 -4.2848163 -4.2825894 -4.2822251 -4.2836256 -4.288497][-4.3296552 -4.3194208 -4.3151784 -4.3172212 -4.3200445 -4.3197427 -4.31463 -4.3080215 -4.3013325 -4.2959051 -4.2955761 -4.3005695 -4.3077717 -4.31551 -4.3204646]]...]
INFO - root - 2017-12-08 00:05:25.081135: step 69810, loss = 2.03, batch loss = 1.97 (11.6 examples/sec; 0.690 sec/batch; 50h:22m:54s remains)
INFO - root - 2017-12-08 00:05:31.947588: step 69820, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 50h:21m:09s remains)
INFO - root - 2017-12-08 00:05:38.669277: step 69830, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 49h:28m:57s remains)
INFO - root - 2017-12-08 00:05:45.364148: step 69840, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 46h:45m:47s remains)
INFO - root - 2017-12-08 00:05:52.135042: step 69850, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 47h:08m:08s remains)
INFO - root - 2017-12-08 00:05:58.879829: step 69860, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 50h:19m:58s remains)
INFO - root - 2017-12-08 00:06:05.637919: step 69870, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 50h:35m:07s remains)
INFO - root - 2017-12-08 00:06:12.423722: step 69880, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 51h:11m:26s remains)
INFO - root - 2017-12-08 00:06:19.215100: step 69890, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.653 sec/batch; 47h:38m:41s remains)
INFO - root - 2017-12-08 00:06:25.993437: step 69900, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 46h:25m:50s remains)
2017-12-08 00:06:26.706927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2860689 -4.2563925 -4.2212186 -4.1876421 -4.1740446 -4.184587 -4.2169838 -4.2494364 -4.258512 -4.2627215 -4.280623 -4.2942162 -4.2842484 -4.2526278 -4.2059188][-4.3290424 -4.3242106 -4.310708 -4.2859492 -4.2707067 -4.27142 -4.2872233 -4.3070717 -4.3117661 -4.3154335 -4.3272829 -4.3345385 -4.3188376 -4.27832 -4.2247019][-4.333674 -4.3454885 -4.3414612 -4.319469 -4.3002381 -4.2926707 -4.298533 -4.310812 -4.3202319 -4.3322496 -4.3504295 -4.3612127 -4.3505893 -4.3140297 -4.2617669][-4.3118968 -4.3345542 -4.3339763 -4.3096261 -4.2823181 -4.2663989 -4.2659116 -4.2749524 -4.290771 -4.3151975 -4.3423872 -4.3596911 -4.3597183 -4.337923 -4.2982545][-4.2628064 -4.2922592 -4.2883062 -4.2535152 -4.2137737 -4.1860371 -4.1791472 -4.1886334 -4.2157445 -4.2568388 -4.2985415 -4.3279414 -4.3422751 -4.3380671 -4.3155074][-4.2109752 -4.2445717 -4.2328067 -4.1764321 -4.1072731 -4.0509992 -4.0232873 -4.0175724 -4.0563054 -4.1308517 -4.20351 -4.2564335 -4.2931652 -4.3111839 -4.308497][-4.1866822 -4.2170839 -4.19641 -4.1209278 -4.0229788 -3.9253514 -3.8464618 -3.7938223 -3.8319783 -3.9512177 -4.070219 -4.156136 -4.2184825 -4.2613282 -4.2772217][-4.2062168 -4.2289705 -4.2061625 -4.1298141 -4.024519 -3.905231 -3.7842803 -3.6711321 -3.6808047 -3.8179369 -3.9660597 -4.0759573 -4.1556425 -4.2143526 -4.2431865][-4.2387 -4.2625513 -4.2517109 -4.1949797 -4.1089115 -4.0061264 -3.899991 -3.7932315 -3.7715106 -3.8619683 -3.9819574 -4.0759292 -4.1455288 -4.1996202 -4.2301388][-4.2799935 -4.3049192 -4.3071737 -4.2759404 -4.216856 -4.1434731 -4.0697355 -4.0000634 -3.9786181 -4.0245609 -4.0989046 -4.1597533 -4.2023029 -4.2362289 -4.2568479][-4.3180408 -4.3403549 -4.3470235 -4.3350449 -4.3042068 -4.2627511 -4.2203412 -4.1822987 -4.1692419 -4.1895213 -4.2277703 -4.2592955 -4.2780952 -4.2924881 -4.3019462][-4.340764 -4.355608 -4.3606091 -4.3586826 -4.34892 -4.333344 -4.3150058 -4.2983 -4.2909751 -4.2969236 -4.3117156 -4.3235393 -4.3285708 -4.331284 -4.3337994][-4.3525281 -4.3597436 -4.3604641 -4.3590717 -4.3571057 -4.3548608 -4.3515096 -4.3489871 -4.34872 -4.3510847 -4.353683 -4.35347 -4.3511181 -4.35039 -4.3498759][-4.3565044 -4.3591709 -4.3586516 -4.3566957 -4.354671 -4.3539777 -4.355103 -4.3579621 -4.3603778 -4.361227 -4.3593836 -4.3562412 -4.3528252 -4.3520265 -4.3506956][-4.3567719 -4.3578844 -4.3578863 -4.3569789 -4.3554029 -4.3541646 -4.3539691 -4.3556323 -4.3570948 -4.3571291 -4.3551412 -4.3534594 -4.3521533 -4.3521256 -4.3513188]]...]
INFO - root - 2017-12-08 00:06:33.222776: step 69910, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 50h:24m:17s remains)
INFO - root - 2017-12-08 00:06:40.074485: step 69920, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 51h:27m:12s remains)
INFO - root - 2017-12-08 00:06:46.827290: step 69930, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 47h:38m:17s remains)
INFO - root - 2017-12-08 00:06:53.619838: step 69940, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 48h:22m:26s remains)
INFO - root - 2017-12-08 00:07:00.560348: step 69950, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.741 sec/batch; 54h:00m:50s remains)
INFO - root - 2017-12-08 00:07:07.404646: step 69960, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 51h:48m:07s remains)
INFO - root - 2017-12-08 00:07:14.166208: step 69970, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 47h:52m:23s remains)
INFO - root - 2017-12-08 00:07:21.114865: step 69980, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 49h:55m:58s remains)
INFO - root - 2017-12-08 00:07:27.991193: step 69990, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 50h:32m:27s remains)
INFO - root - 2017-12-08 00:07:34.755078: step 70000, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 48h:02m:45s remains)
2017-12-08 00:07:35.440383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3264232 -4.3245211 -4.3223362 -4.3211408 -4.3214526 -4.3220854 -4.32288 -4.3231249 -4.3232093 -4.3235087 -4.3240628 -4.3242764 -4.3236012 -4.3225756 -4.3212519][-4.3260212 -4.323112 -4.3188839 -4.3161488 -4.315733 -4.3169003 -4.3185959 -4.3196654 -4.3207054 -4.321362 -4.322535 -4.3236384 -4.3235769 -4.3227572 -4.3212395][-4.3240361 -4.3186393 -4.3120189 -4.3068924 -4.304841 -4.3060741 -4.3085155 -4.3097429 -4.3113303 -4.3131995 -4.3155794 -4.3177133 -4.31938 -4.320858 -4.320869][-4.31455 -4.3038325 -4.2924986 -4.2820492 -4.2748585 -4.271472 -4.2724557 -4.2749434 -4.2798424 -4.2858539 -4.2912602 -4.295506 -4.3008518 -4.3081303 -4.3139396][-4.2956157 -4.2770534 -4.2574182 -4.2377424 -4.2199645 -4.205636 -4.199429 -4.2024317 -4.2140732 -4.2264771 -4.2354012 -4.243916 -4.2571917 -4.2741942 -4.2901406][-4.2650962 -4.2350264 -4.2042184 -4.1698036 -4.1307788 -4.09321 -4.0745344 -4.0829816 -4.1090441 -4.1348863 -4.1518712 -4.167222 -4.190937 -4.2211533 -4.2497635][-4.2328444 -4.194768 -4.1538219 -4.100812 -4.0313206 -3.9586315 -3.9232273 -3.9428706 -3.9909253 -4.0362453 -4.0652628 -4.0864234 -4.1168733 -4.158608 -4.1995182][-4.2009411 -4.1608138 -4.1139112 -4.0495872 -3.9600017 -3.8577843 -3.8026981 -3.8359337 -3.9109581 -3.9792573 -4.0226536 -4.0484834 -4.0774055 -4.117229 -4.1577969][-4.1814489 -4.1506038 -4.1105032 -4.0541458 -3.9759398 -3.8854761 -3.833914 -3.8644218 -3.9407375 -4.0116768 -4.0559268 -4.0809326 -4.1009879 -4.124753 -4.1504188][-4.1904383 -4.1797891 -4.1556153 -4.1175308 -4.0707912 -4.0235548 -3.9992552 -4.0163488 -4.0607944 -4.1046958 -4.1309609 -4.1444016 -4.1507659 -4.1564059 -4.1644411][-4.2178917 -4.2235107 -4.2141156 -4.1927958 -4.1699939 -4.1513166 -4.1438904 -4.1519227 -4.1694608 -4.1867766 -4.1947832 -4.1948743 -4.1874275 -4.1787658 -4.1756296][-4.233552 -4.2473497 -4.2473283 -4.2388082 -4.2315855 -4.2253094 -4.2227006 -4.2231927 -4.2273273 -4.2319574 -4.2302742 -4.2216635 -4.2050595 -4.1870618 -4.1781592][-4.2323322 -4.2481117 -4.2511253 -4.2505903 -4.251739 -4.2517347 -4.2492294 -4.246686 -4.2476611 -4.2499652 -4.2478929 -4.2387576 -4.22055 -4.1997714 -4.1883497][-4.2256637 -4.2325759 -4.2289286 -4.2253485 -4.22718 -4.2308397 -4.2333035 -4.2356405 -4.2400994 -4.2451706 -4.2461033 -4.2398458 -4.2266097 -4.2115369 -4.20189][-4.2307439 -4.2267108 -4.2127709 -4.20053 -4.195416 -4.1969223 -4.204586 -4.2153244 -4.2274103 -4.2387266 -4.2443409 -4.2434893 -4.2378187 -4.2311273 -4.225481]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 00:07:42.635564: step 70010, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.687 sec/batch; 50h:04m:01s remains)
INFO - root - 2017-12-08 00:07:49.321320: step 70020, loss = 2.06, batch loss = 2.00 (15.6 examples/sec; 0.513 sec/batch; 37h:23m:38s remains)
INFO - root - 2017-12-08 00:07:56.083900: step 70030, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 48h:52m:03s remains)
INFO - root - 2017-12-08 00:08:02.923198: step 70040, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 51h:44m:41s remains)
INFO - root - 2017-12-08 00:08:09.739006: step 70050, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.685 sec/batch; 49h:56m:09s remains)
INFO - root - 2017-12-08 00:08:16.588885: step 70060, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 50h:03m:04s remains)
INFO - root - 2017-12-08 00:08:23.346361: step 70070, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 48h:10m:00s remains)
INFO - root - 2017-12-08 00:08:30.179099: step 70080, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 48h:50m:47s remains)
INFO - root - 2017-12-08 00:08:37.007775: step 70090, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 50h:52m:04s remains)
INFO - root - 2017-12-08 00:08:43.814695: step 70100, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 48h:52m:20s remains)
2017-12-08 00:08:44.502702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1533809 -4.1473913 -4.1345963 -4.1294403 -4.1510506 -4.1751423 -4.1912713 -4.1935482 -4.1689353 -4.1189036 -4.0678587 -4.0528989 -4.0883303 -4.1386318 -4.173974][-4.139668 -4.13301 -4.125854 -4.1301312 -4.1609735 -4.1925859 -4.2028193 -4.1989374 -4.1790757 -4.1442747 -4.1130519 -4.113472 -4.1501627 -4.1868472 -4.2097][-4.1416759 -4.1377726 -4.1377974 -4.1483212 -4.1805964 -4.210052 -4.2131166 -4.1975522 -4.1774445 -4.1565652 -4.142643 -4.1523976 -4.1865439 -4.2166162 -4.2315168][-4.150188 -4.1546464 -4.16525 -4.1737227 -4.1911902 -4.2038035 -4.1953907 -4.1684651 -4.1439147 -4.1292262 -4.1262374 -4.1383548 -4.1701756 -4.2007408 -4.2197762][-4.1580858 -4.1707563 -4.1823459 -4.1794004 -4.1810312 -4.18044 -4.1632957 -4.1228681 -4.0902433 -4.0808082 -4.084115 -4.0936561 -4.121182 -4.1573992 -4.1884551][-4.1620436 -4.1694913 -4.1719527 -4.1551518 -4.1414471 -4.13216 -4.10737 -4.0560422 -4.0136175 -4.0089607 -4.02316 -4.0383196 -4.0709877 -4.1197157 -4.1670337][-4.1796103 -4.16735 -4.1537986 -4.12668 -4.0934296 -4.0662193 -4.0245848 -3.9593089 -3.9116442 -3.9186325 -3.9524255 -3.986155 -4.0369372 -4.1009688 -4.1634765][-4.2107344 -4.182374 -4.1638103 -4.1403155 -4.1031418 -4.0608239 -4.0053935 -3.9404535 -3.9025705 -3.92168 -3.9627266 -4.0014634 -4.0547585 -4.11561 -4.176909][-4.228642 -4.2016268 -4.1900525 -4.1823931 -4.1582832 -4.1205359 -4.070713 -4.0265374 -4.0046034 -4.020081 -4.0489779 -4.077559 -4.11584 -4.1603332 -4.2063966][-4.2284951 -4.2153125 -4.2190433 -4.2246366 -4.2183867 -4.193357 -4.1581893 -4.1399503 -4.1331081 -4.1368947 -4.1436357 -4.1503315 -4.1700053 -4.1985211 -4.2296653][-4.2099161 -4.2095318 -4.2264738 -4.2458959 -4.2580805 -4.2491536 -4.232029 -4.2291017 -4.2292857 -4.2233453 -4.21076 -4.1978679 -4.1992831 -4.2133946 -4.23322][-4.2062383 -4.2160578 -4.240231 -4.2629328 -4.2784753 -4.2779589 -4.2711787 -4.275497 -4.280158 -4.2702541 -4.247848 -4.2252789 -4.2164245 -4.2225175 -4.2371073][-4.2385287 -4.2495041 -4.2709422 -4.2883129 -4.2991605 -4.2995486 -4.2965126 -4.2984695 -4.3005915 -4.291019 -4.2700591 -4.2497516 -4.241971 -4.2466688 -4.258194][-4.2738905 -4.281517 -4.2956181 -4.3068666 -4.3120613 -4.3102269 -4.3064008 -4.3045316 -4.3032393 -4.2960472 -4.2840405 -4.2741385 -4.27421 -4.2818255 -4.2924542][-4.2967343 -4.3022456 -4.3099508 -4.3160367 -4.3182993 -4.3161211 -4.3118253 -4.3092136 -4.3075929 -4.3028655 -4.2975106 -4.2958283 -4.3012495 -4.3099141 -4.3173437]]...]
INFO - root - 2017-12-08 00:08:51.119220: step 70110, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 50h:14m:55s remains)
INFO - root - 2017-12-08 00:08:57.928462: step 70120, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 51h:51m:34s remains)
INFO - root - 2017-12-08 00:09:04.638399: step 70130, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 48h:20m:34s remains)
INFO - root - 2017-12-08 00:09:11.408364: step 70140, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 45h:34m:51s remains)
INFO - root - 2017-12-08 00:09:18.113480: step 70150, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 47h:18m:17s remains)
INFO - root - 2017-12-08 00:09:25.016735: step 70160, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 48h:15m:43s remains)
INFO - root - 2017-12-08 00:09:31.926254: step 70170, loss = 2.08, batch loss = 2.03 (11.7 examples/sec; 0.685 sec/batch; 49h:57m:01s remains)
INFO - root - 2017-12-08 00:09:38.776734: step 70180, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 49h:26m:58s remains)
INFO - root - 2017-12-08 00:09:45.660006: step 70190, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 47h:22m:29s remains)
INFO - root - 2017-12-08 00:09:52.419824: step 70200, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 47h:19m:28s remains)
2017-12-08 00:09:53.156064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2080059 -4.2126493 -4.2083659 -4.2057476 -4.2128053 -4.2139187 -4.2134848 -4.1966004 -4.160697 -4.1398716 -4.1459184 -4.178957 -4.2132626 -4.2240767 -4.2175679][-4.2401896 -4.2394404 -4.2376442 -4.2420254 -4.251884 -4.2532086 -4.2557993 -4.2465873 -4.2167554 -4.1963348 -4.197638 -4.2209048 -4.246635 -4.2553263 -4.2495742][-4.2435946 -4.2300057 -4.2242694 -4.2358289 -4.2543678 -4.262589 -4.2651982 -4.2609687 -4.2360845 -4.2140093 -4.2124496 -4.2275195 -4.2464685 -4.2574892 -4.2518034][-4.2204685 -4.2012858 -4.1921749 -4.2015624 -4.2219152 -4.2347307 -4.2378454 -4.2305369 -4.2053556 -4.1829963 -4.1832666 -4.1963482 -4.2149119 -4.2342091 -4.2333827][-4.20731 -4.1873207 -4.1703296 -4.1660314 -4.174159 -4.1808686 -4.1806383 -4.1703258 -4.1439157 -4.1204863 -4.1183686 -4.1304154 -4.1604104 -4.1924686 -4.2052832][-4.2128243 -4.1960573 -4.1777234 -4.1631007 -4.1542416 -4.1462684 -4.137094 -4.1233616 -4.1007271 -4.0717397 -4.0530424 -4.0590825 -4.1043487 -4.1534572 -4.1825881][-4.2174263 -4.2043381 -4.187089 -4.167953 -4.1519117 -4.1362996 -4.121563 -4.1137834 -4.1059289 -4.0802016 -4.0488739 -4.0359211 -4.068254 -4.1147342 -4.1568532][-4.2235374 -4.2126856 -4.1990337 -4.1805463 -4.1606612 -4.1410122 -4.1270733 -4.1295662 -4.135982 -4.1199012 -4.0832682 -4.0523882 -4.0600834 -4.0926881 -4.1426783][-4.2286215 -4.225996 -4.2255378 -4.2169218 -4.1987176 -4.1781926 -4.1665039 -4.1730142 -4.1806822 -4.1671367 -4.1356058 -4.1007028 -4.0904083 -4.1067591 -4.1525888][-4.24112 -4.249104 -4.2595325 -4.2598419 -4.2474217 -4.233798 -4.2267694 -4.22739 -4.229054 -4.2216449 -4.2006311 -4.1660013 -4.1422997 -4.1441512 -4.1814556][-4.2579551 -4.2695045 -4.2849264 -4.2910008 -4.2820511 -4.2717991 -4.2691054 -4.2702909 -4.2729154 -4.2707925 -4.257041 -4.2257719 -4.1928716 -4.1774979 -4.1957812][-4.2574191 -4.2631764 -4.2742357 -4.2816982 -4.2775183 -4.2744751 -4.2831669 -4.29549 -4.3078713 -4.3103786 -4.2963271 -4.26464 -4.22331 -4.1890078 -4.1811876][-4.2430115 -4.234973 -4.2323112 -4.2308087 -4.2285886 -4.2354507 -4.2586956 -4.2871976 -4.31022 -4.3137169 -4.2986 -4.2709665 -4.2311254 -4.1944551 -4.1769252][-4.2314525 -4.2090654 -4.1847253 -4.1609688 -4.1505547 -4.1651874 -4.2062097 -4.2535653 -4.2869649 -4.2897515 -4.2739053 -4.2519231 -4.2218885 -4.1970477 -4.1882982][-4.2422633 -4.221755 -4.18586 -4.1409383 -4.11079 -4.1151443 -4.1557765 -4.2094555 -4.2485561 -4.2524304 -4.2392473 -4.2245321 -4.2070956 -4.1928968 -4.1910009]]...]
INFO - root - 2017-12-08 00:09:59.832250: step 70210, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 47h:54m:12s remains)
INFO - root - 2017-12-08 00:10:06.665890: step 70220, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:48m:30s remains)
INFO - root - 2017-12-08 00:10:13.452242: step 70230, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 51h:11m:41s remains)
INFO - root - 2017-12-08 00:10:20.214448: step 70240, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.681 sec/batch; 49h:38m:05s remains)
INFO - root - 2017-12-08 00:10:26.966418: step 70250, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.668 sec/batch; 48h:37m:39s remains)
INFO - root - 2017-12-08 00:10:33.757045: step 70260, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 49h:18m:53s remains)
INFO - root - 2017-12-08 00:10:40.589174: step 70270, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 49h:55m:58s remains)
INFO - root - 2017-12-08 00:10:47.361221: step 70280, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.719 sec/batch; 52h:23m:14s remains)
INFO - root - 2017-12-08 00:10:54.078754: step 70290, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 48h:16m:55s remains)
INFO - root - 2017-12-08 00:11:00.900753: step 70300, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 50h:49m:43s remains)
2017-12-08 00:11:01.646117: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0674329 -4.04909 -4.0915318 -4.1491642 -4.1812592 -4.1883922 -4.2037005 -4.2370329 -4.2778335 -4.3134985 -4.3323827 -4.3356261 -4.3289003 -4.3199019 -4.3154244][-4.0792265 -4.0698142 -4.1110959 -4.164546 -4.1922579 -4.1945934 -4.2035437 -4.2298422 -4.2652073 -4.2962141 -4.3121505 -4.3148866 -4.30909 -4.302268 -4.3003993][-4.1300473 -4.1362815 -4.1711497 -4.2061095 -4.2180524 -4.2079282 -4.2046933 -4.2199526 -4.247365 -4.2708735 -4.2818537 -4.2839007 -4.2803168 -4.2785954 -4.2818007][-4.1999059 -4.2186956 -4.24392 -4.2558994 -4.2427058 -4.2124147 -4.1912103 -4.1952991 -4.2177525 -4.2382326 -4.2508044 -4.2581716 -4.2609777 -4.2673421 -4.2776051][-4.2483897 -4.2662644 -4.2789946 -4.2757359 -4.2447934 -4.1989784 -4.1595106 -4.1506147 -4.1709981 -4.1960063 -4.2179446 -4.2355981 -4.2459207 -4.2605162 -4.277843][-4.2640581 -4.2740836 -4.2747803 -4.2572217 -4.2161083 -4.1598768 -4.1052094 -4.086555 -4.1103516 -4.1482139 -4.1852832 -4.215301 -4.2317529 -4.2517385 -4.273829][-4.2703781 -4.2624588 -4.2439923 -4.2126732 -4.167171 -4.1091275 -4.0487 -4.0220628 -4.0506511 -4.1061349 -4.1624012 -4.2057118 -4.2266269 -4.2470202 -4.2688823][-4.2804918 -4.2498474 -4.2068567 -4.1592741 -4.1149435 -4.0676641 -4.0183692 -3.9913728 -4.0173049 -4.08209 -4.1553221 -4.2095675 -4.2336621 -4.2520771 -4.2695851][-4.2868538 -4.2466927 -4.1944733 -4.1406846 -4.1031504 -4.0732059 -4.0426197 -4.0224061 -4.0432253 -4.1026578 -4.1783924 -4.2338367 -4.2576637 -4.271678 -4.2811446][-4.2996154 -4.2669492 -4.2221346 -4.1745343 -4.145112 -4.1316748 -4.1202812 -4.1098709 -4.1274686 -4.1743956 -4.2346063 -4.278451 -4.2962255 -4.302319 -4.3032088][-4.3181787 -4.3008051 -4.2717347 -4.2395811 -4.220922 -4.2191782 -4.2215424 -4.2188873 -4.2305679 -4.2608275 -4.2997961 -4.3284073 -4.3393745 -4.3377972 -4.3297963][-4.330821 -4.3282685 -4.3126044 -4.2924876 -4.2829041 -4.2888923 -4.2997551 -4.3044615 -4.312191 -4.3274956 -4.3484392 -4.3648987 -4.370491 -4.3647866 -4.349309][-4.3309464 -4.338161 -4.3350897 -4.3259454 -4.3228126 -4.3306069 -4.3412724 -4.3478093 -4.3523679 -4.3586226 -4.3701148 -4.380765 -4.3843331 -4.3776693 -4.3609004][-4.3251019 -4.3329754 -4.3362021 -4.3346539 -4.3359075 -4.3447413 -4.3534517 -4.3579354 -4.359839 -4.3615222 -4.367907 -4.3759584 -4.3798218 -4.376802 -4.3644924][-4.32373 -4.3263288 -4.3281012 -4.3286247 -4.3305511 -4.3363004 -4.3412738 -4.3430371 -4.3442864 -4.346065 -4.3509068 -4.3583531 -4.3640361 -4.3646493 -4.3590078]]...]
INFO - root - 2017-12-08 00:11:08.050708: step 70310, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 52h:34m:52s remains)
INFO - root - 2017-12-08 00:11:14.894429: step 70320, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 50h:15m:59s remains)
INFO - root - 2017-12-08 00:11:21.509356: step 70330, loss = 2.09, batch loss = 2.03 (15.3 examples/sec; 0.524 sec/batch; 38h:08m:05s remains)
INFO - root - 2017-12-08 00:11:28.455331: step 70340, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 51h:39m:52s remains)
INFO - root - 2017-12-08 00:11:35.356803: step 70350, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 51h:39m:11s remains)
INFO - root - 2017-12-08 00:11:42.107056: step 70360, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 47h:57m:11s remains)
INFO - root - 2017-12-08 00:11:48.903554: step 70370, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 46h:02m:23s remains)
INFO - root - 2017-12-08 00:11:55.803668: step 70380, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.676 sec/batch; 49h:15m:20s remains)
INFO - root - 2017-12-08 00:12:02.568984: step 70390, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 51h:15m:25s remains)
INFO - root - 2017-12-08 00:12:09.345832: step 70400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 51h:16m:12s remains)
2017-12-08 00:12:10.043244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3103213 -4.302094 -4.3017578 -4.3111553 -4.3217897 -4.3286352 -4.3280659 -4.3244076 -4.317986 -4.3164973 -4.3238597 -4.3319654 -4.3376255 -4.3431654 -4.34631][-4.2930632 -4.2834554 -4.2826557 -4.2905426 -4.2982712 -4.2995543 -4.2922583 -4.2844143 -4.2742882 -4.2740526 -4.2897444 -4.3036304 -4.3122334 -4.3167477 -4.3156409][-4.2767963 -4.2630644 -4.259552 -4.2639008 -4.2660847 -4.2572379 -4.2388468 -4.2232137 -4.207725 -4.2098112 -4.2365894 -4.2594008 -4.2719259 -4.2742186 -4.2646708][-4.2635751 -4.2444615 -4.2356129 -4.2346816 -4.2299352 -4.2127018 -4.1872697 -4.16595 -4.1523471 -4.1623816 -4.1960211 -4.2184415 -4.2292705 -4.2273574 -4.2076569][-4.248342 -4.2262678 -4.2145953 -4.2096076 -4.197578 -4.172852 -4.1398969 -4.1092744 -4.1019983 -4.1272345 -4.16493 -4.1863384 -4.1973329 -4.1905341 -4.1645927][-4.238121 -4.217979 -4.2067423 -4.1980782 -4.1714106 -4.1258984 -4.0678558 -4.0228143 -4.0360975 -4.0891337 -4.1370673 -4.1618147 -4.1763163 -4.169704 -4.1515703][-4.2419262 -4.2243457 -4.2127137 -4.19718 -4.1508646 -4.0757413 -3.9783092 -3.9073975 -3.9505568 -4.0401325 -4.0986214 -4.1258364 -4.1416268 -4.1367412 -4.1341972][-4.2505546 -4.2364664 -4.2255859 -4.204392 -4.14579 -4.051672 -3.9223099 -3.8217969 -3.8875053 -4.001143 -4.0603294 -4.0803027 -4.0864644 -4.0827117 -4.0977073][-4.2532167 -4.2435389 -4.2387285 -4.2240047 -4.17599 -4.0957446 -3.98762 -3.9055543 -3.9494898 -4.0307827 -4.0683432 -4.0672812 -4.0564623 -4.0570197 -4.0853238][-4.2519517 -4.2482224 -4.2504964 -4.2467074 -4.2181497 -4.1639614 -4.0945563 -4.0420556 -4.064764 -4.1085606 -4.1234932 -4.1085663 -4.0912642 -4.0974474 -4.128201][-4.2392421 -4.2421403 -4.2529411 -4.2570758 -4.2407951 -4.2074623 -4.1642628 -4.1323819 -4.1446595 -4.1706486 -4.1777725 -4.1607947 -4.1464367 -4.1572266 -4.187819][-4.2190523 -4.2278419 -4.2435508 -4.2541 -4.2473106 -4.2282395 -4.2046165 -4.1907516 -4.1993217 -4.2157192 -4.21623 -4.1968756 -4.1831989 -4.1931143 -4.2198348][-4.2033157 -4.2160726 -4.2326355 -4.2476683 -4.2511992 -4.2440014 -4.23311 -4.2301593 -4.2402916 -4.2514095 -4.2496958 -4.2334146 -4.2212262 -4.2274761 -4.2493148][-4.2052531 -4.2181363 -4.2296481 -4.24501 -4.2537231 -4.2530584 -4.2491589 -4.2518678 -4.2627544 -4.2768288 -4.2817788 -4.2756982 -4.2680879 -4.2704549 -4.2853036][-4.219543 -4.2306905 -4.2355814 -4.2457838 -4.2566328 -4.2595911 -4.2565012 -4.2550044 -4.260757 -4.2782168 -4.2954946 -4.3034344 -4.3044538 -4.3079672 -4.3164721]]...]
INFO - root - 2017-12-08 00:12:16.758890: step 70410, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.708 sec/batch; 51h:30m:55s remains)
INFO - root - 2017-12-08 00:12:23.611921: step 70420, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.674 sec/batch; 49h:05m:02s remains)
INFO - root - 2017-12-08 00:12:30.436063: step 70430, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 49h:06m:28s remains)
INFO - root - 2017-12-08 00:12:37.152266: step 70440, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 47h:58m:01s remains)
INFO - root - 2017-12-08 00:12:43.824601: step 70450, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 46h:15m:33s remains)
INFO - root - 2017-12-08 00:12:50.591523: step 70460, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 48h:15m:01s remains)
INFO - root - 2017-12-08 00:12:57.377760: step 70470, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.705 sec/batch; 51h:19m:03s remains)
INFO - root - 2017-12-08 00:13:04.368466: step 70480, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.776 sec/batch; 56h:30m:56s remains)
INFO - root - 2017-12-08 00:13:11.169001: step 70490, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 48h:05m:33s remains)
INFO - root - 2017-12-08 00:13:17.953486: step 70500, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 46h:47m:44s remains)
2017-12-08 00:13:18.660528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3031178 -4.307445 -4.3133321 -4.3158059 -4.3115487 -4.3064184 -4.3063526 -4.3065181 -4.3025193 -4.2980914 -4.2902994 -4.2866492 -4.2881441 -4.2923841 -4.2983923][-4.2800508 -4.2854671 -4.2994103 -4.3085332 -4.3020062 -4.2913656 -4.2863817 -4.2911606 -4.2937012 -4.2938151 -4.2864075 -4.2810578 -4.2810936 -4.2839246 -4.2889128][-4.2452483 -4.2561488 -4.2823334 -4.298377 -4.2895689 -4.274477 -4.2632074 -4.2691932 -4.2782764 -4.2845025 -4.2822609 -4.2796268 -4.2785559 -4.27867 -4.2819858][-4.195415 -4.21413 -4.2498174 -4.2722397 -4.2601581 -4.2384138 -4.2215543 -4.2297592 -4.24629 -4.2607608 -4.2666979 -4.272512 -4.2751808 -4.2752147 -4.2781777][-4.1467528 -4.17021 -4.2075081 -4.2301478 -4.2110553 -4.1764026 -4.1520081 -4.1682844 -4.2012968 -4.230927 -4.2447944 -4.2589312 -4.2699704 -4.2737932 -4.2781768][-4.1109395 -4.1361675 -4.1709113 -4.1871943 -4.1526208 -4.0927773 -4.0504417 -4.0778828 -4.1367555 -4.1896057 -4.2170057 -4.2389297 -4.2568197 -4.2665539 -4.2749033][-4.0887313 -4.1163044 -4.1479173 -4.14953 -4.087883 -3.9852376 -3.9102857 -3.9546716 -4.0513396 -4.1347313 -4.1813664 -4.212358 -4.2313795 -4.2437687 -4.2579308][-4.1047573 -4.1248302 -4.1480455 -4.1339083 -4.0480962 -3.9106266 -3.8002515 -3.8533077 -3.9834783 -4.0876236 -4.1461921 -4.1805058 -4.2005196 -4.2173872 -4.2380629][-4.1531487 -4.1672373 -4.1837449 -4.1664653 -4.0859671 -3.9618578 -3.8598652 -3.8903499 -3.9963346 -4.0801177 -4.1268849 -4.1558228 -4.1794419 -4.2019005 -4.2272964][-4.1892495 -4.1991134 -4.2141504 -4.2094278 -4.1604686 -4.0835571 -4.0202279 -4.0288711 -4.0838037 -4.1289692 -4.1554947 -4.1749992 -4.1970873 -4.2189507 -4.2401934][-4.2083478 -4.2130713 -4.2276478 -4.2365756 -4.2212896 -4.1873803 -4.1552544 -4.1506534 -4.1668024 -4.1848478 -4.2004304 -4.2156792 -4.2350893 -4.2535238 -4.267436][-4.2180986 -4.2179542 -4.2314816 -4.2470217 -4.2544079 -4.2491794 -4.2384462 -4.2323718 -4.2280579 -4.231915 -4.2422347 -4.2555175 -4.2715769 -4.28702 -4.2968583][-4.2405887 -4.2351317 -4.2433977 -4.2588353 -4.2752352 -4.2853732 -4.2873635 -4.2829289 -4.2701354 -4.2663321 -4.2739248 -4.2859898 -4.2983103 -4.3095818 -4.3165379][-4.2752414 -4.2701125 -4.2723475 -4.2811503 -4.2947583 -4.3080306 -4.3168111 -4.3135824 -4.2991071 -4.2929955 -4.298027 -4.3091092 -4.3179007 -4.3230314 -4.3268981][-4.3038049 -4.30312 -4.3008871 -4.3019567 -4.3083611 -4.3187904 -4.3273811 -4.3246808 -4.3128467 -4.3073058 -4.3114281 -4.3206806 -4.327457 -4.3302503 -4.332459]]...]
INFO - root - 2017-12-08 00:13:25.223691: step 70510, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 49h:28m:47s remains)
INFO - root - 2017-12-08 00:13:32.103567: step 70520, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 46h:55m:57s remains)
INFO - root - 2017-12-08 00:13:38.825875: step 70530, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 46h:51m:38s remains)
INFO - root - 2017-12-08 00:13:45.680107: step 70540, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 50h:57m:30s remains)
INFO - root - 2017-12-08 00:13:52.484495: step 70550, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 51h:46m:44s remains)
INFO - root - 2017-12-08 00:13:59.283837: step 70560, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:38m:55s remains)
INFO - root - 2017-12-08 00:14:06.092606: step 70570, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.652 sec/batch; 47h:26m:11s remains)
INFO - root - 2017-12-08 00:14:12.940474: step 70580, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 52h:07m:45s remains)
INFO - root - 2017-12-08 00:14:19.782078: step 70590, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 52h:21m:58s remains)
INFO - root - 2017-12-08 00:14:26.682376: step 70600, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:53m:31s remains)
2017-12-08 00:14:27.361320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2234 -4.2270861 -4.234385 -4.2358541 -4.2300844 -4.2515864 -4.281486 -4.2893391 -4.281353 -4.2868795 -4.3055067 -4.3141894 -4.3144364 -4.3091426 -4.3013721][-4.2166247 -4.2173762 -4.2234688 -4.2274013 -4.2250872 -4.244143 -4.2715287 -4.2794447 -4.2778888 -4.2896504 -4.3100748 -4.320302 -4.3203082 -4.3149252 -4.3067474][-4.1911006 -4.1870236 -4.1875868 -4.1946888 -4.1963043 -4.2115622 -4.2386427 -4.2522516 -4.2602639 -4.2738118 -4.2915249 -4.3016739 -4.3006716 -4.2947021 -4.2850761][-4.1749134 -4.1609125 -4.1512852 -4.155859 -4.1543155 -4.1666751 -4.1962309 -4.2188644 -4.2325144 -4.2441149 -4.2562556 -4.2694254 -4.2708492 -4.2645802 -4.2503781][-4.165741 -4.1411595 -4.1216345 -4.1175294 -4.1078463 -4.1184082 -4.1475134 -4.1731162 -4.182518 -4.1869116 -4.1951256 -4.2132416 -4.2173724 -4.2111783 -4.1924472][-4.165709 -4.1362243 -4.1085105 -4.0914135 -4.0721908 -4.0784535 -4.0982523 -4.1160722 -4.112124 -4.100553 -4.1071053 -4.1353498 -4.14622 -4.1434903 -4.1252518][-4.176199 -4.1422887 -4.1037989 -4.0673032 -4.032629 -4.0300012 -4.0420904 -4.0518503 -4.0300236 -4.007894 -4.0301213 -4.0792179 -4.1015792 -4.1112485 -4.1050267][-4.1766706 -4.1434627 -4.1053467 -4.0630684 -4.0254931 -4.0221357 -4.0305824 -4.0350647 -4.0038905 -3.9790819 -4.0097885 -4.0668097 -4.0921 -4.1134596 -4.1250944][-4.1899219 -4.1707158 -4.1453791 -4.1095648 -4.0739594 -4.0717249 -4.0805125 -4.0823936 -4.0540929 -4.0353322 -4.0630631 -4.1060848 -4.1218367 -4.1383061 -4.1502504][-4.1917429 -4.183342 -4.1723762 -4.1505966 -4.1277528 -4.1335707 -4.1456127 -4.1507812 -4.1346483 -4.129169 -4.1515188 -4.1741991 -4.1758356 -4.1812215 -4.1848464][-4.1923928 -4.1975636 -4.2008328 -4.1887584 -4.1771455 -4.1944542 -4.2090473 -4.2113013 -4.2002878 -4.198236 -4.2116222 -4.2176008 -4.2101235 -4.2077494 -4.2040081][-4.2206826 -4.2344809 -4.2378273 -4.223392 -4.2124763 -4.2331047 -4.2478733 -4.2485714 -4.2416463 -4.2417035 -4.2468519 -4.2409163 -4.2259326 -4.2170515 -4.20915][-4.2647157 -4.2787476 -4.2783189 -4.2600765 -4.2462082 -4.2658153 -4.2794685 -4.279036 -4.2762666 -4.2780766 -4.2781224 -4.2678165 -4.2507515 -4.2371483 -4.2254548][-4.3033118 -4.3152962 -4.3122907 -4.2934389 -4.2795682 -4.2958388 -4.3069062 -4.3040972 -4.3015223 -4.30244 -4.3014121 -4.2944226 -4.2821565 -4.2705212 -4.2600741][-4.3209605 -4.32979 -4.3249335 -4.3069649 -4.2938547 -4.3047504 -4.3109574 -4.308538 -4.3076329 -4.3083344 -4.30876 -4.3073449 -4.3017607 -4.2946692 -4.2880068]]...]
INFO - root - 2017-12-08 00:14:34.002157: step 70610, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.717 sec/batch; 52h:08m:44s remains)
INFO - root - 2017-12-08 00:14:40.820197: step 70620, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.720 sec/batch; 52h:23m:19s remains)
INFO - root - 2017-12-08 00:14:47.679158: step 70630, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 50h:19m:57s remains)
INFO - root - 2017-12-08 00:14:54.329968: step 70640, loss = 2.10, batch loss = 2.04 (15.9 examples/sec; 0.502 sec/batch; 36h:31m:29s remains)
INFO - root - 2017-12-08 00:15:01.158504: step 70650, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 49h:13m:31s remains)
INFO - root - 2017-12-08 00:15:08.018043: step 70660, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.695 sec/batch; 50h:32m:11s remains)
INFO - root - 2017-12-08 00:15:14.747788: step 70670, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 48h:16m:26s remains)
INFO - root - 2017-12-08 00:15:21.556642: step 70680, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.657 sec/batch; 47h:46m:20s remains)
INFO - root - 2017-12-08 00:15:28.327918: step 70690, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.664 sec/batch; 48h:18m:50s remains)
INFO - root - 2017-12-08 00:15:35.191763: step 70700, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 51h:23m:47s remains)
2017-12-08 00:15:35.836732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3130965 -4.3269429 -4.3345633 -4.3367872 -4.3330956 -4.326313 -4.3217087 -4.3210521 -4.3224869 -4.3233323 -4.3247991 -4.3236189 -4.3187943 -4.3145871 -4.3132768][-4.3149157 -4.3348541 -4.3475561 -4.3506117 -4.3448215 -4.335093 -4.3262968 -4.3237143 -4.3265934 -4.3300772 -4.3321466 -4.3313632 -4.3253369 -4.3202529 -4.3168731][-4.3094325 -4.3314295 -4.346334 -4.3495059 -4.3397274 -4.3254423 -4.3108244 -4.305191 -4.3116212 -4.3198075 -4.3242025 -4.3260145 -4.3214784 -4.315145 -4.306644][-4.2915678 -4.3106003 -4.3238149 -4.3249788 -4.3111992 -4.2890062 -4.2652707 -4.2505546 -4.25639 -4.2729669 -4.2865396 -4.2982373 -4.2998343 -4.2935066 -4.2795663][-4.2519784 -4.2647991 -4.2740669 -4.275301 -4.2594337 -4.2277937 -4.1907105 -4.1629763 -4.1689005 -4.199275 -4.2300835 -4.2560611 -4.2663817 -4.2602115 -4.243001][-4.213089 -4.2161355 -4.217298 -4.2176275 -4.1991224 -4.1512737 -4.08981 -4.0420375 -4.0540333 -4.1115847 -4.169457 -4.2129388 -4.2297149 -4.2231307 -4.2002058][-4.1958928 -4.1866593 -4.176465 -4.1690712 -4.1453037 -4.0806384 -3.986789 -3.9067583 -3.9238069 -4.0181441 -4.1025372 -4.1591587 -4.1797962 -4.1764379 -4.1507721][-4.2083392 -4.1898427 -4.1708531 -4.157495 -4.1255751 -4.0536814 -3.9449382 -3.8442585 -3.8593471 -3.97363 -4.0709887 -4.123879 -4.1405334 -4.1431513 -4.1185513][-4.2162523 -4.2011943 -4.1878819 -4.1795225 -4.153687 -4.0995994 -4.0171371 -3.9473565 -3.9604709 -4.0440135 -4.1129851 -4.1419873 -4.1441855 -4.1425786 -4.1138458][-4.2158079 -4.212635 -4.2082233 -4.2062669 -4.1965456 -4.1709352 -4.1233721 -4.0889025 -4.0995536 -4.1416507 -4.1777725 -4.1893229 -4.1823983 -4.17277 -4.1419353][-4.2086177 -4.2146063 -4.2108879 -4.2135868 -4.2232118 -4.2249718 -4.2051005 -4.1873283 -4.1914382 -4.208158 -4.2283573 -4.2363906 -4.2287822 -4.2199917 -4.1963034][-4.1808119 -4.1957431 -4.1975346 -4.2052693 -4.228838 -4.2474384 -4.243021 -4.2310853 -4.2311082 -4.2423606 -4.2597084 -4.2674513 -4.2623987 -4.2611971 -4.2495956][-4.1534438 -4.1716518 -4.1779985 -4.1879654 -4.2180462 -4.2458282 -4.2507157 -4.2427807 -4.2436147 -4.2567067 -4.2762842 -4.27988 -4.2743716 -4.2756433 -4.2737856][-4.1489792 -4.1631174 -4.1650691 -4.1718717 -4.1988459 -4.228209 -4.2401986 -4.2384133 -4.243566 -4.2618976 -4.2850676 -4.2879143 -4.280972 -4.2828574 -4.2822595][-4.1695843 -4.1767387 -4.1726847 -4.1760206 -4.1979375 -4.2235546 -4.2347045 -4.2394528 -4.2515531 -4.275548 -4.2980595 -4.3029737 -4.2962065 -4.2944479 -4.2919025]]...]
INFO - root - 2017-12-08 00:15:42.449494: step 70710, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 46h:04m:12s remains)
INFO - root - 2017-12-08 00:15:49.349231: step 70720, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 49h:43m:58s remains)
INFO - root - 2017-12-08 00:15:56.149324: step 70730, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 50h:13m:50s remains)
INFO - root - 2017-12-08 00:16:03.045437: step 70740, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.727 sec/batch; 52h:49m:49s remains)
INFO - root - 2017-12-08 00:16:09.859727: step 70750, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 50h:35m:57s remains)
INFO - root - 2017-12-08 00:16:16.657397: step 70760, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 46h:40m:59s remains)
INFO - root - 2017-12-08 00:16:23.564700: step 70770, loss = 2.08, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 47h:55m:06s remains)
INFO - root - 2017-12-08 00:16:30.449774: step 70780, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 51h:14m:27s remains)
INFO - root - 2017-12-08 00:16:37.323128: step 70790, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.744 sec/batch; 54h:06m:52s remains)
INFO - root - 2017-12-08 00:16:44.246785: step 70800, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 49h:56m:18s remains)
2017-12-08 00:16:44.946464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2015066 -4.2328391 -4.2686524 -4.2913756 -4.290524 -4.2683783 -4.235652 -4.21277 -4.2078338 -4.2145829 -4.2279725 -4.23827 -4.2406526 -4.2374945 -4.2313914][-4.2110882 -4.2512379 -4.2904639 -4.3063436 -4.2913227 -4.2503138 -4.1943431 -4.1552849 -4.1522131 -4.177515 -4.2092776 -4.2324839 -4.2443833 -4.2445726 -4.2363243][-4.2349639 -4.2735934 -4.3049426 -4.3101935 -4.2789154 -4.2139163 -4.1317716 -4.0746832 -4.0833969 -4.1412263 -4.1986823 -4.2346277 -4.2524891 -4.2556553 -4.2468386][-4.2698526 -4.301053 -4.3179822 -4.3068624 -4.2586031 -4.1667266 -4.0517483 -3.9787793 -4.0140224 -4.1118822 -4.1961522 -4.2397132 -4.2593808 -4.2663531 -4.2597575][-4.303318 -4.3211274 -4.3222713 -4.2928562 -4.2245545 -4.1022553 -3.9567728 -3.8844137 -3.9596326 -4.0940871 -4.1935596 -4.2359982 -4.2553992 -4.2663445 -4.2658172][-4.3151321 -4.3181653 -4.3060226 -4.2632461 -4.176331 -4.0283833 -3.8674085 -3.8211319 -3.946898 -4.1002107 -4.1931219 -4.2225728 -4.2357392 -4.248837 -4.2556543][-4.2906103 -4.2795153 -4.2625284 -4.2173934 -4.1231194 -3.9665403 -3.8183444 -3.825305 -3.9819357 -4.1219177 -4.1891427 -4.1981578 -4.202261 -4.2169838 -4.2341652][-4.2374196 -4.2175522 -4.2028074 -4.1700411 -4.0871091 -3.9486706 -3.8501208 -3.9020183 -4.0408244 -4.141767 -4.1772771 -4.1717997 -4.173635 -4.1917644 -4.2152514][-4.1911645 -4.173419 -4.1677194 -4.1544657 -4.0953197 -3.9955823 -3.9459138 -4.0040941 -4.1014757 -4.1569066 -4.1618719 -4.14975 -4.155273 -4.1779108 -4.199544][-4.1701035 -4.16918 -4.1762109 -4.1789565 -4.1456571 -4.087646 -4.0677118 -4.1064796 -4.1559205 -4.1732473 -4.1535416 -4.1317129 -4.1396937 -4.1642485 -4.1801758][-4.1663833 -4.1864529 -4.2059669 -4.2183771 -4.2067213 -4.1801128 -4.1749687 -4.1912603 -4.2019787 -4.1924658 -4.1572361 -4.1309767 -4.1411257 -4.1633191 -4.1723809][-4.1690588 -4.1987343 -4.223475 -4.2418647 -4.2435036 -4.236177 -4.2398882 -4.2458782 -4.2391491 -4.2164397 -4.1743412 -4.1510763 -4.1603556 -4.1777773 -4.1805367][-4.1670909 -4.195004 -4.2198477 -4.2389178 -4.2464108 -4.2498007 -4.258635 -4.2655172 -4.2625051 -4.2429967 -4.208324 -4.1933 -4.1988678 -4.2063055 -4.20207][-4.1701813 -4.1921105 -4.2143669 -4.2285066 -4.2329926 -4.2368956 -4.2457013 -4.2588282 -4.2667866 -4.2580342 -4.2365046 -4.2302113 -4.2341037 -4.2356195 -4.2285624][-4.1665564 -4.1799364 -4.1989627 -4.2064767 -4.2074804 -4.211668 -4.2227855 -4.2438784 -4.258883 -4.2563639 -4.2473259 -4.2500815 -4.2550817 -4.2593074 -4.2573085]]...]
INFO - root - 2017-12-08 00:16:51.756100: step 70810, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 52h:20m:34s remains)
INFO - root - 2017-12-08 00:16:58.585270: step 70820, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 47h:44m:06s remains)
INFO - root - 2017-12-08 00:17:05.501513: step 70830, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 45h:59m:37s remains)
INFO - root - 2017-12-08 00:17:12.284206: step 70840, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.656 sec/batch; 47h:41m:34s remains)
INFO - root - 2017-12-08 00:17:19.170650: step 70850, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.696 sec/batch; 50h:35m:24s remains)
INFO - root - 2017-12-08 00:17:26.050298: step 70860, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.735 sec/batch; 53h:22m:57s remains)
INFO - root - 2017-12-08 00:17:32.864708: step 70870, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 47h:10m:29s remains)
INFO - root - 2017-12-08 00:17:39.624040: step 70880, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 49h:43m:55s remains)
INFO - root - 2017-12-08 00:17:46.525033: step 70890, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 47h:14m:32s remains)
INFO - root - 2017-12-08 00:17:53.334312: step 70900, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 48h:45m:48s remains)
2017-12-08 00:17:54.048266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3513613 -4.3574839 -4.3545923 -4.3391032 -4.3124785 -4.2818718 -4.2583447 -4.241353 -4.2346497 -4.2395506 -4.2494106 -4.2575526 -4.2651668 -4.2717595 -4.2717633][-4.3523588 -4.3600836 -4.3537803 -4.3304067 -4.2952981 -4.2561841 -4.2239871 -4.2009549 -4.1970134 -4.2094779 -4.2282581 -4.2479315 -4.2636476 -4.2704682 -4.2660322][-4.3506274 -4.3588834 -4.347816 -4.3144603 -4.2702322 -4.2248168 -4.1803265 -4.1476564 -4.1465697 -4.17084 -4.2040386 -4.24157 -4.2680521 -4.2779956 -4.274971][-4.3497076 -4.3537107 -4.3343115 -4.2914414 -4.2399344 -4.1843572 -4.1231985 -4.0761037 -4.0794573 -4.123549 -4.1785574 -4.23278 -4.2702289 -4.28778 -4.2892022][-4.3499761 -4.3468528 -4.3175831 -4.2675886 -4.2080622 -4.1355805 -4.0484843 -3.9840605 -4.0046239 -4.0829463 -4.1631813 -4.2296987 -4.2744184 -4.297235 -4.3005285][-4.3485327 -4.3368568 -4.2976217 -4.2407537 -4.1678028 -4.0725331 -3.9458964 -3.8519344 -3.9005585 -4.0281396 -4.1395173 -4.2158747 -4.2675233 -4.2945781 -4.2997365][-4.3428855 -4.3246665 -4.2819319 -4.2211695 -4.1405439 -4.0275617 -3.8637447 -3.7316239 -3.8094754 -3.9820812 -4.1175566 -4.1970634 -4.2514868 -4.2807989 -4.2872529][-4.3386412 -4.3186927 -4.2769852 -4.2205491 -4.1466231 -4.0415044 -3.8867815 -3.7633457 -3.8361578 -3.992516 -4.1172714 -4.1904182 -4.2404428 -4.268085 -4.2742763][-4.33944 -4.3235989 -4.2854719 -4.2392588 -4.1857834 -4.1081939 -3.9955294 -3.910172 -3.953217 -4.0558634 -4.1497831 -4.2101531 -4.2512183 -4.2733564 -4.2773762][-4.3459849 -4.336545 -4.3043685 -4.2704372 -4.2342248 -4.180325 -4.1068454 -4.0560822 -4.0768223 -4.1373487 -4.2044368 -4.2522893 -4.28046 -4.2912593 -4.2922616][-4.35615 -4.3533654 -4.3294249 -4.3045807 -4.2772141 -4.2406764 -4.1983318 -4.1718125 -4.181973 -4.2207041 -4.2668171 -4.2991786 -4.3109641 -4.3093929 -4.3062711][-4.362411 -4.3660059 -4.3495507 -4.3307381 -4.3118043 -4.2876964 -4.2678213 -4.2562218 -4.2599449 -4.2841854 -4.3148808 -4.3351159 -4.33661 -4.327672 -4.3223939][-4.3608365 -4.3670192 -4.3559747 -4.3411336 -4.328836 -4.3152962 -4.3072095 -4.3029976 -4.3025613 -4.31537 -4.3359618 -4.3504567 -4.3483744 -4.3368187 -4.3304214][-4.3500051 -4.3551788 -4.3482151 -4.339396 -4.33175 -4.3263507 -4.325829 -4.3240585 -4.3209019 -4.324791 -4.3377056 -4.3466558 -4.3415546 -4.3298922 -4.3217988][-4.3388777 -4.3407135 -4.3354793 -4.3301797 -4.3247466 -4.3233476 -4.3263769 -4.3259025 -4.32179 -4.3212652 -4.3286133 -4.3329124 -4.3271818 -4.3164053 -4.3079267]]...]
INFO - root - 2017-12-08 00:18:00.741506: step 70910, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.655 sec/batch; 47h:36m:00s remains)
INFO - root - 2017-12-08 00:18:07.518458: step 70920, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 48h:20m:10s remains)
INFO - root - 2017-12-08 00:18:14.350335: step 70930, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 50h:06m:38s remains)
INFO - root - 2017-12-08 00:18:21.253466: step 70940, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 52h:40m:35s remains)
INFO - root - 2017-12-08 00:18:28.000481: step 70950, loss = 2.06, batch loss = 2.00 (13.4 examples/sec; 0.596 sec/batch; 43h:15m:54s remains)
INFO - root - 2017-12-08 00:18:34.683964: step 70960, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 49h:16m:44s remains)
INFO - root - 2017-12-08 00:18:41.499686: step 70970, loss = 2.04, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 49h:56m:07s remains)
INFO - root - 2017-12-08 00:18:48.285945: step 70980, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 49h:08m:00s remains)
INFO - root - 2017-12-08 00:18:55.145012: step 70990, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 50h:37m:38s remains)
INFO - root - 2017-12-08 00:19:01.990152: step 71000, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 49h:54m:15s remains)
2017-12-08 00:19:02.764103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2024417 -4.1542382 -4.1240749 -4.1123056 -4.1136804 -4.1270757 -4.1427684 -4.1650348 -4.1845212 -4.1948829 -4.1909728 -4.17419 -4.1602755 -4.14917 -4.1512461][-4.2359023 -4.1998782 -4.1846581 -4.1825347 -4.1891403 -4.2047186 -4.2171121 -4.227572 -4.2296042 -4.22631 -4.2086692 -4.1801791 -4.1546836 -4.1310225 -4.1251564][-4.2664528 -4.2347994 -4.2277055 -4.2318764 -4.2439184 -4.2631702 -4.2788329 -4.2862277 -4.28126 -4.2734489 -4.2549314 -4.2298179 -4.205308 -4.18196 -4.1717339][-4.2900362 -4.2568064 -4.2436662 -4.2453423 -4.258038 -4.278358 -4.2981982 -4.3100748 -4.3088875 -4.3057442 -4.2931128 -4.27738 -4.2630496 -4.2495356 -4.2410669][-4.3060431 -4.2640781 -4.2390561 -4.232337 -4.2415929 -4.2592092 -4.28041 -4.2934084 -4.2971792 -4.3031855 -4.30144 -4.2983432 -4.296309 -4.2936363 -4.2873259][-4.3061891 -4.247591 -4.2010489 -4.1780696 -4.180903 -4.2008877 -4.2314034 -4.2474551 -4.2501531 -4.2627988 -4.2731996 -4.2836766 -4.2948 -4.3053679 -4.3046684][-4.2984872 -4.2180982 -4.1413651 -4.0952444 -4.0904994 -4.1158333 -4.1562328 -4.1747 -4.175828 -4.1926017 -4.2136846 -4.2363291 -4.2617083 -4.2893181 -4.3002028][-4.2899971 -4.2017441 -4.1147232 -4.0566149 -4.0432792 -4.0621004 -4.0953403 -4.1081629 -4.1077209 -4.1264753 -4.1530986 -4.1837649 -4.2192512 -4.2636275 -4.2898417][-4.2806582 -4.20806 -4.1376762 -4.0909319 -4.0766916 -4.0865712 -4.1042933 -4.1095281 -4.1036916 -4.1077456 -4.1256742 -4.1504622 -4.1839285 -4.2364845 -4.2789254][-4.2663851 -4.2115779 -4.1636062 -4.1373372 -4.1318936 -4.1418934 -4.1567845 -4.1629376 -4.1538672 -4.1345148 -4.1294713 -4.1400495 -4.165329 -4.2196021 -4.2714481][-4.2395706 -4.1874666 -4.1518168 -4.1409116 -4.1462889 -4.1629319 -4.1821671 -4.1932468 -4.1778216 -4.1411257 -4.1219025 -4.1294422 -4.1584897 -4.2117019 -4.2669082][-4.2064281 -4.1545076 -4.1257195 -4.1235409 -4.133821 -4.1510005 -4.1763015 -4.18962 -4.1607103 -4.1085567 -4.0849338 -4.0967631 -4.1396713 -4.1996789 -4.2604175][-4.178153 -4.1330881 -4.1094518 -4.1040397 -4.108089 -4.1193452 -4.1446266 -4.1622448 -4.1318297 -4.0817161 -4.0619411 -4.0748773 -4.1252179 -4.19252 -4.2573485][-4.1505275 -4.1154633 -4.0980549 -4.0908504 -4.0896635 -4.0972753 -4.1210213 -4.1444612 -4.134788 -4.1089683 -4.0973544 -4.109869 -4.1577005 -4.2165484 -4.2724771][-4.1351485 -4.1056862 -4.0961304 -4.0932508 -4.0950809 -4.10695 -4.1285448 -4.1510005 -4.16286 -4.1653166 -4.1686225 -4.1854424 -4.2239528 -4.260735 -4.2952285]]...]
INFO - root - 2017-12-08 00:19:09.339486: step 71010, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.674 sec/batch; 48h:56m:30s remains)
INFO - root - 2017-12-08 00:19:16.035633: step 71020, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 47h:17m:43s remains)
INFO - root - 2017-12-08 00:19:22.821098: step 71030, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.666 sec/batch; 48h:22m:14s remains)
INFO - root - 2017-12-08 00:19:29.561497: step 71040, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.643 sec/batch; 46h:41m:38s remains)
INFO - root - 2017-12-08 00:19:36.355912: step 71050, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.688 sec/batch; 49h:59m:36s remains)
INFO - root - 2017-12-08 00:19:43.087492: step 71060, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.643 sec/batch; 46h:42m:15s remains)
INFO - root - 2017-12-08 00:19:49.797661: step 71070, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 49h:03m:52s remains)
INFO - root - 2017-12-08 00:19:56.597713: step 71080, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 49h:48m:05s remains)
INFO - root - 2017-12-08 00:20:03.415255: step 71090, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.661 sec/batch; 48h:00m:37s remains)
INFO - root - 2017-12-08 00:20:10.371431: step 71100, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.690 sec/batch; 50h:07m:36s remains)
2017-12-08 00:20:11.235042: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2721348 -4.271266 -4.2698517 -4.263618 -4.2527294 -4.2341647 -4.2106595 -4.2078438 -4.2279205 -4.2407618 -4.2491717 -4.2566757 -4.2579904 -4.2544909 -4.26158][-4.2713232 -4.2646508 -4.2581844 -4.2454805 -4.227653 -4.2037044 -4.1689878 -4.159544 -4.187685 -4.2042923 -4.2156048 -4.2250543 -4.2232957 -4.21615 -4.2193151][-4.272913 -4.2582884 -4.2460852 -4.223896 -4.1975236 -4.1700506 -4.1249723 -4.1001983 -4.1302276 -4.1536474 -4.1661711 -4.1782036 -4.1791248 -4.1692657 -4.1684232][-4.2707729 -4.2535057 -4.2371731 -4.20975 -4.1779447 -4.1452909 -4.0928483 -4.0518069 -4.0798955 -4.1088209 -4.1186705 -4.131156 -4.1346784 -4.1264687 -4.1250215][-4.2657852 -4.2495289 -4.2328238 -4.2058525 -4.1679149 -4.1257839 -4.0670886 -4.0170636 -4.0428834 -4.0715146 -4.0757895 -4.0793772 -4.0834775 -4.0827303 -4.0819359][-4.2591543 -4.2441692 -4.2278371 -4.197494 -4.1522093 -4.0997419 -4.0283842 -3.9623461 -3.9812999 -4.0080838 -4.0089216 -4.0137529 -4.0256162 -4.0380039 -4.0451293][-4.2563043 -4.2421312 -4.221467 -4.1808887 -4.1260023 -4.0544643 -3.950716 -3.853044 -3.8669889 -3.9131134 -3.9322233 -3.9562263 -3.9863744 -4.014226 -4.0277348][-4.2480259 -4.2316647 -4.2045608 -4.1531897 -4.0810285 -3.9848566 -3.8553391 -3.7439585 -3.7811005 -3.8717394 -3.9182725 -3.9580071 -3.9944258 -4.021347 -4.0261536][-4.2289424 -4.2083192 -4.1800609 -4.1307478 -4.061439 -3.9780939 -3.8814411 -3.80977 -3.8610296 -3.9471059 -3.9850867 -4.0138736 -4.0364823 -4.047163 -4.043654][-4.2041759 -4.187665 -4.1706576 -4.14182 -4.09792 -4.0462608 -3.9911761 -3.9495597 -3.9819613 -4.0352077 -4.0498476 -4.0653334 -4.0786905 -4.0844865 -4.0796671][-4.1967187 -4.1879439 -4.1818047 -4.1683531 -4.1447725 -4.1157937 -4.0807505 -4.0499382 -4.0624213 -4.0916014 -4.096034 -4.1061249 -4.11651 -4.120995 -4.1219745][-4.2096224 -4.205925 -4.2053652 -4.201808 -4.1890316 -4.173965 -4.1546664 -4.1332383 -4.137754 -4.1552181 -4.156929 -4.1614456 -4.1642179 -4.1619911 -4.1662474][-4.2329731 -4.2315288 -4.2376218 -4.2426877 -4.2402458 -4.2350368 -4.2282462 -4.2138553 -4.2143631 -4.225513 -4.2262087 -4.2248688 -4.2199745 -4.2127509 -4.2163057][-4.2543349 -4.2556844 -4.2672505 -4.277679 -4.2825985 -4.2843533 -4.2838116 -4.2736983 -4.2689028 -4.2709966 -4.2676978 -4.2630711 -4.2593188 -4.2577872 -4.2643895][-4.2733417 -4.2771244 -4.2899818 -4.3003731 -4.3067913 -4.3094358 -4.3088794 -4.30085 -4.2945271 -4.291492 -4.2863669 -4.2824521 -4.2829657 -4.2871733 -4.2945113]]...]
INFO - root - 2017-12-08 00:20:17.807412: step 71110, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.648 sec/batch; 47h:01m:51s remains)
INFO - root - 2017-12-08 00:20:24.548597: step 71120, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 46h:32m:55s remains)
INFO - root - 2017-12-08 00:20:31.270265: step 71130, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 49h:22m:35s remains)
INFO - root - 2017-12-08 00:20:38.160321: step 71140, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 51h:59m:24s remains)
INFO - root - 2017-12-08 00:20:44.881079: step 71150, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 49h:33m:31s remains)
INFO - root - 2017-12-08 00:20:51.601013: step 71160, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.642 sec/batch; 46h:37m:14s remains)
INFO - root - 2017-12-08 00:20:58.380256: step 71170, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.659 sec/batch; 47h:49m:07s remains)
INFO - root - 2017-12-08 00:21:05.157221: step 71180, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 51h:07m:30s remains)
INFO - root - 2017-12-08 00:21:11.836636: step 71190, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.673 sec/batch; 48h:49m:48s remains)
INFO - root - 2017-12-08 00:21:18.634964: step 71200, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:30m:24s remains)
2017-12-08 00:21:19.325276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2050843 -4.1781769 -4.1721354 -4.1815505 -4.1861334 -4.1971464 -4.2110105 -4.2022686 -4.1899996 -4.1949573 -4.1994643 -4.1914587 -4.1892724 -4.2051826 -4.2236862][-4.1836343 -4.1581993 -4.1551452 -4.1742687 -4.1867814 -4.1968246 -4.2041626 -4.1894183 -4.1762414 -4.1811781 -4.1844506 -4.1745157 -4.167695 -4.180047 -4.2002249][-4.1990995 -4.1828561 -4.1845412 -4.2014346 -4.2068472 -4.2056255 -4.2013173 -4.1831422 -4.1735315 -4.17976 -4.1855378 -4.1792879 -4.1743808 -4.1857586 -4.2036614][-4.2232394 -4.2164707 -4.2146177 -4.2166305 -4.2061582 -4.1950188 -4.1868939 -4.17706 -4.1735892 -4.182394 -4.1931314 -4.1925607 -4.1909833 -4.2022405 -4.2180963][-4.2312951 -4.2230172 -4.2120013 -4.1944194 -4.1708937 -4.1534495 -4.1417403 -4.1401114 -4.1479425 -4.1665778 -4.1861539 -4.1910267 -4.1896172 -4.2003312 -4.2162004][-4.2059307 -4.1883826 -4.1661434 -4.1369457 -4.1060448 -4.0789804 -4.0515513 -4.0438008 -4.0649648 -4.1044431 -4.1421466 -4.1621237 -4.1693873 -4.1858573 -4.20534][-4.1474485 -4.1252785 -4.09703 -4.05976 -4.0225239 -3.980443 -3.926048 -3.9022739 -3.9456589 -4.01648 -4.0738921 -4.1115513 -4.1334343 -4.1613092 -4.1892271][-4.0936484 -4.0703135 -4.0459857 -4.0132585 -3.9847946 -3.9389553 -3.8687136 -3.8301923 -3.8808441 -3.9642591 -4.0266862 -4.0671206 -4.0973425 -4.1347222 -4.1724219][-4.0790014 -4.0651965 -4.0524268 -4.0351119 -4.0223713 -3.9952896 -3.9516814 -3.9230018 -3.9478753 -4.0015464 -4.0422654 -4.0683618 -4.093287 -4.1296558 -4.1721053][-4.124752 -4.1174474 -4.1155715 -4.1118784 -4.1101255 -4.0973492 -4.0769968 -4.0626063 -4.0756264 -4.1036253 -4.1181016 -4.1263871 -4.1377869 -4.161027 -4.1953907][-4.196382 -4.1895819 -4.1913838 -4.1940789 -4.1971393 -4.1891608 -4.1786079 -4.1754389 -4.1842389 -4.1968794 -4.1975088 -4.1975694 -4.2022905 -4.2145314 -4.2378917][-4.2500567 -4.24519 -4.2461853 -4.2479439 -4.25018 -4.2446032 -4.2374434 -4.2372942 -4.2433076 -4.2509809 -4.2509575 -4.2541509 -4.259985 -4.2684379 -4.2833967][-4.2771597 -4.2754397 -4.2769313 -4.2788377 -4.280818 -4.2779074 -4.2725868 -4.2711716 -4.2738 -4.2784142 -4.2810073 -4.2870297 -4.2944412 -4.3037252 -4.315021][-4.2937069 -4.2937741 -4.2959137 -4.2981997 -4.2987609 -4.296092 -4.2916322 -4.2895379 -4.2904716 -4.2934723 -4.2982154 -4.3060036 -4.3139796 -4.3228521 -4.331439][-4.314888 -4.3150277 -4.3157678 -4.3167086 -4.3166547 -4.3152342 -4.3132057 -4.3123527 -4.3128862 -4.3144517 -4.3179355 -4.323854 -4.3296328 -4.3348427 -4.3396358]]...]
INFO - root - 2017-12-08 00:21:25.967396: step 71210, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 48h:19m:26s remains)
INFO - root - 2017-12-08 00:21:32.850764: step 71220, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 50h:51m:58s remains)
INFO - root - 2017-12-08 00:21:39.606516: step 71230, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 49h:02m:40s remains)
INFO - root - 2017-12-08 00:21:46.530532: step 71240, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 47h:02m:25s remains)
INFO - root - 2017-12-08 00:21:53.381858: step 71250, loss = 2.09, batch loss = 2.04 (12.0 examples/sec; 0.667 sec/batch; 48h:25m:11s remains)
INFO - root - 2017-12-08 00:22:00.268106: step 71260, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 51h:41m:13s remains)
INFO - root - 2017-12-08 00:22:06.847510: step 71270, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 49h:39m:10s remains)
INFO - root - 2017-12-08 00:22:13.630736: step 71280, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.685 sec/batch; 49h:42m:25s remains)
INFO - root - 2017-12-08 00:22:20.511680: step 71290, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 45h:36m:07s remains)
INFO - root - 2017-12-08 00:22:27.447497: step 71300, loss = 2.10, batch loss = 2.04 (11.0 examples/sec; 0.725 sec/batch; 52h:36m:20s remains)
2017-12-08 00:22:28.266147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.104115 -4.07843 -4.057898 -4.0493932 -4.0556855 -4.0745773 -4.09896 -4.1146579 -4.1249261 -4.1309695 -4.1209908 -4.1075172 -4.1016965 -4.1050243 -4.1052618][-4.1436372 -4.1248441 -4.1096973 -4.0988264 -4.0955434 -4.0996242 -4.110168 -4.1152344 -4.1227651 -4.1300211 -4.1218014 -4.1052189 -4.0981345 -4.0976105 -4.0899353][-4.1783338 -4.1677017 -4.1590047 -4.1467404 -4.1404514 -4.1397963 -4.1421509 -4.1429238 -4.1468825 -4.1586885 -4.1568341 -4.1400628 -4.127316 -4.1174345 -4.0991554][-4.2179837 -4.2117963 -4.2047577 -4.1905088 -4.17891 -4.1670337 -4.1548238 -4.148984 -4.1607184 -4.1825562 -4.1884651 -4.1792693 -4.1652932 -4.1467056 -4.1200933][-4.2173395 -4.2118683 -4.210031 -4.2008543 -4.1900005 -4.1688085 -4.1404047 -4.1221943 -4.135046 -4.1655464 -4.1775641 -4.1758423 -4.1654463 -4.1506333 -4.1296391][-4.1576843 -4.1532722 -4.1581821 -4.158112 -4.1486073 -4.1196876 -4.0798221 -4.052772 -4.0675683 -4.11065 -4.1348581 -4.1414404 -4.1355963 -4.1274796 -4.1178246][-4.0589314 -4.0617323 -4.0748262 -4.0804596 -4.065959 -4.0248461 -3.9692144 -3.9263668 -3.9474854 -4.0133204 -4.054739 -4.0740733 -4.0801539 -4.0847692 -4.0876646][-3.9697061 -3.9834633 -4.0023909 -4.0135078 -4.0013313 -3.9542208 -3.8791707 -3.8096735 -3.8300705 -3.9118683 -3.9655504 -4.0023007 -4.0293078 -4.0532079 -4.0685496][-3.9681239 -3.9844534 -4.0031161 -4.0177774 -4.0158439 -3.9799678 -3.9143205 -3.8541806 -3.8668797 -3.921694 -3.95776 -3.9921927 -4.0262861 -4.0579138 -4.0815992][-4.0274467 -4.0307631 -4.0383883 -4.0489311 -4.0544577 -4.0364046 -3.9987376 -3.9700379 -3.9807346 -4.0085344 -4.0216842 -4.0361447 -4.0558562 -4.0828438 -4.1088791][-4.0849028 -4.0778918 -4.077107 -4.083756 -4.0916314 -4.0905085 -4.079567 -4.0735078 -4.0847778 -4.1044097 -4.111886 -4.1116409 -4.1154222 -4.1312289 -4.1497197][-4.1438103 -4.135344 -4.1328435 -4.1368279 -4.1432004 -4.1478548 -4.1450181 -4.1428475 -4.1529531 -4.17128 -4.1784286 -4.1735263 -4.1720519 -4.180697 -4.1908531][-4.1924591 -4.1857729 -4.1837263 -4.1880145 -4.1967211 -4.2034755 -4.2007561 -4.1948929 -4.2002416 -4.2112775 -4.2141862 -4.2090139 -4.2070727 -4.212235 -4.2202096][-4.226984 -4.2239943 -4.2242012 -4.2299347 -4.239265 -4.2453914 -4.2426805 -4.2347927 -4.2331033 -4.236207 -4.235858 -4.2322845 -4.2318521 -4.2354474 -4.2445993][-4.2468953 -4.2465549 -4.2491927 -4.2549529 -4.2612605 -4.2642326 -4.2597337 -4.2497096 -4.2425575 -4.2389822 -4.2336507 -4.2288008 -4.2306304 -4.2376032 -4.2521982]]...]
INFO - root - 2017-12-08 00:22:34.867089: step 71310, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 47h:14m:32s remains)
INFO - root - 2017-12-08 00:22:41.713103: step 71320, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 47h:18m:15s remains)
INFO - root - 2017-12-08 00:22:48.545622: step 71330, loss = 2.07, batch loss = 2.02 (10.8 examples/sec; 0.740 sec/batch; 53h:42m:56s remains)
INFO - root - 2017-12-08 00:22:55.245757: step 71340, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 50h:54m:51s remains)
INFO - root - 2017-12-08 00:23:02.137707: step 71350, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.712 sec/batch; 51h:40m:58s remains)
INFO - root - 2017-12-08 00:23:08.845625: step 71360, loss = 2.04, batch loss = 1.98 (12.3 examples/sec; 0.652 sec/batch; 47h:15m:35s remains)
INFO - root - 2017-12-08 00:23:15.663211: step 71370, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.631 sec/batch; 45h:44m:54s remains)
INFO - root - 2017-12-08 00:23:22.536554: step 71380, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:47m:26s remains)
INFO - root - 2017-12-08 00:23:29.326910: step 71390, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 51h:31m:33s remains)
INFO - root - 2017-12-08 00:23:36.114150: step 71400, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:42m:20s remains)
2017-12-08 00:23:36.821003: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2596407 -4.2567029 -4.2530293 -4.245194 -4.2379966 -4.2323151 -4.2327776 -4.2366967 -4.2410393 -4.2499185 -4.26307 -4.2643228 -4.2575059 -4.2578216 -4.2634549][-4.2295446 -4.2256627 -4.2247877 -4.219389 -4.2168574 -4.2177653 -4.2229419 -4.2267981 -4.2304411 -4.23739 -4.2468119 -4.24474 -4.2331214 -4.2294722 -4.2327809][-4.2084942 -4.2056541 -4.2052174 -4.1994972 -4.1998191 -4.2094836 -4.2210059 -4.2266493 -4.2312469 -4.2357063 -4.2388911 -4.2343445 -4.2222385 -4.21495 -4.2156181][-4.19374 -4.1963139 -4.1959491 -4.1847978 -4.1815486 -4.193471 -4.2081294 -4.2163696 -4.2263584 -4.2333508 -4.2339077 -4.2302208 -4.2188759 -4.2097077 -4.2101259][-4.1840563 -4.1962128 -4.1987748 -4.1810236 -4.16591 -4.1680136 -4.1782918 -4.1866317 -4.2062669 -4.2245326 -4.2293539 -4.2279735 -4.2158966 -4.2025361 -4.2052336][-4.1811476 -4.20096 -4.2073021 -4.1901059 -4.1677036 -4.1519747 -4.1387115 -4.1320019 -4.1625562 -4.2032738 -4.218996 -4.2190776 -4.2059 -4.1895738 -4.1961517][-4.1820965 -4.2047482 -4.2154064 -4.206563 -4.1861849 -4.1534338 -4.1021757 -4.0602021 -4.0972118 -4.1680527 -4.1999116 -4.2053065 -4.1977792 -4.1849289 -4.1902266][-4.18438 -4.2063923 -4.2208595 -4.220356 -4.2061033 -4.1647062 -4.08335 -4.00077 -4.034338 -4.128655 -4.1772156 -4.1903491 -4.1927361 -4.1872997 -4.1879315][-4.1920576 -4.2095776 -4.2237558 -4.2299418 -4.2250247 -4.1917663 -4.1090832 -4.0141926 -4.0323739 -4.1165338 -4.1641693 -4.1814747 -4.1920042 -4.1931829 -4.18978][-4.2001305 -4.20879 -4.2178736 -4.22881 -4.2335677 -4.2204194 -4.1636438 -4.0903597 -4.0914664 -4.1376076 -4.1664672 -4.1835961 -4.198853 -4.2053533 -4.2001004][-4.2107196 -4.2077661 -4.2096858 -4.2212539 -4.2318616 -4.2341285 -4.2042594 -4.159246 -4.1526208 -4.1713653 -4.1835551 -4.193974 -4.204164 -4.2132564 -4.2114835][-4.2257175 -4.215179 -4.2098532 -4.2162614 -4.2266159 -4.2345581 -4.2214818 -4.1998496 -4.1976647 -4.2073197 -4.213057 -4.2117815 -4.2075877 -4.212801 -4.2198143][-4.2462769 -4.2348738 -4.2243395 -4.2222877 -4.2280517 -4.2352047 -4.2307792 -4.2241349 -4.2285237 -4.2398448 -4.2460566 -4.236659 -4.2208652 -4.2185359 -4.23037][-4.2643824 -4.2574058 -4.246181 -4.2394357 -4.2405734 -4.2436218 -4.242331 -4.2414427 -4.2473164 -4.2604704 -4.2696433 -4.2621717 -4.2425737 -4.2326 -4.2413573][-4.2825541 -4.2812262 -4.2721996 -4.2639832 -4.2612729 -4.25966 -4.2590361 -4.259922 -4.2640181 -4.2757735 -4.2848411 -4.281713 -4.2673664 -4.2544055 -4.2550807]]...]
INFO - root - 2017-12-08 00:23:43.533313: step 71410, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 52h:23m:09s remains)
INFO - root - 2017-12-08 00:23:50.367005: step 71420, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 51h:48m:19s remains)
INFO - root - 2017-12-08 00:23:57.200929: step 71430, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 49h:20m:56s remains)
INFO - root - 2017-12-08 00:24:03.961055: step 71440, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 49h:36m:41s remains)
INFO - root - 2017-12-08 00:24:10.838350: step 71450, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.658 sec/batch; 47h:43m:32s remains)
INFO - root - 2017-12-08 00:24:17.610225: step 71460, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 49h:37m:22s remains)
INFO - root - 2017-12-08 00:24:24.514667: step 71470, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.689 sec/batch; 49h:58m:54s remains)
INFO - root - 2017-12-08 00:24:31.276619: step 71480, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 50h:09m:23s remains)
INFO - root - 2017-12-08 00:24:38.015516: step 71490, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 47h:10m:26s remains)
INFO - root - 2017-12-08 00:24:44.757330: step 71500, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.660 sec/batch; 47h:49m:13s remains)
2017-12-08 00:24:45.552663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3078303 -4.307126 -4.2999296 -4.2845106 -4.2671795 -4.2597013 -4.2686625 -4.2851024 -4.2998695 -4.3072352 -4.3062067 -4.2996607 -4.2958455 -4.296721 -4.2975926][-4.3102422 -4.3064075 -4.2913814 -4.26096 -4.2281666 -4.2136116 -4.22733 -4.2572951 -4.2868824 -4.3063073 -4.3118186 -4.3057961 -4.3007979 -4.30043 -4.3002858][-4.3110895 -4.3030119 -4.2782822 -4.2298894 -4.17651 -4.1499276 -4.1652975 -4.209928 -4.2570615 -4.2944083 -4.313611 -4.3123808 -4.3074317 -4.3054209 -4.3033676][-4.3134689 -4.301322 -4.2684641 -4.2031441 -4.1276674 -4.0864754 -4.10118 -4.1584635 -4.2201738 -4.2737317 -4.3082623 -4.3151083 -4.3120985 -4.3082137 -4.3045344][-4.3137174 -4.3001747 -4.2658358 -4.1894956 -4.0927005 -4.0332956 -4.0428543 -4.1098986 -4.1818252 -4.2441244 -4.2909851 -4.3094482 -4.3113828 -4.3070483 -4.3023105][-4.3065419 -4.2960005 -4.2655735 -4.1849723 -4.0698466 -3.9849041 -3.9790273 -4.0514321 -4.1362386 -4.2073851 -4.2637429 -4.2947993 -4.3042908 -4.3028312 -4.2988153][-4.2876573 -4.282867 -4.2606239 -4.1835551 -4.0556879 -3.9412327 -3.9045143 -3.9737341 -4.0757661 -4.162797 -4.2299294 -4.27464 -4.29443 -4.2984257 -4.2966318][-4.2618623 -4.2649198 -4.2511716 -4.182456 -4.05252 -3.9149914 -3.8442643 -3.9024086 -4.0230389 -4.1311221 -4.2100825 -4.2651024 -4.293746 -4.3018565 -4.3007188][-4.2411475 -4.2526855 -4.24687 -4.1922975 -4.0753956 -3.9357076 -3.852304 -3.8988576 -4.0213895 -4.1352859 -4.2147746 -4.2693272 -4.298429 -4.3071947 -4.3062563][-4.2465796 -4.2605805 -4.2575727 -4.2162557 -4.1215324 -3.9966688 -3.917552 -3.9536867 -4.0586748 -4.1576028 -4.2238259 -4.2684603 -4.291081 -4.2999349 -4.30422][-4.2712522 -4.2803645 -4.2764707 -4.24586 -4.1734667 -4.0695868 -4.001328 -4.02475 -4.1064591 -4.1845813 -4.2315845 -4.2586913 -4.2693968 -4.2767406 -4.2895575][-4.2981138 -4.302649 -4.3002048 -4.2799773 -4.2266121 -4.1477227 -4.0945792 -4.1048369 -4.16092 -4.214354 -4.2365217 -4.2396436 -4.2335157 -4.2351756 -4.2573404][-4.3175955 -4.3203421 -4.3201685 -4.3106909 -4.2778363 -4.2271047 -4.1908774 -4.1924405 -4.2256875 -4.249711 -4.2389932 -4.2088637 -4.18062 -4.1746855 -4.208591][-4.3251777 -4.3273048 -4.3294134 -4.3276844 -4.31234 -4.2850542 -4.264379 -4.2624469 -4.2788992 -4.27747 -4.2344332 -4.1689973 -4.1199117 -4.1111755 -4.1596131][-4.3161011 -4.31643 -4.320868 -4.3239937 -4.3205085 -4.30767 -4.297008 -4.2957563 -4.3036089 -4.2876682 -4.2206106 -4.1227603 -4.0524421 -4.0439172 -4.1101866]]...]
INFO - root - 2017-12-08 00:24:52.074826: step 71510, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.687 sec/batch; 49h:46m:20s remains)
INFO - root - 2017-12-08 00:24:58.774567: step 71520, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 47h:14m:18s remains)
INFO - root - 2017-12-08 00:25:05.560758: step 71530, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.648 sec/batch; 46h:59m:47s remains)
INFO - root - 2017-12-08 00:25:12.325992: step 71540, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 51h:14m:38s remains)
INFO - root - 2017-12-08 00:25:19.106883: step 71550, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:43m:15s remains)
INFO - root - 2017-12-08 00:25:25.874145: step 71560, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 49h:27m:58s remains)
INFO - root - 2017-12-08 00:25:32.606387: step 71570, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 46h:59m:01s remains)
INFO - root - 2017-12-08 00:25:39.264991: step 71580, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 50h:56m:28s remains)
INFO - root - 2017-12-08 00:25:46.162410: step 71590, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 50h:47m:38s remains)
INFO - root - 2017-12-08 00:25:52.958706: step 71600, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 50h:22m:08s remains)
2017-12-08 00:25:53.694040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3147311 -4.3134508 -4.3127718 -4.3136387 -4.3150086 -4.3139896 -4.3085136 -4.3067746 -4.3106742 -4.3125072 -4.3134274 -4.3161273 -4.3152261 -4.3102646 -4.3065381][-4.3035884 -4.3005872 -4.3008051 -4.3040447 -4.3077841 -4.3073983 -4.2980847 -4.2957759 -4.30358 -4.3067565 -4.3093719 -4.3138385 -4.3114972 -4.3006721 -4.2914038][-4.28683 -4.2810478 -4.2805619 -4.2857761 -4.2918668 -4.29079 -4.2771974 -4.2749586 -4.2890229 -4.2964616 -4.3051009 -4.3163147 -4.314909 -4.2975445 -4.2765431][-4.2713609 -4.2636642 -4.2646008 -4.2703185 -4.272891 -4.2629538 -4.2415934 -4.2379107 -4.2585158 -4.2759414 -4.2983193 -4.3204803 -4.321569 -4.2981424 -4.2635083][-4.2553749 -4.244031 -4.2451854 -4.2502456 -4.2434573 -4.2172232 -4.1778145 -4.1696877 -4.2017112 -4.2392573 -4.2784395 -4.3116937 -4.3157334 -4.2914233 -4.2482443][-4.2428913 -4.2248645 -4.2219605 -4.223597 -4.2027049 -4.15438 -4.0906773 -4.0703058 -4.1198087 -4.18644 -4.2425132 -4.2794271 -4.2845812 -4.2676096 -4.2309375][-4.2426052 -4.2164226 -4.2045693 -4.19448 -4.157331 -4.080555 -3.9833238 -3.9470854 -4.0209918 -4.1229777 -4.1970181 -4.2380204 -4.2505097 -4.2469459 -4.2247295][-4.2515526 -4.2212296 -4.2004833 -4.1748228 -4.1193118 -4.0128865 -3.8784809 -3.8256869 -3.933131 -4.0719023 -4.1602178 -4.2080641 -4.231576 -4.2423596 -4.2343817][-4.2644892 -4.2398272 -4.2189307 -4.1872663 -4.1272449 -4.023035 -3.8904648 -3.8474793 -3.96107 -4.09276 -4.1718225 -4.2157736 -4.2427006 -4.2589335 -4.2573161][-4.2720346 -4.2562556 -4.2428365 -4.218472 -4.174983 -4.097208 -4.0011535 -3.9890716 -4.0803318 -4.1669526 -4.2169538 -4.2499967 -4.2711983 -4.2844586 -4.2817049][-4.2657228 -4.2553625 -4.2467442 -4.2298527 -4.2008314 -4.1491742 -4.0943336 -4.1072025 -4.177578 -4.22845 -4.2576613 -4.276628 -4.286356 -4.2912335 -4.2829061][-4.2594185 -4.2553844 -4.2525282 -4.2438807 -4.2289548 -4.1971707 -4.1704164 -4.1938481 -4.2450228 -4.275918 -4.2948647 -4.304544 -4.3036456 -4.3000312 -4.2833691][-4.2633429 -4.2659173 -4.2700448 -4.2694917 -4.2656517 -4.2473516 -4.2321072 -4.2517052 -4.2842388 -4.3037882 -4.3164058 -4.319756 -4.313086 -4.3047972 -4.2827463][-4.2798519 -4.2843871 -4.2906542 -4.2927837 -4.2919993 -4.2786326 -4.2663417 -4.277276 -4.2968831 -4.3079853 -4.3125072 -4.3105154 -4.3057508 -4.2997093 -4.2789435][-4.2949572 -4.2955723 -4.2964692 -4.2950821 -4.2924476 -4.2809768 -4.2703466 -4.2774062 -4.2890224 -4.2924929 -4.2917905 -4.2887073 -4.28751 -4.284575 -4.270225]]...]
INFO - root - 2017-12-08 00:26:00.316883: step 71610, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.690 sec/batch; 50h:01m:56s remains)
INFO - root - 2017-12-08 00:26:07.084239: step 71620, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 48h:53m:55s remains)
INFO - root - 2017-12-08 00:26:13.864742: step 71630, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 49h:39m:23s remains)
INFO - root - 2017-12-08 00:26:20.715791: step 71640, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 50h:42m:17s remains)
INFO - root - 2017-12-08 00:26:27.508362: step 71650, loss = 2.03, batch loss = 1.98 (12.2 examples/sec; 0.656 sec/batch; 47h:29m:54s remains)
INFO - root - 2017-12-08 00:26:34.248576: step 71660, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 51h:08m:48s remains)
INFO - root - 2017-12-08 00:26:41.068983: step 71670, loss = 2.07, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:24m:15s remains)
INFO - root - 2017-12-08 00:26:47.912080: step 71680, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.694 sec/batch; 50h:15m:12s remains)
INFO - root - 2017-12-08 00:26:54.763765: step 71690, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 46h:55m:33s remains)
INFO - root - 2017-12-08 00:27:01.490303: step 71700, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 46h:58m:46s remains)
2017-12-08 00:27:02.186901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2402692 -4.2345076 -4.2360907 -4.2401247 -4.2460203 -4.2498307 -4.2472177 -4.2329221 -4.2144008 -4.196157 -4.1809759 -4.1837854 -4.2054873 -4.2271833 -4.2427487][-4.2181826 -4.2089348 -4.2105117 -4.2196093 -4.2292018 -4.2345557 -4.2270188 -4.2029152 -4.1749911 -4.1557508 -4.1440129 -4.1496782 -4.1714754 -4.194396 -4.2120118][-4.2055011 -4.1893215 -4.188139 -4.1982064 -4.2040296 -4.20758 -4.19367 -4.1611881 -4.1225715 -4.1064734 -4.1069794 -4.1198874 -4.1442184 -4.17152 -4.1927323][-4.1903453 -4.1639934 -4.1542053 -4.1590543 -4.1590476 -4.159039 -4.1368918 -4.0885868 -4.0304251 -4.0213032 -4.0427432 -4.0668979 -4.1010308 -4.1401534 -4.1725731][-4.1844225 -4.1481628 -4.129962 -4.1299195 -4.1174583 -4.1003942 -4.0603347 -3.98627 -3.9043474 -3.9149513 -3.9694471 -4.0139637 -4.0641623 -4.1153607 -4.1586509][-4.1924605 -4.1503744 -4.1213036 -4.1096168 -4.0799122 -4.0459566 -3.990634 -3.8931434 -3.7909737 -3.8254647 -3.9166985 -3.9858294 -4.0516386 -4.1116543 -4.1584992][-4.2030983 -4.1581397 -4.1210723 -4.101388 -4.0633917 -4.0251346 -3.97293 -3.8824744 -3.7880764 -3.8283331 -3.9221666 -3.9938896 -4.0617695 -4.1213026 -4.1675076][-4.2111015 -4.1684542 -4.1365209 -4.1147146 -4.07717 -4.0449438 -4.0119171 -3.9540491 -3.8859925 -3.9102571 -3.9733489 -4.0261288 -4.0828118 -4.1403251 -4.1884871][-4.222249 -4.18281 -4.1568971 -4.1337256 -4.1001773 -4.0786552 -4.0641489 -4.0384679 -3.9929919 -3.9978013 -4.0309019 -4.0644941 -4.1128225 -4.1693373 -4.21789][-4.2440176 -4.2109776 -4.1915526 -4.1738405 -4.152297 -4.1408062 -4.13691 -4.1286435 -4.0944738 -4.0882192 -4.1035748 -4.1269927 -4.1668186 -4.2129774 -4.2531619][-4.2624984 -4.2416658 -4.2323194 -4.2226858 -4.209094 -4.2007418 -4.1987696 -4.1990666 -4.1745429 -4.1664577 -4.1778526 -4.1956406 -4.2247081 -4.2557578 -4.2850571][-4.2815628 -4.2699366 -4.2643518 -4.2607903 -4.2532349 -4.2464452 -4.2462406 -4.2526436 -4.2372465 -4.2297091 -4.2410107 -4.2543354 -4.2717524 -4.2923265 -4.3122649][-4.2986751 -4.2928963 -4.2898121 -4.2885647 -4.2853522 -4.2822537 -4.2843022 -4.2896385 -4.2796421 -4.2717528 -4.2784162 -4.2863946 -4.2975354 -4.3127789 -4.3279166][-4.3107862 -4.3071585 -4.3055058 -4.3053236 -4.3036923 -4.3037381 -4.3069849 -4.3087654 -4.3027587 -4.2969255 -4.2989507 -4.3027806 -4.309504 -4.3202991 -4.3318577][-4.3279886 -4.3262086 -4.3251843 -4.3256927 -4.3254976 -4.3262696 -4.3283653 -4.3289256 -4.326376 -4.3234472 -4.3234477 -4.3241649 -4.3263068 -4.3313227 -4.338151]]...]
INFO - root - 2017-12-08 00:27:08.812742: step 71710, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 49h:21m:30s remains)
INFO - root - 2017-12-08 00:27:15.657886: step 71720, loss = 2.02, batch loss = 1.96 (12.5 examples/sec; 0.641 sec/batch; 46h:25m:31s remains)
INFO - root - 2017-12-08 00:27:22.513210: step 71730, loss = 2.10, batch loss = 2.04 (12.4 examples/sec; 0.643 sec/batch; 46h:33m:49s remains)
INFO - root - 2017-12-08 00:27:29.322714: step 71740, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.697 sec/batch; 50h:31m:14s remains)
INFO - root - 2017-12-08 00:27:36.211251: step 71750, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.728 sec/batch; 52h:42m:21s remains)
INFO - root - 2017-12-08 00:27:43.055273: step 71760, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 49h:44m:47s remains)
INFO - root - 2017-12-08 00:27:49.846113: step 71770, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 45h:53m:45s remains)
INFO - root - 2017-12-08 00:27:56.774924: step 71780, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.674 sec/batch; 48h:47m:43s remains)
INFO - root - 2017-12-08 00:28:03.618134: step 71790, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:24m:48s remains)
INFO - root - 2017-12-08 00:28:10.346225: step 71800, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.709 sec/batch; 51h:19m:13s remains)
2017-12-08 00:28:11.144257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1797485 -4.1344724 -4.1285338 -4.1604953 -4.2062793 -4.2328215 -4.2333694 -4.1900997 -4.115705 -4.0398512 -3.99921 -4.0022554 -4.0434227 -4.1026134 -4.1516776][-4.1943331 -4.1429577 -4.1368155 -4.1783385 -4.2350612 -4.2662687 -4.2665014 -4.2205377 -4.1484494 -4.0801411 -4.0418506 -4.0347362 -4.0620451 -4.1182637 -4.173419][-4.2092328 -4.1571579 -4.1495972 -4.1903129 -4.2498512 -4.2842388 -4.2820191 -4.2397232 -4.1763678 -4.1211891 -4.0879555 -4.0691628 -4.0800214 -4.1289287 -4.1860418][-4.2217689 -4.1750803 -4.1650963 -4.1973634 -4.2471633 -4.2768259 -4.271708 -4.2336507 -4.1805081 -4.1353192 -4.1019483 -4.0740309 -4.0763822 -4.1196952 -4.1783652][-4.231318 -4.1912069 -4.175456 -4.1930618 -4.2253804 -4.2443142 -4.2405262 -4.2079439 -4.1626267 -4.1170573 -4.0806165 -4.0560875 -4.0616064 -4.1025739 -4.1608214][-4.2354422 -4.1981487 -4.1772609 -4.1785674 -4.1889672 -4.1893072 -4.1829276 -4.1568146 -4.1169109 -4.0737467 -4.0437202 -4.0320539 -4.0384955 -4.0709438 -4.1286125][-4.248961 -4.2157369 -4.1901231 -4.1716957 -4.1554432 -4.1368341 -4.1246319 -4.1016917 -4.0661869 -4.0288219 -4.0105839 -4.0059967 -4.0050907 -4.0308423 -4.0915847][-4.2661943 -4.2375889 -4.2078323 -4.1756153 -4.1431217 -4.1169391 -4.1006012 -4.0746226 -4.0401545 -4.017323 -4.020206 -4.0204544 -4.0133142 -4.0315356 -4.0803246][-4.2675962 -4.2413416 -4.2132506 -4.181375 -4.1517439 -4.1279325 -4.1066179 -4.073019 -4.0395656 -4.0398617 -4.0650682 -4.0784483 -4.0779419 -4.0879884 -4.1086588][-4.2633543 -4.2361288 -4.2104311 -4.1892672 -4.1733932 -4.1594071 -4.14017 -4.1082683 -4.0814533 -4.0915575 -4.1186023 -4.136498 -4.1398649 -4.1447296 -4.1506071][-4.2503371 -4.2248468 -4.2085977 -4.2023959 -4.2067108 -4.2121077 -4.200995 -4.1763425 -4.1570587 -4.1604991 -4.1744704 -4.1824517 -4.1804109 -4.1806688 -4.1828728][-4.2348127 -4.2139511 -4.209301 -4.2164183 -4.2363133 -4.2580953 -4.2558475 -4.2411451 -4.2293315 -4.2258396 -4.2238035 -4.2190952 -4.2129884 -4.20542 -4.202816][-4.2362814 -4.21723 -4.2127471 -4.2226071 -4.2454219 -4.2694244 -4.2728949 -4.2668996 -4.2621937 -4.260304 -4.257751 -4.2525463 -4.2463036 -4.2359543 -4.2269049][-4.2391672 -4.2190914 -4.2120776 -4.2210312 -4.2392612 -4.2578034 -4.2601695 -4.2588496 -4.2642584 -4.2700372 -4.2723393 -4.2663636 -4.258297 -4.2523131 -4.2409992][-4.2216229 -4.2050772 -4.1992941 -4.2050557 -4.21837 -4.2294979 -4.22835 -4.2314482 -4.2439 -4.2571392 -4.2568626 -4.2400136 -4.2315216 -4.23508 -4.226819]]...]
INFO - root - 2017-12-08 00:28:17.762168: step 71810, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 49h:35m:02s remains)
INFO - root - 2017-12-08 00:28:24.585437: step 71820, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 49h:19m:58s remains)
INFO - root - 2017-12-08 00:28:31.456028: step 71830, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 49h:07m:59s remains)
INFO - root - 2017-12-08 00:28:38.260534: step 71840, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 48h:38m:43s remains)
INFO - root - 2017-12-08 00:28:45.162181: step 71850, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 52h:05m:52s remains)
INFO - root - 2017-12-08 00:28:52.149199: step 71860, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 50h:53m:49s remains)
INFO - root - 2017-12-08 00:28:58.998763: step 71870, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.711 sec/batch; 51h:27m:39s remains)
INFO - root - 2017-12-08 00:29:05.776239: step 71880, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.667 sec/batch; 48h:17m:22s remains)
INFO - root - 2017-12-08 00:29:12.417315: step 71890, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:38m:12s remains)
INFO - root - 2017-12-08 00:29:19.389945: step 71900, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 51h:57m:09s remains)
2017-12-08 00:29:20.097631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3491459 -4.3465991 -4.344275 -4.3433013 -4.3438759 -4.34639 -4.3495011 -4.351872 -4.3521724 -4.3514137 -4.3513117 -4.3510294 -4.3508997 -4.3521123 -4.35375][-4.3465905 -4.3417029 -4.3380923 -4.3364515 -4.336668 -4.3403611 -4.3458567 -4.3503256 -4.3510327 -4.3485265 -4.3478518 -4.3476324 -4.346622 -4.3478441 -4.3508759][-4.3264565 -4.3218575 -4.3157058 -4.3116479 -4.309536 -4.3124766 -4.3193707 -4.3275557 -4.3309412 -4.3281789 -4.3273306 -4.3282919 -4.3282204 -4.330348 -4.3365159][-4.2874475 -4.2849045 -4.2768493 -4.2694945 -4.2658811 -4.2674346 -4.2736244 -4.2828422 -4.2884469 -4.2857337 -4.2896957 -4.2966323 -4.3002672 -4.3049054 -4.3140907][-4.24237 -4.2443771 -4.236094 -4.2250485 -4.2204576 -4.2204232 -4.2208009 -4.2254119 -4.2299633 -4.2272773 -4.2399597 -4.2601137 -4.273025 -4.2821741 -4.2927318][-4.201158 -4.2084813 -4.19945 -4.1854949 -4.17687 -4.1703615 -4.162313 -4.1621771 -4.1630683 -4.1608191 -4.17976 -4.2152619 -4.2379656 -4.2537179 -4.2664948][-4.1529717 -4.1656528 -4.1568069 -4.1410885 -4.1270747 -4.1143351 -4.0989232 -4.0940228 -4.0908356 -4.0885067 -4.1144123 -4.16218 -4.1975474 -4.2225304 -4.2392745][-4.1002688 -4.113143 -4.1097755 -4.0948048 -4.0768309 -4.0590262 -4.0388889 -4.0277538 -4.0158968 -4.0160184 -4.0503726 -4.1092176 -4.1581926 -4.1959281 -4.2180281][-4.0924406 -4.0861564 -4.0815058 -4.072084 -4.0582056 -4.0434628 -4.0258894 -4.0087833 -3.989145 -3.98984 -4.0222778 -4.0779672 -4.1337891 -4.1812706 -4.2074585][-4.1171188 -4.0921249 -4.0861859 -4.0848546 -4.0800605 -4.0745769 -4.0647492 -4.0537815 -4.0395303 -4.0453305 -4.0662565 -4.1012187 -4.1465778 -4.1888356 -4.2097659][-4.1553106 -4.1334815 -4.1271315 -4.1274357 -4.127604 -4.126935 -4.1182356 -4.1140537 -4.1149368 -4.1307249 -4.1462979 -4.1584411 -4.1832871 -4.2104421 -4.225338][-4.2023149 -4.1828108 -4.1700244 -4.1640782 -4.1611633 -4.155848 -4.14131 -4.1410093 -4.1538167 -4.1821551 -4.2030678 -4.2052169 -4.2137012 -4.2285485 -4.23714][-4.2352791 -4.2199521 -4.2005763 -4.1893725 -4.1834178 -4.174304 -4.1550574 -4.1485853 -4.1656113 -4.2043209 -4.2313237 -4.2296581 -4.2299681 -4.2352443 -4.2386217][-4.240809 -4.2342162 -4.2261043 -4.2166929 -4.2113671 -4.2038794 -4.1808133 -4.16432 -4.1758 -4.2100706 -4.2351813 -4.2341022 -4.23306 -4.2277136 -4.2282662][-4.2323995 -4.2312346 -4.2332277 -4.2287884 -4.2267952 -4.220953 -4.196661 -4.172411 -4.1765985 -4.1969438 -4.2147856 -4.21477 -4.2176132 -4.2135105 -4.2186]]...]
INFO - root - 2017-12-08 00:29:26.505088: step 71910, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 46h:24m:30s remains)
INFO - root - 2017-12-08 00:29:33.227969: step 71920, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 50h:13m:20s remains)
INFO - root - 2017-12-08 00:29:39.977867: step 71930, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 50h:20m:36s remains)
INFO - root - 2017-12-08 00:29:46.694521: step 71940, loss = 2.03, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 50h:19m:57s remains)
INFO - root - 2017-12-08 00:29:53.365759: step 71950, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.639 sec/batch; 46h:16m:59s remains)
INFO - root - 2017-12-08 00:30:00.176582: step 71960, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 47h:09m:32s remains)
INFO - root - 2017-12-08 00:30:06.860493: step 71970, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 47h:24m:51s remains)
INFO - root - 2017-12-08 00:30:13.686145: step 71980, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 52h:13m:47s remains)
INFO - root - 2017-12-08 00:30:20.517779: step 71990, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 0.774 sec/batch; 55h:58m:29s remains)
INFO - root - 2017-12-08 00:30:27.379933: step 72000, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 47h:10m:02s remains)
2017-12-08 00:30:28.160815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1938143 -4.2108231 -4.2285695 -4.2388635 -4.2272472 -4.2033587 -4.2027245 -4.2107649 -4.2081957 -4.2112122 -4.2124414 -4.207778 -4.2015834 -4.1920247 -4.1964412][-4.2063437 -4.2208123 -4.2385044 -4.2459259 -4.2227292 -4.1851187 -4.1852493 -4.201437 -4.1966515 -4.1995955 -4.2055511 -4.208035 -4.2021446 -4.1907668 -4.1997018][-4.210577 -4.2119994 -4.2187428 -4.2131667 -4.176383 -4.1253982 -4.1279759 -4.1573014 -4.1622758 -4.1705093 -4.1839318 -4.19745 -4.1969981 -4.1896615 -4.1997895][-4.20938 -4.2054472 -4.204227 -4.1865458 -4.1417727 -4.0838428 -4.0783577 -4.1134415 -4.1329861 -4.1533737 -4.1737437 -4.1933937 -4.19823 -4.1942673 -4.200501][-4.1940222 -4.1997366 -4.2017045 -4.1827807 -4.1446776 -4.0978255 -4.0859885 -4.1124039 -4.1381588 -4.1664157 -4.1819048 -4.19218 -4.1947346 -4.1929936 -4.1957211][-4.1617341 -4.1870489 -4.1918449 -4.1717634 -4.141202 -4.112556 -4.1002107 -4.1104007 -4.1359611 -4.1704717 -4.1815495 -4.1789737 -4.1749525 -4.1676755 -4.1647892][-4.1416645 -4.1730466 -4.1699786 -4.1346107 -4.0923886 -4.065536 -4.056 -4.0634761 -4.0968347 -4.1445031 -4.1608109 -4.1500216 -4.1392951 -4.1242065 -4.1117964][-4.1412015 -4.1609421 -4.1372066 -4.07889 -4.0056353 -3.9644835 -3.9645321 -3.9846852 -4.0328441 -4.0998459 -4.1337996 -4.1276836 -4.1162796 -4.0987844 -4.080296][-4.1529608 -4.16527 -4.1376081 -4.0769439 -3.9952738 -3.9426053 -3.9475849 -3.9733424 -4.0214596 -4.0895739 -4.1302471 -4.12778 -4.1195569 -4.1087584 -4.0871][-4.1499724 -4.1685896 -4.1609468 -4.1233859 -4.060606 -4.0148439 -4.0147519 -4.0349011 -4.0650582 -4.11195 -4.1448803 -4.1425571 -4.1387248 -4.1384635 -4.117928][-4.1247916 -4.1514463 -4.1648712 -4.1531348 -4.1106224 -4.0688257 -4.0636539 -4.0818686 -4.0996189 -4.128027 -4.1596975 -4.1614466 -4.1576247 -4.1575694 -4.1402059][-4.109468 -4.1404505 -4.1667528 -4.1657491 -4.1308517 -4.0878663 -4.0750203 -4.094275 -4.1118979 -4.1326623 -4.1630592 -4.1645508 -4.1578145 -4.1536641 -4.1382875][-4.1202888 -4.1524348 -4.1839805 -4.1857123 -4.1553206 -4.1132107 -4.0949874 -4.1114917 -4.1269326 -4.1413536 -4.159173 -4.1573906 -4.1524096 -4.1480837 -4.1344][-4.1435938 -4.17013 -4.1981006 -4.2010832 -4.1738777 -4.1385207 -4.1244802 -4.1418171 -4.1549544 -4.1621013 -4.163064 -4.1588054 -4.1560769 -4.1541123 -4.1481318][-4.1612616 -4.1777248 -4.1964955 -4.1966896 -4.1772814 -4.1565876 -4.1553011 -4.1772227 -4.186677 -4.1821256 -4.1710157 -4.1655078 -4.1657882 -4.1652493 -4.1657243]]...]
INFO - root - 2017-12-08 00:30:34.733871: step 72010, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:29m:38s remains)
INFO - root - 2017-12-08 00:30:41.512752: step 72020, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 50h:03m:31s remains)
INFO - root - 2017-12-08 00:30:48.223810: step 72030, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 47h:53m:07s remains)
INFO - root - 2017-12-08 00:30:54.885855: step 72040, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 48h:47m:37s remains)
INFO - root - 2017-12-08 00:31:01.669102: step 72050, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 46h:44m:05s remains)
INFO - root - 2017-12-08 00:31:08.536971: step 72060, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.742 sec/batch; 53h:41m:29s remains)
INFO - root - 2017-12-08 00:31:15.309846: step 72070, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.685 sec/batch; 49h:32m:49s remains)
INFO - root - 2017-12-08 00:31:22.017444: step 72080, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 47h:56m:55s remains)
INFO - root - 2017-12-08 00:31:28.788409: step 72090, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 45h:41m:38s remains)
INFO - root - 2017-12-08 00:31:35.702712: step 72100, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 48h:01m:32s remains)
2017-12-08 00:31:36.449548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25114 -4.2497597 -4.252563 -4.2549553 -4.2508698 -4.2450166 -4.2427177 -4.2532749 -4.2721572 -4.279336 -4.2797275 -4.2741885 -4.2689786 -4.2686486 -4.2623649][-4.2171073 -4.2080274 -4.2056003 -4.1962848 -4.1835113 -4.1757569 -4.1741381 -4.1900392 -4.213963 -4.2225718 -4.2267089 -4.2265954 -4.22736 -4.2282872 -4.2188053][-4.192503 -4.1767316 -4.1706171 -4.1510286 -4.1241512 -4.1081433 -4.1030555 -4.11907 -4.1422491 -4.1466079 -4.1515813 -4.1581969 -4.1675615 -4.1713653 -4.1623712][-4.1849337 -4.16444 -4.1518297 -4.12075 -4.0770464 -4.0438752 -4.0264773 -4.0374122 -4.0575981 -4.0595226 -4.0675735 -4.0852852 -4.1014915 -4.1110229 -4.111095][-4.19182 -4.1622105 -4.1349497 -4.0873632 -4.0261288 -3.974998 -3.9425864 -3.9504235 -3.9780409 -3.9896297 -4.00658 -4.0350637 -4.0571542 -4.0765948 -4.0898514][-4.2041755 -4.1662831 -4.1230607 -4.0568342 -3.9735358 -3.9009798 -3.8444982 -3.8447411 -3.8907709 -3.922667 -3.9535661 -3.9991231 -4.0373926 -4.0662284 -4.0872688][-4.2218351 -4.182765 -4.13544 -4.0606685 -3.9654233 -3.8802085 -3.8003604 -3.7819777 -3.8320079 -3.871151 -3.9097445 -3.9734867 -4.0306416 -4.0677094 -4.0893784][-4.235239 -4.1979809 -4.1509776 -4.0823245 -4.0009232 -3.9367456 -3.8714209 -3.8490806 -3.8848443 -3.9098916 -3.9356039 -3.9928172 -4.049201 -4.0876732 -4.1062393][-4.2274904 -4.1938658 -4.1520758 -4.095058 -4.0392551 -4.0083966 -3.9763291 -3.97456 -4.0097103 -4.0262151 -4.0344081 -4.0592513 -4.0943527 -4.1268382 -4.1413302][-4.2122641 -4.1850591 -4.1537018 -4.1083741 -4.0702868 -4.0642524 -4.0560846 -4.0736036 -4.1141915 -4.134654 -4.135859 -4.1423578 -4.1613159 -4.1864085 -4.1967669][-4.2075863 -4.1863441 -4.1681743 -4.1409945 -4.1233072 -4.1320734 -4.1319289 -4.1486635 -4.1808023 -4.1973963 -4.2017436 -4.2044024 -4.219429 -4.2391839 -4.247982][-4.2193861 -4.20652 -4.2009196 -4.1931195 -4.1909842 -4.20259 -4.2031531 -4.2121668 -4.2291031 -4.2335896 -4.2349005 -4.2370157 -4.2522125 -4.2681003 -4.2748752][-4.2362294 -4.230238 -4.2324104 -4.2355251 -4.2392249 -4.2455254 -4.2425718 -4.2443051 -4.2499151 -4.2495985 -4.2513914 -4.2571154 -4.2742391 -4.2911181 -4.297462][-4.2509594 -4.2453671 -4.2477388 -4.2526612 -4.2552848 -4.2556419 -4.2517638 -4.2527666 -4.256434 -4.2567439 -4.2596135 -4.2685928 -4.286644 -4.3045578 -4.3120608][-4.2699294 -4.2617879 -4.260047 -4.2604976 -4.261188 -4.2612829 -4.2594771 -4.2607174 -4.2631636 -4.2655673 -4.2697124 -4.278851 -4.2941403 -4.3086686 -4.3157644]]...]
INFO - root - 2017-12-08 00:31:43.099565: step 72110, loss = 2.06, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 48h:58m:16s remains)
INFO - root - 2017-12-08 00:31:49.827236: step 72120, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 46h:39m:52s remains)
INFO - root - 2017-12-08 00:31:56.657651: step 72130, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.704 sec/batch; 50h:55m:42s remains)
INFO - root - 2017-12-08 00:32:03.423518: step 72140, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 49h:54m:20s remains)
INFO - root - 2017-12-08 00:32:10.135729: step 72150, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 48h:02m:15s remains)
INFO - root - 2017-12-08 00:32:16.951565: step 72160, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 48h:17m:40s remains)
INFO - root - 2017-12-08 00:32:23.730483: step 72170, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 48h:44m:23s remains)
INFO - root - 2017-12-08 00:32:30.540767: step 72180, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 51h:20m:43s remains)
INFO - root - 2017-12-08 00:32:37.443836: step 72190, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.708 sec/batch; 51h:09m:46s remains)
INFO - root - 2017-12-08 00:32:43.991983: step 72200, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.636 sec/batch; 45h:58m:14s remains)
2017-12-08 00:32:44.758233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3455191 -4.3428273 -4.3345428 -4.3251328 -4.31509 -4.3055654 -4.2970138 -4.2905116 -4.2884922 -4.2912455 -4.2965875 -4.3050413 -4.3162508 -4.3269587 -4.3376904][-4.3480382 -4.3446627 -4.33513 -4.32378 -4.3102446 -4.2950854 -4.27881 -4.2662854 -4.2606554 -4.2613893 -4.26796 -4.2814789 -4.2984943 -4.3144622 -4.3304291][-4.348187 -4.3445997 -4.33511 -4.3226929 -4.3076348 -4.2877455 -4.2636356 -4.2456207 -4.2357144 -4.2330251 -4.2385736 -4.2560954 -4.2793188 -4.3017917 -4.3230495][-4.3490243 -4.345005 -4.3336911 -4.3183231 -4.3014231 -4.2778244 -4.2473116 -4.2264423 -4.2148862 -4.2097969 -4.21452 -4.2350035 -4.2640414 -4.292264 -4.3178053][-4.3488116 -4.3432574 -4.3282285 -4.3085737 -4.2889991 -4.2616315 -4.2275648 -4.2059884 -4.1944513 -4.1885414 -4.1934643 -4.2163687 -4.2508221 -4.2840509 -4.3127613][-4.3451219 -4.3370376 -4.319118 -4.2958956 -4.2749662 -4.2450557 -4.2086854 -4.1869235 -4.1774707 -4.1726818 -4.1779971 -4.2024469 -4.2405877 -4.2772202 -4.3081841][-4.3371115 -4.3272018 -4.3092008 -4.2852221 -4.2651539 -4.2360435 -4.19894 -4.1761208 -4.1664662 -4.1621704 -4.1683197 -4.1931496 -4.2329464 -4.2717938 -4.3040581][-4.3285093 -4.3165903 -4.2983284 -4.275846 -4.2584987 -4.2324476 -4.1980782 -4.1742668 -4.1633449 -4.15887 -4.1669888 -4.1912651 -4.2299738 -4.2684989 -4.3007593][-4.3235006 -4.3100691 -4.2931795 -4.2733889 -4.2583065 -4.2354231 -4.2041764 -4.1806865 -4.1693168 -4.1664119 -4.176899 -4.1996074 -4.2341094 -4.2698913 -4.3000512][-4.3243504 -4.3134594 -4.300745 -4.2852287 -4.272 -4.2505322 -4.2206984 -4.1974311 -4.1880579 -4.1879597 -4.1989422 -4.2173309 -4.2453294 -4.2765636 -4.3036027][-4.3261013 -4.3181138 -4.31034 -4.2998943 -4.2892733 -4.2701221 -4.243012 -4.2214994 -4.2138476 -4.2158484 -4.2264051 -4.2401209 -4.2621064 -4.2888732 -4.31157][-4.3303871 -4.3241048 -4.3190155 -4.3123026 -4.3043795 -4.2893615 -4.2681608 -4.2494769 -4.2421484 -4.2437534 -4.2530189 -4.2631083 -4.2798896 -4.3014073 -4.3200331][-4.3327432 -4.3281779 -4.3243246 -4.319653 -4.3137255 -4.3030663 -4.2886915 -4.2753048 -4.2694025 -4.2707839 -4.2773981 -4.2853413 -4.2977314 -4.3140965 -4.3286829][-4.3334794 -4.3308043 -4.3277464 -4.324028 -4.3195219 -4.3126583 -4.3038797 -4.296423 -4.2927117 -4.2937093 -4.2992148 -4.3056231 -4.3148775 -4.3267817 -4.3373461][-4.3344169 -4.3320584 -4.3287835 -4.3255754 -4.32196 -4.3170786 -4.3124185 -4.3094244 -4.3080664 -4.3100572 -4.3150053 -4.320354 -4.3278766 -4.3366065 -4.3443766]]...]
INFO - root - 2017-12-08 00:32:51.331353: step 72210, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.694 sec/batch; 50h:10m:24s remains)
INFO - root - 2017-12-08 00:32:58.174357: step 72220, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 49h:54m:00s remains)
INFO - root - 2017-12-08 00:33:05.022650: step 72230, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.677 sec/batch; 48h:55m:02s remains)
INFO - root - 2017-12-08 00:33:11.796149: step 72240, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 47h:43m:12s remains)
INFO - root - 2017-12-08 00:33:18.603837: step 72250, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:16m:00s remains)
INFO - root - 2017-12-08 00:33:25.492195: step 72260, loss = 2.03, batch loss = 1.97 (11.0 examples/sec; 0.729 sec/batch; 52h:40m:16s remains)
INFO - root - 2017-12-08 00:33:32.268267: step 72270, loss = 2.07, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 50h:16m:24s remains)
INFO - root - 2017-12-08 00:33:39.032063: step 72280, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 51h:04m:40s remains)
INFO - root - 2017-12-08 00:33:45.867988: step 72290, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 47h:34m:48s remains)
INFO - root - 2017-12-08 00:33:52.671809: step 72300, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.710 sec/batch; 51h:20m:19s remains)
2017-12-08 00:33:53.401465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.190661 -4.2103753 -4.2330179 -4.2482967 -4.2534761 -4.2478976 -4.229394 -4.2101483 -4.1998839 -4.1986837 -4.1995163 -4.2047486 -4.2155724 -4.2085867 -4.1822877][-4.1810293 -4.2129469 -4.2389789 -4.2505159 -4.2437792 -4.2245235 -4.1957049 -4.1705947 -4.1601181 -4.1636777 -4.1712418 -4.1802735 -4.192553 -4.1846046 -4.1566162][-4.1998916 -4.241961 -4.2696009 -4.2705169 -4.2454653 -4.2114677 -4.1751118 -4.1467457 -4.1408353 -4.1498294 -4.1633759 -4.1755691 -4.1863203 -4.1797395 -4.155405][-4.2281313 -4.273829 -4.3007464 -4.2919827 -4.2514062 -4.2022963 -4.1564226 -4.1253114 -4.12862 -4.1492729 -4.169322 -4.18264 -4.1925955 -4.1906376 -4.1773496][-4.2442136 -4.2875624 -4.31159 -4.2961149 -4.2447023 -4.1796656 -4.1207194 -4.0854492 -4.1016288 -4.1378341 -4.1633258 -4.1786442 -4.1901097 -4.1947508 -4.1954794][-4.2448053 -4.2806211 -4.295927 -4.2713733 -4.2090983 -4.1312447 -4.0584817 -4.0190439 -4.05005 -4.107821 -4.1453075 -4.1670823 -4.1796484 -4.1857867 -4.1911902][-4.2310228 -4.2590685 -4.2674828 -4.2353363 -4.1643825 -4.0776229 -3.9902222 -3.9425952 -3.99279 -4.0794697 -4.1375751 -4.1668935 -4.1748309 -4.172224 -4.1755414][-4.2065878 -4.2272563 -4.230443 -4.1962671 -4.1243057 -4.0395532 -3.9504888 -3.9039731 -3.9725277 -4.0780168 -4.1484227 -4.1793 -4.1780753 -4.166275 -4.1674747][-4.1822329 -4.1955962 -4.1927195 -4.1594338 -4.1009231 -4.0410233 -3.983608 -3.9634924 -4.0338554 -4.1274209 -4.1849031 -4.2052674 -4.1930895 -4.1779895 -4.1787682][-4.1682 -4.1755643 -4.1674175 -4.1410728 -4.1023178 -4.0664287 -4.0387197 -4.04557 -4.1128507 -4.1880627 -4.2299118 -4.2386765 -4.21509 -4.1959343 -4.1913524][-4.1695967 -4.1675653 -4.1565237 -4.1413493 -4.1193495 -4.0924215 -4.0752244 -4.0930357 -4.1532521 -4.217093 -4.2527146 -4.2533965 -4.2229209 -4.2001948 -4.1930027][-4.1824436 -4.1736827 -4.1626763 -4.1591797 -4.15257 -4.1322842 -4.1196222 -4.1362247 -4.180479 -4.2276654 -4.2518935 -4.2454357 -4.2122154 -4.187233 -4.1781483][-4.1941957 -4.1846771 -4.1801538 -4.1865635 -4.1862812 -4.1665697 -4.1544204 -4.1672993 -4.1978993 -4.2310934 -4.2488785 -4.2405338 -4.2100916 -4.1857061 -4.1750083][-4.1935773 -4.1844306 -4.1862431 -4.2032084 -4.210072 -4.1914754 -4.178081 -4.1888089 -4.2118039 -4.2376337 -4.2522039 -4.2460613 -4.2190032 -4.1961679 -4.1859608][-4.1732044 -4.1650152 -4.1705623 -4.1927762 -4.2064009 -4.1970692 -4.1924253 -4.2056255 -4.2261362 -4.247354 -4.2604322 -4.2560654 -4.2343869 -4.2149816 -4.205719]]...]
INFO - root - 2017-12-08 00:34:00.039821: step 72310, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 46h:33m:43s remains)
INFO - root - 2017-12-08 00:34:06.898915: step 72320, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.683 sec/batch; 49h:20m:35s remains)
INFO - root - 2017-12-08 00:34:13.663882: step 72330, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.723 sec/batch; 52h:13m:46s remains)
INFO - root - 2017-12-08 00:34:20.465724: step 72340, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.686 sec/batch; 49h:36m:13s remains)
INFO - root - 2017-12-08 00:34:27.306846: step 72350, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.668 sec/batch; 48h:16m:33s remains)
INFO - root - 2017-12-08 00:34:34.079224: step 72360, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 48h:13m:38s remains)
INFO - root - 2017-12-08 00:34:40.880530: step 72370, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 51h:40m:00s remains)
INFO - root - 2017-12-08 00:34:47.807933: step 72380, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.724 sec/batch; 52h:18m:14s remains)
INFO - root - 2017-12-08 00:34:54.557605: step 72390, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 49h:08m:46s remains)
INFO - root - 2017-12-08 00:35:01.317740: step 72400, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.650 sec/batch; 46h:56m:29s remains)
2017-12-08 00:35:02.053829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2070036 -4.2009935 -4.2018542 -4.2023473 -4.196589 -4.188323 -4.1853504 -4.1816583 -4.1739755 -4.1735754 -4.1833973 -4.202033 -4.212286 -4.2221842 -4.2321496][-4.2062178 -4.2013917 -4.2042809 -4.2083416 -4.2071095 -4.2020192 -4.2014961 -4.1994238 -4.1917386 -4.1833277 -4.1810055 -4.1889663 -4.1896524 -4.1977811 -4.2154932][-4.2067981 -4.2017031 -4.2066307 -4.2164426 -4.2233505 -4.2244306 -4.2272859 -4.2285318 -4.2193208 -4.2022429 -4.1878824 -4.1812048 -4.1691513 -4.1690788 -4.1915956][-4.21065 -4.2005796 -4.20033 -4.2083631 -4.218082 -4.2290955 -4.246768 -4.2595477 -4.2550955 -4.2355189 -4.2126813 -4.1925435 -4.1652765 -4.152216 -4.170845][-4.2050066 -4.185307 -4.172173 -4.1714067 -4.1782103 -4.1988168 -4.2373252 -4.2675624 -4.2744231 -4.2603474 -4.2381988 -4.213181 -4.1788468 -4.1550412 -4.1626472][-4.1946316 -4.1617246 -4.1334314 -4.1199455 -4.1181064 -4.1409225 -4.1906824 -4.2405 -4.2652574 -4.2606244 -4.2411771 -4.2199593 -4.1916151 -4.1688786 -4.16545][-4.1847544 -4.1417122 -4.1004682 -4.0711827 -4.0520825 -4.0577784 -4.0994987 -4.1632423 -4.21858 -4.2402081 -4.2354288 -4.2209449 -4.2018623 -4.1873055 -4.1761894][-4.1761246 -4.123311 -4.0696359 -4.021719 -3.9764678 -3.9532571 -3.9660408 -4.0282192 -4.1211543 -4.1928926 -4.2202969 -4.2205677 -4.2101321 -4.2048087 -4.1921091][-4.1873283 -4.13241 -4.0703459 -4.0033851 -3.9345202 -3.8924127 -3.8729315 -3.9074049 -4.018332 -4.1375189 -4.2027411 -4.2198315 -4.213768 -4.2121768 -4.2023168][-4.20417 -4.1523385 -4.0926418 -4.0222206 -3.9598951 -3.9425631 -3.9360781 -3.9468439 -4.0211787 -4.129961 -4.2035708 -4.2249451 -4.2156157 -4.2118945 -4.2035718][-4.2092185 -4.1655087 -4.1135664 -4.0511951 -4.0104465 -4.0258908 -4.0465646 -4.0521865 -4.0803113 -4.1463828 -4.2074695 -4.2258458 -4.2097611 -4.2017703 -4.19628][-4.2139664 -4.1782303 -4.1373458 -4.0943809 -4.0764422 -4.1015387 -4.129704 -4.1313019 -4.1274953 -4.1550255 -4.2008138 -4.2195125 -4.2009335 -4.1862121 -4.1817622][-4.2205067 -4.191968 -4.1579828 -4.1301618 -4.1242709 -4.143816 -4.1629648 -4.15876 -4.1412663 -4.149889 -4.1886673 -4.2135744 -4.2026486 -4.1876307 -4.1842556][-4.2367487 -4.2125368 -4.1804934 -4.1592021 -4.1582184 -4.1666503 -4.1689157 -4.1581903 -4.1433644 -4.1535015 -4.1944885 -4.2274594 -4.2290235 -4.2173696 -4.2137036][-4.2511334 -4.228497 -4.200048 -4.1877651 -4.1954064 -4.201004 -4.1927075 -4.1791391 -4.1693597 -4.1801214 -4.2192655 -4.2571273 -4.2665548 -4.2570429 -4.250504]]...]
INFO - root - 2017-12-08 00:35:08.674525: step 72410, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.703 sec/batch; 50h:48m:53s remains)
INFO - root - 2017-12-08 00:35:15.507088: step 72420, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 49h:46m:41s remains)
INFO - root - 2017-12-08 00:35:22.373804: step 72430, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 47h:23m:23s remains)
INFO - root - 2017-12-08 00:35:29.206847: step 72440, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.660 sec/batch; 47h:39m:35s remains)
INFO - root - 2017-12-08 00:35:36.045449: step 72450, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 49h:46m:55s remains)
INFO - root - 2017-12-08 00:35:42.905298: step 72460, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 50h:46m:15s remains)
INFO - root - 2017-12-08 00:35:49.621798: step 72470, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 48h:33m:46s remains)
INFO - root - 2017-12-08 00:35:56.353370: step 72480, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 46h:50m:21s remains)
INFO - root - 2017-12-08 00:36:03.198100: step 72490, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.679 sec/batch; 49h:04m:20s remains)
INFO - root - 2017-12-08 00:36:10.007681: step 72500, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 51h:55m:20s remains)
2017-12-08 00:36:10.792256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2681417 -4.2675676 -4.2617731 -4.257472 -4.2588506 -4.2671742 -4.2748175 -4.2798872 -4.2791181 -4.2708645 -4.2551508 -4.2348962 -4.2075415 -4.1861506 -4.1837392][-4.2602339 -4.2594171 -4.2554011 -4.2543454 -4.2550311 -4.2615485 -4.2680969 -4.2715907 -4.2699924 -4.2612233 -4.2426476 -4.2169895 -4.1790957 -4.1484513 -4.1440439][-4.2634082 -4.2572594 -4.2500496 -4.2475295 -4.2470479 -4.2510228 -4.2578807 -4.2609038 -4.2573066 -4.2464552 -4.226234 -4.2020321 -4.1594486 -4.11932 -4.1105075][-4.2752814 -4.2591276 -4.2454839 -4.24137 -4.2410479 -4.2430339 -4.2477989 -4.2510505 -4.2480226 -4.2357712 -4.214035 -4.1935592 -4.1563067 -4.1144056 -4.1023073][-4.2828627 -4.2536731 -4.2289491 -4.2242389 -4.225945 -4.2250638 -4.2243853 -4.2261958 -4.2273607 -4.2188029 -4.1974378 -4.1788135 -4.1464686 -4.1080818 -4.099144][-4.2707863 -4.2303643 -4.1943374 -4.1871829 -4.1892042 -4.1835818 -4.1767435 -4.1719241 -4.1768823 -4.1771994 -4.1605458 -4.1459341 -4.1197896 -4.0867038 -4.0844188][-4.2539511 -4.2069597 -4.1619015 -4.1452336 -4.1376357 -4.1248889 -4.1134443 -4.1028385 -4.1090155 -4.1169953 -4.11298 -4.1125393 -4.0996037 -4.07487 -4.077312][-4.2242808 -4.175209 -4.1284103 -4.1054659 -4.0892878 -4.0726213 -4.0627003 -4.0516257 -4.0561213 -4.064342 -4.0709805 -4.0874019 -4.0942526 -4.0889463 -4.09764][-4.1961961 -4.1456127 -4.1047363 -4.0865874 -4.0713353 -4.0592852 -4.0561423 -4.0493689 -4.0478764 -4.043735 -4.0468678 -4.0679879 -4.0875854 -4.0978479 -4.1129227][-4.180336 -4.1338973 -4.1051674 -4.1002626 -4.0910864 -4.080791 -4.0805993 -4.0790448 -4.0698977 -4.0502548 -4.0418844 -4.0555387 -4.0735455 -4.0884819 -4.1119647][-4.1818666 -4.1444974 -4.1265507 -4.1319847 -4.1281323 -4.1196604 -4.1212 -4.1218615 -4.1093245 -4.08645 -4.0729127 -4.0770621 -4.0842967 -4.0913754 -4.1126275][-4.2044158 -4.1822581 -4.1734734 -4.1819549 -4.1803532 -4.1783862 -4.1841164 -4.185194 -4.1727238 -4.1503539 -4.1358547 -4.1330652 -4.12889 -4.1222243 -4.1301637][-4.2388206 -4.2322497 -4.2336912 -4.2433352 -4.2431035 -4.2419262 -4.2448535 -4.2431512 -4.230597 -4.2135115 -4.2020469 -4.195745 -4.1855521 -4.1683545 -4.1611867][-4.273912 -4.2745972 -4.2829504 -4.29314 -4.2943187 -4.2894206 -4.2849865 -4.2804518 -4.2713375 -4.2606974 -4.2529497 -4.2483649 -4.2395859 -4.2198205 -4.20269][-4.3021712 -4.3049612 -4.3126278 -4.3195653 -4.3209095 -4.3151836 -4.3074751 -4.3020039 -4.2967162 -4.2911267 -4.2870121 -4.2859273 -4.2799683 -4.2633514 -4.2452383]]...]
INFO - root - 2017-12-08 00:36:17.082530: step 72510, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 48h:06m:02s remains)
INFO - root - 2017-12-08 00:36:23.916232: step 72520, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.681 sec/batch; 49h:10m:25s remains)
INFO - root - 2017-12-08 00:36:30.677513: step 72530, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 47h:12m:30s remains)
INFO - root - 2017-12-08 00:36:37.501828: step 72540, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 52h:13m:32s remains)
INFO - root - 2017-12-08 00:36:44.225130: step 72550, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.659 sec/batch; 47h:35m:59s remains)
INFO - root - 2017-12-08 00:36:51.086842: step 72560, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.676 sec/batch; 48h:48m:54s remains)
INFO - root - 2017-12-08 00:36:58.046679: step 72570, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.747 sec/batch; 53h:55m:25s remains)
INFO - root - 2017-12-08 00:37:04.970659: step 72580, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.693 sec/batch; 50h:01m:22s remains)
INFO - root - 2017-12-08 00:37:11.785390: step 72590, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.640 sec/batch; 46h:14m:13s remains)
INFO - root - 2017-12-08 00:37:18.611752: step 72600, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.645 sec/batch; 46h:32m:42s remains)
2017-12-08 00:37:19.445504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0966134 -4.1185875 -4.1183534 -4.0858321 -4.0467243 -4.0295658 -4.0406284 -4.0587521 -4.0723572 -4.0952191 -4.1233125 -4.1517053 -4.1688466 -4.1722803 -4.1636777][-4.0717554 -4.1043468 -4.1095667 -4.0804758 -4.0443416 -4.0297189 -4.0441971 -4.0619617 -4.0713758 -4.0886507 -4.109623 -4.1300521 -4.142477 -4.1469088 -4.1386719][-4.0516882 -4.09911 -4.1180158 -4.0981278 -4.0690413 -4.0579896 -4.0737724 -4.088685 -4.0935421 -4.1047039 -4.1169081 -4.1270838 -4.1348438 -4.1382031 -4.1265793][-4.0532432 -4.1073303 -4.1358743 -4.1225486 -4.0977664 -4.0911951 -4.1083226 -4.12211 -4.1251063 -4.1325417 -4.1368046 -4.1379237 -4.1422949 -4.1423917 -4.1268616][-4.076601 -4.1275315 -4.1526527 -4.13761 -4.1157541 -4.1092172 -4.1249356 -4.137682 -4.1430926 -4.1498537 -4.1499166 -4.1477509 -4.1529727 -4.1553726 -4.136179][-4.0680385 -4.1129227 -4.1275973 -4.1041703 -4.0807867 -4.0762286 -4.0915523 -4.1097741 -4.1257362 -4.1390409 -4.1403537 -4.1385436 -4.1457253 -4.1493225 -4.1258397][-4.0560117 -4.0947533 -4.1064029 -4.0795751 -4.0521879 -4.0522876 -4.0729933 -4.0994387 -4.1240759 -4.1413679 -4.1447811 -4.1398144 -4.1437669 -4.1444845 -4.1143088][-4.0839739 -4.1127596 -4.1206455 -4.0914636 -4.0616164 -4.071085 -4.1008124 -4.133141 -4.1586895 -4.1740751 -4.1781878 -4.1735873 -4.1751041 -4.1744323 -4.1471167][-4.1175718 -4.1360059 -4.1421227 -4.1189523 -4.0947366 -4.1118464 -4.147181 -4.1760316 -4.1932583 -4.2007279 -4.2024217 -4.1994276 -4.20086 -4.1992559 -4.1817746][-4.1615829 -4.1739058 -4.1812863 -4.1645842 -4.1477871 -4.1645641 -4.1966057 -4.2185521 -4.228301 -4.2301793 -4.2291594 -4.2256041 -4.22222 -4.2198019 -4.2140255][-4.1828728 -4.1947508 -4.2096395 -4.2022538 -4.1902909 -4.2004876 -4.2216711 -4.2351418 -4.2413826 -4.2425456 -4.2392774 -4.2366085 -4.232192 -4.2292004 -4.2286987][-4.1635695 -4.1833711 -4.207067 -4.2084274 -4.2008281 -4.2067943 -4.2195797 -4.2264357 -4.2285318 -4.2262397 -4.2175717 -4.210012 -4.2022319 -4.1997018 -4.203589][-4.1244049 -4.1549268 -4.1867404 -4.1978393 -4.1966147 -4.2002869 -4.2064695 -4.2093983 -4.2068472 -4.1986432 -4.1843061 -4.1728992 -4.1602392 -4.1505823 -4.149003][-4.0974011 -4.1344271 -4.1711378 -4.1862345 -4.1863518 -4.1849966 -4.1871543 -4.1923122 -4.1936073 -4.1853395 -4.1716409 -4.1605048 -4.1405034 -4.1184182 -4.1033125][-4.0805287 -4.1176319 -4.1541758 -4.1673784 -4.1670952 -4.1615524 -4.163271 -4.1752319 -4.183362 -4.1761847 -4.1625018 -4.1500554 -4.1260262 -4.0953588 -4.0700421]]...]
INFO - root - 2017-12-08 00:37:26.139179: step 72610, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 52h:20m:10s remains)
INFO - root - 2017-12-08 00:37:32.916205: step 72620, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 46h:08m:20s remains)
INFO - root - 2017-12-08 00:37:39.737500: step 72630, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.672 sec/batch; 48h:31m:38s remains)
INFO - root - 2017-12-08 00:37:46.614131: step 72640, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 50h:42m:06s remains)
INFO - root - 2017-12-08 00:37:53.341436: step 72650, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 47h:58m:25s remains)
INFO - root - 2017-12-08 00:38:00.218564: step 72660, loss = 2.03, batch loss = 1.97 (12.9 examples/sec; 0.622 sec/batch; 44h:55m:46s remains)
INFO - root - 2017-12-08 00:38:07.118624: step 72670, loss = 2.10, batch loss = 2.04 (12.1 examples/sec; 0.661 sec/batch; 47h:41m:39s remains)
INFO - root - 2017-12-08 00:38:13.986223: step 72680, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 50h:41m:42s remains)
INFO - root - 2017-12-08 00:38:20.834217: step 72690, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 51h:00m:39s remains)
INFO - root - 2017-12-08 00:38:27.654543: step 72700, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 49h:36m:24s remains)
2017-12-08 00:38:28.370937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1855979 -4.17944 -4.1747146 -4.1796913 -4.1951456 -4.1980047 -4.193243 -4.1807642 -4.1662769 -4.1651664 -4.1784711 -4.1924644 -4.1970978 -4.1903095 -4.1782956][-4.2003069 -4.1980405 -4.1928596 -4.191349 -4.1990557 -4.2006421 -4.1964231 -4.1828508 -4.1659179 -4.1590953 -4.16785 -4.1768436 -4.1760721 -4.1661549 -4.1514111][-4.2108154 -4.2098403 -4.2057605 -4.204237 -4.2079558 -4.2055788 -4.198884 -4.18471 -4.1668315 -4.1581049 -4.1630912 -4.168211 -4.163568 -4.1523013 -4.1361809][-4.2070551 -4.20525 -4.2047954 -4.2065997 -4.2098889 -4.2025414 -4.1883087 -4.1709142 -4.1548686 -4.1487694 -4.1543593 -4.1623664 -4.1625385 -4.1541877 -4.1381717][-4.1822467 -4.1795769 -4.1803517 -4.1825304 -4.1836009 -4.1702657 -4.1481471 -4.130383 -4.1205554 -4.1232228 -4.1361122 -4.1516161 -4.1595726 -4.1567435 -4.1435213][-4.1397319 -4.1386266 -4.1426578 -4.1447473 -4.1416345 -4.1217279 -4.0950723 -4.0780907 -4.0735526 -4.0863657 -4.1089354 -4.1325579 -4.1466846 -4.1489081 -4.1394596][-4.0958962 -4.0954938 -4.1009164 -4.1041012 -4.0972486 -4.0719013 -4.0431995 -4.0270615 -4.0266204 -4.0479803 -4.0783324 -4.1075892 -4.1261458 -4.1313481 -4.1241531][-4.0631328 -4.0598049 -4.0654445 -4.0712957 -4.0621986 -4.03443 -4.0068884 -3.9909122 -3.9905403 -4.0163188 -4.0523648 -4.0856347 -4.1065612 -4.1123524 -4.1064377][-4.0435166 -4.03754 -4.046175 -4.0610571 -4.0604067 -4.0398107 -4.0177307 -4.0014272 -3.9976621 -4.0204997 -4.0529442 -4.082613 -4.1013675 -4.1070623 -4.1032076][-4.03148 -4.02733 -4.0423369 -4.0681581 -4.0814981 -4.07512 -4.0629859 -4.05074 -4.0461526 -4.0631604 -4.0878596 -4.1084194 -4.1190886 -4.1204958 -4.1168694][-4.0215163 -4.0212455 -4.0404029 -4.0734138 -4.0984569 -4.1053715 -4.1038666 -4.0985255 -4.0977216 -4.111094 -4.1276479 -4.1376705 -4.138885 -4.1346345 -4.1294484][-4.0289726 -4.0319538 -4.0501986 -4.0818553 -4.1092315 -4.1204004 -4.1213665 -4.119113 -4.121665 -4.1322775 -4.1431479 -4.1470313 -4.1444178 -4.1380539 -4.1343117][-4.068831 -4.0736504 -4.0861335 -4.1072545 -4.1256232 -4.1316247 -4.1288104 -4.1252995 -4.1272993 -4.1338477 -4.1407213 -4.1440187 -4.1446214 -4.1434317 -4.1459384][-4.1134229 -4.1199207 -4.1255188 -4.130846 -4.1348085 -4.1324534 -4.1265907 -4.123692 -4.1262393 -4.1301322 -4.1338387 -4.1374197 -4.141355 -4.1463265 -4.1553154][-4.1330361 -4.139843 -4.1414113 -4.1363168 -4.1303458 -4.1242819 -4.12009 -4.1217871 -4.1279421 -4.1314087 -4.1326351 -4.134552 -4.1393056 -4.1467648 -4.1556826]]...]
INFO - root - 2017-12-08 00:38:35.026534: step 72710, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 52h:25m:00s remains)
INFO - root - 2017-12-08 00:38:41.998222: step 72720, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.726 sec/batch; 52h:23m:06s remains)
INFO - root - 2017-12-08 00:38:48.782298: step 72730, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.665 sec/batch; 48h:00m:02s remains)
INFO - root - 2017-12-08 00:38:55.566554: step 72740, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 47h:47m:45s remains)
INFO - root - 2017-12-08 00:39:02.469292: step 72750, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 49h:10m:10s remains)
INFO - root - 2017-12-08 00:39:09.348773: step 72760, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 50h:01m:51s remains)
INFO - root - 2017-12-08 00:39:16.154070: step 72770, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:30m:08s remains)
INFO - root - 2017-12-08 00:39:22.879861: step 72780, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.665 sec/batch; 47h:56m:30s remains)
INFO - root - 2017-12-08 00:39:29.584129: step 72790, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 47h:13m:45s remains)
INFO - root - 2017-12-08 00:39:36.401883: step 72800, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 51h:14m:07s remains)
2017-12-08 00:39:37.161761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2513943 -4.2525725 -4.264782 -4.2692981 -4.2613068 -4.2497449 -4.2431574 -4.2424192 -4.2440805 -4.2434 -4.227169 -4.2042561 -4.208312 -4.245296 -4.2929325][-4.2460465 -4.2504158 -4.258379 -4.2554502 -4.2444115 -4.2370491 -4.2385845 -4.2480745 -4.2539225 -4.2493377 -4.2307897 -4.20745 -4.2087436 -4.2450266 -4.2950578][-4.2512741 -4.2536778 -4.2511239 -4.2383723 -4.2274332 -4.2257223 -4.2332592 -4.2484136 -4.2628894 -4.2598629 -4.2401347 -4.21602 -4.2120881 -4.2438951 -4.2948847][-4.2627392 -4.2567153 -4.2402725 -4.2154431 -4.2041121 -4.2074652 -4.2218671 -4.2436881 -4.2657924 -4.2692976 -4.2521377 -4.2268348 -4.2154822 -4.2366467 -4.2843294][-4.2716565 -4.25543 -4.2175455 -4.1692386 -4.1465006 -4.153985 -4.1816363 -4.2216458 -4.2587438 -4.2706695 -4.2564969 -4.2319446 -4.214045 -4.222796 -4.26331][-4.2668529 -4.2421417 -4.1823745 -4.1015167 -4.0527229 -4.0614119 -4.1089668 -4.1756792 -4.2331524 -4.2549953 -4.244329 -4.2219496 -4.2030139 -4.2067184 -4.2432947][-4.2636209 -4.2344589 -4.1608963 -4.0536489 -3.9716916 -3.9706135 -4.0337472 -4.1279869 -4.2034297 -4.2332859 -4.2259574 -4.2085772 -4.194365 -4.2006583 -4.2368269][-4.2782421 -4.2528915 -4.1837225 -4.0741725 -3.9726427 -3.9447718 -4.0003242 -4.1008482 -4.179244 -4.2102213 -4.2095842 -4.203485 -4.19898 -4.2103319 -4.2451262][-4.3077064 -4.2918525 -4.2383847 -4.14793 -4.0490532 -3.9951377 -4.0213614 -4.1026039 -4.1667118 -4.189188 -4.1939311 -4.1996694 -4.2057514 -4.2226362 -4.257102][-4.32795 -4.3207688 -4.2814779 -4.2148447 -4.133472 -4.0680876 -4.0590129 -4.1062479 -4.1491332 -4.161953 -4.1697822 -4.1875744 -4.20625 -4.2332573 -4.27048][-4.3254752 -4.3219972 -4.2939448 -4.2478876 -4.1882691 -4.1277385 -4.0949812 -4.1083078 -4.1261873 -4.128643 -4.13794 -4.1676025 -4.2005739 -4.2396884 -4.28243][-4.3185825 -4.3143792 -4.2920814 -4.26036 -4.2202053 -4.1722107 -4.1331739 -4.1243768 -4.1200089 -4.1059217 -4.108695 -4.1450605 -4.1905208 -4.2414403 -4.2915545][-4.3201861 -4.3156743 -4.2979074 -4.2750154 -4.2486525 -4.2143126 -4.1815443 -4.1661062 -4.1491776 -4.120873 -4.1113863 -4.1434116 -4.1905293 -4.2434731 -4.2956452][-4.3170643 -4.3133473 -4.3020115 -4.2875204 -4.2712717 -4.248826 -4.2239294 -4.2080121 -4.1872015 -4.1544471 -4.1348624 -4.157362 -4.2010407 -4.251924 -4.3015947][-4.3091364 -4.3053885 -4.2997322 -4.2925138 -4.2847285 -4.2727814 -4.2559528 -4.242166 -4.2225189 -4.1937509 -4.170857 -4.1834841 -4.2227111 -4.2721915 -4.3168364]]...]
INFO - root - 2017-12-08 00:39:43.809916: step 72810, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 44h:57m:22s remains)
INFO - root - 2017-12-08 00:39:50.459003: step 72820, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 50h:35m:41s remains)
INFO - root - 2017-12-08 00:39:57.286241: step 72830, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 53h:44m:10s remains)
INFO - root - 2017-12-08 00:40:04.079791: step 72840, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 47h:36m:48s remains)
INFO - root - 2017-12-08 00:40:10.814099: step 72850, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 47h:37m:55s remains)
INFO - root - 2017-12-08 00:40:17.519696: step 72860, loss = 2.04, batch loss = 1.98 (12.7 examples/sec; 0.629 sec/batch; 45h:22m:29s remains)
INFO - root - 2017-12-08 00:40:24.491757: step 72870, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.709 sec/batch; 51h:07m:46s remains)
INFO - root - 2017-12-08 00:40:31.427309: step 72880, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 51h:38m:10s remains)
INFO - root - 2017-12-08 00:40:38.158571: step 72890, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.704 sec/batch; 50h:44m:29s remains)
INFO - root - 2017-12-08 00:40:44.925081: step 72900, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.652 sec/batch; 46h:59m:35s remains)
2017-12-08 00:40:45.640245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1998863 -4.215837 -4.2254977 -4.2151651 -4.1849451 -4.1360173 -4.0760889 -4.027719 -4.013227 -4.0388093 -4.0815229 -4.1231165 -4.1640639 -4.1878371 -4.183136][-4.206368 -4.2165418 -4.2206936 -4.2021008 -4.1581488 -4.0950403 -4.0224662 -3.97162 -3.9752734 -4.0249496 -4.0847287 -4.1408367 -4.1857929 -4.2088227 -4.2048936][-4.2184911 -4.2157168 -4.211235 -4.1870046 -4.1376691 -4.068759 -3.9989696 -3.9654768 -4.0005903 -4.070394 -4.1326003 -4.1854229 -4.2250686 -4.2398105 -4.235733][-4.2325635 -4.2179122 -4.2001662 -4.1676078 -4.1185637 -4.0590138 -4.0103564 -4.0046816 -4.0628381 -4.1328826 -4.1825562 -4.2218804 -4.2510352 -4.2580438 -4.2532644][-4.2502904 -4.2286892 -4.2037449 -4.1674376 -4.1203918 -4.0675654 -4.0298924 -4.035171 -4.10008 -4.1681962 -4.2094779 -4.2420478 -4.2696967 -4.2735004 -4.2639627][-4.2463536 -4.2248244 -4.2033339 -4.1664166 -4.11728 -4.0620642 -4.0180845 -4.015985 -4.0870395 -4.16325 -4.2095623 -4.246943 -4.2796221 -4.2864208 -4.2729816][-4.2235694 -4.2048283 -4.1789355 -4.1390676 -4.0918756 -4.0361 -3.9869809 -3.9794362 -4.0496421 -4.1321416 -4.1919394 -4.2427964 -4.2821703 -4.2925038 -4.2844644][-4.1850071 -4.1726189 -4.1474195 -4.1126113 -4.0711732 -4.0230827 -3.9848013 -3.9896231 -4.0522213 -4.1329412 -4.1991296 -4.2527356 -4.2915692 -4.3054461 -4.2999721][-4.1347365 -4.1360474 -4.1205349 -4.0940337 -4.0660105 -4.0418425 -4.0339861 -4.0608149 -4.1130228 -4.1772051 -4.2326612 -4.2754049 -4.3062019 -4.3195705 -4.3091178][-4.1183372 -4.1235576 -4.1045794 -4.07802 -4.0675945 -4.0740061 -4.0999031 -4.1434312 -4.1835036 -4.2283592 -4.271606 -4.2993531 -4.3170028 -4.3229995 -4.3049326][-4.15724 -4.1465888 -4.1119037 -4.0795078 -4.0792646 -4.107605 -4.155591 -4.2064843 -4.2383094 -4.2695236 -4.3005757 -4.3156528 -4.3206592 -4.3146849 -4.28887][-4.2078338 -4.1862764 -4.1455569 -4.1129823 -4.1197934 -4.1608324 -4.2151914 -4.2602725 -4.2825713 -4.2995486 -4.3152437 -4.3200855 -4.3146653 -4.295856 -4.2634892][-4.2483544 -4.2290545 -4.1963267 -4.1720376 -4.1822062 -4.2227955 -4.2716565 -4.3056431 -4.3170552 -4.3179612 -4.3188167 -4.3158436 -4.2987442 -4.2663641 -4.226532][-4.2756147 -4.2663646 -4.2470875 -4.231648 -4.2403188 -4.2731562 -4.3129411 -4.3375206 -4.3388648 -4.32759 -4.317481 -4.3073153 -4.2818789 -4.2389278 -4.1908703][-4.2874684 -4.2859974 -4.2782297 -4.2720613 -4.2799711 -4.3050756 -4.3349652 -4.3504796 -4.3476286 -4.3327689 -4.3148804 -4.2950091 -4.26623 -4.227551 -4.183754]]...]
INFO - root - 2017-12-08 00:40:52.277175: step 72910, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.731 sec/batch; 52h:44m:28s remains)
INFO - root - 2017-12-08 00:40:59.099397: step 72920, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 49h:59m:50s remains)
INFO - root - 2017-12-08 00:41:06.060448: step 72930, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 48h:14m:10s remains)
INFO - root - 2017-12-08 00:41:12.878815: step 72940, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 48h:08m:26s remains)
INFO - root - 2017-12-08 00:41:19.715748: step 72950, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 52h:05m:30s remains)
INFO - root - 2017-12-08 00:41:26.548285: step 72960, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 50h:38m:35s remains)
INFO - root - 2017-12-08 00:41:33.299967: step 72970, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 50h:44m:31s remains)
INFO - root - 2017-12-08 00:41:40.073579: step 72980, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.645 sec/batch; 46h:31m:20s remains)
INFO - root - 2017-12-08 00:41:46.810014: step 72990, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 47h:07m:30s remains)
INFO - root - 2017-12-08 00:41:53.625095: step 73000, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 50h:09m:53s remains)
2017-12-08 00:41:54.422693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3546081 -4.3453135 -4.3289022 -4.306911 -4.2926612 -4.2942343 -4.3064222 -4.32088 -4.3306627 -4.3300629 -4.3212404 -4.3116951 -4.30568 -4.3005972 -4.2969828][-4.3381906 -4.3317685 -4.3118973 -4.2833877 -4.2618842 -4.2599535 -4.2730989 -4.2919574 -4.310894 -4.3226995 -4.3235478 -4.3185153 -4.31145 -4.3029976 -4.2961164][-4.3041444 -4.3029056 -4.2834363 -4.2474113 -4.2144585 -4.2042875 -4.2149386 -4.2352719 -4.263741 -4.2916985 -4.3099113 -4.3175459 -4.315558 -4.3073325 -4.2988987][-4.2701607 -4.2776256 -4.26113 -4.2170181 -4.1667647 -4.1365557 -4.1372118 -4.1603584 -4.2002821 -4.244936 -4.2813458 -4.3057814 -4.3181996 -4.3166847 -4.3089437][-4.245729 -4.2643995 -4.2482066 -4.1945348 -4.1241474 -4.0685897 -4.0509038 -4.0757704 -4.1313968 -4.1913104 -4.2389927 -4.2760916 -4.3040614 -4.3174267 -4.3177757][-4.2238 -4.2472296 -4.2327266 -4.1715941 -4.0842514 -4.0032511 -3.9613235 -3.9821379 -4.0577493 -4.134706 -4.1915469 -4.2361135 -4.2777448 -4.3082108 -4.319746][-4.211165 -4.2369833 -4.2303386 -4.1747236 -4.08477 -3.9929969 -3.9304702 -3.9352193 -4.0106726 -4.0924168 -4.1509686 -4.1984372 -4.2473593 -4.2890639 -4.3116713][-4.2105188 -4.240972 -4.245152 -4.2041049 -4.1332612 -4.0653009 -4.0154715 -4.0013752 -4.0397553 -4.0939169 -4.1338811 -4.1719079 -4.2196665 -4.265626 -4.2973866][-4.2214823 -4.2575164 -4.2694168 -4.2420087 -4.1925263 -4.1529927 -4.1270671 -4.1126213 -4.123702 -4.144588 -4.1582766 -4.1798291 -4.2205038 -4.2646232 -4.2978706][-4.24559 -4.2817831 -4.2960958 -4.2772579 -4.2411418 -4.2167625 -4.2037711 -4.1922832 -4.1936474 -4.2009192 -4.2062664 -4.2181187 -4.2494955 -4.2862797 -4.3137932][-4.2806959 -4.3084764 -4.3178549 -4.2977319 -4.26406 -4.240766 -4.2282877 -4.2145543 -4.2111893 -4.2208967 -4.235991 -4.2501531 -4.2746119 -4.304554 -4.3265862][-4.3114576 -4.3253794 -4.3233333 -4.2939811 -4.2499161 -4.2164397 -4.1966357 -4.1832714 -4.1854515 -4.2077379 -4.2393765 -4.2610922 -4.2844825 -4.3132296 -4.3324809][-4.3111095 -4.3154368 -4.3064928 -4.2679777 -4.2094069 -4.1554661 -4.11623 -4.0962892 -4.1116533 -4.160007 -4.211062 -4.2430658 -4.2692466 -4.3012075 -4.3260283][-4.281528 -4.2840896 -4.2760677 -4.2344604 -4.1647444 -4.0885391 -4.0210834 -3.9811208 -4.0034213 -4.0821986 -4.1577587 -4.2033434 -4.2376428 -4.2779937 -4.3110504][-4.2397866 -4.2478342 -4.2493563 -4.2196531 -4.1558123 -4.0724025 -3.9886456 -3.9271824 -3.9359937 -4.0197043 -4.1050529 -4.1566935 -4.19733 -4.2445741 -4.285666]]...]
INFO - root - 2017-12-08 00:42:00.993867: step 73010, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 47h:27m:18s remains)
INFO - root - 2017-12-08 00:42:07.666321: step 73020, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 46h:06m:51s remains)
INFO - root - 2017-12-08 00:42:14.416408: step 73030, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 50h:18m:17s remains)
INFO - root - 2017-12-08 00:42:21.231624: step 73040, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 50h:35m:59s remains)
INFO - root - 2017-12-08 00:42:28.033743: step 73050, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 49h:08m:51s remains)
INFO - root - 2017-12-08 00:42:34.768355: step 73060, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 48h:17m:27s remains)
INFO - root - 2017-12-08 00:42:41.482227: step 73070, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.616 sec/batch; 44h:23m:29s remains)
INFO - root - 2017-12-08 00:42:48.414736: step 73080, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.682 sec/batch; 49h:10m:02s remains)
INFO - root - 2017-12-08 00:42:55.310282: step 73090, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 51h:31m:27s remains)
INFO - root - 2017-12-08 00:43:02.137396: step 73100, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 50h:58m:27s remains)
2017-12-08 00:43:02.813783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654023 -4.2386684 -4.2222018 -4.2111893 -4.2060409 -4.2165771 -4.2192283 -4.211431 -4.1973696 -4.1990814 -4.2065315 -4.2202692 -4.2211289 -4.2055793 -4.1933742][-4.2731481 -4.2450151 -4.2209988 -4.1955261 -4.1804304 -4.1816278 -4.1838813 -4.178062 -4.1691656 -4.1777792 -4.1928229 -4.2120523 -4.208395 -4.1907544 -4.1833196][-4.2508 -4.2201791 -4.1908112 -4.1566491 -4.135458 -4.1253686 -4.1183567 -4.1137395 -4.1142521 -4.1249204 -4.1466732 -4.1720309 -4.1685257 -4.1513648 -4.1463437][-4.2123804 -4.1777167 -4.1449428 -4.1103129 -4.0885887 -4.07092 -4.0540643 -4.0469346 -4.0488014 -4.0527225 -4.076478 -4.1070251 -4.1118412 -4.1020088 -4.097446][-4.1527948 -4.1161923 -4.0781054 -4.0428076 -4.0217876 -4.0015764 -3.9802108 -3.9695978 -3.9727297 -3.9746475 -3.9988947 -4.0362716 -4.0527868 -4.0514474 -4.0496149][-4.0759435 -4.0324869 -3.9880283 -3.948174 -3.9218533 -3.8965049 -3.8748624 -3.8696895 -3.8801603 -3.8912504 -3.9187517 -3.9515338 -3.966202 -3.9685066 -3.9681273][-3.9875462 -3.9330366 -3.8775508 -3.8236179 -3.7793789 -3.7405767 -3.7177286 -3.7264729 -3.7518921 -3.7750182 -3.8073869 -3.8342826 -3.8425388 -3.8490496 -3.8508959][-3.9332774 -3.8686531 -3.8037052 -3.7373734 -3.6832719 -3.639009 -3.6153727 -3.6300015 -3.6554985 -3.6759529 -3.7054155 -3.7306252 -3.7431936 -3.7546911 -3.7574229][-3.9796417 -3.9237781 -3.8710752 -3.8161819 -3.7739778 -3.7506175 -3.739301 -3.7487583 -3.7575684 -3.7572727 -3.765357 -3.775806 -3.7854366 -3.7891707 -3.7749579][-4.0908065 -4.0544214 -4.02052 -3.9837482 -3.9584935 -3.9547291 -3.9557431 -3.963006 -3.9628077 -3.9520032 -3.9467907 -3.9434531 -3.9429493 -3.9386208 -3.9153402][-4.1975904 -4.1777782 -4.1576738 -4.134326 -4.1208758 -4.1247611 -4.1295166 -4.1349726 -4.134223 -4.1255188 -4.1197143 -4.114964 -4.1136942 -4.1121416 -4.0968661][-4.2740059 -4.2657747 -4.2543015 -4.2408037 -4.2346377 -4.2393632 -4.2445679 -4.248539 -4.248076 -4.2420783 -4.2376556 -4.233583 -4.2316012 -4.23141 -4.2263827][-4.329886 -4.3270078 -4.3212028 -4.313911 -4.3099537 -4.3110466 -4.3128853 -4.3146286 -4.3142829 -4.3114548 -4.3079934 -4.3042436 -4.3011131 -4.3000612 -4.2996435][-4.3603172 -4.3601036 -4.3575268 -4.3542953 -4.3523779 -4.3522525 -4.3525419 -4.3527155 -4.3521891 -4.3505263 -4.3479681 -4.344769 -4.3424268 -4.341929 -4.3429484][-4.3704586 -4.3714676 -4.3711882 -4.3705416 -4.3703 -4.3705378 -4.3705306 -4.3703904 -4.3700647 -4.3693733 -4.36809 -4.3665118 -4.3653636 -4.3653984 -4.3661547]]...]
INFO - root - 2017-12-08 00:43:09.468008: step 73110, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 46h:57m:05s remains)
INFO - root - 2017-12-08 00:43:16.261070: step 73120, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 49h:58m:25s remains)
INFO - root - 2017-12-08 00:43:22.775899: step 73130, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 48h:56m:42s remains)
INFO - root - 2017-12-08 00:43:29.601621: step 73140, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.678 sec/batch; 48h:50m:09s remains)
INFO - root - 2017-12-08 00:43:36.232740: step 73150, loss = 2.08, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 44h:35m:17s remains)
INFO - root - 2017-12-08 00:43:43.141586: step 73160, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.684 sec/batch; 49h:14m:32s remains)
INFO - root - 2017-12-08 00:43:50.006980: step 73170, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.721 sec/batch; 51h:58m:00s remains)
INFO - root - 2017-12-08 00:43:56.809754: step 73180, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 50h:13m:21s remains)
INFO - root - 2017-12-08 00:44:03.685331: step 73190, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 48h:13m:51s remains)
INFO - root - 2017-12-08 00:44:10.455761: step 73200, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.646 sec/batch; 46h:32m:19s remains)
2017-12-08 00:44:11.145384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3206558 -4.3224735 -4.3252 -4.3282809 -4.3272586 -4.3273168 -4.3301969 -4.3432341 -4.3566451 -4.3590345 -4.3376951 -4.2741737 -4.1930928 -4.1210632 -4.065145][-4.3355532 -4.3362031 -4.335897 -4.3347225 -4.3294063 -4.3215742 -4.31399 -4.3195667 -4.3315425 -4.3396564 -4.3231363 -4.2569866 -4.1689119 -4.0824423 -4.006134][-4.3508143 -4.34825 -4.3435268 -4.3363581 -4.3228526 -4.3042126 -4.2846918 -4.2788506 -4.2876062 -4.3026366 -4.2996979 -4.2524381 -4.1811376 -4.10411 -4.0310879][-4.3593488 -4.3546619 -4.3468075 -4.3349323 -4.3143382 -4.2858148 -4.2547917 -4.2356925 -4.2403722 -4.2629318 -4.2786417 -4.2620411 -4.22135 -4.1646447 -4.1115532][-4.35869 -4.3553143 -4.3483515 -4.3336773 -4.3063169 -4.2667184 -4.2225 -4.1911249 -4.19522 -4.2334123 -4.2674851 -4.2754545 -4.25862 -4.2175665 -4.1765933][-4.35271 -4.349678 -4.3415594 -4.3204012 -4.2813735 -4.2278318 -4.166995 -4.1198692 -4.1254897 -4.1918736 -4.2536445 -4.2787404 -4.2675176 -4.2323503 -4.1937609][-4.3338923 -4.3290358 -4.3176122 -4.2878685 -4.2284517 -4.149241 -4.0575619 -3.9794757 -3.9826157 -4.0920043 -4.20439 -4.2615924 -4.2569952 -4.2181034 -4.1719022][-4.3115382 -4.3025613 -4.2876563 -4.2483993 -4.1686478 -4.058455 -3.9207549 -3.7910733 -3.7800961 -3.934793 -4.1073279 -4.2090745 -4.2307014 -4.1965418 -4.1426015][-4.2956614 -4.2831593 -4.2661343 -4.2230268 -4.1390057 -4.0176635 -3.856205 -3.6858959 -3.649966 -3.8155417 -4.0155048 -4.1470156 -4.1988358 -4.1847782 -4.1381741][-4.2949481 -4.2820311 -4.2663407 -4.2322936 -4.1655331 -4.0653076 -3.9333749 -3.7923012 -3.7513127 -3.8632755 -4.0179048 -4.1315637 -4.1916733 -4.1982408 -4.1684804][-4.3091331 -4.29757 -4.2851381 -4.2653542 -4.2249088 -4.1592326 -4.0733018 -3.9833808 -3.9535286 -4.0108156 -4.1004667 -4.1732473 -4.2211361 -4.234879 -4.220263][-4.3257432 -4.3169265 -4.3079476 -4.2991815 -4.2811975 -4.2472696 -4.2003813 -4.149899 -4.1259656 -4.1486316 -4.193768 -4.2345805 -4.267622 -4.2823992 -4.2793779][-4.334825 -4.32813 -4.3223262 -4.3193269 -4.3142362 -4.30073 -4.2791562 -4.2531443 -4.2377214 -4.2411833 -4.2603087 -4.2829413 -4.3052506 -4.3223705 -4.3295655][-4.3367424 -4.3319697 -4.3288121 -4.3281279 -4.328301 -4.3256626 -4.3198228 -4.3087759 -4.3008523 -4.2991214 -4.3064818 -4.3192835 -4.3337278 -4.3498168 -4.3622332][-4.335515 -4.3323712 -4.3311396 -4.3325133 -4.3342085 -4.3353624 -4.3377028 -4.3353915 -4.3300447 -4.3262854 -4.3290949 -4.3381009 -4.3499169 -4.3647518 -4.3743711]]...]
INFO - root - 2017-12-08 00:44:17.771706: step 73210, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 50h:29m:14s remains)
INFO - root - 2017-12-08 00:44:24.586048: step 73220, loss = 2.11, batch loss = 2.05 (12.4 examples/sec; 0.643 sec/batch; 46h:17m:10s remains)
INFO - root - 2017-12-08 00:44:31.435777: step 73230, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 46h:28m:55s remains)
INFO - root - 2017-12-08 00:44:38.286812: step 73240, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 50h:30m:15s remains)
INFO - root - 2017-12-08 00:44:45.146857: step 73250, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 49h:35m:00s remains)
INFO - root - 2017-12-08 00:44:52.048915: step 73260, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.687 sec/batch; 49h:29m:00s remains)
INFO - root - 2017-12-08 00:44:58.769123: step 73270, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 46h:34m:41s remains)
INFO - root - 2017-12-08 00:45:05.593947: step 73280, loss = 2.03, batch loss = 1.97 (12.1 examples/sec; 0.661 sec/batch; 47h:35m:50s remains)
INFO - root - 2017-12-08 00:45:12.382895: step 73290, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.681 sec/batch; 49h:00m:19s remains)
INFO - root - 2017-12-08 00:45:19.329290: step 73300, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 51h:01m:09s remains)
2017-12-08 00:45:20.298654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3371544 -4.3348613 -4.3331604 -4.3348308 -4.3408628 -4.3467407 -4.348557 -4.345746 -4.3398271 -4.3333917 -4.3293052 -4.3300076 -4.3344493 -4.3419 -4.3480353][-4.3423996 -4.3384032 -4.3364773 -4.3380508 -4.3426714 -4.3452239 -4.3426046 -4.3347564 -4.32485 -4.3153667 -4.3074837 -4.3057475 -4.3127737 -4.3263741 -4.3382144][-4.3378358 -4.3344064 -4.3350978 -4.3366833 -4.3360214 -4.3301911 -4.3204436 -4.3069687 -4.2925844 -4.2767162 -4.2631474 -4.2623882 -4.2772532 -4.3007975 -4.3210754][-4.3217444 -4.3191686 -4.3237615 -4.3243465 -4.3177614 -4.3024964 -4.2795467 -4.2518506 -4.2240276 -4.1950326 -4.1805587 -4.1941609 -4.22385 -4.2603993 -4.2912025][-4.2832737 -4.2753906 -4.2750487 -4.270576 -4.2582016 -4.2330723 -4.193985 -4.1488242 -4.1054692 -4.0763431 -4.0834432 -4.123961 -4.1700826 -4.2163439 -4.2583318][-4.2147307 -4.2019553 -4.1979446 -4.1917639 -4.1774669 -4.1465044 -4.0877795 -4.0108137 -3.947773 -3.9415879 -3.9949372 -4.0653315 -4.12778 -4.1873317 -4.238903][-4.14975 -4.1314473 -4.1254039 -4.1236672 -4.109231 -4.0601416 -3.9625731 -3.8269706 -3.7411096 -3.7958968 -3.9206586 -4.0292077 -4.1116385 -4.1850109 -4.2419887][-4.092567 -4.0619974 -4.0571012 -4.0668778 -4.055027 -3.988575 -3.8458445 -3.6523263 -3.5741744 -3.7264962 -3.9105492 -4.042161 -4.1377411 -4.213501 -4.2648621][-4.061223 -4.0356364 -4.0410776 -4.0641012 -4.0595942 -4.0012922 -3.8686266 -3.7207842 -3.7126729 -3.8705633 -4.0154791 -4.1182361 -4.1995654 -4.2570853 -4.2925115][-4.0783958 -4.0789189 -4.0947819 -4.116828 -4.1130672 -4.0708084 -3.9861772 -3.9272351 -3.96318 -4.0654907 -4.1420741 -4.2045903 -4.2586308 -4.2944093 -4.3144789][-4.1021042 -4.1139655 -4.1265 -4.1386614 -4.1402369 -4.1229963 -4.087275 -4.0825992 -4.1305094 -4.1910067 -4.2289867 -4.2695737 -4.3047247 -4.3222237 -4.3321085][-4.1059036 -4.1103015 -4.1111569 -4.1138544 -4.1327114 -4.1490917 -4.1500487 -4.16857 -4.2104735 -4.245492 -4.269321 -4.303688 -4.3292103 -4.3379869 -4.3428922][-4.1016231 -4.0862293 -4.0647211 -4.0580049 -4.0954986 -4.1466274 -4.1765428 -4.2058539 -4.2395878 -4.2581964 -4.2785945 -4.3148685 -4.3374023 -4.3426313 -4.3454084][-4.0881577 -4.0512958 -4.0174494 -4.0196252 -4.0766778 -4.1421194 -4.1868029 -4.2229919 -4.2520709 -4.2663321 -4.28923 -4.3227482 -4.3414412 -4.346868 -4.3475771][-4.0696964 -4.0353127 -4.0184741 -4.0436807 -4.1047831 -4.1653204 -4.2103024 -4.2431035 -4.2678361 -4.2842917 -4.308341 -4.3341732 -4.348238 -4.3526998 -4.3496661]]...]
INFO - root - 2017-12-08 00:45:26.957630: step 73310, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.717 sec/batch; 51h:36m:33s remains)
INFO - root - 2017-12-08 00:45:33.756722: step 73320, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 51h:10m:45s remains)
INFO - root - 2017-12-08 00:45:40.682401: step 73330, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 48h:48m:11s remains)
INFO - root - 2017-12-08 00:45:47.599073: step 73340, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 47h:21m:08s remains)
INFO - root - 2017-12-08 00:45:54.402038: step 73350, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.643 sec/batch; 46h:16m:45s remains)
INFO - root - 2017-12-08 00:46:01.270468: step 73360, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 49h:51m:49s remains)
INFO - root - 2017-12-08 00:46:08.113536: step 73370, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.729 sec/batch; 52h:29m:37s remains)
INFO - root - 2017-12-08 00:46:14.941235: step 73380, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.676 sec/batch; 48h:40m:41s remains)
INFO - root - 2017-12-08 00:46:21.774061: step 73390, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 47h:05m:14s remains)
INFO - root - 2017-12-08 00:46:28.664702: step 73400, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 49h:47m:44s remains)
2017-12-08 00:46:29.490658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.321034 -4.315011 -4.3021822 -4.2925906 -4.2822752 -4.2697463 -4.2651038 -4.2712035 -4.2814393 -4.2862697 -4.288938 -4.2907953 -4.2984152 -4.3080344 -4.30619][-4.2985873 -4.2902036 -4.2746372 -4.2615142 -4.2429938 -4.2176447 -4.2017288 -4.2067742 -4.2255507 -4.237596 -4.247273 -4.2540989 -4.2648153 -4.276731 -4.2743597][-4.2774644 -4.26536 -4.2474627 -4.2323551 -4.2081275 -4.1714191 -4.1458921 -4.1505361 -4.17579 -4.1911378 -4.2039061 -4.212924 -4.2254105 -4.2413869 -4.2412572][-4.2599463 -4.2416506 -4.2175469 -4.1977296 -4.1678557 -4.1245942 -4.096837 -4.1074758 -4.1391659 -4.1510034 -4.1561685 -4.161005 -4.1733875 -4.1923065 -4.2000751][-4.2465782 -4.2210484 -4.1868377 -4.1578183 -4.1171932 -4.065927 -4.0330248 -4.0580778 -4.1028557 -4.1134806 -4.1097426 -4.1065755 -4.11485 -4.1359029 -4.1536746][-4.2401128 -4.2081652 -4.1657705 -4.1245508 -4.065433 -3.9926212 -3.9459445 -3.9901032 -4.0562625 -4.0781007 -4.07891 -4.0740333 -4.0765991 -4.0957389 -4.12301][-4.2422705 -4.2088733 -4.1658969 -4.1206675 -4.0476341 -3.9479876 -3.8769171 -3.9258401 -4.0070543 -4.0469022 -4.0589824 -4.0562158 -4.056983 -4.0720496 -4.1047888][-4.2492943 -4.2213726 -4.186718 -4.1499891 -4.0843577 -3.9874189 -3.9079938 -3.9312983 -3.9951885 -4.0339227 -4.0544381 -4.0597715 -4.0605145 -4.0665607 -4.0928164][-4.2568083 -4.2368627 -4.2125921 -4.1893368 -4.1424313 -4.0718679 -4.0103588 -4.0054607 -4.0294695 -4.0464115 -4.068224 -4.0883131 -4.0945406 -4.0929956 -4.1019692][-4.2624531 -4.24858 -4.2292581 -4.211781 -4.1789207 -4.1347246 -4.0957913 -4.0834503 -4.081748 -4.0730109 -4.0880904 -4.1237135 -4.142725 -4.1423082 -4.1372271][-4.262764 -4.250123 -4.2339606 -4.2181582 -4.19258 -4.1648903 -4.1439815 -4.1370564 -4.1290884 -4.1049271 -4.1091809 -4.1467719 -4.1719546 -4.1748366 -4.1710873][-4.259419 -4.2443647 -4.2276344 -4.2121439 -4.1921549 -4.1775107 -4.1781526 -4.1841421 -4.1773157 -4.1479816 -4.1412539 -4.1677575 -4.1909518 -4.1973953 -4.1987925][-4.2570567 -4.2390513 -4.2171516 -4.198215 -4.180563 -4.1745038 -4.1884694 -4.2073984 -4.2086959 -4.1844034 -4.1717076 -4.1838274 -4.1991639 -4.2058129 -4.2090311][-4.2579236 -4.2374411 -4.2091303 -4.18244 -4.1606727 -4.1561756 -4.174027 -4.2017488 -4.2146831 -4.202858 -4.1890268 -4.1910052 -4.1997981 -4.2062192 -4.2079234][-4.2640634 -4.2439194 -4.2141361 -4.1828017 -4.154532 -4.14389 -4.1584606 -4.1904464 -4.2119017 -4.2112107 -4.1979723 -4.1947031 -4.2005296 -4.2087283 -4.2102675]]...]
INFO - root - 2017-12-08 00:46:36.090758: step 73410, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 47h:08m:42s remains)
INFO - root - 2017-12-08 00:46:42.821143: step 73420, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 46h:39m:58s remains)
INFO - root - 2017-12-08 00:46:49.646503: step 73430, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.689 sec/batch; 49h:33m:30s remains)
INFO - root - 2017-12-08 00:46:56.301828: step 73440, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 50h:33m:25s remains)
INFO - root - 2017-12-08 00:47:03.176444: step 73450, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 49h:41m:02s remains)
INFO - root - 2017-12-08 00:47:10.062292: step 73460, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.670 sec/batch; 48h:11m:23s remains)
INFO - root - 2017-12-08 00:47:16.831425: step 73470, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:18m:41s remains)
INFO - root - 2017-12-08 00:47:23.773837: step 73480, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.725 sec/batch; 52h:08m:45s remains)
INFO - root - 2017-12-08 00:47:30.649826: step 73490, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 47h:07m:24s remains)
INFO - root - 2017-12-08 00:47:37.488916: step 73500, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.627 sec/batch; 45h:04m:36s remains)
2017-12-08 00:47:38.191879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2294979 -4.2277961 -4.2290831 -4.2285247 -4.2196784 -4.2083297 -4.1963735 -4.1902461 -4.1925888 -4.1963887 -4.1991978 -4.2001123 -4.2042189 -4.20679 -4.2081718][-4.2023945 -4.2006054 -4.1991558 -4.1950564 -4.1827621 -4.1651444 -4.1463518 -4.1370606 -4.1416531 -4.1495676 -4.1589031 -4.1630268 -4.1667714 -4.1668572 -4.1643405][-4.1848946 -4.1826744 -4.1777372 -4.16868 -4.1488533 -4.1198993 -4.0875287 -4.0701909 -4.0756879 -4.0926905 -4.1132164 -4.1247559 -4.1318531 -4.1366835 -4.1374354][-4.1667476 -4.1659007 -4.1618662 -4.1508222 -4.1226664 -4.0796962 -4.0290995 -3.9995654 -4.0092883 -4.0429769 -4.0850353 -4.1114445 -4.1279321 -4.1412382 -4.1472564][-4.1363277 -4.1385379 -4.1407433 -4.1328411 -4.0941181 -4.0287523 -3.9536085 -3.9096859 -3.9272842 -3.9884748 -4.0621066 -4.1132607 -4.1408134 -4.159574 -4.1691566][-4.0991178 -4.1023707 -4.1106696 -4.1039934 -4.0551662 -3.9653161 -3.8493524 -3.7705846 -3.8000124 -3.9058716 -4.0195589 -4.0982895 -4.1396155 -4.1591716 -4.1669393][-4.0538068 -4.0525289 -4.0616975 -4.05465 -4.0013366 -3.8954892 -3.7420428 -3.6073315 -3.6433392 -3.8031731 -3.9565504 -4.0567393 -4.106626 -4.1321554 -4.1438713][-4.0245209 -4.0130329 -4.0174632 -4.0090532 -3.9661238 -3.8714964 -3.7215362 -3.5651598 -3.5802293 -3.7464719 -3.9103026 -4.019958 -4.0715914 -4.0994844 -4.117312][-4.03392 -4.0126896 -4.0053058 -3.9969876 -3.975595 -3.9158442 -3.8152261 -3.7111487 -3.7030745 -3.803544 -3.9247391 -4.0209045 -4.0728006 -4.0956683 -4.1095433][-4.0771942 -4.0538177 -4.0372419 -4.0258737 -4.0201836 -3.9924064 -3.9421751 -3.8902111 -3.8785281 -3.92501 -3.9977043 -4.0664482 -4.10912 -4.1280828 -4.1347346][-4.1346531 -4.112462 -4.0954275 -4.0863166 -4.0883832 -4.0761313 -4.0509109 -4.0272207 -4.0193839 -4.0447187 -4.0941238 -4.1424484 -4.1754403 -4.1888247 -4.1887236][-4.1908855 -4.1687493 -4.1583061 -4.1585364 -4.1646609 -4.157865 -4.14267 -4.1273503 -4.1191335 -4.136117 -4.1789818 -4.215766 -4.2397032 -4.2473679 -4.2434382][-4.2296524 -4.2084651 -4.2024879 -4.2082038 -4.2195444 -4.2178679 -4.2049465 -4.1902661 -4.1833415 -4.1984391 -4.23094 -4.2583365 -4.2766223 -4.2798667 -4.2737818][-4.2574 -4.2383523 -4.234807 -4.2418633 -4.2549877 -4.2568736 -4.2467628 -4.2312946 -4.226109 -4.2393842 -4.2603579 -4.27889 -4.292902 -4.2944613 -4.2875924][-4.2839518 -4.2704444 -4.265491 -4.2700663 -4.2811875 -4.2862444 -4.2812295 -4.2688332 -4.2618637 -4.2671618 -4.2794251 -4.2944331 -4.3068361 -4.3072968 -4.3005171]]...]
INFO - root - 2017-12-08 00:47:44.894935: step 73510, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 47h:36m:18s remains)
INFO - root - 2017-12-08 00:47:51.771829: step 73520, loss = 2.06, batch loss = 2.01 (12.2 examples/sec; 0.657 sec/batch; 47h:14m:36s remains)
INFO - root - 2017-12-08 00:47:58.570723: step 73530, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.641 sec/batch; 46h:06m:46s remains)
INFO - root - 2017-12-08 00:48:05.368305: step 73540, loss = 2.07, batch loss = 2.02 (11.3 examples/sec; 0.711 sec/batch; 51h:07m:57s remains)
INFO - root - 2017-12-08 00:48:12.265644: step 73550, loss = 2.04, batch loss = 1.98 (11.0 examples/sec; 0.726 sec/batch; 52h:13m:17s remains)
INFO - root - 2017-12-08 00:48:19.165803: step 73560, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 48h:44m:21s remains)
INFO - root - 2017-12-08 00:48:25.969036: step 73570, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.671 sec/batch; 48h:14m:09s remains)
INFO - root - 2017-12-08 00:48:32.703583: step 73580, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 45h:45m:10s remains)
INFO - root - 2017-12-08 00:48:39.634010: step 73590, loss = 2.09, batch loss = 2.04 (11.7 examples/sec; 0.686 sec/batch; 49h:20m:04s remains)
INFO - root - 2017-12-08 00:48:46.467364: step 73600, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 51h:22m:24s remains)
2017-12-08 00:48:47.220200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1095176 -4.1139622 -4.1253886 -4.1437674 -4.1544576 -4.1572204 -4.1418715 -4.1232047 -4.1193771 -4.1303749 -4.1351347 -4.1268506 -4.1228008 -4.1171174 -4.11046][-4.1100979 -4.1126633 -4.123806 -4.1365557 -4.1367049 -4.1231532 -4.0920038 -4.0713263 -4.0769162 -4.0968595 -4.1099257 -4.1071696 -4.1122622 -4.1122689 -4.1029353][-4.103909 -4.1124277 -4.1306167 -4.144155 -4.1420746 -4.121305 -4.0798283 -4.0558753 -4.0639544 -4.0870848 -4.1054058 -4.1115675 -4.1239219 -4.1279116 -4.1143026][-4.0935674 -4.105782 -4.126853 -4.141715 -4.1416163 -4.1220446 -4.080894 -4.05137 -4.0520697 -4.0709224 -4.0978489 -4.1162329 -4.1354752 -4.1419325 -4.1268106][-4.1051507 -4.1107955 -4.1201477 -4.1248741 -4.1170025 -4.0993328 -4.0593996 -4.0240083 -4.0124955 -4.0305104 -4.0697169 -4.1020045 -4.1261396 -4.1369061 -4.1237621][-4.1307197 -4.1276717 -4.11954 -4.1043344 -4.0843167 -4.064198 -4.0267968 -3.9871743 -3.9680858 -3.9870336 -4.0350814 -4.0740452 -4.097353 -4.1104355 -4.1004109][-4.1453171 -4.1314917 -4.1066732 -4.0769024 -4.0435176 -4.0219088 -3.9896231 -3.947813 -3.9227643 -3.9364855 -3.9830525 -4.0254965 -4.0520806 -4.0703082 -4.0718679][-4.1450477 -4.1194663 -4.0836921 -4.0380983 -3.9940066 -3.9823494 -3.968468 -3.9359164 -3.9114475 -3.9183314 -3.9534912 -3.9957323 -4.0279088 -4.054615 -4.0707908][-4.136539 -4.10577 -4.0642114 -4.0108752 -3.9710131 -3.9816368 -3.9883583 -3.9664328 -3.9578984 -3.9746056 -4.0012193 -4.033215 -4.0568733 -4.0824523 -4.1039305][-4.132051 -4.1133389 -4.086071 -4.0442152 -4.0181069 -4.0349765 -4.0468969 -4.0366154 -4.0422931 -4.0697126 -4.0909743 -4.1084766 -4.1175585 -4.1309638 -4.1464386][-4.143158 -4.1420145 -4.1356874 -4.1150517 -4.1013994 -4.1169477 -4.124629 -4.11477 -4.1215615 -4.1480179 -4.1669893 -4.1777663 -4.1795135 -4.1851625 -4.1906509][-4.1775303 -4.1886716 -4.1967854 -4.1925521 -4.1894684 -4.202086 -4.2036848 -4.1878214 -4.1880336 -4.2099428 -4.2267413 -4.2341037 -4.2314196 -4.2308087 -4.2279468][-4.2203159 -4.2358336 -4.2512841 -4.2586894 -4.2610741 -4.2698112 -4.2680788 -4.2512894 -4.2460637 -4.2591591 -4.2705655 -4.2726088 -4.2658396 -4.2584062 -4.2495165][-4.246191 -4.2619829 -4.2782974 -4.2888889 -4.2926178 -4.2963591 -4.2948589 -4.2820458 -4.2758193 -4.28393 -4.2887964 -4.2865973 -4.277349 -4.2670388 -4.2569504][-4.25741 -4.2695017 -4.2818871 -4.2911291 -4.2929091 -4.2894607 -4.2828884 -4.2718415 -4.2684455 -4.2758636 -4.281877 -4.28233 -4.2782135 -4.270678 -4.261034]]...]
INFO - root - 2017-12-08 00:48:53.828850: step 73610, loss = 2.04, batch loss = 1.98 (12.4 examples/sec; 0.646 sec/batch; 46h:28m:43s remains)
INFO - root - 2017-12-08 00:49:00.642457: step 73620, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.696 sec/batch; 50h:03m:26s remains)
INFO - root - 2017-12-08 00:49:07.516318: step 73630, loss = 2.11, batch loss = 2.05 (11.6 examples/sec; 0.691 sec/batch; 49h:41m:01s remains)
INFO - root - 2017-12-08 00:49:14.239485: step 73640, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.666 sec/batch; 47h:53m:51s remains)
INFO - root - 2017-12-08 00:49:20.952240: step 73650, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.658 sec/batch; 47h:19m:28s remains)
INFO - root - 2017-12-08 00:49:27.719926: step 73660, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.659 sec/batch; 47h:22m:50s remains)
INFO - root - 2017-12-08 00:49:34.443317: step 73670, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 48h:30m:26s remains)
INFO - root - 2017-12-08 00:49:41.234363: step 73680, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 50h:24m:10s remains)
INFO - root - 2017-12-08 00:49:48.044131: step 73690, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 48h:43m:11s remains)
INFO - root - 2017-12-08 00:49:54.726762: step 73700, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 45h:21m:46s remains)
2017-12-08 00:49:55.430394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3043094 -4.2964911 -4.2887869 -4.2852812 -4.2817597 -4.2757525 -4.2678933 -4.2596197 -4.2584705 -4.266046 -4.2808447 -4.2913589 -4.2954021 -4.2951851 -4.2907009][-4.3004045 -4.2910223 -4.28317 -4.2801404 -4.2746739 -4.2674918 -4.25583 -4.2399831 -4.2337193 -4.2413821 -4.2589269 -4.2723217 -4.2780905 -4.2795925 -4.2749033][-4.297657 -4.2874975 -4.280683 -4.2745757 -4.2660651 -4.2573671 -4.2405787 -4.2198057 -4.2047434 -4.2101011 -4.2292848 -4.2456989 -4.2575321 -4.2651625 -4.2622204][-4.2949185 -4.2821083 -4.2726874 -4.2604713 -4.2505779 -4.2434764 -4.2252374 -4.2039604 -4.1817322 -4.1807809 -4.1967025 -4.2170916 -4.2390404 -4.2576041 -4.2585397][-4.2814212 -4.25822 -4.2382388 -4.216208 -4.20724 -4.2078562 -4.1990113 -4.1788583 -4.1539297 -4.1449904 -4.1522961 -4.1703262 -4.1989374 -4.2340584 -4.2484617][-4.2616577 -4.2236557 -4.1901646 -4.1587553 -4.1510749 -4.1589422 -4.1622968 -4.1490946 -4.1260428 -4.1057816 -4.0953078 -4.099494 -4.1297655 -4.1835423 -4.2178884][-4.2512674 -4.2021003 -4.1599269 -4.124958 -4.1137919 -4.1195765 -4.1264353 -4.1197968 -4.0976982 -4.0715852 -4.0448451 -4.0319681 -4.0591969 -4.1201739 -4.1687551][-4.2491722 -4.1919351 -4.1430335 -4.1047969 -4.0901217 -4.0963326 -4.1073475 -4.1039629 -4.0802116 -4.0549607 -4.0244942 -4.0055 -4.0275879 -4.0832629 -4.1320763][-4.2552371 -4.195765 -4.1410923 -4.1008449 -4.0884013 -4.0993981 -4.1136684 -4.1101623 -4.0848193 -4.0582948 -4.030705 -4.0154066 -4.0357909 -4.0826011 -4.1194067][-4.2704153 -4.2163973 -4.163065 -4.1220608 -4.110857 -4.1255746 -4.1415372 -4.1386733 -4.11652 -4.0929241 -4.0685139 -4.0579934 -4.080008 -4.1192422 -4.1424675][-4.2815948 -4.23479 -4.1884069 -4.1554375 -4.1453996 -4.1594739 -4.1808014 -4.1838942 -4.1693382 -4.1493745 -4.122283 -4.110167 -4.1330652 -4.167449 -4.18095][-4.2967238 -4.2572427 -4.2184663 -4.1934819 -4.1886415 -4.2050166 -4.2325482 -4.2437477 -4.2334847 -4.2156982 -4.188952 -4.1732793 -4.189878 -4.2163086 -4.2232566][-4.3148117 -4.2857938 -4.2536759 -4.2334075 -4.2341719 -4.2541046 -4.2822275 -4.2987747 -4.2947874 -4.2798328 -4.2571797 -4.2398396 -4.2457824 -4.262126 -4.2646165][-4.3246679 -4.3054729 -4.2800274 -4.2637239 -4.2674227 -4.2872763 -4.3126478 -4.3301206 -4.3324261 -4.3238897 -4.3077369 -4.2929959 -4.2937031 -4.3009353 -4.2980733][-4.3296113 -4.3185105 -4.2992859 -4.285347 -4.2869182 -4.3011332 -4.3195577 -4.3334274 -4.3379307 -4.3361216 -4.3288231 -4.3218007 -4.3222141 -4.3241143 -4.3182621]]...]
INFO - root - 2017-12-08 00:50:02.068486: step 73710, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.724 sec/batch; 52h:02m:46s remains)
INFO - root - 2017-12-08 00:50:08.961789: step 73720, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 49h:03m:18s remains)
INFO - root - 2017-12-08 00:50:15.781749: step 73730, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.672 sec/batch; 48h:17m:24s remains)
INFO - root - 2017-12-08 00:50:22.706321: step 73740, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 48h:08m:39s remains)
INFO - root - 2017-12-08 00:50:29.292513: step 73750, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.689 sec/batch; 49h:29m:59s remains)
INFO - root - 2017-12-08 00:50:36.091524: step 73760, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.700 sec/batch; 50h:18m:59s remains)
INFO - root - 2017-12-08 00:50:42.864877: step 73770, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 50h:29m:56s remains)
INFO - root - 2017-12-08 00:50:49.587024: step 73780, loss = 2.08, batch loss = 2.02 (12.6 examples/sec; 0.634 sec/batch; 45h:35m:16s remains)
INFO - root - 2017-12-08 00:50:56.361023: step 73790, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.682 sec/batch; 49h:01m:18s remains)
INFO - root - 2017-12-08 00:51:03.128324: step 73800, loss = 2.09, batch loss = 2.04 (10.4 examples/sec; 0.772 sec/batch; 55h:29m:02s remains)
2017-12-08 00:51:03.851131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2556286 -4.2543592 -4.2513814 -4.24774 -4.2412014 -4.2339668 -4.2269292 -4.2132335 -4.1990042 -4.1944938 -4.2069178 -4.2222695 -4.2269206 -4.2269759 -4.2244081][-4.2682838 -4.2674003 -4.2644286 -4.2598014 -4.2543507 -4.2492466 -4.2436333 -4.2322779 -4.2213278 -4.2172318 -4.2264352 -4.2387586 -4.2432828 -4.2473516 -4.253159][-4.274128 -4.2733293 -4.2700062 -4.2641296 -4.2570963 -4.248611 -4.2375932 -4.2236958 -4.2155867 -4.2146645 -4.2215838 -4.2310143 -4.2366452 -4.2467389 -4.2619128][-4.2632432 -4.26141 -4.2565136 -4.2475519 -4.235312 -4.21655 -4.1939712 -4.1764417 -4.1730146 -4.1794696 -4.187079 -4.1936941 -4.1983094 -4.2120738 -4.2317991][-4.2287283 -4.2272682 -4.2241206 -4.2140474 -4.1915326 -4.1543713 -4.1141424 -4.0906763 -4.0938659 -4.1105843 -4.1218929 -4.1269574 -4.1314473 -4.1487856 -4.172761][-4.1686358 -4.1693835 -4.1696091 -4.1570578 -4.1208258 -4.062737 -4.0005207 -3.9689605 -3.9837053 -4.0164785 -4.0413833 -4.0541668 -4.0634689 -4.0862303 -4.1152039][-4.1004219 -4.1022511 -4.1049271 -4.08892 -4.0405087 -3.9643683 -3.8817146 -3.8455663 -3.8822868 -3.9426129 -3.9905982 -4.016623 -4.0321765 -4.058568 -4.0920782][-4.0390172 -4.0411215 -4.0486879 -4.0344048 -3.9867547 -3.91449 -3.8450079 -3.8284535 -3.8821621 -3.9566789 -4.0170512 -4.0507665 -4.0675225 -4.0897617 -4.1216874][-4.0348611 -4.0358744 -4.0468969 -4.0400805 -4.0098829 -3.9700637 -3.9428992 -3.9511447 -3.9928997 -4.0479894 -4.0969987 -4.1251616 -4.1372204 -4.149344 -4.1700563][-4.1032271 -4.0989852 -4.1043582 -4.0994067 -4.0842962 -4.0710812 -4.0723534 -4.0910835 -4.1189342 -4.1522765 -4.1802077 -4.191433 -4.1924481 -4.1897011 -4.1928496][-4.186646 -4.17644 -4.1724076 -4.166255 -4.1601372 -4.1591816 -4.1695228 -4.186173 -4.2014031 -4.216845 -4.2251077 -4.2214131 -4.2103848 -4.1947556 -4.1862416][-4.2490892 -4.23422 -4.2233324 -4.2170544 -4.217063 -4.2177758 -4.22324 -4.2303514 -4.233088 -4.2329946 -4.223928 -4.2042003 -4.1798725 -4.1554556 -4.1480532][-4.2839293 -4.2667551 -4.2513032 -4.2451849 -4.2474661 -4.2448812 -4.2420249 -4.2405787 -4.2338061 -4.2221661 -4.1984177 -4.1654797 -4.1306987 -4.102335 -4.0974622][-4.2936492 -4.276094 -4.2587347 -4.2542238 -4.2565374 -4.2522955 -4.2446852 -4.2377315 -4.2244596 -4.2062063 -4.1758904 -4.1392941 -4.0988765 -4.0661297 -4.0586777][-4.2798085 -4.2647939 -4.2499671 -4.2488623 -4.25303 -4.2488313 -4.2415771 -4.2383466 -4.2298169 -4.2117491 -4.1786928 -4.1421223 -4.1020927 -4.0672011 -4.0561228]]...]
INFO - root - 2017-12-08 00:51:10.565568: step 73810, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 47h:38m:49s remains)
INFO - root - 2017-12-08 00:51:17.405791: step 73820, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 52h:48m:22s remains)
INFO - root - 2017-12-08 00:51:24.277666: step 73830, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 52h:15m:23s remains)
INFO - root - 2017-12-08 00:51:31.142423: step 73840, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 49h:38m:33s remains)
INFO - root - 2017-12-08 00:51:37.894174: step 73850, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 46h:52m:34s remains)
INFO - root - 2017-12-08 00:51:44.707607: step 73860, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 47h:03m:49s remains)
INFO - root - 2017-12-08 00:51:51.573251: step 73870, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 51h:43m:06s remains)
INFO - root - 2017-12-08 00:51:58.353736: step 73880, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.694 sec/batch; 49h:51m:07s remains)
INFO - root - 2017-12-08 00:52:05.168127: step 73890, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.686 sec/batch; 49h:17m:39s remains)
INFO - root - 2017-12-08 00:52:12.021040: step 73900, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 45h:15m:52s remains)
2017-12-08 00:52:12.745753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3062716 -4.2955103 -4.287991 -4.2850189 -4.2853775 -4.288693 -4.2917466 -4.2908697 -4.2926011 -4.2978363 -4.2999229 -4.3023977 -4.3153734 -4.3306017 -4.3387465][-4.2692904 -4.2508583 -4.2383919 -4.2349277 -4.2387295 -4.24703 -4.2498093 -4.2409182 -4.2380791 -4.2477331 -4.2529521 -4.2591276 -4.2818232 -4.3103757 -4.3268776][-4.2144217 -4.1889024 -4.1725016 -4.1703515 -4.1811266 -4.1961384 -4.1955519 -4.1764421 -4.1707492 -4.1896291 -4.2007155 -4.2094941 -4.241991 -4.2839355 -4.3095603][-4.148387 -4.1184878 -4.1005716 -4.1033807 -4.1243725 -4.1454344 -4.1380734 -4.1037211 -4.0917382 -4.1241393 -4.145257 -4.1566577 -4.2008204 -4.2537384 -4.2880287][-4.0800753 -4.0493488 -4.0262914 -4.0311818 -4.0629449 -4.095727 -4.0810375 -4.0199223 -3.9945624 -4.0483232 -4.0872984 -4.1052732 -4.1587391 -4.2196188 -4.2623353][-4.0169387 -3.9829278 -3.9435463 -3.938246 -3.9785533 -4.017828 -3.9878135 -3.8891644 -3.8623013 -3.9568698 -4.0237741 -4.0533767 -4.1166897 -4.1860523 -4.2374496][-3.9941802 -3.9550877 -3.8974445 -3.8715651 -3.9001637 -3.9212718 -3.8571634 -3.7113786 -3.6932278 -3.8415468 -3.9428554 -3.9956584 -4.0788016 -4.1610651 -4.2240381][-4.0206351 -3.9846008 -3.9223571 -3.8860776 -3.8998253 -3.8932688 -3.7973228 -3.6267424 -3.6209488 -3.7935061 -3.9094515 -3.9780433 -4.0753098 -4.1640472 -4.2304254][-4.0695858 -4.0477314 -3.9993694 -3.9720459 -3.98557 -3.9699502 -3.8851123 -3.7564418 -3.7588313 -3.8869767 -3.9729753 -4.0330572 -4.1204548 -4.1957693 -4.2519732][-4.1188774 -4.1102095 -4.0801053 -4.0626159 -4.0772166 -4.0690913 -4.0080371 -3.924778 -3.9316366 -4.0118752 -4.0624642 -4.1036267 -4.1707129 -4.2303038 -4.2726808][-4.1732483 -4.1691275 -4.1496515 -4.1337876 -4.1431608 -4.1391134 -4.0978417 -4.0474072 -4.0577216 -4.1064596 -4.1340442 -4.1619515 -4.21353 -4.259201 -4.2888632][-4.2268028 -4.2245522 -4.212419 -4.1997786 -4.20131 -4.1974926 -4.1708069 -4.1430578 -4.151391 -4.1797929 -4.1953979 -4.215414 -4.2520037 -4.2843003 -4.3037224][-4.2689266 -4.266489 -4.2599735 -4.2508135 -4.2488437 -4.2467389 -4.23343 -4.2203803 -4.22369 -4.2398248 -4.2505236 -4.2653518 -4.288168 -4.3065577 -4.3169255][-4.294878 -4.2915936 -4.2881145 -4.2832656 -4.2821655 -4.2824097 -4.2786813 -4.2742829 -4.2773767 -4.2855415 -4.2913885 -4.2998767 -4.3114429 -4.3203163 -4.3254437][-4.3089323 -4.30512 -4.3035579 -4.3020005 -4.3031387 -4.3053226 -4.3054056 -4.3037176 -4.3036752 -4.3076029 -4.3101087 -4.3136959 -4.3191519 -4.3242068 -4.3273377]]...]
INFO - root - 2017-12-08 00:52:19.388224: step 73910, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.708 sec/batch; 50h:50m:05s remains)
INFO - root - 2017-12-08 00:52:26.223412: step 73920, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 51h:09m:09s remains)
INFO - root - 2017-12-08 00:52:33.028310: step 73930, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.693 sec/batch; 49h:47m:21s remains)
INFO - root - 2017-12-08 00:52:39.925273: step 73940, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.679 sec/batch; 48h:44m:45s remains)
INFO - root - 2017-12-08 00:52:46.768042: step 73950, loss = 2.08, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 51h:31m:43s remains)
INFO - root - 2017-12-08 00:52:53.750127: step 73960, loss = 2.08, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 52h:24m:22s remains)
INFO - root - 2017-12-08 00:53:00.607481: step 73970, loss = 2.06, batch loss = 2.01 (11.2 examples/sec; 0.717 sec/batch; 51h:29m:02s remains)
INFO - root - 2017-12-08 00:53:07.297311: step 73980, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 46h:53m:39s remains)
INFO - root - 2017-12-08 00:53:14.159763: step 73990, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:16m:51s remains)
INFO - root - 2017-12-08 00:53:20.888202: step 74000, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 50h:08m:35s remains)
2017-12-08 00:53:21.597347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2301269 -4.2380905 -4.2474394 -4.2479024 -4.2337103 -4.2025833 -4.1829123 -4.1870584 -4.18873 -4.1823015 -4.1751766 -4.1621933 -4.1298714 -4.0974617 -4.1034737][-4.2718406 -4.2738161 -4.2757711 -4.2702279 -4.2541656 -4.2285409 -4.2106075 -4.2118306 -4.2077045 -4.2012024 -4.1986055 -4.1900525 -4.1623187 -4.1333661 -4.1366105][-4.3068008 -4.3018627 -4.2945905 -4.2755594 -4.2572417 -4.2419858 -4.230103 -4.2289257 -4.2241707 -4.217258 -4.2161627 -4.2118874 -4.1938095 -4.1765471 -4.1805654][-4.3289948 -4.3159008 -4.2977953 -4.2708449 -4.2499566 -4.2381349 -4.2295294 -4.2276406 -4.2222648 -4.2161627 -4.2147641 -4.2162895 -4.2132244 -4.2150126 -4.220067][-4.334486 -4.315011 -4.2898717 -4.2596745 -4.2289367 -4.2049007 -4.1963682 -4.1932087 -4.1846957 -4.1830087 -4.1857834 -4.1958957 -4.2109246 -4.234179 -4.2464471][-4.3210721 -4.2960591 -4.269433 -4.2361326 -4.1969953 -4.1619868 -4.1539917 -4.1487341 -4.13648 -4.1340246 -4.1465006 -4.1713667 -4.2037244 -4.2390075 -4.2608461][-4.2917852 -4.2692685 -4.2455659 -4.2142868 -4.1769438 -4.1456141 -4.1374841 -4.1306214 -4.1106482 -4.0991974 -4.1246886 -4.1665888 -4.2053542 -4.2362223 -4.2600517][-4.260273 -4.2474418 -4.23251 -4.2102356 -4.1828537 -4.1608081 -4.150774 -4.1435008 -4.1111059 -4.0897927 -4.1228938 -4.1752238 -4.2145338 -4.2366695 -4.2542162][-4.2374706 -4.2380586 -4.2338104 -4.2189507 -4.199676 -4.18155 -4.1727109 -4.1671605 -4.1355405 -4.1125803 -4.1393061 -4.1937671 -4.2317643 -4.250648 -4.2616768][-4.2260566 -4.2347031 -4.237009 -4.2272067 -4.2135959 -4.197845 -4.1941609 -4.1990104 -4.1840434 -4.170444 -4.1839476 -4.2232985 -4.253171 -4.2683554 -4.2749104][-4.224453 -4.2332416 -4.2363763 -4.2292614 -4.2221317 -4.2136955 -4.2133942 -4.2303767 -4.2352653 -4.2315125 -4.2353954 -4.2564816 -4.2697449 -4.2750463 -4.2750654][-4.2322383 -4.2368722 -4.2398705 -4.2372313 -4.2336459 -4.2299643 -4.2304974 -4.2505355 -4.2639084 -4.2637124 -4.2645807 -4.2737961 -4.2750707 -4.2696147 -4.2611365][-4.2371716 -4.2338886 -4.2334876 -4.2368684 -4.2417107 -4.2445111 -4.2462406 -4.2598863 -4.2722788 -4.2714577 -4.2697015 -4.2724175 -4.2692742 -4.2608004 -4.2472587][-4.2285933 -4.216558 -4.2111359 -4.2186527 -4.2316041 -4.2386045 -4.2424951 -4.2535982 -4.2652645 -4.2706532 -4.2695689 -4.2624235 -4.250977 -4.2406631 -4.23121][-4.2081585 -4.1867929 -4.1787891 -4.1935663 -4.2182417 -4.2348776 -4.2423372 -4.254077 -4.2631416 -4.2685328 -4.2635 -4.2478037 -4.2293868 -4.217227 -4.2089434]]...]
INFO - root - 2017-12-08 00:53:28.107149: step 74010, loss = 2.10, batch loss = 2.04 (11.8 examples/sec; 0.680 sec/batch; 48h:47m:32s remains)
INFO - root - 2017-12-08 00:53:34.846787: step 74020, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.632 sec/batch; 45h:23m:05s remains)
INFO - root - 2017-12-08 00:53:41.760143: step 74030, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 48h:40m:09s remains)
INFO - root - 2017-12-08 00:53:48.614700: step 74040, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 50h:42m:37s remains)
INFO - root - 2017-12-08 00:53:55.404944: step 74050, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.729 sec/batch; 52h:20m:36s remains)
INFO - root - 2017-12-08 00:54:01.998919: step 74060, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 46h:42m:57s remains)
INFO - root - 2017-12-08 00:54:08.875847: step 74070, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.680 sec/batch; 48h:48m:20s remains)
INFO - root - 2017-12-08 00:54:15.652389: step 74080, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.739 sec/batch; 53h:03m:23s remains)
INFO - root - 2017-12-08 00:54:22.491422: step 74090, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.707 sec/batch; 50h:44m:24s remains)
INFO - root - 2017-12-08 00:54:29.324295: step 74100, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 49h:00m:51s remains)
2017-12-08 00:54:30.068262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.358129 -4.3568149 -4.3567352 -4.3573804 -4.3596034 -4.3629556 -4.3653812 -4.3673267 -4.3667 -4.3624244 -4.3588824 -4.35632 -4.3551826 -4.3573508 -4.3592963][-4.346292 -4.3422194 -4.3426061 -4.3437953 -4.3462329 -4.3492975 -4.3508019 -4.3531575 -4.3518677 -4.3468928 -4.3445168 -4.3409534 -4.3387594 -4.3407383 -4.34436][-4.3220606 -4.3137732 -4.3144126 -4.3171926 -4.3178229 -4.31529 -4.313035 -4.316843 -4.3180051 -4.3168306 -4.3207493 -4.3217883 -4.3220854 -4.3235397 -4.3247967][-4.2722964 -4.263814 -4.2700582 -4.2799478 -4.2827549 -4.275063 -4.2675638 -4.2673597 -4.268743 -4.2748909 -4.2891555 -4.301023 -4.3084717 -4.3125138 -4.3092475][-4.1965747 -4.1923165 -4.2174106 -4.2493281 -4.2585583 -4.2425189 -4.2195859 -4.2078834 -4.2133389 -4.2284765 -4.2499061 -4.2699785 -4.2862177 -4.2947197 -4.2850289][-4.1181588 -4.1203985 -4.1688604 -4.219543 -4.2212529 -4.1815805 -4.1287746 -4.0973573 -4.1115532 -4.1495976 -4.1870432 -4.214889 -4.239378 -4.2469783 -4.2178917][-4.0441432 -4.0605083 -4.124649 -4.1752758 -4.1565871 -4.0788293 -3.9764843 -3.9163463 -3.9516621 -4.0308313 -4.0878263 -4.1225581 -4.1501112 -4.1470208 -4.0943694][-4.0101333 -4.0284576 -4.0802679 -4.1106658 -4.063787 -3.9401324 -3.7804475 -3.6965723 -3.7825065 -3.9182258 -3.9938686 -4.0301166 -4.0502658 -4.0315456 -3.9518731][-4.023973 -4.0290856 -4.0529251 -4.0552487 -3.9906709 -3.8600302 -3.7034922 -3.6462982 -3.7736738 -3.9245934 -3.9948 -4.0188937 -4.0214777 -3.98215 -3.8857982][-4.0717173 -4.0677686 -4.0714726 -4.0647364 -4.0221572 -3.944953 -3.8658152 -3.859827 -3.94808 -4.0446668 -4.0863547 -4.0933108 -4.0836821 -4.0420408 -3.9665568][-4.1238451 -4.1224446 -4.1222639 -4.1200581 -4.1046662 -4.0721736 -4.0408983 -4.0403833 -4.0779958 -4.1281629 -4.1494007 -4.1510782 -4.1443377 -4.1253614 -4.0926876][-4.1734715 -4.1755128 -4.1810141 -4.1849356 -4.1837158 -4.1703253 -4.1484456 -4.13187 -4.130136 -4.1472673 -4.1535597 -4.1546154 -4.16187 -4.1737142 -4.1801567][-4.2246161 -4.2266545 -4.2347264 -4.2375231 -4.2332354 -4.214385 -4.1810112 -4.1475663 -4.1299438 -4.1340928 -4.1392164 -4.1520681 -4.1802578 -4.216042 -4.2416229][-4.2658911 -4.2676458 -4.2710915 -4.2669754 -4.25713 -4.2302132 -4.1842947 -4.1424379 -4.1271753 -4.1323462 -4.1488266 -4.174181 -4.2136173 -4.2578278 -4.2848983][-4.2927523 -4.2953477 -4.2962489 -4.2884879 -4.2726235 -4.24044 -4.1928372 -4.1563177 -4.1505604 -4.1651769 -4.1928444 -4.2208185 -4.2537618 -4.2849665 -4.3024879]]...]
INFO - root - 2017-12-08 00:54:36.576910: step 74110, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.705 sec/batch; 50h:37m:48s remains)
INFO - root - 2017-12-08 00:54:43.335970: step 74120, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.664 sec/batch; 47h:39m:09s remains)
INFO - root - 2017-12-08 00:54:50.067490: step 74130, loss = 2.10, batch loss = 2.04 (12.3 examples/sec; 0.651 sec/batch; 46h:45m:27s remains)
INFO - root - 2017-12-08 00:54:56.937178: step 74140, loss = 2.09, batch loss = 2.04 (12.4 examples/sec; 0.646 sec/batch; 46h:21m:37s remains)
INFO - root - 2017-12-08 00:55:03.799373: step 74150, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.722 sec/batch; 51h:46m:54s remains)
INFO - root - 2017-12-08 00:55:10.684150: step 74160, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.699 sec/batch; 50h:09m:51s remains)
INFO - root - 2017-12-08 00:55:17.557132: step 74170, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 48h:41m:55s remains)
INFO - root - 2017-12-08 00:55:24.354036: step 74180, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 45h:13m:10s remains)
INFO - root - 2017-12-08 00:55:31.240965: step 74190, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 48h:55m:06s remains)
INFO - root - 2017-12-08 00:55:38.129878: step 74200, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.732 sec/batch; 52h:31m:53s remains)
2017-12-08 00:55:38.865357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.04527 -4.0661006 -4.0805707 -4.1063228 -4.13673 -4.1647668 -4.1775002 -4.1660337 -4.134429 -4.1003909 -4.0736876 -4.0682979 -4.0828056 -4.1211162 -4.1499033][-4.0684123 -4.0891838 -4.1012416 -4.1221972 -4.1502872 -4.1846151 -4.2080226 -4.2042885 -4.1818943 -4.156045 -4.1286316 -4.0998373 -4.0844555 -4.11029 -4.1426458][-4.1031432 -4.1294765 -4.1424155 -4.1538444 -4.1741138 -4.20519 -4.2240996 -4.2160959 -4.1990275 -4.1857057 -4.1639905 -4.1258121 -4.08756 -4.0933352 -4.1192141][-4.1221595 -4.1503453 -4.1633897 -4.1696568 -4.1844053 -4.2028575 -4.1996307 -4.1776586 -4.1661234 -4.1700764 -4.172307 -4.151403 -4.1161141 -4.1098948 -4.1321425][-4.1138611 -4.1462016 -4.1683841 -4.1815872 -4.1886196 -4.1803689 -4.1430759 -4.0987811 -4.0843487 -4.1011848 -4.1374021 -4.1552825 -4.1461625 -4.144443 -4.1647105][-4.0780821 -4.1231556 -4.1604629 -4.1810994 -4.1764808 -4.1381421 -4.0653181 -3.9869413 -3.9489374 -3.9630263 -4.0415807 -4.1206532 -4.1540155 -4.1682825 -4.1881971][-4.0120029 -4.0732641 -4.1277242 -4.1520214 -4.1357322 -4.0734954 -3.9719946 -3.8590093 -3.7797363 -3.7848394 -3.9081371 -4.052598 -4.1351514 -4.173111 -4.1937504][-3.9431477 -4.0076241 -4.0675011 -4.0938773 -4.0791941 -4.0132055 -3.9012957 -3.7713275 -3.6787992 -3.6940064 -3.8386879 -4.0077896 -4.1155868 -4.1685 -4.1781034][-3.9439349 -3.9877517 -4.0293651 -4.049984 -4.0445857 -3.9998543 -3.9165049 -3.8222585 -3.764761 -3.7861614 -3.8971081 -4.0303965 -4.1228571 -4.16461 -4.1611295][-4.0132852 -4.0316563 -4.0496545 -4.0608897 -4.0635128 -4.0512562 -4.0092506 -3.9617014 -3.9374728 -3.9525778 -4.0159931 -4.0971451 -4.1542921 -4.1735673 -4.162241][-4.10464 -4.1086688 -4.1125813 -4.1135135 -4.113667 -4.1140976 -4.0960112 -4.0783787 -4.0767641 -4.0878983 -4.1192713 -4.1619611 -4.1902947 -4.1930113 -4.184412][-4.1901207 -4.1897955 -4.1852589 -4.1751571 -4.1662049 -4.1649766 -4.157547 -4.1571665 -4.1686292 -4.1807084 -4.1986141 -4.2215357 -4.235651 -4.232533 -4.2262363][-4.2609491 -4.2562065 -4.2465067 -4.2294865 -4.2146997 -4.2102771 -4.2072477 -4.2165871 -4.2376289 -4.2548943 -4.2675428 -4.2785783 -4.2845821 -4.2801952 -4.2716942][-4.3172626 -4.3121052 -4.2984824 -4.279726 -4.2648549 -4.2590952 -4.2595129 -4.2722659 -4.2957349 -4.3149447 -4.3254409 -4.3284097 -4.326252 -4.3189836 -4.3078427][-4.3484035 -4.3452787 -4.3338671 -4.3188491 -4.3065877 -4.3009834 -4.302475 -4.3139467 -4.3333964 -4.3484836 -4.3554697 -4.354713 -4.3490224 -4.3403244 -4.3303976]]...]
INFO - root - 2017-12-08 00:55:45.563804: step 74210, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 46h:39m:56s remains)
INFO - root - 2017-12-08 00:55:52.386228: step 74220, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.702 sec/batch; 50h:20m:29s remains)
INFO - root - 2017-12-08 00:55:59.200193: step 74230, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.723 sec/batch; 51h:53m:57s remains)
INFO - root - 2017-12-08 00:56:06.007309: step 74240, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 49h:00m:53s remains)
INFO - root - 2017-12-08 00:56:12.752970: step 74250, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.670 sec/batch; 48h:02m:00s remains)
INFO - root - 2017-12-08 00:56:19.623304: step 74260, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 47h:32m:48s remains)
INFO - root - 2017-12-08 00:56:26.520906: step 74270, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 50h:30m:58s remains)
INFO - root - 2017-12-08 00:56:33.425109: step 74280, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 49h:17m:08s remains)
INFO - root - 2017-12-08 00:56:40.325984: step 74290, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 48h:39m:36s remains)
INFO - root - 2017-12-08 00:56:47.067079: step 74300, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 47h:56m:14s remains)
2017-12-08 00:56:47.820533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.248477 -4.249177 -4.2487259 -4.2326593 -4.2070751 -4.1839352 -4.1735296 -4.1857014 -4.1880693 -4.1901536 -4.1768432 -4.1556497 -4.1612458 -4.179142 -4.2016587][-4.2813749 -4.2774944 -4.269671 -4.2483354 -4.2186122 -4.187355 -4.1671653 -4.1820655 -4.1920209 -4.2007113 -4.189497 -4.1646023 -4.1598029 -4.1768761 -4.2053785][-4.2980428 -4.294137 -4.2790394 -4.2431884 -4.1964445 -4.1538897 -4.127522 -4.1547432 -4.1864729 -4.2087221 -4.2027087 -4.1712666 -4.1527114 -4.165513 -4.199964][-4.297823 -4.3004065 -4.2840118 -4.2320323 -4.1610951 -4.09802 -4.0601611 -4.1009612 -4.162158 -4.2076464 -4.2114639 -4.1773448 -4.1454048 -4.1480331 -4.1809258][-4.2735028 -4.283864 -4.2714133 -4.2063913 -4.1103611 -4.0166111 -3.9534843 -4.0086675 -4.1044827 -4.1755824 -4.1967149 -4.1685939 -4.1264429 -4.1165819 -4.141315][-4.222136 -4.2386045 -4.2365785 -4.1667061 -4.0475163 -3.9170163 -3.8117132 -3.8770394 -4.0153084 -4.1167426 -4.164547 -4.1540208 -4.1069393 -4.0847263 -4.0889888][-4.1708212 -4.192904 -4.2080789 -4.1499295 -4.02708 -3.8733339 -3.7215557 -3.7747803 -3.9398873 -4.0598078 -4.1335163 -4.1481977 -4.1097198 -4.0814557 -4.0643039][-4.1492753 -4.1752205 -4.2040787 -4.1667638 -4.0609121 -3.9246275 -3.782676 -3.8042459 -3.9429352 -4.0454407 -4.1201324 -4.1474347 -4.1251006 -4.106792 -4.0835142][-4.147573 -4.175385 -4.2116165 -4.1947885 -4.1123295 -4.0107627 -3.9102254 -3.9144788 -4.0101485 -4.0805583 -4.1383152 -4.1656742 -4.1544418 -4.1481647 -4.1258092][-4.1643987 -4.1881571 -4.2250896 -4.2260442 -4.1692944 -4.0979824 -4.0302343 -4.0258231 -4.0891995 -4.1343412 -4.1742263 -4.1980329 -4.1936574 -4.1957278 -4.1800137][-4.1798544 -4.1978173 -4.230257 -4.2408342 -4.2087741 -4.166738 -4.1233425 -4.1201973 -4.1635675 -4.1894507 -4.2149997 -4.2347946 -4.2357149 -4.2401738 -4.237752][-4.1871262 -4.2016497 -4.2296362 -4.2474694 -4.2377381 -4.21848 -4.1938038 -4.1932888 -4.2232265 -4.2393351 -4.2558293 -4.2693405 -4.2706237 -4.2736969 -4.2797155][-4.2040291 -4.2161207 -4.2403955 -4.2634187 -4.2712221 -4.268754 -4.25494 -4.2559557 -4.2795653 -4.2909431 -4.298069 -4.3014588 -4.2996778 -4.3018126 -4.3076887][-4.2437034 -4.2516184 -4.2699652 -4.2904925 -4.3042331 -4.3079414 -4.2979717 -4.2959661 -4.3095775 -4.3158574 -4.3158793 -4.3124547 -4.3109379 -4.3123879 -4.3154016][-4.2754087 -4.2807875 -4.2929378 -4.3078933 -4.31919 -4.3234444 -4.3171129 -4.3140273 -4.3184443 -4.3183789 -4.3141294 -4.3087993 -4.3064556 -4.3068357 -4.309639]]...]
INFO - root - 2017-12-08 00:56:54.451109: step 74310, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 50h:43m:51s remains)
INFO - root - 2017-12-08 00:57:01.292601: step 74320, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.667 sec/batch; 47h:50m:59s remains)
INFO - root - 2017-12-08 00:57:08.085109: step 74330, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 49h:02m:10s remains)
INFO - root - 2017-12-08 00:57:14.900725: step 74340, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.655 sec/batch; 46h:58m:26s remains)
INFO - root - 2017-12-08 00:57:21.718668: step 74350, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 50h:43m:26s remains)
INFO - root - 2017-12-08 00:57:28.464981: step 74360, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 49h:05m:48s remains)
INFO - root - 2017-12-08 00:57:35.031006: step 74370, loss = 2.09, batch loss = 2.03 (12.4 examples/sec; 0.644 sec/batch; 46h:08m:32s remains)
INFO - root - 2017-12-08 00:57:41.820947: step 74380, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 47h:55m:28s remains)
INFO - root - 2017-12-08 00:57:48.697711: step 74390, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 52h:00m:39s remains)
INFO - root - 2017-12-08 00:57:55.551662: step 74400, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 48h:03m:24s remains)
2017-12-08 00:57:56.278497: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.294549 -4.29926 -4.3137345 -4.3176193 -4.307436 -4.2937169 -4.2807608 -4.2729135 -4.2721467 -4.2751961 -4.275331 -4.2725506 -4.27001 -4.2745571 -4.278821][-4.2566538 -4.2631254 -4.290688 -4.304738 -4.2925444 -4.2697835 -4.2455344 -4.2286649 -4.2264166 -4.2323432 -4.2361741 -4.2343869 -4.2314525 -4.2373219 -4.2425461][-4.2224317 -4.2270494 -4.2660456 -4.2921853 -4.2860518 -4.2618484 -4.2311473 -4.2084332 -4.204649 -4.212584 -4.2201905 -4.2218785 -4.2185054 -4.2217822 -4.2244921][-4.1856837 -4.1889892 -4.2376184 -4.2722883 -4.2725906 -4.2540665 -4.2247915 -4.200779 -4.197648 -4.20646 -4.2167687 -4.2192712 -4.2119818 -4.2076926 -4.2047153][-4.1293316 -4.1310525 -4.1938372 -4.2365794 -4.239934 -4.2234287 -4.1979213 -4.1755919 -4.1701541 -4.1759725 -4.1854305 -4.1893544 -4.1807952 -4.1711969 -4.1650972][-4.0442061 -4.0390878 -4.1155338 -4.1728921 -4.1824441 -4.1717472 -4.15469 -4.1419482 -4.1350894 -4.1318717 -4.1375055 -4.140667 -4.1319294 -4.1223464 -4.1172113][-3.9535725 -3.9321938 -4.0116014 -4.0868144 -4.1106124 -4.1074648 -4.1001954 -4.1007638 -4.100883 -4.094697 -4.0899606 -4.0890365 -4.0796132 -4.0745592 -4.0778713][-3.9166458 -3.8776133 -3.945312 -4.0197392 -4.0451326 -4.042367 -4.0440359 -4.0606885 -4.0733585 -4.0734262 -4.0607104 -4.0524511 -4.046433 -4.0526257 -4.0671263][-3.9505732 -3.9083109 -3.9588661 -4.0128741 -4.022779 -4.0136461 -4.0192685 -4.0507126 -4.0805764 -4.0873771 -4.0742474 -4.0682616 -4.0668397 -4.0790567 -4.0999794][-4.0090518 -3.9690557 -4.0035763 -4.0380239 -4.0406108 -4.031208 -4.0425963 -4.086688 -4.1276064 -4.1418991 -4.1368809 -4.1364503 -4.1373935 -4.15021 -4.1683283][-4.0593972 -4.0161381 -4.0373058 -4.0651503 -4.0758162 -4.0740252 -4.0911746 -4.1389656 -4.1850338 -4.2063122 -4.2077136 -4.2072492 -4.204514 -4.2126846 -4.2242093][-4.1038947 -4.0526762 -4.0607185 -4.0807781 -4.1004562 -4.1173186 -4.1429725 -4.1887646 -4.2327218 -4.2597675 -4.2716866 -4.2747488 -4.2683926 -4.2677035 -4.2716069][-4.1220679 -4.059257 -4.0587053 -4.0720243 -4.1006413 -4.1345692 -4.1730518 -4.2223158 -4.2655773 -4.2952118 -4.315196 -4.3221955 -4.3173141 -4.3134708 -4.3139954][-4.1133013 -4.0390353 -4.0279021 -4.0360494 -4.0713377 -4.1136208 -4.1608553 -4.2161207 -4.2613878 -4.294765 -4.32144 -4.3350992 -4.3339076 -4.3305163 -4.330071][-4.1281009 -4.0499897 -4.0266719 -4.0286756 -4.0635743 -4.1038046 -4.1494484 -4.2026448 -4.247293 -4.2781882 -4.3064427 -4.3226156 -4.3238349 -4.3213277 -4.3205767]]...]
INFO - root - 2017-12-08 00:58:02.896571: step 74410, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 52h:16m:46s remains)
INFO - root - 2017-12-08 00:58:09.693369: step 74420, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:56m:35s remains)
INFO - root - 2017-12-08 00:58:16.421762: step 74430, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 47h:29m:36s remains)
INFO - root - 2017-12-08 00:58:23.276733: step 74440, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.682 sec/batch; 48h:54m:08s remains)
INFO - root - 2017-12-08 00:58:30.193572: step 74450, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 50h:48m:20s remains)
INFO - root - 2017-12-08 00:58:36.971438: step 74460, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:07m:48s remains)
INFO - root - 2017-12-08 00:58:43.690337: step 74470, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.674 sec/batch; 48h:17m:47s remains)
INFO - root - 2017-12-08 00:58:50.464550: step 74480, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.652 sec/batch; 46h:43m:52s remains)
INFO - root - 2017-12-08 00:58:57.318558: step 74490, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 47h:21m:02s remains)
INFO - root - 2017-12-08 00:59:04.232243: step 74500, loss = 2.11, batch loss = 2.05 (11.5 examples/sec; 0.696 sec/batch; 49h:53m:20s remains)
2017-12-08 00:59:04.926674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3238397 -4.3245497 -4.3242269 -4.3226271 -4.3194046 -4.3151484 -4.3098512 -4.304143 -4.3004684 -4.3004055 -4.3018446 -4.3043628 -4.3093519 -4.3132539 -4.31558][-4.3115373 -4.310822 -4.310122 -4.3060102 -4.2976894 -4.2874074 -4.2763429 -4.265543 -4.2608676 -4.26451 -4.2717576 -4.283709 -4.2984085 -4.3078294 -4.3122697][-4.2896924 -4.2865977 -4.2839551 -4.2758684 -4.2617712 -4.2448773 -4.2275558 -4.212358 -4.209444 -4.2194214 -4.2348657 -4.2593727 -4.2852082 -4.30009 -4.3052855][-4.26755 -4.2616348 -4.25567 -4.24293 -4.2233858 -4.1986427 -4.1706529 -4.1489534 -4.15025 -4.1706657 -4.1970868 -4.2340422 -4.2696595 -4.2883191 -4.2925677][-4.2481327 -4.2398586 -4.2287655 -4.2095237 -4.1829934 -4.1479588 -4.1042976 -4.0714688 -4.0791955 -4.1169162 -4.1610084 -4.211236 -4.255116 -4.276823 -4.2797461][-4.2347469 -4.2212262 -4.1987519 -4.1662955 -4.1272259 -4.078618 -4.0192456 -3.9764633 -3.9930525 -4.0562344 -4.1247678 -4.1905227 -4.243475 -4.2703676 -4.27283][-4.2272043 -4.2033997 -4.1632481 -4.1128292 -4.0609255 -4.004632 -3.9384179 -3.8903713 -3.9158106 -4.0033569 -4.0960722 -4.1760149 -4.2368455 -4.2690535 -4.2717328][-4.2331948 -4.1987948 -4.1446667 -4.0829887 -4.0281429 -3.9773295 -3.9220848 -3.8847642 -3.9175 -4.0113997 -4.107976 -4.18563 -4.244657 -4.2759447 -4.2777486][-4.25174 -4.2134604 -4.1557961 -4.0955606 -4.0496135 -4.0155659 -3.9853749 -3.9711771 -4.0035315 -4.0797758 -4.158525 -4.2184038 -4.2635522 -4.2871618 -4.28835][-4.2704968 -4.23417 -4.1811662 -4.1297455 -4.0968666 -4.0780215 -4.0668769 -4.0697694 -4.0992932 -4.1542287 -4.2121234 -4.2546778 -4.28479 -4.3006625 -4.3011312][-4.2853522 -4.2570167 -4.2146769 -4.1744757 -4.1512012 -4.1401167 -4.1368194 -4.1464891 -4.1708689 -4.2099876 -4.2521987 -4.2833233 -4.3032513 -4.3126926 -4.3106561][-4.2998939 -4.2821894 -4.2529058 -4.2232466 -4.2055054 -4.1969471 -4.1948476 -4.2037292 -4.22231 -4.2499762 -4.2809477 -4.3050103 -4.3182926 -4.3213205 -4.3140068][-4.3118811 -4.3044152 -4.2867846 -4.2654839 -4.24864 -4.2359705 -4.2267375 -4.2273183 -4.2391877 -4.2596159 -4.28456 -4.3066311 -4.3179512 -4.316875 -4.3050475][-4.3106718 -4.3102059 -4.3005548 -4.2843165 -4.2662969 -4.2478251 -4.2285657 -4.2192893 -4.2259259 -4.2431979 -4.2663116 -4.2886763 -4.30069 -4.2977748 -4.2837291][-4.2938519 -4.296443 -4.2911115 -4.2780771 -4.2606554 -4.2408781 -4.2175913 -4.2038951 -4.2082396 -4.224318 -4.2455883 -4.2658019 -4.27644 -4.2723408 -4.2584491]]...]
INFO - root - 2017-12-08 00:59:11.599270: step 74510, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.641 sec/batch; 45h:54m:09s remains)
INFO - root - 2017-12-08 00:59:18.413680: step 74520, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 47h:48m:22s remains)
INFO - root - 2017-12-08 00:59:25.268470: step 74530, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 49h:14m:00s remains)
INFO - root - 2017-12-08 00:59:32.136021: step 74540, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 52h:08m:40s remains)
INFO - root - 2017-12-08 00:59:38.889265: step 74550, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 47h:26m:51s remains)
INFO - root - 2017-12-08 00:59:45.705548: step 74560, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 47h:01m:59s remains)
INFO - root - 2017-12-08 00:59:52.526874: step 74570, loss = 2.08, batch loss = 2.03 (11.5 examples/sec; 0.696 sec/batch; 49h:53m:06s remains)
INFO - root - 2017-12-08 00:59:59.343299: step 74580, loss = 2.09, batch loss = 2.03 (11.3 examples/sec; 0.707 sec/batch; 50h:39m:35s remains)
INFO - root - 2017-12-08 01:00:06.159564: step 74590, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 48h:27m:59s remains)
INFO - root - 2017-12-08 01:00:13.007921: step 74600, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.650 sec/batch; 46h:34m:04s remains)
2017-12-08 01:00:13.774406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1579943 -4.1879287 -4.2144666 -4.2335629 -4.2467937 -4.2525516 -4.2581234 -4.2573113 -4.2524276 -4.2542858 -4.266674 -4.2766485 -4.2601843 -4.2299161 -4.2117395][-4.163486 -4.188499 -4.2160182 -4.2378588 -4.2518945 -4.2560453 -4.2584529 -4.2567453 -4.2553792 -4.2621126 -4.2763886 -4.2868471 -4.2718182 -4.2417827 -4.2208505][-4.1821103 -4.202147 -4.2337632 -4.2590156 -4.2710481 -4.2682953 -4.2581258 -4.2466135 -4.2455 -4.2584629 -4.2765789 -4.2895231 -4.2810612 -4.25785 -4.2389774][-4.2082915 -4.2252693 -4.2574549 -4.2804084 -4.2843595 -4.2681537 -4.235692 -4.2057838 -4.202961 -4.2267575 -4.2554631 -4.2770882 -4.2786374 -4.2648211 -4.2522945][-4.2265472 -4.2450609 -4.2729182 -4.2864132 -4.2733645 -4.2345147 -4.1701183 -4.1137342 -4.1115828 -4.1546588 -4.2046194 -4.2419672 -4.2571363 -4.2547779 -4.2487774][-4.2316589 -4.2523112 -4.2723742 -4.2699542 -4.2364907 -4.1712856 -4.0724473 -3.9875066 -3.9913282 -4.0663533 -4.1460152 -4.1985784 -4.2250547 -4.2325096 -4.2322526][-4.2366228 -4.2542319 -4.259841 -4.2378068 -4.1826162 -4.09356 -3.9652033 -3.8545527 -3.8722923 -3.993963 -4.1069446 -4.171813 -4.2024951 -4.2126379 -4.2142963][-4.2405429 -4.249764 -4.2413116 -4.2063851 -4.1398444 -4.0471249 -3.9216156 -3.8205218 -3.8535838 -3.9924884 -4.1121321 -4.1736693 -4.1974473 -4.2027063 -4.2030821][-4.2394319 -4.2435875 -4.231184 -4.1965189 -4.1400847 -4.072289 -3.9906511 -3.9293358 -3.9500055 -4.0503731 -4.144011 -4.1889243 -4.1982393 -4.1964612 -4.1983457][-4.2376995 -4.2443748 -4.2387934 -4.2146058 -4.1782222 -4.1413021 -4.097105 -4.0576878 -4.0583391 -4.1175227 -4.1799407 -4.2054939 -4.199614 -4.1928005 -4.1974406][-4.2337375 -4.248508 -4.254406 -4.2423949 -4.221354 -4.204206 -4.1825891 -4.153512 -4.1390524 -4.1657662 -4.2014046 -4.2125206 -4.1974945 -4.1879187 -4.1952138][-4.2217369 -4.2412338 -4.2557588 -4.2556958 -4.2461247 -4.238646 -4.2279668 -4.2048759 -4.1841345 -4.1922851 -4.2106652 -4.2140694 -4.1973343 -4.1857204 -4.1922812][-4.2114959 -4.2311749 -4.2503319 -4.2604957 -4.2600417 -4.2562938 -4.24992 -4.22896 -4.2043271 -4.2032971 -4.2168813 -4.2201529 -4.2061448 -4.19528 -4.2022176][-4.220973 -4.2344747 -4.2514448 -4.2652583 -4.2699227 -4.2679996 -4.2621641 -4.2430649 -4.218267 -4.2134662 -4.2262063 -4.2346153 -4.2292089 -4.2239518 -4.2309861][-4.2498913 -4.2530041 -4.2637177 -4.2765489 -4.2831674 -4.2827621 -4.2775569 -4.2598581 -4.2371106 -4.2316256 -4.2449026 -4.2576556 -4.259603 -4.2589922 -4.2654943]]...]
INFO - root - 2017-12-08 01:00:20.556156: step 74610, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 52h:45m:43s remains)
INFO - root - 2017-12-08 01:00:27.514148: step 74620, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:08m:04s remains)
INFO - root - 2017-12-08 01:00:34.258110: step 74630, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.632 sec/batch; 45h:15m:59s remains)
INFO - root - 2017-12-08 01:00:41.060020: step 74640, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.683 sec/batch; 48h:57m:04s remains)
INFO - root - 2017-12-08 01:00:47.912604: step 74650, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 52h:00m:47s remains)
INFO - root - 2017-12-08 01:00:54.830926: step 74660, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.692 sec/batch; 49h:34m:53s remains)
INFO - root - 2017-12-08 01:01:01.646335: step 74670, loss = 2.05, batch loss = 1.99 (12.7 examples/sec; 0.630 sec/batch; 45h:06m:12s remains)
INFO - root - 2017-12-08 01:01:08.252731: step 74680, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.737 sec/batch; 52h:48m:17s remains)
INFO - root - 2017-12-08 01:01:15.129399: step 74690, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 51h:10m:05s remains)
INFO - root - 2017-12-08 01:01:21.927655: step 74700, loss = 2.10, batch loss = 2.04 (11.9 examples/sec; 0.670 sec/batch; 48h:00m:23s remains)
2017-12-08 01:01:22.710412: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3279347 -4.3253317 -4.3240328 -4.3243847 -4.3261971 -4.3267832 -4.3249803 -4.3217983 -4.3214512 -4.3226891 -4.3233256 -4.3223939 -4.3171473 -4.3159275 -4.3189454][-4.3132267 -4.3074517 -4.305644 -4.3049521 -4.3063869 -4.3057451 -4.298378 -4.288373 -4.2848029 -4.2875471 -4.2905111 -4.2878971 -4.2796226 -4.2808123 -4.2888889][-4.28089 -4.2699928 -4.2656178 -4.2610559 -4.2595563 -4.2550039 -4.2399964 -4.2253723 -4.2218256 -4.2278452 -4.2313237 -4.2257872 -4.2197323 -4.2293768 -4.2481422][-4.230422 -4.215467 -4.2080274 -4.1997042 -4.1947575 -4.1849513 -4.1630416 -4.1479044 -4.155117 -4.1701422 -4.1727929 -4.1639862 -4.1612678 -4.1792564 -4.2076626][-4.1763129 -4.1582956 -4.1482506 -4.1349277 -4.1255703 -4.1082644 -4.07429 -4.0665574 -4.0971737 -4.1256132 -4.1279078 -4.1151876 -4.1137586 -4.1370606 -4.1734257][-4.124402 -4.1071658 -4.0932679 -4.0702925 -4.0460014 -4.000906 -3.9366779 -3.9378939 -4.0024495 -4.0536509 -4.0659924 -4.0598378 -4.0650706 -4.0972676 -4.1433029][-4.087522 -4.0751562 -4.0606151 -4.0274768 -3.9757521 -3.8847446 -3.7766581 -3.7857976 -3.8886786 -3.9679685 -4.003654 -4.0157723 -4.0362425 -4.0794439 -4.1325893][-4.0864177 -4.0839233 -4.0777259 -4.0497222 -3.9912403 -3.8914244 -3.7826324 -3.7934692 -3.8961737 -3.9802227 -4.0314703 -4.05829 -4.0821881 -4.1194267 -4.1615605][-4.1095767 -4.1171002 -4.1272173 -4.1199665 -4.0846291 -4.0206957 -3.9512067 -3.9514956 -4.0139589 -4.0738735 -4.1159453 -4.1379433 -4.1519694 -4.1745095 -4.1992788][-4.1108427 -4.1204638 -4.1382256 -4.152317 -4.1474442 -4.1242218 -4.0872784 -4.0828614 -4.1175127 -4.1558 -4.1778674 -4.184772 -4.1874204 -4.20039 -4.2178574][-4.0963955 -4.1004243 -4.1190224 -4.1407127 -4.154079 -4.1609044 -4.1492085 -4.1488848 -4.1692219 -4.194366 -4.2074142 -4.2055492 -4.2013955 -4.2098417 -4.223731][-4.1047416 -4.1055865 -4.1218753 -4.1427159 -4.1605515 -4.1792321 -4.1838222 -4.1848583 -4.1919394 -4.2029152 -4.2118058 -4.20927 -4.2059665 -4.2158842 -4.2295628][-4.1450143 -4.1496983 -4.157537 -4.1689057 -4.1838021 -4.2047439 -4.2155223 -4.214396 -4.2146964 -4.2186112 -4.2232971 -4.2230558 -4.2269354 -4.2383652 -4.2508507][-4.1891141 -4.1939669 -4.1969223 -4.2047105 -4.2177582 -4.236608 -4.2456536 -4.2413492 -4.2374935 -4.2364798 -4.2368369 -4.2388897 -4.2468243 -4.2566056 -4.2677665][-4.2216873 -4.224843 -4.2294579 -4.2353473 -4.2445211 -4.2559586 -4.2608294 -4.2558918 -4.2509537 -4.2478638 -4.2453976 -4.24905 -4.2585912 -4.2680874 -4.2788415]]...]
INFO - root - 2017-12-08 01:01:29.392449: step 74710, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.720 sec/batch; 51h:35m:30s remains)
INFO - root - 2017-12-08 01:01:36.245274: step 74720, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 47h:41m:51s remains)
INFO - root - 2017-12-08 01:01:43.083361: step 74730, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 46h:10m:02s remains)
INFO - root - 2017-12-08 01:01:49.810640: step 74740, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 46h:42m:37s remains)
INFO - root - 2017-12-08 01:01:56.675220: step 74750, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 50h:26m:35s remains)
INFO - root - 2017-12-08 01:02:03.552481: step 74760, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.688 sec/batch; 49h:13m:49s remains)
INFO - root - 2017-12-08 01:02:10.264718: step 74770, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 50h:02m:51s remains)
INFO - root - 2017-12-08 01:02:16.976774: step 74780, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:53m:00s remains)
INFO - root - 2017-12-08 01:02:23.733802: step 74790, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 47h:22m:23s remains)
INFO - root - 2017-12-08 01:02:30.562960: step 74800, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 47h:35m:23s remains)
2017-12-08 01:02:31.348837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2392473 -4.2266293 -4.2197294 -4.2115178 -4.2006683 -4.1921844 -4.1844258 -4.17913 -4.186614 -4.2002645 -4.2114439 -4.2135034 -4.2156229 -4.2236872 -4.2376733][-4.2023683 -4.1892605 -4.1870351 -4.1838636 -4.1698608 -4.1547704 -4.1408081 -4.1368947 -4.1473289 -4.1652956 -4.1796565 -4.1800513 -4.1747937 -4.1787157 -4.1910996][-4.1746902 -4.1561093 -4.1564932 -4.1555457 -4.1389904 -4.1174827 -4.1015806 -4.1044197 -4.1218524 -4.145967 -4.1626072 -4.1621618 -4.1485224 -4.1437306 -4.1511421][-4.1737719 -4.1444736 -4.1440516 -4.1443377 -4.1316657 -4.1014209 -4.0752563 -4.0811396 -4.1151376 -4.1555734 -4.1792517 -4.1790223 -4.1574636 -4.1451893 -4.1480227][-4.1891842 -4.1542873 -4.1528058 -4.1554031 -4.142127 -4.0960817 -4.0441413 -4.044539 -4.1025453 -4.1621313 -4.1975312 -4.201004 -4.1750236 -4.1595006 -4.1622057][-4.2022438 -4.165298 -4.1588526 -4.1571894 -4.1353717 -4.0615673 -3.959681 -3.946701 -4.0410976 -4.13325 -4.1817636 -4.1898408 -4.1629844 -4.15134 -4.1608782][-4.2011795 -4.1684046 -4.1579251 -4.1442266 -4.10438 -3.9918067 -3.8255084 -3.8005934 -3.9531028 -4.0874286 -4.1479511 -4.1525106 -4.1215448 -4.1088066 -4.1283665][-4.1856928 -4.1642838 -4.1557584 -4.1319547 -4.0786157 -3.9585147 -3.7693825 -3.7317977 -3.9131529 -4.0676785 -4.1279845 -4.119544 -4.0798345 -4.0575752 -4.0789495][-4.195178 -4.1893167 -4.1936007 -4.1767759 -4.13544 -4.0459805 -3.9114132 -3.8736372 -3.9899726 -4.1035995 -4.1398911 -4.1119504 -4.0559492 -4.0216141 -4.0400782][-4.2220335 -4.2270489 -4.2431049 -4.2356114 -4.211308 -4.152154 -4.0649633 -4.0336938 -4.0926852 -4.1562781 -4.1674948 -4.1316061 -4.0659733 -4.0267472 -4.0466847][-4.2528839 -4.2611394 -4.2826433 -4.2899046 -4.2763081 -4.2357564 -4.1752496 -4.1471758 -4.1708035 -4.2023163 -4.2046795 -4.1770496 -4.1206584 -4.0821295 -4.1009722][-4.2898388 -4.2870321 -4.3054318 -4.3232937 -4.3213439 -4.2952104 -4.2492266 -4.2233319 -4.2286353 -4.2481456 -4.2524848 -4.2315192 -4.1880193 -4.1550765 -4.1679239][-4.312397 -4.3014889 -4.3121486 -4.3307023 -4.3362727 -4.3214092 -4.2917628 -4.2756486 -4.2790432 -4.2972188 -4.3022542 -4.2839007 -4.2499919 -4.2224979 -4.2245169][-4.3191729 -4.3122673 -4.3184357 -4.3300271 -4.3356967 -4.3276739 -4.3115768 -4.3025327 -4.3088245 -4.3261981 -4.3308845 -4.3162594 -4.28974 -4.268127 -4.2671614][-4.3156128 -4.3136158 -4.31572 -4.3200788 -4.322772 -4.3191977 -4.3133507 -4.3107553 -4.3171992 -4.3288088 -4.3332438 -4.3244405 -4.3068132 -4.2936559 -4.2939925]]...]
INFO - root - 2017-12-08 01:02:37.979733: step 74810, loss = 2.11, batch loss = 2.05 (12.1 examples/sec; 0.663 sec/batch; 47h:28m:44s remains)
INFO - root - 2017-12-08 01:02:44.750084: step 74820, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.631 sec/batch; 45h:10m:17s remains)
INFO - root - 2017-12-08 01:02:51.567576: step 74830, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.663 sec/batch; 47h:27m:39s remains)
INFO - root - 2017-12-08 01:02:58.422193: step 74840, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.736 sec/batch; 52h:40m:13s remains)
INFO - root - 2017-12-08 01:03:05.239958: step 74850, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.729 sec/batch; 52h:10m:29s remains)
INFO - root - 2017-12-08 01:03:12.085613: step 74860, loss = 2.04, batch loss = 1.98 (12.0 examples/sec; 0.665 sec/batch; 47h:35m:26s remains)
INFO - root - 2017-12-08 01:03:18.843708: step 74870, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.651 sec/batch; 46h:37m:12s remains)
INFO - root - 2017-12-08 01:03:25.686430: step 74880, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 49h:59m:34s remains)
INFO - root - 2017-12-08 01:03:32.422111: step 74890, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 51h:30m:45s remains)
INFO - root - 2017-12-08 01:03:39.290376: step 74900, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.691 sec/batch; 49h:24m:37s remains)
2017-12-08 01:03:40.026768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1700125 -4.1837273 -4.1952996 -4.1965051 -4.1965122 -4.1929951 -4.1804929 -4.1661806 -4.1548066 -4.1523833 -4.1590462 -4.1778283 -4.2085638 -4.2335353 -4.2505322][-4.1711245 -4.1794839 -4.18916 -4.1894913 -4.1859813 -4.1857033 -4.1820436 -4.176394 -4.1754541 -4.1796708 -4.1874666 -4.2021885 -4.2237196 -4.2435155 -4.2588353][-4.180459 -4.1682868 -4.1581912 -4.152565 -4.1549706 -4.1665716 -4.1762238 -4.1788731 -4.1830244 -4.1862926 -4.1900768 -4.2006288 -4.2153234 -4.2345314 -4.2534695][-4.1875644 -4.1565394 -4.1206789 -4.1006217 -4.1048613 -4.1259623 -4.1526418 -4.1675644 -4.1751361 -4.1815615 -4.1854792 -4.1897235 -4.1964512 -4.2140918 -4.2382994][-4.1797743 -4.1408343 -4.0930762 -4.0638943 -4.0599589 -4.0758982 -4.1098127 -4.1393661 -4.1548824 -4.171103 -4.1793003 -4.1800046 -4.1833067 -4.1989727 -4.2240486][-4.15516 -4.1217866 -4.0884476 -4.0669212 -4.0439906 -4.0232544 -4.0393982 -4.0831785 -4.1310706 -4.1764417 -4.1985617 -4.1994267 -4.2001667 -4.2114649 -4.2316542][-4.1265349 -4.0842953 -4.0710449 -4.0741315 -4.0416751 -3.9678695 -3.9348681 -3.9908087 -4.0905571 -4.1771164 -4.2177668 -4.2172861 -4.2132168 -4.2233462 -4.2426982][-4.0986338 -4.0440168 -4.0420079 -4.071753 -4.0390892 -3.9184096 -3.8173742 -3.8698273 -4.0150142 -4.1418672 -4.2097979 -4.2178564 -4.2069988 -4.2137508 -4.2381406][-4.0976024 -4.0455189 -4.0504479 -4.0869656 -4.0625849 -3.953809 -3.8421052 -3.8610163 -3.9798188 -4.0983052 -4.1730924 -4.1970658 -4.1916318 -4.1913195 -4.2162566][-4.1279283 -4.0895276 -4.0940533 -4.1239438 -4.1172347 -4.0580225 -3.9968383 -3.9977551 -4.0440383 -4.1097393 -4.1642814 -4.1887245 -4.1903458 -4.1909919 -4.207088][-4.1566253 -4.1389956 -4.1400275 -4.1538868 -4.1511664 -4.1287518 -4.1105294 -4.1118541 -4.1175556 -4.1396623 -4.1708889 -4.1940403 -4.2027144 -4.2087321 -4.219244][-4.1681876 -4.1664963 -4.1624551 -4.1673584 -4.1665378 -4.1607819 -4.1670265 -4.1755667 -4.1681561 -4.1599555 -4.1668606 -4.1841879 -4.2026672 -4.2207022 -4.2353907][-4.1923923 -4.1965995 -4.1930118 -4.1922607 -4.1895857 -4.1923318 -4.20705 -4.2193108 -4.2103443 -4.1931496 -4.1883383 -4.1989636 -4.219841 -4.2403884 -4.2561574][-4.2331848 -4.2389665 -4.2377439 -4.2330108 -4.2268381 -4.22642 -4.239593 -4.251842 -4.2478995 -4.2371187 -4.2308788 -4.2345257 -4.2510152 -4.268888 -4.2825103][-4.2677832 -4.2713242 -4.2730446 -4.2710285 -4.2662654 -4.2636766 -4.2704735 -4.2782989 -4.2793856 -4.2765951 -4.2734709 -4.2737217 -4.28375 -4.2955346 -4.3049097]]...]
INFO - root - 2017-12-08 01:03:46.629773: step 74910, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.711 sec/batch; 50h:54m:21s remains)
INFO - root - 2017-12-08 01:03:53.447214: step 74920, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 50h:12m:39s remains)
INFO - root - 2017-12-08 01:04:00.272314: step 74930, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 46h:25m:37s remains)
INFO - root - 2017-12-08 01:04:06.948147: step 74940, loss = 2.06, batch loss = 2.01 (12.6 examples/sec; 0.637 sec/batch; 45h:35m:19s remains)
INFO - root - 2017-12-08 01:04:13.774690: step 74950, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 47h:32m:13s remains)
INFO - root - 2017-12-08 01:04:20.618033: step 74960, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.723 sec/batch; 51h:41m:55s remains)
INFO - root - 2017-12-08 01:04:27.488146: step 74970, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 48h:03m:36s remains)
INFO - root - 2017-12-08 01:04:34.433853: step 74980, loss = 2.07, batch loss = 2.02 (10.9 examples/sec; 0.731 sec/batch; 52h:17m:27s remains)
INFO - root - 2017-12-08 01:04:41.021930: step 74990, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.664 sec/batch; 47h:28m:12s remains)
INFO - root - 2017-12-08 01:04:47.865863: step 75000, loss = 2.09, batch loss = 2.04 (11.3 examples/sec; 0.708 sec/batch; 50h:39m:18s remains)
2017-12-08 01:04:48.611135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3150725 -4.3039908 -4.294837 -4.2879753 -4.2829142 -4.2803469 -4.280551 -4.2811 -4.2833185 -4.2862563 -4.2913871 -4.2958765 -4.2972035 -4.300725 -4.2992883][-4.2968941 -4.2855988 -4.2787886 -4.2734418 -4.2717786 -4.2718487 -4.2682581 -4.2639947 -4.2640491 -4.2638392 -4.2707939 -4.2815366 -4.2886004 -4.2976761 -4.2946849][-4.2694616 -4.2584019 -4.2525454 -4.2495 -4.2514572 -4.2549653 -4.2478256 -4.240274 -4.2449613 -4.2464437 -4.2564073 -4.2710919 -4.2810822 -4.2922692 -4.2886329][-4.2369452 -4.2233763 -4.2191339 -4.2182708 -4.2208571 -4.2238178 -4.2138286 -4.1979275 -4.2072916 -4.2180276 -4.2356796 -4.2548146 -4.2652664 -4.2768126 -4.2731404][-4.2182651 -4.1993032 -4.1920638 -4.1889782 -4.184998 -4.1827331 -4.1653347 -4.1336122 -4.1411748 -4.1655164 -4.1926131 -4.2214432 -4.2411304 -4.2560635 -4.25688][-4.2049212 -4.1771593 -4.1602292 -4.1495352 -4.1371622 -4.1272874 -4.0968332 -4.0377083 -4.0320463 -4.0726848 -4.1173859 -4.1617532 -4.1984277 -4.2236743 -4.2364092][-4.1956263 -4.1622434 -4.1386747 -4.1206164 -4.1008267 -4.082242 -4.0395885 -3.9572628 -3.9386339 -3.9955304 -4.056448 -4.1115632 -4.1593242 -4.1917868 -4.2124076][-4.2011642 -4.1740975 -4.1547866 -4.137259 -4.1189222 -4.1005793 -4.0625768 -3.9959178 -3.9870555 -4.0266776 -4.0684934 -4.1103516 -4.1511889 -4.1755347 -4.1886382][-4.2138767 -4.1980348 -4.1861396 -4.1714411 -4.1553941 -4.1410594 -4.1134825 -4.0736346 -4.077642 -4.0962048 -4.1091237 -4.1293068 -4.1588697 -4.1763968 -4.1764216][-4.2317891 -4.2249894 -4.2159572 -4.2022777 -4.1862731 -4.1731229 -4.1528206 -4.1312623 -4.1435919 -4.1498246 -4.1400414 -4.1406331 -4.1615257 -4.1769857 -4.1733356][-4.2536731 -4.2482471 -4.2376657 -4.2208166 -4.201221 -4.1858335 -4.1714468 -4.1612754 -4.1786327 -4.1797357 -4.1596055 -4.14959 -4.1639295 -4.1792569 -4.176034][-4.2718754 -4.2656431 -4.25436 -4.2374539 -4.2174048 -4.2012253 -4.1903148 -4.183939 -4.2036576 -4.2068634 -4.1903753 -4.1796842 -4.1887875 -4.2027283 -4.2027073][-4.2836475 -4.2776833 -4.270515 -4.25899 -4.2448816 -4.2316213 -4.2254453 -4.2242103 -4.2422242 -4.2473159 -4.2380743 -4.2309613 -4.2348018 -4.2450147 -4.2482667][-4.293045 -4.2869568 -4.2838554 -4.2806511 -4.2754316 -4.2652411 -4.2614555 -4.2639017 -4.2785888 -4.2829733 -4.2763953 -4.2720647 -4.2719383 -4.278688 -4.2852659][-4.3088365 -4.3034143 -4.3034225 -4.3050389 -4.30423 -4.2973642 -4.2947683 -4.29747 -4.30581 -4.3067384 -4.3022575 -4.2991834 -4.297358 -4.300745 -4.30747]]...]
INFO - root - 2017-12-08 01:04:55.230375: step 75010, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.653 sec/batch; 46h:43m:43s remains)
INFO - root - 2017-12-08 01:05:02.042709: step 75020, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.636 sec/batch; 45h:29m:08s remains)
INFO - root - 2017-12-08 01:05:08.946946: step 75030, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.734 sec/batch; 52h:30m:39s remains)
INFO - root - 2017-12-08 01:05:15.829768: step 75040, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 51h:54m:46s remains)
INFO - root - 2017-12-08 01:05:22.623462: step 75050, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.658 sec/batch; 47h:05m:21s remains)
INFO - root - 2017-12-08 01:05:29.390018: step 75060, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.638 sec/batch; 45h:39m:28s remains)
INFO - root - 2017-12-08 01:05:36.188313: step 75070, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.697 sec/batch; 49h:51m:01s remains)
INFO - root - 2017-12-08 01:05:42.913877: step 75080, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.719 sec/batch; 51h:22m:38s remains)
INFO - root - 2017-12-08 01:05:49.695929: step 75090, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.665 sec/batch; 47h:34m:35s remains)
INFO - root - 2017-12-08 01:05:56.424255: step 75100, loss = 2.05, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 45h:36m:38s remains)
2017-12-08 01:05:57.261887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28956 -4.320622 -4.345469 -4.3584709 -4.3639584 -4.3629246 -4.3550396 -4.3487186 -4.3419271 -4.327301 -4.2999258 -4.2693481 -4.2511668 -4.2531061 -4.2663674][-4.34819 -4.3648791 -4.3774457 -4.3793764 -4.3765216 -4.3687625 -4.3560448 -4.3457723 -4.3365469 -4.3182983 -4.290462 -4.2633171 -4.2504797 -4.2536449 -4.2671404][-4.3721709 -4.3766079 -4.3813019 -4.3798738 -4.3751826 -4.3644304 -4.3477936 -4.3334036 -4.3203168 -4.2993016 -4.2722325 -4.2510633 -4.2448144 -4.2518435 -4.26787][-4.3726611 -4.3690166 -4.3693767 -4.3671212 -4.3598151 -4.3447537 -4.3239107 -4.304317 -4.2881227 -4.2684793 -4.2461705 -4.2342772 -4.2353234 -4.2482972 -4.2693892][-4.360003 -4.3499031 -4.3458929 -4.3402338 -4.3277144 -4.30637 -4.2787352 -4.2506542 -4.2305474 -4.21526 -4.2041192 -4.2049489 -4.2164922 -4.2394876 -4.2683167][-4.3354716 -4.3192687 -4.3104157 -4.3015571 -4.2857842 -4.2590561 -4.2207413 -4.1834397 -4.1600804 -4.149416 -4.1497183 -4.1626921 -4.1875753 -4.2231569 -4.2604222][-4.3023663 -4.2807317 -4.26776 -4.25517 -4.23264 -4.1966329 -4.1467948 -4.1031566 -4.0833168 -4.0836291 -4.0999756 -4.129971 -4.1700387 -4.2135191 -4.2526817][-4.2686315 -4.243979 -4.2271385 -4.2068973 -4.1719913 -4.1232486 -4.06359 -4.0215626 -4.0111613 -4.0277176 -4.0641832 -4.1132717 -4.1608033 -4.2027106 -4.2358613][-4.2497163 -4.2223077 -4.1990819 -4.1703739 -4.1265612 -4.071516 -4.0142484 -3.9815979 -3.9835093 -4.0129223 -4.0583239 -4.1046195 -4.141448 -4.1719556 -4.1959295][-4.2548637 -4.2301884 -4.2067904 -4.1777515 -4.1374106 -4.0876122 -4.0400143 -4.0160937 -4.0193968 -4.0430393 -4.07643 -4.106288 -4.1302981 -4.1491642 -4.1648312][-4.2820225 -4.2662535 -4.2505183 -4.2307353 -4.20358 -4.1670475 -4.1301413 -4.1068335 -4.0996213 -4.105032 -4.1202374 -4.1379213 -4.1564932 -4.1717191 -4.1833291][-4.3198214 -4.3123312 -4.3044429 -4.29341 -4.277245 -4.254127 -4.2281885 -4.2071161 -4.1924124 -4.18512 -4.1875381 -4.1993203 -4.216876 -4.2324753 -4.243197][-4.3494248 -4.3452244 -4.3400512 -4.3327885 -4.3231277 -4.3117075 -4.2993293 -4.2891726 -4.28019 -4.2741184 -4.2733765 -4.2789917 -4.2885423 -4.297472 -4.3021812][-4.3686256 -4.3680258 -4.3664308 -4.3626962 -4.3578229 -4.3521142 -4.3465519 -4.3407149 -4.3348103 -4.3312831 -4.3299661 -4.3313375 -4.3339968 -4.3356953 -4.3349805][-4.3800821 -4.3812685 -4.3819833 -4.3814831 -4.3807859 -4.3787661 -4.3751 -4.3690281 -4.3620491 -4.3568892 -4.3543034 -4.3545513 -4.3555207 -4.3553486 -4.3534827]]...]
INFO - root - 2017-12-08 01:06:03.868379: step 75110, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.698 sec/batch; 49h:54m:11s remains)
INFO - root - 2017-12-08 01:06:10.747937: step 75120, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 49h:18m:25s remains)
INFO - root - 2017-12-08 01:06:17.432978: step 75130, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 45h:26m:31s remains)
INFO - root - 2017-12-08 01:06:24.217090: step 75140, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 46h:26m:04s remains)
INFO - root - 2017-12-08 01:06:30.990416: step 75150, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 50h:41m:41s remains)
INFO - root - 2017-12-08 01:06:37.779127: step 75160, loss = 2.10, batch loss = 2.04 (11.4 examples/sec; 0.699 sec/batch; 49h:58m:14s remains)
INFO - root - 2017-12-08 01:06:44.527460: step 75170, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.651 sec/batch; 46h:31m:50s remains)
INFO - root - 2017-12-08 01:06:51.260387: step 75180, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 46h:10m:38s remains)
INFO - root - 2017-12-08 01:06:58.010910: step 75190, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 48h:20m:47s remains)
INFO - root - 2017-12-08 01:07:04.763241: step 75200, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.681 sec/batch; 48h:41m:54s remains)
2017-12-08 01:07:05.452795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1619549 -4.1645613 -4.1757588 -4.1761775 -4.1639748 -4.1615 -4.1741881 -4.1843176 -4.1842003 -4.1929665 -4.2173929 -4.2440085 -4.2520242 -4.2546759 -4.2586117][-4.1746182 -4.1744256 -4.1836219 -4.1893277 -4.1824431 -4.1835117 -4.1962571 -4.2044225 -4.205296 -4.2133169 -4.2340226 -4.2488632 -4.2395105 -4.2338305 -4.2402916][-4.1884484 -4.184917 -4.1882539 -4.1943483 -4.1883464 -4.1812396 -4.1806374 -4.17984 -4.1783352 -4.1846204 -4.2042732 -4.2162471 -4.2058649 -4.208549 -4.2313704][-4.1918397 -4.1786933 -4.1679559 -4.1649556 -4.1530771 -4.1331687 -4.1166482 -4.1102214 -4.1134744 -4.1255713 -4.152215 -4.1708932 -4.1734266 -4.1927543 -4.2337112][-4.1847529 -4.1576133 -4.1291361 -4.115665 -4.098536 -4.0676503 -4.0364027 -4.0304084 -4.0488992 -4.07162 -4.1013126 -4.1272678 -4.1427679 -4.1745667 -4.2251968][-4.178762 -4.1335092 -4.0826564 -4.0549865 -4.0283155 -3.9819586 -3.9326859 -3.9305248 -3.9768941 -4.0158839 -4.0449758 -4.0809107 -4.113698 -4.1567926 -4.2107725][-4.17272 -4.1115074 -4.0404415 -3.9965372 -3.9635429 -3.9040866 -3.8335996 -3.8353159 -3.9125907 -3.9714479 -4.00561 -4.0501003 -4.0976644 -4.1486516 -4.1988316][-4.1720572 -4.1032114 -4.0269346 -3.9814856 -3.9588432 -3.9144082 -3.8549738 -3.854562 -3.9305925 -3.9919393 -4.0247588 -4.0602608 -4.1055303 -4.1545753 -4.1922832][-4.1798763 -4.1154861 -4.0496392 -4.0126524 -4.0066385 -3.9877319 -3.9565535 -3.9611347 -4.01515 -4.0625086 -4.0889816 -4.1121793 -4.1423273 -4.175364 -4.1952534][-4.1921344 -4.1414723 -4.0940857 -4.0647535 -4.0644784 -4.0629234 -4.0578618 -4.0758252 -4.1171656 -4.1488976 -4.1653948 -4.1762619 -4.1908126 -4.2051172 -4.2049031][-4.2126532 -4.1816339 -4.1524534 -4.128726 -4.1268468 -4.1323342 -4.1408634 -4.1628647 -4.1928229 -4.211338 -4.217608 -4.2184138 -4.2248492 -4.2243881 -4.2059669][-4.2379193 -4.2215939 -4.2022657 -4.1826358 -4.1798477 -4.1855597 -4.197227 -4.2152338 -4.2328029 -4.2403636 -4.2369442 -4.2303009 -4.2294569 -4.2226996 -4.1977782][-4.2617702 -4.2544 -4.2398548 -4.2222557 -4.2170734 -4.2213516 -4.2307096 -4.2420011 -4.2478752 -4.2439694 -4.2305851 -4.2194185 -4.2164736 -4.2100739 -4.189806][-4.2819943 -4.279809 -4.2678881 -4.2512612 -4.2441888 -4.2445993 -4.2481132 -4.2501822 -4.2442794 -4.2297 -4.2127223 -4.20459 -4.2056489 -4.2027555 -4.1902618][-4.2917371 -4.2903986 -4.2786016 -4.2648439 -4.2580128 -4.2559013 -4.2540765 -4.2489176 -4.2360873 -4.2173734 -4.2040062 -4.2040372 -4.2098236 -4.2086205 -4.1973524]]...]
INFO - root - 2017-12-08 01:07:12.084462: step 75210, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.627 sec/batch; 44h:49m:32s remains)
INFO - root - 2017-12-08 01:07:18.956149: step 75220, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 48h:24m:48s remains)
INFO - root - 2017-12-08 01:07:25.820457: step 75230, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 50h:41m:21s remains)
INFO - root - 2017-12-08 01:07:32.687530: step 75240, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.720 sec/batch; 51h:28m:51s remains)
INFO - root - 2017-12-08 01:07:39.556494: step 75250, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.705 sec/batch; 50h:20m:40s remains)
INFO - root - 2017-12-08 01:07:46.354562: step 75260, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.631 sec/batch; 45h:06m:35s remains)
INFO - root - 2017-12-08 01:07:53.141913: step 75270, loss = 2.09, batch loss = 2.03 (11.9 examples/sec; 0.675 sec/batch; 48h:13m:34s remains)
INFO - root - 2017-12-08 01:08:00.011358: step 75280, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.704 sec/batch; 50h:19m:26s remains)
INFO - root - 2017-12-08 01:08:06.797096: step 75290, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.646 sec/batch; 46h:10m:12s remains)
INFO - root - 2017-12-08 01:08:13.344860: step 75300, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 48h:02m:37s remains)
2017-12-08 01:08:14.104977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2880383 -4.2847767 -4.28222 -4.280477 -4.2780952 -4.2759557 -4.2742805 -4.27232 -4.2716036 -4.2716193 -4.2709241 -4.2682719 -4.256391 -4.2302179 -4.1897569][-4.2841687 -4.2760253 -4.269712 -4.2658219 -4.2627068 -4.2605891 -4.259768 -4.2577095 -4.2563486 -4.2552361 -4.2523713 -4.2474108 -4.2313371 -4.198698 -4.1501226][-4.2839456 -4.2734251 -4.2627115 -4.2542491 -4.2484026 -4.2462945 -4.2457857 -4.2427382 -4.240212 -4.23946 -4.23753 -4.2335377 -4.2164884 -4.1817427 -4.1307545][-4.2719383 -4.2618775 -4.2478786 -4.2358184 -4.2288027 -4.2261143 -4.224061 -4.2187705 -4.2158303 -4.2204027 -4.2265716 -4.2293262 -4.2183065 -4.1898174 -4.1452823][-4.2321181 -4.2260313 -4.210772 -4.1931858 -4.181335 -4.1766624 -4.1709213 -4.1610208 -4.159431 -4.1729922 -4.1909242 -4.2040558 -4.204144 -4.1906362 -4.1633377][-4.1781292 -4.177887 -4.165781 -4.1448255 -4.1294618 -4.1172967 -4.0964479 -4.0705767 -4.070529 -4.098557 -4.1297503 -4.1519704 -4.1665831 -4.1734433 -4.1683345][-4.1309175 -4.1377916 -4.1318874 -4.1112218 -4.0911431 -4.0651364 -4.0146761 -3.9618416 -3.9672348 -4.019875 -4.069705 -4.1001544 -4.1257019 -4.1475921 -4.1594272][-4.0715027 -4.0948977 -4.1071639 -4.0966253 -4.0734549 -4.0254521 -3.9386542 -3.8600719 -3.880336 -3.9623756 -4.0294247 -4.0661073 -4.09752 -4.1258788 -4.1446233][-4.0046911 -4.0493073 -4.0882416 -4.0966139 -4.0831952 -4.0394664 -3.9606993 -3.8955398 -3.9188406 -3.9902182 -4.0417662 -4.0696478 -4.09947 -4.1272378 -4.1427369][-4.0005279 -4.0469685 -4.0963683 -4.1221166 -4.1284947 -4.1096811 -4.0627375 -4.0215039 -4.0320778 -4.0707951 -4.0974121 -4.1145105 -4.1386371 -4.157774 -4.1615462][-4.0561972 -4.0869269 -4.1271448 -4.1581225 -4.1761565 -4.1742506 -4.1510062 -4.13057 -4.1345153 -4.148366 -4.1570053 -4.1655569 -4.1816635 -4.1893311 -4.1803741][-4.1260152 -4.1403995 -4.1665273 -4.1935081 -4.2148991 -4.223269 -4.2158937 -4.2089233 -4.2109246 -4.2140632 -4.2157607 -4.21721 -4.2216225 -4.214551 -4.1952043][-4.1833315 -4.1919079 -4.2077861 -4.2248225 -4.2424064 -4.2560563 -4.2592807 -4.2599573 -4.2610464 -4.2608886 -4.2587013 -4.2541194 -4.2473421 -4.2273607 -4.2014532][-4.230454 -4.2351718 -4.2428336 -4.2498941 -4.2594676 -4.2694306 -4.2739511 -4.2754784 -4.2768559 -4.277288 -4.2746 -4.2698054 -4.2598329 -4.2354908 -4.2084527][-4.2663417 -4.26819 -4.268661 -4.2680783 -4.2704639 -4.2743959 -4.275136 -4.2734618 -4.2747893 -4.2773619 -4.2766943 -4.2739496 -4.2649584 -4.242208 -4.2185555]]...]
INFO - root - 2017-12-08 01:08:20.788257: step 75310, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.664 sec/batch; 47h:26m:00s remains)
INFO - root - 2017-12-08 01:08:27.585013: step 75320, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 49h:02m:10s remains)
INFO - root - 2017-12-08 01:08:34.305933: step 75330, loss = 2.05, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 45h:57m:44s remains)
INFO - root - 2017-12-08 01:08:41.100767: step 75340, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.680 sec/batch; 48h:32m:26s remains)
INFO - root - 2017-12-08 01:08:47.856524: step 75350, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.693 sec/batch; 49h:28m:33s remains)
INFO - root - 2017-12-08 01:08:54.645041: step 75360, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.703 sec/batch; 50h:12m:12s remains)
INFO - root - 2017-12-08 01:09:01.309005: step 75370, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.677 sec/batch; 48h:22m:05s remains)
INFO - root - 2017-12-08 01:09:08.057110: step 75380, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.641 sec/batch; 45h:46m:26s remains)
INFO - root - 2017-12-08 01:09:14.924832: step 75390, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.663 sec/batch; 47h:22m:33s remains)
INFO - root - 2017-12-08 01:09:21.778530: step 75400, loss = 2.07, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 51h:55m:09s remains)
2017-12-08 01:09:22.467855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3201423 -4.3173308 -4.3193521 -4.3227496 -4.3268867 -4.3302064 -4.3283544 -4.3219872 -4.3148289 -4.3114347 -4.3139749 -4.3231006 -4.3346353 -4.3455105 -4.3537][-4.2883329 -4.2820573 -4.2857361 -4.2891164 -4.2914214 -4.2942109 -4.2916183 -4.2811933 -4.2702541 -4.267715 -4.275631 -4.2919412 -4.3098831 -4.3259039 -4.3390541][-4.2375031 -4.2290745 -4.237052 -4.242774 -4.240797 -4.2379241 -4.2289591 -4.2119575 -4.1976671 -4.1990161 -4.2176085 -4.2475452 -4.2753248 -4.297544 -4.3163414][-4.1707449 -4.1659589 -4.1830316 -4.1920509 -4.1824584 -4.1635957 -4.1374855 -4.1100826 -4.0933614 -4.1025434 -4.1404066 -4.1904941 -4.2324367 -4.2630219 -4.2882953][-4.0992661 -4.1036658 -4.1337428 -4.1477551 -4.1260419 -4.08419 -4.03345 -3.9937315 -3.9799132 -4.0047536 -4.068686 -4.1438742 -4.1989384 -4.2344112 -4.2621169][-4.0336514 -4.0457926 -4.0897145 -4.1097016 -4.0826697 -4.0286989 -3.9599347 -3.9098024 -3.8975446 -3.9326539 -4.0163693 -4.1112041 -4.1755342 -4.2121058 -4.2378855][-3.9971991 -4.0098443 -4.0576739 -4.0832434 -4.0651331 -4.0193586 -3.9558916 -3.9075341 -3.8918405 -3.9217436 -4.0026088 -4.0977907 -4.1668591 -4.2045407 -4.2293611][-4.0098572 -4.0144424 -4.0503812 -4.0762725 -4.0774674 -4.053709 -4.0114765 -3.9745073 -3.9537315 -3.9658523 -4.0309286 -4.1140366 -4.1784558 -4.2140121 -4.2371993][-4.0646124 -4.0575457 -4.076036 -4.0998106 -4.1174932 -4.1137085 -4.0918465 -4.0687556 -4.0478582 -4.0447512 -4.0922689 -4.1605692 -4.2142773 -4.2423573 -4.2595291][-4.1325703 -4.1215549 -4.1306376 -4.1534328 -4.178288 -4.1851764 -4.1757517 -4.1643181 -4.14856 -4.139411 -4.1755977 -4.2291579 -4.26661 -4.2848806 -4.2927179][-4.1935377 -4.1867342 -4.1956053 -4.2161317 -4.2378683 -4.2471027 -4.243948 -4.2382154 -4.230257 -4.2284102 -4.2584224 -4.2962303 -4.317677 -4.3267283 -4.3266382][-4.2437506 -4.2430334 -4.2535033 -4.270225 -4.286283 -4.2950068 -4.2917676 -4.2847486 -4.2813306 -4.2888255 -4.3146 -4.3411674 -4.3530936 -4.3568258 -4.3517079][-4.2840519 -4.2865248 -4.2960215 -4.3075933 -4.3179083 -4.3229861 -4.3180876 -4.3100338 -4.3090348 -4.3196373 -4.3406544 -4.3602905 -4.3682814 -4.3695574 -4.3630447][-4.3136678 -4.3160496 -4.3220487 -4.3277316 -4.3317256 -4.3326311 -4.3273354 -4.322916 -4.3262587 -4.3369279 -4.3521271 -4.3660474 -4.3717523 -4.3722076 -4.3662148][-4.3335218 -4.3349428 -4.3379726 -4.339355 -4.3390813 -4.3378448 -4.3341274 -4.3323455 -4.3366566 -4.3457608 -4.3562045 -4.3652062 -4.36902 -4.369256 -4.3651757]]...]
INFO - root - 2017-12-08 01:09:29.064637: step 75410, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 47h:02m:34s remains)
INFO - root - 2017-12-08 01:09:35.870421: step 75420, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.664 sec/batch; 47h:25m:50s remains)
INFO - root - 2017-12-08 01:09:42.783645: step 75430, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.685 sec/batch; 48h:57m:00s remains)
INFO - root - 2017-12-08 01:09:49.573205: step 75440, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.745 sec/batch; 53h:11m:33s remains)
INFO - root - 2017-12-08 01:09:56.370902: step 75450, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.692 sec/batch; 49h:25m:35s remains)
INFO - root - 2017-12-08 01:10:03.128769: step 75460, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 45h:50m:14s remains)
INFO - root - 2017-12-08 01:10:10.085164: step 75470, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 47h:55m:51s remains)
INFO - root - 2017-12-08 01:10:16.916252: step 75480, loss = 2.07, batch loss = 2.02 (11.4 examples/sec; 0.702 sec/batch; 50h:08m:17s remains)
INFO - root - 2017-12-08 01:10:23.868034: step 75490, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 51h:05m:20s remains)
INFO - root - 2017-12-08 01:10:30.682360: step 75500, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 46h:43m:53s remains)
2017-12-08 01:10:31.433839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2995038 -4.2928376 -4.2859225 -4.2743635 -4.2619081 -4.2580895 -4.2655854 -4.2722425 -4.2788515 -4.2821054 -4.2785287 -4.2639608 -4.248445 -4.2366047 -4.2192626][-4.3025413 -4.2909665 -4.275146 -4.2509427 -4.2269406 -4.2182608 -4.2268171 -4.2396975 -4.2539496 -4.2725697 -4.2856946 -4.2825403 -4.2736444 -4.2637391 -4.2485371][-4.28939 -4.2723269 -4.2478356 -4.2104726 -4.1721611 -4.1540842 -4.156476 -4.1741586 -4.1972671 -4.2325339 -4.2674694 -4.2836876 -4.2893395 -4.29008 -4.2829342][-4.2521706 -4.2334194 -4.20614 -4.1597085 -4.1070023 -4.0719762 -4.0566435 -4.0736642 -4.1144605 -4.169981 -4.2221165 -4.2580023 -4.2832327 -4.2969813 -4.2977514][-4.1924338 -4.17328 -4.1464128 -4.0984769 -4.033884 -3.9688306 -3.9197595 -3.9335704 -4.0053663 -4.0931482 -4.1632214 -4.2124343 -4.2548671 -4.2836132 -4.2909894][-4.1079288 -4.08188 -4.0635824 -4.0299921 -3.9674048 -3.868578 -3.7658956 -3.762208 -3.8720469 -3.9988632 -4.0911541 -4.1479645 -4.199512 -4.2453904 -4.2631636][-3.9984579 -3.9740655 -3.9736462 -3.96408 -3.909868 -3.7843509 -3.6283667 -3.5969605 -3.7428358 -3.9043732 -4.0075669 -4.0602641 -4.1164756 -4.1782856 -4.2097645][-3.9374294 -3.924252 -3.941406 -3.9465556 -3.9106884 -3.8023224 -3.654453 -3.6092627 -3.7282262 -3.8614402 -3.9389734 -3.9755759 -4.0313668 -4.1074538 -4.1532145][-3.9364209 -3.9384131 -3.9561098 -3.961493 -3.9486163 -3.8912189 -3.8012738 -3.766494 -3.8187177 -3.8810217 -3.9145195 -3.9383361 -3.999711 -4.0823841 -4.1324573][-3.9878163 -3.9967995 -4.0111442 -4.0168309 -4.0154848 -3.9919419 -3.9441078 -3.9175022 -3.924129 -3.9274492 -3.928117 -3.9501472 -4.0161982 -4.0950761 -4.1391][-4.0864196 -4.0907154 -4.1021223 -4.1055846 -4.1013036 -4.0860229 -4.05836 -4.0423374 -4.0379934 -4.0222554 -4.0083079 -4.01965 -4.0737367 -4.1356916 -4.1703095][-4.2003527 -4.2006307 -4.2113867 -4.2120161 -4.1995978 -4.1808586 -4.1604376 -4.1541376 -4.1558375 -4.1428685 -4.1247487 -4.1249223 -4.1579976 -4.1974421 -4.2193427][-4.2836108 -4.2794261 -4.2884393 -4.2910409 -4.280673 -4.2646708 -4.2507849 -4.2472224 -4.2500644 -4.2411814 -4.2242966 -4.218998 -4.2341814 -4.254437 -4.2643905][-4.3152785 -4.304285 -4.3080387 -4.3156548 -4.3192892 -4.3144674 -4.3055267 -4.3009987 -4.3026438 -4.2966752 -4.2854462 -4.2801509 -4.2848825 -4.2928433 -4.2936492][-4.308743 -4.293726 -4.2929406 -4.3005514 -4.3183079 -4.3270588 -4.3269372 -4.3288875 -4.3347 -4.3299661 -4.3202996 -4.3148904 -4.3133073 -4.3141322 -4.3083634]]...]
INFO - root - 2017-12-08 01:10:38.163557: step 75510, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.740 sec/batch; 52h:49m:23s remains)
INFO - root - 2017-12-08 01:10:45.000695: step 75520, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 46h:31m:13s remains)
INFO - root - 2017-12-08 01:10:51.796138: step 75530, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 45h:57m:28s remains)
INFO - root - 2017-12-08 01:10:58.713215: step 75540, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 48h:31m:22s remains)
INFO - root - 2017-12-08 01:11:05.592830: step 75550, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 50h:53m:14s remains)
INFO - root - 2017-12-08 01:11:12.379759: step 75560, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 49h:19m:14s remains)
INFO - root - 2017-12-08 01:11:19.188371: step 75570, loss = 2.04, batch loss = 1.99 (12.4 examples/sec; 0.644 sec/batch; 45h:57m:38s remains)
INFO - root - 2017-12-08 01:11:25.947575: step 75580, loss = 2.04, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 46h:43m:55s remains)
INFO - root - 2017-12-08 01:11:32.779639: step 75590, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.706 sec/batch; 50h:21m:20s remains)
INFO - root - 2017-12-08 01:11:39.690402: step 75600, loss = 2.08, batch loss = 2.02 (11.4 examples/sec; 0.700 sec/batch; 49h:56m:08s remains)
2017-12-08 01:11:40.443442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2700396 -4.2723107 -4.2775292 -4.2852659 -4.283113 -4.2763705 -4.2689385 -4.2630739 -4.2673445 -4.2720737 -4.2793808 -4.2861581 -4.2825904 -4.2807055 -4.2840319][-4.2246113 -4.2336111 -4.2472143 -4.262157 -4.2600412 -4.2493978 -4.2348976 -4.2222457 -4.2240481 -4.2310948 -4.2426915 -4.2503967 -4.2464395 -4.2470574 -4.254158][-4.198554 -4.21515 -4.24207 -4.263936 -4.2610645 -4.2443585 -4.2210784 -4.1999702 -4.1972494 -4.2054162 -4.2215595 -4.2315288 -4.2295475 -4.2306271 -4.2391467][-4.1820068 -4.2093 -4.2464623 -4.2658849 -4.2580619 -4.2387238 -4.2081671 -4.1818304 -4.1749887 -4.1845856 -4.2128944 -4.2347364 -4.2421 -4.2458372 -4.24647][-4.177628 -4.2117667 -4.24234 -4.2515655 -4.2354956 -4.2058721 -4.1607847 -4.1263514 -4.1145883 -4.1220574 -4.1702075 -4.2181311 -4.2434793 -4.2601318 -4.2596726][-4.167695 -4.1914825 -4.2037349 -4.1992874 -4.1773868 -4.1291366 -4.0524254 -3.9995341 -3.9771619 -3.9733229 -4.0515685 -4.1452174 -4.1997371 -4.2352428 -4.243135][-4.178669 -4.1851497 -4.1764188 -4.1544895 -4.1221957 -4.0531087 -3.9341986 -3.8479786 -3.793674 -3.7747688 -3.9032757 -4.0585656 -4.1504841 -4.2054405 -4.2246604][-4.2108464 -4.1994133 -4.1752725 -4.1433849 -4.10908 -4.043251 -3.9247532 -3.8353329 -3.7682018 -3.748796 -3.8862667 -4.0460238 -4.1435647 -4.20497 -4.2263465][-4.2531157 -4.23968 -4.2119017 -4.179431 -4.1571836 -4.1148748 -4.033092 -3.97536 -3.9231951 -3.9001341 -3.992069 -4.1042676 -4.1759038 -4.2235494 -4.241848][-4.2768707 -4.2758994 -4.2607245 -4.2349238 -4.2176714 -4.1858664 -4.1395416 -4.1083918 -4.0746989 -4.0476575 -4.0965815 -4.1650214 -4.2097044 -4.2369981 -4.2452555][-4.2894645 -4.2989373 -4.3012919 -4.2870569 -4.2721539 -4.2462063 -4.2213173 -4.2091637 -4.1868992 -4.1627874 -4.18182 -4.2144446 -4.2351685 -4.2452207 -4.2457228][-4.2828794 -4.2935591 -4.2995706 -4.2909303 -4.2749381 -4.2539754 -4.2422338 -4.24275 -4.2307577 -4.2085881 -4.2107668 -4.2221785 -4.2298231 -4.2336779 -4.2354078][-4.2710023 -4.276824 -4.2766609 -4.2692056 -4.2524161 -4.2332587 -4.2268338 -4.2322416 -4.2291369 -4.2190008 -4.2171388 -4.2175393 -4.2201381 -4.223208 -4.2308159][-4.264389 -4.2719846 -4.2729373 -4.26822 -4.2554789 -4.2425847 -4.2379079 -4.2398725 -4.2384825 -4.2361364 -4.2371044 -4.2359653 -4.2372742 -4.2402306 -4.2477813][-4.2684064 -4.2792726 -4.2856836 -4.2875314 -4.2843556 -4.2812548 -4.2808628 -4.2805586 -4.2798958 -4.2798882 -4.2804885 -4.2780924 -4.2764292 -4.277565 -4.2831717]]...]
INFO - root - 2017-12-08 01:11:46.930777: step 75610, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.694 sec/batch; 49h:31m:05s remains)
INFO - root - 2017-12-08 01:11:53.821521: step 75620, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.701 sec/batch; 50h:00m:22s remains)
INFO - root - 2017-12-08 01:12:00.713195: step 75630, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 52h:21m:10s remains)
INFO - root - 2017-12-08 01:12:07.498606: step 75640, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.628 sec/batch; 44h:50m:15s remains)
INFO - root - 2017-12-08 01:12:14.315300: step 75650, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 49h:02m:08s remains)
INFO - root - 2017-12-08 01:12:21.153407: step 75660, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.722 sec/batch; 51h:31m:41s remains)
INFO - root - 2017-12-08 01:12:28.012286: step 75670, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 50h:54m:18s remains)
INFO - root - 2017-12-08 01:12:34.774181: step 75680, loss = 2.03, batch loss = 1.97 (12.0 examples/sec; 0.669 sec/batch; 47h:42m:58s remains)
INFO - root - 2017-12-08 01:12:41.530060: step 75690, loss = 2.07, batch loss = 2.02 (12.4 examples/sec; 0.647 sec/batch; 46h:10m:42s remains)
INFO - root - 2017-12-08 01:12:48.425209: step 75700, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.721 sec/batch; 51h:24m:37s remains)
2017-12-08 01:12:49.113830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2820649 -4.2800031 -4.2787075 -4.2776017 -4.2794318 -4.2795873 -4.276741 -4.2854671 -4.2995296 -4.3049126 -4.3024344 -4.2881336 -4.2687902 -4.2566833 -4.2525291][-4.2651868 -4.2629886 -4.2647076 -4.2617607 -4.2590866 -4.2525454 -4.24355 -4.2568903 -4.2776728 -4.2846017 -4.2811532 -4.2633324 -4.2388105 -4.2221894 -4.2180505][-4.2455497 -4.2391219 -4.242424 -4.2407722 -4.233397 -4.2143373 -4.1924362 -4.2056952 -4.2363148 -4.2552915 -4.2579746 -4.2433052 -4.2174468 -4.2007685 -4.195725][-4.2198648 -4.2069726 -4.2083631 -4.2062449 -4.1914492 -4.1566048 -4.1181993 -4.1281347 -4.1715946 -4.2054081 -4.219264 -4.216373 -4.2013683 -4.1955218 -4.1964846][-4.1940641 -4.1701756 -4.162508 -4.1507235 -4.1235471 -4.0696526 -4.0131869 -4.0205841 -4.0798149 -4.1390214 -4.1768203 -4.1932759 -4.1971459 -4.2026949 -4.2090454][-4.1868143 -4.1550055 -4.1375356 -4.1077604 -4.0593967 -3.9773455 -3.8869345 -3.884038 -3.960659 -4.053401 -4.1226025 -4.1593642 -4.1747255 -4.1843004 -4.1933594][-4.1920733 -4.1597328 -4.1364355 -4.091393 -4.0225606 -3.9136331 -3.7868032 -3.7750022 -3.8660326 -3.9821517 -4.0732355 -4.1242127 -4.1439171 -4.1488981 -4.15767][-4.1943932 -4.1635957 -4.1458282 -4.1071329 -4.0470719 -3.9428134 -3.8224239 -3.8081198 -3.8862209 -3.9918485 -4.0790825 -4.1321878 -4.1506305 -4.1510224 -4.1591635][-4.1853867 -4.1511712 -4.1408954 -4.1217747 -4.0868177 -4.0146027 -3.9288864 -3.9111013 -3.9607494 -4.0360274 -4.1102543 -4.1598167 -4.1797366 -4.1871305 -4.1977253][-4.1748962 -4.1360373 -4.1324263 -4.1299639 -4.1181912 -4.0755 -4.0184565 -4.003994 -4.0327263 -4.080915 -4.1385851 -4.1835265 -4.2071829 -4.2202392 -4.23117][-4.1781578 -4.140152 -4.1414852 -4.146831 -4.1434855 -4.1163979 -4.0799918 -4.0753412 -4.0948172 -4.12163 -4.1602378 -4.1990218 -4.226459 -4.2473116 -4.2631822][-4.1959848 -4.1638489 -4.1669641 -4.1710172 -4.165906 -4.1476669 -4.1267681 -4.1298513 -4.1429968 -4.1523705 -4.1760592 -4.2043109 -4.2262306 -4.2441788 -4.2625494][-4.2192369 -4.1943107 -4.1960621 -4.1951323 -4.188138 -4.1755829 -4.165638 -4.1737537 -4.1869364 -4.1924367 -4.2040339 -4.2191739 -4.232502 -4.2420907 -4.2559891][-4.2486238 -4.2298732 -4.2269459 -4.2222471 -4.2135921 -4.2034678 -4.2003436 -4.2107034 -4.2240558 -4.2281413 -4.23141 -4.2367835 -4.2434707 -4.248559 -4.2582173][-4.2759891 -4.2587533 -4.2522206 -4.2458658 -4.2397046 -4.2357907 -4.2361817 -4.2436509 -4.2538314 -4.2559924 -4.2555828 -4.2558274 -4.2593951 -4.2641582 -4.2732096]]...]
INFO - root - 2017-12-08 01:12:55.804890: step 75710, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:41m:24s remains)
INFO - root - 2017-12-08 01:13:02.611882: step 75720, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.680 sec/batch; 48h:30m:28s remains)
INFO - root - 2017-12-08 01:13:09.589811: step 75730, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 51h:47m:48s remains)
INFO - root - 2017-12-08 01:13:16.401690: step 75740, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.701 sec/batch; 50h:01m:35s remains)
INFO - root - 2017-12-08 01:13:23.187619: step 75750, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.699 sec/batch; 49h:49m:29s remains)
INFO - root - 2017-12-08 01:13:30.081510: step 75760, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.678 sec/batch; 48h:23m:02s remains)
INFO - root - 2017-12-08 01:13:36.850250: step 75770, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.687 sec/batch; 48h:59m:06s remains)
INFO - root - 2017-12-08 01:13:43.716207: step 75780, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.739 sec/batch; 52h:40m:26s remains)
INFO - root - 2017-12-08 01:13:50.521269: step 75790, loss = 2.04, batch loss = 1.98 (11.5 examples/sec; 0.697 sec/batch; 49h:43m:42s remains)
INFO - root - 2017-12-08 01:13:57.355750: step 75800, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 47h:48m:11s remains)
2017-12-08 01:13:58.127845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2578173 -4.2543039 -4.2541766 -4.2524328 -4.2495637 -4.2473264 -4.2456422 -4.2450266 -4.2457695 -4.2468 -4.2460551 -4.2418647 -4.2305408 -4.2138038 -4.2034292][-4.2679443 -4.2625866 -4.2645831 -4.266118 -4.2668304 -4.2648597 -4.2617483 -4.2611127 -4.263876 -4.2670794 -4.2664552 -4.2591119 -4.2414317 -4.2153058 -4.1999726][-4.2649088 -4.2557087 -4.2598653 -4.2662764 -4.270607 -4.268374 -4.2638707 -4.2624612 -4.2665987 -4.2711811 -4.2700949 -4.260488 -4.2401409 -4.20899 -4.1895537][-4.2576036 -4.2415271 -4.2430921 -4.2487221 -4.2529278 -4.249299 -4.2433834 -4.2413487 -4.2460322 -4.2525678 -4.2540007 -4.2461724 -4.2285223 -4.199358 -4.1784105][-4.2469482 -4.2213793 -4.2114816 -4.2104583 -4.2119493 -4.206882 -4.1991291 -4.1940355 -4.1978097 -4.2072191 -4.2157469 -4.2166553 -4.2090526 -4.1876526 -4.1694126][-4.2291613 -4.1937785 -4.1700072 -4.1607041 -4.1560993 -4.1467013 -4.13271 -4.11925 -4.1183729 -4.1330342 -4.1555386 -4.1733303 -4.1803107 -4.1714535 -4.1611152][-4.2028346 -4.1624103 -4.1323891 -4.1168814 -4.1044025 -4.08595 -4.0591016 -4.0257273 -4.0081677 -4.0262966 -4.0651946 -4.1033478 -4.126461 -4.1326256 -4.1351357][-4.1752458 -4.1385918 -4.1107903 -4.0907125 -4.0677695 -4.0369325 -3.9937272 -3.9344959 -3.892164 -3.9120567 -3.9663181 -4.0255275 -4.0648818 -4.0842304 -4.0963092][-4.1848669 -4.1566496 -4.1360006 -4.1174412 -4.0941286 -4.0645771 -4.0215936 -3.9593029 -3.9087245 -3.9174538 -3.9625785 -4.0205894 -4.06153 -4.081593 -4.0905104][-4.2092967 -4.1884375 -4.1732135 -4.1579161 -4.1421466 -4.1240969 -4.0989809 -4.0578694 -4.0221553 -4.0173669 -4.0341969 -4.0681052 -4.0955653 -4.1051912 -4.1046805][-4.2197943 -4.2014894 -4.1877313 -4.1753345 -4.1669455 -4.1610155 -4.1533566 -4.1347446 -4.1137047 -4.1034908 -4.1016541 -4.1154413 -4.1280069 -4.12733 -4.1225691][-4.2093987 -4.1920066 -4.1812897 -4.17393 -4.1726675 -4.1750488 -4.1754975 -4.1705875 -4.1624947 -4.1575375 -4.1541381 -4.1578326 -4.16051 -4.1515756 -4.1456785][-4.2025137 -4.184535 -4.1782842 -4.1782069 -4.181603 -4.1886144 -4.1923041 -4.1935754 -4.1937671 -4.1963153 -4.1987281 -4.1987429 -4.1935019 -4.1769919 -4.1665363][-4.2021427 -4.1831045 -4.1780109 -4.1811681 -4.1876793 -4.1957932 -4.2004814 -4.2042761 -4.2097726 -4.2170663 -4.2235875 -4.2237115 -4.2167859 -4.1982517 -4.1833415][-4.20717 -4.1908832 -4.1844711 -4.1856456 -4.1924081 -4.2015285 -4.2075491 -4.2131615 -4.2202229 -4.2277107 -4.23433 -4.2349181 -4.2299805 -4.2139034 -4.1988673]]...]
INFO - root - 2017-12-08 01:14:04.650181: step 75810, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.663 sec/batch; 47h:14m:53s remains)
INFO - root - 2017-12-08 01:14:11.374994: step 75820, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 47h:59m:53s remains)
INFO - root - 2017-12-08 01:14:18.094847: step 75830, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 47h:54m:46s remains)
INFO - root - 2017-12-08 01:14:24.845042: step 75840, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 46h:13m:27s remains)
INFO - root - 2017-12-08 01:14:31.601611: step 75850, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 49h:35m:28s remains)
INFO - root - 2017-12-08 01:14:38.398646: step 75860, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 48h:55m:52s remains)
INFO - root - 2017-12-08 01:14:45.182046: step 75870, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.712 sec/batch; 50h:44m:44s remains)
INFO - root - 2017-12-08 01:14:51.955812: step 75880, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.657 sec/batch; 46h:48m:44s remains)
INFO - root - 2017-12-08 01:14:58.691540: step 75890, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 45h:28m:31s remains)
INFO - root - 2017-12-08 01:15:05.446810: step 75900, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 47h:50m:20s remains)
2017-12-08 01:15:06.204713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2614532 -4.2607136 -4.2675653 -4.28529 -4.3115091 -4.3386326 -4.3600626 -4.3620472 -4.3174877 -4.23859 -4.1747231 -4.1653705 -4.2039337 -4.2499948 -4.2787442][-4.2370391 -4.2364016 -4.2437215 -4.2614737 -4.2867794 -4.3126287 -4.3365755 -4.3487258 -4.3192186 -4.2492075 -4.1896653 -4.1809378 -4.2164669 -4.2562985 -4.2773037][-4.218461 -4.2149067 -4.2219214 -4.2389345 -4.2637115 -4.2887316 -4.3123417 -4.3306408 -4.3194389 -4.268826 -4.2238255 -4.2186589 -4.2475338 -4.2774277 -4.2873044][-4.2150168 -4.2081637 -4.2113495 -4.2236691 -4.2427778 -4.2647223 -4.2853403 -4.3040471 -4.3028617 -4.2699838 -4.242619 -4.2453313 -4.2738032 -4.3004847 -4.3066349][-4.2217569 -4.211936 -4.2111526 -4.215106 -4.2263203 -4.2417345 -4.2544155 -4.2668152 -4.2642245 -4.2401857 -4.2276373 -4.2425413 -4.2778406 -4.3121872 -4.32766][-4.223712 -4.2118826 -4.2073817 -4.2035503 -4.2089272 -4.2164655 -4.2154584 -4.2152319 -4.2055755 -4.1832404 -4.17892 -4.2076936 -4.2590537 -4.3099227 -4.3408761][-4.2254572 -4.2113667 -4.2030888 -4.1937304 -4.1945033 -4.1963115 -4.186574 -4.1717715 -4.1508851 -4.1252494 -4.1260414 -4.1679258 -4.2322435 -4.2952156 -4.3397441][-4.2254744 -4.2123737 -4.2038941 -4.1936183 -4.1903787 -4.186461 -4.1727009 -4.1469779 -4.1094189 -4.0757217 -4.0813651 -4.136888 -4.213284 -4.28311 -4.3302712][-4.2289205 -4.2207303 -4.2147593 -4.2030587 -4.1953235 -4.1868038 -4.171216 -4.1393013 -4.0888186 -4.0462708 -4.0520535 -4.1148863 -4.1991353 -4.2721605 -4.3177586][-4.2296839 -4.2255869 -4.2232094 -4.2139831 -4.2038169 -4.1922851 -4.1769686 -4.1481915 -4.0999675 -4.0588942 -4.0622907 -4.1174064 -4.1962423 -4.265635 -4.3066063][-4.2348447 -4.2310948 -4.231617 -4.227468 -4.2199492 -4.2106485 -4.2009187 -4.1844382 -4.1526341 -4.1247373 -4.12692 -4.1633935 -4.2189126 -4.2699008 -4.2992654][-4.2659507 -4.2623696 -4.2622252 -4.2590432 -4.2552414 -4.2528038 -4.2522578 -4.2481856 -4.2343464 -4.2224259 -4.2233186 -4.2392011 -4.2660856 -4.291822 -4.3064723][-4.3011532 -4.2990074 -4.2990417 -4.298367 -4.2974696 -4.2975912 -4.3000965 -4.3012924 -4.2978616 -4.2929506 -4.2901678 -4.2941322 -4.3052392 -4.3159819 -4.3215537][-4.3245091 -4.3229585 -4.323699 -4.3246651 -4.3248248 -4.3247337 -4.3260503 -4.3280416 -4.3281703 -4.3267012 -4.3238273 -4.3232594 -4.3266115 -4.3310494 -4.3332958][-4.3377953 -4.3366084 -4.33692 -4.33775 -4.3376389 -4.3370423 -4.3367481 -4.3373418 -4.3384552 -4.3392158 -4.3384204 -4.3373733 -4.3373675 -4.3387928 -4.3400917]]...]
INFO - root - 2017-12-08 01:15:12.833132: step 75910, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.675 sec/batch; 48h:04m:32s remains)
INFO - root - 2017-12-08 01:15:19.290568: step 75920, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 46h:11m:45s remains)
INFO - root - 2017-12-08 01:15:26.203667: step 75930, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.700 sec/batch; 49h:54m:09s remains)
INFO - root - 2017-12-08 01:15:33.031770: step 75940, loss = 2.05, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 47h:35m:47s remains)
INFO - root - 2017-12-08 01:15:39.887774: step 75950, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:38m:52s remains)
INFO - root - 2017-12-08 01:15:46.694816: step 75960, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 45h:14m:46s remains)
INFO - root - 2017-12-08 01:15:53.491648: step 75970, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.686 sec/batch; 48h:52m:01s remains)
INFO - root - 2017-12-08 01:16:00.443327: step 75980, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.713 sec/batch; 50h:49m:38s remains)
INFO - root - 2017-12-08 01:16:07.197087: step 75990, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 49h:29m:51s remains)
INFO - root - 2017-12-08 01:16:14.093725: step 76000, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.645 sec/batch; 45h:57m:25s remains)
2017-12-08 01:16:14.852708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32796 -4.3251295 -4.3251371 -4.3271303 -4.3308659 -4.3355384 -4.338666 -4.3412814 -4.3437114 -4.34512 -4.344152 -4.3419533 -4.3402457 -4.3372107 -4.3328352][-4.3187389 -4.3143005 -4.3142695 -4.3173642 -4.3232508 -4.3292966 -4.3316011 -4.3317738 -4.3329315 -4.33524 -4.3347974 -4.33244 -4.3314128 -4.3284478 -4.3229222][-4.3047609 -4.2985005 -4.2987471 -4.3010564 -4.3063865 -4.3122845 -4.3108125 -4.3061371 -4.3069177 -4.3129749 -4.3145957 -4.3127108 -4.3105769 -4.3070154 -4.302505][-4.2793627 -4.2645483 -4.2619953 -4.26691 -4.2745833 -4.2807584 -4.2742391 -4.2643323 -4.2659435 -4.2772331 -4.2812414 -4.2794976 -4.2737231 -4.2694592 -4.2701254][-4.2384787 -4.2079639 -4.1967807 -4.2024655 -4.2101831 -4.2148967 -4.1998563 -4.184176 -4.1931167 -4.2188888 -4.2310724 -4.2299457 -4.2202959 -4.2156692 -4.2246623][-4.1905613 -4.1426706 -4.1210709 -4.1233439 -4.1280532 -4.125011 -4.0946097 -4.0697889 -4.0935712 -4.1420784 -4.1671052 -4.1698875 -4.1594753 -4.1609879 -4.1808171][-4.1493359 -4.08675 -4.0540361 -4.0516481 -4.0469995 -4.0261507 -3.9674683 -3.9314981 -3.9823034 -4.06018 -4.1020374 -4.1128473 -4.1067567 -4.1152792 -4.1443419][-4.11973 -4.0490522 -4.0037389 -3.9892805 -3.9713688 -3.9220848 -3.8266931 -3.7825823 -3.8687744 -3.9740272 -4.0366488 -4.0623684 -4.0687914 -4.0878344 -4.1246619][-4.10598 -4.0343962 -3.9842565 -3.9655581 -3.946717 -3.8893461 -3.7894962 -3.7588437 -3.8620653 -3.9635892 -4.0299611 -4.0665131 -4.0821333 -4.1048493 -4.1373343][-4.1187868 -4.0670247 -4.0359726 -4.0335712 -4.0330191 -3.9977939 -3.9305904 -3.9153583 -3.9859862 -4.0474 -4.0955043 -4.1292324 -4.144701 -4.1577168 -4.1779227][-4.1381049 -4.1078486 -4.0968189 -4.1103377 -4.1254559 -4.113626 -4.0772762 -4.0685096 -4.1001363 -4.1274147 -4.1640363 -4.1947393 -4.2083087 -4.2110271 -4.22023][-4.1714659 -4.1538825 -4.1513252 -4.1687169 -4.1877952 -4.1887717 -4.1719384 -4.1640139 -4.174161 -4.1863284 -4.2171621 -4.2432165 -4.2562294 -4.2547655 -4.2573853][-4.2076411 -4.1963134 -4.1965728 -4.2115512 -4.2295537 -4.2352343 -4.2286248 -4.2201219 -4.2189307 -4.2237554 -4.2484436 -4.2700334 -4.2843351 -4.2856889 -4.2875905][-4.2382841 -4.2288637 -4.227808 -4.2355971 -4.2446756 -4.2479029 -4.2433157 -4.232862 -4.2265582 -4.22873 -4.2483182 -4.2697349 -4.2866168 -4.2922478 -4.296454][-4.2694545 -4.2600355 -4.256124 -4.2564759 -4.2590322 -4.2590566 -4.2556286 -4.2475233 -4.2410693 -4.2408876 -4.254952 -4.2737851 -4.2900724 -4.2973204 -4.3014526]]...]
INFO - root - 2017-12-08 01:16:21.485111: step 76010, loss = 2.10, batch loss = 2.04 (11.1 examples/sec; 0.723 sec/batch; 51h:31m:24s remains)
INFO - root - 2017-12-08 01:16:28.295271: step 76020, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 49h:35m:52s remains)
INFO - root - 2017-12-08 01:16:35.097620: step 76030, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.668 sec/batch; 47h:34m:08s remains)
INFO - root - 2017-12-08 01:16:41.916762: step 76040, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.656 sec/batch; 46h:45m:26s remains)
INFO - root - 2017-12-08 01:16:48.749424: step 76050, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.692 sec/batch; 49h:19m:07s remains)
INFO - root - 2017-12-08 01:16:55.599790: step 76060, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 50h:33m:05s remains)
INFO - root - 2017-12-08 01:17:02.373010: step 76070, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.667 sec/batch; 47h:31m:51s remains)
INFO - root - 2017-12-08 01:17:09.137914: step 76080, loss = 2.05, batch loss = 1.99 (12.3 examples/sec; 0.653 sec/batch; 46h:28m:42s remains)
INFO - root - 2017-12-08 01:17:16.039211: step 76090, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 48h:20m:12s remains)
INFO - root - 2017-12-08 01:17:22.839397: step 76100, loss = 2.09, batch loss = 2.04 (11.2 examples/sec; 0.717 sec/batch; 51h:03m:12s remains)
2017-12-08 01:17:23.631895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1819863 -4.1953588 -4.2161579 -4.2224779 -4.2072287 -4.1832919 -4.1655297 -4.1577644 -4.1474128 -4.1224689 -4.112155 -4.1312509 -4.1537666 -4.1632233 -4.1700206][-4.1908045 -4.1993127 -4.2127037 -4.2098556 -4.1866736 -4.1606383 -4.1443853 -4.1461339 -4.1415482 -4.1187935 -4.1114364 -4.136446 -4.1633554 -4.1761503 -4.1817451][-4.2015233 -4.202035 -4.2079167 -4.1992855 -4.1706758 -4.1420317 -4.1263127 -4.1371861 -4.1438808 -4.1295843 -4.1249022 -4.1460814 -4.1656809 -4.169064 -4.1639862][-4.1964755 -4.1911097 -4.192183 -4.1826129 -4.1510291 -4.1162186 -4.0995178 -4.118885 -4.1410842 -4.1391144 -4.1387615 -4.15505 -4.164022 -4.1569238 -4.1399264][-4.1662979 -4.1654849 -4.1645322 -4.1546535 -4.1207252 -4.0791035 -4.0603971 -4.0872073 -4.12652 -4.1392279 -4.143713 -4.1551075 -4.1577773 -4.152709 -4.1379027][-4.1301365 -4.1384449 -4.1424432 -4.134562 -4.1021118 -4.05593 -4.0322833 -4.0601168 -4.1070995 -4.1239743 -4.1291265 -4.1390109 -4.1490545 -4.156518 -4.1517711][-4.102447 -4.1148934 -4.1254768 -4.1249561 -4.1018353 -4.0584908 -4.0296116 -4.05036 -4.0901566 -4.0999827 -4.0988903 -4.1068048 -4.1298289 -4.1534519 -4.163517][-4.085506 -4.0984373 -4.1156049 -4.1237173 -4.1131163 -4.0760951 -4.045619 -4.0596719 -4.089376 -4.0910473 -4.0836363 -4.0884128 -4.1130915 -4.1387768 -4.1572771][-4.0699263 -4.0846663 -4.1063247 -4.1215777 -4.1171207 -4.0861955 -4.0546932 -4.0603042 -4.0814228 -4.082016 -4.0738893 -4.0780296 -4.0969844 -4.1145639 -4.1288052][-4.0751319 -4.0828581 -4.0987344 -4.1094623 -4.1027603 -4.07321 -4.0439773 -4.0446563 -4.0611644 -4.0663056 -4.0624352 -4.0677123 -4.0817361 -4.0944996 -4.1062551][-4.0983958 -4.0942931 -4.1001821 -4.1051235 -4.0961423 -4.0706668 -4.0470223 -4.0465307 -4.0597115 -4.0658712 -4.0624223 -4.0586157 -4.0625582 -4.0704508 -4.0815377][-4.1278076 -4.1182656 -4.1172085 -4.1167293 -4.104311 -4.0816789 -4.060338 -4.0567794 -4.0655694 -4.0717773 -4.066999 -4.0522633 -4.0414567 -4.0432477 -4.0535712][-4.1395726 -4.1298347 -4.1269298 -4.1216407 -4.1081967 -4.0898752 -4.0732365 -4.0678844 -4.0760827 -4.085938 -4.0822358 -4.058701 -4.0374403 -4.0352769 -4.0434132][-4.1322503 -4.1266985 -4.1268411 -4.124608 -4.1172771 -4.1072125 -4.098062 -4.0946131 -4.1032782 -4.1148238 -4.1097322 -4.0805211 -4.0532746 -4.0472832 -4.0511565][-4.1248775 -4.1223478 -4.12548 -4.1272511 -4.1252961 -4.1220551 -4.1186724 -4.118741 -4.1281967 -4.1399379 -4.1378164 -4.1133704 -4.0868936 -4.0772595 -4.0787444]]...]
INFO - root - 2017-12-08 01:17:30.161518: step 76110, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.672 sec/batch; 47h:51m:08s remains)
INFO - root - 2017-12-08 01:17:36.930848: step 76120, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 49h:35m:36s remains)
INFO - root - 2017-12-08 01:17:43.920786: step 76130, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.726 sec/batch; 51h:43m:34s remains)
INFO - root - 2017-12-08 01:17:50.669386: step 76140, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.681 sec/batch; 48h:29m:30s remains)
INFO - root - 2017-12-08 01:17:57.369630: step 76150, loss = 2.06, batch loss = 2.01 (12.3 examples/sec; 0.648 sec/batch; 46h:09m:37s remains)
INFO - root - 2017-12-08 01:18:04.246222: step 76160, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 47h:56m:53s remains)
INFO - root - 2017-12-08 01:18:11.094715: step 76170, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 49h:11m:21s remains)
INFO - root - 2017-12-08 01:18:18.065690: step 76180, loss = 2.04, batch loss = 1.99 (11.4 examples/sec; 0.701 sec/batch; 49h:56m:17s remains)
INFO - root - 2017-12-08 01:18:24.964750: step 76190, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.707 sec/batch; 50h:18m:40s remains)
INFO - root - 2017-12-08 01:18:31.718862: step 76200, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 45h:53m:35s remains)
2017-12-08 01:18:32.428043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2693119 -4.2635407 -4.2618914 -4.2651286 -4.265502 -4.2619543 -4.2647996 -4.2702594 -4.2712755 -4.2717562 -4.2692242 -4.2641563 -4.2627335 -4.2642636 -4.2599316][-4.2496133 -4.2452869 -4.2454386 -4.2461042 -4.2379532 -4.2302938 -4.2342992 -4.2422976 -4.246387 -4.2504172 -4.249825 -4.2486119 -4.2499714 -4.2541595 -4.2456059][-4.2224526 -4.2173629 -4.2126746 -4.2042665 -4.187324 -4.174675 -4.1789927 -4.193707 -4.2061663 -4.2134366 -4.2140832 -4.2164016 -4.2235484 -4.2345681 -4.2258191][-4.2032042 -4.1954336 -4.1807919 -4.1607628 -4.1369486 -4.1159143 -4.1140542 -4.1335049 -4.1565695 -4.1697583 -4.1727567 -4.1756768 -4.1848817 -4.2005835 -4.1965742][-4.1977015 -4.1810565 -4.1556611 -4.1251326 -4.0954032 -4.0681262 -4.0572891 -4.0731091 -4.105957 -4.1307297 -4.138237 -4.1350851 -4.1398396 -4.1601191 -4.1637311][-4.2002621 -4.1756129 -4.1448016 -4.1091042 -4.076848 -4.0481844 -4.0247512 -4.0298 -4.0697851 -4.1078696 -4.1248431 -4.1143732 -4.1051559 -4.1257939 -4.137908][-4.1895213 -4.1568727 -4.126956 -4.0935707 -4.0598583 -4.0158882 -3.9660156 -3.9587071 -4.024075 -4.0863204 -4.1129174 -4.097641 -4.0779176 -4.0912313 -4.1046276][-4.1748352 -4.1369658 -4.109993 -4.0825582 -4.0495319 -3.9913523 -3.9149077 -3.8904605 -3.9821804 -4.0693107 -4.1010256 -4.0752711 -4.0389094 -4.0392766 -4.0615778][-4.1704845 -4.13834 -4.1166162 -4.0981307 -4.0775604 -4.0343046 -3.973098 -3.9473271 -4.0214767 -4.0930037 -4.11375 -4.0746474 -4.018929 -3.998589 -4.0291028][-4.1680479 -4.1483526 -4.130362 -4.1182404 -4.1092834 -4.084619 -4.0507903 -4.0400748 -4.0859575 -4.1297345 -4.1408911 -4.1062136 -4.0571837 -4.0304747 -4.0510225][-4.1668591 -4.1586103 -4.1449256 -4.1329174 -4.1248131 -4.1065755 -4.0840092 -4.0797114 -4.1063476 -4.1355138 -4.1423926 -4.1221876 -4.0919542 -4.0748281 -4.0881567][-4.1663446 -4.1697884 -4.1641088 -4.1530356 -4.1414957 -4.1233292 -4.103723 -4.1004815 -4.115582 -4.1314297 -4.1330752 -4.1208196 -4.1037421 -4.0949883 -4.1074986][-4.1646266 -4.1762886 -4.1791906 -4.1716957 -4.1616545 -4.1461358 -4.1302934 -4.127017 -4.13029 -4.1318083 -4.1270828 -4.1206408 -4.1127038 -4.1111259 -4.1247706][-4.1703858 -4.180563 -4.1893468 -4.1894674 -4.1814666 -4.16542 -4.1499429 -4.1461091 -4.1432333 -4.1369214 -4.1239414 -4.1162176 -4.1145158 -4.1143332 -4.1273317][-4.1765118 -4.1812921 -4.1927071 -4.2016559 -4.1965594 -4.1784291 -4.159236 -4.1516471 -4.1462259 -4.1386232 -4.1186719 -4.1034923 -4.1029296 -4.1025004 -4.1129751]]...]
INFO - root - 2017-12-08 01:18:39.021902: step 76210, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 47h:53m:07s remains)
INFO - root - 2017-12-08 01:18:45.887932: step 76220, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 49h:09m:29s remains)
INFO - root - 2017-12-08 01:18:52.431191: step 76230, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 48h:06m:53s remains)
INFO - root - 2017-12-08 01:18:59.291050: step 76240, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 50h:04m:05s remains)
INFO - root - 2017-12-08 01:19:06.147684: step 76250, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.728 sec/batch; 51h:49m:07s remains)
INFO - root - 2017-12-08 01:19:12.932179: step 76260, loss = 2.08, batch loss = 2.02 (11.7 examples/sec; 0.686 sec/batch; 48h:50m:43s remains)
INFO - root - 2017-12-08 01:19:19.639218: step 76270, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.660 sec/batch; 46h:56m:48s remains)
INFO - root - 2017-12-08 01:19:26.578111: step 76280, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.655 sec/batch; 46h:38m:51s remains)
INFO - root - 2017-12-08 01:19:33.418836: step 76290, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.718 sec/batch; 51h:06m:20s remains)
INFO - root - 2017-12-08 01:19:40.296216: step 76300, loss = 2.10, batch loss = 2.04 (10.6 examples/sec; 0.753 sec/batch; 53h:35m:47s remains)
2017-12-08 01:19:41.002837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2440815 -4.2367482 -4.2425756 -4.2545161 -4.2647176 -4.27005 -4.2688713 -4.2660861 -4.2680011 -4.2721915 -4.2743373 -4.2714262 -4.2615938 -4.2475829 -4.2411275][-4.2138834 -4.2088165 -4.2183251 -4.2312245 -4.2404337 -4.2411203 -4.2336612 -4.2275338 -4.2299037 -4.2350955 -4.2371855 -4.2368517 -4.2320652 -4.2253785 -4.2251229][-4.1866288 -4.1902695 -4.206337 -4.2197304 -4.2247992 -4.2163591 -4.1993041 -4.1896706 -4.1917896 -4.1951365 -4.192719 -4.1901078 -4.1923542 -4.1995883 -4.2117844][-4.16087 -4.176702 -4.1998844 -4.2128506 -4.2104454 -4.1880093 -4.1599627 -4.1540761 -4.1637063 -4.1687446 -4.1618571 -4.1523404 -4.1575031 -4.1744514 -4.1968908][-4.1350427 -4.159739 -4.1882744 -4.1978769 -4.1806564 -4.134696 -4.0914087 -4.1003728 -4.1356387 -4.1536841 -4.1459885 -4.1275864 -4.128355 -4.1464562 -4.1719818][-4.1224685 -4.1469131 -4.1729331 -4.1760707 -4.1402264 -4.0614128 -3.9915388 -4.0168896 -4.091341 -4.1351829 -4.1342134 -4.1110497 -4.1051259 -4.1226606 -4.1495719][-4.1267424 -4.1420808 -4.1627026 -4.161622 -4.1102338 -4.0038133 -3.8981252 -3.9256232 -4.0338845 -4.1056767 -4.121573 -4.10647 -4.1034269 -4.11878 -4.1442986][-4.1411867 -4.1433916 -4.1586123 -4.1616368 -4.1208134 -4.0227213 -3.9098723 -3.91307 -4.0148964 -4.0929613 -4.1201305 -4.1183834 -4.1257486 -4.1435809 -4.163754][-4.1532874 -4.1392145 -4.1481009 -4.1618733 -4.1508346 -4.0972285 -4.0227656 -4.010056 -4.0660987 -4.1171002 -4.1389771 -4.1437 -4.1570921 -4.1778603 -4.1945672][-4.1547565 -4.1275229 -4.1270027 -4.1451049 -4.1569114 -4.1446486 -4.1167955 -4.1132379 -4.1440439 -4.1707735 -4.1785078 -4.1796641 -4.1894393 -4.2054491 -4.2180157][-4.1621175 -4.1348882 -4.1266189 -4.1362867 -4.1513386 -4.1624813 -4.1653113 -4.1732864 -4.1997833 -4.21984 -4.2214417 -4.2187672 -4.2199106 -4.2258844 -4.2324996][-4.1770892 -4.1591644 -4.1504917 -4.1469374 -4.1522269 -4.1689095 -4.187407 -4.2031894 -4.2255425 -4.2464666 -4.25251 -4.2524176 -4.248219 -4.2423377 -4.2407041][-4.1919284 -4.1851459 -4.1815352 -4.1670747 -4.1562829 -4.1645265 -4.1852336 -4.2082329 -4.2288885 -4.249507 -4.2612267 -4.2676973 -4.2638168 -4.2521992 -4.2451138][-4.1936803 -4.1960735 -4.2013063 -4.1888461 -4.17026 -4.1703143 -4.1850595 -4.2095485 -4.2281103 -4.2455072 -4.2607532 -4.2716737 -4.2696128 -4.2579684 -4.2510376][-4.1996241 -4.2068696 -4.217792 -4.2131147 -4.1986346 -4.1953211 -4.2032576 -4.218637 -4.2326756 -4.2472949 -4.2629375 -4.2745275 -4.2773204 -4.2726917 -4.2705131]]...]
INFO - root - 2017-12-08 01:19:47.607331: step 76310, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.672 sec/batch; 47h:50m:17s remains)
INFO - root - 2017-12-08 01:19:54.430288: step 76320, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 50h:51m:08s remains)
INFO - root - 2017-12-08 01:20:01.236107: step 76330, loss = 2.07, batch loss = 2.02 (12.0 examples/sec; 0.666 sec/batch; 47h:25m:24s remains)
INFO - root - 2017-12-08 01:20:08.033594: step 76340, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 50h:38m:33s remains)
INFO - root - 2017-12-08 01:20:14.936489: step 76350, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 47h:22m:17s remains)
INFO - root - 2017-12-08 01:20:21.706164: step 76360, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.699 sec/batch; 49h:45m:06s remains)
INFO - root - 2017-12-08 01:20:28.592773: step 76370, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.724 sec/batch; 51h:31m:19s remains)
INFO - root - 2017-12-08 01:20:35.426431: step 76380, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 49h:45m:03s remains)
INFO - root - 2017-12-08 01:20:42.280465: step 76390, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 48h:50m:34s remains)
INFO - root - 2017-12-08 01:20:49.098777: step 76400, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 46h:57m:46s remains)
2017-12-08 01:20:49.877132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1029682 -4.0768957 -4.0862379 -4.1535482 -4.2422233 -4.3094096 -4.3372788 -4.3297834 -4.304297 -4.2623391 -4.2036271 -4.1601205 -4.1608696 -4.2029796 -4.2565703][-4.1769714 -4.1705837 -4.1888065 -4.2437496 -4.3027797 -4.3354983 -4.3379178 -4.3177533 -4.2857885 -4.2403078 -4.1892295 -4.168551 -4.1920862 -4.2399058 -4.2869158][-4.2549973 -4.2577105 -4.2719512 -4.3038688 -4.328289 -4.3308716 -4.3158789 -4.2917962 -4.2582526 -4.2186313 -4.1907887 -4.1968975 -4.2346783 -4.2782269 -4.3122663][-4.3139997 -4.3204865 -4.3233156 -4.3293042 -4.3265285 -4.3077645 -4.2798886 -4.2480097 -4.2139463 -4.1932764 -4.1976595 -4.2273016 -4.2701221 -4.3051882 -4.3274755][-4.3457208 -4.3493004 -4.3419452 -4.3302855 -4.3070493 -4.2679496 -4.2231669 -4.17955 -4.1523705 -4.159636 -4.1929245 -4.23778 -4.2827754 -4.3150587 -4.3304658][-4.3588314 -4.3563075 -4.3395548 -4.3105631 -4.2638927 -4.1959109 -4.1259894 -4.0764079 -4.0704184 -4.1144986 -4.1758633 -4.2321348 -4.2804289 -4.3140707 -4.3249435][-4.3616729 -4.3511186 -4.3201609 -4.2694063 -4.19485 -4.0942969 -4.0005741 -3.9553022 -3.9833152 -4.0681825 -4.1557364 -4.2237797 -4.2755604 -4.3086596 -4.3181973][-4.3540435 -4.3348384 -4.289463 -4.2224774 -4.1315389 -4.0207024 -3.9209721 -3.8892231 -3.9496627 -4.0644054 -4.1672568 -4.2366014 -4.2798982 -4.3033915 -4.3109508][-4.3418388 -4.3170452 -4.2694335 -4.2005205 -4.1127853 -4.0229125 -3.95015 -3.9394271 -4.0112896 -4.1238065 -4.210721 -4.2600875 -4.2822547 -4.2940035 -4.3007584][-4.334259 -4.3126616 -4.2750516 -4.2168956 -4.1473374 -4.090847 -4.0554838 -4.0603209 -4.117094 -4.1967916 -4.2472744 -4.2694917 -4.2730989 -4.2780838 -4.2863426][-4.3310666 -4.31827 -4.2960138 -4.2563529 -4.2104311 -4.1801949 -4.1683517 -4.1741743 -4.2046909 -4.2444639 -4.2583661 -4.2564678 -4.2487965 -4.2511992 -4.2648177][-4.3282304 -4.3245087 -4.3168063 -4.2955613 -4.2727509 -4.2611747 -4.2575316 -4.2567134 -4.2623596 -4.2666297 -4.2492318 -4.2234654 -4.2053175 -4.2086296 -4.2317863][-4.3264623 -4.3284311 -4.3301353 -4.3244667 -4.3181238 -4.3163147 -4.3135638 -4.3038168 -4.2865548 -4.2560291 -4.2023711 -4.1487269 -4.123333 -4.1398277 -4.1832075][-4.3284111 -4.3329492 -4.3402939 -4.3433723 -4.343679 -4.342443 -4.3359952 -4.316411 -4.278348 -4.2097774 -4.1134405 -4.0280495 -4.0042453 -4.052474 -4.128624][-4.3305306 -4.3370185 -4.3467331 -4.3531404 -4.3541489 -4.3508968 -4.338994 -4.3078895 -4.2470517 -4.1445251 -4.0164051 -3.9147279 -3.912946 -4.0052023 -4.1165066]]...]
INFO - root - 2017-12-08 01:20:56.489274: step 76410, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.689 sec/batch; 49h:01m:34s remains)
INFO - root - 2017-12-08 01:21:03.221817: step 76420, loss = 2.07, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 46h:20m:41s remains)
INFO - root - 2017-12-08 01:21:10.176786: step 76430, loss = 2.07, batch loss = 2.02 (11.8 examples/sec; 0.680 sec/batch; 48h:21m:25s remains)
INFO - root - 2017-12-08 01:21:17.133262: step 76440, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 50h:33m:39s remains)
INFO - root - 2017-12-08 01:21:23.948944: step 76450, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 51h:35m:36s remains)
INFO - root - 2017-12-08 01:21:30.801726: step 76460, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 47h:54m:02s remains)
INFO - root - 2017-12-08 01:21:37.531058: step 76470, loss = 2.08, batch loss = 2.02 (12.8 examples/sec; 0.623 sec/batch; 44h:19m:07s remains)
INFO - root - 2017-12-08 01:21:44.286223: step 76480, loss = 2.10, batch loss = 2.04 (11.7 examples/sec; 0.681 sec/batch; 48h:25m:41s remains)
INFO - root - 2017-12-08 01:21:51.063842: step 76490, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.713 sec/batch; 50h:44m:22s remains)
INFO - root - 2017-12-08 01:21:57.911951: step 76500, loss = 2.10, batch loss = 2.04 (11.5 examples/sec; 0.699 sec/batch; 49h:41m:00s remains)
2017-12-08 01:21:58.649231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1984363 -4.20243 -4.2075334 -4.2111 -4.2104645 -4.2185192 -4.2280774 -4.221302 -4.2022891 -4.1932125 -4.1842084 -4.1778259 -4.17177 -4.17008 -4.1853495][-4.1732807 -4.1730986 -4.1782379 -4.1889935 -4.1943502 -4.2002788 -4.2039723 -4.1923351 -4.1790323 -4.1747932 -4.1737795 -4.1785183 -4.177897 -4.1790538 -4.1933904][-4.1466694 -4.1434112 -4.1523919 -4.169919 -4.1806564 -4.1822109 -4.1796274 -4.1699548 -4.1655984 -4.1679544 -4.1751461 -4.1871758 -4.1885285 -4.1875739 -4.2004752][-4.1390424 -4.1358404 -4.1431985 -4.1626906 -4.1762362 -4.176652 -4.1726117 -4.1666775 -4.1661878 -4.1707659 -4.1810765 -4.1970072 -4.197763 -4.1935706 -4.203968][-4.1273108 -4.1297359 -4.1441212 -4.16623 -4.1786838 -4.1757512 -4.1701927 -4.1647849 -4.1646066 -4.1701212 -4.181015 -4.1953216 -4.1917658 -4.1855989 -4.1927834][-4.1255846 -4.12862 -4.1468778 -4.1675677 -4.1708035 -4.1576695 -4.14207 -4.1265206 -4.122406 -4.1290021 -4.141273 -4.1548729 -4.1526084 -4.1495991 -4.1609573][-4.1213331 -4.1133394 -4.1205297 -4.1337209 -4.1298366 -4.1095695 -4.0828547 -4.0516429 -4.0434151 -4.0601473 -4.0829921 -4.1038828 -4.1095695 -4.1129475 -4.1309719][-4.0941982 -4.0764532 -4.0682678 -4.0671458 -4.0541992 -4.0310717 -3.9976947 -3.9529777 -3.9492185 -3.9916775 -4.0321054 -4.0602407 -4.0706258 -4.0794835 -4.1046591][-4.0768585 -4.0586796 -4.0468178 -4.0382204 -4.0174475 -3.9950197 -3.9717917 -3.9399762 -3.9496779 -3.9966648 -4.0324411 -4.0543518 -4.0615897 -4.0731668 -4.1035738][-4.0958223 -4.0843573 -4.0782461 -4.073431 -4.05986 -4.0481076 -4.0396943 -4.0254736 -4.035655 -4.0629578 -4.0737729 -4.0789213 -4.0809636 -4.0917578 -4.1180634][-4.1579542 -4.1489258 -4.1448479 -4.14081 -4.1331077 -4.1265182 -4.1227655 -4.1181026 -4.1279173 -4.1385589 -4.1305485 -4.1172194 -4.1083784 -4.1171994 -4.1385612][-4.2237325 -4.2186913 -4.2157478 -4.2103333 -4.2033553 -4.19739 -4.1895795 -4.1832576 -4.18884 -4.193037 -4.1807618 -4.1598339 -4.1432858 -4.145103 -4.161272][-4.2682939 -4.2671413 -4.2700195 -4.2711196 -4.2673187 -4.2610564 -4.2504859 -4.2450294 -4.2487407 -4.2499561 -4.2365746 -4.216186 -4.1985354 -4.1944366 -4.1992278][-4.2754288 -4.2763815 -4.2827578 -4.2885122 -4.288301 -4.2842078 -4.2781391 -4.2756233 -4.2762775 -4.2736111 -4.26221 -4.2473006 -4.2336512 -4.2293768 -4.2282047][-4.2728043 -4.2706785 -4.2736459 -4.27785 -4.2790823 -4.2788024 -4.2789779 -4.2801137 -4.2793927 -4.2736173 -4.2632141 -4.252883 -4.2435474 -4.240284 -4.2412481]]...]
INFO - root - 2017-12-08 01:22:05.376344: step 76510, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.697 sec/batch; 49h:33m:52s remains)
INFO - root - 2017-12-08 01:22:12.192478: step 76520, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 51h:21m:14s remains)
INFO - root - 2017-12-08 01:22:19.043283: step 76530, loss = 2.03, batch loss = 1.97 (11.5 examples/sec; 0.696 sec/batch; 49h:30m:29s remains)
INFO - root - 2017-12-08 01:22:25.663317: step 76540, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:34m:07s remains)
INFO - root - 2017-12-08 01:22:32.487750: step 76550, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.711 sec/batch; 50h:35m:03s remains)
INFO - root - 2017-12-08 01:22:39.447690: step 76560, loss = 2.11, batch loss = 2.05 (11.1 examples/sec; 0.718 sec/batch; 51h:01m:25s remains)
INFO - root - 2017-12-08 01:22:46.283209: step 76570, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.687 sec/batch; 48h:48m:29s remains)
INFO - root - 2017-12-08 01:22:53.101853: step 76580, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 47h:09m:25s remains)
INFO - root - 2017-12-08 01:22:59.847392: step 76590, loss = 2.11, batch loss = 2.06 (12.1 examples/sec; 0.661 sec/batch; 46h:58m:13s remains)
INFO - root - 2017-12-08 01:23:06.650936: step 76600, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 50h:25m:41s remains)
2017-12-08 01:23:07.423445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438736 -4.256412 -4.2751923 -4.2797251 -4.2635674 -4.2395406 -4.2002373 -4.1729736 -4.1838861 -4.2152696 -4.2345486 -4.23679 -4.2402039 -4.2455425 -4.25902][-4.212503 -4.2319736 -4.2554269 -4.2620926 -4.2448273 -4.2199855 -4.1872945 -4.1705427 -4.1908441 -4.223362 -4.2386703 -4.2357154 -4.2350569 -4.2385187 -4.2532997][-4.1759644 -4.2086859 -4.2424531 -4.2558022 -4.2435265 -4.224597 -4.2003012 -4.1886125 -4.2056956 -4.2320366 -4.2413325 -4.2372923 -4.2369547 -4.2398591 -4.253612][-4.1146145 -4.1776872 -4.2320414 -4.2563362 -4.2570987 -4.2500811 -4.2325597 -4.2203321 -4.2285151 -4.2442679 -4.2494521 -4.246696 -4.2465496 -4.2493992 -4.2602715][-4.0542631 -4.1541815 -4.2300792 -4.2616563 -4.26749 -4.2652154 -4.25003 -4.2304831 -4.228941 -4.2423549 -4.2521229 -4.2552838 -4.2551079 -4.2590275 -4.268815][-4.0451236 -4.1628995 -4.240406 -4.2698035 -4.2755442 -4.2694368 -4.2445507 -4.2107391 -4.19782 -4.2130446 -4.2318506 -4.2437558 -4.2509584 -4.2600265 -4.2729397][-4.1089244 -4.2066097 -4.2597961 -4.2702312 -4.263011 -4.2433858 -4.2045655 -4.1510272 -4.1258712 -4.1483245 -4.1845913 -4.2142844 -4.2345443 -4.2538691 -4.27394][-4.1578751 -4.2300515 -4.2590332 -4.2502046 -4.2252069 -4.1910892 -4.1342816 -4.0590043 -4.0273151 -4.0639944 -4.1245565 -4.1788235 -4.2179413 -4.2501154 -4.2756543][-4.1756315 -4.2306986 -4.2480273 -4.2257261 -4.1877379 -4.1421146 -4.0634217 -3.9698327 -3.9421589 -3.9980457 -4.0801415 -4.1538644 -4.2077947 -4.2477131 -4.2744241][-4.1790857 -4.2236991 -4.2356663 -4.2136459 -4.1747603 -4.121944 -4.0353484 -3.9412355 -3.9239707 -3.9914901 -4.0764623 -4.150444 -4.2040172 -4.2440066 -4.270195][-4.195118 -4.2300072 -4.2395287 -4.2235003 -4.1892776 -4.1389422 -4.06583 -3.9935219 -3.9833384 -4.0378208 -4.1045942 -4.1656818 -4.2129059 -4.2484236 -4.2698693][-4.22761 -4.2536268 -4.262876 -4.2511959 -4.2196531 -4.1803012 -4.1261458 -4.0717597 -4.061975 -4.0993671 -4.1455932 -4.1910028 -4.2298026 -4.2605314 -4.2759609][-4.2563396 -4.2715521 -4.2778792 -4.2672038 -4.2412286 -4.2130346 -4.1731009 -4.1314454 -4.12376 -4.1526971 -4.1865849 -4.218502 -4.2494669 -4.2748175 -4.2860165][-4.2763252 -4.2813096 -4.2813787 -4.2712178 -4.2551694 -4.2372484 -4.2101326 -4.1811938 -4.1778259 -4.2018776 -4.2239132 -4.2448254 -4.2692914 -4.2888632 -4.2970676][-4.2771668 -4.2752738 -4.2715168 -4.2686958 -4.2677822 -4.2643018 -4.2504225 -4.2312365 -4.2297549 -4.2465763 -4.2574883 -4.2686958 -4.28505 -4.29907 -4.3059506]]...]
INFO - root - 2017-12-08 01:23:13.996018: step 76610, loss = 2.08, batch loss = 2.03 (12.4 examples/sec; 0.647 sec/batch; 45h:57m:34s remains)
INFO - root - 2017-12-08 01:23:20.915662: step 76620, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.694 sec/batch; 49h:21m:42s remains)
INFO - root - 2017-12-08 01:23:27.850903: step 76630, loss = 2.07, batch loss = 2.01 (10.9 examples/sec; 0.734 sec/batch; 52h:09m:29s remains)
INFO - root - 2017-12-08 01:23:34.637676: step 76640, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 50h:01m:01s remains)
INFO - root - 2017-12-08 01:23:41.387389: step 76650, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 44h:36m:26s remains)
INFO - root - 2017-12-08 01:23:48.184824: step 76660, loss = 2.04, batch loss = 1.98 (12.5 examples/sec; 0.640 sec/batch; 45h:27m:07s remains)
INFO - root - 2017-12-08 01:23:55.064768: step 76670, loss = 2.09, batch loss = 2.04 (11.5 examples/sec; 0.694 sec/batch; 49h:19m:57s remains)
INFO - root - 2017-12-08 01:24:01.841250: step 76680, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.717 sec/batch; 50h:57m:25s remains)
INFO - root - 2017-12-08 01:24:08.634339: step 76690, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 48h:34m:30s remains)
INFO - root - 2017-12-08 01:24:15.291318: step 76700, loss = 2.05, batch loss = 2.00 (12.8 examples/sec; 0.627 sec/batch; 44h:31m:08s remains)
2017-12-08 01:24:16.013087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34021 -4.3332896 -4.3288617 -4.3265467 -4.3255415 -4.3262773 -4.3292871 -4.3322635 -4.3326278 -4.331871 -4.3329988 -4.335403 -4.3391037 -4.3458514 -4.3548517][-4.32651 -4.3156447 -4.3077674 -4.3010778 -4.294456 -4.2891455 -4.2884111 -4.2918005 -4.2951608 -4.298264 -4.3045874 -4.3096862 -4.315022 -4.325676 -4.3414159][-4.3097429 -4.2946591 -4.2821603 -4.2681513 -4.2518859 -4.2371874 -4.2279992 -4.2288036 -4.2377615 -4.2493434 -4.2642827 -4.2750053 -4.2830138 -4.298708 -4.3221807][-4.287971 -4.268878 -4.2521996 -4.2324886 -4.20669 -4.180398 -4.1591859 -4.1550851 -4.1696987 -4.1932197 -4.2207003 -4.2392063 -4.2510905 -4.2708735 -4.2994595][-4.2689586 -4.2496872 -4.2291527 -4.2067122 -4.1725173 -4.1326771 -4.0938382 -4.0810995 -4.1029525 -4.1422062 -4.1799684 -4.2039509 -4.2197323 -4.2416139 -4.2723842][-4.2518873 -4.232399 -4.2092342 -4.185792 -4.1473184 -4.0941143 -4.0342956 -4.0009489 -4.0228052 -4.0753546 -4.1229825 -4.1523337 -4.1720929 -4.2016311 -4.2380571][-4.2306042 -4.2046194 -4.1706691 -4.1406412 -4.0983453 -4.034832 -3.9528854 -3.889 -3.9066045 -3.9793901 -4.0441461 -4.0867372 -4.1196833 -4.1621084 -4.2072845][-4.2085705 -4.1756835 -4.1318502 -4.0919075 -4.043859 -3.9721074 -3.8680048 -3.7710459 -3.7778912 -3.8715594 -3.9635935 -4.029695 -4.0795374 -4.1330543 -4.1855478][-4.2048988 -4.1724658 -4.1319976 -4.0894122 -4.0404305 -3.9721944 -3.8738477 -3.7772174 -3.7699194 -3.8553019 -3.9517989 -4.0239429 -4.0759945 -4.1284447 -4.1815205][-4.2341757 -4.2128096 -4.1856327 -4.1533718 -4.1144495 -4.0621943 -3.9933829 -3.9290128 -3.9166534 -3.9659591 -4.0345521 -4.0883908 -4.125412 -4.1624804 -4.204886][-4.2616258 -4.2497768 -4.2365766 -4.2190733 -4.1925173 -4.1549706 -4.1101751 -4.0727124 -4.063868 -4.0904851 -4.1335769 -4.1688981 -4.1933637 -4.2175279 -4.2459059][-4.2900081 -4.285398 -4.2837772 -4.2782388 -4.263555 -4.2378654 -4.2094398 -4.1902251 -4.1866736 -4.1999617 -4.2258954 -4.2473216 -4.2622685 -4.2766256 -4.2928109][-4.3066807 -4.3027153 -4.3065753 -4.3107095 -4.3077874 -4.2950487 -4.2810416 -4.2713757 -4.2708082 -4.2794576 -4.2950716 -4.3066559 -4.31419 -4.322279 -4.32945][-4.3212824 -4.3157063 -4.3187537 -4.3245487 -4.3264084 -4.3235831 -4.3188972 -4.3133965 -4.3122158 -4.318809 -4.3291249 -4.3353033 -4.3388171 -4.3442721 -4.3482943][-4.3370132 -4.3301339 -4.3302851 -4.3342404 -4.3373528 -4.337883 -4.3364859 -4.3340783 -4.3327661 -4.3364372 -4.3425312 -4.3459921 -4.3485966 -4.3521037 -4.3549752]]...]
INFO - root - 2017-12-08 01:24:22.661415: step 76710, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.690 sec/batch; 49h:02m:06s remains)
INFO - root - 2017-12-08 01:24:29.436401: step 76720, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 49h:24m:42s remains)
INFO - root - 2017-12-08 01:24:36.213167: step 76730, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 45h:41m:17s remains)
INFO - root - 2017-12-08 01:24:43.027766: step 76740, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 47h:05m:42s remains)
INFO - root - 2017-12-08 01:24:49.788703: step 76750, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 48h:11m:42s remains)
INFO - root - 2017-12-08 01:24:56.573902: step 76760, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.691 sec/batch; 49h:06m:52s remains)
INFO - root - 2017-12-08 01:25:03.307040: step 76770, loss = 2.09, batch loss = 2.03 (11.7 examples/sec; 0.684 sec/batch; 48h:36m:12s remains)
INFO - root - 2017-12-08 01:25:10.012103: step 76780, loss = 2.08, batch loss = 2.03 (12.2 examples/sec; 0.654 sec/batch; 46h:28m:31s remains)
INFO - root - 2017-12-08 01:25:16.801744: step 76790, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.647 sec/batch; 45h:56m:21s remains)
INFO - root - 2017-12-08 01:25:23.626232: step 76800, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.713 sec/batch; 50h:37m:14s remains)
2017-12-08 01:25:24.373200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.351707 -4.351336 -4.3342094 -4.29664 -4.2546821 -4.2231817 -4.2096696 -4.2158017 -4.2190447 -4.2306075 -4.2342739 -4.2198734 -4.1846 -4.1299605 -4.0781207][-4.3578129 -4.3608427 -4.3450351 -4.3022647 -4.2478261 -4.205997 -4.190742 -4.2053452 -4.2200704 -4.2399831 -4.2420731 -4.22072 -4.1764126 -4.1168165 -4.0662122][-4.3616238 -4.3682122 -4.3546438 -4.3079982 -4.2394509 -4.183969 -4.1645808 -4.1868258 -4.2126851 -4.2412968 -4.246716 -4.2283053 -4.1871147 -4.1303854 -4.081202][-4.3637171 -4.3722053 -4.3600321 -4.3105745 -4.2317214 -4.1624718 -4.1319122 -4.1551752 -4.1932583 -4.2306795 -4.2437944 -4.2358303 -4.2079415 -4.164824 -4.1220222][-4.3630409 -4.3707824 -4.3589253 -4.3079495 -4.2226825 -4.1382 -4.0848475 -4.0937314 -4.1381583 -4.1875987 -4.2174172 -4.2289677 -4.2210922 -4.1974649 -4.1673937][-4.359272 -4.3650241 -4.3520312 -4.2981176 -4.2043285 -4.0991273 -4.0172729 -4.0065589 -4.0543351 -4.1207771 -4.1752148 -4.2138038 -4.2283807 -4.2206817 -4.1987066][-4.3537326 -4.3564992 -4.3393583 -4.2803326 -4.1796432 -4.0593371 -3.9557166 -3.9294605 -3.9807229 -4.0628767 -4.1377597 -4.2009673 -4.2372046 -4.2442803 -4.2292976][-4.3472733 -4.345973 -4.3236022 -4.2606211 -4.1572485 -4.0328445 -3.9210844 -3.8848076 -3.9352574 -4.0285964 -4.1200032 -4.2005057 -4.25165 -4.2702875 -4.2624664][-4.3414679 -4.3361735 -4.3106718 -4.2478719 -4.1482286 -4.0292711 -3.9209857 -3.8809001 -3.9266632 -4.0210156 -4.1191025 -4.2040386 -4.2614727 -4.2885342 -4.2898331][-4.3357124 -4.3270583 -4.3021436 -4.2453909 -4.158258 -4.0560994 -3.962579 -3.9258895 -3.9640303 -4.0483942 -4.1377411 -4.214355 -4.2665653 -4.2950344 -4.3014665][-4.3306065 -4.3195524 -4.2966003 -4.2491417 -4.1768703 -4.0931787 -4.0165472 -3.9878533 -4.0192852 -4.089313 -4.1628656 -4.225862 -4.2682476 -4.2928944 -4.3009562][-4.3268504 -4.3150353 -4.2954488 -4.2566314 -4.1971068 -4.1308017 -4.0723343 -4.0526328 -4.0800452 -4.1367 -4.1960335 -4.2474709 -4.28219 -4.3026476 -4.3103023][-4.3263984 -4.3156142 -4.3004479 -4.272306 -4.2277956 -4.1787453 -4.1367736 -4.1238437 -4.1462693 -4.1900153 -4.2347889 -4.2733574 -4.29972 -4.3156648 -4.3225241][-4.3294969 -4.3202419 -4.3092785 -4.2922487 -4.26373 -4.2314806 -4.2048559 -4.1984963 -4.2146225 -4.244154 -4.2729764 -4.2970014 -4.313971 -4.3237648 -4.3277192][-4.3346519 -4.3268275 -4.3191142 -4.3101373 -4.2952166 -4.2777247 -4.2636576 -4.2616777 -4.2716541 -4.2881851 -4.3031044 -4.314352 -4.3225255 -4.326725 -4.3278432]]...]
INFO - root - 2017-12-08 01:25:30.817157: step 76810, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 45h:09m:31s remains)
INFO - root - 2017-12-08 01:25:37.672921: step 76820, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 47h:00m:52s remains)
INFO - root - 2017-12-08 01:25:44.564958: step 76830, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.745 sec/batch; 52h:53m:15s remains)
INFO - root - 2017-12-08 01:25:51.431815: step 76840, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.718 sec/batch; 50h:58m:15s remains)
INFO - root - 2017-12-08 01:25:58.003847: step 76850, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.677 sec/batch; 48h:02m:45s remains)
INFO - root - 2017-12-08 01:26:04.735571: step 76860, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.638 sec/batch; 45h:19m:33s remains)
INFO - root - 2017-12-08 01:26:11.606571: step 76870, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.673 sec/batch; 47h:45m:53s remains)
INFO - root - 2017-12-08 01:26:18.469716: step 76880, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.737 sec/batch; 52h:20m:11s remains)
INFO - root - 2017-12-08 01:26:25.288069: step 76890, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.672 sec/batch; 47h:43m:57s remains)
INFO - root - 2017-12-08 01:26:32.051027: step 76900, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.665 sec/batch; 47h:11m:52s remains)
2017-12-08 01:26:32.771775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884059 -4.2963338 -4.3074164 -4.3179913 -4.3217487 -4.319262 -4.31348 -4.3065171 -4.298471 -4.2914348 -4.2875829 -4.2881589 -4.2881246 -4.2857018 -4.2850847][-4.2491431 -4.2673469 -4.2895188 -4.309659 -4.3188124 -4.3175726 -4.3100929 -4.3005939 -4.2880011 -4.2761831 -4.2691493 -4.2675061 -4.2633934 -4.2564683 -4.254271][-4.2059917 -4.2417927 -4.2763085 -4.3037415 -4.3178134 -4.3191662 -4.3108292 -4.2990375 -4.2833476 -4.2681427 -4.2584643 -4.2518396 -4.2389064 -4.2230172 -4.2172546][-4.1630983 -4.212543 -4.255404 -4.2865534 -4.3030543 -4.30553 -4.2989416 -4.2859716 -4.2706695 -4.2563176 -4.2468777 -4.2363253 -4.214787 -4.18938 -4.181335][-4.1497707 -4.1962447 -4.236331 -4.2648139 -4.2795639 -4.2772245 -4.2656078 -4.2489333 -4.2379503 -4.2305756 -4.2267809 -4.217093 -4.1918683 -4.16278 -4.155858][-4.181282 -4.2091079 -4.2331538 -4.2451386 -4.2408957 -4.2177749 -4.193439 -4.1738858 -4.1738672 -4.1866293 -4.1993165 -4.1974349 -4.1743526 -4.1469421 -4.1424532][-4.2196507 -4.2277546 -4.2323079 -4.2189212 -4.1787882 -4.1194186 -4.0653062 -4.0279794 -4.047307 -4.0989971 -4.145483 -4.1608319 -4.1449914 -4.1258769 -4.129055][-4.2240906 -4.2206688 -4.213522 -4.1795573 -4.1008224 -3.9885232 -3.8803167 -3.8102305 -3.8612823 -3.9701138 -4.0628033 -4.1039882 -4.102201 -4.0989728 -4.1154752][-4.1937442 -4.1926351 -4.1943069 -4.1618123 -4.0671043 -3.9225245 -3.7579441 -3.6447215 -3.7240915 -3.8788989 -4.0056934 -4.0642576 -4.0727615 -4.0811415 -4.1097736][-4.1525741 -4.1648455 -4.1871495 -4.1776657 -4.1115966 -3.9988437 -3.8626566 -3.7666349 -3.8086281 -3.9230163 -4.0312219 -4.07978 -4.0801191 -4.0887074 -4.121336][-4.0862594 -4.1159606 -4.1624217 -4.1853456 -4.1612906 -4.1046515 -4.0326715 -3.9787335 -3.9930072 -4.0493116 -4.1118016 -4.1312337 -4.1102738 -4.1079063 -4.1362133][-4.0409765 -4.07355 -4.12829 -4.1687384 -4.1750569 -4.159173 -4.13266 -4.110321 -4.1228709 -4.1582165 -4.1945152 -4.1928005 -4.1566043 -4.1385818 -4.1543231][-4.0696049 -4.0881538 -4.129817 -4.1653962 -4.1827106 -4.1901064 -4.1889167 -4.1828189 -4.1967082 -4.2263207 -4.2473598 -4.2381115 -4.2015586 -4.176497 -4.1792531][-4.1232958 -4.1316123 -4.1534605 -4.1739597 -4.1934652 -4.2090993 -4.2200961 -4.2219906 -4.2351537 -4.2581716 -4.2685313 -4.2555189 -4.2238083 -4.2000079 -4.1970196][-4.1521173 -4.1600442 -4.1739187 -4.1810884 -4.1941423 -4.2127934 -4.2278042 -4.2334833 -4.2452326 -4.258903 -4.2569609 -4.2393994 -4.2166591 -4.2041588 -4.2043242]]...]
INFO - root - 2017-12-08 01:26:39.480178: step 76910, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 50h:45m:09s remains)
INFO - root - 2017-12-08 01:26:46.289069: step 76920, loss = 2.09, batch loss = 2.03 (12.1 examples/sec; 0.659 sec/batch; 46h:47m:58s remains)
INFO - root - 2017-12-08 01:26:53.024517: step 76930, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 45h:53m:56s remains)
INFO - root - 2017-12-08 01:26:59.802557: step 76940, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.652 sec/batch; 46h:17m:39s remains)
INFO - root - 2017-12-08 01:27:06.650075: step 76950, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.711 sec/batch; 50h:27m:01s remains)
INFO - root - 2017-12-08 01:27:13.554410: step 76960, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.715 sec/batch; 50h:44m:23s remains)
INFO - root - 2017-12-08 01:27:20.448446: step 76970, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.712 sec/batch; 50h:33m:14s remains)
INFO - root - 2017-12-08 01:27:27.205983: step 76980, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.671 sec/batch; 47h:36m:19s remains)
INFO - root - 2017-12-08 01:27:33.964127: step 76990, loss = 2.08, batch loss = 2.02 (12.4 examples/sec; 0.645 sec/batch; 45h:48m:40s remains)
INFO - root - 2017-12-08 01:27:40.834368: step 77000, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.704 sec/batch; 49h:56m:01s remains)
2017-12-08 01:27:41.568276: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3272114 -4.3286481 -4.3310094 -4.3334904 -4.3347235 -4.3358879 -4.3368769 -4.3368597 -4.3383379 -4.3408856 -4.3424435 -4.3428779 -4.3435254 -4.3432183 -4.3401022][-4.3131933 -4.3153253 -4.3197641 -4.3241735 -4.3249507 -4.3249311 -4.3231359 -4.3190222 -4.3169613 -4.3187723 -4.323391 -4.3280959 -4.3316245 -4.3326421 -4.3292127][-4.3032546 -4.3041654 -4.3074045 -4.3108292 -4.3110876 -4.3095784 -4.3054152 -4.29738 -4.2906513 -4.2893105 -4.2975554 -4.3076277 -4.3143177 -4.3162441 -4.3135428][-4.2809963 -4.2783966 -4.2767925 -4.2732377 -4.2707677 -4.2654872 -4.2594042 -4.2518272 -4.2423906 -4.2393851 -4.2477765 -4.2629194 -4.2766924 -4.2835684 -4.2839031][-4.2487373 -4.2415848 -4.234261 -4.2227631 -4.2119818 -4.1935563 -4.1806321 -4.1757207 -4.1676068 -4.1621351 -4.1725698 -4.1979742 -4.2242308 -4.243999 -4.2532053][-4.2035646 -4.1882377 -4.1736422 -4.1556568 -4.1279278 -4.0884476 -4.0671496 -4.0647326 -4.0540156 -4.0419459 -4.0573635 -4.1053934 -4.1605535 -4.2002215 -4.2256017][-4.151125 -4.1176863 -4.0898924 -4.0632443 -4.0159883 -3.9574223 -3.9266133 -3.9286637 -3.9197116 -3.9052429 -3.9380004 -4.0219693 -4.1138859 -4.1759658 -4.2151337][-4.1226559 -4.0681372 -4.0211368 -3.9819376 -3.9291544 -3.8789062 -3.8558097 -3.86411 -3.8666129 -3.8685422 -3.9139888 -4.0113292 -4.1168404 -4.1878014 -4.2326326][-4.1515555 -4.0871792 -4.0289378 -3.9820933 -3.9387472 -3.9183295 -3.914119 -3.9253109 -3.9451985 -3.9733305 -4.0201421 -4.0947232 -4.1776233 -4.2370272 -4.27477][-4.2249551 -4.1729007 -4.1232786 -4.0876508 -4.0634866 -4.0605545 -4.061841 -4.0659127 -4.0878754 -4.1249003 -4.165956 -4.210701 -4.25984 -4.2971249 -4.3208055][-4.2958474 -4.2658968 -4.2350616 -4.2150321 -4.2054796 -4.2050266 -4.2076349 -4.2083726 -4.2234454 -4.252069 -4.2782331 -4.2999649 -4.3223996 -4.3406038 -4.3519912][-4.3390031 -4.3253069 -4.3093295 -4.299325 -4.2955446 -4.2945447 -4.2962928 -4.2967329 -4.3031049 -4.3171711 -4.3309464 -4.341548 -4.3513684 -4.3597746 -4.3639383][-4.3596644 -4.3556786 -4.348382 -4.3419847 -4.3387485 -4.335619 -4.33313 -4.3339534 -4.3374915 -4.3425341 -4.3479514 -4.3525825 -4.3570356 -4.3605251 -4.3614111][-4.3623738 -4.3627586 -4.3609262 -4.3576069 -4.3534431 -4.3499341 -4.3473945 -4.3480458 -4.3499508 -4.3512778 -4.3530288 -4.3549542 -4.3559995 -4.3560553 -4.3543653][-4.3559856 -4.358139 -4.3579431 -4.3559818 -4.3524251 -4.3494496 -4.348352 -4.3493047 -4.3500824 -4.3501034 -4.3499484 -4.3506927 -4.351006 -4.3500218 -4.3482342]]...]
INFO - root - 2017-12-08 01:27:48.187287: step 77010, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 46h:29m:15s remains)
INFO - root - 2017-12-08 01:27:55.046968: step 77020, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.700 sec/batch; 49h:39m:45s remains)
INFO - root - 2017-12-08 01:28:01.862249: step 77030, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.681 sec/batch; 48h:18m:56s remains)
INFO - root - 2017-12-08 01:28:08.668059: step 77040, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.720 sec/batch; 51h:06m:51s remains)
INFO - root - 2017-12-08 01:28:15.472614: step 77050, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.658 sec/batch; 46h:41m:51s remains)
INFO - root - 2017-12-08 01:28:22.351578: step 77060, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 46h:12m:26s remains)
INFO - root - 2017-12-08 01:28:29.183998: step 77070, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 50h:37m:38s remains)
INFO - root - 2017-12-08 01:28:36.060168: step 77080, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.696 sec/batch; 49h:24m:30s remains)
INFO - root - 2017-12-08 01:28:42.925885: step 77090, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 48h:43m:24s remains)
INFO - root - 2017-12-08 01:28:49.754192: step 77100, loss = 2.06, batch loss = 2.01 (12.5 examples/sec; 0.639 sec/batch; 45h:19m:37s remains)
2017-12-08 01:28:50.472284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1898804 -4.1895609 -4.2043529 -4.2234168 -4.2479267 -4.2618361 -4.2631245 -4.2541685 -4.2281032 -4.2008982 -4.1897798 -4.1880445 -4.1964869 -4.2208018 -4.2588625][-4.1813507 -4.1782451 -4.1910596 -4.2090034 -4.2295375 -4.2385545 -4.2371941 -4.2280827 -4.2037363 -4.1791191 -4.1719651 -4.1730714 -4.1832471 -4.2084918 -4.2493114][-4.1739831 -4.16691 -4.1721015 -4.1863995 -4.204649 -4.2082729 -4.1995311 -4.1856103 -4.1654549 -4.148695 -4.1487284 -4.1574035 -4.173389 -4.2002587 -4.2430964][-4.1663203 -4.1533413 -4.1542358 -4.1684957 -4.1894627 -4.1900287 -4.1714077 -4.147861 -4.12767 -4.1176038 -4.1249819 -4.1425171 -4.1672187 -4.1978612 -4.2419953][-4.1628785 -4.1411476 -4.1353488 -4.1467409 -4.1683064 -4.1634989 -4.1330695 -4.0955591 -4.0680141 -4.0619049 -4.0814457 -4.1143765 -4.1506 -4.1889219 -4.2375631][-4.1086712 -4.0830879 -4.0720563 -4.078867 -4.0956292 -4.0864067 -4.0441189 -3.9897819 -3.9512835 -3.9479322 -3.9875603 -4.0454397 -4.1053176 -4.1632118 -4.2230492][-4.0472617 -4.0157814 -3.9972785 -3.9923692 -3.9944642 -3.9678187 -3.9087741 -3.8364377 -3.7829902 -3.7835703 -3.8522151 -3.9456127 -4.0412517 -4.126308 -4.2021723][-4.0676179 -4.0417786 -4.0201492 -4.0027204 -3.9820673 -3.9340858 -3.8609357 -3.7807441 -3.7217336 -3.7197719 -3.7976198 -3.9080789 -4.020709 -4.1168952 -4.1970019][-4.1351018 -4.122211 -4.1056976 -4.0860209 -4.0632634 -4.0213118 -3.963814 -3.9055231 -3.8619869 -3.8548074 -3.9057059 -3.9867435 -4.0758653 -4.151876 -4.2168088][-4.1981263 -4.1936531 -4.1852493 -4.1719794 -4.1582184 -4.1301603 -4.088274 -4.0444694 -4.0116067 -4.0020566 -4.0295534 -4.0783911 -4.1351495 -4.1881356 -4.2376709][-4.2595091 -4.2559772 -4.2493219 -4.23754 -4.2277589 -4.2074881 -4.1744552 -4.1372995 -4.1095705 -4.1041317 -4.1233726 -4.1535144 -4.18815 -4.2228889 -4.258739][-4.3081522 -4.3073473 -4.30308 -4.2952881 -4.2907286 -4.2802005 -4.2591362 -4.23092 -4.205946 -4.1983685 -4.2077417 -4.2242131 -4.2422857 -4.2608466 -4.2835217][-4.3136725 -4.3145661 -4.3135548 -4.3120818 -4.3148232 -4.312098 -4.3027139 -4.2828441 -4.2611618 -4.2513423 -4.2548537 -4.266191 -4.2767434 -4.2877374 -4.3028712][-4.3080626 -4.30934 -4.3100324 -4.3100629 -4.31216 -4.3087115 -4.3005271 -4.2856131 -4.2711558 -4.2669506 -4.274539 -4.2876859 -4.2972393 -4.3060136 -4.3163748][-4.3102465 -4.3092437 -4.3085046 -4.3068228 -4.30497 -4.2985225 -4.2893062 -4.2797413 -4.2738256 -4.2750778 -4.2862449 -4.3014369 -4.3109164 -4.3188634 -4.3266315]]...]
INFO - root - 2017-12-08 01:28:57.098763: step 77110, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.678 sec/batch; 48h:07m:30s remains)
INFO - root - 2017-12-08 01:29:03.871380: step 77120, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.643 sec/batch; 45h:38m:25s remains)
INFO - root - 2017-12-08 01:29:10.661080: step 77130, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 47h:30m:13s remains)
INFO - root - 2017-12-08 01:29:17.413868: step 77140, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.704 sec/batch; 49h:57m:56s remains)
INFO - root - 2017-12-08 01:29:24.180372: step 77150, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.715 sec/batch; 50h:44m:15s remains)
INFO - root - 2017-12-08 01:29:30.757193: step 77160, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.643 sec/batch; 45h:35m:51s remains)
INFO - root - 2017-12-08 01:29:37.579434: step 77170, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:28m:39s remains)
INFO - root - 2017-12-08 01:29:44.430341: step 77180, loss = 2.08, batch loss = 2.02 (11.1 examples/sec; 0.723 sec/batch; 51h:15m:19s remains)
INFO - root - 2017-12-08 01:29:51.215887: step 77190, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 51h:24m:32s remains)
INFO - root - 2017-12-08 01:29:58.052573: step 77200, loss = 2.06, batch loss = 2.00 (12.1 examples/sec; 0.662 sec/batch; 46h:56m:10s remains)
2017-12-08 01:29:58.755507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1983352 -4.1530523 -4.1423893 -4.1598191 -4.1731014 -4.1699209 -4.1588745 -4.1469345 -4.1621041 -4.1933508 -4.2280426 -4.2521276 -4.2559505 -4.2454996 -4.2181664][-4.20494 -4.1536989 -4.13288 -4.1400838 -4.1516805 -4.1500506 -4.1335764 -4.1211472 -4.1390839 -4.1703677 -4.2032719 -4.2263231 -4.2327094 -4.2204938 -4.1915116][-4.2095051 -4.1626682 -4.1353354 -4.1358285 -4.1439023 -4.1410694 -4.1230164 -4.1130505 -4.1296582 -4.1594977 -4.1899323 -4.2067289 -4.206409 -4.1850963 -4.1522217][-4.1881166 -4.1465921 -4.1204491 -4.1254988 -4.13027 -4.1267538 -4.1138525 -4.1074939 -4.1213703 -4.1496105 -4.1773729 -4.1827879 -4.1633482 -4.129406 -4.0960865][-4.1634688 -4.1179132 -4.0936437 -4.109025 -4.1113906 -4.09893 -4.0840344 -4.0838952 -4.1043315 -4.1359367 -4.16113 -4.1574526 -4.1225281 -4.0786018 -4.0485058][-4.144136 -4.089447 -4.0651951 -4.0815134 -4.0746307 -4.0400524 -4.0067778 -4.0184484 -4.0654445 -4.1137576 -4.1481004 -4.1491523 -4.1143012 -4.0704041 -4.0429645][-4.1402564 -4.0777864 -4.0490465 -4.0504107 -4.01805 -3.9462318 -3.8739755 -3.8962564 -3.9891973 -4.0764508 -4.138402 -4.1586456 -4.1372356 -4.0969996 -4.0612741][-4.15283 -4.0942535 -4.0574102 -4.0328984 -3.9654932 -3.8442123 -3.7117448 -3.7335243 -3.8836 -4.0189662 -4.1129475 -4.1561031 -4.1551857 -4.1261978 -4.08232][-4.1790581 -4.1310711 -4.0943089 -4.0507283 -3.9666421 -3.8431818 -3.7064095 -3.7178636 -3.8693194 -4.0101776 -4.1056328 -4.1552625 -4.1686983 -4.14989 -4.1044469][-4.2077079 -4.1742644 -4.1444731 -4.1009617 -4.0307693 -3.943846 -3.857497 -3.865541 -3.9698277 -4.0744972 -4.1444564 -4.1830716 -4.1879063 -4.1624169 -4.1197762][-4.243 -4.2191658 -4.1930671 -4.155273 -4.1036625 -4.0504904 -4.0087929 -4.0212345 -4.0846119 -4.1498375 -4.1955004 -4.2187796 -4.2113929 -4.1812873 -4.1456685][-4.281281 -4.2643571 -4.2419691 -4.2119422 -4.1781592 -4.14897 -4.1351471 -4.1519303 -4.1921282 -4.2288203 -4.2516418 -4.2594242 -4.2429314 -4.2113147 -4.1843753][-4.303772 -4.2931027 -4.2773266 -4.258235 -4.2420497 -4.23131 -4.2301278 -4.2436938 -4.2688718 -4.2898989 -4.2983503 -4.2986922 -4.2824354 -4.2566934 -4.2377081][-4.3152995 -4.3052044 -4.2913823 -4.2803993 -4.2784176 -4.280961 -4.2863545 -4.2945514 -4.3119006 -4.328042 -4.3312063 -4.3267827 -4.3127708 -4.294898 -4.2851229][-4.3073053 -4.30273 -4.2982664 -4.2954044 -4.3019838 -4.3100023 -4.3188276 -4.326149 -4.338892 -4.3525028 -4.3542061 -4.3478923 -4.3352513 -4.320652 -4.3130445]]...]
INFO - root - 2017-12-08 01:30:05.312664: step 77210, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.692 sec/batch; 49h:05m:04s remains)
INFO - root - 2017-12-08 01:30:12.085658: step 77220, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 46h:29m:17s remains)
INFO - root - 2017-12-08 01:30:19.031852: step 77230, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 45h:10m:57s remains)
INFO - root - 2017-12-08 01:30:25.954515: step 77240, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.674 sec/batch; 47h:47m:06s remains)
INFO - root - 2017-12-08 01:30:32.756234: step 77250, loss = 2.04, batch loss = 1.98 (11.6 examples/sec; 0.687 sec/batch; 48h:42m:35s remains)
INFO - root - 2017-12-08 01:30:39.629869: step 77260, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.728 sec/batch; 51h:37m:36s remains)
INFO - root - 2017-12-08 01:30:46.469377: step 77270, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 48h:38m:14s remains)
INFO - root - 2017-12-08 01:30:53.285922: step 77280, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.625 sec/batch; 44h:16m:58s remains)
INFO - root - 2017-12-08 01:31:00.115847: step 77290, loss = 2.04, batch loss = 1.99 (12.1 examples/sec; 0.664 sec/batch; 47h:02m:12s remains)
INFO - root - 2017-12-08 01:31:06.913052: step 77300, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.721 sec/batch; 51h:06m:23s remains)
2017-12-08 01:31:07.700882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.178988 -4.1523623 -4.1320386 -4.1324625 -4.14679 -4.154151 -4.1635914 -4.1937704 -4.2053633 -4.1861124 -4.1798978 -4.18111 -4.1728749 -4.1728325 -4.1748638][-4.2052813 -4.1790471 -4.1527438 -4.1465707 -4.1603627 -4.1721468 -4.1806808 -4.2078867 -4.216156 -4.20133 -4.2013326 -4.2051105 -4.1933179 -4.1974154 -4.2091575][-4.2373586 -4.2255516 -4.2029753 -4.1900711 -4.2037973 -4.2170792 -4.22223 -4.2399912 -4.2396789 -4.2207942 -4.2169347 -4.2164965 -4.2007208 -4.1995959 -4.2177277][-4.2667279 -4.2688828 -4.256968 -4.2447772 -4.2535853 -4.2622228 -4.2610993 -4.2714987 -4.2716169 -4.253777 -4.2425432 -4.2345195 -4.2151008 -4.2053671 -4.2191777][-4.2712851 -4.2812839 -4.2752867 -4.2596741 -4.2572193 -4.2575479 -4.2557678 -4.2726831 -4.2823248 -4.2765589 -4.2721481 -4.2647438 -4.2423329 -4.2261715 -4.2285604][-4.2580709 -4.2607026 -4.2515612 -4.2270851 -4.2099476 -4.19474 -4.1901312 -4.2137356 -4.2373285 -4.2470875 -4.2545366 -4.2600737 -4.25109 -4.2419033 -4.2412672][-4.2272015 -4.2089496 -4.1857553 -4.1532903 -4.1248465 -4.0901265 -4.0768027 -4.1100631 -4.1530724 -4.1831937 -4.2072058 -4.2292018 -4.2388849 -4.2388058 -4.2367373][-4.174634 -4.1404157 -4.1112614 -4.0825267 -4.0513344 -4.0046954 -3.981545 -4.0143385 -4.0681067 -4.1168966 -4.1607795 -4.197814 -4.2205563 -4.2277856 -4.2256789][-4.1269269 -4.0913043 -4.0751877 -4.0693808 -4.0542011 -4.0192318 -3.9937916 -4.0042162 -4.0402846 -4.0893641 -4.1515093 -4.2018161 -4.2281585 -4.2346416 -4.2302747][-4.1037531 -4.0790491 -4.0822525 -4.096909 -4.100317 -4.0877371 -4.0696788 -4.0576129 -4.0597014 -4.0872779 -4.1473742 -4.2023225 -4.2302394 -4.2370911 -4.2330656][-4.1233993 -4.110405 -4.126132 -4.1499739 -4.1638975 -4.1688657 -4.1607738 -4.1396794 -4.1179419 -4.1179724 -4.1577959 -4.2035103 -4.2267032 -4.2332597 -4.225821][-4.1801362 -4.1739712 -4.1888676 -4.2087703 -4.2252283 -4.2375894 -4.2369943 -4.2188053 -4.190836 -4.1794662 -4.1979971 -4.2249069 -4.2391043 -4.2438512 -4.2328939][-4.2390265 -4.23768 -4.2449722 -4.2570348 -4.2730861 -4.2840185 -4.2854867 -4.2722054 -4.2485433 -4.2372336 -4.2445407 -4.2569027 -4.265172 -4.2728767 -4.2662787][-4.2712579 -4.2757816 -4.2781563 -4.2829194 -4.2942648 -4.3025341 -4.3042746 -4.297678 -4.282692 -4.2761989 -4.2772875 -4.2788992 -4.2824655 -4.2907729 -4.2894149][-4.2853322 -4.2904162 -4.2896323 -4.28931 -4.2958484 -4.2998719 -4.302279 -4.3030672 -4.299027 -4.2976365 -4.295495 -4.290586 -4.288744 -4.2917466 -4.2929587]]...]
INFO - root - 2017-12-08 01:31:14.287755: step 77310, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.662 sec/batch; 46h:53m:36s remains)
INFO - root - 2017-12-08 01:31:21.063842: step 77320, loss = 2.04, batch loss = 1.98 (11.2 examples/sec; 0.711 sec/batch; 50h:25m:48s remains)
INFO - root - 2017-12-08 01:31:27.816064: step 77330, loss = 2.04, batch loss = 1.99 (11.2 examples/sec; 0.715 sec/batch; 50h:40m:50s remains)
INFO - root - 2017-12-08 01:31:34.580429: step 77340, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.678 sec/batch; 48h:05m:17s remains)
INFO - root - 2017-12-08 01:31:41.359919: step 77350, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 45h:43m:47s remains)
INFO - root - 2017-12-08 01:31:48.247555: step 77360, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.679 sec/batch; 48h:05m:28s remains)
INFO - root - 2017-12-08 01:31:55.057417: step 77370, loss = 2.08, batch loss = 2.03 (11.6 examples/sec; 0.690 sec/batch; 48h:55m:11s remains)
INFO - root - 2017-12-08 01:32:01.807121: step 77380, loss = 2.05, batch loss = 1.99 (11.0 examples/sec; 0.730 sec/batch; 51h:44m:51s remains)
INFO - root - 2017-12-08 01:32:08.604907: step 77390, loss = 2.04, batch loss = 1.99 (11.5 examples/sec; 0.696 sec/batch; 49h:20m:00s remains)
INFO - root - 2017-12-08 01:32:15.410011: step 77400, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.656 sec/batch; 46h:27m:35s remains)
2017-12-08 01:32:16.119417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3044152 -4.3143687 -4.3241029 -4.3290625 -4.3303342 -4.3295207 -4.3259263 -4.322381 -4.3166671 -4.3076968 -4.2985606 -4.289763 -4.2823639 -4.2775316 -4.2792811][-4.2444625 -4.2591853 -4.2733688 -4.2811074 -4.2833815 -4.2818894 -4.2786875 -4.27686 -4.2731781 -4.2659698 -4.2572174 -4.2475457 -4.2379785 -4.2277861 -4.2237005][-4.1681414 -4.1899848 -4.2104912 -4.22347 -4.2285762 -4.2267962 -4.2229319 -4.22476 -4.2287612 -4.2272596 -4.2210283 -4.2113204 -4.1985216 -4.1787906 -4.1632667][-4.1221733 -4.15308 -4.1780891 -4.1903076 -4.192976 -4.186502 -4.1769218 -4.1794147 -4.1910882 -4.1956911 -4.1908178 -4.1817336 -4.1679144 -4.1431742 -4.1191206][-4.13339 -4.1598892 -4.1770592 -4.1801891 -4.1740494 -4.1562572 -4.1370292 -4.1424475 -4.1655431 -4.1775341 -4.1738086 -4.1654963 -4.1543703 -4.131084 -4.1049151][-4.1618538 -4.166523 -4.163559 -4.1495581 -4.1267676 -4.0889845 -4.0553474 -4.0643735 -4.1079855 -4.139472 -4.1481671 -4.147481 -4.1477103 -4.1384368 -4.1220765][-4.1830916 -4.1579142 -4.127203 -4.0898423 -4.0417662 -3.9771161 -3.9239557 -3.933392 -3.9992423 -4.0618038 -4.09557 -4.1120977 -4.1348386 -4.1547365 -4.1659384][-4.1885996 -4.1433949 -4.0930748 -4.0397992 -3.9782212 -3.8975065 -3.8257797 -3.8250694 -3.8999765 -3.9850357 -4.041461 -4.0743179 -4.1173625 -4.1641569 -4.2041287][-4.2073846 -4.1588635 -4.1058555 -4.0541573 -4.0010791 -3.9319043 -3.8641934 -3.8522696 -3.9132528 -3.9950335 -4.0535841 -4.0892048 -4.1341896 -4.1858163 -4.2338214][-4.24656 -4.2100825 -4.1703558 -4.1338091 -4.0987649 -4.0526237 -4.0050082 -3.9922452 -4.0307941 -4.0878277 -4.1297355 -4.1572356 -4.191637 -4.2304435 -4.2677355][-4.2832465 -4.2649703 -4.2437668 -4.2258325 -4.2079225 -4.1835165 -4.1566858 -4.1488857 -4.170465 -4.2041698 -4.2288418 -4.2450824 -4.2654052 -4.2856374 -4.304925][-4.3068714 -4.3020854 -4.2955775 -4.2908344 -4.2840567 -4.2752843 -4.2643266 -4.2604771 -4.2714396 -4.2898927 -4.3029981 -4.3107743 -4.3181062 -4.3208404 -4.3231316][-4.3171406 -4.315968 -4.3145552 -4.3152208 -4.3169823 -4.3175874 -4.313611 -4.3109617 -4.3158646 -4.3265376 -4.3342772 -4.3378119 -4.3383017 -4.3325067 -4.3269615][-4.3270965 -4.3242297 -4.3194418 -4.3183789 -4.3215938 -4.3266945 -4.3270354 -4.3251481 -4.3268819 -4.3324008 -4.3376031 -4.3404932 -4.3400588 -4.3335075 -4.3275633][-4.3316875 -4.3276868 -4.3190255 -4.3128486 -4.3135719 -4.3186588 -4.3210297 -4.3195405 -4.3191862 -4.3221421 -4.3263903 -4.3307352 -4.3317485 -4.32747 -4.3237963]]...]
INFO - root - 2017-12-08 01:32:22.814634: step 77410, loss = 2.05, batch loss = 1.99 (10.8 examples/sec; 0.738 sec/batch; 52h:18m:43s remains)
INFO - root - 2017-12-08 01:32:29.638319: step 77420, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 48h:27m:42s remains)
INFO - root - 2017-12-08 01:32:36.480163: step 77430, loss = 2.05, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 48h:21m:52s remains)
INFO - root - 2017-12-08 01:32:43.378778: step 77440, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.665 sec/batch; 47h:05m:29s remains)
INFO - root - 2017-12-08 01:32:50.185573: step 77450, loss = 2.05, batch loss = 2.00 (11.6 examples/sec; 0.691 sec/batch; 48h:55m:32s remains)
INFO - root - 2017-12-08 01:32:57.032800: step 77460, loss = 2.07, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 50h:13m:51s remains)
INFO - root - 2017-12-08 01:33:03.714174: step 77470, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.640 sec/batch; 45h:21m:42s remains)
INFO - root - 2017-12-08 01:33:10.514298: step 77480, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 45h:29m:46s remains)
INFO - root - 2017-12-08 01:33:17.339776: step 77490, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.706 sec/batch; 49h:59m:28s remains)
INFO - root - 2017-12-08 01:33:24.155985: step 77500, loss = 2.10, batch loss = 2.05 (11.4 examples/sec; 0.702 sec/batch; 49h:43m:08s remains)
2017-12-08 01:33:24.871547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3260112 -4.3259521 -4.3239026 -4.3263073 -4.330719 -4.3275738 -4.3150148 -4.29737 -4.2751451 -4.2504439 -4.2346525 -4.2311239 -4.234417 -4.2495737 -4.2673912][-4.3370957 -4.341548 -4.3393793 -4.33577 -4.330821 -4.31697 -4.2944531 -4.272234 -4.2510614 -4.2296395 -4.2203126 -4.2253995 -4.234652 -4.2523284 -4.2704725][-4.3452992 -4.3521647 -4.347681 -4.3325009 -4.3108482 -4.2828279 -4.2503619 -4.228591 -4.2133307 -4.1989436 -4.1976933 -4.2117705 -4.2276721 -4.2498655 -4.2689323][-4.3413806 -4.3484974 -4.3423505 -4.3166318 -4.2852435 -4.2482266 -4.2060127 -4.1818662 -4.1755042 -4.1705027 -4.17622 -4.1966643 -4.2177811 -4.2436581 -4.2633653][-4.3248892 -4.3330388 -4.3250194 -4.2932029 -4.2574654 -4.2147417 -4.157382 -4.1250072 -4.1342421 -4.1498818 -4.168025 -4.1954732 -4.2221107 -4.25186 -4.2725964][-4.3064313 -4.3119078 -4.3008957 -4.262084 -4.2200832 -4.1678543 -4.088171 -4.0449686 -4.0808954 -4.1296053 -4.1688275 -4.205555 -4.2386575 -4.271812 -4.2898211][-4.2881608 -4.2862487 -4.2711325 -4.227572 -4.1761322 -4.104815 -3.9995792 -3.9525557 -4.0205946 -4.1017227 -4.1597247 -4.203866 -4.2412353 -4.2748976 -4.2900286][-4.2814775 -4.27058 -4.252738 -4.2109938 -4.1492887 -4.0605512 -3.9392037 -3.8974454 -3.9893527 -4.0847759 -4.1463289 -4.1895938 -4.2281022 -4.2627649 -4.2781162][-4.2719164 -4.2563787 -4.2437706 -4.2139592 -4.1589212 -4.079905 -3.9816391 -3.949826 -4.0237112 -4.0969439 -4.1428318 -4.1773262 -4.2137089 -4.248929 -4.2656879][-4.2643957 -4.2541261 -4.249649 -4.2327428 -4.1962662 -4.1490884 -4.0926433 -4.064 -4.0907817 -4.1301861 -4.1580949 -4.1813464 -4.2136583 -4.24736 -4.264008][-4.2684088 -4.2618308 -4.2615438 -4.2490578 -4.2238307 -4.2012968 -4.1691942 -4.1390028 -4.1411085 -4.1606584 -4.1800523 -4.1987658 -4.2261071 -4.25523 -4.2683067][-4.287715 -4.2822609 -4.2781596 -4.2649589 -4.2467113 -4.2348857 -4.2094579 -4.1774573 -4.1722817 -4.1823988 -4.1947026 -4.2098942 -4.2322736 -4.2558103 -4.2687874][-4.3088636 -4.3003173 -4.294539 -4.2832026 -4.2718368 -4.2633953 -4.24022 -4.2108312 -4.1969543 -4.1939759 -4.1952467 -4.2056885 -4.22382 -4.2426419 -4.2589917][-4.313024 -4.3052888 -4.3001537 -4.2949038 -4.2889123 -4.2819777 -4.2621903 -4.2374268 -4.2136645 -4.196382 -4.1917524 -4.2025409 -4.2148805 -4.2251053 -4.2405372][-4.2915268 -4.288579 -4.2917666 -4.2979393 -4.2997 -4.2925525 -4.2761121 -4.2594633 -4.2297854 -4.2002797 -4.1948056 -4.2094355 -4.2190423 -4.2210264 -4.2308574]]...]
INFO - root - 2017-12-08 01:33:31.567904: step 77510, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.722 sec/batch; 51h:09m:58s remains)
INFO - root - 2017-12-08 01:33:38.523042: step 77520, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.713 sec/batch; 50h:31m:31s remains)
INFO - root - 2017-12-08 01:33:45.264435: step 77530, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.675 sec/batch; 47h:47m:48s remains)
INFO - root - 2017-12-08 01:33:52.064835: step 77540, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.670 sec/batch; 47h:27m:45s remains)
INFO - root - 2017-12-08 01:33:58.744103: step 77550, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 46h:03m:22s remains)
INFO - root - 2017-12-08 01:34:05.599045: step 77560, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.698 sec/batch; 49h:25m:05s remains)
INFO - root - 2017-12-08 01:34:12.420728: step 77570, loss = 2.04, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 47h:41m:40s remains)
INFO - root - 2017-12-08 01:34:19.187472: step 77580, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 48h:02m:15s remains)
INFO - root - 2017-12-08 01:34:25.940998: step 77590, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.619 sec/batch; 43h:49m:57s remains)
INFO - root - 2017-12-08 01:34:32.785596: step 77600, loss = 2.05, batch loss = 2.00 (12.2 examples/sec; 0.657 sec/batch; 46h:31m:17s remains)
2017-12-08 01:34:33.475875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3390121 -4.3243022 -4.3098726 -4.3071389 -4.3023152 -4.283083 -4.256319 -4.2276673 -4.2003984 -4.1907568 -4.2139335 -4.2528362 -4.2869978 -4.3066039 -4.3244839][-4.3468051 -4.3310375 -4.3100858 -4.2987604 -4.2884574 -4.2729979 -4.2516432 -4.2234368 -4.19425 -4.1831508 -4.204062 -4.2416639 -4.2746005 -4.2996521 -4.3254714][-4.3462734 -4.3302846 -4.306932 -4.2903934 -4.2741456 -4.2565813 -4.235868 -4.2091737 -4.1822066 -4.1738448 -4.1906905 -4.2257476 -4.2598052 -4.291821 -4.3239288][-4.3418202 -4.3328266 -4.3150721 -4.2961679 -4.2741466 -4.2510056 -4.2244358 -4.1991668 -4.1790905 -4.1718721 -4.1850662 -4.2171774 -4.2503891 -4.28543 -4.3178682][-4.3289809 -4.3278704 -4.3150077 -4.2909708 -4.2579703 -4.218646 -4.1832561 -4.1610003 -4.1550546 -4.1571035 -4.1733375 -4.2070789 -4.2359219 -4.268127 -4.2986879][-4.3094606 -4.3140197 -4.3018942 -4.2697582 -4.2216825 -4.1628032 -4.1151991 -4.0955057 -4.1162167 -4.1458468 -4.1749139 -4.2030911 -4.2197075 -4.2416587 -4.2658319][-4.2916303 -4.3045688 -4.2940469 -4.2571983 -4.1918173 -4.1092539 -4.04025 -4.0198522 -4.0805435 -4.1517448 -4.1916089 -4.2048259 -4.2036123 -4.2108269 -4.223846][-4.2696352 -4.2900496 -4.280529 -4.2382531 -4.1517324 -4.0387397 -3.9350228 -3.9091096 -4.0126181 -4.1315441 -4.1852422 -4.1896348 -4.179451 -4.1742959 -4.1759315][-4.2483506 -4.2757335 -4.2690673 -4.2236052 -4.1247945 -3.9934821 -3.8720045 -3.8494184 -3.9724097 -4.109684 -4.1666093 -4.1640024 -4.1520786 -4.1419525 -4.1348634][-4.2336679 -4.2636213 -4.2653995 -4.2280951 -4.1403995 -4.0205131 -3.9219823 -3.9167469 -4.0238371 -4.1358247 -4.1746731 -4.1662288 -4.1575174 -4.1437497 -4.1315594][-4.2322521 -4.2567759 -4.265646 -4.2462077 -4.1832209 -4.0864325 -4.0121489 -4.0152774 -4.0909963 -4.1650949 -4.1855865 -4.1776443 -4.1733785 -4.1603851 -4.1500478][-4.2465 -4.2603321 -4.2684317 -4.2631803 -4.2280846 -4.1611476 -4.1101074 -4.11508 -4.1607885 -4.197721 -4.2007723 -4.1941814 -4.1936412 -4.1877356 -4.1811342][-4.2656364 -4.2763782 -4.2831144 -4.2844062 -4.2676091 -4.2228055 -4.18573 -4.1901436 -4.2133117 -4.2224464 -4.2089467 -4.203649 -4.2122331 -4.216826 -4.2134881][-4.2714658 -4.2839255 -4.29461 -4.2990394 -4.2904058 -4.255815 -4.2220807 -4.2200675 -4.225944 -4.2174258 -4.1981964 -4.1951094 -4.2125559 -4.2274213 -4.23585][-4.2638721 -4.2795444 -4.2939768 -4.3005457 -4.2940779 -4.2686887 -4.2410436 -4.2312112 -4.226768 -4.2122517 -4.1955824 -4.1944418 -4.2161851 -4.2394996 -4.2570233]]...]
INFO - root - 2017-12-08 01:34:40.101704: step 77610, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.707 sec/batch; 50h:04m:44s remains)
INFO - root - 2017-12-08 01:34:46.908264: step 77620, loss = 2.09, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 46h:28m:11s remains)
INFO - root - 2017-12-08 01:34:53.697503: step 77630, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 46h:03m:33s remains)
INFO - root - 2017-12-08 01:35:00.530483: step 77640, loss = 2.05, batch loss = 2.00 (11.9 examples/sec; 0.671 sec/batch; 47h:31m:47s remains)
INFO - root - 2017-12-08 01:35:07.375663: step 77650, loss = 2.05, batch loss = 2.00 (11.7 examples/sec; 0.684 sec/batch; 48h:25m:17s remains)
INFO - root - 2017-12-08 01:35:14.231098: step 77660, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.711 sec/batch; 50h:17m:45s remains)
INFO - root - 2017-12-08 01:35:21.104094: step 77670, loss = 2.06, batch loss = 2.00 (11.6 examples/sec; 0.690 sec/batch; 48h:49m:21s remains)
INFO - root - 2017-12-08 01:35:27.886498: step 77680, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.680 sec/batch; 48h:10m:02s remains)
INFO - root - 2017-12-08 01:35:34.780742: step 77690, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 50h:04m:12s remains)
INFO - root - 2017-12-08 01:35:41.615410: step 77700, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 49h:29m:53s remains)
2017-12-08 01:35:42.335808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1705079 -4.16434 -4.1489835 -4.1430049 -4.1454816 -4.1407571 -4.1280413 -4.1234579 -4.1380529 -4.1681442 -4.18813 -4.1968169 -4.20367 -4.2105546 -4.2140012][-4.1682763 -4.159297 -4.1451349 -4.1316237 -4.1178131 -4.092402 -4.0629 -4.058569 -4.086647 -4.1306939 -4.1612997 -4.1776485 -4.1888914 -4.1971316 -4.1998477][-4.1726646 -4.159142 -4.1424408 -4.1188879 -4.0846314 -4.0397954 -3.9973145 -3.996979 -4.041636 -4.0970964 -4.133482 -4.1518283 -4.1614451 -4.166574 -4.1668692][-4.1782832 -4.16318 -4.1466084 -4.1202517 -4.0817347 -4.0327029 -3.9924078 -3.9938555 -4.0432043 -4.0945764 -4.1238794 -4.1328917 -4.1308622 -4.1313314 -4.1332021][-4.1867304 -4.1719565 -4.1602907 -4.1427298 -4.1128654 -4.0698771 -4.0306292 -4.0258107 -4.0683594 -4.1132927 -4.1365929 -4.1368141 -4.1222019 -4.1167483 -4.1200075][-4.1946559 -4.177784 -4.1671224 -4.1593485 -4.1336937 -4.0858679 -4.0413666 -4.027 -4.0669909 -4.1144934 -4.1433234 -4.1475558 -4.1292229 -4.1165838 -4.1158681][-4.1949239 -4.1784582 -4.1673293 -4.157999 -4.1252027 -4.0612946 -4.0051184 -3.9849987 -4.0353746 -4.1026993 -4.1464648 -4.1618485 -4.1500254 -4.139874 -4.1338515][-4.1754565 -4.1641769 -4.1545773 -4.1452093 -4.1128531 -4.0460548 -3.9797249 -3.95503 -4.0146065 -4.0964093 -4.1510549 -4.1730766 -4.1706381 -4.1652474 -4.158874][-4.1428781 -4.1401372 -4.1370921 -4.1362996 -4.1191521 -4.0720778 -4.016891 -3.9890819 -4.04444 -4.1214385 -4.1703467 -4.190042 -4.1903391 -4.1851525 -4.17925][-4.1224508 -4.1315513 -4.1392608 -4.1499271 -4.1510305 -4.130816 -4.0978017 -4.0696344 -4.1071515 -4.1667671 -4.1987762 -4.2068205 -4.203341 -4.1932282 -4.1836929][-4.1236382 -4.1388335 -4.1545606 -4.1716328 -4.1875391 -4.1847382 -4.1658716 -4.1358833 -4.1534934 -4.1986485 -4.2214031 -4.222455 -4.2120781 -4.1991873 -4.1859875][-4.1413922 -4.1522846 -4.1685271 -4.1909957 -4.2153382 -4.2228923 -4.21398 -4.186976 -4.1927376 -4.2265 -4.243135 -4.2389808 -4.2220678 -4.2077971 -4.194418][-4.1519365 -4.1589842 -4.1762395 -4.2014756 -4.2268176 -4.2435889 -4.2474852 -4.2276621 -4.2294493 -4.2544866 -4.26269 -4.249423 -4.2260547 -4.2108569 -4.2027636][-4.1601033 -4.1656933 -4.1847968 -4.211102 -4.2330055 -4.2537527 -4.2650332 -4.2534647 -4.2529168 -4.2701769 -4.2717004 -4.2533178 -4.22738 -4.2102909 -4.2033844][-4.1712828 -4.1744051 -4.194397 -4.2208166 -4.235713 -4.2558508 -4.2724776 -4.26527 -4.2596645 -4.2671118 -4.2654405 -4.2511344 -4.229218 -4.2119656 -4.2054558]]...]
INFO - root - 2017-12-08 01:35:49.032675: step 77710, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.669 sec/batch; 47h:19m:56s remains)
INFO - root - 2017-12-08 01:35:55.848909: step 77720, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.686 sec/batch; 48h:32m:28s remains)
INFO - root - 2017-12-08 01:36:02.698119: step 77730, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.728 sec/batch; 51h:32m:14s remains)
INFO - root - 2017-12-08 01:36:09.667632: step 77740, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.687 sec/batch; 48h:37m:54s remains)
INFO - root - 2017-12-08 01:36:16.442146: step 77750, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 46h:12m:55s remains)
INFO - root - 2017-12-08 01:36:23.267062: step 77760, loss = 2.07, batch loss = 2.02 (11.7 examples/sec; 0.684 sec/batch; 48h:22m:51s remains)
INFO - root - 2017-12-08 01:36:30.077557: step 77770, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 48h:21m:53s remains)
INFO - root - 2017-12-08 01:36:36.743714: step 77780, loss = 2.06, batch loss = 2.00 (12.5 examples/sec; 0.638 sec/batch; 45h:07m:29s remains)
INFO - root - 2017-12-08 01:36:43.468850: step 77790, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 46h:08m:09s remains)
INFO - root - 2017-12-08 01:36:50.337986: step 77800, loss = 2.06, batch loss = 2.01 (11.6 examples/sec; 0.690 sec/batch; 48h:47m:29s remains)
2017-12-08 01:36:51.141476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3533483 -4.3602905 -4.3482523 -4.311378 -4.2672215 -4.2109218 -4.1406155 -4.1445017 -4.1972241 -4.2237415 -4.2374549 -4.2455873 -4.25572 -4.2642117 -4.2600007][-4.3521109 -4.3601842 -4.3501048 -4.3153548 -4.2704282 -4.208498 -4.12884 -4.1276693 -4.1749773 -4.1950679 -4.2077627 -4.2159004 -4.2303963 -4.24762 -4.2484][-4.3505049 -4.3597956 -4.3512969 -4.3184881 -4.2717314 -4.20444 -4.1209664 -4.1172528 -4.1608129 -4.1778407 -4.1929522 -4.2028365 -4.2185173 -4.2404633 -4.2437119][-4.3480773 -4.3578687 -4.3511653 -4.3201962 -4.2743983 -4.2060418 -4.1225653 -4.1147647 -4.151814 -4.1657023 -4.1847887 -4.1990318 -4.215354 -4.2397728 -4.2473264][-4.3468323 -4.3562131 -4.3506889 -4.3217916 -4.2771635 -4.2104483 -4.1258984 -4.1080265 -4.1342721 -4.1455493 -4.1724486 -4.1956649 -4.2109866 -4.2361073 -4.2486959][-4.34542 -4.3542147 -4.3486996 -4.3198924 -4.2753687 -4.2081947 -4.1187496 -4.0847297 -4.0951 -4.1026087 -4.1441813 -4.1815448 -4.2003446 -4.227386 -4.2435393][-4.3432646 -4.350461 -4.3444791 -4.3153038 -4.2715182 -4.2061 -4.1095467 -4.0597644 -4.0561075 -4.0624866 -4.1157951 -4.1645904 -4.1859031 -4.2133632 -4.2302861][-4.3423381 -4.3493137 -4.3448763 -4.3184166 -4.2808723 -4.2257004 -4.1307607 -4.0797772 -4.0783668 -4.0860176 -4.131382 -4.1727734 -4.1867356 -4.2086039 -4.2217712][-4.3432956 -4.350801 -4.3494534 -4.3279309 -4.2983103 -4.2539167 -4.1674857 -4.1254964 -4.1345572 -4.1452527 -4.1730876 -4.1939735 -4.1957378 -4.2103329 -4.2200685][-4.3452048 -4.3534989 -4.3541746 -4.3353553 -4.3108091 -4.2726674 -4.1929564 -4.1575012 -4.1722579 -4.1825662 -4.19986 -4.2089324 -4.2027626 -4.2115932 -4.2182579][-4.3465767 -4.3543863 -4.3547215 -4.3356881 -4.3132415 -4.2797875 -4.2090721 -4.1819053 -4.1991563 -4.2065997 -4.2184 -4.2256932 -4.2207379 -4.2252874 -4.225493][-4.3477774 -4.3552151 -4.3543892 -4.3325438 -4.3092155 -4.2805247 -4.2207789 -4.2048378 -4.2265487 -4.229548 -4.2357 -4.2424703 -4.2417097 -4.2428603 -4.2366133][-4.3497276 -4.3579741 -4.355732 -4.3292093 -4.3018613 -4.273355 -4.2200165 -4.2134118 -4.2378068 -4.2359004 -4.23744 -4.2437968 -4.248558 -4.2512283 -4.2424846][-4.3513207 -4.358942 -4.352159 -4.3218307 -4.2915397 -4.2616882 -4.2122865 -4.2108994 -4.235621 -4.2322669 -4.2328687 -4.2400894 -4.249701 -4.2530937 -4.2427988][-4.3516312 -4.3566151 -4.3440132 -4.3089314 -4.2762465 -4.2459316 -4.1996608 -4.2033019 -4.2308092 -4.2296095 -4.2308187 -4.239305 -4.2476587 -4.2460485 -4.2312651]]...]
INFO - root - 2017-12-08 01:36:57.760937: step 77810, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 47h:06m:59s remains)
INFO - root - 2017-12-08 01:37:04.486074: step 77820, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 48h:31m:18s remains)
INFO - root - 2017-12-08 01:37:11.268143: step 77830, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.714 sec/batch; 50h:30m:47s remains)
INFO - root - 2017-12-08 01:37:18.024696: step 77840, loss = 2.07, batch loss = 2.01 (11.2 examples/sec; 0.714 sec/batch; 50h:30m:40s remains)
INFO - root - 2017-12-08 01:37:24.832414: step 77850, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 45h:51m:28s remains)
INFO - root - 2017-12-08 01:37:31.704516: step 77860, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 46h:06m:20s remains)
INFO - root - 2017-12-08 01:37:38.536632: step 77870, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.674 sec/batch; 47h:41m:28s remains)
INFO - root - 2017-12-08 01:37:45.401740: step 77880, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.718 sec/batch; 50h:46m:08s remains)
INFO - root - 2017-12-08 01:37:52.197277: step 77890, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.725 sec/batch; 51h:15m:47s remains)
INFO - root - 2017-12-08 01:37:58.898489: step 77900, loss = 2.10, batch loss = 2.04 (12.0 examples/sec; 0.666 sec/batch; 47h:06m:37s remains)
2017-12-08 01:37:59.670805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.371531 -4.373569 -4.3740644 -4.3739858 -4.3735456 -4.3729224 -4.3726969 -4.3718677 -4.3692141 -4.3570166 -4.3319616 -4.3002419 -4.2760706 -4.2654262 -4.2705436][-4.3756618 -4.3767972 -4.3747592 -4.37163 -4.3686128 -4.3662033 -4.3650413 -4.3635917 -4.359766 -4.3448319 -4.3115168 -4.2630167 -4.2170596 -4.1906471 -4.1971211][-4.3775268 -4.3772593 -4.3701892 -4.3599734 -4.3519807 -4.3465495 -4.3433485 -4.3420434 -4.3408403 -4.3285527 -4.2901683 -4.2232289 -4.1481895 -4.0925193 -4.0896931][-4.3750243 -4.3704495 -4.3560486 -4.3376794 -4.3239446 -4.3151507 -4.3096709 -4.3090277 -4.3132772 -4.3084507 -4.274508 -4.2036877 -4.1065269 -4.0096674 -3.9733133][-4.36717 -4.3561664 -4.3327475 -4.3047323 -4.2828732 -4.2679486 -4.2593727 -4.2632937 -4.2782421 -4.2875714 -4.2704549 -4.2158418 -4.1178174 -3.9903276 -3.9047761][-4.3571086 -4.3371511 -4.3007126 -4.2574606 -4.2203145 -4.191927 -4.1762486 -4.1871619 -4.2218542 -4.2564073 -4.2664762 -4.2413592 -4.1744838 -4.0686622 -3.976928][-4.349638 -4.3202877 -4.2694054 -4.204586 -4.1429067 -4.0928054 -4.06295 -4.0764818 -4.1354437 -4.2018385 -4.2451787 -4.251914 -4.2268486 -4.1740561 -4.1217666][-4.3498425 -4.3154192 -4.2540922 -4.1697493 -4.0830994 -4.004416 -3.9467957 -3.9457161 -4.0237145 -4.1236963 -4.1983566 -4.236516 -4.2480936 -4.2415619 -4.2268143][-4.3594761 -4.3298635 -4.2710986 -4.182981 -4.0839987 -3.981746 -3.8910978 -3.8533242 -3.927371 -4.0469794 -4.1464491 -4.2104754 -4.2486043 -4.2714429 -4.28049][-4.3721585 -4.3538518 -4.3120956 -4.2440114 -4.1602345 -4.0658789 -3.9735017 -3.9132457 -3.9466629 -4.0421648 -4.1373253 -4.2111831 -4.2614627 -4.2938209 -4.3087091][-4.3765092 -4.3674016 -4.3447862 -4.3062916 -4.253593 -4.1882648 -4.1195288 -4.0668125 -4.0677867 -4.12185 -4.1898885 -4.2538562 -4.2999473 -4.3248358 -4.33192][-4.3725114 -4.3684669 -4.3599114 -4.3439989 -4.3186193 -4.2811484 -4.2382236 -4.2016277 -4.1923733 -4.2186537 -4.25964 -4.303679 -4.3361368 -4.34909 -4.3444543][-4.3656878 -4.3623719 -4.3606744 -4.3580327 -4.3509493 -4.3341522 -4.3106456 -4.289217 -4.2811704 -4.2934575 -4.3140154 -4.3360233 -4.3501639 -4.3492594 -4.332943][-4.3623962 -4.35872 -4.3573818 -4.3589444 -4.3594041 -4.3543224 -4.344501 -4.332757 -4.3270431 -4.330533 -4.3372746 -4.3434606 -4.3403964 -4.3227553 -4.29261][-4.36024 -4.3573279 -4.3553271 -4.3566828 -4.359077 -4.359664 -4.3584633 -4.3546376 -4.3507056 -4.3483748 -4.344471 -4.3351974 -4.3163133 -4.2808175 -4.2322221]]...]
INFO - root - 2017-12-08 01:38:06.351003: step 77910, loss = 2.06, batch loss = 2.01 (11.4 examples/sec; 0.699 sec/batch; 49h:27m:17s remains)
INFO - root - 2017-12-08 01:38:13.177820: step 77920, loss = 2.07, batch loss = 2.01 (11.6 examples/sec; 0.691 sec/batch; 48h:51m:54s remains)
INFO - root - 2017-12-08 01:38:19.862494: step 77930, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 44h:55m:21s remains)
INFO - root - 2017-12-08 01:38:26.544766: step 77940, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 46h:14m:38s remains)
INFO - root - 2017-12-08 01:38:33.256274: step 77950, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 49h:50m:14s remains)
INFO - root - 2017-12-08 01:38:40.035004: step 77960, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 50h:07m:30s remains)
INFO - root - 2017-12-08 01:38:46.808181: step 77970, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.677 sec/batch; 47h:52m:31s remains)
INFO - root - 2017-12-08 01:38:53.619583: step 77980, loss = 2.06, batch loss = 2.01 (12.1 examples/sec; 0.664 sec/batch; 46h:56m:07s remains)
INFO - root - 2017-12-08 01:39:00.430150: step 77990, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.656 sec/batch; 46h:21m:04s remains)
INFO - root - 2017-12-08 01:39:07.239586: step 78000, loss = 2.07, batch loss = 2.02 (11.2 examples/sec; 0.716 sec/batch; 50h:36m:20s remains)
2017-12-08 01:39:08.009618: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3670392 -4.3718085 -4.3645415 -4.3351145 -4.2711878 -4.1788425 -4.0960732 -4.0460014 -4.0470052 -4.0970526 -4.1650934 -4.2271566 -4.2705827 -4.29884 -4.3129892][-4.3692269 -4.3676753 -4.3557839 -4.327096 -4.2676039 -4.1775317 -4.0861025 -4.0230441 -4.0212541 -4.080277 -4.1597672 -4.2283287 -4.2735195 -4.3006258 -4.3125787][-4.3681149 -4.3619971 -4.3477316 -4.3176565 -4.2557645 -4.1599212 -4.0530624 -3.9760883 -3.9861171 -4.0685792 -4.163115 -4.2385216 -4.2836285 -4.30671 -4.3139076][-4.3658619 -4.3556685 -4.3373351 -4.29963 -4.2309165 -4.1245384 -4.0004663 -3.9165585 -3.9518733 -4.0598674 -4.1686683 -4.2519722 -4.2983165 -4.3167486 -4.3172636][-4.3582311 -4.3422771 -4.3156872 -4.2660174 -4.1896911 -4.0780358 -3.9564238 -3.8881726 -3.9437311 -4.0600929 -4.1735487 -4.2601919 -4.3094325 -4.3258734 -4.3225722][-4.3454113 -4.32309 -4.2873383 -4.2222247 -4.1358385 -4.0255876 -3.9263 -3.8878143 -3.9522369 -4.0578675 -4.1671653 -4.2553873 -4.3100085 -4.3291268 -4.3267117][-4.3318191 -4.303679 -4.265626 -4.1964836 -4.1095839 -4.0053806 -3.92309 -3.8989639 -3.9572847 -4.0478959 -4.1479197 -4.2370458 -4.3002377 -4.3258514 -4.3274136][-4.3233213 -4.2952061 -4.2616072 -4.2026634 -4.1310244 -4.0392184 -3.9579771 -3.9199605 -3.95668 -4.0335145 -4.1270819 -4.2182846 -4.2871041 -4.3182144 -4.3241897][-4.3242178 -4.2978015 -4.26772 -4.222353 -4.170835 -4.0974283 -4.0156116 -3.9632955 -3.9799452 -4.045609 -4.1346321 -4.2215395 -4.2854977 -4.31572 -4.32226][-4.3306613 -4.310142 -4.2855778 -4.2560763 -4.2257209 -4.17375 -4.1002235 -4.0481858 -4.0516038 -4.1004815 -4.1752291 -4.2488012 -4.2982168 -4.3195877 -4.3240061][-4.3376379 -4.3253903 -4.30901 -4.2922273 -4.2751393 -4.23851 -4.1788926 -4.1363988 -4.1354389 -4.1702466 -4.226934 -4.2828774 -4.3155651 -4.3267412 -4.3273854][-4.3399115 -4.3345885 -4.3282781 -4.3198042 -4.3090963 -4.2848473 -4.2427273 -4.2164288 -4.2177706 -4.2409859 -4.2784553 -4.3147931 -4.3322549 -4.3345346 -4.3307629][-4.3408985 -4.3390541 -4.3379188 -4.3346696 -4.3300786 -4.3165388 -4.2906837 -4.2771111 -4.2811651 -4.296731 -4.3186808 -4.3382182 -4.3433151 -4.3391356 -4.3323641][-4.338933 -4.3368807 -4.3357463 -4.3323092 -4.3285155 -4.3200579 -4.3073106 -4.303648 -4.3088903 -4.3216786 -4.3367829 -4.3465948 -4.3454266 -4.338438 -4.33143][-4.3366165 -4.3336825 -4.3310471 -4.3270569 -4.3218808 -4.315608 -4.3080764 -4.3081965 -4.3135357 -4.3236809 -4.3340387 -4.3393946 -4.3373675 -4.3320317 -4.327445]]...]
INFO - root - 2017-12-08 01:39:14.629090: step 78010, loss = 2.05, batch loss = 1.99 (12.0 examples/sec; 0.669 sec/batch; 47h:19m:19s remains)
INFO - root - 2017-12-08 01:39:21.449297: step 78020, loss = 2.06, batch loss = 2.01 (11.1 examples/sec; 0.721 sec/batch; 50h:59m:19s remains)
INFO - root - 2017-12-08 01:39:28.241471: step 78030, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.706 sec/batch; 49h:52m:21s remains)
INFO - root - 2017-12-08 01:39:35.099879: step 78040, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 47h:48m:35s remains)
INFO - root - 2017-12-08 01:39:41.930017: step 78050, loss = 2.08, batch loss = 2.03 (12.5 examples/sec; 0.639 sec/batch; 45h:10m:04s remains)
INFO - root - 2017-12-08 01:39:48.811132: step 78060, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 51h:01m:33s remains)
INFO - root - 2017-12-08 01:39:55.768837: step 78070, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.733 sec/batch; 51h:46m:29s remains)
INFO - root - 2017-12-08 01:40:02.527453: step 78080, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 47h:47m:06s remains)
INFO - root - 2017-12-08 01:40:09.073669: step 78090, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.649 sec/batch; 45h:51m:47s remains)
INFO - root - 2017-12-08 01:40:15.894929: step 78100, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.700 sec/batch; 49h:28m:43s remains)
2017-12-08 01:40:16.618154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2405376 -4.2350039 -4.2344127 -4.2355318 -4.2291918 -4.2188449 -4.2155066 -4.2296562 -4.2378864 -4.2305627 -4.2155886 -4.201715 -4.195375 -4.1954045 -4.2004361][-4.2311277 -4.230669 -4.2344503 -4.2407603 -4.2354488 -4.221096 -4.2115574 -4.2236419 -4.2327514 -4.2255421 -4.2112632 -4.1990376 -4.1943288 -4.197011 -4.2036486][-4.2399421 -4.2402244 -4.2426038 -4.2475328 -4.2420449 -4.2213264 -4.2023277 -4.2083449 -4.2182288 -4.214757 -4.2027493 -4.1907578 -4.1865025 -4.1910334 -4.2032695][-4.2540421 -4.250596 -4.2501307 -4.2516866 -4.2455835 -4.219676 -4.190722 -4.1906552 -4.2000132 -4.1983232 -4.1831112 -4.1645827 -4.1589909 -4.1664276 -4.1882596][-4.2545471 -4.2504807 -4.2483296 -4.2447987 -4.2306337 -4.1974282 -4.1626878 -4.1624002 -4.1762877 -4.1796794 -4.1605186 -4.1356378 -4.1275835 -4.1369791 -4.170373][-4.2355008 -4.2320681 -4.2265487 -4.2151084 -4.1888342 -4.1429935 -4.0967317 -4.0919704 -4.1173639 -4.1388721 -4.1321182 -4.1089597 -4.0943441 -4.1020813 -4.1391354][-4.1856527 -4.1803565 -4.1742473 -4.1592097 -4.114666 -4.0438752 -3.972187 -3.963414 -4.0146961 -4.0687528 -4.0859275 -4.0685568 -4.0484838 -4.0501995 -4.087903][-4.1179061 -4.1005616 -4.0875392 -4.0642309 -3.9993711 -3.9002049 -3.802155 -3.7981699 -3.8878553 -3.9786181 -4.0209804 -4.0118833 -3.9899707 -3.9903646 -4.0279608][-4.0463123 -4.00987 -3.9876692 -3.9641144 -3.902596 -3.8038049 -3.7025266 -3.7053554 -3.8192172 -3.9290659 -3.9821551 -3.9775772 -3.9578416 -3.9577563 -3.9906273][-4.0256748 -3.980422 -3.9586375 -3.9496129 -3.9197781 -3.8564911 -3.783263 -3.7833037 -3.8668532 -3.9526525 -3.998544 -3.9961152 -3.9809415 -3.9790304 -4.0044165][-4.0923491 -4.0551348 -4.0402527 -4.0428681 -4.0383983 -4.0068951 -3.9605992 -3.9532282 -3.9994717 -4.0567236 -4.0915112 -4.0900469 -4.0787654 -4.0766149 -4.0940824][-4.20254 -4.1882653 -4.1869073 -4.195313 -4.1980491 -4.1782155 -4.1471891 -4.1385274 -4.1623511 -4.1986709 -4.2208514 -4.2182183 -4.210104 -4.2073274 -4.2186732][-4.2909064 -4.2953205 -4.3044462 -4.3141403 -4.3172617 -4.303678 -4.2815566 -4.2734375 -4.2845893 -4.3043909 -4.3163314 -4.3149781 -4.311 -4.3094645 -4.3141222][-4.337059 -4.3469944 -4.3568578 -4.3650241 -4.3684998 -4.3603735 -4.3468103 -4.3401194 -4.3454213 -4.3551297 -4.3601813 -4.3591452 -4.3593173 -4.3596535 -4.361567][-4.352232 -4.3598218 -4.3669639 -4.3728304 -4.3750324 -4.3712568 -4.3641725 -4.3603487 -4.36369 -4.3684592 -4.3699656 -4.3699007 -4.3705487 -4.3711405 -4.3716989]]...]
INFO - root - 2017-12-08 01:40:23.176810: step 78110, loss = 2.07, batch loss = 2.02 (12.1 examples/sec; 0.662 sec/batch; 46h:44m:49s remains)
INFO - root - 2017-12-08 01:40:30.009022: step 78120, loss = 2.03, batch loss = 1.98 (12.4 examples/sec; 0.645 sec/batch; 45h:34m:24s remains)
INFO - root - 2017-12-08 01:40:36.834272: step 78130, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.675 sec/batch; 47h:39m:50s remains)
INFO - root - 2017-12-08 01:40:43.760835: step 78140, loss = 2.07, batch loss = 2.01 (10.8 examples/sec; 0.737 sec/batch; 52h:06m:30s remains)
INFO - root - 2017-12-08 01:40:50.575540: step 78150, loss = 2.06, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 49h:06m:28s remains)
INFO - root - 2017-12-08 01:40:57.303105: step 78160, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.631 sec/batch; 44h:35m:00s remains)
INFO - root - 2017-12-08 01:41:04.104788: step 78170, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 50h:32m:53s remains)
INFO - root - 2017-12-08 01:41:10.947533: step 78180, loss = 2.04, batch loss = 1.98 (11.3 examples/sec; 0.706 sec/batch; 49h:52m:49s remains)
INFO - root - 2017-12-08 01:41:17.646552: step 78190, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.662 sec/batch; 46h:44m:26s remains)
INFO - root - 2017-12-08 01:41:24.494941: step 78200, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 44h:38m:49s remains)
2017-12-08 01:41:25.257053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3441262 -4.3372951 -4.32958 -4.3240776 -4.3178229 -4.310966 -4.3008533 -4.2939811 -4.2900209 -4.2876897 -4.2901626 -4.3003321 -4.3123617 -4.3227139 -4.3333979][-4.3329687 -4.3231344 -4.3137031 -4.3068695 -4.2993464 -4.2906704 -4.2735262 -4.2596936 -4.2537484 -4.253839 -4.2566247 -4.2669711 -4.2837472 -4.300159 -4.3159938][-4.3154683 -4.2999864 -4.2857413 -4.2734003 -4.2620516 -4.2508206 -4.2268996 -4.2100368 -4.2083049 -4.2199869 -4.2329216 -4.2477884 -4.2671175 -4.2882624 -4.3069868][-4.2978649 -4.2739811 -4.2527232 -4.2350364 -4.2196684 -4.2041645 -4.1769757 -4.1588531 -4.163435 -4.18924 -4.2185359 -4.2435031 -4.2706556 -4.2954521 -4.3142228][-4.276268 -4.2419024 -4.2118025 -4.1881552 -4.16591 -4.14072 -4.1094151 -4.0906305 -4.1016665 -4.138566 -4.1804628 -4.2180777 -4.2577276 -4.2902184 -4.3133783][-4.2572985 -4.2114949 -4.1698556 -4.1315627 -4.0878425 -4.0341525 -3.9902329 -3.9760668 -4.0009913 -4.0519361 -4.1101947 -4.1653824 -4.2212863 -4.2657866 -4.2992048][-4.2406468 -4.1849246 -4.1295371 -4.0703325 -3.9921069 -3.8969181 -3.8353636 -3.8256855 -3.8730419 -3.9510417 -4.036675 -4.1127191 -4.1833639 -4.2423897 -4.2859221][-4.2286754 -4.1723037 -4.1132636 -4.0503645 -3.9688106 -3.8749533 -3.8099322 -3.7976537 -3.8557668 -3.9426441 -4.0348 -4.1150432 -4.1858077 -4.2445488 -4.2884059][-4.2294021 -4.1806955 -4.1323724 -4.0883408 -4.0401459 -3.9868994 -3.939754 -3.9243469 -3.9650788 -4.0293193 -4.1009841 -4.1652656 -4.2222114 -4.2697086 -4.3027105][-4.2421236 -4.2012696 -4.1641731 -4.1354275 -4.1096272 -4.0807772 -4.048481 -4.03543 -4.0659676 -4.1110663 -4.1608348 -4.2094469 -4.2548018 -4.2913837 -4.3157492][-4.2620625 -4.2287683 -4.20176 -4.1822443 -4.1684771 -4.1556292 -4.13804 -4.1292439 -4.1512551 -4.1789594 -4.2121735 -4.2480245 -4.2837548 -4.3116045 -4.3284621][-4.2817063 -4.25538 -4.2382874 -4.2291245 -4.2239823 -4.2201681 -4.2126689 -4.2071538 -4.2211256 -4.2386379 -4.2598014 -4.2835288 -4.3082891 -4.3280458 -4.3400679][-4.2994347 -4.2804885 -4.2697172 -4.2664948 -4.2669106 -4.2669778 -4.262661 -4.2583628 -4.2659068 -4.2778254 -4.291429 -4.3069715 -4.32359 -4.3383713 -4.3475118][-4.3178525 -4.3035488 -4.29464 -4.291482 -4.2921972 -4.2930336 -4.291132 -4.2896519 -4.2949257 -4.3036022 -4.3131857 -4.3238082 -4.3356624 -4.3455782 -4.3520775][-4.3425112 -4.3336811 -4.3278027 -4.3249946 -4.3243551 -4.3249664 -4.3253155 -4.3266935 -4.330637 -4.3353162 -4.3397622 -4.3448625 -4.3505373 -4.35497 -4.3586159]]...]
INFO - root - 2017-12-08 01:41:31.880136: step 78210, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.714 sec/batch; 50h:24m:41s remains)
INFO - root - 2017-12-08 01:41:38.653947: step 78220, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.705 sec/batch; 49h:45m:46s remains)
INFO - root - 2017-12-08 01:41:45.352865: step 78230, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 43h:56m:15s remains)
INFO - root - 2017-12-08 01:41:52.330551: step 78240, loss = 2.05, batch loss = 1.99 (11.5 examples/sec; 0.693 sec/batch; 48h:56m:37s remains)
INFO - root - 2017-12-08 01:41:59.184790: step 78250, loss = 2.05, batch loss = 1.99 (10.9 examples/sec; 0.734 sec/batch; 51h:48m:15s remains)
INFO - root - 2017-12-08 01:42:05.999025: step 78260, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.663 sec/batch; 46h:49m:02s remains)
INFO - root - 2017-12-08 01:42:12.703923: step 78270, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.646 sec/batch; 45h:38m:53s remains)
INFO - root - 2017-12-08 01:42:19.491043: step 78280, loss = 2.10, batch loss = 2.04 (12.5 examples/sec; 0.640 sec/batch; 45h:09m:47s remains)
INFO - root - 2017-12-08 01:42:26.296279: step 78290, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.703 sec/batch; 49h:37m:44s remains)
INFO - root - 2017-12-08 01:42:33.048881: step 78300, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 49h:36m:28s remains)
2017-12-08 01:42:33.780718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3223038 -4.316546 -4.309094 -4.306294 -4.3048711 -4.2945414 -4.2735586 -4.2596359 -4.2612748 -4.2718987 -4.2675037 -4.2398071 -4.2230973 -4.2178593 -4.2347307][-4.32186 -4.3120866 -4.2993484 -4.2868962 -4.2744436 -4.2536812 -4.2245507 -4.2110496 -4.219914 -4.2407908 -4.2533731 -4.2412176 -4.2382255 -4.2426634 -4.2627039][-4.3129616 -4.3000851 -4.2795238 -4.2508535 -4.218976 -4.1795859 -4.1339712 -4.122344 -4.14326 -4.1808338 -4.216908 -4.2273245 -4.2386265 -4.2463603 -4.2618895][-4.3028893 -4.2844591 -4.2521033 -4.201797 -4.1480846 -4.0907292 -4.0308418 -4.027411 -4.0709834 -4.1260705 -4.1830125 -4.2160311 -4.2383718 -4.2397385 -4.2397933][-4.2954488 -4.2675509 -4.2200956 -4.1492944 -4.0789022 -4.0093155 -3.936882 -3.9475744 -4.0230813 -4.0985665 -4.1727486 -4.2203059 -4.239429 -4.2204461 -4.19624][-4.2870088 -4.2517524 -4.1916318 -4.1034675 -4.0167627 -3.9282095 -3.8314071 -3.8513782 -3.9669094 -4.0698147 -4.1625185 -4.2246466 -4.2336211 -4.187675 -4.1384106][-4.2703457 -4.234673 -4.1698737 -4.0707917 -3.96763 -3.8533578 -3.7195857 -3.7461026 -3.9012065 -4.0312572 -4.1394014 -4.210114 -4.20973 -4.1448436 -4.0774965][-4.2505331 -4.217639 -4.1615286 -4.0742683 -3.98276 -3.8780913 -3.7487159 -3.779582 -3.9362836 -4.0618687 -4.156426 -4.2195 -4.203196 -4.1213064 -4.0421133][-4.2388458 -4.2193809 -4.1852069 -4.1277814 -4.0701761 -4.0091491 -3.9306698 -3.9617693 -4.0822668 -4.1762314 -4.2348175 -4.266407 -4.2270589 -4.1321435 -4.0520558][-4.2277265 -4.2265706 -4.2161093 -4.1873026 -4.1515031 -4.1153855 -4.0701642 -4.1001997 -4.194932 -4.2671447 -4.3027887 -4.3135424 -4.263494 -4.1740627 -4.1110635][-4.2134333 -4.22003 -4.2220731 -4.2096405 -4.187211 -4.1673937 -4.1386228 -4.1578536 -4.2313676 -4.293735 -4.3232875 -4.3328385 -4.2935495 -4.2252355 -4.1839051][-4.2123775 -4.2152295 -4.2176752 -4.2133503 -4.2000165 -4.1930609 -4.180481 -4.191493 -4.2419977 -4.2872329 -4.3122735 -4.3242555 -4.3013 -4.2550125 -4.2303104][-4.2356997 -4.2324305 -4.232111 -4.2310166 -4.2226119 -4.2211928 -4.2227969 -4.232245 -4.2606783 -4.2846856 -4.3017197 -4.3101921 -4.2948127 -4.2616162 -4.2431445][-4.2712584 -4.2650719 -4.2627578 -4.2598066 -4.2483959 -4.2423005 -4.2461357 -4.2554488 -4.270277 -4.2828455 -4.2925992 -4.2958226 -4.2843237 -4.2613535 -4.2512984][-4.2931323 -4.2895164 -4.2872305 -4.2819452 -4.2673736 -4.2571988 -4.2594142 -4.2644181 -4.2727995 -4.2811222 -4.28833 -4.2898197 -4.2841449 -4.2739263 -4.2720737]]...]
INFO - root - 2017-12-08 01:42:40.437235: step 78310, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 49h:33m:24s remains)
INFO - root - 2017-12-08 01:42:47.414022: step 78320, loss = 2.08, batch loss = 2.02 (11.2 examples/sec; 0.715 sec/batch; 50h:28m:28s remains)
INFO - root - 2017-12-08 01:42:54.171231: step 78330, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 48h:08m:45s remains)
INFO - root - 2017-12-08 01:43:01.102549: step 78340, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.700 sec/batch; 49h:25m:40s remains)
INFO - root - 2017-12-08 01:43:08.007074: step 78350, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.687 sec/batch; 48h:31m:41s remains)
INFO - root - 2017-12-08 01:43:14.867742: step 78360, loss = 2.03, batch loss = 1.97 (11.2 examples/sec; 0.712 sec/batch; 50h:14m:39s remains)
INFO - root - 2017-12-08 01:43:21.691245: step 78370, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.693 sec/batch; 48h:56m:33s remains)
INFO - root - 2017-12-08 01:43:28.460667: step 78380, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.703 sec/batch; 49h:37m:13s remains)
INFO - root - 2017-12-08 01:43:35.178130: step 78390, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.625 sec/batch; 44h:07m:45s remains)
INFO - root - 2017-12-08 01:43:41.882395: step 78400, loss = 2.04, batch loss = 1.98 (11.1 examples/sec; 0.719 sec/batch; 50h:44m:18s remains)
2017-12-08 01:43:42.613488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3117461 -4.2684212 -4.1853714 -4.1043091 -4.0616531 -4.0643129 -4.0987325 -4.1457915 -4.181345 -4.2061272 -4.2375383 -4.270371 -4.2821488 -4.2693453 -4.2431321][-4.2942667 -4.2306995 -4.1282525 -4.036212 -3.9921169 -4.005353 -4.0540786 -4.1180449 -4.1641111 -4.1956253 -4.2359438 -4.2769356 -4.2923188 -4.2780929 -4.250144][-4.2639818 -4.1814375 -4.0709543 -3.9822648 -3.9472935 -3.9711728 -4.026288 -4.09623 -4.1499043 -4.188015 -4.230103 -4.2712517 -4.2904348 -4.2806892 -4.2571268][-4.2304792 -4.1394162 -4.0371189 -3.9664493 -3.9446976 -3.9693234 -4.0215855 -4.0896435 -4.1487012 -4.1912255 -4.2324882 -4.2720804 -4.2944059 -4.2903228 -4.2717333][-4.2005076 -4.1142945 -4.03714 -3.9931 -3.9773684 -3.9910877 -4.0306044 -4.0925193 -4.153717 -4.1987276 -4.2367549 -4.2745018 -4.299778 -4.3027673 -4.2912722][-4.1810966 -4.1130571 -4.063292 -4.0379248 -4.0235105 -4.0228567 -4.0457525 -4.0971508 -4.1568432 -4.2028952 -4.2387495 -4.2746382 -4.3002567 -4.3092871 -4.3057661][-4.1687365 -4.1279492 -4.1018147 -4.085907 -4.0672793 -4.0524268 -4.0584693 -4.0944638 -4.1441159 -4.188046 -4.2262154 -4.2621942 -4.2872481 -4.2968369 -4.295733][-4.1687918 -4.1520677 -4.1403184 -4.126853 -4.1033316 -4.0788851 -4.0725479 -4.0922365 -4.1279011 -4.1692514 -4.2110391 -4.2459497 -4.2660141 -4.2707429 -4.2659092][-4.1773281 -4.1749945 -4.1704893 -4.1594005 -4.1351891 -4.10707 -4.0923767 -4.0982018 -4.1219234 -4.156693 -4.1957331 -4.2269626 -4.2425518 -4.2439733 -4.2365012][-4.20399 -4.2075186 -4.2052836 -4.1963692 -4.1779633 -4.1548715 -4.1391754 -4.1387491 -4.1542249 -4.1798635 -4.20882 -4.23104 -4.241087 -4.2395186 -4.2319269][-4.2394156 -4.2442012 -4.2439008 -4.2388387 -4.2272282 -4.2112894 -4.19956 -4.1988344 -4.2100029 -4.2275486 -4.2455664 -4.2575879 -4.2615676 -4.25903 -4.2537384][-4.2826586 -4.286037 -4.2858038 -4.28276 -4.2759881 -4.2669778 -4.2602105 -4.259583 -4.2662158 -4.2766695 -4.2870207 -4.2941465 -4.2963028 -4.2935557 -4.2886028][-4.3212552 -4.3229823 -4.3228655 -4.3209352 -4.3172679 -4.3125014 -4.3085952 -4.3073134 -4.3099055 -4.3153186 -4.3216591 -4.3268194 -4.3287129 -4.3263812 -4.3219857][-4.3424644 -4.3427281 -4.3415589 -4.3398833 -4.3378749 -4.3356881 -4.3339133 -4.3329172 -4.3334064 -4.33551 -4.3392019 -4.3430839 -4.3451104 -4.3438258 -4.3401628][-4.3423996 -4.3415909 -4.3412094 -4.3409433 -4.3406692 -4.3402567 -4.3395753 -4.3389893 -4.3389692 -4.3398662 -4.3417068 -4.3436651 -4.3447285 -4.3441825 -4.3421688]]...]
INFO - root - 2017-12-08 01:43:49.223079: step 78410, loss = 2.09, batch loss = 2.03 (12.5 examples/sec; 0.638 sec/batch; 45h:01m:45s remains)
INFO - root - 2017-12-08 01:43:56.100701: step 78420, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.667 sec/batch; 47h:05m:27s remains)
INFO - root - 2017-12-08 01:44:03.063884: step 78430, loss = 2.09, batch loss = 2.03 (11.2 examples/sec; 0.716 sec/batch; 50h:32m:12s remains)
INFO - root - 2017-12-08 01:44:09.982747: step 78440, loss = 2.10, batch loss = 2.04 (11.6 examples/sec; 0.687 sec/batch; 48h:31m:03s remains)
INFO - root - 2017-12-08 01:44:16.841369: step 78450, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.664 sec/batch; 46h:50m:41s remains)
INFO - root - 2017-12-08 01:44:23.668921: step 78460, loss = 2.08, batch loss = 2.02 (12.1 examples/sec; 0.661 sec/batch; 46h:37m:44s remains)
INFO - root - 2017-12-08 01:44:30.434469: step 78470, loss = 2.06, batch loss = 2.00 (11.8 examples/sec; 0.678 sec/batch; 47h:51m:40s remains)
INFO - root - 2017-12-08 01:44:37.248788: step 78480, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.705 sec/batch; 49h:46m:22s remains)
INFO - root - 2017-12-08 01:44:44.152655: step 78490, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.709 sec/batch; 50h:01m:40s remains)
INFO - root - 2017-12-08 01:44:51.014854: step 78500, loss = 2.06, batch loss = 2.01 (11.7 examples/sec; 0.685 sec/batch; 48h:19m:26s remains)
2017-12-08 01:44:51.813486: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2726212 -4.2568493 -4.2431316 -4.2336454 -4.2260976 -4.2227006 -4.2214203 -4.2231765 -4.2243981 -4.2215695 -4.2128377 -4.2024193 -4.1952376 -4.1918721 -4.199749][-4.2657905 -4.2500839 -4.2358937 -4.2285995 -4.2230778 -4.2194991 -4.2142043 -4.2201376 -4.2259951 -4.2216806 -4.2072453 -4.1915522 -4.1804857 -4.175591 -4.1866307][-4.2675366 -4.2504373 -4.2325149 -4.226336 -4.2194262 -4.2108803 -4.199966 -4.2097006 -4.2185 -4.2140079 -4.1974378 -4.1801562 -4.1654863 -4.1592174 -4.1745791][-4.2576632 -4.2388787 -4.2176213 -4.2081389 -4.1970854 -4.1848969 -4.1709433 -4.1836905 -4.1965475 -4.1909151 -4.1741562 -4.1580806 -4.1435795 -4.1413093 -4.16037][-4.2362051 -4.2181387 -4.1970091 -4.1855607 -4.167397 -4.1494918 -4.1341844 -4.150713 -4.1666589 -4.1586285 -4.1422615 -4.1329942 -4.1256313 -4.1316366 -4.14821][-4.2016912 -4.1810045 -4.1593351 -4.1439819 -4.1204495 -4.094532 -4.0718746 -4.0924954 -4.1146035 -4.1118264 -4.1036825 -4.1064978 -4.108758 -4.1235867 -4.1413083][-4.1574969 -4.1247663 -4.0974689 -4.0736 -4.0386758 -3.9971859 -3.9597063 -3.9909575 -4.0317411 -4.0461936 -4.0526729 -4.0722642 -4.0906105 -4.1136045 -4.1299253][-4.140039 -4.0920577 -4.0546527 -4.0216413 -3.97663 -3.9216943 -3.8742011 -3.9187894 -3.9762397 -4.0084176 -4.0269308 -4.0579939 -4.0872712 -4.112175 -4.1279416][-4.1876259 -4.1463041 -4.1081414 -4.0728955 -4.0343122 -3.9898255 -3.9459274 -3.97967 -4.0247355 -4.0557113 -4.0743828 -4.101635 -4.1310234 -4.1535573 -4.1723452][-4.2625403 -4.2412252 -4.2117996 -4.180666 -4.149168 -4.1151571 -4.0780249 -4.0968046 -4.1281867 -4.1513777 -4.1644239 -4.18314 -4.2056947 -4.2209892 -4.2372303][-4.314353 -4.3114634 -4.2989984 -4.2779384 -4.253418 -4.2272344 -4.1971159 -4.207016 -4.2292032 -4.2471232 -4.2555184 -4.262713 -4.2744088 -4.2803106 -4.2927542][-4.32987 -4.3379622 -4.3380632 -4.3289986 -4.3130546 -4.295928 -4.2772493 -4.2844853 -4.2987337 -4.311563 -4.3129649 -4.3113365 -4.3150711 -4.3177838 -4.3283215][-4.3277774 -4.3358526 -4.3403392 -4.3395529 -4.3328748 -4.3237457 -4.3146591 -4.3215475 -4.3327456 -4.3423014 -4.341609 -4.3350229 -4.3328085 -4.33389 -4.3408365][-4.31922 -4.3246012 -4.3290076 -4.3314338 -4.3302469 -4.326611 -4.3231235 -4.3276095 -4.3338313 -4.3379645 -4.3362479 -4.3304524 -4.3277974 -4.3277984 -4.3301525][-4.3190775 -4.3228159 -4.3254695 -4.3273125 -4.326827 -4.3250594 -4.3236241 -4.325295 -4.3268151 -4.3270583 -4.3253765 -4.3229513 -4.3226128 -4.3236609 -4.3253288]]...]
INFO - root - 2017-12-08 01:44:58.472965: step 78510, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.712 sec/batch; 50h:12m:47s remains)
INFO - root - 2017-12-08 01:45:05.260897: step 78520, loss = 2.11, batch loss = 2.05 (11.4 examples/sec; 0.701 sec/batch; 49h:27m:36s remains)
INFO - root - 2017-12-08 01:45:12.123919: step 78530, loss = 2.07, batch loss = 2.02 (12.5 examples/sec; 0.642 sec/batch; 45h:17m:28s remains)
INFO - root - 2017-12-08 01:45:19.037012: step 78540, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.714 sec/batch; 50h:21m:35s remains)
INFO - root - 2017-12-08 01:45:25.809963: step 78550, loss = 2.04, batch loss = 1.98 (11.4 examples/sec; 0.702 sec/batch; 49h:31m:56s remains)
INFO - root - 2017-12-08 01:45:32.699695: step 78560, loss = 2.04, batch loss = 1.98 (11.7 examples/sec; 0.687 sec/batch; 48h:26m:01s remains)
INFO - root - 2017-12-08 01:45:39.534850: step 78570, loss = 2.05, batch loss = 1.99 (12.5 examples/sec; 0.640 sec/batch; 45h:07m:16s remains)
INFO - root - 2017-12-08 01:45:46.459370: step 78580, loss = 2.05, batch loss = 1.99 (12.1 examples/sec; 0.661 sec/batch; 46h:35m:22s remains)
INFO - root - 2017-12-08 01:45:53.344093: step 78590, loss = 2.06, batch loss = 2.00 (11.5 examples/sec; 0.696 sec/batch; 49h:04m:27s remains)
INFO - root - 2017-12-08 01:46:00.255676: step 78600, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.674 sec/batch; 47h:30m:13s remains)
2017-12-08 01:46:00.952311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2282577 -4.2310381 -4.2317924 -4.2373638 -4.2430043 -4.2375093 -4.2318673 -4.2217517 -4.20917 -4.2070389 -4.2132492 -4.2335896 -4.2497406 -4.2530055 -4.2478533][-4.2338524 -4.2411337 -4.2436342 -4.2455053 -4.2424769 -4.2295704 -4.2152619 -4.1943393 -4.1764431 -4.1706438 -4.1751633 -4.1964259 -4.2152805 -4.2191205 -4.2224207][-4.2347765 -4.2414722 -4.2443643 -4.2447419 -4.236145 -4.2218647 -4.2070789 -4.1850986 -4.1727405 -4.1695423 -4.1723661 -4.1887679 -4.1995969 -4.1974969 -4.2024908][-4.2142391 -4.2084494 -4.205204 -4.20128 -4.1887589 -4.17622 -4.15922 -4.1424222 -4.1487594 -4.1624789 -4.1694622 -4.1814485 -4.1837873 -4.1727624 -4.1720915][-4.1807885 -4.1648388 -4.1572108 -4.1469159 -4.1239648 -4.1043773 -4.0775814 -4.0509057 -4.0709391 -4.1140566 -4.1383076 -4.1577134 -4.1616974 -4.1410766 -4.1346684][-4.1335564 -4.109025 -4.0921679 -4.0702281 -4.0345149 -4.0018682 -3.9503419 -3.8928151 -3.9341173 -4.0166917 -4.0660477 -4.0969329 -4.1066051 -4.0878081 -4.0802112][-4.0810146 -4.0595841 -4.0360112 -4.0031228 -3.9541912 -3.9015942 -3.8057873 -3.6960349 -3.7633419 -3.9003074 -3.9779403 -4.0178666 -4.0372477 -4.0290875 -4.0287294][-4.0995069 -4.0887604 -4.0768251 -4.0575352 -4.0199685 -3.9760468 -3.9002085 -3.8073785 -3.8532009 -3.9594946 -4.0162535 -4.046989 -4.0684066 -4.0654483 -4.0668993][-4.1764517 -4.1705589 -4.1701512 -4.1650462 -4.1477056 -4.1308765 -4.0982447 -4.0519032 -4.0776224 -4.134202 -4.1587043 -4.1707273 -4.1803966 -4.1772442 -4.1728163][-4.2560177 -4.25282 -4.2555428 -4.2565608 -4.2528458 -4.2505751 -4.23504 -4.2074056 -4.2209139 -4.2553639 -4.2672811 -4.2688847 -4.2711711 -4.2712779 -4.2648363][-4.2863569 -4.290061 -4.2950025 -4.299 -4.2956614 -4.295043 -4.2857981 -4.267776 -4.27702 -4.2982883 -4.3045669 -4.3014865 -4.3003936 -4.303597 -4.2966204][-4.2606187 -4.26551 -4.2704577 -4.2773647 -4.278059 -4.2843213 -4.287796 -4.28207 -4.2915883 -4.306417 -4.3116431 -4.3032813 -4.2968407 -4.3014293 -4.294982][-4.2187548 -4.2237878 -4.2256861 -4.2341332 -4.240994 -4.2511744 -4.2577972 -4.2600851 -4.2695417 -4.2783651 -4.2752619 -4.2616892 -4.2528472 -4.2571278 -4.2498031][-4.1849337 -4.1871758 -4.1860604 -4.1990633 -4.2157617 -4.2261872 -4.2308793 -4.2346478 -4.2406926 -4.2417812 -4.2308226 -4.214747 -4.2082205 -4.2135744 -4.204186][-4.1780524 -4.1713943 -4.1683569 -4.1846008 -4.2054791 -4.2135048 -4.2132034 -4.2114758 -4.208528 -4.2052112 -4.1926227 -4.1777663 -4.1775508 -4.1895061 -4.1861887]]...]
INFO - root - 2017-12-08 01:46:07.645329: step 78610, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.648 sec/batch; 45h:39m:59s remains)
INFO - root - 2017-12-08 01:46:14.458075: step 78620, loss = 2.05, batch loss = 1.99 (11.2 examples/sec; 0.716 sec/batch; 50h:29m:14s remains)
INFO - root - 2017-12-08 01:46:21.374243: step 78630, loss = 2.09, batch loss = 2.03 (11.0 examples/sec; 0.730 sec/batch; 51h:28m:23s remains)
INFO - root - 2017-12-08 01:46:28.169837: step 78640, loss = 2.09, batch loss = 2.03 (12.2 examples/sec; 0.655 sec/batch; 46h:10m:38s remains)
INFO - root - 2017-12-08 01:46:34.878518: step 78650, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.624 sec/batch; 43h:58m:26s remains)
INFO - root - 2017-12-08 01:46:41.654104: step 78660, loss = 2.05, batch loss = 1.99 (12.2 examples/sec; 0.658 sec/batch; 46h:22m:35s remains)
INFO - root - 2017-12-08 01:46:48.446853: step 78670, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.711 sec/batch; 50h:05m:48s remains)
INFO - root - 2017-12-08 01:46:55.210797: step 78680, loss = 2.06, batch loss = 2.01 (11.3 examples/sec; 0.709 sec/batch; 49h:57m:23s remains)
INFO - root - 2017-12-08 01:47:02.016272: step 78690, loss = 2.05, batch loss = 1.99 (12.4 examples/sec; 0.647 sec/batch; 45h:36m:44s remains)
INFO - root - 2017-12-08 01:47:08.813875: step 78700, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.645 sec/batch; 45h:29m:08s remains)
2017-12-08 01:47:09.567473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2844181 -4.2864656 -4.2881355 -4.2885308 -4.285697 -4.2806435 -4.2762947 -4.27288 -4.2725205 -4.276629 -4.2836108 -4.2898836 -4.2922869 -4.2934322 -4.2957921][-4.298542 -4.2980971 -4.2955055 -4.2894468 -4.2787485 -4.2660718 -4.2550244 -4.2472863 -4.2456346 -4.2512803 -4.2642422 -4.2804427 -4.293509 -4.3022089 -4.3088589][-4.3082867 -4.303494 -4.2943578 -4.2798944 -4.2621675 -4.2445416 -4.2306557 -4.2197628 -4.2150946 -4.220551 -4.2368274 -4.2602706 -4.2831187 -4.30062 -4.3129177][-4.3106742 -4.3011794 -4.2855487 -4.2644935 -4.2430487 -4.222611 -4.2057228 -4.1901226 -4.1797981 -4.1818695 -4.1991034 -4.2286315 -4.2600331 -4.2858129 -4.3043556][-4.3051124 -4.291873 -4.2724347 -4.2489119 -4.2254982 -4.2017741 -4.1781497 -4.1543131 -4.1357217 -4.1309438 -4.1481419 -4.1825962 -4.2218971 -4.2562456 -4.2818141][-4.2939954 -4.2787638 -4.2589121 -4.2358971 -4.2112966 -4.1825142 -4.1480236 -4.1136546 -4.087254 -4.0768356 -4.094614 -4.1339188 -4.1795263 -4.2201657 -4.2511148][-4.2810335 -4.2654519 -4.2459393 -4.2239714 -4.1984 -4.165236 -4.1217341 -4.079978 -4.0502925 -4.0395617 -4.05795 -4.0981522 -4.1475487 -4.190403 -4.2235365][-4.2727156 -4.2579379 -4.2411947 -4.2242727 -4.2029634 -4.1729031 -4.1295767 -4.0907922 -4.064671 -4.0570464 -4.0736747 -4.1053772 -4.1484637 -4.1852064 -4.21304][-4.2744045 -4.2621465 -4.2500997 -4.2403097 -4.2276325 -4.2079325 -4.1752019 -4.1473756 -4.1289992 -4.1249003 -4.1374464 -4.1578307 -4.1854844 -4.2076988 -4.2220626][-4.2854342 -4.2761211 -4.2682962 -4.2640438 -4.2600732 -4.2527847 -4.2347069 -4.2185669 -4.2065978 -4.2029581 -4.2096944 -4.2181368 -4.2277279 -4.232573 -4.2326989][-4.2972879 -4.2910123 -4.285821 -4.2854733 -4.2877355 -4.2905965 -4.2862992 -4.2809544 -4.27423 -4.2698431 -4.2698245 -4.2667618 -4.2602816 -4.2493334 -4.2388673][-4.3031254 -4.3006968 -4.2977724 -4.2985897 -4.3025122 -4.3098073 -4.3131862 -4.3144383 -4.3126426 -4.309505 -4.3057451 -4.2963858 -4.2805772 -4.261373 -4.2455873][-4.2991543 -4.3012857 -4.3013678 -4.3032188 -4.3073235 -4.3142376 -4.3206277 -4.3256974 -4.32737 -4.325954 -4.3206768 -4.3095641 -4.2920289 -4.2718978 -4.2563057][-4.2869272 -4.29379 -4.2971411 -4.3001504 -4.3038588 -4.3089042 -4.3149228 -4.3213472 -4.32498 -4.3246908 -4.3197947 -4.3105054 -4.2964587 -4.2803822 -4.2683306][-4.2749882 -4.2851019 -4.2909517 -4.2943029 -4.2967634 -4.299521 -4.3030391 -4.3073378 -4.3106275 -4.3113003 -4.3089843 -4.3045855 -4.297492 -4.2886987 -4.2814832]]...]
INFO - root - 2017-12-08 01:47:15.995338: step 78710, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.651 sec/batch; 45h:51m:52s remains)
INFO - root - 2017-12-08 01:47:22.770206: step 78720, loss = 2.07, batch loss = 2.01 (12.4 examples/sec; 0.647 sec/batch; 45h:34m:33s remains)
INFO - root - 2017-12-08 01:47:29.525571: step 78730, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.676 sec/batch; 47h:39m:33s remains)
INFO - root - 2017-12-08 01:47:36.452384: step 78740, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.757 sec/batch; 53h:19m:50s remains)
INFO - root - 2017-12-08 01:47:43.216379: step 78750, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.686 sec/batch; 48h:21m:46s remains)
INFO - root - 2017-12-08 01:47:50.066018: step 78760, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 46h:12m:33s remains)
INFO - root - 2017-12-08 01:47:56.920570: step 78770, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.655 sec/batch; 46h:11m:41s remains)
INFO - root - 2017-12-08 01:48:03.633086: step 78780, loss = 2.06, batch loss = 2.00 (11.9 examples/sec; 0.673 sec/batch; 47h:27m:17s remains)
INFO - root - 2017-12-08 01:48:10.467303: step 78790, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.699 sec/batch; 49h:15m:40s remains)
INFO - root - 2017-12-08 01:48:17.231148: step 78800, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 48h:06m:43s remains)
2017-12-08 01:48:18.097757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2640529 -4.2298512 -4.172071 -4.1278296 -4.124599 -4.156621 -4.1910825 -4.2087789 -4.2117229 -4.1984682 -4.1833253 -4.1692657 -4.1639853 -4.1666689 -4.1749592][-4.2781296 -4.2552629 -4.1991835 -4.1443977 -4.1211643 -4.143084 -4.1784735 -4.1961555 -4.1981525 -4.18841 -4.1755004 -4.1645803 -4.1577516 -4.1584725 -4.1667261][-4.2628603 -4.2595491 -4.2226653 -4.1804342 -4.1492867 -4.1500311 -4.1641135 -4.1672006 -4.1618757 -4.1545506 -4.1554275 -4.162529 -4.1626406 -4.1694207 -4.1813378][-4.2216907 -4.2351837 -4.2265744 -4.2033534 -4.1777129 -4.1624613 -4.1469259 -4.1285014 -4.1166949 -4.1167107 -4.133966 -4.167069 -4.1852908 -4.1972704 -4.2098508][-4.1832657 -4.1906333 -4.1929846 -4.1773968 -4.1610475 -4.135653 -4.0965872 -4.0656166 -4.0551939 -4.0686154 -4.1053572 -4.1618156 -4.1987424 -4.2160339 -4.2260933][-4.1654234 -4.1528263 -4.1550312 -4.13845 -4.1200185 -4.0812178 -4.0167942 -3.9862745 -3.9948118 -4.0223808 -4.070209 -4.1402512 -4.1864452 -4.2097759 -4.2241449][-4.1876016 -4.1568384 -4.1536169 -4.1373615 -4.1099796 -4.0621271 -3.981478 -3.9420989 -3.9665594 -4.0038023 -4.0472846 -4.1118212 -4.1608377 -4.1903405 -4.2103491][-4.224474 -4.1892672 -4.1780229 -4.1656914 -4.1422 -4.0995431 -4.0196376 -3.9724579 -3.9942744 -4.0300007 -4.0643306 -4.1109419 -4.1554017 -4.1867809 -4.2054157][-4.2531562 -4.2274971 -4.2186651 -4.2144761 -4.199532 -4.1654019 -4.095098 -4.0400114 -4.0496244 -4.08189 -4.1114326 -4.1428623 -4.1748018 -4.1976981 -4.2090554][-4.2646775 -4.2491016 -4.2476592 -4.2510309 -4.249002 -4.2301884 -4.1790195 -4.1235433 -4.1142716 -4.134552 -4.156311 -4.1748061 -4.195354 -4.2094398 -4.2163129][-4.2570724 -4.2539663 -4.260159 -4.2691641 -4.2736712 -4.2692823 -4.245822 -4.2064581 -4.1870489 -4.191175 -4.1976619 -4.201664 -4.2126579 -4.2216592 -4.2309108][-4.22055 -4.2301483 -4.2455235 -4.2620091 -4.2745714 -4.2786126 -4.2751446 -4.257102 -4.2391777 -4.23579 -4.2298956 -4.22036 -4.22105 -4.2256341 -4.2341642][-4.1844492 -4.196516 -4.212729 -4.23133 -4.24932 -4.2610846 -4.2696643 -4.2676125 -4.2552705 -4.2484469 -4.2402878 -4.2258182 -4.2198358 -4.2215662 -4.2269239][-4.1729159 -4.180583 -4.1909018 -4.204761 -4.2199435 -4.2327356 -4.2442937 -4.2502289 -4.2441425 -4.2372761 -4.2281761 -4.2125254 -4.2043786 -4.2059731 -4.2100816][-4.1673036 -4.169394 -4.1732879 -4.1827025 -4.1945014 -4.2064667 -4.2192655 -4.2311382 -4.2306967 -4.2220836 -4.2083082 -4.1894455 -4.1807165 -4.1819782 -4.1893373]]...]
INFO - root - 2017-12-08 01:48:24.643075: step 78810, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.683 sec/batch; 48h:09m:34s remains)
INFO - root - 2017-12-08 01:48:31.412299: step 78820, loss = 2.04, batch loss = 1.98 (11.8 examples/sec; 0.677 sec/batch; 47h:42m:26s remains)
INFO - root - 2017-12-08 01:48:38.177989: step 78830, loss = 2.05, batch loss = 1.99 (11.4 examples/sec; 0.700 sec/batch; 49h:20m:24s remains)
INFO - root - 2017-12-08 01:48:44.977485: step 78840, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.654 sec/batch; 46h:03m:11s remains)
INFO - root - 2017-12-08 01:48:51.788469: step 78850, loss = 2.08, batch loss = 2.02 (12.0 examples/sec; 0.665 sec/batch; 46h:51m:00s remains)
INFO - root - 2017-12-08 01:48:58.583320: step 78860, loss = 2.03, batch loss = 1.97 (11.4 examples/sec; 0.704 sec/batch; 49h:36m:56s remains)
INFO - root - 2017-12-08 01:49:05.336234: step 78870, loss = 2.06, batch loss = 2.00 (11.2 examples/sec; 0.714 sec/batch; 50h:20m:07s remains)
INFO - root - 2017-12-08 01:49:12.095154: step 78880, loss = 2.05, batch loss = 1.99 (11.9 examples/sec; 0.673 sec/batch; 47h:24m:03s remains)
INFO - root - 2017-12-08 01:49:18.889773: step 78890, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.651 sec/batch; 45h:49m:35s remains)
INFO - root - 2017-12-08 01:49:25.792920: step 78900, loss = 2.06, batch loss = 2.00 (11.7 examples/sec; 0.682 sec/batch; 48h:02m:42s remains)
2017-12-08 01:49:26.600459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1912451 -4.1770163 -4.1814981 -4.1700506 -4.1793532 -4.1726236 -4.1649456 -4.1681709 -4.161428 -4.1680341 -4.1642613 -4.1611686 -4.1587682 -4.1555395 -4.1623821][-4.2054181 -4.1884546 -4.1875997 -4.1740284 -4.1858616 -4.1857262 -4.1792154 -4.1861973 -4.1831441 -4.1889768 -4.1837358 -4.1772633 -4.1785455 -4.1807609 -4.1900659][-4.1910524 -4.1684179 -4.160604 -4.1425409 -4.1576977 -4.1694665 -4.1697335 -4.1831555 -4.1853213 -4.1899858 -4.1827946 -4.1726313 -4.1738691 -4.1743393 -4.1839266][-4.1680446 -4.1413679 -4.124475 -4.1039133 -4.1225176 -4.1464949 -4.1564302 -4.1724553 -4.1749096 -4.1773157 -4.1702609 -4.1622725 -4.1610436 -4.1577549 -4.1634297][-4.1394286 -4.10765 -4.0825653 -4.0640531 -4.0861855 -4.1239882 -4.1444678 -4.1591411 -4.1606426 -4.1583657 -4.155457 -4.1505589 -4.1492424 -4.1452522 -4.1424708][-4.1195993 -4.0853448 -4.06413 -4.0537639 -4.0780883 -4.1207027 -4.1450577 -4.1563754 -4.1556077 -4.1446648 -4.1436157 -4.1414275 -4.1414919 -4.1387811 -4.1315374][-4.095818 -4.0645933 -4.0575151 -4.0628181 -4.0902486 -4.1345081 -4.1578274 -4.163887 -4.1541429 -4.1331348 -4.1298246 -4.1247072 -4.1291723 -4.1281986 -4.1207643][-4.0662608 -4.0396996 -4.0501537 -4.0747294 -4.1057091 -4.1510744 -4.1748552 -4.1757288 -4.1583128 -4.1269426 -4.1152554 -4.104413 -4.1102419 -4.1116819 -4.1064363][-4.0697613 -4.0467305 -4.0650477 -4.0955563 -4.1253376 -4.1673241 -4.1923394 -4.1891937 -4.1682777 -4.135644 -4.1213017 -4.1078916 -4.1128116 -4.1141319 -4.1108494][-4.1262221 -4.107233 -4.11962 -4.1376595 -4.1535249 -4.1830292 -4.2053418 -4.204247 -4.1869354 -4.1592617 -4.1503062 -4.1434174 -4.1493073 -4.1513166 -4.1492805][-4.1927657 -4.1824226 -4.1866288 -4.1879644 -4.1846724 -4.1941319 -4.2068849 -4.2081232 -4.1958714 -4.1787605 -4.1792569 -4.183042 -4.1920576 -4.1943483 -4.1912208][-4.239037 -4.2330427 -4.2324362 -4.2230253 -4.2076511 -4.201992 -4.2052364 -4.2072968 -4.1996479 -4.1919484 -4.20255 -4.2143016 -4.2231812 -4.2263541 -4.2210979][-4.2601438 -4.2547112 -4.2540636 -4.2433519 -4.228734 -4.2182751 -4.2166252 -4.2194343 -4.2143993 -4.211019 -4.2252154 -4.2372661 -4.2460294 -4.2511759 -4.2474489][-4.2613134 -4.2572474 -4.2609611 -4.2560911 -4.2493854 -4.2433481 -4.2399607 -4.2424788 -4.2381 -4.2376556 -4.2495279 -4.2547355 -4.2594976 -4.2614875 -4.2569523][-4.2409959 -4.2394962 -4.2470794 -4.2509403 -4.252809 -4.2530346 -4.24988 -4.2520761 -4.2513337 -4.2533588 -4.2611208 -4.2592278 -4.257853 -4.2528129 -4.2462239]]...]
INFO - root - 2017-12-08 01:49:33.186338: step 78910, loss = 2.03, batch loss = 1.98 (11.5 examples/sec; 0.695 sec/batch; 48h:57m:35s remains)
INFO - root - 2017-12-08 01:49:39.953228: step 78920, loss = 2.04, batch loss = 1.98 (12.6 examples/sec; 0.633 sec/batch; 44h:35m:32s remains)
INFO - root - 2017-12-08 01:49:46.718178: step 78930, loss = 2.09, batch loss = 2.03 (12.3 examples/sec; 0.651 sec/batch; 45h:49m:31s remains)
INFO - root - 2017-12-08 01:49:53.599301: step 78940, loss = 2.06, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 50h:51m:16s remains)
INFO - root - 2017-12-08 01:50:00.535721: step 78950, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 49h:56m:45s remains)
INFO - root - 2017-12-08 01:50:07.372340: step 78960, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 48h:10m:00s remains)
INFO - root - 2017-12-08 01:50:14.113027: step 78970, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 46h:01m:08s remains)
INFO - root - 2017-12-08 01:50:20.915980: step 78980, loss = 2.11, batch loss = 2.05 (12.2 examples/sec; 0.656 sec/batch; 46h:13m:45s remains)
INFO - root - 2017-12-08 01:50:27.761042: step 78990, loss = 2.05, batch loss = 2.00 (11.4 examples/sec; 0.701 sec/batch; 49h:21m:56s remains)
INFO - root - 2017-12-08 01:50:34.533610: step 79000, loss = 2.08, batch loss = 2.02 (11.5 examples/sec; 0.695 sec/batch; 48h:56m:01s remains)
2017-12-08 01:50:35.194391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2271385 -4.233511 -4.2392268 -4.24127 -4.2408838 -4.2433171 -4.2538691 -4.2572622 -4.2645097 -4.2747269 -4.2748594 -4.2724833 -4.2809186 -4.2979379 -4.3189034][-4.2133489 -4.2206645 -4.2229867 -4.2229524 -4.2188158 -4.221416 -4.2313323 -4.2338023 -4.2470322 -4.2674308 -4.2726173 -4.2673264 -4.27371 -4.2916284 -4.3158069][-4.1827817 -4.1914921 -4.1981239 -4.2033381 -4.199059 -4.1948423 -4.1985016 -4.1973429 -4.2167397 -4.2510376 -4.2637906 -4.2610993 -4.2690487 -4.2874722 -4.3132019][-4.1487432 -4.1621466 -4.1793346 -4.1944685 -4.1979809 -4.1814275 -4.1664987 -4.1573229 -4.1836386 -4.2312155 -4.2538428 -4.2598453 -4.2713113 -4.2896595 -4.313045][-4.1288357 -4.1523905 -4.1738138 -4.1941171 -4.2027636 -4.17651 -4.1362562 -4.1132779 -4.147912 -4.2132597 -4.2477279 -4.26064 -4.2762828 -4.2944093 -4.3148828][-4.105782 -4.1390119 -4.1662378 -4.1840558 -4.1904278 -4.1585584 -4.0999727 -4.063674 -4.1075683 -4.1923213 -4.2398243 -4.2613068 -4.2811747 -4.2999258 -4.3182473][-4.0882 -4.1244378 -4.1579032 -4.1688771 -4.1643414 -4.1257424 -4.0589166 -4.0139289 -4.0686975 -4.1733603 -4.2295094 -4.2576528 -4.2832155 -4.3041935 -4.3218656][-4.07378 -4.0999513 -4.1311579 -4.1371913 -4.1267161 -4.0862684 -4.0141292 -3.9600196 -4.0188708 -4.1391554 -4.2049975 -4.2416844 -4.2761765 -4.3026872 -4.3225513][-4.0507064 -4.0579548 -4.0836387 -4.0916739 -4.0903945 -4.0559435 -3.9841428 -3.9247956 -3.9782205 -4.1014705 -4.1726913 -4.2176018 -4.261106 -4.2955089 -4.320117][-4.055656 -4.0425 -4.059227 -4.0709658 -4.086123 -4.0642514 -4.0063291 -3.95888 -3.9994614 -4.10412 -4.1657763 -4.2057137 -4.2496314 -4.2879071 -4.31614][-4.0933104 -4.0699224 -4.0768423 -4.0920553 -4.1175184 -4.1109228 -4.0767384 -4.0492253 -4.0775809 -4.1516719 -4.1942725 -4.2178655 -4.2506361 -4.2851644 -4.3130584][-4.1476984 -4.1268997 -4.1285596 -4.1403246 -4.16524 -4.1704473 -4.1561236 -4.1459332 -4.1687765 -4.2185445 -4.2404575 -4.2450085 -4.2625103 -4.2891183 -4.31429][-4.1998487 -4.18792 -4.1919169 -4.1986551 -4.2121854 -4.2214212 -4.2230778 -4.2255692 -4.2480755 -4.283268 -4.2894711 -4.27816 -4.2815332 -4.2998848 -4.3216319][-4.2412453 -4.2378187 -4.2448506 -4.2468338 -4.2489281 -4.2540421 -4.2602286 -4.2673411 -4.2892332 -4.3182569 -4.3172512 -4.3012724 -4.3006458 -4.3141403 -4.3316646][-4.2799125 -4.279047 -4.2835865 -4.2829146 -4.2792196 -4.2810349 -4.285377 -4.2922025 -4.3118429 -4.3364391 -4.3330526 -4.3177342 -4.3170362 -4.3270588 -4.339467]]...]
INFO - root - 2017-12-08 01:50:41.835532: step 79010, loss = 2.06, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 47h:24m:20s remains)
INFO - root - 2017-12-08 01:50:48.613579: step 79020, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 49h:24m:40s remains)
INFO - root - 2017-12-08 01:50:55.580356: step 79030, loss = 2.07, batch loss = 2.01 (12.2 examples/sec; 0.656 sec/batch; 46h:13m:20s remains)
INFO - root - 2017-12-08 01:51:02.335826: step 79040, loss = 2.06, batch loss = 2.00 (12.7 examples/sec; 0.628 sec/batch; 44h:14m:01s remains)
INFO - root - 2017-12-08 01:51:09.188148: step 79050, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.724 sec/batch; 50h:57m:36s remains)
INFO - root - 2017-12-08 01:51:16.024108: step 79060, loss = 2.05, batch loss = 2.00 (11.1 examples/sec; 0.722 sec/batch; 50h:47m:49s remains)
INFO - root - 2017-12-08 01:51:22.772500: step 79070, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.692 sec/batch; 48h:40m:57s remains)
INFO - root - 2017-12-08 01:51:29.518335: step 79080, loss = 2.09, batch loss = 2.03 (13.3 examples/sec; 0.602 sec/batch; 42h:24m:12s remains)
INFO - root - 2017-12-08 01:51:36.382273: step 79090, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.666 sec/batch; 46h:54m:53s remains)
INFO - root - 2017-12-08 01:51:43.215591: step 79100, loss = 2.06, batch loss = 2.00 (11.0 examples/sec; 0.727 sec/batch; 51h:09m:07s remains)
2017-12-08 01:51:43.957482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2794361 -4.2675695 -4.2580943 -4.2579789 -4.2565002 -4.2489877 -4.2364244 -4.218451 -4.2067513 -4.21193 -4.2243981 -4.2185125 -4.20547 -4.2027178 -4.2094092][-4.2906027 -4.2781119 -4.2712164 -4.2742867 -4.2748556 -4.2662339 -4.24681 -4.2210517 -4.2047334 -4.2141061 -4.2383385 -4.2427392 -4.235178 -4.2301559 -4.2320461][-4.28977 -4.282198 -4.2806072 -4.28362 -4.2814178 -4.2687926 -4.2455926 -4.2176061 -4.2043371 -4.2215238 -4.254468 -4.2683344 -4.2656293 -4.2572308 -4.25371][-4.2918739 -4.287004 -4.2817588 -4.2758517 -4.2643652 -4.247715 -4.2259889 -4.2014575 -4.1945181 -4.2174439 -4.2536163 -4.2765784 -4.2793503 -4.2713413 -4.2672048][-4.2899294 -4.281055 -4.2671051 -4.2497764 -4.2297215 -4.2151442 -4.2023005 -4.1836653 -4.1795273 -4.1988235 -4.2290516 -4.2538214 -4.2629762 -4.2615809 -4.2643361][-4.2800722 -4.2662377 -4.24786 -4.22489 -4.203433 -4.196507 -4.1948271 -4.1834674 -4.1778517 -4.18284 -4.1969156 -4.2166839 -4.2304015 -4.2377014 -4.2495084][-4.24933 -4.2336621 -4.2194028 -4.2030339 -4.1924391 -4.1977868 -4.2064877 -4.2042871 -4.1983128 -4.1864862 -4.1822281 -4.1927185 -4.2062135 -4.2173662 -4.236062][-4.2032118 -4.1900473 -4.1884151 -4.1859474 -4.1892552 -4.2057629 -4.2235589 -4.2331839 -4.2303529 -4.2102013 -4.1939416 -4.1939678 -4.2022381 -4.2118096 -4.2347198][-4.1653075 -4.1629939 -4.1785192 -4.1896381 -4.2025003 -4.2246194 -4.244102 -4.2575035 -4.2572904 -4.2352943 -4.2152519 -4.2079806 -4.2103958 -4.219974 -4.2443538][-4.1438723 -4.1528668 -4.1821284 -4.2020092 -4.2193365 -4.239922 -4.2568064 -4.26911 -4.2667861 -4.2495909 -4.2364645 -4.2291894 -4.2293286 -4.2414212 -4.2653975][-4.1460843 -4.1603689 -4.1892934 -4.2080803 -4.2229023 -4.2400837 -4.25368 -4.2658381 -4.2626696 -4.2518649 -4.2489238 -4.2468824 -4.2479692 -4.2620368 -4.2851219][-4.1720533 -4.1835961 -4.2003 -4.2104168 -4.2209077 -4.234798 -4.2461829 -4.257741 -4.2579026 -4.2546449 -4.2595234 -4.2619925 -4.2638364 -4.276803 -4.2963243][-4.2056551 -4.2132735 -4.2208562 -4.2254443 -4.2331262 -4.2423997 -4.249423 -4.2583675 -4.2591424 -4.2568245 -4.2638054 -4.2693315 -4.2732263 -4.2870722 -4.30245][-4.2352037 -4.2428231 -4.2469206 -4.2504811 -4.2568173 -4.2624273 -4.2663903 -4.269815 -4.2670231 -4.2613955 -4.2645431 -4.2707705 -4.2779984 -4.2947769 -4.3089681][-4.2559958 -4.2624035 -4.2671676 -4.2740808 -4.2815952 -4.2871709 -4.2898841 -4.2876968 -4.28097 -4.2724643 -4.2693739 -4.2715626 -4.2787986 -4.2964039 -4.3102903]]...]
INFO - root - 2017-12-08 01:51:50.538188: step 79110, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.652 sec/batch; 45h:53m:24s remains)
INFO - root - 2017-12-08 01:51:57.311556: step 79120, loss = 2.04, batch loss = 1.99 (11.7 examples/sec; 0.683 sec/batch; 48h:03m:43s remains)
INFO - root - 2017-12-08 01:52:04.174528: step 79130, loss = 2.07, batch loss = 2.01 (11.4 examples/sec; 0.702 sec/batch; 49h:22m:48s remains)
INFO - root - 2017-12-08 01:52:10.886752: step 79140, loss = 2.09, batch loss = 2.03 (11.6 examples/sec; 0.689 sec/batch; 48h:27m:50s remains)
INFO - root - 2017-12-08 01:52:17.789776: step 79150, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.722 sec/batch; 50h:47m:40s remains)
INFO - root - 2017-12-08 01:52:24.470785: step 79160, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.660 sec/batch; 46h:25m:19s remains)
INFO - root - 2017-12-08 01:52:31.397992: step 79170, loss = 2.07, batch loss = 2.01 (12.1 examples/sec; 0.663 sec/batch; 46h:40m:10s remains)
INFO - root - 2017-12-08 01:52:38.261935: step 79180, loss = 2.09, batch loss = 2.03 (11.4 examples/sec; 0.703 sec/batch; 49h:28m:41s remains)
INFO - root - 2017-12-08 01:52:45.050277: step 79190, loss = 2.06, batch loss = 2.00 (11.3 examples/sec; 0.707 sec/batch; 49h:45m:02s remains)
INFO - root - 2017-12-08 01:52:51.921866: step 79200, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.705 sec/batch; 49h:34m:39s remains)
2017-12-08 01:52:52.726681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2449474 -4.237711 -4.2403669 -4.2554226 -4.2728996 -4.2743125 -4.2656808 -4.2592773 -4.2572284 -4.2535677 -4.2462811 -4.2407451 -4.2407846 -4.2441278 -4.2508707][-4.2255239 -4.2212267 -4.2259321 -4.2402329 -4.2532682 -4.2506719 -4.24067 -4.2336178 -4.2308259 -4.2280064 -4.2235866 -4.21921 -4.2189345 -4.2220592 -4.2271976][-4.2418027 -4.2420144 -4.2448654 -4.2492032 -4.2487817 -4.2352629 -4.2203403 -4.2150826 -4.216774 -4.2189388 -4.2155905 -4.2093468 -4.2084837 -4.2128415 -4.216114][-4.26563 -4.269906 -4.2667694 -4.2553229 -4.2341514 -4.2043891 -4.1856532 -4.1853952 -4.1925869 -4.1986785 -4.1933022 -4.1824908 -4.1861639 -4.2004027 -4.2097335][-4.2689266 -4.2704811 -4.258358 -4.23367 -4.1939616 -4.1525207 -4.1349845 -4.1362762 -4.146102 -4.1544375 -4.1478949 -4.1340137 -4.1461663 -4.1766319 -4.1982994][-4.2406373 -4.2325659 -4.2098227 -4.1728563 -4.1211629 -4.0730991 -4.0493574 -4.0513783 -4.0707951 -4.0867367 -4.0842781 -4.0759478 -4.0972662 -4.1449018 -4.1838226][-4.1876802 -4.1638627 -4.1282282 -4.0791926 -4.0166636 -3.9506638 -3.9102993 -3.9171968 -3.9579439 -3.9937472 -4.0091157 -4.0149007 -4.0458236 -4.108748 -4.167315][-4.1096873 -4.0619607 -4.0075717 -3.9434061 -3.8698654 -3.7859364 -3.727457 -3.7446687 -3.8167081 -3.8833575 -3.926245 -3.9524848 -3.9954033 -4.0716662 -4.14897][-4.03826 -3.9748576 -3.9111395 -3.8482068 -3.7860949 -3.7180688 -3.68155 -3.7160859 -3.8012595 -3.8762178 -3.9317856 -3.9671333 -4.01082 -4.0852442 -4.1590433][-4.0300083 -3.9748766 -3.9314029 -3.9010665 -3.8749356 -3.8469167 -3.8461545 -3.8844528 -3.9466493 -3.9966474 -4.0375605 -4.0603275 -4.0886741 -4.1414728 -4.1942205][-4.0682411 -4.038518 -4.0259666 -4.0294232 -4.0300694 -4.0215559 -4.0268679 -4.0551543 -4.0915437 -4.1197729 -4.1449871 -4.1571703 -4.16885 -4.1944313 -4.2215853][-4.12651 -4.1183786 -4.1247191 -4.1434155 -4.1554036 -4.1568227 -4.1616168 -4.1761312 -4.1913328 -4.200561 -4.2111034 -4.2123418 -4.2084103 -4.2130494 -4.2230039][-4.1775546 -4.1848421 -4.203773 -4.2269831 -4.2403746 -4.2434864 -4.2437277 -4.2433987 -4.2393775 -4.2342896 -4.2336659 -4.2265668 -4.2156119 -4.2138462 -4.2218904][-4.2170873 -4.2318692 -4.2515769 -4.26993 -4.2798972 -4.2815037 -4.279254 -4.2736921 -4.2623625 -4.2509046 -4.24588 -4.2384796 -4.2306008 -4.2316294 -4.243165][-4.2552767 -4.2680569 -4.2817259 -4.2935848 -4.2994757 -4.3008156 -4.2992511 -4.2929783 -4.2821751 -4.2725005 -4.2684493 -4.2655954 -4.2633295 -4.2671795 -4.2768893]]...]
INFO - root - 2017-12-08 01:52:59.355898: step 79210, loss = 2.09, batch loss = 2.03 (11.5 examples/sec; 0.695 sec/batch; 48h:54m:50s remains)
INFO - root - 2017-12-08 01:53:06.325671: step 79220, loss = 2.07, batch loss = 2.01 (11.5 examples/sec; 0.695 sec/batch; 48h:52m:34s remains)
INFO - root - 2017-12-08 01:53:13.102557: step 79230, loss = 2.07, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:04m:04s remains)
INFO - root - 2017-12-08 01:53:20.018419: step 79240, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.669 sec/batch; 47h:04m:56s remains)
INFO - root - 2017-12-08 01:53:26.855099: step 79250, loss = 2.06, batch loss = 2.00 (11.4 examples/sec; 0.703 sec/batch; 49h:27m:03s remains)
INFO - root - 2017-12-08 01:53:33.657511: step 79260, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.731 sec/batch; 51h:24m:38s remains)
INFO - root - 2017-12-08 01:53:40.547475: step 79270, loss = 2.07, batch loss = 2.01 (11.7 examples/sec; 0.684 sec/batch; 48h:05m:40s remains)
INFO - root - 2017-12-08 01:53:47.271410: step 79280, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.649 sec/batch; 45h:39m:17s remains)
INFO - root - 2017-12-08 01:53:54.058135: step 79290, loss = 2.06, batch loss = 2.00 (12.2 examples/sec; 0.653 sec/batch; 45h:57m:01s remains)
INFO - root - 2017-12-08 01:54:00.802443: step 79300, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.679 sec/batch; 47h:46m:05s remains)
2017-12-08 01:54:01.594204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2136583 -4.1720548 -4.1312881 -4.0980859 -4.1280351 -4.16092 -4.17761 -4.200479 -4.2121372 -4.2239113 -4.2408867 -4.2465091 -4.237792 -4.2179804 -4.1955681][-4.2549424 -4.223464 -4.1954088 -4.1733527 -4.1972656 -4.2176032 -4.2228785 -4.2319503 -4.2386937 -4.25552 -4.273541 -4.2798324 -4.2680907 -4.2450361 -4.2195191][-4.2780752 -4.2569418 -4.2395191 -4.226048 -4.2376952 -4.2396693 -4.2304654 -4.2264733 -4.2344689 -4.2573953 -4.2802496 -4.2850318 -4.2704849 -4.245883 -4.2238951][-4.2873077 -4.2730727 -4.2628841 -4.2542191 -4.2513523 -4.2340269 -4.2061524 -4.19062 -4.2057533 -4.2440305 -4.2741208 -4.2775521 -4.2589927 -4.2320213 -4.2116318][-4.2865772 -4.2720175 -4.2634554 -4.2538362 -4.2378097 -4.2021646 -4.1537204 -4.1258931 -4.1500854 -4.2062459 -4.2496228 -4.2595 -4.2458272 -4.2228346 -4.2029052][-4.2790046 -4.2605143 -4.2470436 -4.2277455 -4.1928453 -4.1286855 -4.0461597 -3.9923477 -4.0259886 -4.1107683 -4.1793933 -4.2097111 -4.2139287 -4.2004185 -4.1850624][-4.2725697 -4.2491903 -4.2257514 -4.193099 -4.1370053 -4.0450225 -3.9222019 -3.8318162 -3.876694 -3.9951177 -4.0939331 -4.1473889 -4.1684675 -4.1628079 -4.1540308][-4.2764726 -4.2534 -4.2268915 -4.1910629 -4.1358051 -4.0511289 -3.938226 -3.8533006 -3.8931704 -4.0016084 -4.0955977 -4.1471295 -4.1692591 -4.1652079 -4.1564388][-4.2886443 -4.2695322 -4.2485471 -4.219377 -4.1815848 -4.1311226 -4.0668015 -4.0211916 -4.0481505 -4.1172028 -4.1806583 -4.21345 -4.2233748 -4.2111344 -4.1941719][-4.2937512 -4.2779369 -4.26067 -4.2350497 -4.2071261 -4.1785054 -4.1509748 -4.1378922 -4.1604209 -4.2025771 -4.2434206 -4.2656488 -4.2683554 -4.2514791 -4.2323022][-4.2913318 -4.2775269 -4.2595944 -4.23378 -4.2096734 -4.189713 -4.1828508 -4.1885295 -4.2122121 -4.2443242 -4.2750998 -4.2933774 -4.2937369 -4.2777538 -4.2636933][-4.2985063 -4.2864394 -4.2675366 -4.2392211 -4.2127938 -4.1925726 -4.1928287 -4.2080154 -4.2338691 -4.2638011 -4.2946854 -4.314631 -4.31615 -4.3014355 -4.2879696][-4.3148637 -4.3032818 -4.2855577 -4.260417 -4.2351055 -4.2121506 -4.2065921 -4.2177663 -4.2400165 -4.2691107 -4.3019762 -4.3261547 -4.3321066 -4.3207188 -4.3057895][-4.3297806 -4.3169513 -4.3000007 -4.2799983 -4.2584805 -4.2366447 -4.2251763 -4.2274957 -4.2406411 -4.2633781 -4.2919106 -4.3154745 -4.3250775 -4.3191733 -4.3085046][-4.3385887 -4.3262038 -4.3113794 -4.293251 -4.2717357 -4.2488904 -4.2322159 -4.2259688 -4.2288795 -4.24215 -4.263298 -4.2822618 -4.2915683 -4.2929058 -4.2921171]]...]
INFO - root - 2017-12-08 01:54:08.060741: step 79310, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.641 sec/batch; 45h:06m:06s remains)
INFO - root - 2017-12-08 01:54:14.912732: step 79320, loss = 2.08, batch loss = 2.02 (11.9 examples/sec; 0.670 sec/batch; 47h:06m:45s remains)
INFO - root - 2017-12-08 01:54:21.613166: step 79330, loss = 2.09, batch loss = 2.03 (11.1 examples/sec; 0.720 sec/batch; 50h:37m:36s remains)
INFO - root - 2017-12-08 01:54:28.382673: step 79340, loss = 2.04, batch loss = 1.99 (11.3 examples/sec; 0.709 sec/batch; 49h:52m:12s remains)
INFO - root - 2017-12-08 01:54:35.191445: step 79350, loss = 2.04, batch loss = 1.98 (11.9 examples/sec; 0.671 sec/batch; 47h:09m:59s remains)
INFO - root - 2017-12-08 01:54:41.964258: step 79360, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 45h:43m:47s remains)
INFO - root - 2017-12-08 01:54:48.814299: step 79370, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.710 sec/batch; 49h:54m:16s remains)
INFO - root - 2017-12-08 01:54:55.702506: step 79380, loss = 2.05, batch loss = 2.00 (11.2 examples/sec; 0.716 sec/batch; 50h:19m:14s remains)
INFO - root - 2017-12-08 01:55:02.478326: step 79390, loss = 2.09, batch loss = 2.03 (11.8 examples/sec; 0.676 sec/batch; 47h:33m:41s remains)
INFO - root - 2017-12-08 01:55:09.261650: step 79400, loss = 2.08, batch loss = 2.02 (12.3 examples/sec; 0.650 sec/batch; 45h:43m:27s remains)
2017-12-08 01:55:10.005627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3067622 -4.3026853 -4.2922406 -4.2831173 -4.2778506 -4.2741876 -4.2728748 -4.2748218 -4.2794161 -4.285881 -4.2959175 -4.3059444 -4.3158584 -4.3241439 -4.3234563][-4.28768 -4.2854247 -4.2768478 -4.2673016 -4.259325 -4.2515769 -4.2452145 -4.241776 -4.2444062 -4.2541356 -4.2715449 -4.2908506 -4.3102131 -4.32476 -4.32527][-4.2745714 -4.27713 -4.2748833 -4.26785 -4.2577953 -4.24487 -4.2313519 -4.2207804 -4.2204394 -4.2303891 -4.2513 -4.2764974 -4.3021059 -4.319428 -4.3209748][-4.2723279 -4.2790747 -4.2808909 -4.2727365 -4.2566829 -4.2365003 -4.2145233 -4.1975813 -4.1938276 -4.2023544 -4.2242908 -4.2526522 -4.2837505 -4.3048682 -4.3097706][-4.2683678 -4.274838 -4.2766743 -4.2653975 -4.2443938 -4.2186522 -4.188735 -4.1638088 -4.1553774 -4.1644 -4.1879549 -4.2173524 -4.2511096 -4.2786074 -4.2918549][-4.25766 -4.2613263 -4.2622838 -4.2488112 -4.222796 -4.1926303 -4.1558123 -4.1205025 -4.1061168 -4.1191258 -4.1478457 -4.1776862 -4.2115335 -4.2465415 -4.2686706][-4.2332783 -4.2325468 -4.2317948 -4.2155962 -4.1834736 -4.1481638 -4.1069012 -4.0648046 -4.0488815 -4.0726604 -4.1124835 -4.145638 -4.1812491 -4.2223763 -4.2499185][-4.2024193 -4.2014251 -4.2007356 -4.1814194 -4.143661 -4.10388 -4.0579305 -4.01026 -3.9938507 -4.035 -4.0929308 -4.1355224 -4.1756763 -4.2177653 -4.2447667][-4.1836524 -4.1845956 -4.1809511 -4.1545229 -4.1141825 -4.0746303 -4.0338292 -3.9925644 -3.9816024 -4.0346751 -4.1029658 -4.1509366 -4.1917605 -4.2283978 -4.2511482][-4.1768889 -4.1792579 -4.1698523 -4.1378322 -4.1013618 -4.0722337 -4.0489616 -4.0300112 -4.0350232 -4.0890341 -4.1484518 -4.1872411 -4.2199521 -4.2460217 -4.2617493][-4.1737032 -4.1761484 -4.1652708 -4.1357784 -4.1068258 -4.0891948 -4.0838871 -4.0887718 -4.1130204 -4.1634641 -4.2086663 -4.2352366 -4.2556953 -4.2700467 -4.2761793][-4.1745329 -4.1749277 -4.167623 -4.1502295 -4.1349435 -4.1279197 -4.1324358 -4.1480923 -4.1797352 -4.2206721 -4.2501721 -4.269031 -4.2823553 -4.2884941 -4.2859406][-4.1891909 -4.1899476 -4.1900072 -4.1883817 -4.1891146 -4.188416 -4.1920657 -4.2011404 -4.2217507 -4.2446556 -4.2583933 -4.2724514 -4.2860994 -4.2927947 -4.2867427][-4.2146335 -4.219429 -4.2250109 -4.2347879 -4.2457986 -4.2465725 -4.2413797 -4.2347565 -4.2336054 -4.2364931 -4.2371669 -4.2516565 -4.2736239 -4.2865376 -4.2816606][-4.2311292 -4.2378807 -4.2459488 -4.2618246 -4.2770686 -4.2756305 -4.2595263 -4.23911 -4.2230611 -4.2125287 -4.2083483 -4.2281308 -4.2598162 -4.2792068 -4.2761421]]...]
INFO - root - 2017-12-08 01:55:16.657409: step 79410, loss = 2.07, batch loss = 2.01 (11.1 examples/sec; 0.718 sec/batch; 50h:28m:56s remains)
INFO - root - 2017-12-08 01:55:23.427798: step 79420, loss = 2.07, batch loss = 2.01 (11.9 examples/sec; 0.673 sec/batch; 47h:18m:54s remains)
INFO - root - 2017-12-08 01:55:30.206451: step 79430, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.623 sec/batch; 43h:45m:38s remains)
INFO - root - 2017-12-08 01:55:36.985381: step 79440, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.669 sec/batch; 47h:01m:59s remains)
INFO - root - 2017-12-08 01:55:43.800900: step 79450, loss = 2.03, batch loss = 1.97 (11.7 examples/sec; 0.683 sec/batch; 47h:59m:44s remains)
INFO - root - 2017-12-08 01:55:50.623640: step 79460, loss = 2.05, batch loss = 1.99 (11.3 examples/sec; 0.710 sec/batch; 49h:54m:12s remains)
INFO - root - 2017-12-08 01:55:57.450083: step 79470, loss = 2.05, batch loss = 1.99 (11.6 examples/sec; 0.690 sec/batch; 48h:28m:10s remains)
INFO - root - 2017-12-08 01:56:04.198054: step 79480, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.633 sec/batch; 44h:27m:20s remains)
INFO - root - 2017-12-08 01:56:11.109593: step 79490, loss = 2.06, batch loss = 2.00 (12.0 examples/sec; 0.668 sec/batch; 46h:56m:14s remains)
INFO - root - 2017-12-08 01:56:17.912580: step 79500, loss = 2.08, batch loss = 2.02 (11.6 examples/sec; 0.687 sec/batch; 48h:16m:31s remains)
2017-12-08 01:56:18.621591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3421535 -4.3404016 -4.3358684 -4.3299341 -4.324616 -4.3246517 -4.3315697 -4.341547 -4.3456564 -4.3408346 -4.3368597 -4.3343096 -4.3328333 -4.3317676 -4.3328795][-4.3259983 -4.3201389 -4.309269 -4.2973189 -4.2879982 -4.2863007 -4.2944636 -4.3079138 -4.3113356 -4.300756 -4.2953706 -4.2948513 -4.2967777 -4.2968407 -4.2988625][-4.3073068 -4.29314 -4.2737 -4.2547054 -4.2396116 -4.2337646 -4.2412958 -4.2556205 -4.2570376 -4.2438765 -4.2408648 -4.2444468 -4.24967 -4.2504368 -4.2534618][-4.2835593 -4.2607141 -4.2320523 -4.2058907 -4.1842227 -4.1693339 -4.1700149 -4.1797071 -4.1796618 -4.1727667 -4.1774025 -4.18603 -4.1948009 -4.19958 -4.2065353][-4.2657628 -4.2375946 -4.2038379 -4.1698222 -4.14034 -4.1142774 -4.1023154 -4.1038089 -4.1041121 -4.1080346 -4.1194811 -4.1307011 -4.1432157 -4.1539216 -4.1671929][-4.2542005 -4.2257576 -4.1907849 -4.1492295 -4.1066155 -4.0697017 -4.0483718 -4.0492659 -4.0558891 -4.0668526 -4.0774484 -4.0857115 -4.1018944 -4.1204977 -4.1404734][-4.2369413 -4.2069426 -4.162962 -4.10156 -4.0335021 -3.9833474 -3.9648924 -3.9841425 -4.0072384 -4.0250688 -4.0305748 -4.0306072 -4.04723 -4.0807352 -4.1152849][-4.221046 -4.18641 -4.1267586 -4.03985 -3.9453146 -3.8822966 -3.8714843 -3.9139168 -3.9546332 -3.9795022 -3.9823577 -3.9792976 -3.9988346 -4.0459356 -4.0946741][-4.2214332 -4.1879425 -4.1270733 -4.0398622 -3.9478974 -3.885479 -3.877259 -3.9261987 -3.9670634 -3.9837365 -3.9824872 -3.9822459 -4.0038218 -4.047286 -4.0905147][-4.2294879 -4.1992826 -4.1531119 -4.0898595 -4.02319 -3.9781094 -3.9723129 -4.0135918 -4.0412827 -4.0418081 -4.0340772 -4.0307722 -4.0450997 -4.074656 -4.1018496][-4.2489238 -4.2264829 -4.1954069 -4.1570282 -4.1167622 -4.093905 -4.0948882 -4.125442 -4.1383185 -4.1281738 -4.1140537 -4.105051 -4.1110573 -4.1247296 -4.1301341][-4.2710576 -4.2563815 -4.2387285 -4.2171683 -4.1920609 -4.1805186 -4.182981 -4.203876 -4.2110829 -4.2026162 -4.1896286 -4.1785836 -4.1804819 -4.1847854 -4.1754403][-4.2994156 -4.2918897 -4.2826405 -4.2680669 -4.2461686 -4.2325726 -4.2325578 -4.2500811 -4.2590456 -4.255321 -4.2441187 -4.2340679 -4.23584 -4.2387996 -4.2248974][-4.3191729 -4.3154697 -4.3106785 -4.2995458 -4.2814627 -4.266602 -4.2642903 -4.2784815 -4.2873049 -4.2851892 -4.2762971 -4.267591 -4.2677345 -4.2699223 -4.2582488][-4.3210411 -4.3169703 -4.3139677 -4.3067451 -4.2936215 -4.2828722 -4.2816238 -4.2924485 -4.3004179 -4.2995653 -4.2945166 -4.2896819 -4.2882361 -4.2887182 -4.2793632]]...]
INFO - root - 2017-12-08 01:56:25.235492: step 79510, loss = 2.07, batch loss = 2.01 (12.5 examples/sec; 0.642 sec/batch; 45h:07m:40s remains)
INFO - root - 2017-12-08 01:56:32.082377: step 79520, loss = 2.08, batch loss = 2.02 (11.8 examples/sec; 0.676 sec/batch; 47h:29m:19s remains)
