INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "73"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 10:37:05.225547: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:37:05.225618: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:37:05.225645: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:37:05.225666: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:37:05.225686: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 10:37:06.327868: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 6.39GiB
2017-12-05 10:37:06.327905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 10:37:06.327912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 10:37:06.327920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 10:37:17.062836: step 0, loss = 2.02, batch loss = 1.96 (1.1 examples/sec; 7.219 sec/batch; 666h:46m:57s remains)
2017-12-05 10:37:17.900072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2475362 -4.2294483 -4.2063861 -4.1805153 -4.167068 -4.1685991 -4.1809249 -4.1975431 -4.2054958 -4.2116189 -4.2120047 -4.1999283 -4.1732082 -4.1433821 -4.1212888][-4.2385912 -4.2243524 -4.20803 -4.1883812 -4.1745248 -4.170876 -4.1782026 -4.1879067 -4.1878281 -4.1878839 -4.1872253 -4.1780009 -4.1576648 -4.1357679 -4.1241894][-4.2257433 -4.2217016 -4.2168908 -4.2078958 -4.1970372 -4.1898956 -4.1894288 -4.1911979 -4.1886744 -4.1895227 -4.1928773 -4.1904693 -4.17754 -4.1599913 -4.15378][-4.2215748 -4.2257285 -4.2293725 -4.2255883 -4.2121706 -4.19924 -4.1940689 -4.1955214 -4.1994624 -4.2088981 -4.2182646 -4.2212124 -4.2127805 -4.1961088 -4.189517][-4.2264967 -4.2338967 -4.239192 -4.2307792 -4.2051277 -4.1835051 -4.1771059 -4.184299 -4.2010889 -4.222466 -4.2375417 -4.2417688 -4.2341785 -4.2167315 -4.2067652][-4.2178407 -4.2244673 -4.2241588 -4.2035918 -4.161725 -4.1290674 -4.1275444 -4.1499863 -4.184206 -4.218636 -4.240417 -4.2449651 -4.2344546 -4.2124982 -4.1998453][-4.1785913 -4.1804385 -4.1720309 -4.1414518 -4.0873 -4.0533061 -4.0691333 -4.1140194 -4.1597276 -4.1940131 -4.2131414 -4.2130413 -4.1976652 -4.1739383 -4.16352][-4.1289392 -4.1232362 -4.1096249 -4.0796041 -4.0320978 -4.0140462 -4.0476432 -4.1011658 -4.1399775 -4.1581907 -4.1655092 -4.1607819 -4.1437368 -4.1210175 -4.1152196][-4.123569 -4.11773 -4.1125326 -4.0979257 -4.0737567 -4.0719123 -4.0998273 -4.1338639 -4.1487861 -4.146667 -4.1442165 -4.1398311 -4.1300073 -4.1144118 -4.1099787][-4.153758 -4.1533375 -4.1574016 -4.1542587 -4.1436362 -4.1459284 -4.1606045 -4.17376 -4.17248 -4.1651945 -4.1616611 -4.1591196 -4.1539049 -4.1435509 -4.1368151][-4.1814256 -4.1807919 -4.1869597 -4.1862044 -4.17889 -4.1806569 -4.1903167 -4.1946535 -4.1907606 -4.1878786 -4.1899137 -4.187223 -4.1793504 -4.170568 -4.1621122][-4.1866932 -4.1824479 -4.186882 -4.1885042 -4.1856742 -4.1888242 -4.198617 -4.2031522 -4.2035017 -4.2041869 -4.2081614 -4.2060742 -4.1949916 -4.1838818 -4.1738157][-4.1861839 -4.1820192 -4.1855245 -4.1918044 -4.1951852 -4.1990161 -4.2043986 -4.2083383 -4.2112479 -4.2115617 -4.2123604 -4.2120485 -4.2025452 -4.1902795 -4.179172][-4.1972547 -4.1956663 -4.199944 -4.2101965 -4.2183089 -4.2230859 -4.2245564 -4.225419 -4.2277336 -4.2283525 -4.227643 -4.2271013 -4.2191443 -4.2065554 -4.1964812][-4.2202616 -4.2209821 -4.2258158 -4.2354445 -4.2441258 -4.2490387 -4.2487068 -4.2470365 -4.2477894 -4.2493234 -4.2501774 -4.2507496 -4.2448616 -4.2333951 -4.2250037]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 10:37:25.177782: step 10, loss = 2.06, batch loss = 2.01 (12.0 examples/sec; 0.668 sec/batch; 61h:42m:41s remains)
INFO - root - 2017-12-05 10:37:31.522895: step 20, loss = 2.05, batch loss = 1.99 (11.8 examples/sec; 0.681 sec/batch; 62h:50m:58s remains)
INFO - root - 2017-12-05 10:37:37.787231: step 30, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.629 sec/batch; 58h:06m:53s remains)
INFO - root - 2017-12-05 10:37:44.095173: step 40, loss = 2.09, batch loss = 2.03 (13.0 examples/sec; 0.615 sec/batch; 56h:44m:59s remains)
INFO - root - 2017-12-05 10:37:50.318639: step 50, loss = 2.08, batch loss = 2.02 (13.2 examples/sec; 0.605 sec/batch; 55h:52m:28s remains)
INFO - root - 2017-12-05 10:37:56.656261: step 60, loss = 2.10, batch loss = 2.04 (12.7 examples/sec; 0.629 sec/batch; 58h:03m:35s remains)
INFO - root - 2017-12-05 10:38:03.010560: step 70, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 56h:58m:04s remains)
INFO - root - 2017-12-05 10:38:09.278173: step 80, loss = 2.05, batch loss = 1.99 (13.0 examples/sec; 0.617 sec/batch; 56h:56m:36s remains)
INFO - root - 2017-12-05 10:38:15.566248: step 90, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.631 sec/batch; 58h:18m:22s remains)
INFO - root - 2017-12-05 10:38:21.868785: step 100, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.626 sec/batch; 57h:47m:37s remains)
2017-12-05 10:38:22.400018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2159419 -4.1784415 -4.1343622 -4.1016822 -4.0934582 -4.108871 -4.1326303 -4.1610279 -4.199441 -4.2213736 -4.2225218 -4.223814 -4.2200351 -4.1981797 -4.1738415][-4.2170672 -4.1710033 -4.1162863 -4.0730228 -4.0559893 -4.0699239 -4.0989323 -4.1302557 -4.1661038 -4.1903324 -4.1989794 -4.2091451 -4.2128148 -4.1987047 -4.1806669][-4.2198682 -4.1680741 -4.107995 -4.06211 -4.0424104 -4.054522 -4.0816741 -4.1095686 -4.1418457 -4.1689372 -4.1812797 -4.1934276 -4.2003736 -4.19269 -4.1805081][-4.2226872 -4.169735 -4.1108484 -4.0666318 -4.047195 -4.0552435 -4.0767927 -4.1002755 -4.1309533 -4.1620727 -4.1788831 -4.190135 -4.1979408 -4.1941538 -4.1841092][-4.2249737 -4.1720424 -4.1158533 -4.0728669 -4.0502968 -4.0499239 -4.0658045 -4.0872278 -4.1185341 -4.1535869 -4.1767955 -4.1886806 -4.19726 -4.1992936 -4.1923389][-4.2259231 -4.175324 -4.1221251 -4.0798793 -4.0526414 -4.0431724 -4.0520582 -4.0670705 -4.0893712 -4.1220489 -4.152905 -4.1705456 -4.1844115 -4.19352 -4.1923456][-4.2255058 -4.1783776 -4.1284976 -4.0858293 -4.0531826 -4.0353718 -4.0369587 -4.0392456 -4.044795 -4.0721574 -4.1115561 -4.1382 -4.1581168 -4.1738591 -4.1789031][-4.2197752 -4.1731224 -4.1218433 -4.0761819 -4.0406256 -4.0195823 -4.0186896 -4.0127826 -4.0055933 -4.0283833 -4.0736523 -4.1028252 -4.1235209 -4.1419039 -4.1507869][-4.2160769 -4.1687317 -4.1152487 -4.0679297 -4.0347347 -4.0183687 -4.0199008 -4.01442 -4.0039878 -4.0230145 -4.0627065 -4.0842185 -4.0985918 -4.1139059 -4.1215849][-4.2150583 -4.1672778 -4.1130652 -4.065877 -4.0355983 -4.0240331 -4.0279579 -4.0268965 -4.0216217 -4.0400424 -4.0703545 -4.0819454 -4.0871186 -4.0964026 -4.098794][-4.2199106 -4.1707339 -4.1166372 -4.0703363 -4.0416703 -4.0298328 -4.0328245 -4.0365067 -4.0407538 -4.0599976 -4.0815282 -4.0864482 -4.0841546 -4.0865211 -4.0842237][-4.2343645 -4.1868429 -4.1357489 -4.0932894 -4.0682907 -4.0570836 -4.057694 -4.065249 -4.0748568 -4.0914273 -4.1064086 -4.1076946 -4.1008492 -4.0990987 -4.0960531][-4.2539635 -4.2118096 -4.1670995 -4.1312408 -4.1118312 -4.103529 -4.10666 -4.1189952 -4.130774 -4.143918 -4.153903 -4.1548996 -4.1472325 -4.1425519 -4.1380978][-4.2781048 -4.2448006 -4.21021 -4.1828256 -4.1676307 -4.16258 -4.1713505 -4.1881986 -4.2006416 -4.2106419 -4.2171273 -4.2175393 -4.209341 -4.2025142 -4.1974854][-4.3029976 -4.2817039 -4.2595172 -4.2417016 -4.2295737 -4.2260532 -4.235877 -4.2516589 -4.2618904 -4.2681007 -4.2722478 -4.2716432 -4.263092 -4.255949 -4.2527952]]...]
INFO - root - 2017-12-05 10:38:28.753577: step 110, loss = 2.09, batch loss = 2.03 (12.8 examples/sec; 0.627 sec/batch; 57h:53m:28s remains)
INFO - root - 2017-12-05 10:38:34.966380: step 120, loss = 2.05, batch loss = 1.99 (13.1 examples/sec; 0.612 sec/batch; 56h:30m:38s remains)
INFO - root - 2017-12-05 10:38:41.256615: step 130, loss = 2.06, batch loss = 2.00 (12.4 examples/sec; 0.644 sec/batch; 59h:28m:30s remains)
INFO - root - 2017-12-05 10:38:47.559933: step 140, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.648 sec/batch; 59h:48m:32s remains)
INFO - root - 2017-12-05 10:38:53.847518: step 150, loss = 2.10, batch loss = 2.04 (12.2 examples/sec; 0.656 sec/batch; 60h:31m:43s remains)
INFO - root - 2017-12-05 10:39:00.057930: step 160, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.619 sec/batch; 57h:07m:59s remains)
INFO - root - 2017-12-05 10:39:06.284826: step 170, loss = 2.07, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 58h:07m:38s remains)
INFO - root - 2017-12-05 10:39:12.469347: step 180, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.634 sec/batch; 58h:31m:57s remains)
INFO - root - 2017-12-05 10:39:18.705951: step 190, loss = 2.08, batch loss = 2.02 (13.1 examples/sec; 0.610 sec/batch; 56h:16m:25s remains)
INFO - root - 2017-12-05 10:39:24.990854: step 200, loss = 2.07, batch loss = 2.01 (13.1 examples/sec; 0.612 sec/batch; 56h:28m:30s remains)
2017-12-05 10:39:25.576598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.238728 -4.2355065 -4.2272434 -4.2145061 -4.20528 -4.2092867 -4.2190976 -4.2264166 -4.2270336 -4.22935 -4.2390828 -4.2470994 -4.2505865 -4.2488046 -4.2404232][-4.2371993 -4.2244287 -4.2075834 -4.1946054 -4.1868253 -4.1891031 -4.1970935 -4.2031927 -4.2029757 -4.2067256 -4.2213078 -4.2350321 -4.2391324 -4.2357249 -4.2265339][-4.2304664 -4.2200212 -4.2046046 -4.1928244 -4.1857786 -4.1882854 -4.1924739 -4.2022929 -4.2077389 -4.21203 -4.2195916 -4.22911 -4.2324243 -4.2280083 -4.219913][-4.2095747 -4.2076206 -4.2033148 -4.1945529 -4.1899619 -4.188889 -4.1896582 -4.2064366 -4.2253222 -4.2350516 -4.2329311 -4.2320156 -4.2314496 -4.2239928 -4.2145653][-4.1990614 -4.1986213 -4.2051544 -4.19854 -4.19115 -4.1805658 -4.1722126 -4.19418 -4.2258019 -4.2414155 -4.2365909 -4.2306705 -4.2237263 -4.2147384 -4.2096882][-4.1946454 -4.1895862 -4.2003064 -4.1937771 -4.176589 -4.1504946 -4.1271229 -4.1514268 -4.2006435 -4.2287779 -4.2325478 -4.2246747 -4.2125497 -4.2023234 -4.2037234][-4.1897979 -4.1754322 -4.1804605 -4.1688733 -4.1395884 -4.0841541 -4.0262051 -4.0507731 -4.1348772 -4.1900821 -4.2087722 -4.2028103 -4.1881304 -4.1798167 -4.1898546][-4.1679039 -4.145915 -4.1507487 -4.1376534 -4.0929756 -4.0032816 -3.8913689 -3.908144 -4.0396762 -4.1317248 -4.1677184 -4.1633129 -4.1472778 -4.1497931 -4.1718612][-4.147768 -4.1315289 -4.1477971 -4.1423626 -4.0990257 -4.0123997 -3.8933082 -3.896034 -4.0252662 -4.1276565 -4.17258 -4.1759319 -4.1596832 -4.1582227 -4.171906][-4.1561112 -4.1482944 -4.1725974 -4.1792955 -4.1543431 -4.1004686 -4.0256038 -4.0203795 -4.0988617 -4.1759596 -4.2189507 -4.2303109 -4.2169609 -4.20596 -4.2040534][-4.1785841 -4.1727977 -4.1925712 -4.2012177 -4.1949315 -4.1662626 -4.1223774 -4.1131334 -4.1577482 -4.2122688 -4.247128 -4.2564006 -4.2413225 -4.2274184 -4.2218723][-4.1903114 -4.1843004 -4.1940942 -4.1947541 -4.2003508 -4.1879368 -4.1616888 -4.1530066 -4.1767597 -4.2138042 -4.2394505 -4.2458892 -4.2373939 -4.2277527 -4.2263594][-4.1995778 -4.1964278 -4.2016983 -4.1981997 -4.2069511 -4.2028613 -4.1867909 -4.18255 -4.1958117 -4.2182961 -4.2353678 -4.2409186 -4.2442307 -4.246181 -4.2517805][-4.2301135 -4.233623 -4.2415524 -4.2371435 -4.2436051 -4.2397013 -4.2233324 -4.2191887 -4.2275424 -4.2412739 -4.2553277 -4.2618 -4.270472 -4.2793989 -4.2883925][-4.2550044 -4.26294 -4.2732229 -4.2728939 -4.2782059 -4.2747736 -4.2612495 -4.2563949 -4.258934 -4.2658653 -4.2775021 -4.28445 -4.2901363 -4.2966876 -4.3048787]]...]
INFO - root - 2017-12-05 10:39:31.933211: step 210, loss = 2.05, batch loss = 2.00 (12.6 examples/sec; 0.632 sec/batch; 58h:22m:42s remains)
INFO - root - 2017-12-05 10:39:38.021190: step 220, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.615 sec/batch; 56h:46m:19s remains)
INFO - root - 2017-12-05 10:39:44.273905: step 230, loss = 2.04, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 57h:28m:29s remains)
INFO - root - 2017-12-05 10:39:50.509678: step 240, loss = 2.05, batch loss = 1.99 (12.8 examples/sec; 0.623 sec/batch; 57h:29m:52s remains)
INFO - root - 2017-12-05 10:39:56.673852: step 250, loss = 2.06, batch loss = 2.00 (12.9 examples/sec; 0.622 sec/batch; 57h:24m:04s remains)
INFO - root - 2017-12-05 10:40:02.978886: step 260, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 58h:33m:49s remains)
INFO - root - 2017-12-05 10:40:09.212877: step 270, loss = 2.09, batch loss = 2.03 (12.7 examples/sec; 0.632 sec/batch; 58h:18m:24s remains)
INFO - root - 2017-12-05 10:40:15.519897: step 280, loss = 2.03, batch loss = 1.97 (12.8 examples/sec; 0.625 sec/batch; 57h:40m:34s remains)
INFO - root - 2017-12-05 10:40:21.772428: step 290, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.634 sec/batch; 58h:27m:38s remains)
INFO - root - 2017-12-05 10:40:28.040399: step 300, loss = 2.08, batch loss = 2.02 (13.0 examples/sec; 0.614 sec/batch; 56h:41m:54s remains)
2017-12-05 10:40:28.631627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1386266 -4.1319294 -4.1352038 -4.1544485 -4.1892676 -4.219173 -4.2289224 -4.2140689 -4.1880178 -4.1703281 -4.1733375 -4.1844039 -4.1861854 -4.1800451 -4.173264][-4.1961155 -4.1805105 -4.1624117 -4.1595373 -4.1850729 -4.2182827 -4.2336187 -4.2295856 -4.2192507 -4.2171836 -4.2265177 -4.2328095 -4.2236781 -4.2054181 -4.1845956][-4.2308073 -4.2128563 -4.1820779 -4.1602087 -4.1732979 -4.2059488 -4.2275386 -4.237546 -4.2445321 -4.2523193 -4.2623377 -4.2612948 -4.2434845 -4.2148957 -4.1848578][-4.2235422 -4.2067614 -4.1739039 -4.1452112 -4.1507444 -4.1777864 -4.2021613 -4.2231574 -4.2405429 -4.251298 -4.2590265 -4.2502508 -4.22505 -4.1911373 -4.1644115][-4.1898746 -4.1748157 -4.1485844 -4.1244259 -4.1265531 -4.1420221 -4.1628389 -4.1902595 -4.2087469 -4.2134485 -4.2154646 -4.2024269 -4.1733861 -4.1371121 -4.1206241][-4.1649146 -4.1494136 -4.128058 -4.1113424 -4.1117334 -4.1164403 -4.1281486 -4.1565375 -4.1700668 -4.165854 -4.1568236 -4.1388292 -4.1076522 -4.0739241 -4.0692344][-4.1581025 -4.1400757 -4.1200404 -4.1087251 -4.1050949 -4.1028767 -4.1083989 -4.1330061 -4.1411424 -4.1334357 -4.1168919 -4.092463 -4.0550489 -4.0201383 -4.0264931][-4.1660767 -4.1501932 -4.1343107 -4.1273327 -4.1217451 -4.1130786 -4.1134138 -4.1314178 -4.135932 -4.1281433 -4.1107483 -4.087244 -4.0530314 -4.0227795 -4.0381012][-4.1888185 -4.1822376 -4.1743135 -4.1729279 -4.1701007 -4.158165 -4.1497278 -4.1606174 -4.1651883 -4.1598067 -4.1460886 -4.1316695 -4.1098714 -4.0889597 -4.1041293][-4.2113905 -4.2133307 -4.2123885 -4.2152877 -4.2191629 -4.2100434 -4.1951327 -4.1969218 -4.19938 -4.1951241 -4.1878328 -4.1856227 -4.1763744 -4.1645246 -4.1750755][-4.2161222 -4.226994 -4.2323632 -4.2409496 -4.2501588 -4.24327 -4.2241006 -4.2170968 -4.2150006 -4.20624 -4.2044621 -4.2146153 -4.2174244 -4.2156034 -4.2237716][-4.2063718 -4.2282686 -4.2401481 -4.2542529 -4.26711 -4.2609529 -4.2410107 -4.2273035 -4.2181659 -4.2028894 -4.2026196 -4.221909 -4.2318158 -4.2358408 -4.2456713][-4.1902881 -4.2142754 -4.2290578 -4.2479353 -4.2640638 -4.261858 -4.2456007 -4.2277904 -4.2102027 -4.1916523 -4.1921787 -4.2146182 -4.226892 -4.2335835 -4.2484174][-4.1899185 -4.2081418 -4.2218404 -4.2422833 -4.2601256 -4.26311 -4.2527146 -4.2339249 -4.2119789 -4.1911168 -4.1900024 -4.2095594 -4.22223 -4.2313557 -4.2476859][-4.2003431 -4.2077861 -4.2146225 -4.2291946 -4.2450128 -4.2545953 -4.2548103 -4.2413859 -4.2197375 -4.1984477 -4.1941824 -4.2064805 -4.2154789 -4.2230144 -4.2329869]]...]
INFO - root - 2017-12-05 10:40:34.934210: step 310, loss = 2.06, batch loss = 2.00 (12.3 examples/sec; 0.653 sec/batch; 60h:12m:51s remains)
INFO - root - 2017-12-05 10:40:41.209791: step 320, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.634 sec/batch; 58h:30m:33s remains)
INFO - root - 2017-12-05 10:40:47.497442: step 330, loss = 2.07, batch loss = 2.01 (12.6 examples/sec; 0.634 sec/batch; 58h:29m:33s remains)
INFO - root - 2017-12-05 10:40:53.737355: step 340, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.622 sec/batch; 57h:22m:51s remains)
INFO - root - 2017-12-05 10:40:59.994412: step 350, loss = 2.07, batch loss = 2.01 (12.8 examples/sec; 0.625 sec/batch; 57h:41m:09s remains)
INFO - root - 2017-12-05 10:41:06.186689: step 360, loss = 2.07, batch loss = 2.01 (13.0 examples/sec; 0.615 sec/batch; 56h:45m:21s remains)
INFO - root - 2017-12-05 10:41:12.502036: step 370, loss = 2.07, batch loss = 2.02 (12.9 examples/sec; 0.619 sec/batch; 57h:05m:36s remains)
INFO - root - 2017-12-05 10:41:18.767700: step 380, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.628 sec/batch; 57h:58m:21s remains)
INFO - root - 2017-12-05 10:41:25.063062: step 390, loss = 2.05, batch loss = 2.00 (12.7 examples/sec; 0.632 sec/batch; 58h:18m:38s remains)
INFO - root - 2017-12-05 10:41:31.368638: step 400, loss = 2.05, batch loss = 2.00 (12.3 examples/sec; 0.649 sec/batch; 59h:50m:00s remains)
2017-12-05 10:41:31.910295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2541504 -4.2420144 -4.2286396 -4.222106 -4.2237492 -4.2320671 -4.2431879 -4.2523475 -4.2525616 -4.2453561 -4.2319808 -4.2191291 -4.21167 -4.2081208 -4.2075019][-4.2782793 -4.2696047 -4.2578864 -4.2508006 -4.2544956 -4.2684703 -4.2879782 -4.3051848 -4.3100553 -4.3036957 -4.2840438 -4.2563448 -4.2313023 -4.212862 -4.2038341][-4.2795386 -4.2735276 -4.2628193 -4.2562122 -4.261404 -4.2776761 -4.3007636 -4.3255954 -4.3387547 -4.3412795 -4.3262577 -4.2940073 -4.2545085 -4.2185125 -4.1916447][-4.2674932 -4.2579217 -4.2448511 -4.2384844 -4.24649 -4.2644329 -4.2851214 -4.31044 -4.3328061 -4.3478417 -4.3443356 -4.3163152 -4.2717037 -4.2208424 -4.1710758][-4.256062 -4.2321539 -4.2074871 -4.1944323 -4.2010441 -4.2160025 -4.2327571 -4.255445 -4.2837 -4.3148742 -4.3304486 -4.316618 -4.2764297 -4.2177477 -4.1472487][-4.2487245 -4.2070017 -4.1617775 -4.1332626 -4.1321192 -4.1373916 -4.1437736 -4.1670957 -4.2098532 -4.2626572 -4.3005991 -4.3030167 -4.2714071 -4.21174 -4.1286025][-4.2478323 -4.1949258 -4.1302452 -4.0786262 -4.0530443 -4.0323491 -4.0183105 -4.0413318 -4.1102905 -4.1952944 -4.2566175 -4.2752805 -4.2604914 -4.2146778 -4.1368513][-4.2533078 -4.2005491 -4.127696 -4.0551963 -3.9928064 -3.92766 -3.8714046 -3.8812845 -3.9783506 -4.1006117 -4.189744 -4.2320151 -4.2429953 -4.2286649 -4.1779232][-4.2648506 -4.224926 -4.1635785 -4.0942369 -4.0170894 -3.9196937 -3.8202291 -3.7933087 -3.8815231 -4.0061746 -4.1057487 -4.1682405 -4.2074823 -4.22834 -4.217864][-4.2789426 -4.2564435 -4.221199 -4.1762295 -4.1186519 -4.0327382 -3.9337568 -3.8847198 -3.9243484 -3.9989767 -4.06963 -4.1302977 -4.1841412 -4.2261128 -4.244442][-4.2962809 -4.2875371 -4.2739749 -4.2540445 -4.2244096 -4.1729293 -4.1063919 -4.0633907 -4.0676341 -4.0936322 -4.126914 -4.1664634 -4.2106428 -4.2499628 -4.2724667][-4.3130097 -4.3125658 -4.309845 -4.3046784 -4.2952161 -4.2756128 -4.2458258 -4.2215428 -4.2139854 -4.2158546 -4.2271314 -4.245481 -4.2700634 -4.2933865 -4.3051848][-4.3269162 -4.3297029 -4.32928 -4.3273368 -4.3257151 -4.3236365 -4.3174396 -4.3095908 -4.3049817 -4.3020868 -4.302187 -4.3063679 -4.3161211 -4.3258362 -4.3270674][-4.3381572 -4.3422079 -4.3414278 -4.3383512 -4.33562 -4.3350716 -4.3360739 -4.336935 -4.3375478 -4.3375664 -4.3362713 -4.3349385 -4.3382344 -4.340425 -4.3357558][-4.3433661 -4.3466921 -4.3457823 -4.3426723 -4.3401341 -4.3393 -4.3389955 -4.3388195 -4.339407 -4.3403654 -4.3405185 -4.3400736 -4.341938 -4.3423038 -4.3366971]]...]
INFO - root - 2017-12-05 10:41:38.187431: step 410, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.653 sec/batch; 60h:16m:18s remains)
INFO - root - 2017-12-05 10:41:44.445124: step 420, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.635 sec/batch; 58h:36m:11s remains)
INFO - root - 2017-12-05 10:41:50.807189: step 430, loss = 2.07, batch loss = 2.01 (11.8 examples/sec; 0.677 sec/batch; 62h:28m:30s remains)
INFO - root - 2017-12-05 10:41:57.120309: step 440, loss = 2.06, batch loss = 2.00 (13.3 examples/sec; 0.600 sec/batch; 55h:19m:50s remains)
INFO - root - 2017-12-05 10:42:03.353732: step 450, loss = 2.09, batch loss = 2.03 (12.6 examples/sec; 0.633 sec/batch; 58h:25m:42s remains)
INFO - root - 2017-12-05 10:42:09.537834: step 460, loss = 2.06, batch loss = 2.00 (14.4 examples/sec; 0.556 sec/batch; 51h:17m:48s remains)
INFO - root - 2017-12-05 10:42:15.824108: step 470, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.623 sec/batch; 57h:26m:51s remains)
INFO - root - 2017-12-05 10:42:22.079081: step 480, loss = 2.05, batch loss = 1.99 (12.6 examples/sec; 0.637 sec/batch; 58h:42m:48s remains)
INFO - root - 2017-12-05 10:42:28.407369: step 490, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.637 sec/batch; 58h:42m:22s remains)
INFO - root - 2017-12-05 10:42:34.769493: step 500, loss = 2.08, batch loss = 2.02 (12.2 examples/sec; 0.654 sec/batch; 60h:17m:34s remains)
2017-12-05 10:42:35.343862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1771131 -4.173666 -4.1765013 -4.1783352 -4.1774044 -4.18151 -4.1909571 -4.1979795 -4.1978755 -4.1941566 -4.1851768 -4.1673679 -4.1462259 -4.1282549 -4.1200113][-4.1522136 -4.1470275 -4.1456194 -4.1417122 -4.1389189 -4.1462684 -4.1607065 -4.1759238 -4.1837778 -4.1816187 -4.1712551 -4.1485438 -4.1229191 -4.1038556 -4.0958261][-4.1386566 -4.1319451 -4.1228533 -4.1076827 -4.102304 -4.1189556 -4.1389918 -4.1601434 -4.1732717 -4.1688862 -4.1490889 -4.1188331 -4.0887766 -4.0699272 -4.0618768][-4.133934 -4.1267061 -4.1107631 -4.083684 -4.0703349 -4.0830517 -4.0985956 -4.1214085 -4.1405973 -4.1328983 -4.1039429 -4.0708709 -4.0480318 -4.0408263 -4.0448341][-4.128159 -4.1220784 -4.1017704 -4.0658288 -4.0426316 -4.0436206 -4.0493636 -4.0670156 -4.0868673 -4.0805058 -4.0480566 -4.02118 -4.0158844 -4.0277581 -4.0516543][-4.1171694 -4.1147881 -4.0968213 -4.0626392 -4.0313263 -4.0112138 -3.9952068 -3.9986846 -4.0173731 -4.0190892 -3.9926786 -3.9769483 -3.995331 -4.0270505 -4.0642037][-4.1048465 -4.1082149 -4.1007681 -4.0773187 -4.0395427 -3.9879003 -3.9297411 -3.902981 -3.930372 -3.9578278 -3.95311 -3.9594107 -3.9965894 -4.0351191 -4.0694623][-4.1017179 -4.1039104 -4.1017327 -4.0861979 -4.0485592 -3.9816921 -3.88865 -3.8294582 -3.8699503 -3.9318376 -3.9571073 -3.9859788 -4.027935 -4.0597348 -4.0822487][-4.1071553 -4.0978217 -4.0891457 -4.0785341 -4.0544753 -4.00613 -3.9249775 -3.8699338 -3.904995 -3.9657166 -4.0008655 -4.0384841 -4.0758429 -4.0965605 -4.1078596][-4.1030645 -4.0905566 -4.083086 -4.0866318 -4.0861053 -4.0649614 -4.0145054 -3.9789512 -3.9974213 -4.0275612 -4.0495672 -4.08343 -4.1175423 -4.1339593 -4.1354737][-4.0972509 -4.0983868 -4.1055188 -4.120327 -4.1312861 -4.1255579 -4.0998616 -4.0784793 -4.0845633 -4.0857363 -4.0851769 -4.1065984 -4.1361418 -4.1494212 -4.1420031][-4.1182938 -4.135479 -4.15063 -4.1628828 -4.1685486 -4.1651421 -4.1502628 -4.1341004 -4.131053 -4.1155696 -4.0997705 -4.1082511 -4.1303148 -4.1419539 -4.1406827][-4.1553435 -4.1741271 -4.1873145 -4.1933484 -4.1923928 -4.1878719 -4.1749434 -4.160511 -4.1521854 -4.1330032 -4.1143923 -4.1170096 -4.1319761 -4.1460133 -4.1596346][-4.1913252 -4.20469 -4.2148528 -4.2172909 -4.2136178 -4.2073784 -4.1949477 -4.1862731 -4.1836791 -4.1734276 -4.1636658 -4.164257 -4.1684813 -4.1804571 -4.2003741][-4.2138062 -4.2228913 -4.2303276 -4.2305012 -4.2261691 -4.2197142 -4.20874 -4.2054062 -4.2102413 -4.2130709 -4.2136436 -4.2121396 -4.206811 -4.2123365 -4.2249656]]...]
INFO - root - 2017-12-05 10:42:41.670135: step 510, loss = 2.08, batch loss = 2.02 (12.7 examples/sec; 0.630 sec/batch; 58h:06m:59s remains)
INFO - root - 2017-12-05 10:42:47.851646: step 520, loss = 2.06, batch loss = 2.00 (12.8 examples/sec; 0.626 sec/batch; 57h:44m:02s remains)
INFO - root - 2017-12-05 10:42:54.115850: step 530, loss = 2.04, batch loss = 1.99 (12.3 examples/sec; 0.648 sec/batch; 59h:47m:09s remains)
INFO - root - 2017-12-05 10:43:00.397664: step 540, loss = 2.07, batch loss = 2.01 (12.3 examples/sec; 0.650 sec/batch; 59h:58m:17s remains)
INFO - root - 2017-12-05 10:43:06.754049: step 550, loss = 2.06, batch loss = 2.00 (12.6 examples/sec; 0.633 sec/batch; 58h:24m:33s remains)
INFO - root - 2017-12-05 10:43:13.086491: step 560, loss = 2.06, batch loss = 2.00 (13.0 examples/sec; 0.614 sec/batch; 56h:36m:27s remains)
INFO - root - 2017-12-05 10:43:19.289711: step 570, loss = 2.07, batch loss = 2.01 (12.9 examples/sec; 0.621 sec/batch; 57h:15m:12s remains)
INFO - root - 2017-12-05 10:43:25.605868: step 580, loss = 2.07, batch loss = 2.01 (12.7 examples/sec; 0.629 sec/batch; 58h:00m:09s remains)
INFO - root - 2017-12-05 10:43:31.864741: step 590, loss = 2.08, batch loss = 2.02 (12.5 examples/sec; 0.639 sec/batch; 58h:56m:55s remains)
INFO - root - 2017-12-05 10:43:38.424368: step 600, loss = 2.04, batch loss = 1.99 (12.6 examples/sec; 0.636 sec/batch; 58h:35m:37s remains)
2017-12-05 10:43:38.991536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3167868 -4.3059864 -4.2983084 -4.2962904 -4.3013229 -4.3099203 -4.3163013 -4.3194761 -4.32146 -4.3239794 -4.3273063 -4.331512 -4.3362217 -4.3402357 -4.3427243][-4.3043242 -4.2888012 -4.2768507 -4.2726579 -4.2773414 -4.2855406 -4.2920318 -4.296073 -4.3009939 -4.3059087 -4.3104334 -4.316514 -4.3254724 -4.3343534 -4.3388314][-4.2951508 -4.2774229 -4.2626858 -4.2542462 -4.2520175 -4.2533321 -4.2543173 -4.2585435 -4.2701316 -4.2809038 -4.287261 -4.2947598 -4.30719 -4.32201 -4.332078][-4.2845893 -4.2649579 -4.246521 -4.2328391 -4.2220263 -4.2149553 -4.2089291 -4.215436 -4.235703 -4.2557573 -4.2656183 -4.274756 -4.2887654 -4.3075972 -4.3227334][-4.2748842 -4.2475762 -4.2183614 -4.1942091 -4.1734233 -4.1550531 -4.1432767 -4.1577282 -4.1937103 -4.22939 -4.2496934 -4.2636504 -4.2792735 -4.2990694 -4.3176312][-4.25819 -4.2163553 -4.1676879 -4.1275253 -4.0883484 -4.0486736 -4.0250182 -4.0492139 -4.1108537 -4.1745191 -4.2194343 -4.2510209 -4.2771473 -4.2993355 -4.3194542][-4.2428026 -4.1865411 -4.1155996 -4.0497909 -3.9791148 -3.9070778 -3.8669338 -3.8981118 -3.9892993 -4.0860062 -4.1597834 -4.2159619 -4.2611384 -4.2933421 -4.3175964][-4.2369661 -4.1705909 -4.0801616 -3.9856138 -3.8780973 -3.7658696 -3.701138 -3.7329502 -3.8503585 -3.977716 -4.0788321 -4.1610413 -4.2279463 -4.2742143 -4.3057694][-4.2510042 -4.1897559 -4.1011114 -4.002656 -3.8874817 -3.7598302 -3.6755085 -3.6962471 -3.8121264 -3.9434361 -4.0530963 -4.1429462 -4.2163267 -4.2660837 -4.2983508][-4.2779775 -4.2364645 -4.1731253 -4.1000423 -4.0095739 -3.9059987 -3.8298655 -3.8355265 -3.9190784 -4.0181088 -4.106977 -4.1817741 -4.2418184 -4.2806525 -4.3044505][-4.3021755 -4.282474 -4.2503839 -4.2098813 -4.1554179 -4.0923762 -4.0432153 -4.0411758 -4.0867538 -4.144537 -4.2024393 -4.2509432 -4.2881861 -4.3091855 -4.3190565][-4.31853 -4.3141732 -4.3045321 -4.2902389 -4.2668257 -4.2363114 -4.2097578 -4.2027793 -4.2220283 -4.2512579 -4.2832642 -4.3080931 -4.325408 -4.3323736 -4.3319168][-4.3262596 -4.3267188 -4.3267517 -4.3270721 -4.3223295 -4.3128552 -4.303781 -4.3006477 -4.3082409 -4.3214459 -4.334547 -4.3440089 -4.3489418 -4.3490171 -4.3432693][-4.32909 -4.3274994 -4.3289413 -4.3328 -4.3342981 -4.3334165 -4.3329411 -4.3343067 -4.3398795 -4.3469548 -4.3514533 -4.3547668 -4.3555441 -4.353044 -4.3468347][-4.3338418 -4.3312359 -4.3304672 -4.3313651 -4.3318696 -4.3323879 -4.3334422 -4.336225 -4.3404288 -4.3445005 -4.3468785 -4.34909 -4.3496881 -4.3478732 -4.3444963]]...]
INFO - root - 2017-12-05 10:43:47.732115: step 610, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 75h:56m:26s remains)
INFO - root - 2017-12-05 10:43:56.034831: step 620, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 77h:10m:43s remains)
INFO - root - 2017-12-05 10:44:04.273682: step 630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:30m:27s remains)
INFO - root - 2017-12-05 10:44:12.613927: step 640, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 77h:49m:30s remains)
INFO - root - 2017-12-05 10:44:21.054752: step 650, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 77h:40m:23s remains)
INFO - root - 2017-12-05 10:44:29.430585: step 660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 79h:04m:35s remains)
INFO - root - 2017-12-05 10:44:37.627636: step 670, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 77h:31m:06s remains)
INFO - root - 2017-12-05 10:44:45.862723: step 680, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 77h:15m:40s remains)
INFO - root - 2017-12-05 10:44:54.230350: step 690, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 77h:38m:02s remains)
INFO - root - 2017-12-05 10:45:02.584657: step 700, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 76h:36m:30s remains)
2017-12-05 10:45:03.302185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1488943 -4.155242 -4.1556606 -4.1589904 -4.1671362 -4.17744 -4.1856909 -4.1865883 -4.1840267 -4.1770697 -4.1706166 -4.1589451 -4.1450267 -4.1345558 -4.1272273][-4.1474748 -4.1497688 -4.1436582 -4.1421561 -4.1476173 -4.1635361 -4.183537 -4.1925874 -4.1911836 -4.1784773 -4.158617 -4.128202 -4.1019092 -4.0939174 -4.1007075][-4.1852908 -4.1788249 -4.1637197 -4.1544728 -4.1537585 -4.1707907 -4.1987996 -4.2181425 -4.2215061 -4.2040563 -4.16777 -4.1150684 -4.0721688 -4.0632424 -4.0806231][-4.227355 -4.2139859 -4.1920528 -4.172616 -4.1594238 -4.1637292 -4.1872721 -4.2165947 -4.2329888 -4.2189426 -4.1759682 -4.1082506 -4.0492334 -4.0361705 -4.060389][-4.2451935 -4.2266245 -4.1992598 -4.1700115 -4.1387916 -4.1220894 -4.1322737 -4.1710787 -4.2104359 -4.2157989 -4.1797876 -4.1085796 -4.0373149 -4.0170503 -4.0371032][-4.2264385 -4.1978273 -4.1608257 -4.1210408 -4.0684481 -4.0252519 -4.0200443 -4.0673785 -4.140511 -4.1802783 -4.1640396 -4.10412 -4.0342231 -4.0034547 -4.0084515][-4.1821837 -4.1455426 -4.0988851 -4.039185 -3.9557483 -3.879262 -3.8493998 -3.9005399 -4.0117822 -4.0966792 -4.1127763 -4.0749531 -4.0176978 -3.982703 -3.9703519][-4.1475434 -4.1111255 -4.0587726 -3.9851952 -3.8810961 -3.7682996 -3.6905384 -3.7272658 -3.8732531 -4.0069366 -4.0655003 -4.0574603 -4.0186872 -3.9846828 -3.958066][-4.1287217 -4.10592 -4.0626931 -3.9924741 -3.8942647 -3.7799046 -3.684032 -3.6974478 -3.8365767 -3.9837859 -4.0638371 -4.0767155 -4.0540042 -4.0266948 -3.9961596][-4.1152244 -4.1081843 -4.0846004 -4.0347004 -3.9635496 -3.8845177 -3.8226209 -3.8307252 -3.9222097 -4.03625 -4.10412 -4.1163044 -4.0985107 -4.0787668 -4.0508156][-4.1050444 -4.1052747 -4.095942 -4.0684376 -4.0296974 -3.995878 -3.9735503 -3.9835236 -4.0330777 -4.0970411 -4.13103 -4.1287317 -4.1042814 -4.0844493 -4.0605869][-4.1271896 -4.1253266 -4.1168013 -4.0996332 -4.0848656 -4.0874934 -4.0935283 -4.1042686 -4.1249561 -4.1474881 -4.1501594 -4.1302433 -4.0994868 -4.0762291 -4.060205][-4.1767545 -4.173655 -4.1634965 -4.1470108 -4.1429853 -4.1644635 -4.1876369 -4.2014589 -4.2053709 -4.1969118 -4.17706 -4.1482444 -4.1186709 -4.0962529 -4.0862775][-4.2221985 -4.2217789 -4.2118788 -4.1945672 -4.1927505 -4.2190194 -4.2466903 -4.2574358 -4.2476492 -4.2225418 -4.1936908 -4.1668687 -4.1434178 -4.1297417 -4.1343012][-4.2425647 -4.2437949 -4.23638 -4.22252 -4.2237949 -4.2472587 -4.2682467 -4.2711244 -4.2529392 -4.2254257 -4.2000794 -4.1794415 -4.1622834 -4.1610322 -4.1822267]]...]
INFO - root - 2017-12-05 10:45:11.568690: step 710, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 75h:03m:39s remains)
INFO - root - 2017-12-05 10:45:19.815869: step 720, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 75h:42m:38s remains)
INFO - root - 2017-12-05 10:45:28.145964: step 730, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 76h:53m:22s remains)
INFO - root - 2017-12-05 10:45:36.462117: step 740, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 75h:54m:44s remains)
INFO - root - 2017-12-05 10:45:44.873019: step 750, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 76h:20m:57s remains)
INFO - root - 2017-12-05 10:45:53.172498: step 760, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 77h:34m:13s remains)
INFO - root - 2017-12-05 10:46:01.482388: step 770, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 76h:28m:23s remains)
INFO - root - 2017-12-05 10:46:09.829002: step 780, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 77h:54m:35s remains)
INFO - root - 2017-12-05 10:46:18.194439: step 790, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 76h:03m:42s remains)
INFO - root - 2017-12-05 10:46:26.512026: step 800, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 77h:52m:02s remains)
2017-12-05 10:46:27.194468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2680922 -4.258409 -4.24893 -4.2468953 -4.2412357 -4.2228112 -4.1993275 -4.1813354 -4.1743813 -4.1804967 -4.1918368 -4.201973 -4.2154179 -4.2242384 -4.2214193][-4.273108 -4.2649446 -4.2554941 -4.2512159 -4.2436433 -4.2254477 -4.202199 -4.1815071 -4.167572 -4.1666431 -4.1760969 -4.1865034 -4.1991124 -4.2054782 -4.1973114][-4.2670846 -4.259809 -4.2496433 -4.24278 -4.23196 -4.2135735 -4.1921239 -4.1707511 -4.1521974 -4.1467957 -4.1576047 -4.1725411 -4.1871605 -4.1942267 -4.1862736][-4.2547245 -4.2459579 -4.2323465 -4.22001 -4.2033148 -4.1826959 -4.1602821 -4.1392226 -4.1194429 -4.1142049 -4.1310077 -4.1553278 -4.1742163 -4.1840625 -4.178896][-4.237669 -4.2255831 -4.2079048 -4.1908822 -4.1679239 -4.1447287 -4.1229224 -4.1042743 -4.0869722 -4.0860062 -4.1113663 -4.1436973 -4.1649051 -4.1743374 -4.1689372][-4.214849 -4.1997824 -4.1757569 -4.1504569 -4.1210542 -4.097394 -4.0776281 -4.0588684 -4.0432467 -4.0519543 -4.0908451 -4.1330452 -4.1589127 -4.16816 -4.1622915][-4.1916227 -4.1769743 -4.1470304 -4.110785 -4.0724697 -4.0427408 -4.0151525 -3.9858232 -3.9639728 -3.9840298 -4.0444303 -4.1059279 -4.1422358 -4.151639 -4.1408439][-4.1744652 -4.1676397 -4.14135 -4.1015105 -4.0576134 -4.0180607 -3.9744802 -3.9245822 -3.8906014 -3.9186521 -3.9955897 -4.0692883 -4.1100426 -4.1172328 -4.0972834][-4.1693325 -4.1720562 -4.1550779 -4.1202168 -4.07981 -4.0443573 -4.0051484 -3.959192 -3.9294262 -3.9528909 -4.0164661 -4.0769029 -4.10894 -4.1102209 -4.0859351][-4.1786575 -4.1864252 -4.1758947 -4.1491008 -4.1179848 -4.09302 -4.0669675 -4.0323873 -4.007484 -4.0202527 -4.0614743 -4.1023498 -4.1239824 -4.1215715 -4.0999951][-4.2048092 -4.2100763 -4.2004766 -4.1802969 -4.1585879 -4.1426067 -4.1262693 -4.0985551 -4.0739913 -4.0776663 -4.1022367 -4.1289544 -4.1445055 -4.1418443 -4.1231112][-4.241 -4.2443552 -4.2377043 -4.2240086 -4.209805 -4.1980605 -4.1837249 -4.1578636 -4.1344929 -4.1329112 -4.14687 -4.1646571 -4.1786203 -4.1770072 -4.1596942][-4.272007 -4.274807 -4.27166 -4.2639551 -4.2550015 -4.246933 -4.2360625 -4.2170854 -4.2009468 -4.2000155 -4.2076221 -4.2167988 -4.2255707 -4.2248559 -4.2095351][-4.2952332 -4.2979674 -4.2977691 -4.2950082 -4.2895193 -4.2828293 -4.2745957 -4.2630863 -4.2550631 -4.2568421 -4.2616138 -4.2647967 -4.267767 -4.2649384 -4.2535229][-4.3127313 -4.3146353 -4.3146958 -4.3125615 -4.3077621 -4.30107 -4.2949529 -4.2896051 -4.2877793 -4.2905383 -4.2933154 -4.2928562 -4.2910333 -4.287046 -4.2793059]]...]
INFO - root - 2017-12-05 10:46:35.513438: step 810, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 75h:42m:42s remains)
INFO - root - 2017-12-05 10:46:43.805525: step 820, loss = 2.02, batch loss = 1.96 (9.6 examples/sec; 0.833 sec/batch; 76h:43m:01s remains)
INFO - root - 2017-12-05 10:46:52.121828: step 830, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 78h:35m:56s remains)
INFO - root - 2017-12-05 10:47:00.462004: step 840, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 77h:44m:21s remains)
INFO - root - 2017-12-05 10:47:08.769373: step 850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 78h:19m:11s remains)
INFO - root - 2017-12-05 10:47:17.111850: step 860, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 77h:09m:03s remains)
INFO - root - 2017-12-05 10:47:25.456851: step 870, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 79h:13m:15s remains)
INFO - root - 2017-12-05 10:47:33.814526: step 880, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 80h:46m:59s remains)
INFO - root - 2017-12-05 10:47:47.678774: step 890, loss = 2.09, batch loss = 2.03 (1.3 examples/sec; 6.338 sec/batch; 583h:49m:10s remains)
INFO - root - 2017-12-05 10:47:55.940768: step 900, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.826 sec/batch; 76h:05m:36s remains)
2017-12-05 10:47:56.631163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654257 -4.2580357 -4.2614565 -4.2727828 -4.2855835 -4.2946963 -4.3011723 -4.3063555 -4.312686 -4.3151708 -4.3159475 -4.3166714 -4.3185167 -4.3187947 -4.3182783][-4.2402048 -4.2260127 -4.2241974 -4.2321544 -4.2453527 -4.2596602 -4.2738547 -4.2868009 -4.2994246 -4.3061104 -4.3090968 -4.3121743 -4.3158402 -4.3162575 -4.3151813][-4.2118082 -4.1885624 -4.1739335 -4.1729445 -4.1848516 -4.2048078 -4.2277269 -4.2501426 -4.2716813 -4.2865477 -4.29696 -4.3066764 -4.3146977 -4.3163738 -4.3151054][-4.1849346 -4.1486626 -4.1130962 -4.0980449 -4.1058269 -4.1270919 -4.1559877 -4.1890039 -4.2250109 -4.2523584 -4.2742934 -4.2948527 -4.3108592 -4.3171873 -4.3168631][-4.159379 -4.108994 -4.0563488 -4.0258551 -4.0221319 -4.0354209 -4.0644636 -4.1110063 -4.168642 -4.2111707 -4.2445741 -4.277226 -4.3013544 -4.3135943 -4.317553][-4.1416554 -4.0819793 -4.0184565 -3.9742138 -3.9497075 -3.9392815 -3.9531839 -4.0082612 -4.0915041 -4.15723 -4.2056413 -4.2512751 -4.2841544 -4.3033013 -4.3127351][-4.1378622 -4.0765367 -4.008738 -3.9532306 -3.9049659 -3.8599067 -3.8338709 -3.8705816 -3.9759262 -4.0693908 -4.1412578 -4.206037 -4.2531209 -4.2855272 -4.3040395][-4.1524754 -4.0971289 -4.0350571 -3.9802055 -3.9236569 -3.8561845 -3.7885199 -3.7749822 -3.8694329 -3.9764855 -4.0695271 -4.1555719 -4.2197452 -4.2686062 -4.2996607][-4.1771636 -4.1307068 -4.0814033 -4.0338025 -3.9830661 -3.9218285 -3.8423772 -3.780453 -3.81999 -3.9099355 -4.0054908 -4.1028309 -4.1840286 -4.2498283 -4.2934875][-4.2024107 -4.1652617 -4.1301641 -4.0947728 -4.0557632 -4.0120993 -3.9514711 -3.8841798 -3.8782678 -3.9191189 -3.9866695 -4.0774903 -4.1668229 -4.2399344 -4.2877889][-4.2310424 -4.2023482 -4.17752 -4.1527519 -4.1264377 -4.0982046 -4.0639014 -4.0206728 -4.005362 -4.0124121 -4.0378594 -4.0995479 -4.1798759 -4.2463646 -4.2896867][-4.2595797 -4.2381215 -4.2205963 -4.2025108 -4.1855083 -4.1713777 -4.1569881 -4.1382904 -4.1294885 -4.126101 -4.1297259 -4.1605167 -4.2162814 -4.2645483 -4.2973561][-4.2824254 -4.2665362 -4.2542844 -4.2415791 -4.2323341 -4.2302189 -4.2296381 -4.2243829 -4.220315 -4.2166796 -4.2146187 -4.2284441 -4.2608109 -4.2889733 -4.3099341][-4.30192 -4.2911224 -4.2833714 -4.2750487 -4.2688184 -4.2707462 -4.2759585 -4.2748938 -4.2727003 -4.2735605 -4.2733612 -4.2795143 -4.2958345 -4.3118653 -4.3231082][-4.31734 -4.3116279 -4.3069663 -4.3018689 -4.2982612 -4.299 -4.302 -4.3011775 -4.2994695 -4.3018994 -4.3040385 -4.3069916 -4.3134637 -4.3218813 -4.32781]]...]
INFO - root - 2017-12-05 10:48:04.882219: step 910, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 0.795 sec/batch; 73h:16m:09s remains)
INFO - root - 2017-12-05 10:48:13.193896: step 920, loss = 2.09, batch loss = 2.04 (9.8 examples/sec; 0.813 sec/batch; 74h:52m:03s remains)
INFO - root - 2017-12-05 10:48:21.410610: step 930, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.815 sec/batch; 75h:06m:05s remains)
INFO - root - 2017-12-05 10:48:29.620146: step 940, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 76h:07m:07s remains)
INFO - root - 2017-12-05 10:48:37.943460: step 950, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.815 sec/batch; 75h:03m:56s remains)
INFO - root - 2017-12-05 10:48:46.432612: step 960, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 77h:27m:57s remains)
INFO - root - 2017-12-05 10:48:54.893718: step 970, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 78h:19m:47s remains)
INFO - root - 2017-12-05 10:49:03.277311: step 980, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 75h:31m:12s remains)
INFO - root - 2017-12-05 10:49:11.748580: step 990, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.842 sec/batch; 77h:33m:01s remains)
INFO - root - 2017-12-05 10:49:20.274399: step 1000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 79h:02m:50s remains)
2017-12-05 10:49:21.033549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1398854 -4.1437178 -4.1523352 -4.170897 -4.1827168 -4.1778483 -4.1615806 -4.1531124 -4.150991 -4.1544952 -4.1794949 -4.2005863 -4.2304296 -4.263803 -4.2982531][-4.1190257 -4.1263614 -4.141345 -4.1639686 -4.1774383 -4.1689153 -4.1468344 -4.1358428 -4.1363254 -4.1446748 -4.1780934 -4.2052512 -4.238709 -4.2732821 -4.3058033][-4.1150808 -4.1296158 -4.1477323 -4.1694579 -4.1808429 -4.1667571 -4.1391048 -4.1269035 -4.1311536 -4.1434321 -4.1777616 -4.206152 -4.2408056 -4.2773409 -4.3091826][-4.1307297 -4.14962 -4.1667943 -4.1822782 -4.1862783 -4.1650124 -4.1318011 -4.1214719 -4.1310749 -4.1498017 -4.1803012 -4.2047911 -4.237886 -4.2743378 -4.3073564][-4.1530533 -4.1678319 -4.1769977 -4.1775885 -4.1662626 -4.1335368 -4.0971913 -4.0961132 -4.1197896 -4.1517248 -4.1828227 -4.2050281 -4.2376194 -4.2741456 -4.3076172][-4.180181 -4.1849322 -4.1789 -4.1596851 -4.127665 -4.0798907 -4.0373764 -4.0479965 -4.0945311 -4.1466694 -4.185041 -4.2095346 -4.2429423 -4.2799306 -4.313704][-4.2009983 -4.1954765 -4.1771421 -4.1415806 -4.0930123 -4.0293016 -3.9751761 -3.9980466 -4.0694976 -4.1425772 -4.1921024 -4.2210951 -4.2536621 -4.2878408 -4.3198638][-4.2012095 -4.1894007 -4.16668 -4.1254616 -4.0694127 -3.9957459 -3.9316506 -3.9632449 -4.05212 -4.1404548 -4.2001672 -4.2327819 -4.2627625 -4.2935448 -4.3237906][-4.2065048 -4.1935325 -4.1741061 -4.1392121 -4.0926418 -4.031805 -3.9769695 -4.0073338 -4.0848956 -4.164546 -4.218966 -4.2459931 -4.2709875 -4.29913 -4.3282247][-4.202363 -4.1895533 -4.1755667 -4.1549091 -4.1303377 -4.0970035 -4.0600595 -4.0786753 -4.1286769 -4.1850004 -4.2251577 -4.2453346 -4.2683444 -4.2976146 -4.3283877][-4.1937943 -4.1804271 -4.1708589 -4.1625686 -4.1567864 -4.1465468 -4.1231861 -4.1298604 -4.1573977 -4.193985 -4.2226863 -4.2395568 -4.2639813 -4.2957225 -4.32853][-4.1833754 -4.1710529 -4.1675768 -4.1698341 -4.1766281 -4.18076 -4.1656413 -4.1662135 -4.1818256 -4.2060237 -4.2284493 -4.2452197 -4.2711363 -4.302494 -4.3332262][-4.1815448 -4.1733479 -4.1739593 -4.1820507 -4.1939092 -4.2030864 -4.1916785 -4.1908746 -4.2024131 -4.2196274 -4.2410269 -4.261611 -4.2884941 -4.3165965 -4.3415732][-4.1983194 -4.1945076 -4.1967831 -4.2058325 -4.217082 -4.2242785 -4.2128463 -4.2126784 -4.2244086 -4.2402964 -4.2628627 -4.2855263 -4.3102236 -4.3329268 -4.3511243][-4.231472 -4.2314215 -4.232873 -4.2391877 -4.2472143 -4.2498217 -4.2390075 -4.2407703 -4.2522483 -4.2681537 -4.2901945 -4.31015 -4.3291483 -4.3455782 -4.3583417]]...]
INFO - root - 2017-12-05 10:49:29.460096: step 1010, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 76h:31m:55s remains)
INFO - root - 2017-12-05 10:49:38.060091: step 1020, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 78h:57m:07s remains)
INFO - root - 2017-12-05 10:49:46.506457: step 1030, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 77h:38m:56s remains)
INFO - root - 2017-12-05 10:49:54.929043: step 1040, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 78h:26m:48s remains)
INFO - root - 2017-12-05 10:50:03.448764: step 1050, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 81h:17m:43s remains)
INFO - root - 2017-12-05 10:50:12.014959: step 1060, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 78h:12m:11s remains)
INFO - root - 2017-12-05 10:50:20.538725: step 1070, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:51m:46s remains)
INFO - root - 2017-12-05 10:50:29.107499: step 1080, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 80h:57m:35s remains)
INFO - root - 2017-12-05 10:50:37.714544: step 1090, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 77h:00m:47s remains)
INFO - root - 2017-12-05 10:50:46.357977: step 1100, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 76h:36m:30s remains)
2017-12-05 10:50:47.098645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1601892 -4.1247749 -4.0776625 -4.0625806 -4.0824051 -4.0966611 -4.1175542 -4.1683488 -4.2192516 -4.2509956 -4.2640042 -4.2720294 -4.2884388 -4.3034253 -4.3071957][-4.1791024 -4.14514 -4.098835 -4.06896 -4.0585127 -4.040699 -4.0455775 -4.1034794 -4.1780457 -4.2284 -4.2482696 -4.2575917 -4.2764516 -4.2965646 -4.3074627][-4.1861315 -4.1566491 -4.1081867 -4.06317 -4.02402 -3.9769289 -3.9762843 -4.0450592 -4.1362591 -4.2029457 -4.2348976 -4.2492781 -4.2680874 -4.2910275 -4.3065629][-4.1879563 -4.1631365 -4.1204405 -4.0745921 -4.0265169 -3.9704654 -3.9643505 -4.0258164 -4.111877 -4.184721 -4.2253451 -4.2470155 -4.2689962 -4.2907519 -4.3072348][-4.1773939 -4.15915 -4.1290731 -4.0947123 -4.0579343 -4.0124326 -4.0040708 -4.0443397 -4.10519 -4.1675882 -4.208499 -4.2363997 -4.2637997 -4.2890215 -4.3063993][-4.145021 -4.13323 -4.1101589 -4.083662 -4.0583615 -4.0261884 -4.0152683 -4.0283656 -4.0610633 -4.1124282 -4.1604371 -4.2004266 -4.2397594 -4.2768116 -4.3009138][-4.1291304 -4.1190948 -4.09577 -4.067678 -4.046802 -4.0194364 -3.9988313 -3.9826219 -3.9901156 -4.0359426 -4.0943065 -4.1501036 -4.2061329 -4.2586851 -4.2928143][-4.1526947 -4.1408539 -4.1150489 -4.0851793 -4.0647111 -4.0385485 -4.0112939 -3.9778533 -3.9674373 -4.0057774 -4.0666184 -4.1275158 -4.1916614 -4.25132 -4.2915998][-4.1959677 -4.1858349 -4.1593919 -4.1294384 -4.1045566 -4.0765257 -4.0480452 -4.0166149 -4.0048766 -4.0341196 -4.0871811 -4.1444817 -4.2052732 -4.2601676 -4.2994447][-4.2184396 -4.2111073 -4.1890225 -4.1616216 -4.1319423 -4.0991697 -4.0698833 -4.0477452 -4.0422444 -4.0617 -4.1049862 -4.1629639 -4.2229209 -4.27344 -4.3091416][-4.2100506 -4.2051358 -4.19269 -4.1766796 -4.1530609 -4.1224804 -4.091948 -4.0721955 -4.064662 -4.06884 -4.1008348 -4.1597404 -4.2211576 -4.27241 -4.3103294][-4.2044573 -4.2038569 -4.201992 -4.198278 -4.187149 -4.1655326 -4.1388192 -4.118773 -4.10391 -4.0894337 -4.1061716 -4.1569762 -4.21381 -4.2652922 -4.3057122][-4.2252994 -4.223845 -4.22483 -4.2263484 -4.2236476 -4.21222 -4.1947336 -4.1763339 -4.1544461 -4.1252279 -4.128365 -4.1670766 -4.2151804 -4.2596765 -4.2976356][-4.2507386 -4.2463789 -4.2435241 -4.2435341 -4.2434173 -4.2404494 -4.2330394 -4.2201738 -4.1954622 -4.1588192 -4.1488452 -4.173388 -4.2129064 -4.2495203 -4.2837219][-4.261538 -4.2549648 -4.2462778 -4.2408385 -4.2390823 -4.2391376 -4.2398114 -4.2332187 -4.2105775 -4.1739125 -4.1564145 -4.1710711 -4.2045245 -4.2359619 -4.2673111]]...]
INFO - root - 2017-12-05 10:50:55.608965: step 1110, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 81h:02m:12s remains)
INFO - root - 2017-12-05 10:51:04.113542: step 1120, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 79h:16m:29s remains)
INFO - root - 2017-12-05 10:51:12.675667: step 1130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 77h:13m:28s remains)
INFO - root - 2017-12-05 10:51:21.125887: step 1140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:54m:51s remains)
INFO - root - 2017-12-05 10:51:29.740424: step 1150, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 80h:34m:30s remains)
INFO - root - 2017-12-05 10:51:38.240988: step 1160, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 78h:57m:26s remains)
INFO - root - 2017-12-05 10:51:46.770029: step 1170, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 77h:03m:06s remains)
INFO - root - 2017-12-05 10:51:55.317010: step 1180, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 79h:39m:33s remains)
INFO - root - 2017-12-05 10:52:03.812416: step 1190, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 79h:22m:32s remains)
INFO - root - 2017-12-05 10:52:12.371758: step 1200, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 79h:23m:08s remains)
2017-12-05 10:52:13.135183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2378197 -4.2349582 -4.2340384 -4.242126 -4.2505751 -4.2520576 -4.2553673 -4.2569242 -4.2583923 -4.2547336 -4.2582541 -4.2710433 -4.2876778 -4.2951679 -4.293644][-4.2278743 -4.2207689 -4.2209382 -4.2349367 -4.2479429 -4.2507286 -4.2550936 -4.2584591 -4.266274 -4.2646351 -4.2656078 -4.2739635 -4.2880449 -4.2947311 -4.2945242][-4.2030993 -4.1921477 -4.1899734 -4.2066932 -4.2230897 -4.2291389 -4.2366147 -4.2427297 -4.2560029 -4.2585707 -4.2588072 -4.2616935 -4.2711816 -4.2799215 -4.2799268][-4.1850743 -4.1662359 -4.15768 -4.177958 -4.2024283 -4.2118554 -4.2161551 -4.2220373 -4.2334867 -4.2419014 -4.2510796 -4.2575064 -4.267539 -4.2770023 -4.275331][-4.1786389 -4.1607895 -4.1510115 -4.1724091 -4.1995497 -4.205862 -4.1995187 -4.1968331 -4.2032285 -4.2184415 -4.2413507 -4.2581377 -4.2726574 -4.28626 -4.28618][-4.1787243 -4.1689587 -4.1611395 -4.1767149 -4.1952648 -4.1904984 -4.1654191 -4.1462078 -4.1512036 -4.1772432 -4.214314 -4.2449012 -4.2689042 -4.2891021 -4.2929368][-4.1725597 -4.1700778 -4.1704736 -4.1798491 -4.1806154 -4.1557651 -4.1027384 -4.0471835 -4.0444074 -4.1004868 -4.1633754 -4.2070904 -4.2447548 -4.2777386 -4.2859039][-4.1597195 -4.165688 -4.1768322 -4.1786842 -4.1590929 -4.1106672 -4.0267348 -3.9208457 -3.901643 -4.0055652 -4.1069713 -4.168633 -4.2139268 -4.2532353 -4.2660584][-4.150259 -4.1567616 -4.1690378 -4.1684113 -4.1412344 -4.08716 -4.0025129 -3.8901737 -3.8607011 -3.9783509 -4.0884414 -4.152667 -4.1948943 -4.2290125 -4.2450628][-4.1477447 -4.1518159 -4.1609735 -4.1663923 -4.14997 -4.1150479 -4.0683508 -4.0099616 -3.9928775 -4.0626717 -4.1334724 -4.1786017 -4.2066522 -4.229075 -4.2428188][-4.1682687 -4.1689939 -4.1791182 -4.1920366 -4.1893282 -4.1770473 -4.1641674 -4.1468072 -4.1406722 -4.1673727 -4.2009988 -4.2246652 -4.2360773 -4.2452264 -4.2555237][-4.2076335 -4.2060542 -4.21544 -4.2289343 -4.2312145 -4.2334042 -4.2378016 -4.2395644 -4.2399087 -4.2444477 -4.2546992 -4.2617154 -4.2620811 -4.2633138 -4.2668486][-4.2461052 -4.2434134 -4.2496552 -4.255 -4.2574253 -4.2632914 -4.2701778 -4.277565 -4.2769146 -4.2681251 -4.261673 -4.2608023 -4.2607527 -4.2608857 -4.2595191][-4.2686133 -4.2647982 -4.2675128 -4.2699842 -4.2726789 -4.2731872 -4.2731996 -4.2765336 -4.2723522 -4.2561383 -4.2439508 -4.2422481 -4.2438531 -4.2482862 -4.2478347][-4.2835207 -4.2765779 -4.2763653 -4.2778769 -4.279695 -4.2763972 -4.2713456 -4.2701445 -4.2636209 -4.24867 -4.2384868 -4.2369041 -4.2365861 -4.2446747 -4.2492347]]...]
INFO - root - 2017-12-05 10:52:21.585645: step 1210, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 77h:02m:46s remains)
INFO - root - 2017-12-05 10:52:30.160061: step 1220, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 79h:46m:13s remains)
INFO - root - 2017-12-05 10:52:38.607571: step 1230, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 78h:30m:38s remains)
INFO - root - 2017-12-05 10:52:47.109815: step 1240, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 76h:29m:03s remains)
INFO - root - 2017-12-05 10:52:55.517517: step 1250, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 77h:36m:17s remains)
INFO - root - 2017-12-05 10:53:04.058943: step 1260, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 81h:28m:41s remains)
INFO - root - 2017-12-05 10:53:12.686269: step 1270, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 78h:49m:46s remains)
INFO - root - 2017-12-05 10:53:21.151547: step 1280, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 77h:45m:56s remains)
INFO - root - 2017-12-05 10:53:29.735679: step 1290, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 81h:07m:58s remains)
INFO - root - 2017-12-05 10:53:38.187272: step 1300, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 77h:16m:21s remains)
2017-12-05 10:53:38.926522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2153144 -4.2317753 -4.2431488 -4.2489319 -4.2465587 -4.2332716 -4.1943493 -4.1249 -4.0698085 -4.0796609 -4.1123629 -4.1507826 -4.2003117 -4.247056 -4.2741113][-4.1826482 -4.1977811 -4.2093511 -4.2169151 -4.2203364 -4.2158432 -4.1828094 -4.1118965 -4.0541592 -4.0637136 -4.0955153 -4.1343231 -4.1874924 -4.2397318 -4.2696528][-4.146409 -4.1573567 -4.16834 -4.1779714 -4.1904168 -4.1982613 -4.1717391 -4.0996552 -4.0406561 -4.0521307 -4.083313 -4.1207943 -4.1752744 -4.2308135 -4.2635908][-4.1126494 -4.115479 -4.12447 -4.1371055 -4.1602426 -4.1811051 -4.1604338 -4.0876141 -4.0303335 -4.0474148 -4.0823827 -4.1204233 -4.17516 -4.2303839 -4.2623749][-4.0853333 -4.0802884 -4.086319 -4.1000414 -4.1292849 -4.1602087 -4.1424546 -4.06648 -4.0123363 -4.0397253 -4.0833778 -4.1259537 -4.18298 -4.2370238 -4.2660122][-4.0671034 -4.0548639 -4.0574155 -4.0675979 -4.0936441 -4.1254396 -4.1048789 -4.0224538 -3.9741039 -4.0170479 -4.0744748 -4.1271019 -4.1909204 -4.2458448 -4.2723303][-4.0827446 -4.0626726 -4.0570703 -4.0562882 -4.0674357 -4.085803 -4.0524163 -3.9604216 -3.9187534 -3.9811952 -4.0560584 -4.1215382 -4.19424 -4.2524042 -4.2777915][-4.1307425 -4.107831 -4.0958233 -4.0849752 -4.07975 -4.0754824 -4.0224614 -3.9207797 -3.8837807 -3.9582057 -4.0432205 -4.1160297 -4.1947932 -4.2560854 -4.2809877][-4.18678 -4.1691704 -4.1572819 -4.1421714 -4.1295619 -4.1092834 -4.0443888 -3.9450734 -3.9131341 -3.9828773 -4.0600147 -4.1253104 -4.1996856 -4.2591724 -4.2826962][-4.2373152 -4.2283206 -4.2194018 -4.2052889 -4.1923523 -4.1659083 -4.1013217 -4.0152874 -3.9877956 -4.0442381 -4.1037865 -4.1531963 -4.2139869 -4.2650638 -4.2844949][-4.2790394 -4.2772856 -4.2705431 -4.2564764 -4.2445178 -4.2168927 -4.1585822 -4.0894976 -4.0659995 -4.1096616 -4.1546507 -4.1912503 -4.2376928 -4.2773242 -4.2898803][-4.310811 -4.3127694 -4.3075008 -4.2943935 -4.2840905 -4.2579393 -4.2098212 -4.1558452 -4.1347585 -4.1687703 -4.203794 -4.2324057 -4.2671247 -4.2945428 -4.2991014][-4.3328753 -4.3370924 -4.3326902 -4.3213477 -4.3125405 -4.2901888 -4.2534304 -4.2121205 -4.1957235 -4.2237763 -4.2519636 -4.2735887 -4.2980103 -4.3137922 -4.31056][-4.3489 -4.3536067 -4.3494191 -4.3405128 -4.3336992 -4.3157854 -4.2895055 -4.2591629 -4.2479343 -4.2723241 -4.295526 -4.3122597 -4.32826 -4.3338265 -4.3233018][-4.3501492 -4.3526516 -4.34773 -4.3410559 -4.3356967 -4.3238273 -4.3065844 -4.28603 -4.2794471 -4.3010774 -4.3210864 -4.3345852 -4.3458118 -4.3457427 -4.3320975]]...]
INFO - root - 2017-12-05 10:53:47.401132: step 1310, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:59m:57s remains)
INFO - root - 2017-12-05 10:53:55.993678: step 1320, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 78h:11m:02s remains)
INFO - root - 2017-12-05 10:54:04.483538: step 1330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 77h:09m:05s remains)
INFO - root - 2017-12-05 10:54:13.046583: step 1340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 78h:50m:45s remains)
INFO - root - 2017-12-05 10:54:21.594639: step 1350, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 78h:19m:33s remains)
INFO - root - 2017-12-05 10:54:30.029824: step 1360, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 77h:58m:16s remains)
INFO - root - 2017-12-05 10:54:38.572148: step 1370, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 78h:20m:49s remains)
INFO - root - 2017-12-05 10:54:47.017088: step 1380, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 78h:02m:26s remains)
INFO - root - 2017-12-05 10:54:55.543484: step 1390, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.884 sec/batch; 81h:16m:31s remains)
INFO - root - 2017-12-05 10:55:04.115236: step 1400, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 78h:15m:05s remains)
2017-12-05 10:55:04.895904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2060037 -4.2125349 -4.2137623 -4.2146258 -4.2110209 -4.2080245 -4.2039852 -4.1928191 -4.1743793 -4.1560655 -4.1449819 -4.1447635 -4.1526432 -4.1672978 -4.1867228][-4.2041149 -4.2161455 -4.2217793 -4.2242007 -4.2194195 -4.2137036 -4.2085037 -4.1962738 -4.1788616 -4.165472 -4.1567121 -4.1549058 -4.1572542 -4.1673303 -4.1857619][-4.1971431 -4.2107596 -4.2171564 -4.2207894 -4.2156558 -4.2087154 -4.2040544 -4.1935878 -4.1805606 -4.1733918 -4.1698709 -4.168839 -4.1685047 -4.1758237 -4.1923842][-4.187181 -4.1937847 -4.195364 -4.1956863 -4.1882534 -4.179533 -4.1753559 -4.1713276 -4.1715107 -4.1773443 -4.1832309 -4.1844625 -4.1820016 -4.1850581 -4.1954951][-4.1711988 -4.1660995 -4.15973 -4.1528335 -4.1402135 -4.1265597 -4.1204863 -4.1229844 -4.1374111 -4.1580939 -4.1751761 -4.184864 -4.1882648 -4.1915588 -4.1974711][-4.1594009 -4.1422663 -4.1235251 -4.1030116 -4.0765243 -4.0529308 -4.0440326 -4.0510607 -4.0816555 -4.1233492 -4.1553617 -4.1755795 -4.186986 -4.1937213 -4.1971703][-4.1744471 -4.1462879 -4.1149287 -4.0756879 -4.026732 -3.9819589 -3.9636889 -3.9721708 -4.0169473 -4.0858517 -4.1377373 -4.1717062 -4.1916986 -4.2037482 -4.2067337][-4.2058306 -4.1704779 -4.1297903 -4.0782876 -4.0165348 -3.9587519 -3.9342966 -3.9392896 -3.9917028 -4.078526 -4.1449394 -4.1902103 -4.2174845 -4.2333755 -4.236145][-4.24199 -4.2052331 -4.1606474 -4.106133 -4.0439572 -3.9904401 -3.9704752 -3.9735615 -4.0215588 -4.1049833 -4.1674137 -4.2114587 -4.2395716 -4.2578125 -4.2612329][-4.2743487 -4.2432704 -4.204114 -4.1539207 -4.0989404 -4.0545053 -4.0421805 -4.0490575 -4.0882211 -4.1519194 -4.1970563 -4.2278342 -4.2458467 -4.2588329 -4.2599087][-4.2912555 -4.2721081 -4.243793 -4.20606 -4.1647768 -4.1314669 -4.1213884 -4.1240807 -4.1469045 -4.1830339 -4.2073789 -4.2246885 -4.2347217 -4.2415729 -4.2410173][-4.2893367 -4.286294 -4.2701244 -4.2462921 -4.21847 -4.1929131 -4.1778178 -4.1689606 -4.1717629 -4.1821051 -4.1917052 -4.20459 -4.2162352 -4.2256007 -4.22817][-4.2673626 -4.2809229 -4.28136 -4.2715516 -4.2556009 -4.2365823 -4.2173166 -4.1970515 -4.1843109 -4.1786709 -4.1794767 -4.1930246 -4.210032 -4.2261691 -4.2331138][-4.2247667 -4.2503829 -4.2642474 -4.2672749 -4.262722 -4.2520156 -4.2346511 -4.2127604 -4.1942449 -4.1828833 -4.1817417 -4.1976137 -4.2176042 -4.2372565 -4.2455664][-4.1805677 -4.2092867 -4.2306 -4.2431808 -4.249094 -4.2485123 -4.2384081 -4.2222934 -4.2065535 -4.1951842 -4.1944027 -4.2097049 -4.2269015 -4.2419424 -4.2483444]]...]
INFO - root - 2017-12-05 10:55:13.349317: step 1410, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 76h:55m:27s remains)
INFO - root - 2017-12-05 10:55:21.853673: step 1420, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 77h:52m:29s remains)
INFO - root - 2017-12-05 10:55:30.372172: step 1430, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.822 sec/batch; 75h:33m:08s remains)
INFO - root - 2017-12-05 10:55:38.799210: step 1440, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 78h:49m:41s remains)
INFO - root - 2017-12-05 10:55:47.252292: step 1450, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 75h:38m:49s remains)
INFO - root - 2017-12-05 10:55:55.718084: step 1460, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 79h:59m:15s remains)
INFO - root - 2017-12-05 10:56:04.207231: step 1470, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 77h:01m:59s remains)
INFO - root - 2017-12-05 10:56:12.810570: step 1480, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 79h:31m:19s remains)
INFO - root - 2017-12-05 10:56:21.368155: step 1490, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 80h:09m:21s remains)
INFO - root - 2017-12-05 10:56:29.922932: step 1500, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 78h:42m:29s remains)
2017-12-05 10:56:30.653215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2978268 -4.297605 -4.2982225 -4.3012915 -4.3004851 -4.2868481 -4.2624969 -4.244123 -4.2403436 -4.2434511 -4.2431655 -4.2312436 -4.2062731 -4.1895266 -4.2032919][-4.2965569 -4.293983 -4.2855883 -4.2796869 -4.2778354 -4.272903 -4.2607956 -4.2486219 -4.2438235 -4.2471457 -4.2485852 -4.2314587 -4.1970224 -4.1749511 -4.1903467][-4.2847719 -4.2762537 -4.2619758 -4.2537179 -4.2555327 -4.2598853 -4.2552233 -4.2433968 -4.2362227 -4.243371 -4.2471447 -4.2252221 -4.1829371 -4.1588187 -4.1754832][-4.2776628 -4.2650814 -4.2489247 -4.241858 -4.2455263 -4.2534041 -4.2483754 -4.2349968 -4.2311921 -4.2418165 -4.2405014 -4.2097149 -4.1649537 -4.1485815 -4.1687078][-4.2770176 -4.2648358 -4.2492428 -4.2407112 -4.2427588 -4.2437472 -4.2339225 -4.2270494 -4.2308159 -4.2398591 -4.2318172 -4.1965637 -4.1601605 -4.1584673 -4.1833405][-4.2721553 -4.2607584 -4.2443223 -4.2305961 -4.2221947 -4.207921 -4.1958189 -4.205133 -4.2226362 -4.2285833 -4.2147393 -4.1892438 -4.1730022 -4.1830912 -4.2066383][-4.2732635 -4.2631803 -4.2435122 -4.2199173 -4.1912184 -4.1565657 -4.1471496 -4.1794763 -4.2115564 -4.2149634 -4.2011542 -4.1899853 -4.1904163 -4.2043777 -4.221137][-4.2795696 -4.2724347 -4.2538352 -4.2245255 -4.1799726 -4.136095 -4.1371126 -4.1775856 -4.2049012 -4.2033415 -4.1969318 -4.1989317 -4.2051373 -4.2139721 -4.2237163][-4.2799664 -4.2779965 -4.2683268 -4.2417912 -4.19626 -4.1607318 -4.1635537 -4.1863585 -4.1960149 -4.1950254 -4.1998982 -4.2065258 -4.2110252 -4.2176046 -4.2234931][-4.2704916 -4.2759256 -4.2755704 -4.2543893 -4.21389 -4.1832418 -4.1758289 -4.1781659 -4.1839237 -4.1927042 -4.20603 -4.2102084 -4.209918 -4.2153597 -4.2194009][-4.2548256 -4.2634048 -4.26509 -4.2460113 -4.2108068 -4.18087 -4.161993 -4.1563411 -4.1688423 -4.1898308 -4.2083611 -4.2085729 -4.2032876 -4.2077742 -4.211967][-4.2373233 -4.24529 -4.2449508 -4.2290378 -4.2032232 -4.1740375 -4.1518574 -4.1470428 -4.1627026 -4.1881971 -4.2045956 -4.2010255 -4.198575 -4.2077341 -4.2173138][-4.2128968 -4.2226486 -4.2240081 -4.2114158 -4.1955023 -4.1750178 -4.1566734 -4.1541505 -4.1656041 -4.1875372 -4.2022619 -4.2010446 -4.2062674 -4.2217274 -4.2352481][-4.1943126 -4.2068233 -4.2093124 -4.2000022 -4.1894608 -4.1791134 -4.1682563 -4.1664038 -4.1709437 -4.1871128 -4.2003894 -4.2046185 -4.2150717 -4.2309084 -4.2407322][-4.1994472 -4.2126527 -4.2128792 -4.2019691 -4.1929684 -4.1885304 -4.1853895 -4.1850877 -4.1837254 -4.1881943 -4.1951265 -4.2001085 -4.2121134 -4.2240238 -4.2282195]]...]
INFO - root - 2017-12-05 10:56:39.035192: step 1510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 78h:31m:33s remains)
INFO - root - 2017-12-05 10:56:47.571515: step 1520, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 77h:34m:54s remains)
INFO - root - 2017-12-05 10:56:56.134943: step 1530, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 79h:43m:09s remains)
INFO - root - 2017-12-05 10:57:04.741684: step 1540, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 80h:31m:33s remains)
INFO - root - 2017-12-05 10:57:13.205463: step 1550, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 75h:19m:57s remains)
INFO - root - 2017-12-05 10:57:21.687669: step 1560, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 76h:31m:52s remains)
INFO - root - 2017-12-05 10:57:30.125763: step 1570, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 75h:47m:13s remains)
INFO - root - 2017-12-05 10:57:38.607772: step 1580, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 76h:52m:04s remains)
INFO - root - 2017-12-05 10:57:47.068155: step 1590, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 79h:46m:26s remains)
INFO - root - 2017-12-05 10:57:55.623966: step 1600, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 78h:45m:11s remains)
2017-12-05 10:57:56.362973: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1786294 -4.1857791 -4.1884365 -4.19924 -4.2068992 -4.2107849 -4.2216005 -4.2304859 -4.2294164 -4.2100153 -4.1711092 -4.1396546 -4.1293583 -4.1291819 -4.1268535][-4.1777349 -4.1923571 -4.20238 -4.2129655 -4.2156024 -4.2113295 -4.2146654 -4.2189417 -4.2132587 -4.1870055 -4.1425767 -4.1075392 -4.0973148 -4.102705 -4.1075988][-4.1777916 -4.1923604 -4.1980987 -4.2025437 -4.1956477 -4.18307 -4.1758914 -4.1716704 -4.163341 -4.1422992 -4.1064358 -4.0797086 -4.076035 -4.0875182 -4.1014967][-4.1703596 -4.1795063 -4.1763725 -4.1700253 -4.1518016 -4.128695 -4.1089654 -4.0932236 -4.0874939 -4.0844879 -4.06799 -4.0545506 -4.0578332 -4.0732937 -4.0927525][-4.1485925 -4.1531105 -4.1475277 -4.1350646 -4.1083732 -4.0722094 -4.0328526 -4.0012183 -4.0046067 -4.0309682 -4.0410132 -4.0398674 -4.0465555 -4.0631418 -4.0792046][-4.1158733 -4.1134777 -4.1087594 -4.0985169 -4.0728469 -4.0257897 -3.9573083 -3.8963039 -3.9103091 -3.9772148 -4.0238376 -4.0407929 -4.0498943 -4.0595584 -4.0643983][-4.0806808 -4.0674763 -4.0617504 -4.0612907 -4.0463762 -3.99911 -3.908308 -3.8115652 -3.8239083 -3.9236135 -4.0045276 -4.0433083 -4.0619774 -4.0663557 -4.0649772][-4.0632949 -4.0393715 -4.0302496 -4.0390987 -4.0409942 -4.0137234 -3.9376338 -3.8419182 -3.8349316 -3.9186659 -4.0010853 -4.0535555 -4.0843344 -4.0924406 -4.0895815][-4.0800591 -4.0485573 -4.0334249 -4.0436816 -4.0575223 -4.0517693 -4.0057316 -3.9380293 -3.9148095 -3.95149 -4.004735 -4.0564704 -4.098331 -4.1151209 -4.1130714][-4.11455 -4.0805659 -4.062058 -4.0688467 -4.0839448 -4.08654 -4.0614386 -4.0196576 -3.9929445 -3.997 -4.019424 -4.0596781 -4.0986505 -4.1175613 -4.1183805][-4.1460648 -4.1136756 -4.0957923 -4.1023188 -4.1168175 -4.122118 -4.1104221 -4.0875 -4.0637145 -4.0476379 -4.0458884 -4.0697331 -4.09824 -4.1155071 -4.1208882][-4.172545 -4.14798 -4.13619 -4.1460485 -4.1618075 -4.1680036 -4.1633806 -4.1510282 -4.1296844 -4.1029229 -4.0853848 -4.0930376 -4.1119514 -4.1289034 -4.1388564][-4.1924877 -4.1804466 -4.1757379 -4.1881995 -4.2058611 -4.2135625 -4.21273 -4.2050071 -4.1862469 -4.1565456 -4.132585 -4.1325178 -4.1484962 -4.1649075 -4.1737604][-4.2017503 -4.1976027 -4.196692 -4.209322 -4.228581 -4.2389832 -4.2402349 -4.2366939 -4.2241039 -4.2001424 -4.1784382 -4.1754055 -4.1874685 -4.1998711 -4.2038407][-4.1970263 -4.1960907 -4.1931229 -4.2036576 -4.2253709 -4.2402554 -4.2422185 -4.2395225 -4.233088 -4.2163854 -4.1992621 -4.194943 -4.202424 -4.2089834 -4.2080846]]...]
INFO - root - 2017-12-05 10:58:04.772544: step 1610, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 78h:58m:13s remains)
INFO - root - 2017-12-05 10:58:13.388647: step 1620, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 79h:44m:00s remains)
INFO - root - 2017-12-05 10:58:22.031611: step 1630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 78h:30m:08s remains)
INFO - root - 2017-12-05 10:58:30.545404: step 1640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 78h:04m:55s remains)
INFO - root - 2017-12-05 10:58:38.970432: step 1650, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 75h:11m:36s remains)
INFO - root - 2017-12-05 10:58:47.551681: step 1660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 78h:54m:57s remains)
INFO - root - 2017-12-05 10:58:55.980559: step 1670, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 79h:04m:02s remains)
INFO - root - 2017-12-05 10:59:04.533584: step 1680, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 79h:21m:55s remains)
INFO - root - 2017-12-05 10:59:13.027975: step 1690, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 79h:30m:19s remains)
INFO - root - 2017-12-05 10:59:21.526159: step 1700, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 77h:51m:16s remains)
2017-12-05 10:59:22.316884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1952596 -4.1378107 -4.100163 -4.1144986 -4.1624885 -4.2178106 -4.2631574 -4.2814484 -4.2832842 -4.2799273 -4.2722411 -4.2690873 -4.2730074 -4.2730846 -4.2614141][-4.1802592 -4.107091 -4.0640535 -4.0927224 -4.1561651 -4.2209258 -4.2644968 -4.2800531 -4.2804546 -4.2772202 -4.269774 -4.266644 -4.2736216 -4.2778435 -4.2605443][-4.1458187 -4.0638795 -4.0216951 -4.0567384 -4.1341825 -4.2099566 -4.2484689 -4.2571964 -4.2520452 -4.2482214 -4.2466464 -4.2522063 -4.2689271 -4.2796283 -4.2591596][-4.1219769 -4.0357513 -4.004159 -4.04801 -4.1348081 -4.2142334 -4.239892 -4.2343121 -4.2181048 -4.2115684 -4.2207785 -4.2415771 -4.2697387 -4.2830811 -4.2601485][-4.1345072 -4.0658488 -4.0567074 -4.1036859 -4.18181 -4.2423964 -4.2408824 -4.207047 -4.1717482 -4.167592 -4.1947093 -4.2327652 -4.2706656 -4.2825327 -4.2570729][-4.1776829 -4.1382442 -4.1499014 -4.1896167 -4.2413182 -4.2642407 -4.2286716 -4.1570487 -4.0986905 -4.1053123 -4.1566625 -4.2107148 -4.2565751 -4.267386 -4.2411609][-4.2165375 -4.205071 -4.2229781 -4.2497325 -4.271183 -4.2563281 -4.1812053 -4.06204 -3.9791415 -4.0077915 -4.0954709 -4.17144 -4.2242789 -4.2359385 -4.2145495][-4.2334075 -4.2392921 -4.2528315 -4.2618766 -4.2525344 -4.1987481 -4.0786138 -3.9091327 -3.8149118 -3.8854475 -4.0194468 -4.1227727 -4.1870689 -4.2048712 -4.192997][-4.2412362 -4.2578421 -4.2636366 -4.255425 -4.2213483 -4.141737 -3.9907732 -3.7986059 -3.719393 -3.8270133 -3.9851358 -4.1018496 -4.1696839 -4.1964564 -4.1935005][-4.2536917 -4.2720428 -4.267561 -4.2478862 -4.2057924 -4.12829 -4.0015454 -3.8645635 -3.8356419 -3.9281721 -4.0486736 -4.1430426 -4.1988244 -4.2198763 -4.2133379][-4.2661114 -4.2794566 -4.2659593 -4.244545 -4.2082357 -4.1498942 -4.06977 -4.0008044 -4.00257 -4.0676708 -4.1463513 -4.2108173 -4.2459412 -4.2524118 -4.2379217][-4.2663193 -4.2744389 -4.2616072 -4.2447748 -4.2191114 -4.1833148 -4.1399159 -4.1098232 -4.1215982 -4.1697092 -4.2226439 -4.2649808 -4.2872114 -4.286901 -4.2703071][-4.2675123 -4.2710595 -4.2621694 -4.2507868 -4.2367754 -4.218399 -4.1975055 -4.1871929 -4.2032118 -4.2388725 -4.2743249 -4.3006091 -4.3146811 -4.3123279 -4.2958531][-4.2812123 -4.2812047 -4.2729192 -4.2668281 -4.2601728 -4.2529292 -4.2451491 -4.2456932 -4.2619267 -4.2864652 -4.3081069 -4.3218613 -4.3296852 -4.327364 -4.31412][-4.3062115 -4.3035235 -4.2957377 -4.290493 -4.2874837 -4.286139 -4.2847972 -4.2884374 -4.300034 -4.3148451 -4.3253822 -4.3296313 -4.3341446 -4.3343453 -4.325161]]...]
INFO - root - 2017-12-05 10:59:30.863441: step 1710, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 78h:47m:23s remains)
INFO - root - 2017-12-05 10:59:39.323141: step 1720, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:19m:06s remains)
INFO - root - 2017-12-05 10:59:47.900717: step 1730, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 79h:34m:10s remains)
INFO - root - 2017-12-05 10:59:56.366624: step 1740, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 77h:06m:24s remains)
INFO - root - 2017-12-05 11:00:04.820123: step 1750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 77h:42m:18s remains)
INFO - root - 2017-12-05 11:00:13.270775: step 1760, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 80h:12m:04s remains)
INFO - root - 2017-12-05 11:00:21.717011: step 1770, loss = 2.07, batch loss = 2.02 (10.7 examples/sec; 0.748 sec/batch; 68h:43m:06s remains)
INFO - root - 2017-12-05 11:00:30.280977: step 1780, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 77h:55m:00s remains)
INFO - root - 2017-12-05 11:00:38.755958: step 1790, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 77h:19m:24s remains)
INFO - root - 2017-12-05 11:00:47.170126: step 1800, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 77h:35m:23s remains)
2017-12-05 11:00:47.901470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1425643 -4.140871 -4.1424708 -4.1390057 -4.1346116 -4.1484923 -4.172925 -4.1999435 -4.21401 -4.2168279 -4.2095733 -4.2040558 -4.2009578 -4.1953597 -4.187336][-4.1146846 -4.1128597 -4.1149898 -4.1094446 -4.1044517 -4.1248226 -4.1579309 -4.1869831 -4.2019238 -4.2015133 -4.189558 -4.1795473 -4.1762819 -4.1729374 -4.167809][-4.0973992 -4.0868778 -4.082952 -4.0754838 -4.0726805 -4.0987673 -4.1431265 -4.1799989 -4.1991138 -4.1967864 -4.1823092 -4.1702223 -4.16399 -4.1625 -4.1622691][-4.0972786 -4.0852175 -4.0769114 -4.0607581 -4.0530252 -4.0795689 -4.1304913 -4.1786633 -4.2033968 -4.2023597 -4.186121 -4.1682487 -4.1543379 -4.1498942 -4.1567178][-4.1265068 -4.1227393 -4.1150537 -4.0785933 -4.0459466 -4.0557365 -4.1062965 -4.1671782 -4.2067976 -4.2113795 -4.1926022 -4.1658616 -4.1414003 -4.1332874 -4.1440744][-4.1575484 -4.1612196 -4.1568613 -4.1065946 -4.0453758 -4.0269122 -4.0651407 -4.1322923 -4.1870713 -4.1987219 -4.1821532 -4.1512651 -4.1100368 -4.08713 -4.0989842][-4.1766191 -4.1870041 -4.1822886 -4.1270037 -4.04396 -3.9942644 -4.0083 -4.0710864 -4.1380529 -4.1643386 -4.1571445 -4.123611 -4.0656509 -4.0169821 -4.023675][-4.1788583 -4.1933379 -4.185308 -4.1296391 -4.028811 -3.9348576 -3.9170647 -3.9804697 -4.0674076 -4.11569 -4.1240063 -4.0930128 -4.02437 -3.9524732 -3.94487][-4.1497307 -4.1693211 -4.16472 -4.1137319 -4.00989 -3.8784463 -3.8172772 -3.8752403 -3.9925346 -4.0699039 -4.0991349 -4.0831709 -4.0231514 -3.9422879 -3.9210784][-4.1013517 -4.1279426 -4.1336489 -4.0982594 -4.0187783 -3.89726 -3.8084991 -3.8402946 -3.9519634 -4.042192 -4.085484 -4.0895185 -4.0572529 -4.0001192 -3.9803591][-4.060277 -4.0898418 -4.1098909 -4.0961723 -4.0577478 -3.9940817 -3.9301291 -3.9262147 -3.9891934 -4.0582604 -4.0978303 -4.1135015 -4.1092963 -4.0822234 -4.0710831][-4.0732803 -4.0965633 -4.11849 -4.1171603 -4.1124926 -4.1057353 -4.0845113 -4.0667562 -4.0826631 -4.1151996 -4.1415153 -4.1563692 -4.1637049 -4.1540728 -4.1486259][-4.1327815 -4.1405721 -4.1517062 -4.1521425 -4.1622839 -4.1799512 -4.1812372 -4.1704473 -4.1681881 -4.1779509 -4.1955466 -4.2111588 -4.2246504 -4.2244935 -4.2190385][-4.2038178 -4.1988592 -4.1986141 -4.2013226 -4.2195382 -4.2399588 -4.2451215 -4.2418265 -4.2392507 -4.2433677 -4.2560592 -4.2700973 -4.2846112 -4.2871709 -4.2762642][-4.2512217 -4.2465706 -4.2425666 -4.2473145 -4.2665262 -4.2811184 -4.2824349 -4.2822566 -4.2835159 -4.2890553 -4.2965283 -4.303782 -4.311893 -4.3132253 -4.302618]]...]
INFO - root - 2017-12-05 11:00:56.349884: step 1810, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 78h:29m:37s remains)
INFO - root - 2017-12-05 11:01:04.920352: step 1820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 79h:34m:51s remains)
INFO - root - 2017-12-05 11:01:13.451009: step 1830, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 77h:28m:45s remains)
INFO - root - 2017-12-05 11:01:21.863318: step 1840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:33m:36s remains)
INFO - root - 2017-12-05 11:01:30.402031: step 1850, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 79h:05m:51s remains)
INFO - root - 2017-12-05 11:01:38.858712: step 1860, loss = 2.01, batch loss = 1.95 (9.2 examples/sec; 0.865 sec/batch; 79h:27m:16s remains)
INFO - root - 2017-12-05 11:01:47.402386: step 1870, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 79h:04m:13s remains)
INFO - root - 2017-12-05 11:01:55.800232: step 1880, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 75h:10m:03s remains)
INFO - root - 2017-12-05 11:02:04.283414: step 1890, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 78h:44m:24s remains)
INFO - root - 2017-12-05 11:02:12.806289: step 1900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 77h:50m:16s remains)
2017-12-05 11:02:13.650049: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1893044 -4.1845822 -4.1919012 -4.2027717 -4.2137275 -4.229887 -4.2475038 -4.2552953 -4.2578206 -4.2573147 -4.2467065 -4.2361908 -4.2332954 -4.2282429 -4.2149506][-4.1903563 -4.1902127 -4.2042365 -4.2219729 -4.2399597 -4.2574253 -4.274394 -4.2811966 -4.2823205 -4.2789092 -4.2675686 -4.2601662 -4.262145 -4.2616835 -4.25264][-4.2246981 -4.2258582 -4.2391076 -4.257443 -4.277833 -4.2956686 -4.3104353 -4.316576 -4.3168807 -4.3124313 -4.30301 -4.3000383 -4.3043709 -4.3054709 -4.3001919][-4.2633882 -4.2599115 -4.2644353 -4.2753625 -4.2895069 -4.3012395 -4.31007 -4.3122554 -4.3103294 -4.3067141 -4.3028021 -4.3062496 -4.3148122 -4.3188028 -4.317801][-4.2859421 -4.2775149 -4.2710085 -4.2659945 -4.2628236 -4.2568874 -4.2488046 -4.2381258 -4.2339654 -4.2389154 -4.2483463 -4.2623234 -4.2787991 -4.29044 -4.2976527][-4.2806044 -4.2679172 -4.2484584 -4.223208 -4.1946177 -4.1634092 -4.132884 -4.1052284 -4.0995626 -4.1168246 -4.1398945 -4.1656561 -4.1890488 -4.2098165 -4.2279553][-4.2364049 -4.21541 -4.1814094 -4.1348376 -4.0816 -4.0201516 -3.958534 -3.909102 -3.9025595 -3.9391317 -3.98664 -4.0331187 -4.0702 -4.1040545 -4.1356134][-4.1940112 -4.1647635 -4.1195874 -4.0610223 -3.9966025 -3.9266725 -3.8589714 -3.8146465 -3.8256989 -3.8868954 -3.9555206 -4.0144567 -4.0566864 -4.08791 -4.1168332][-4.1898928 -4.1617031 -4.1216764 -4.07528 -4.032414 -3.9935694 -3.955925 -3.9380531 -3.9566247 -4.009861 -4.0650764 -4.1085873 -4.1381679 -4.1541824 -4.1675239][-4.1976404 -4.1747546 -4.1486864 -4.1248746 -4.1079874 -4.096374 -4.0824018 -4.079206 -4.097394 -4.1370764 -4.1750097 -4.2008533 -4.2186184 -4.2233071 -4.2227554][-4.2110043 -4.1996841 -4.1930833 -4.1926332 -4.1941943 -4.1968384 -4.1929526 -4.1959195 -4.2126927 -4.2401791 -4.262753 -4.2734451 -4.2811289 -4.2788548 -4.2691011][-4.2466044 -4.2442288 -4.2494645 -4.26066 -4.2675552 -4.2701545 -4.2683554 -4.2706013 -4.2806082 -4.2950144 -4.3058567 -4.3064308 -4.3060756 -4.3004942 -4.2886209][-4.2746792 -4.2756333 -4.2852764 -4.3013277 -4.3087273 -4.3081288 -4.3045812 -4.300137 -4.2988167 -4.3034887 -4.3065915 -4.3018584 -4.2997808 -4.2961411 -4.2863631][-4.2774777 -4.278409 -4.2854886 -4.2986741 -4.3032746 -4.299664 -4.2977452 -4.2934704 -4.2903175 -4.2925224 -4.2930522 -4.2875204 -4.2846055 -4.2821774 -4.2736163][-4.2574687 -4.2540784 -4.2535233 -4.2587981 -4.2574964 -4.253922 -4.2589684 -4.2651782 -4.2666326 -4.2716513 -4.27564 -4.2758331 -4.2779493 -4.2785411 -4.2726927]]...]
INFO - root - 2017-12-05 11:02:22.028433: step 1910, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.812 sec/batch; 74h:33m:32s remains)
INFO - root - 2017-12-05 11:02:30.539016: step 1920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:35m:18s remains)
INFO - root - 2017-12-05 11:02:39.096625: step 1930, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 80h:12m:11s remains)
INFO - root - 2017-12-05 11:02:47.616666: step 1940, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:49m:02s remains)
INFO - root - 2017-12-05 11:02:56.059494: step 1950, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 77h:06m:01s remains)
INFO - root - 2017-12-05 11:03:04.530707: step 1960, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 74h:59m:31s remains)
INFO - root - 2017-12-05 11:03:13.128963: step 1970, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 78h:36m:14s remains)
INFO - root - 2017-12-05 11:03:21.694716: step 1980, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 78h:50m:00s remains)
INFO - root - 2017-12-05 11:03:30.171994: step 1990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 79h:19m:46s remains)
INFO - root - 2017-12-05 11:03:38.527857: step 2000, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 77h:09m:59s remains)
2017-12-05 11:03:39.264422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2379313 -4.2333136 -4.2429228 -4.2541575 -4.2537475 -4.2512736 -4.2537675 -4.2486906 -4.2396755 -4.2390356 -4.2402253 -4.2434616 -4.2460747 -4.2506628 -4.2579637][-4.2355804 -4.2275133 -4.2342777 -4.2462492 -4.2465725 -4.2440095 -4.2473288 -4.2451773 -4.2339005 -4.2234468 -4.214323 -4.2118855 -4.2107286 -4.2158279 -4.2303505][-4.2368274 -4.2292752 -4.2321286 -4.2403674 -4.2368207 -4.2293792 -4.2307529 -4.234355 -4.2254596 -4.20582 -4.1890144 -4.1811514 -4.1709027 -4.1682272 -4.182004][-4.2306905 -4.2243781 -4.2214084 -4.2230082 -4.2132835 -4.200458 -4.1973529 -4.2078028 -4.2093821 -4.1926656 -4.1767316 -4.1678338 -4.1512375 -4.1366854 -4.1423144][-4.2089882 -4.2012396 -4.1927419 -4.1872783 -4.1722136 -4.1523046 -4.1420922 -4.1569314 -4.17424 -4.1760397 -4.171669 -4.1666946 -4.150836 -4.1317277 -4.133152][-4.1894197 -4.1744032 -4.1609416 -4.1522479 -4.1319308 -4.0969706 -4.0695786 -4.0840893 -4.1236091 -4.1547241 -4.169384 -4.1726446 -4.1609769 -4.1433687 -4.1448812][-4.1866293 -4.1610565 -4.1406932 -4.1306438 -4.104795 -4.0488052 -3.9914141 -3.9952159 -4.0608735 -4.1259556 -4.1617732 -4.1771832 -4.1725454 -4.16156 -4.1691685][-4.1878448 -4.1547441 -4.1276178 -4.1141634 -4.0870004 -4.0248923 -3.9466686 -3.9335275 -4.0116611 -4.0986619 -4.1494951 -4.1740923 -4.1760244 -4.1710835 -4.183322][-4.1877193 -4.1556535 -4.1327286 -4.1231833 -4.1096978 -4.0710678 -4.0098057 -3.9886854 -4.0420527 -4.1130915 -4.157969 -4.179513 -4.1795554 -4.1768947 -4.1889372][-4.1837273 -4.1608491 -4.1504331 -4.1520038 -4.1564484 -4.1452842 -4.1109571 -4.0911245 -4.1136088 -4.1519094 -4.1808362 -4.1941385 -4.1913052 -4.1887803 -4.1999516][-4.1716447 -4.1604548 -4.1634836 -4.1761312 -4.1935358 -4.2012825 -4.1861763 -4.1677861 -4.1720424 -4.1903806 -4.2091637 -4.2191563 -4.2168975 -4.211431 -4.2160039][-4.172451 -4.1687107 -4.1775103 -4.1930552 -4.21336 -4.2283082 -4.2231431 -4.2079544 -4.207211 -4.2185044 -4.2316875 -4.240943 -4.2414455 -4.2359581 -4.2347155][-4.1895537 -4.1865687 -4.19234 -4.2025285 -4.2201047 -4.2371249 -4.2397923 -4.2313652 -4.232461 -4.240591 -4.2475243 -4.2571621 -4.261487 -4.2597775 -4.2575021][-4.2074413 -4.2015896 -4.2016768 -4.2043729 -4.2173815 -4.2358513 -4.2450318 -4.2421412 -4.2448316 -4.2506022 -4.2527094 -4.2610316 -4.2684546 -4.2712955 -4.2707667][-4.2263818 -4.2180853 -4.2128506 -4.2096071 -4.2171831 -4.2333956 -4.2444119 -4.2464461 -4.2498369 -4.2529817 -4.2529082 -4.2584929 -4.2668786 -4.2710238 -4.2723942]]...]
INFO - root - 2017-12-05 11:03:47.730312: step 2010, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 79h:27m:15s remains)
INFO - root - 2017-12-05 11:03:56.247582: step 2020, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 78h:15m:07s remains)
INFO - root - 2017-12-05 11:04:04.802049: step 2030, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 76h:51m:11s remains)
INFO - root - 2017-12-05 11:04:13.351243: step 2040, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 76h:18m:27s remains)
INFO - root - 2017-12-05 11:04:21.889179: step 2050, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 78h:10m:32s remains)
INFO - root - 2017-12-05 11:04:30.385558: step 2060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:43m:37s remains)
INFO - root - 2017-12-05 11:04:38.920510: step 2070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 78h:09m:01s remains)
INFO - root - 2017-12-05 11:04:47.371970: step 2080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 77h:53m:54s remains)
INFO - root - 2017-12-05 11:04:55.893119: step 2090, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 75h:56m:07s remains)
INFO - root - 2017-12-05 11:05:04.286601: step 2100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 79h:37m:33s remains)
2017-12-05 11:05:04.992196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.12874 -4.1189322 -4.1173506 -4.1304774 -4.1407251 -4.1471162 -4.154542 -4.1709666 -4.1923156 -4.21401 -4.2382059 -4.2506475 -4.2560492 -4.2645264 -4.2649422][-4.1376095 -4.1354146 -4.1394877 -4.1509161 -4.1618772 -4.1672325 -4.1700039 -4.1871061 -4.2163215 -4.245182 -4.2671089 -4.2753325 -4.2769608 -4.2806573 -4.2793856][-4.1601734 -4.1552591 -4.1563206 -4.1660366 -4.1754417 -4.1770535 -4.1741724 -4.190196 -4.2194271 -4.249114 -4.2693963 -4.2814503 -4.2906542 -4.2929006 -4.2891197][-4.1751065 -4.1594357 -4.1554193 -4.1642509 -4.1672063 -4.1580963 -4.1482735 -4.1589594 -4.1879692 -4.2204876 -4.2463865 -4.2646017 -4.2818775 -4.2881522 -4.2883129][-4.1795 -4.15811 -4.1506009 -4.1572785 -4.1452913 -4.1120567 -4.0855865 -4.0874767 -4.122426 -4.1701946 -4.2120228 -4.2389827 -4.2660332 -4.2804985 -4.2906132][-4.1893578 -4.17004 -4.1580114 -4.1546855 -4.1232338 -4.0592208 -4.0007935 -3.9886427 -4.0349565 -4.1094747 -4.1740909 -4.2181478 -4.2534456 -4.2763014 -4.2952862][-4.212883 -4.1985178 -4.1846271 -4.1722536 -4.1249228 -4.0392132 -3.9502769 -3.9197869 -3.9706268 -4.062314 -4.1418943 -4.2021761 -4.2476387 -4.277154 -4.2994208][-4.2266865 -4.2224393 -4.209168 -4.1926074 -4.1438203 -4.0564146 -3.9599698 -3.9206085 -3.9614291 -4.046474 -4.1260047 -4.1920848 -4.240984 -4.2705154 -4.2887454][-4.2294297 -4.2317286 -4.2165732 -4.1960092 -4.149138 -4.0710645 -3.9837525 -3.9479651 -3.9829915 -4.058887 -4.1314154 -4.1929359 -4.2357016 -4.2603941 -4.2734966][-4.2281494 -4.2322216 -4.2138748 -4.1884446 -4.1463795 -4.0850067 -4.0145764 -3.9880571 -4.0192547 -4.0816593 -4.142489 -4.1955595 -4.2288737 -4.2467709 -4.254313][-4.2205477 -4.22214 -4.2019587 -4.1781979 -4.1479797 -4.1070786 -4.0574641 -4.0356212 -4.0605044 -4.1098409 -4.1559935 -4.1920018 -4.2118759 -4.226779 -4.2342997][-4.2150221 -4.2121415 -4.1925411 -4.1726608 -4.1535826 -4.1326332 -4.1045136 -4.0893488 -4.1079707 -4.1484137 -4.1819744 -4.1980896 -4.2004986 -4.2047987 -4.2137804][-4.2211933 -4.2184548 -4.202282 -4.1847944 -4.1710157 -4.1577086 -4.1416845 -4.1339087 -4.1510682 -4.182569 -4.2052712 -4.2053127 -4.1859107 -4.1717443 -4.1778026][-4.2321115 -4.2340331 -4.2251816 -4.2131543 -4.2011671 -4.1896057 -4.1784039 -4.1771126 -4.1907425 -4.2114429 -4.2226734 -4.2076278 -4.1687603 -4.1318293 -4.1294527][-4.2291861 -4.2315788 -4.2295656 -4.2263927 -4.2211423 -4.2150273 -4.2100444 -4.213325 -4.2226615 -4.2329597 -4.2343769 -4.2117138 -4.1636887 -4.1098819 -4.0906949]]...]
INFO - root - 2017-12-05 11:05:13.352241: step 2110, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 77h:41m:30s remains)
INFO - root - 2017-12-05 11:05:21.926649: step 2120, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 78h:34m:59s remains)
INFO - root - 2017-12-05 11:05:30.457777: step 2130, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 76h:24m:48s remains)
INFO - root - 2017-12-05 11:05:38.845501: step 2140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 78h:01m:28s remains)
INFO - root - 2017-12-05 11:05:47.372694: step 2150, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 78h:13m:51s remains)
INFO - root - 2017-12-05 11:05:55.839450: step 2160, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 77h:47m:32s remains)
INFO - root - 2017-12-05 11:06:04.271966: step 2170, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 75h:34m:00s remains)
INFO - root - 2017-12-05 11:06:12.855522: step 2180, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 75h:13m:32s remains)
INFO - root - 2017-12-05 11:06:21.397920: step 2190, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 79h:06m:16s remains)
INFO - root - 2017-12-05 11:06:29.895819: step 2200, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 77h:55m:43s remains)
2017-12-05 11:06:30.656007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2380328 -4.2391906 -4.2543793 -4.2662606 -4.2536469 -4.2142467 -4.153132 -4.1008644 -4.09416 -4.1311026 -4.1735191 -4.2002816 -4.2231541 -4.2449651 -4.2504535][-4.2417722 -4.2434754 -4.26031 -4.2704606 -4.2472668 -4.1979461 -4.1251049 -4.07129 -4.0729604 -4.1209016 -4.1772728 -4.2163529 -4.2472878 -4.2704091 -4.2695107][-4.2434444 -4.2471089 -4.2653012 -4.2711124 -4.2346177 -4.17167 -4.085155 -4.0286579 -4.0406075 -4.1042504 -4.1760054 -4.2286849 -4.2662187 -4.2879 -4.2790632][-4.2532663 -4.2572165 -4.269062 -4.2668028 -4.2173543 -4.1365323 -4.0327744 -3.9709761 -3.9962258 -4.0823994 -4.1748581 -4.2409639 -4.2812977 -4.2979393 -4.2777691][-4.2647033 -4.2700319 -4.2747321 -4.2600317 -4.1981878 -4.0974007 -3.9768486 -3.9098854 -3.9486518 -4.0625272 -4.178041 -4.2555914 -4.2942729 -4.3047147 -4.2770839][-4.2754264 -4.2827444 -4.2813368 -4.2549181 -4.1843963 -4.0667343 -3.9285698 -3.8502741 -3.8974433 -4.0385232 -4.175858 -4.2629609 -4.3011637 -4.3060384 -4.2744741][-4.2852345 -4.2933121 -4.2864933 -4.2524605 -4.181572 -4.059588 -3.9113777 -3.8145735 -3.8564513 -4.008873 -4.1622639 -4.2611537 -4.3030262 -4.3048925 -4.2681689][-4.3005347 -4.306294 -4.2945209 -4.25781 -4.192481 -4.0781984 -3.9352589 -3.828258 -3.8545692 -3.9947326 -4.1493416 -4.2551651 -4.30165 -4.3051395 -4.2686][-4.3187728 -4.3220105 -4.3083348 -4.2727637 -4.2142591 -4.1168957 -3.9928112 -3.8940077 -3.9034016 -4.0157528 -4.154036 -4.25161 -4.2982163 -4.3052349 -4.275424][-4.3275943 -4.3277664 -4.3150883 -4.2850747 -4.2384634 -4.162396 -4.0672917 -3.9874322 -3.9825826 -4.058641 -4.1680875 -4.246685 -4.286386 -4.2960505 -4.275136][-4.3287177 -4.3228378 -4.3111548 -4.2897358 -4.2574277 -4.207521 -4.14484 -4.0873137 -4.0742531 -4.1182532 -4.19454 -4.2514744 -4.2806659 -4.2876229 -4.2736478][-4.3251724 -4.3135271 -4.3027515 -4.2876015 -4.2649388 -4.2347932 -4.1982841 -4.1648073 -4.1569619 -4.1853747 -4.2373962 -4.2752414 -4.2922516 -4.2945032 -4.2826085][-4.3184762 -4.3020267 -4.2920012 -4.2827835 -4.2666883 -4.2457719 -4.2234492 -4.207582 -4.209404 -4.2321863 -4.2694659 -4.2930861 -4.3006835 -4.3008962 -4.2911158][-4.3156943 -4.29704 -4.2861805 -4.27854 -4.2670593 -4.2526603 -4.2390585 -4.2327952 -4.2380838 -4.254571 -4.2796254 -4.2938752 -4.2970943 -4.2979078 -4.2923851][-4.3176432 -4.2995868 -4.2870355 -4.2786708 -4.2703753 -4.260735 -4.2526007 -4.2496653 -4.2540326 -4.2639265 -4.2794032 -4.2900896 -4.2926025 -4.2924051 -4.2888789]]...]
INFO - root - 2017-12-05 11:06:38.905453: step 2210, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 81h:11m:12s remains)
INFO - root - 2017-12-05 11:06:47.396553: step 2220, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 77h:50m:41s remains)
INFO - root - 2017-12-05 11:06:55.923293: step 2230, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 78h:22m:59s remains)
INFO - root - 2017-12-05 11:07:04.444796: step 2240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 78h:25m:40s remains)
INFO - root - 2017-12-05 11:07:13.049599: step 2250, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 80h:15m:07s remains)
INFO - root - 2017-12-05 11:07:21.559365: step 2260, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 75h:22m:17s remains)
INFO - root - 2017-12-05 11:07:30.041735: step 2270, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:43m:01s remains)
INFO - root - 2017-12-05 11:07:38.549651: step 2280, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 74h:38m:32s remains)
INFO - root - 2017-12-05 11:07:47.046543: step 2290, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 79h:28m:10s remains)
INFO - root - 2017-12-05 11:07:55.617397: step 2300, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.856 sec/batch; 78h:28m:21s remains)
2017-12-05 11:07:56.380889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275238 -4.2710567 -4.2574906 -4.232151 -4.199728 -4.17237 -4.1562824 -4.1432185 -4.1357307 -4.1327872 -4.1180844 -4.0931983 -4.0749459 -4.0933576 -4.1411138][-4.2560372 -4.2499943 -4.2284718 -4.197093 -4.1650705 -4.1415806 -4.1252627 -4.10762 -4.0906353 -4.0761027 -4.0694 -4.07263 -4.0869431 -4.1216478 -4.1653724][-4.2381945 -4.2294636 -4.1997337 -4.1626191 -4.1334338 -4.1129661 -4.0928483 -4.0680981 -4.0379109 -4.01029 -4.0223336 -4.0606427 -4.1058993 -4.15017 -4.195694][-4.2264185 -4.2082753 -4.1686788 -4.1275873 -4.1065545 -4.0979328 -4.0779934 -4.0477109 -4.0119753 -3.9964402 -4.0325031 -4.085207 -4.1365838 -4.1824284 -4.2258048][-4.2224627 -4.1994233 -4.1594281 -4.1217117 -4.1017361 -4.09082 -4.0657663 -4.030314 -4.009583 -4.0279827 -4.0827465 -4.1356106 -4.1805868 -4.2175937 -4.2501011][-4.2148056 -4.1960135 -4.1650109 -4.128962 -4.0992646 -4.06975 -4.0236678 -3.9701049 -3.9644544 -4.0273733 -4.1132469 -4.17273 -4.2110605 -4.24252 -4.2661819][-4.1986365 -4.187324 -4.1708803 -4.1412463 -4.09456 -4.0274997 -3.9388292 -3.8599281 -3.8832102 -4.0086069 -4.1223526 -4.1880951 -4.2269311 -4.2577839 -4.2760911][-4.1707282 -4.1610637 -4.1535187 -4.128098 -4.0702291 -3.9873514 -3.8986008 -3.8410349 -3.9030843 -4.0438662 -4.1437922 -4.2001777 -4.2381353 -4.2669678 -4.2790494][-4.1203332 -4.1053243 -4.102643 -4.0948744 -4.067101 -4.0297647 -3.9991548 -3.994014 -4.0546489 -4.139317 -4.1911106 -4.22198 -4.2500725 -4.2697358 -4.2778797][-4.0659122 -4.0610151 -4.0799923 -4.1001987 -4.1043038 -4.1064768 -4.1098189 -4.1247668 -4.1668811 -4.2092791 -4.2292042 -4.2432485 -4.2625585 -4.27419 -4.2820897][-4.0662313 -4.0853233 -4.1130981 -4.1372795 -4.151937 -4.1621842 -4.1679368 -4.1844306 -4.2146106 -4.2365484 -4.2445946 -4.2556243 -4.2729182 -4.2819462 -4.2909346][-4.1146646 -4.1412115 -4.1622214 -4.1774058 -4.1862106 -4.1907668 -4.1894975 -4.2010541 -4.2243476 -4.2422333 -4.2526374 -4.2667284 -4.2825432 -4.2912374 -4.3008051][-4.1514678 -4.1747632 -4.1869574 -4.1931987 -4.20017 -4.2026076 -4.2006764 -4.2088804 -4.2273178 -4.2438064 -4.2571073 -4.2747893 -4.29159 -4.3013606 -4.3086591][-4.1658263 -4.184267 -4.1875196 -4.1907349 -4.2004261 -4.2076321 -4.2144618 -4.2250261 -4.2386041 -4.2551088 -4.2696266 -4.2870822 -4.3024583 -4.310111 -4.3127789][-4.1721 -4.1902289 -4.1920528 -4.1945157 -4.2039938 -4.2167926 -4.2335148 -4.2470732 -4.2582932 -4.2736325 -4.2876711 -4.3020177 -4.3107257 -4.3130937 -4.3111243]]...]
INFO - root - 2017-12-05 11:08:04.807894: step 2310, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 78h:30m:42s remains)
INFO - root - 2017-12-05 11:08:13.183929: step 2320, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 76h:47m:00s remains)
INFO - root - 2017-12-05 11:08:21.630258: step 2330, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 79h:38m:11s remains)
INFO - root - 2017-12-05 11:08:30.166270: step 2340, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 80h:07m:10s remains)
INFO - root - 2017-12-05 11:08:38.706435: step 2350, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 77h:10m:53s remains)
INFO - root - 2017-12-05 11:08:47.310194: step 2360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 78h:53m:26s remains)
INFO - root - 2017-12-05 11:08:55.754451: step 2370, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 77h:58m:40s remains)
INFO - root - 2017-12-05 11:09:04.305081: step 2380, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 76h:15m:23s remains)
INFO - root - 2017-12-05 11:09:12.913147: step 2390, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 77h:43m:35s remains)
INFO - root - 2017-12-05 11:09:21.526569: step 2400, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 78h:24m:47s remains)
2017-12-05 11:09:22.332783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1211238 -4.1104922 -4.1237154 -4.1405025 -4.1597323 -4.1824889 -4.1942048 -4.1879034 -4.1865306 -4.2069716 -4.2305484 -4.2476234 -4.2723794 -4.29907 -4.3148723][-4.1180248 -4.1139116 -4.1337991 -4.1546049 -4.1718068 -4.1902676 -4.2031174 -4.1939707 -4.1849084 -4.2003179 -4.2205935 -4.2392383 -4.2703409 -4.3013768 -4.3161612][-4.1055112 -4.1128583 -4.1428666 -4.1723356 -4.1876535 -4.1978459 -4.2009268 -4.1856036 -4.1739068 -4.1841931 -4.1987166 -4.2185664 -4.25624 -4.2936158 -4.3113894][-4.1045828 -4.1214681 -4.158803 -4.1909542 -4.2023082 -4.2035279 -4.1938796 -4.1759005 -4.1639538 -4.1695433 -4.1815772 -4.2040124 -4.243711 -4.2830758 -4.3030448][-4.129425 -4.1484036 -4.1820831 -4.2059374 -4.2088423 -4.1982827 -4.1735868 -4.1502419 -4.1391215 -4.1489568 -4.1685 -4.198884 -4.2398357 -4.2777433 -4.2992377][-4.1304808 -4.1464772 -4.1738086 -4.1883745 -4.1801963 -4.1522441 -4.1077566 -4.070013 -4.0599713 -4.0883808 -4.1294956 -4.1777668 -4.22828 -4.271162 -4.2968106][-4.1160183 -4.1278734 -4.1423545 -4.1426172 -4.1236386 -4.0783358 -4.0058246 -3.9344025 -3.9171696 -3.9726288 -4.0460496 -4.1201882 -4.1902714 -4.2470832 -4.2828455][-4.0945449 -4.1059432 -4.1111984 -4.1024442 -4.0813141 -4.0301232 -3.9413738 -3.8413069 -3.8082478 -3.8745153 -3.9639187 -4.055089 -4.1412072 -4.212709 -4.2606888][-4.0868874 -4.1045985 -4.1123838 -4.1086636 -4.1017442 -4.0729694 -4.0067425 -3.9165883 -3.8732924 -3.9082813 -3.9707828 -4.04799 -4.1282434 -4.1978183 -4.2488823][-4.11443 -4.13547 -4.1482363 -4.1563745 -4.1657095 -4.1601019 -4.1252656 -4.0648546 -4.0267015 -4.0302024 -4.0544953 -4.0998058 -4.1576886 -4.2121654 -4.2552528][-4.1696029 -4.1855078 -4.1960964 -4.2076135 -4.2208328 -4.2240191 -4.2070012 -4.1721926 -4.1451249 -4.1395903 -4.1458893 -4.1691647 -4.2023826 -4.2391577 -4.2731218][-4.2211566 -4.231235 -4.2380209 -4.2471724 -4.2580667 -4.2650433 -4.260623 -4.24515 -4.2305555 -4.2253509 -4.2244706 -4.2339258 -4.248457 -4.2674351 -4.2903204][-4.2679358 -4.2757449 -4.2797565 -4.2852917 -4.2925138 -4.2990174 -4.300683 -4.2957215 -4.2892303 -4.2855749 -4.282433 -4.2866478 -4.2917089 -4.2984586 -4.3091159][-4.3070011 -4.3124375 -4.3152013 -4.3176093 -4.3213797 -4.3263645 -4.32941 -4.3280377 -4.3240724 -4.3204746 -4.3180823 -4.3204784 -4.3225513 -4.3242683 -4.3275928][-4.3257484 -4.3269033 -4.3265333 -4.326612 -4.3284307 -4.3315368 -4.3328018 -4.3319325 -4.3293276 -4.326438 -4.3258533 -4.3294153 -4.3333349 -4.3361149 -4.3379769]]...]
INFO - root - 2017-12-05 11:09:30.839835: step 2410, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 78h:33m:38s remains)
INFO - root - 2017-12-05 11:09:39.472627: step 2420, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 81h:48m:45s remains)
INFO - root - 2017-12-05 11:09:48.091621: step 2430, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 79h:34m:19s remains)
INFO - root - 2017-12-05 11:09:56.721050: step 2440, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 78h:57m:02s remains)
INFO - root - 2017-12-05 11:10:05.324916: step 2450, loss = 2.02, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 78h:51m:36s remains)
INFO - root - 2017-12-05 11:10:13.978886: step 2460, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 79h:33m:28s remains)
INFO - root - 2017-12-05 11:10:22.601214: step 2470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 78h:51m:05s remains)
INFO - root - 2017-12-05 11:10:31.234489: step 2480, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 79h:08m:10s remains)
INFO - root - 2017-12-05 11:10:39.910096: step 2490, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 81h:45m:38s remains)
INFO - root - 2017-12-05 11:10:48.545599: step 2500, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 81h:10m:53s remains)
2017-12-05 11:10:49.314979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2786765 -4.2773957 -4.2792592 -4.2852378 -4.2914119 -4.2957931 -4.2990832 -4.3049669 -4.3125229 -4.3135705 -4.3062935 -4.2991786 -4.2966504 -4.296483 -4.2994776][-4.2612319 -4.2571731 -4.2591677 -4.2657356 -4.2687936 -4.2697668 -4.2725029 -4.2839155 -4.2988248 -4.3065243 -4.2993064 -4.2874031 -4.2824984 -4.2811589 -4.2824788][-4.2322712 -4.2250876 -4.2253141 -4.2314806 -4.23397 -4.231442 -4.2353835 -4.2548132 -4.278379 -4.2915349 -4.2851419 -4.2699404 -4.2601295 -4.2574697 -4.2586513][-4.1877236 -4.17565 -4.1746535 -4.1786776 -4.185523 -4.1819539 -4.1817265 -4.205718 -4.2417078 -4.2651234 -4.2627292 -4.2480702 -4.2368336 -4.2318144 -4.231205][-4.1562047 -4.1415195 -4.1391082 -4.1440172 -4.149652 -4.1370063 -4.1220379 -4.1422644 -4.1908092 -4.2250977 -4.2294264 -4.2177396 -4.2064509 -4.20184 -4.2015371][-4.1368442 -4.1174288 -4.1082659 -4.10349 -4.105638 -4.0839953 -4.054441 -4.0697703 -4.1316223 -4.1818094 -4.1957808 -4.1900563 -4.1887245 -4.1943932 -4.1980181][-4.1228304 -4.0935445 -4.0714335 -4.0515118 -4.0429664 -4.0102291 -3.959738 -3.9634073 -4.0428538 -4.1210933 -4.1570287 -4.1666641 -4.1756444 -4.1889744 -4.20237][-4.1290617 -4.0925126 -4.0606384 -4.0274739 -3.999949 -3.9474502 -3.8684549 -3.8462074 -3.9380915 -4.0469537 -4.1113534 -4.1416597 -4.1606779 -4.1816349 -4.201436][-4.1362038 -4.096168 -4.0592132 -4.0208912 -3.9859967 -3.9390016 -3.8647981 -3.84055 -3.9226322 -4.0191588 -4.0793219 -4.112905 -4.1366997 -4.1652184 -4.1944971][-4.15673 -4.1204867 -4.083878 -4.04691 -4.015656 -3.9783731 -3.9181304 -3.8994629 -3.9542179 -4.0150285 -4.0529976 -4.0823164 -4.1090293 -4.141643 -4.1787434][-4.1491046 -4.1178155 -4.0920691 -4.069715 -4.0523167 -4.0269318 -3.9843845 -3.9666069 -3.9903023 -4.0127726 -4.0267453 -4.0535 -4.0855885 -4.1224914 -4.1608872][-4.13002 -4.1032581 -4.0847692 -4.0770111 -4.0752788 -4.0644064 -4.0393891 -4.0268416 -4.0352 -4.0362873 -4.0346165 -4.0554504 -4.0892916 -4.124341 -4.1600208][-4.1567383 -4.1320333 -4.1143112 -4.1096859 -4.1176448 -4.1192241 -4.1107788 -4.1072474 -4.1141381 -4.1068187 -4.0962782 -4.1116405 -4.1427259 -4.1698413 -4.1960154][-4.1996779 -4.1820874 -4.1703467 -4.1658292 -4.1734948 -4.1833429 -4.1818085 -4.1835237 -4.1929646 -4.1855006 -4.1728954 -4.1845894 -4.20854 -4.2277727 -4.2459664][-4.2458453 -4.2356038 -4.2307086 -4.2282672 -4.2354794 -4.2476339 -4.2504225 -4.2554317 -4.2662282 -4.2614875 -4.2535992 -4.2622471 -4.2751837 -4.2849607 -4.2947459]]...]
INFO - root - 2017-12-05 11:10:57.707893: step 2510, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 77h:45m:48s remains)
INFO - root - 2017-12-05 11:11:06.466147: step 2520, loss = 2.06, batch loss = 2.00 (8.4 examples/sec; 0.949 sec/batch; 86h:56m:43s remains)
INFO - root - 2017-12-05 11:11:15.045892: step 2530, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 79h:56m:49s remains)
INFO - root - 2017-12-05 11:11:23.578376: step 2540, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 79h:40m:22s remains)
INFO - root - 2017-12-05 11:11:32.173303: step 2550, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 77h:04m:28s remains)
INFO - root - 2017-12-05 11:11:40.795415: step 2560, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.856 sec/batch; 78h:24m:56s remains)
INFO - root - 2017-12-05 11:11:49.328159: step 2570, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 75h:50m:25s remains)
INFO - root - 2017-12-05 11:11:57.893106: step 2580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 77h:56m:40s remains)
INFO - root - 2017-12-05 11:12:06.545834: step 2590, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 79h:01m:04s remains)
INFO - root - 2017-12-05 11:12:15.056699: step 2600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 79h:05m:43s remains)
2017-12-05 11:12:15.808260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1922092 -4.1968679 -4.19628 -4.1905818 -4.1892467 -4.1899457 -4.190836 -4.191463 -4.187242 -4.1783075 -4.1691031 -4.1528749 -4.139226 -4.1446567 -4.1598091][-4.1801682 -4.1756291 -4.1702905 -4.1683125 -4.1775584 -4.1921773 -4.20398 -4.210145 -4.2043538 -4.1883907 -4.1679106 -4.14313 -4.1298943 -4.13787 -4.1548586][-4.1694336 -4.1631889 -4.1625514 -4.1699562 -4.1857333 -4.204927 -4.2189708 -4.2223244 -4.2092385 -4.1865525 -4.1588221 -4.1299639 -4.1190615 -4.126617 -4.1365166][-4.163588 -4.1612539 -4.1704354 -4.189405 -4.2083163 -4.2214246 -4.2265129 -4.2201223 -4.2036572 -4.1815629 -4.1545625 -4.1273937 -4.1121545 -4.1088829 -4.1057096][-4.1646328 -4.1694613 -4.1864738 -4.2129717 -4.2308388 -4.233882 -4.2255168 -4.2081318 -4.192637 -4.1801481 -4.1650448 -4.1425176 -4.1205916 -4.1025128 -4.0847883][-4.1743035 -4.1870437 -4.2067285 -4.2322288 -4.2441044 -4.2379608 -4.2195859 -4.1919394 -4.17292 -4.169415 -4.1680942 -4.1512127 -4.1238093 -4.0968018 -4.0737829][-4.178988 -4.1953321 -4.2143264 -4.23372 -4.2378006 -4.2243867 -4.1950893 -4.1553273 -4.1291208 -4.1308961 -4.1394043 -4.1304846 -4.1097112 -4.0937929 -4.0830407][-4.1690817 -4.189795 -4.2081442 -4.2208924 -4.2192888 -4.2008009 -4.1654234 -4.1186252 -4.0895681 -4.0922842 -4.1051664 -4.1093254 -4.1059942 -4.1086316 -4.1097364][-4.1592832 -4.1935987 -4.2153363 -4.2224836 -4.2150955 -4.1930017 -4.1558948 -4.1106768 -4.0858874 -4.0871229 -4.0979 -4.1091409 -4.1211762 -4.1323214 -4.1384034][-4.1535273 -4.19498 -4.218802 -4.2224655 -4.2137213 -4.1918478 -4.1571369 -4.1178179 -4.100832 -4.1052794 -4.1159773 -4.1325889 -4.1496091 -4.1622572 -4.1689091][-4.1546116 -4.1927953 -4.2157207 -4.2204485 -4.2130094 -4.1928015 -4.1634622 -4.1332307 -4.1247807 -4.1341419 -4.147604 -4.1617546 -4.1698055 -4.1744928 -4.1786175][-4.1660304 -4.1944065 -4.2113948 -4.2188888 -4.2153621 -4.1953 -4.1673656 -4.1447597 -4.1436396 -4.15758 -4.1724744 -4.1798992 -4.1764588 -4.1736259 -4.1784873][-4.1749048 -4.1964965 -4.2092819 -4.2171764 -4.2183828 -4.2026515 -4.1793876 -4.1634164 -4.1663923 -4.1823664 -4.1962681 -4.1981707 -4.18772 -4.1809382 -4.1869349][-4.188592 -4.2051382 -4.2168279 -4.225873 -4.2316866 -4.2239447 -4.2093582 -4.1988559 -4.2011003 -4.2138176 -4.223753 -4.2203913 -4.20739 -4.2011032 -4.2072639][-4.22101 -4.2304907 -4.2404652 -4.2481065 -4.2537012 -4.2505951 -4.2415638 -4.2347736 -4.235456 -4.2427111 -4.2491426 -4.2456551 -4.2355695 -4.2319722 -4.2378592]]...]
INFO - root - 2017-12-05 11:12:24.405009: step 2610, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 80h:01m:11s remains)
INFO - root - 2017-12-05 11:12:32.927867: step 2620, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 75h:47m:21s remains)
INFO - root - 2017-12-05 11:12:41.556141: step 2630, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 78h:50m:47s remains)
INFO - root - 2017-12-05 11:12:50.040201: step 2640, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 76h:02m:44s remains)
INFO - root - 2017-12-05 11:12:58.520204: step 2650, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 80h:26m:21s remains)
INFO - root - 2017-12-05 11:13:07.155453: step 2660, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 79h:35m:41s remains)
INFO - root - 2017-12-05 11:13:15.714752: step 2670, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 79h:06m:02s remains)
INFO - root - 2017-12-05 11:13:24.290741: step 2680, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 78h:15m:51s remains)
INFO - root - 2017-12-05 11:13:32.878392: step 2690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 77h:07m:03s remains)
INFO - root - 2017-12-05 11:13:41.491922: step 2700, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 76h:31m:49s remains)
2017-12-05 11:13:42.200264: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1218476 -4.1371937 -4.1789989 -4.2192836 -4.2418361 -4.2492409 -4.2583847 -4.2677941 -4.2650247 -4.2381558 -4.1875005 -4.1469617 -4.1364145 -4.1449656 -4.1555314][-4.0855227 -4.0978885 -4.145535 -4.1950321 -4.2277727 -4.237514 -4.2489295 -4.2650037 -4.2688527 -4.2408524 -4.1741223 -4.1135917 -4.0932136 -4.0985436 -4.1105771][-4.0600653 -4.0748024 -4.1276984 -4.1789818 -4.2096243 -4.2068548 -4.2114468 -4.2363081 -4.2560282 -4.23769 -4.1706576 -4.1062708 -4.0813251 -4.0838742 -4.0975709][-4.0787892 -4.0946736 -4.1401243 -4.17858 -4.1923966 -4.1622548 -4.15015 -4.1829224 -4.2309642 -4.2420378 -4.1996765 -4.1508155 -4.1280012 -4.1288133 -4.139956][-4.1278667 -4.1394982 -4.1684608 -4.1869793 -4.1734505 -4.1070566 -4.0663319 -4.1051469 -4.1899371 -4.2414432 -4.2358027 -4.2060704 -4.1850204 -4.184267 -4.1884785][-4.1645575 -4.1753292 -4.1885285 -4.1856794 -4.1366482 -4.0192666 -3.9349589 -3.9857962 -4.1134486 -4.2090421 -4.2422547 -4.2328415 -4.2208314 -4.2198172 -4.2197623][-4.1927853 -4.1960239 -4.1877708 -4.1543536 -4.0580378 -3.8735912 -3.7328897 -3.7989378 -3.9805744 -4.1259646 -4.2004657 -4.2216592 -4.23096 -4.2402711 -4.2461948][-4.1986289 -4.1934576 -4.1691418 -4.1168394 -3.9934394 -3.7664809 -3.5876575 -3.6716354 -3.896904 -4.0726275 -4.1697659 -4.2102118 -4.2336664 -4.2530236 -4.2638688][-4.190732 -4.1902871 -4.1740565 -4.1330152 -4.0203061 -3.8144584 -3.6647785 -3.7425931 -3.9398444 -4.0933576 -4.182333 -4.2214942 -4.2445226 -4.2662 -4.278306][-4.1773133 -4.1913161 -4.1975307 -4.180603 -4.0969872 -3.9402838 -3.830399 -3.8827305 -4.0247951 -4.1378813 -4.2082705 -4.2431884 -4.2656574 -4.2830563 -4.2861304][-4.1583619 -4.1903839 -4.2210617 -4.2222333 -4.1654921 -4.0545969 -3.9786928 -4.007484 -4.1018448 -4.1820788 -4.2355409 -4.2673025 -4.2882633 -4.2998157 -4.2917275][-4.1423573 -4.1859946 -4.232883 -4.2478161 -4.2177105 -4.149404 -4.1028829 -4.1178169 -4.1755037 -4.2288151 -4.2652974 -4.2926235 -4.3117352 -4.3181777 -4.3027759][-4.1334882 -4.1825294 -4.2366781 -4.259428 -4.2552047 -4.2260036 -4.2062669 -4.2135515 -4.2426677 -4.2707782 -4.2909503 -4.3088188 -4.3256745 -4.3293395 -4.310782][-4.1453052 -4.1915212 -4.2438288 -4.2701583 -4.2813182 -4.2783823 -4.27587 -4.2784939 -4.2882638 -4.2972512 -4.3054419 -4.3161359 -4.3277693 -4.32883 -4.3113513][-4.1511831 -4.1950574 -4.2463226 -4.2750325 -4.292757 -4.3018785 -4.3083797 -4.31069 -4.3129969 -4.3123674 -4.312542 -4.3174362 -4.3231959 -4.3225632 -4.3083839]]...]
INFO - root - 2017-12-05 11:13:50.619643: step 2710, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.886 sec/batch; 81h:08m:13s remains)
INFO - root - 2017-12-05 11:13:59.117767: step 2720, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 78h:05m:05s remains)
INFO - root - 2017-12-05 11:14:07.562089: step 2730, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 75h:20m:38s remains)
INFO - root - 2017-12-05 11:14:16.078503: step 2740, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 77h:43m:46s remains)
INFO - root - 2017-12-05 11:14:24.697939: step 2750, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:43m:55s remains)
INFO - root - 2017-12-05 11:14:33.163459: step 2760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 78h:32m:33s remains)
INFO - root - 2017-12-05 11:14:41.738724: step 2770, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 78h:06m:28s remains)
INFO - root - 2017-12-05 11:14:50.247015: step 2780, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 76h:29m:08s remains)
INFO - root - 2017-12-05 11:14:58.752115: step 2790, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 76h:51m:13s remains)
INFO - root - 2017-12-05 11:15:07.376976: step 2800, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 80h:10m:52s remains)
2017-12-05 11:15:08.187168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1693525 -4.1783442 -4.1713486 -4.1702876 -4.1811967 -4.1889057 -4.1627841 -4.1030354 -4.0506954 -4.0444255 -4.0964155 -4.1724477 -4.2481337 -4.3046336 -4.335907][-4.158987 -4.1559815 -4.1440549 -4.1434112 -4.1589971 -4.1737342 -4.1520853 -4.0934806 -4.0418363 -4.037456 -4.0945945 -4.1752286 -4.2513814 -4.3060031 -4.3341775][-4.1661935 -4.1487527 -4.1298618 -4.1215444 -4.1351714 -4.1571956 -4.1484351 -4.0967178 -4.041048 -4.0372391 -4.1019268 -4.1833181 -4.257381 -4.3075423 -4.3332343][-4.19059 -4.1684146 -4.14235 -4.1226249 -4.1295872 -4.1460195 -4.1386404 -4.0888562 -4.0259218 -4.0223608 -4.0934768 -4.1812658 -4.2576213 -4.3074803 -4.3330297][-4.219306 -4.2008352 -4.1725197 -4.1381536 -4.127533 -4.1271195 -4.1072974 -4.0544343 -3.9870312 -3.9887407 -4.0719681 -4.1699004 -4.2486477 -4.3006849 -4.3279376][-4.2318306 -4.2173576 -4.1885905 -4.1430254 -4.1039472 -4.0780697 -4.04828 -3.9916513 -3.9231751 -3.9311204 -4.0331783 -4.1457887 -4.2330217 -4.29006 -4.3209486][-4.222755 -4.2132125 -4.1849966 -4.1298161 -4.0586319 -4.0058322 -3.9775648 -3.9316254 -3.8686492 -3.885983 -4.0001345 -4.12269 -4.2184691 -4.2816486 -4.3164868][-4.2036948 -4.1999378 -4.1738062 -4.1147704 -4.02696 -3.9589636 -3.9309053 -3.8923337 -3.8402724 -3.8671708 -3.9869971 -4.1143165 -4.2154284 -4.2812719 -4.3167162][-4.1917853 -4.1839356 -4.1615868 -4.1166434 -4.0546012 -4.0094557 -3.9796069 -3.9276519 -3.8713267 -3.8926976 -3.9985785 -4.1199641 -4.2230535 -4.2889519 -4.322062][-4.189878 -4.1751375 -4.1547046 -4.1306195 -4.1216116 -4.1250215 -4.0996327 -4.0315447 -3.9633451 -3.9654181 -4.0436635 -4.1412525 -4.2344546 -4.2946243 -4.3250442][-4.1963015 -4.1774812 -4.1520448 -4.1318736 -4.1479845 -4.1818113 -4.1694469 -4.1069012 -4.041369 -4.0327897 -4.09097 -4.1669693 -4.2437696 -4.2950177 -4.3241787][-4.2074122 -4.1856313 -4.150353 -4.1241889 -4.1398096 -4.1765261 -4.171834 -4.1239629 -4.070787 -4.0625238 -4.1109309 -4.178226 -4.2447262 -4.2897735 -4.321619][-4.2113094 -4.1930413 -4.1586127 -4.1340933 -4.14325 -4.1667771 -4.1552258 -4.1130896 -4.0796785 -4.0758519 -4.1167793 -4.1804318 -4.2404642 -4.2827649 -4.3165336][-4.2028804 -4.1901503 -4.1646342 -4.1530151 -4.1616282 -4.1656361 -4.1388488 -4.1043582 -4.085638 -4.0871267 -4.1229849 -4.180233 -4.2349195 -4.276999 -4.3122134][-4.1835375 -4.1643662 -4.1386075 -4.1351972 -4.1469908 -4.1476088 -4.1238513 -4.1043506 -4.0991087 -4.1057768 -4.1365623 -4.1878448 -4.2365103 -4.2746229 -4.3090248]]...]
INFO - root - 2017-12-05 11:15:16.718391: step 2810, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 79h:06m:13s remains)
INFO - root - 2017-12-05 11:15:25.299821: step 2820, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 81h:09m:39s remains)
INFO - root - 2017-12-05 11:15:33.884142: step 2830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 78h:45m:56s remains)
INFO - root - 2017-12-05 11:15:42.414817: step 2840, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 79h:17m:17s remains)
INFO - root - 2017-12-05 11:15:51.049894: step 2850, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 78h:07m:42s remains)
INFO - root - 2017-12-05 11:15:59.554363: step 2860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 79h:37m:12s remains)
INFO - root - 2017-12-05 11:16:08.109961: step 2870, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 79h:52m:37s remains)
INFO - root - 2017-12-05 11:16:16.782692: step 2880, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 78h:06m:08s remains)
INFO - root - 2017-12-05 11:16:25.331860: step 2890, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:32m:21s remains)
INFO - root - 2017-12-05 11:16:33.883407: step 2900, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 76h:33m:34s remains)
2017-12-05 11:16:34.628403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367158 -4.251792 -4.2576714 -4.260613 -4.2547078 -4.2334108 -4.1993389 -4.1650548 -4.1512628 -4.1747789 -4.2199087 -4.2508268 -4.2517042 -4.2314415 -4.2142425][-4.237042 -4.2454433 -4.2441235 -4.2393823 -4.2307444 -4.2102947 -4.1795616 -4.1524792 -4.1471887 -4.180913 -4.2324934 -4.2636814 -4.2622337 -4.240561 -4.2208576][-4.2331033 -4.230773 -4.2210665 -4.2158484 -4.2149086 -4.2047453 -4.1843739 -4.164217 -4.1618466 -4.1944046 -4.2420931 -4.2665358 -4.2593775 -4.2373629 -4.2213225][-4.2364588 -4.2245245 -4.2083607 -4.20589 -4.215621 -4.217689 -4.2057285 -4.1879339 -4.1833444 -4.2072892 -4.2413445 -4.2554 -4.2455115 -4.2269979 -4.21618][-4.2376919 -4.2245412 -4.2106013 -4.21443 -4.2284284 -4.2345037 -4.225781 -4.2097273 -4.2025447 -4.2138438 -4.23084 -4.2343674 -4.2221589 -4.206573 -4.2001886][-4.2392883 -4.2366638 -4.2317758 -4.2387991 -4.249238 -4.2522426 -4.2436895 -4.2276735 -4.2171383 -4.2170763 -4.2192454 -4.2139587 -4.1985312 -4.185813 -4.1848135][-4.2495308 -4.2576218 -4.2599025 -4.2673035 -4.271915 -4.2687573 -4.2584772 -4.2440944 -4.2351222 -4.2329664 -4.2304721 -4.2202425 -4.2023335 -4.1891904 -4.1878195][-4.2620764 -4.2757649 -4.2780776 -4.2789555 -4.2736311 -4.2649441 -4.2538586 -4.2430563 -4.2399373 -4.2434816 -4.2458191 -4.2400527 -4.2258058 -4.2157464 -4.2111988][-4.2681012 -4.2867708 -4.2876759 -4.2799463 -4.2629809 -4.2488308 -4.2373686 -4.2296877 -4.2328625 -4.2416129 -4.2494 -4.2487149 -4.2428946 -4.2414551 -4.2369127][-4.2620935 -4.2879519 -4.2901421 -4.2762394 -4.2503591 -4.2303643 -4.219141 -4.2131042 -4.2200589 -4.2300396 -4.2401404 -4.2423348 -4.2435136 -4.2527361 -4.25487][-4.2529378 -4.2839165 -4.2872076 -4.2706938 -4.2389951 -4.2139082 -4.2033281 -4.1971164 -4.1988306 -4.204288 -4.211751 -4.2114272 -4.2162337 -4.2349424 -4.2482777][-4.2596231 -4.2888827 -4.2904067 -4.2709503 -4.2368808 -4.21043 -4.1992188 -4.190177 -4.1831722 -4.1808252 -4.1811719 -4.1723723 -4.1745806 -4.1966972 -4.217649][-4.2739158 -4.2916775 -4.2882376 -4.2685323 -4.2379131 -4.2147431 -4.2036214 -4.1943564 -4.1851091 -4.1783276 -4.1711664 -4.1556492 -4.1525331 -4.1693811 -4.1839762][-4.276854 -4.2818484 -4.2755013 -4.2634988 -4.2441039 -4.2257304 -4.2127028 -4.205647 -4.2022638 -4.196691 -4.1850834 -4.1681652 -4.1614313 -4.164803 -4.1612477][-4.2668042 -4.2644839 -4.2617769 -4.2610011 -4.2535815 -4.2401724 -4.2262063 -4.2229519 -4.2261243 -4.223084 -4.2106342 -4.1959853 -4.1869659 -4.1774116 -4.1567416]]...]
INFO - root - 2017-12-05 11:16:43.133780: step 2910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 78h:23m:01s remains)
INFO - root - 2017-12-05 11:16:51.794536: step 2920, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:36m:17s remains)
INFO - root - 2017-12-05 11:17:00.293154: step 2930, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.856 sec/batch; 78h:20m:27s remains)
INFO - root - 2017-12-05 11:17:08.842304: step 2940, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 80h:33m:44s remains)
INFO - root - 2017-12-05 11:17:17.440766: step 2950, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.886 sec/batch; 81h:04m:37s remains)
INFO - root - 2017-12-05 11:17:25.875249: step 2960, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.856 sec/batch; 78h:23m:35s remains)
INFO - root - 2017-12-05 11:17:34.620412: step 2970, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 81h:02m:55s remains)
INFO - root - 2017-12-05 11:17:43.122631: step 2980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 79h:06m:48s remains)
INFO - root - 2017-12-05 11:17:51.717411: step 2990, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:25m:17s remains)
INFO - root - 2017-12-05 11:18:00.228955: step 3000, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 81h:58m:58s remains)
2017-12-05 11:18:00.973819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2296638 -4.2328477 -4.2441578 -4.2582245 -4.2625327 -4.2530017 -4.2422142 -4.23606 -4.231679 -4.2345624 -4.2492328 -4.2675338 -4.2831812 -4.298079 -4.314683][-4.20272 -4.207562 -4.2241116 -4.2449884 -4.2525716 -4.2453218 -4.2365623 -4.2300363 -4.2244983 -4.2220364 -4.2328281 -4.248239 -4.2665114 -4.2841148 -4.2956305][-4.1839695 -4.1884546 -4.2086039 -4.2369714 -4.2502756 -4.245801 -4.2359662 -4.2286549 -4.224463 -4.217432 -4.2153811 -4.2231674 -4.2432551 -4.2650166 -4.2714868][-4.1671429 -4.1683331 -4.1879692 -4.21718 -4.2338181 -4.229105 -4.2142172 -4.20577 -4.2044854 -4.1916986 -4.1757 -4.1785512 -4.2035732 -4.227078 -4.2326522][-4.1479478 -4.1437874 -4.1584697 -4.1811395 -4.1924081 -4.1786556 -4.1577559 -4.1533837 -4.1578517 -4.13685 -4.111414 -4.118372 -4.1479859 -4.1752996 -4.1900797][-4.1208453 -4.106926 -4.1113071 -4.120337 -4.120193 -4.0967164 -4.0721536 -4.0742869 -4.0848918 -4.0632415 -4.0377207 -4.0523367 -4.0889816 -4.1226268 -4.1538157][-4.0888562 -4.063611 -4.0533028 -4.0470777 -4.0347481 -4.0074081 -3.9869785 -3.9974527 -4.01773 -4.0085511 -3.9866836 -4.0029354 -4.0390015 -4.0807986 -4.12283][-4.059495 -4.0274687 -4.0084577 -3.9935675 -3.9723773 -3.9412184 -3.9236996 -3.9408188 -3.9750922 -3.9879546 -3.9779308 -3.986907 -4.0142312 -4.0554438 -4.09469][-4.0361462 -3.9986596 -3.9756703 -3.9577215 -3.9378431 -3.9145031 -3.9054503 -3.9224346 -3.9549017 -3.9757686 -3.9735322 -3.9745455 -3.9924181 -4.0267148 -4.0594378][-4.046741 -4.0079279 -3.9855304 -3.969203 -3.9548104 -3.939939 -3.9387989 -3.9475453 -3.9641483 -3.9781668 -3.9768553 -3.9783585 -3.9918098 -4.0219846 -4.0531592][-4.0872278 -4.0513754 -4.0287681 -4.0102844 -3.9956794 -3.9846141 -3.9848371 -3.9876888 -3.9945016 -4.0039144 -4.0057254 -4.0122595 -4.0275 -4.0558996 -4.0879931][-4.1426005 -4.1162047 -4.0964308 -4.0784464 -4.064085 -4.0548296 -4.0540509 -4.0558357 -4.0613551 -4.0693231 -4.0729117 -4.0816727 -4.0978279 -4.1207924 -4.1488323][-4.2052593 -4.1916924 -4.1803403 -4.1688385 -4.1585035 -4.1515865 -4.14928 -4.1501193 -4.1541386 -4.1599183 -4.1644154 -4.1716146 -4.1829591 -4.1992235 -4.2203565][-4.2597446 -4.2562823 -4.2530866 -4.24825 -4.2432575 -4.2394533 -4.2365952 -4.2350655 -4.23627 -4.2397289 -4.2429485 -4.2473497 -4.2538004 -4.2630715 -4.2757306][-4.2922373 -4.2931042 -4.2931995 -4.29102 -4.2888937 -4.2873259 -4.2860794 -4.2853408 -4.2856226 -4.2868295 -4.2880673 -4.2906523 -4.2949071 -4.3007903 -4.3073988]]...]
INFO - root - 2017-12-05 11:18:09.460626: step 3010, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 77h:40m:43s remains)
INFO - root - 2017-12-05 11:18:18.099344: step 3020, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 80h:21m:56s remains)
INFO - root - 2017-12-05 11:18:26.733572: step 3030, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 80h:23m:33s remains)
INFO - root - 2017-12-05 11:18:35.275346: step 3040, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:51m:29s remains)
INFO - root - 2017-12-05 11:18:43.944281: step 3050, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 79h:09m:28s remains)
INFO - root - 2017-12-05 11:18:52.435029: step 3060, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.773 sec/batch; 70h:47m:01s remains)
INFO - root - 2017-12-05 11:19:00.942811: step 3070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:30m:51s remains)
INFO - root - 2017-12-05 11:19:09.446642: step 3080, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 80h:02m:04s remains)
INFO - root - 2017-12-05 11:19:17.957549: step 3090, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 76h:34m:53s remains)
INFO - root - 2017-12-05 11:19:26.620481: step 3100, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 79h:09m:27s remains)
2017-12-05 11:19:27.354493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3247252 -4.3223987 -4.3217282 -4.3217115 -4.3207827 -4.3194513 -4.3217092 -4.3240151 -4.3244424 -4.3235865 -4.3212004 -4.3209229 -4.3220921 -4.3206758 -4.3183413][-4.3194742 -4.316258 -4.3144975 -4.3129678 -4.309454 -4.3042345 -4.3065572 -4.3110909 -4.3113256 -4.3100829 -4.3070712 -4.3064246 -4.3067966 -4.3021588 -4.2973046][-4.3061953 -4.303339 -4.3001065 -4.2961297 -4.2896495 -4.279789 -4.2822161 -4.2908697 -4.2940388 -4.2917767 -4.2862005 -4.2851787 -4.281765 -4.2709112 -4.2619634][-4.2819247 -4.2813482 -4.2775593 -4.2710443 -4.260335 -4.2457542 -4.2465677 -4.2578554 -4.2654295 -4.2671752 -4.2601938 -4.2561488 -4.2456264 -4.228044 -4.2151752][-4.2489367 -4.2516088 -4.2467942 -4.2360778 -4.2178955 -4.1971407 -4.1898136 -4.20181 -4.2191114 -4.2250495 -4.2100015 -4.1964169 -4.1777487 -4.1543884 -4.1382394][-4.2079868 -4.2093606 -4.1989694 -4.1807036 -4.1522169 -4.1188412 -4.0965333 -4.1088219 -4.1437125 -4.1569886 -4.1341519 -4.113049 -4.0965977 -4.0782475 -4.063942][-4.1751862 -4.1662183 -4.1404133 -4.1062536 -4.0588217 -4.0016375 -3.9539564 -3.9705698 -4.0350738 -4.0656986 -4.0506124 -4.0376568 -4.042316 -4.0474358 -4.0452552][-4.1775303 -4.1517515 -4.1024389 -4.042582 -3.971704 -3.8860455 -3.8038244 -3.8268747 -3.9293473 -3.9888694 -3.9969854 -4.0118256 -4.0497527 -4.0867434 -4.1030464][-4.2075605 -4.1705008 -4.1058135 -4.0292568 -3.945756 -3.8479886 -3.7483864 -3.7716148 -3.892549 -3.9700952 -4.00127 -4.0392566 -4.1000156 -4.15879 -4.1910748][-4.2454882 -4.21054 -4.1509619 -4.0766025 -3.9992433 -3.9190798 -3.8490584 -3.8652701 -3.9525967 -4.0183129 -4.0569363 -4.1010017 -4.1623354 -4.2229667 -4.2621293][-4.2876172 -4.2629132 -4.2207284 -4.1633263 -4.1011367 -4.0450544 -4.0036073 -4.0093489 -4.0547743 -4.0968394 -4.1322756 -4.1715837 -4.2195225 -4.2691 -4.3026958][-4.3192167 -4.3072281 -4.2827883 -4.2475753 -4.2064886 -4.1711493 -4.1475954 -4.1487756 -4.1686077 -4.1899242 -4.2135797 -4.238265 -4.266624 -4.2943606 -4.3133082][-4.3320303 -4.3298235 -4.3200231 -4.3030849 -4.2831755 -4.266778 -4.2568541 -4.2577343 -4.2666731 -4.2740541 -4.28301 -4.2904067 -4.3005667 -4.3089237 -4.3146863][-4.3296585 -4.332449 -4.3306417 -4.3251977 -4.3188777 -4.3158503 -4.3159394 -4.3184776 -4.3220334 -4.3231611 -4.3232512 -4.3212252 -4.3217797 -4.3203545 -4.3192825][-4.3230648 -4.3272362 -4.3291521 -4.3284364 -4.3297715 -4.3331666 -4.336781 -4.3405905 -4.3430638 -4.3425431 -4.3403726 -4.33642 -4.3343053 -4.3290415 -4.3251028]]...]
INFO - root - 2017-12-05 11:19:35.801830: step 3110, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 77h:35m:40s remains)
INFO - root - 2017-12-05 11:19:44.414464: step 3120, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 79h:06m:20s remains)
INFO - root - 2017-12-05 11:19:52.892100: step 3130, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 79h:31m:02s remains)
INFO - root - 2017-12-05 11:20:01.433993: step 3140, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 78h:24m:27s remains)
INFO - root - 2017-12-05 11:20:09.998732: step 3150, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 80h:40m:15s remains)
INFO - root - 2017-12-05 11:20:18.652634: step 3160, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 76h:09m:19s remains)
INFO - root - 2017-12-05 11:20:27.145772: step 3170, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:31m:26s remains)
INFO - root - 2017-12-05 11:20:35.703892: step 3180, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 78h:26m:16s remains)
INFO - root - 2017-12-05 11:20:44.383901: step 3190, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 79h:17m:43s remains)
INFO - root - 2017-12-05 11:20:52.796673: step 3200, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.808 sec/batch; 73h:52m:58s remains)
2017-12-05 11:20:53.640989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2392311 -4.2567477 -4.2682285 -4.2573071 -4.2366185 -4.2159629 -4.2021246 -4.1909628 -4.1706233 -4.1489019 -4.1422253 -4.1480455 -4.1632891 -4.1876116 -4.2163811][-4.2434096 -4.2566314 -4.2667212 -4.255466 -4.2359505 -4.2171078 -4.206151 -4.20111 -4.190815 -4.1811142 -4.1750593 -4.1717849 -4.1841736 -4.2117128 -4.2454181][-4.2481003 -4.2619076 -4.2739997 -4.2686419 -4.257175 -4.2453032 -4.2361917 -4.2328467 -4.2323475 -4.2345595 -4.2308412 -4.221489 -4.2270861 -4.2485452 -4.2761855][-4.2571454 -4.2714038 -4.28336 -4.284481 -4.2814083 -4.2751427 -4.2637048 -4.2533059 -4.2550216 -4.2609739 -4.2573729 -4.2489142 -4.2527671 -4.2659693 -4.2850904][-4.2656517 -4.2797337 -4.2871761 -4.2879486 -4.2835793 -4.2712021 -4.2455668 -4.2174611 -4.2124581 -4.2209325 -4.2258692 -4.2325583 -4.2479668 -4.2600846 -4.2711425][-4.27109 -4.28195 -4.2807264 -4.2706108 -4.2513146 -4.2183204 -4.1668577 -4.1148467 -4.1078644 -4.1369042 -4.16873 -4.203095 -4.235261 -4.2478423 -4.2529616][-4.2790346 -4.2804618 -4.2639952 -4.2366195 -4.1963429 -4.1394877 -4.0656695 -3.9973919 -4.005547 -4.07334 -4.1403856 -4.1995678 -4.2388635 -4.246418 -4.2423606][-4.2922425 -4.2811217 -4.2497387 -4.2094474 -4.1575217 -4.0959597 -4.0294189 -3.976383 -4.0052376 -4.0943146 -4.1722364 -4.2316356 -4.2637596 -4.2608128 -4.24076][-4.29932 -4.2812376 -4.2457395 -4.2071061 -4.1634021 -4.1255441 -4.0930538 -4.0696135 -4.094943 -4.1598315 -4.2155375 -4.2565689 -4.279109 -4.2651896 -4.2258487][-4.2992 -4.2858987 -4.2606711 -4.2331338 -4.2034407 -4.1857553 -4.1753526 -4.1665158 -4.1765537 -4.2037559 -4.2210317 -4.2326436 -4.2432981 -4.2229867 -4.1719818][-4.2950778 -4.2912917 -4.2780337 -4.2592821 -4.2360072 -4.2245455 -4.2208252 -4.218039 -4.2129025 -4.2035074 -4.1808586 -4.16125 -4.161777 -4.1468616 -4.1028485][-4.286056 -4.2883325 -4.2809505 -4.2689056 -4.2523851 -4.2440929 -4.2414894 -4.2350082 -4.2117391 -4.1747079 -4.126791 -4.0920997 -4.09307 -4.0891943 -4.0602083][-4.273088 -4.2799921 -4.2794666 -4.2747464 -4.264163 -4.2611904 -4.2596312 -4.2449484 -4.2113261 -4.1643372 -4.1154761 -4.0873046 -4.0911765 -4.0879936 -4.0682383][-4.2624421 -4.273716 -4.281661 -4.2836113 -4.2741456 -4.2719746 -4.2685661 -4.2535796 -4.2249908 -4.1853838 -4.1476684 -4.1280217 -4.1281662 -4.1163311 -4.0994287][-4.2573929 -4.2725554 -4.2844558 -4.2840495 -4.2733965 -4.2718244 -4.2700348 -4.2593908 -4.2383113 -4.2067428 -4.1783595 -4.1626873 -4.1563396 -4.1420212 -4.1321025]]...]
INFO - root - 2017-12-05 11:21:02.241269: step 3210, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 81h:01m:35s remains)
INFO - root - 2017-12-05 11:21:10.866634: step 3220, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 77h:19m:50s remains)
INFO - root - 2017-12-05 11:21:19.380316: step 3230, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 79h:13m:00s remains)
INFO - root - 2017-12-05 11:21:27.935124: step 3240, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 76h:57m:45s remains)
INFO - root - 2017-12-05 11:21:36.491528: step 3250, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 77h:05m:02s remains)
INFO - root - 2017-12-05 11:21:45.016417: step 3260, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 78h:52m:09s remains)
INFO - root - 2017-12-05 11:21:53.593212: step 3270, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 77h:26m:27s remains)
INFO - root - 2017-12-05 11:22:02.112022: step 3280, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 79h:52m:50s remains)
INFO - root - 2017-12-05 11:22:10.692833: step 3290, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 79h:12m:54s remains)
INFO - root - 2017-12-05 11:22:19.257278: step 3300, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.813 sec/batch; 74h:18m:29s remains)
2017-12-05 11:22:20.013121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1017308 -4.1103849 -4.1483078 -4.1967635 -4.2300825 -4.2535138 -4.2714067 -4.2826858 -4.2940221 -4.2999663 -4.303967 -4.3051767 -4.2928176 -4.2667713 -4.2394586][-4.1524062 -4.159524 -4.1883621 -4.2246604 -4.2483506 -4.2653503 -4.2753091 -4.2805338 -4.2904887 -4.2976255 -4.3006172 -4.2971687 -4.275682 -4.2409749 -4.2112036][-4.1957326 -4.2039618 -4.222116 -4.2445745 -4.2583241 -4.2695117 -4.2751484 -4.2767496 -4.2841921 -4.2900023 -4.2891607 -4.2743573 -4.2379484 -4.190609 -4.1593113][-4.2375188 -4.2484446 -4.253427 -4.2567897 -4.2580962 -4.2656431 -4.2719469 -4.2740145 -4.2809086 -4.2854075 -4.2778506 -4.2472959 -4.1893325 -4.1207423 -4.0840979][-4.2617965 -4.2751374 -4.2643461 -4.2446342 -4.228384 -4.2312307 -4.2407804 -4.2514806 -4.265554 -4.2738962 -4.262845 -4.2178617 -4.13617 -4.0415273 -3.9886558][-4.2667279 -4.281374 -4.257812 -4.2214541 -4.1912804 -4.1824632 -4.1901684 -4.2062125 -4.2290072 -4.2465734 -4.238646 -4.1895742 -4.101182 -3.9950738 -3.927742][-4.2606511 -4.2769556 -4.2486458 -4.2049351 -4.1648164 -4.1416068 -4.1375175 -4.1440763 -4.1643348 -4.1899266 -4.1957774 -4.1642494 -4.1023197 -4.0200067 -3.9584842][-4.2482457 -4.2633123 -4.2373776 -4.19618 -4.1550288 -4.1184711 -4.0935717 -4.0770054 -4.0816617 -4.1126032 -4.1442809 -4.1522212 -4.140449 -4.1092043 -4.0779634][-4.2209592 -4.2218761 -4.1947126 -4.1601882 -4.1289439 -4.0895429 -4.0488615 -4.0137877 -4.0038891 -4.0399375 -4.1006403 -4.1479359 -4.179657 -4.191916 -4.1915674][-4.1760259 -4.1497011 -4.1104145 -4.0797677 -4.0653868 -4.0363207 -3.9960063 -3.9628127 -3.9551306 -4.0048242 -4.0876074 -4.1594296 -4.2126284 -4.2458944 -4.2607241][-4.1375628 -4.0849638 -4.0323563 -3.9982789 -3.9941554 -3.9745829 -3.9451051 -3.9302037 -3.9429255 -4.0079803 -4.1008754 -4.1821556 -4.2433267 -4.2821093 -4.3001194][-4.1265554 -4.06478 -4.0102158 -3.9743912 -3.9678757 -3.9477172 -3.9248776 -3.9243436 -3.9577787 -4.0346141 -4.1314597 -4.2127938 -4.2720561 -4.3071494 -4.3209391][-4.1470094 -4.0884638 -4.042942 -4.0159006 -4.0097671 -3.9916108 -3.9732006 -3.9742904 -4.0094485 -4.0834723 -4.1735148 -4.2460003 -4.2957845 -4.3230047 -4.331337][-4.1894989 -4.14596 -4.11364 -4.0968642 -4.09295 -4.08104 -4.0674086 -4.0650139 -4.0895405 -4.1472726 -4.2200165 -4.274713 -4.3113823 -4.331296 -4.336112][-4.2356644 -4.2127347 -4.1976051 -4.1900454 -4.1880169 -4.179533 -4.1699057 -4.1674995 -4.1810713 -4.2169132 -4.2628074 -4.2947435 -4.3169746 -4.3306308 -4.3344116]]...]
INFO - root - 2017-12-05 11:22:28.547272: step 3310, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:40m:22s remains)
INFO - root - 2017-12-05 11:22:37.072542: step 3320, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.863 sec/batch; 78h:53m:17s remains)
INFO - root - 2017-12-05 11:22:45.493084: step 3330, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 76h:32m:11s remains)
INFO - root - 2017-12-05 11:22:54.121615: step 3340, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 78h:34m:25s remains)
INFO - root - 2017-12-05 11:23:02.637389: step 3350, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 79h:43m:59s remains)
INFO - root - 2017-12-05 11:23:11.184652: step 3360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 79h:05m:59s remains)
INFO - root - 2017-12-05 11:23:19.801569: step 3370, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 79h:44m:33s remains)
INFO - root - 2017-12-05 11:23:28.445419: step 3380, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 78h:26m:52s remains)
INFO - root - 2017-12-05 11:23:36.984424: step 3390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 77h:49m:39s remains)
INFO - root - 2017-12-05 11:23:45.519752: step 3400, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 79h:48m:24s remains)
2017-12-05 11:23:46.209377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0597515 -4.0741239 -4.0941234 -4.1225009 -4.1423688 -4.1434164 -4.1387029 -4.1488872 -4.1691971 -4.1889071 -4.204268 -4.2140675 -4.20845 -4.199079 -4.1945019][-4.0306621 -4.0668468 -4.1008048 -4.1331429 -4.1589603 -4.1657262 -4.1652026 -4.1722083 -4.181242 -4.1929121 -4.2100883 -4.2224259 -4.2135167 -4.1903658 -4.1725945][-4.0360837 -4.0831327 -4.1282425 -4.1631966 -4.1906567 -4.2001009 -4.1984153 -4.1936345 -4.1886687 -4.1962447 -4.2149458 -4.2286925 -4.2185044 -4.1836329 -4.1569095][-4.0788026 -4.1262393 -4.1698122 -4.1945152 -4.2155161 -4.2237544 -4.2145681 -4.1941943 -4.1768608 -4.1868477 -4.213387 -4.2339296 -4.2265463 -4.18777 -4.1595259][-4.1220436 -4.1628647 -4.1931705 -4.2051272 -4.2150049 -4.2145386 -4.193048 -4.1566744 -4.1306772 -4.1449232 -4.1811295 -4.2121148 -4.2132845 -4.1834927 -4.1644535][-4.1407127 -4.1690979 -4.1861281 -4.1905379 -4.1898727 -4.1752882 -4.1334863 -4.0730166 -4.0375118 -4.0649185 -4.1224189 -4.1694412 -4.1839824 -4.17225 -4.1666527][-4.1451941 -4.1562347 -4.1576185 -4.1532154 -4.138299 -4.1006465 -4.0320086 -3.9422276 -3.9039907 -3.9700694 -4.0679617 -4.1370435 -4.1717758 -4.1809993 -4.1836367][-4.1526394 -4.1478758 -4.1289358 -4.1111317 -4.0863233 -4.0391374 -3.955688 -3.8582437 -3.8440709 -3.953341 -4.0754738 -4.1524234 -4.1940188 -4.2105722 -4.210577][-4.1609569 -4.1471848 -4.1246662 -4.1049352 -4.0831442 -4.0451031 -3.9873555 -3.9367507 -3.9546781 -4.0471606 -4.1398153 -4.1937809 -4.2182064 -4.22735 -4.2214222][-4.1726923 -4.152247 -4.1330462 -4.1214895 -4.1103158 -4.0888877 -4.0607328 -4.0485106 -4.0715313 -4.1352334 -4.193234 -4.2167482 -4.2176771 -4.2124338 -4.1994734][-4.1904206 -4.1657004 -4.1448679 -4.1382742 -4.1365023 -4.1230865 -4.1111779 -4.1147337 -4.1371117 -4.1806531 -4.2185588 -4.2227011 -4.2021012 -4.1773186 -4.1585493][-4.1973467 -4.1705809 -4.1461215 -4.1374111 -4.1406126 -4.1367936 -4.1332145 -4.1461406 -4.16975 -4.2008963 -4.2241096 -4.2141461 -4.1775832 -4.1396422 -4.1162214][-4.183322 -4.1640749 -4.1404452 -4.1278563 -4.13031 -4.1299949 -4.1314726 -4.146059 -4.1695728 -4.1945 -4.2091212 -4.1905379 -4.145576 -4.1048665 -4.0881863][-4.1665869 -4.1560488 -4.14135 -4.1311097 -4.1258483 -4.1178651 -4.1128163 -4.1206017 -4.1405745 -4.1633897 -4.1755009 -4.1610923 -4.122931 -4.0929012 -4.0868368][-4.1704807 -4.1634274 -4.1538053 -4.1407623 -4.1278481 -4.1147547 -4.1053281 -4.108891 -4.1231337 -4.1438842 -4.1581545 -4.1517291 -4.1276455 -4.1130714 -4.1160278]]...]
INFO - root - 2017-12-05 11:23:54.793005: step 3410, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 76h:00m:34s remains)
INFO - root - 2017-12-05 11:24:03.389218: step 3420, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 77h:44m:29s remains)
INFO - root - 2017-12-05 11:24:11.898469: step 3430, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 76h:29m:23s remains)
INFO - root - 2017-12-05 11:24:20.442902: step 3440, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 80h:27m:25s remains)
INFO - root - 2017-12-05 11:24:28.980249: step 3450, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:45m:24s remains)
INFO - root - 2017-12-05 11:24:37.672830: step 3460, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 77h:42m:31s remains)
INFO - root - 2017-12-05 11:24:46.243513: step 3470, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 78h:03m:19s remains)
INFO - root - 2017-12-05 11:24:54.701328: step 3480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 77h:29m:59s remains)
INFO - root - 2017-12-05 11:25:03.189376: step 3490, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:52m:11s remains)
INFO - root - 2017-12-05 11:25:11.775849: step 3500, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.877 sec/batch; 80h:07m:20s remains)
2017-12-05 11:25:12.561905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1373081 -4.1614385 -4.1702166 -4.1777487 -4.1898532 -4.1989403 -4.2010841 -4.2018251 -4.200016 -4.1992078 -4.2018657 -4.1974478 -4.188468 -4.1774554 -4.1650677][-4.1137061 -4.1374979 -4.1468992 -4.1554995 -4.1708479 -4.1814165 -4.1796222 -4.1790328 -4.1776729 -4.1790905 -4.1836052 -4.1779222 -4.1645422 -4.1495242 -4.134419][-4.0774822 -4.1028705 -4.1167436 -4.1278462 -4.1437106 -4.1521211 -4.1460342 -4.1452284 -4.1454396 -4.1474628 -4.1550488 -4.1528821 -4.139451 -4.1234779 -4.1041546][-4.0635414 -4.0929861 -4.1125693 -4.1268582 -4.1401424 -4.1430526 -4.1324048 -4.129107 -4.1310344 -4.135056 -4.1450109 -4.1456537 -4.131506 -4.1115093 -4.0882339][-4.0940566 -4.123188 -4.1468449 -4.1610026 -4.1672263 -4.1605368 -4.139843 -4.1313672 -4.1359439 -4.1424503 -4.1513271 -4.15418 -4.141921 -4.1171851 -4.0931787][-4.1429548 -4.1586666 -4.1739964 -4.1835914 -4.185328 -4.1700592 -4.1400743 -4.1301303 -4.1401629 -4.1546593 -4.1670961 -4.1738591 -4.1669674 -4.1418381 -4.1179266][-4.1702409 -4.1692257 -4.1736026 -4.178247 -4.1749692 -4.1547346 -4.1230369 -4.1168685 -4.1367731 -4.1623645 -4.179358 -4.1907687 -4.1897655 -4.1654582 -4.1344948][-4.1708937 -4.159565 -4.1559086 -4.1542444 -4.1451778 -4.121994 -4.0948005 -4.0949416 -4.1224389 -4.1506104 -4.1692333 -4.1844096 -4.1872487 -4.1633511 -4.1267452][-4.17205 -4.1606016 -4.153625 -4.1507463 -4.1407919 -4.12291 -4.101789 -4.1019673 -4.1243467 -4.1430798 -4.1520605 -4.1595163 -4.1591492 -4.1397142 -4.1091609][-4.166863 -4.1661735 -4.1732869 -4.1820111 -4.17864 -4.1687121 -4.1542468 -4.1532044 -4.1628838 -4.1660624 -4.1596384 -4.1547289 -4.1500063 -4.1384597 -4.121521][-4.1720424 -4.1847434 -4.2063885 -4.223412 -4.22525 -4.222043 -4.2152958 -4.2126822 -4.2139673 -4.2088346 -4.1953392 -4.1828136 -4.1763139 -4.172709 -4.1668115][-4.1724691 -4.1998954 -4.2271442 -4.2453985 -4.2500038 -4.2522559 -4.2534003 -4.254087 -4.2528729 -4.248014 -4.2389193 -4.2280073 -4.2193003 -4.2140541 -4.2098718][-4.1744204 -4.2079511 -4.2361045 -4.256968 -4.2661862 -4.2741952 -4.2795553 -4.2819057 -4.2810063 -4.2789469 -4.274653 -4.2673535 -4.2583337 -4.2521544 -4.2487168][-4.1897869 -4.223835 -4.2503734 -4.2694678 -4.2804089 -4.291718 -4.2978315 -4.3000135 -4.3011045 -4.3039131 -4.3046236 -4.2991672 -4.2935624 -4.2909446 -4.2880464][-4.1931434 -4.22654 -4.2513905 -4.2683797 -4.2785392 -4.2895331 -4.2938352 -4.2939014 -4.2958 -4.3010006 -4.3068194 -4.3075857 -4.3083034 -4.3088784 -4.305007]]...]
INFO - root - 2017-12-05 11:25:21.078822: step 3510, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 79h:24m:52s remains)
INFO - root - 2017-12-05 11:25:29.577863: step 3520, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 77h:11m:33s remains)
INFO - root - 2017-12-05 11:25:38.051171: step 3530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:47m:44s remains)
INFO - root - 2017-12-05 11:25:46.604141: step 3540, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 77h:53m:57s remains)
INFO - root - 2017-12-05 11:25:55.098581: step 3550, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 78h:51m:42s remains)
INFO - root - 2017-12-05 11:26:03.704543: step 3560, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 76h:59m:59s remains)
INFO - root - 2017-12-05 11:26:12.291268: step 3570, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 79h:03m:56s remains)
INFO - root - 2017-12-05 11:26:20.846531: step 3580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 79h:08m:21s remains)
INFO - root - 2017-12-05 11:26:29.326616: step 3590, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 76h:58m:41s remains)
INFO - root - 2017-12-05 11:26:37.717091: step 3600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:54m:37s remains)
2017-12-05 11:26:38.477598: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2574682 -4.2730894 -4.2711043 -4.2511854 -4.229 -4.20379 -4.165204 -4.1228228 -4.0935097 -4.0845723 -4.0924349 -4.1221204 -4.1716814 -4.2175717 -4.2473674][-4.2675714 -4.2801371 -4.2739577 -4.2534423 -4.2362242 -4.2157784 -4.183249 -4.1490588 -4.1254792 -4.1138258 -4.1204753 -4.1458821 -4.1827655 -4.2167373 -4.2438498][-4.2488227 -4.2619886 -4.253221 -4.2320404 -4.2177062 -4.1978741 -4.1677389 -4.1395025 -4.1241736 -4.1203942 -4.1335249 -4.1555424 -4.1772509 -4.1970873 -4.22074][-4.2052312 -4.2230048 -4.2126389 -4.1887393 -4.1722441 -4.1466994 -4.1129279 -4.08616 -4.0821528 -4.0949292 -4.1158705 -4.1339893 -4.1453004 -4.159327 -4.1848192][-4.1705327 -4.1806588 -4.165123 -4.1384573 -4.1158853 -4.080719 -4.0407262 -4.0161166 -4.0244131 -4.0510917 -4.0828285 -4.102838 -4.1097631 -4.1147361 -4.1373949][-4.1534324 -4.1507111 -4.1277924 -4.0929518 -4.0553231 -4.0088434 -3.9576368 -3.9286673 -3.945518 -3.9852252 -4.0328317 -4.0621328 -4.0663795 -4.0640459 -4.0872293][-4.1534872 -4.141788 -4.1122646 -4.0692749 -4.025053 -3.9776278 -3.920557 -3.8842077 -3.9049509 -3.9520092 -4.0087309 -4.0364523 -4.03116 -4.0160294 -4.0399318][-4.172802 -4.1564732 -4.1280637 -4.0916128 -4.0634117 -4.0361662 -3.988637 -3.9545674 -3.9732904 -4.0109167 -4.057302 -4.0660958 -4.0394163 -4.0057278 -4.0248771][-4.1939535 -4.1733251 -4.150423 -4.1307168 -4.1274195 -4.1216903 -4.0901732 -4.0654411 -4.0780029 -4.0978746 -4.1279812 -4.125524 -4.0883489 -4.0522485 -4.0701895][-4.2146888 -4.1922984 -4.1763115 -4.1691608 -4.1809511 -4.1890521 -4.1731534 -4.1575856 -4.1639781 -4.1706128 -4.1866817 -4.1796856 -4.1479249 -4.1212549 -4.1397243][-4.2401662 -4.2151895 -4.2023835 -4.2005196 -4.2151585 -4.2296963 -4.2267222 -4.2164154 -4.2177663 -4.2173514 -4.2265134 -4.2211962 -4.1967349 -4.1812263 -4.1973186][-4.2541127 -4.2270684 -4.2133451 -4.2112103 -4.2219272 -4.2367377 -4.2437944 -4.2421436 -4.242548 -4.2415285 -4.2514348 -4.2512164 -4.23536 -4.2268257 -4.2394409][-4.2561398 -4.2273684 -4.2111745 -4.2057018 -4.2105136 -4.2219386 -4.2342935 -4.2411308 -4.2459598 -4.2493439 -4.2615519 -4.2678227 -4.2593966 -4.2567072 -4.2675924][-4.2608318 -4.230391 -4.20927 -4.1987958 -4.1991348 -4.2051535 -4.2166739 -4.2264967 -4.2358713 -4.2448606 -4.2581773 -4.2676692 -4.2656827 -4.2698774 -4.2818475][-4.2718954 -4.2416945 -4.2143106 -4.1958485 -4.1880169 -4.1854548 -4.1904273 -4.2006121 -4.2186069 -4.2385521 -4.2529454 -4.262032 -4.2634835 -4.2709742 -4.2842126]]...]
INFO - root - 2017-12-05 11:26:46.940372: step 3610, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 80h:16m:20s remains)
INFO - root - 2017-12-05 11:26:55.418197: step 3620, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 77h:49m:10s remains)
INFO - root - 2017-12-05 11:27:04.010495: step 3630, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 75h:52m:09s remains)
INFO - root - 2017-12-05 11:27:12.450165: step 3640, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 79h:23m:48s remains)
INFO - root - 2017-12-05 11:27:20.928553: step 3650, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 80h:18m:32s remains)
INFO - root - 2017-12-05 11:27:29.527570: step 3660, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 78h:03m:45s remains)
INFO - root - 2017-12-05 11:27:37.998850: step 3670, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 75h:49m:08s remains)
INFO - root - 2017-12-05 11:27:46.591961: step 3680, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 76h:45m:28s remains)
INFO - root - 2017-12-05 11:27:55.108909: step 3690, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 78h:12m:58s remains)
INFO - root - 2017-12-05 11:28:03.605546: step 3700, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 75h:19m:49s remains)
2017-12-05 11:28:04.351800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2471309 -4.2433934 -4.2198391 -4.1877527 -4.1757531 -4.17778 -4.1803975 -4.1776075 -4.1629715 -4.1482272 -4.159492 -4.1811666 -4.1917439 -4.1963067 -4.2062006][-4.2306614 -4.2290859 -4.2052588 -4.1746464 -4.1678052 -4.1731815 -4.1733389 -4.1614308 -4.1336865 -4.1130953 -4.1310873 -4.160924 -4.1742587 -4.1792612 -4.1900806][-4.1993446 -4.1962552 -4.1736579 -4.1503954 -4.1503062 -4.1603727 -4.1645527 -4.1580291 -4.1345463 -4.1256742 -4.1546359 -4.1859593 -4.19484 -4.1926641 -4.190793][-4.1700873 -4.1659107 -4.1441011 -4.1298971 -4.1337996 -4.142488 -4.1508389 -4.1567483 -4.1514916 -4.1598 -4.1941152 -4.2194104 -4.21903 -4.2048445 -4.1852026][-4.14108 -4.1340041 -4.115097 -4.1030869 -4.104773 -4.1063633 -4.1063094 -4.1130404 -4.1233788 -4.1430473 -4.1773348 -4.2008915 -4.2019224 -4.1856966 -4.1610842][-4.0967388 -4.0815268 -4.0544691 -4.0336623 -4.0245943 -4.0126209 -3.9966331 -4.0013256 -4.0298762 -4.0679736 -4.111783 -4.144536 -4.1549311 -4.1470356 -4.1334438][-4.0050321 -3.9687672 -3.9230566 -3.8924818 -3.8764043 -3.8525474 -3.8245332 -3.8399584 -3.8978083 -3.9635005 -4.0269418 -4.0748057 -4.0967326 -4.103683 -4.1089873][-3.9257147 -3.8660779 -3.8037426 -3.7714288 -3.7613997 -3.7420647 -3.7180185 -3.7422984 -3.8105519 -3.884336 -3.9539945 -4.0098376 -4.0430713 -4.0690742 -4.0938396][-3.9887581 -3.9403207 -3.8953445 -3.8766406 -3.8789146 -3.8777161 -3.86956 -3.8808699 -3.9164677 -3.9591336 -4.0052166 -4.0440359 -4.0662122 -4.0853786 -4.109458][-4.0989809 -4.0652895 -4.0365043 -4.0249586 -4.0273423 -4.0316005 -4.0315957 -4.0348077 -4.045825 -4.0643182 -4.0901957 -4.1130748 -4.123785 -4.1308193 -4.1449823][-4.1799679 -4.1561065 -4.1372223 -4.12874 -4.1295924 -4.133316 -4.1380119 -4.1416306 -4.1449146 -4.1522536 -4.1657434 -4.1769633 -4.1797838 -4.1791067 -4.1837234][-4.2282991 -4.2095895 -4.1963444 -4.1908278 -4.1922951 -4.19742 -4.2042117 -4.20931 -4.2114553 -4.2133994 -4.2169256 -4.2189674 -4.2167325 -4.2148585 -4.2198415][-4.2546043 -4.2405863 -4.2304511 -4.2245626 -4.2241807 -4.2269459 -4.2315249 -4.2365718 -4.2385259 -4.2390208 -4.2392607 -4.238955 -4.239305 -4.2425613 -4.248652][-4.2752609 -4.2646866 -4.2562823 -4.25036 -4.2493739 -4.2519431 -4.2546582 -4.2562389 -4.255795 -4.2546167 -4.2542453 -4.2539635 -4.2561145 -4.2599435 -4.26523][-4.2917156 -4.284821 -4.2781506 -4.2734165 -4.273438 -4.2769823 -4.2796378 -4.2802682 -4.2786937 -4.2768526 -4.2757335 -4.2745481 -4.2750511 -4.276938 -4.2806306]]...]
INFO - root - 2017-12-05 11:28:12.827774: step 3710, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 79h:34m:45s remains)
INFO - root - 2017-12-05 11:28:21.352017: step 3720, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 81h:15m:12s remains)
INFO - root - 2017-12-05 11:28:29.941147: step 3730, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 77h:20m:15s remains)
INFO - root - 2017-12-05 11:28:38.722662: step 3740, loss = 2.08, batch loss = 2.02 (8.1 examples/sec; 0.984 sec/batch; 89h:49m:22s remains)
INFO - root - 2017-12-05 11:28:47.243784: step 3750, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 79h:28m:58s remains)
INFO - root - 2017-12-05 11:28:55.849070: step 3760, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 78h:52m:01s remains)
INFO - root - 2017-12-05 11:29:04.238250: step 3770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 79h:15m:21s remains)
INFO - root - 2017-12-05 11:29:12.833664: step 3780, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 78h:16m:44s remains)
INFO - root - 2017-12-05 11:29:21.447593: step 3790, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 80h:00m:45s remains)
INFO - root - 2017-12-05 11:29:30.014657: step 3800, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 79h:19m:49s remains)
2017-12-05 11:29:30.770092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2976456 -4.279892 -4.2724266 -4.2746277 -4.2795339 -4.2822556 -4.2838283 -4.2889204 -4.2896938 -4.2851453 -4.2819619 -4.2820058 -4.2840853 -4.2885075 -4.29394][-4.2783914 -4.2558494 -4.2457395 -4.2452369 -4.2526288 -4.2606835 -4.2654009 -4.2715664 -4.2728448 -4.2671442 -4.2650752 -4.2675967 -4.2704258 -4.2739434 -4.2806387][-4.2524085 -4.2196789 -4.2003527 -4.1948562 -4.2071142 -4.2239666 -4.2358727 -4.2465806 -4.2498827 -4.24818 -4.2491593 -4.2534928 -4.2573047 -4.26289 -4.271244][-4.2340522 -4.1883879 -4.1564369 -4.1414933 -4.1552892 -4.1804419 -4.1973338 -4.2143707 -4.2274256 -4.238266 -4.2471166 -4.2544074 -4.2616305 -4.2697821 -4.2759075][-4.2250013 -4.1693578 -4.1254654 -4.0944734 -4.095366 -4.1173038 -4.1354389 -4.1599393 -4.1867671 -4.2145839 -4.2372589 -4.2525988 -4.2661138 -4.2779827 -4.2828584][-4.2242231 -4.159934 -4.104568 -4.05493 -4.0303679 -4.032155 -4.0342155 -4.0553889 -4.1018991 -4.1567669 -4.1965413 -4.2229409 -4.2458353 -4.2639565 -4.2735][-4.2245173 -4.1583486 -4.0978775 -4.0351844 -3.9833632 -3.9465423 -3.9015369 -3.8824146 -3.9425249 -4.0431652 -4.1224856 -4.17055 -4.2054849 -4.232028 -4.2504025][-4.2315049 -4.1724534 -4.1153731 -4.0542159 -3.9960592 -3.9331489 -3.833616 -3.7371831 -3.7705853 -3.8990319 -4.0149593 -4.0914631 -4.1454072 -4.1870117 -4.2212152][-4.245944 -4.1974578 -4.1532416 -4.1132379 -4.0811758 -4.0359979 -3.9484963 -3.8476973 -3.8317962 -3.9123912 -4.0109739 -4.0894012 -4.14633 -4.1885281 -4.2218966][-4.254425 -4.213552 -4.1810846 -4.1597743 -4.1525707 -4.1382923 -4.0965991 -4.0451784 -4.0210037 -4.0502682 -4.1055279 -4.1590257 -4.1994424 -4.2281332 -4.2485094][-4.2649026 -4.2321444 -4.2060256 -4.1948566 -4.2014208 -4.2049751 -4.1933503 -4.182446 -4.171052 -4.1808271 -4.2085543 -4.2393584 -4.2636123 -4.2791805 -4.2856636][-4.2689109 -4.2442889 -4.2225642 -4.2105951 -4.2192678 -4.2302957 -4.233736 -4.2416778 -4.2445993 -4.2552438 -4.2751522 -4.2963462 -4.310853 -4.3189588 -4.3173742][-4.2834659 -4.2700057 -4.2563319 -4.2447295 -4.2475882 -4.2554622 -4.2619267 -4.2729931 -4.278862 -4.2876611 -4.3036766 -4.3207979 -4.3324385 -4.3379693 -4.3335829][-4.3051248 -4.3022118 -4.2999845 -4.2949944 -4.2944779 -4.297307 -4.3024559 -4.3093343 -4.31256 -4.3157382 -4.3237848 -4.3337789 -4.3406897 -4.3423481 -4.3382697][-4.3195839 -4.3187909 -4.3220019 -4.32223 -4.3214989 -4.3206072 -4.3216524 -4.3232255 -4.3235846 -4.3247838 -4.3282857 -4.33335 -4.3365417 -4.3371406 -4.3352757]]...]
INFO - root - 2017-12-05 11:29:39.106399: step 3810, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 79h:44m:29s remains)
INFO - root - 2017-12-05 11:29:47.734259: step 3820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 78h:51m:14s remains)
INFO - root - 2017-12-05 11:29:56.374187: step 3830, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 80h:08m:58s remains)
INFO - root - 2017-12-05 11:30:04.827274: step 3840, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 79h:29m:25s remains)
INFO - root - 2017-12-05 11:30:13.419220: step 3850, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 81h:00m:49s remains)
INFO - root - 2017-12-05 11:30:21.873703: step 3860, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 76h:59m:42s remains)
INFO - root - 2017-12-05 11:30:30.470793: step 3870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 78h:46m:07s remains)
INFO - root - 2017-12-05 11:30:38.997799: step 3880, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 79h:33m:24s remains)
INFO - root - 2017-12-05 11:30:47.550168: step 3890, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 75h:40m:15s remains)
INFO - root - 2017-12-05 11:30:56.055586: step 3900, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 76h:34m:10s remains)
2017-12-05 11:30:56.890457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.234273 -4.1878033 -4.1383452 -4.1060004 -4.0892878 -4.1047397 -4.1450925 -4.1865177 -4.2073112 -4.1969953 -4.1850367 -4.1756558 -4.1658664 -4.1559334 -4.1534009][-4.233109 -4.1819935 -4.12236 -4.0825782 -4.0644817 -4.0925732 -4.1467819 -4.1882339 -4.204792 -4.1879659 -4.1689782 -4.1591516 -4.158071 -4.1606145 -4.1645346][-4.2310333 -4.1752558 -4.104969 -4.0550604 -4.037487 -4.0827031 -4.1510782 -4.190526 -4.2048635 -4.1893206 -4.1683488 -4.1589246 -4.1583552 -4.1635842 -4.1681128][-4.229712 -4.1736994 -4.0991106 -4.0391588 -4.0183296 -4.0765352 -4.1564007 -4.196898 -4.2111993 -4.1969824 -4.1743121 -4.1643214 -4.1618915 -4.1646986 -4.1701183][-4.2311597 -4.1783843 -4.1078906 -4.0402784 -4.0089035 -4.0698357 -4.1571331 -4.2043023 -4.220233 -4.2069187 -4.1846309 -4.1701369 -4.1652174 -4.1698842 -4.1784606][-4.2385097 -4.1938057 -4.1293058 -4.0580626 -4.0136881 -4.0665579 -4.1531472 -4.2048798 -4.2228236 -4.2134986 -4.1930718 -4.176672 -4.1716957 -4.1776671 -4.1881576][-4.2443552 -4.2077456 -4.1534986 -4.0861945 -4.033855 -4.0717974 -4.1491623 -4.1978512 -4.2191749 -4.217217 -4.1962461 -4.17611 -4.1683497 -4.17149 -4.1811881][-4.2469139 -4.2161379 -4.1706352 -4.1130652 -4.0650883 -4.0944872 -4.1582484 -4.19884 -4.2169542 -4.2152996 -4.191752 -4.1687436 -4.1574855 -4.1559768 -4.1640792][-4.246696 -4.2186418 -4.1817312 -4.1364703 -4.101028 -4.1267891 -4.1790166 -4.2114053 -4.2230687 -4.220469 -4.1989007 -4.1797948 -4.1675677 -4.1601324 -4.1646571][-4.2411475 -4.2148829 -4.1829257 -4.1470313 -4.1251698 -4.1520319 -4.1999731 -4.2280011 -4.2347832 -4.23004 -4.2113371 -4.1962342 -4.1864262 -4.1766424 -4.1777511][-4.2309365 -4.2054753 -4.1768389 -4.1464033 -4.1327987 -4.1590943 -4.205771 -4.2366204 -4.2450633 -4.2400122 -4.2246532 -4.2145228 -4.2085304 -4.1969838 -4.1943574][-4.2253661 -4.2020378 -4.1770773 -4.1503057 -4.1397238 -4.1612997 -4.2036633 -4.2355433 -4.2470331 -4.2455029 -4.2362642 -4.2306185 -4.2273045 -4.2179165 -4.2165532][-4.2238836 -4.2040553 -4.18464 -4.1630569 -4.1539907 -4.1677485 -4.2017856 -4.2306 -4.2421055 -4.2432561 -4.2395635 -4.2391043 -4.2401829 -4.2345819 -4.2372003][-4.2262759 -4.2122273 -4.1994543 -4.1832104 -4.1760449 -4.1862063 -4.2133989 -4.2352576 -4.2416563 -4.2388887 -4.2349005 -4.2388554 -4.2467232 -4.2484341 -4.253336][-4.237721 -4.2283735 -4.2195868 -4.2087822 -4.2041893 -4.2122631 -4.231348 -4.2442641 -4.2445927 -4.2356205 -4.226728 -4.2303777 -4.2421441 -4.251883 -4.2595692]]...]
INFO - root - 2017-12-05 11:31:05.368569: step 3910, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 77h:11m:49s remains)
INFO - root - 2017-12-05 11:31:13.909310: step 3920, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 79h:40m:46s remains)
INFO - root - 2017-12-05 11:31:22.310244: step 3930, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.838 sec/batch; 76h:31m:31s remains)
INFO - root - 2017-12-05 11:31:30.900729: step 3940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 78h:55m:03s remains)
INFO - root - 2017-12-05 11:31:39.474095: step 3950, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 78h:33m:25s remains)
INFO - root - 2017-12-05 11:31:48.016155: step 3960, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 75h:56m:24s remains)
INFO - root - 2017-12-05 11:31:56.629627: step 3970, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 78h:56m:41s remains)
INFO - root - 2017-12-05 11:32:05.179798: step 3980, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 77h:41m:59s remains)
INFO - root - 2017-12-05 11:32:13.790226: step 3990, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 76h:56m:29s remains)
INFO - root - 2017-12-05 11:32:22.367712: step 4000, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 77h:31m:45s remains)
2017-12-05 11:32:23.102400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2131047 -4.2036119 -4.210485 -4.2216325 -4.2336645 -4.2507 -4.2735167 -4.2949233 -4.3069487 -4.31096 -4.3091874 -4.3029575 -4.2947044 -4.2868819 -4.278964][-4.2065744 -4.2019548 -4.2154455 -4.2318716 -4.2420912 -4.2530336 -4.2700176 -4.2872381 -4.3008046 -4.3090334 -4.3066893 -4.2993402 -4.2926445 -4.2837076 -4.2716446][-4.2004886 -4.196404 -4.2099648 -4.2259164 -4.2338395 -4.236002 -4.2408662 -4.2553616 -4.2744365 -4.2906604 -4.2923775 -4.2889528 -4.2874861 -4.2798476 -4.2634616][-4.1894526 -4.1811147 -4.1899009 -4.2053561 -4.2127123 -4.208549 -4.2025409 -4.213016 -4.2368922 -4.2629533 -4.2736039 -4.2768693 -4.2802181 -4.2740827 -4.2536454][-4.1799569 -4.16593 -4.1685472 -4.1836224 -4.1935406 -4.1879611 -4.1749454 -4.1792879 -4.2039294 -4.2336516 -4.2531166 -4.2627454 -4.2673097 -4.2625465 -4.2422342][-4.1798019 -4.1605387 -4.1585689 -4.1737146 -4.1865349 -4.1802335 -4.1591525 -4.1521626 -4.1722331 -4.2035933 -4.2277393 -4.2450461 -4.253139 -4.2527404 -4.2356052][-4.1861696 -4.169991 -4.1666546 -4.1794043 -4.1901851 -4.1778364 -4.1428895 -4.1167083 -4.1285996 -4.1639771 -4.1946812 -4.2230763 -4.2432008 -4.2508335 -4.2410736][-4.2006888 -4.1938167 -4.1924529 -4.2015057 -4.2048445 -4.18 -4.1238933 -4.0691252 -4.0685563 -4.1153822 -4.1634893 -4.2073293 -4.2394619 -4.2551007 -4.2518592][-4.2148232 -4.2183809 -4.2201748 -4.2246294 -4.2212281 -4.1890645 -4.1217513 -4.0438404 -4.0272851 -4.0834417 -4.1475348 -4.2026811 -4.2416167 -4.2607713 -4.2596946][-4.226387 -4.2350497 -4.2385778 -4.2402306 -4.233201 -4.2027011 -4.1420293 -4.066268 -4.0391145 -4.0854349 -4.1500087 -4.2063127 -4.2457442 -4.2631359 -4.2620559][-4.2346272 -4.2445455 -4.2488284 -4.248354 -4.2392464 -4.212409 -4.1642318 -4.1037817 -4.075057 -4.1055708 -4.161272 -4.2128263 -4.250308 -4.2670178 -4.2676768][-4.2402 -4.2504745 -4.2546382 -4.2537541 -4.24657 -4.2248812 -4.1885643 -4.1424918 -4.1136632 -4.131875 -4.1779275 -4.2220659 -4.2560678 -4.2745123 -4.2776957][-4.2439771 -4.2525635 -4.2535925 -4.2492514 -4.2424059 -4.226666 -4.2020545 -4.1724677 -4.1499424 -4.1627007 -4.2012687 -4.2370381 -4.2640753 -4.2813969 -4.2844491][-4.2508826 -4.2552109 -4.251163 -4.2401929 -4.2290015 -4.2150011 -4.1995187 -4.1850643 -4.1747346 -4.1880503 -4.221313 -4.2495656 -4.268075 -4.2801976 -4.2812629][-4.2611794 -4.2613554 -4.2519445 -4.2337046 -4.2132134 -4.1929135 -4.1814637 -4.1787791 -4.1802487 -4.196228 -4.2278886 -4.25449 -4.2697248 -4.2800107 -4.2801247]]...]
INFO - root - 2017-12-05 11:32:31.545936: step 4010, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:48m:35s remains)
INFO - root - 2017-12-05 11:32:39.995462: step 4020, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.800 sec/batch; 72h:58m:00s remains)
INFO - root - 2017-12-05 11:32:48.642118: step 4030, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 75h:25m:58s remains)
INFO - root - 2017-12-05 11:32:57.063247: step 4040, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 77h:52m:38s remains)
INFO - root - 2017-12-05 11:33:05.568298: step 4050, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 79h:01m:25s remains)
INFO - root - 2017-12-05 11:33:14.039708: step 4060, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.805 sec/batch; 73h:26m:14s remains)
INFO - root - 2017-12-05 11:33:22.639089: step 4070, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 77h:58m:34s remains)
INFO - root - 2017-12-05 11:33:31.081373: step 4080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 78h:45m:03s remains)
INFO - root - 2017-12-05 11:33:39.578825: step 4090, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 79h:47m:49s remains)
INFO - root - 2017-12-05 11:33:48.160436: step 4100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 78h:29m:23s remains)
2017-12-05 11:33:48.930373: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2062173 -4.1990476 -4.1851125 -4.1818438 -4.1887169 -4.1889176 -4.1838012 -4.1775389 -4.1602869 -4.1240597 -4.0927458 -4.1084394 -4.1612253 -4.22027 -4.2674246][-4.161952 -4.16016 -4.1519346 -4.149929 -4.1562843 -4.1595879 -4.1590834 -4.1553984 -4.1378288 -4.0998936 -4.0649176 -4.0784268 -4.1328282 -4.1975045 -4.249929][-4.150928 -4.1506577 -4.1479864 -4.1458 -4.1518974 -4.1603422 -4.1645684 -4.1656909 -4.1560464 -4.1298685 -4.1044307 -4.1132736 -4.1535468 -4.2077537 -4.2525911][-4.1661105 -4.1694832 -4.1697145 -4.1663651 -4.1679163 -4.1704268 -4.1701522 -4.1739678 -4.1734347 -4.1653795 -4.1610832 -4.1719918 -4.1980581 -4.2394981 -4.2757692][-4.1889186 -4.1946888 -4.1941924 -4.1891556 -4.17891 -4.1590533 -4.1362772 -4.1350732 -4.1506748 -4.1750832 -4.2013278 -4.2227674 -4.2417736 -4.2722898 -4.3009081][-4.1990142 -4.2047176 -4.2048917 -4.1965842 -4.1654978 -4.1075258 -4.0411634 -4.0083857 -4.0362282 -4.1109257 -4.1877351 -4.2366204 -4.262063 -4.2864871 -4.3116469][-4.2026772 -4.209013 -4.209795 -4.1956315 -4.14153 -4.03875 -3.9069395 -3.8072972 -3.8309646 -3.9646828 -4.1001363 -4.1893969 -4.2385054 -4.2729445 -4.3040051][-4.2139411 -4.2241578 -4.2243619 -4.2012005 -4.1287432 -3.9982843 -3.8239398 -3.6683826 -3.6712513 -3.8379807 -4.0103216 -4.130291 -4.2055845 -4.2532425 -4.2897649][-4.2279277 -4.242126 -4.2447529 -4.2223883 -4.1568747 -4.04843 -3.9082198 -3.7781711 -3.7625127 -3.8768902 -4.0116348 -4.1179991 -4.1983066 -4.2524085 -4.2907453][-4.2386446 -4.2502503 -4.2549615 -4.2419696 -4.2011909 -4.1314826 -4.0412078 -3.9551024 -3.9300663 -3.9819195 -4.05785 -4.1350193 -4.208158 -4.2635884 -4.300694][-4.2429562 -4.2473335 -4.2555356 -4.2569823 -4.2403045 -4.2019897 -4.1473961 -4.095891 -4.0700765 -4.084301 -4.1187959 -4.1710653 -4.2313247 -4.2804747 -4.3134046][-4.2558246 -4.252296 -4.2579331 -4.2657003 -4.2625928 -4.2449379 -4.2151208 -4.188477 -4.1703582 -4.1689787 -4.1792655 -4.2118979 -4.2578092 -4.2975273 -4.3243146][-4.2791348 -4.2725677 -4.2726336 -4.2784958 -4.2816072 -4.276721 -4.2642851 -4.253243 -4.243628 -4.2365379 -4.2347851 -4.251524 -4.2830257 -4.3134646 -4.3345556][-4.3047791 -4.2986417 -4.2951217 -4.2951908 -4.2970138 -4.297493 -4.2959142 -4.2928176 -4.2892203 -4.2843628 -4.2805557 -4.28931 -4.3098168 -4.331449 -4.3467994][-4.3175349 -4.3110905 -4.3053322 -4.3022032 -4.3023019 -4.305367 -4.3083973 -4.3090076 -4.3086486 -4.3079143 -4.3067527 -4.3131566 -4.326951 -4.3420482 -4.3519993]]...]
INFO - root - 2017-12-05 11:33:57.319828: step 4110, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.815 sec/batch; 74h:20m:58s remains)
INFO - root - 2017-12-05 11:34:05.802713: step 4120, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 76h:30m:41s remains)
INFO - root - 2017-12-05 11:34:14.332341: step 4130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:45m:59s remains)
INFO - root - 2017-12-05 11:34:22.985056: step 4140, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 79h:33m:03s remains)
INFO - root - 2017-12-05 11:34:31.501562: step 4150, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 76h:20m:56s remains)
INFO - root - 2017-12-05 11:34:40.093007: step 4160, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.910 sec/batch; 82h:58m:29s remains)
INFO - root - 2017-12-05 11:34:48.530565: step 4170, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 76h:14m:16s remains)
INFO - root - 2017-12-05 11:34:57.117000: step 4180, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 78h:20m:37s remains)
INFO - root - 2017-12-05 11:35:05.569321: step 4190, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 76h:55m:55s remains)
INFO - root - 2017-12-05 11:35:14.145998: step 4200, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.808 sec/batch; 73h:42m:26s remains)
2017-12-05 11:35:14.911932: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2289624 -4.2233577 -4.2184882 -4.2185965 -4.215416 -4.1999192 -4.1770592 -4.1538844 -4.1293087 -4.1019511 -4.0787921 -4.0657673 -4.071445 -4.1057386 -4.1579266][-4.2252774 -4.2290053 -4.2236118 -4.2123 -4.1892948 -4.1515408 -4.1075153 -4.0679579 -4.0346136 -4.00576 -3.9821374 -3.9692054 -3.9783962 -4.0187526 -4.0806437][-4.2200165 -4.227705 -4.2177238 -4.194644 -4.1556273 -4.1011839 -4.0441313 -3.9980903 -3.9703355 -3.9568603 -3.9480996 -3.9485953 -3.9676895 -4.0090823 -4.0655227][-4.1853809 -4.1955061 -4.1864123 -4.1589484 -4.11346 -4.0548491 -4.0012951 -3.9668922 -3.9610562 -3.9751108 -3.9914272 -4.0112414 -4.0382333 -4.0714092 -4.1084051][-4.1178045 -4.137001 -4.1370964 -4.1135693 -4.0690045 -4.0169544 -3.9809337 -3.9714203 -3.9912605 -4.0265059 -4.0596747 -4.090487 -4.1197042 -4.1450896 -4.1667209][-4.0488935 -4.081583 -4.0921555 -4.0761318 -4.035264 -3.9901679 -3.967417 -3.9767492 -4.0121989 -4.0573716 -4.0986781 -4.1346045 -4.1660695 -4.1904573 -4.2088923][-3.9916208 -4.0370841 -4.060689 -4.0589705 -4.0348282 -4.0071306 -3.998106 -4.01371 -4.0503354 -4.0913091 -4.1262612 -4.1574168 -4.1899161 -4.2195654 -4.2445374][-3.9812212 -4.0316739 -4.0693421 -4.0881085 -4.087729 -4.0802512 -4.0789204 -4.0886693 -4.1078787 -4.1278658 -4.1454544 -4.1657028 -4.1984358 -4.2376862 -4.2759][-4.0229559 -4.0728517 -4.1193166 -4.1499171 -4.1633577 -4.1663952 -4.166873 -4.1678667 -4.1673627 -4.1631351 -4.1606193 -4.1654134 -4.1925836 -4.2366409 -4.2859459][-4.0808821 -4.1241961 -4.1689239 -4.2017817 -4.2220263 -4.2337122 -4.2378483 -4.2350078 -4.2195086 -4.1936769 -4.1683245 -4.152739 -4.1661596 -4.2071996 -4.2631159][-4.1311183 -4.1658664 -4.2032857 -4.23529 -4.2617259 -4.2830458 -4.295012 -4.2923589 -4.26662 -4.2239203 -4.177959 -4.1420679 -4.1397181 -4.1734972 -4.23018][-4.1923876 -4.2169862 -4.24387 -4.2702928 -4.2952976 -4.31634 -4.33136 -4.3294344 -4.303061 -4.2612767 -4.2152681 -4.1762033 -4.1671124 -4.1883321 -4.2281718][-4.2562518 -4.2716417 -4.2874713 -4.3058887 -4.3236775 -4.3374548 -4.3481708 -4.347868 -4.3304915 -4.3021822 -4.2712393 -4.2436342 -4.2322545 -4.2334647 -4.2419972][-4.3100042 -4.3183942 -4.3262515 -4.3361011 -4.3439093 -4.3486495 -4.3527827 -4.3524914 -4.342566 -4.3252816 -4.307045 -4.289824 -4.2741532 -4.2539024 -4.23214][-4.346993 -4.3500547 -4.3512497 -4.3535628 -4.3537145 -4.3509879 -4.3478975 -4.3439827 -4.3345771 -4.3194146 -4.3028116 -4.2854357 -4.2625632 -4.227242 -4.1852794]]...]
INFO - root - 2017-12-05 11:35:23.412459: step 4210, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 79h:11m:04s remains)
INFO - root - 2017-12-05 11:35:31.945808: step 4220, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 80h:45m:54s remains)
INFO - root - 2017-12-05 11:35:40.544863: step 4230, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 80h:51m:34s remains)
INFO - root - 2017-12-05 11:35:48.984486: step 4240, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 76h:03m:42s remains)
INFO - root - 2017-12-05 11:35:57.381384: step 4250, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 77h:27m:38s remains)
INFO - root - 2017-12-05 11:36:05.956168: step 4260, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 78h:26m:55s remains)
INFO - root - 2017-12-05 11:36:14.554508: step 4270, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 77h:51m:33s remains)
INFO - root - 2017-12-05 11:36:23.107413: step 4280, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.822 sec/batch; 74h:55m:11s remains)
INFO - root - 2017-12-05 11:36:31.676527: step 4290, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:21m:00s remains)
INFO - root - 2017-12-05 11:36:40.192136: step 4300, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:09m:15s remains)
2017-12-05 11:36:40.921366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1768985 -4.1799192 -4.1746311 -4.1602626 -4.1391416 -4.1155238 -4.102211 -4.107686 -4.128159 -4.1524639 -4.1729832 -4.1837621 -4.1834674 -4.1759272 -4.1667156][-4.1626024 -4.1687384 -4.1629348 -4.1420875 -4.1082649 -4.0717487 -4.0548363 -4.0673542 -4.0983033 -4.1311736 -4.1606112 -4.1792035 -4.1817017 -4.1722636 -4.1600013][-4.146306 -4.152626 -4.1421947 -4.1125126 -4.0658884 -4.0193062 -4.0021148 -4.0235252 -4.0648079 -4.1064181 -4.1450648 -4.170722 -4.1743917 -4.1620131 -4.1452808][-4.1369548 -4.1392922 -4.1210613 -4.0816789 -4.0258245 -3.9755445 -3.9602096 -3.9896872 -4.0400877 -4.0885434 -4.1348581 -4.1656384 -4.1689358 -4.1520934 -4.1282177][-4.13743 -4.130291 -4.102201 -4.0551829 -3.9937396 -3.944782 -3.9330473 -3.9701562 -4.0288167 -4.0833573 -4.1348653 -4.1684947 -4.170392 -4.1476216 -4.1133962][-4.1480594 -4.1277013 -4.0887871 -4.0340981 -3.9677896 -3.9195309 -3.912035 -3.9586763 -4.0277376 -4.0903993 -4.1460891 -4.1811385 -4.1786718 -4.1461535 -4.0977249][-4.1575136 -4.1284738 -4.083147 -4.0213938 -3.9475677 -3.8936465 -3.8862615 -3.943835 -4.0257807 -4.1005726 -4.163703 -4.200676 -4.1930547 -4.1489558 -4.0856724][-4.1576347 -4.1283274 -4.086236 -4.0252767 -3.9466274 -3.8820682 -3.87081 -3.9362841 -4.0277963 -4.1135187 -4.1821337 -4.2192554 -4.2071629 -4.1560245 -4.0829506][-4.1459322 -4.1221581 -4.0933313 -4.0481548 -3.9800189 -3.913872 -3.8989193 -3.9617372 -4.0520449 -4.1383176 -4.204205 -4.2376771 -4.2245884 -4.1738625 -4.1015716][-4.1287112 -4.1124043 -4.1003537 -4.0778165 -4.0318332 -3.9791427 -3.9702227 -4.0259376 -4.1043606 -4.1786809 -4.2314739 -4.256043 -4.2430148 -4.1980505 -4.1338897][-4.114655 -4.1079893 -4.1118565 -4.1096687 -4.0854511 -4.0499463 -4.0506091 -4.1000133 -4.1650505 -4.2237639 -4.2616367 -4.2757998 -4.2627773 -4.2226353 -4.1628919][-4.1051383 -4.1081066 -4.1250057 -4.1371684 -4.1289477 -4.107017 -4.1116357 -4.1516428 -4.2035327 -4.24948 -4.2783728 -4.2884445 -4.2760782 -4.2370505 -4.1760149][-4.0960298 -4.1075883 -4.1339364 -4.1548529 -4.1586065 -4.1498938 -4.15703 -4.187901 -4.2282896 -4.2639151 -4.2860632 -4.2934475 -4.2804008 -4.2395344 -4.1754665][-4.09124 -4.1098866 -4.1408448 -4.1649394 -4.1751657 -4.1743441 -4.1796336 -4.2005053 -4.2323112 -4.2626138 -4.2823577 -4.289331 -4.2769375 -4.2370481 -4.1766744][-4.0909643 -4.1159811 -4.146769 -4.1694293 -4.179883 -4.1812973 -4.1808519 -4.1899424 -4.2148519 -4.2433057 -4.2645307 -4.2743816 -4.2641468 -4.2287107 -4.179327]]...]
INFO - root - 2017-12-05 11:36:49.419534: step 4310, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 77h:55m:46s remains)
INFO - root - 2017-12-05 11:36:57.948617: step 4320, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 76h:53m:20s remains)
INFO - root - 2017-12-05 11:37:06.490437: step 4330, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 80h:51m:29s remains)
INFO - root - 2017-12-05 11:37:14.994701: step 4340, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 76h:57m:34s remains)
INFO - root - 2017-12-05 11:37:23.543107: step 4350, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 79h:01m:12s remains)
INFO - root - 2017-12-05 11:37:32.064809: step 4360, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:19m:55s remains)
INFO - root - 2017-12-05 11:37:40.580991: step 4370, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 77h:31m:04s remains)
INFO - root - 2017-12-05 11:37:49.179054: step 4380, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 79h:42m:22s remains)
INFO - root - 2017-12-05 11:37:57.716462: step 4390, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 77h:17m:58s remains)
INFO - root - 2017-12-05 11:38:06.333717: step 4400, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 78h:23m:38s remains)
2017-12-05 11:38:07.060076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2506175 -4.2429709 -4.2381086 -4.2334628 -4.2333713 -4.2313333 -4.2276974 -4.2243166 -4.215981 -4.2086482 -4.2120242 -4.223053 -4.2331891 -4.2454948 -4.2531276][-4.2143183 -4.1971307 -4.1834531 -4.1710858 -4.1692138 -4.17031 -4.167923 -4.1638637 -4.1568136 -4.1539254 -4.1593046 -4.1707468 -4.1797943 -4.1945586 -4.2077103][-4.1813974 -4.1589966 -4.1380291 -4.1197534 -4.1177645 -4.1224413 -4.1233616 -4.1186957 -4.1102872 -4.1102905 -4.1157603 -4.1244287 -4.1312003 -4.1458287 -4.1631336][-4.1650238 -4.1436424 -4.1218972 -4.1032691 -4.1000023 -4.1042213 -4.105917 -4.099318 -4.0885053 -4.0890532 -4.0987153 -4.106617 -4.1068549 -4.1128268 -4.1308255][-4.1593504 -4.1377721 -4.1209412 -4.1067467 -4.1037993 -4.105258 -4.1068521 -4.1014185 -4.08826 -4.0822554 -4.0885067 -4.097733 -4.0914335 -4.0851564 -4.098063][-4.14607 -4.1184063 -4.1015778 -4.0882816 -4.0817742 -4.0819316 -4.0890946 -4.0923328 -4.0816703 -4.0681014 -4.0674748 -4.0745063 -4.0658469 -4.0485816 -4.05594][-4.1308012 -4.0907903 -4.0609121 -4.0396481 -4.0258045 -4.0288463 -4.048614 -4.0638456 -4.059391 -4.0480556 -4.0389385 -4.0372038 -4.0249782 -4.0097518 -4.019979][-4.1195192 -4.064889 -4.0144634 -3.9796991 -3.9603965 -3.9678056 -4.0042439 -4.0375466 -4.0448351 -4.0393548 -4.0201182 -3.9997463 -3.9854946 -3.9752672 -3.9839065][-4.1032734 -4.0404162 -3.9773273 -3.9362721 -3.9158711 -3.9191055 -3.9634409 -4.0169687 -4.046042 -4.0519223 -4.0307517 -3.9938149 -3.9690211 -3.9520831 -3.9497013][-4.1009054 -4.039938 -3.9770317 -3.9385941 -3.9198961 -3.9128308 -3.9525549 -4.0179787 -4.0649385 -4.0845118 -4.0686779 -4.0281215 -3.9937925 -3.9685259 -3.9577935][-4.1072507 -4.051959 -4.0012221 -3.9713907 -3.9599838 -3.9563053 -3.991961 -4.0516291 -4.0992126 -4.1250954 -4.1194539 -4.0878286 -4.0539403 -4.0286369 -4.0198212][-4.1245918 -4.0757785 -4.0376854 -4.0150518 -4.0129118 -4.0192838 -4.0494757 -4.0919023 -4.1278567 -4.1545572 -4.1589117 -4.1409926 -4.1181436 -4.0982904 -4.0906043][-4.1473532 -4.1026688 -4.072103 -4.0602741 -4.0727873 -4.0924058 -4.1173358 -4.1416297 -4.1629128 -4.1830769 -4.1945643 -4.1900821 -4.1782846 -4.162899 -4.1551738][-4.1928287 -4.1560774 -4.1336508 -4.13296 -4.153492 -4.1780376 -4.1970186 -4.2078118 -4.2166 -4.2280536 -4.2373753 -4.2396841 -4.2358932 -4.2259183 -4.2172909][-4.2407556 -4.2149286 -4.1994081 -4.2026596 -4.2221169 -4.2431345 -4.25871 -4.2660646 -4.2710524 -4.2749271 -4.2780051 -4.280797 -4.2807441 -4.2755113 -4.2694278]]...]
INFO - root - 2017-12-05 11:38:15.445124: step 4410, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 78h:19m:06s remains)
INFO - root - 2017-12-05 11:38:23.884242: step 4420, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 75h:55m:59s remains)
INFO - root - 2017-12-05 11:38:32.413873: step 4430, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 79h:34m:49s remains)
INFO - root - 2017-12-05 11:38:40.917603: step 4440, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 78h:46m:03s remains)
INFO - root - 2017-12-05 11:38:49.365793: step 4450, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:53m:42s remains)
INFO - root - 2017-12-05 11:38:57.920978: step 4460, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 77h:13m:11s remains)
INFO - root - 2017-12-05 11:39:06.579056: step 4470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 78h:38m:12s remains)
INFO - root - 2017-12-05 11:39:15.137025: step 4480, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 80h:10m:33s remains)
INFO - root - 2017-12-05 11:39:23.668467: step 4490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 78h:10m:40s remains)
INFO - root - 2017-12-05 11:39:32.277401: step 4500, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 78h:48m:51s remains)
2017-12-05 11:39:33.012525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2811551 -4.2809024 -4.2727489 -4.2652698 -4.2624445 -4.2629747 -4.2610512 -4.249052 -4.2457952 -4.2468491 -4.2523203 -4.2709117 -4.2923789 -4.3059978 -4.3145189][-4.2642961 -4.2643566 -4.2547421 -4.2445765 -4.2384663 -4.2381253 -4.2291641 -4.2033539 -4.1938338 -4.2008624 -4.2192688 -4.2514195 -4.2872152 -4.3069983 -4.31757][-4.2504363 -4.2509232 -4.2379408 -4.2231183 -4.2145591 -4.2128115 -4.1921792 -4.1462007 -4.1274877 -4.1495638 -4.1895475 -4.2370791 -4.283689 -4.3089767 -4.3216033][-4.2420964 -4.2411013 -4.22337 -4.204875 -4.1970997 -4.1897154 -4.154356 -4.0812197 -4.0583391 -4.1019673 -4.1646338 -4.226028 -4.2790565 -4.3071008 -4.3214259][-4.238265 -4.2324381 -4.2094107 -4.1851072 -4.1699533 -4.1495218 -4.0918059 -3.997 -3.9773202 -4.0467892 -4.129869 -4.2038789 -4.2648621 -4.298553 -4.3168855][-4.2381639 -4.2282495 -4.202065 -4.1748862 -4.1476574 -4.1076269 -4.0158014 -3.892565 -3.8871973 -3.9945507 -4.09939 -4.1824031 -4.2455678 -4.2845883 -4.3098474][-4.2397537 -4.2323546 -4.209465 -4.1830349 -4.1487827 -4.0909114 -3.9590104 -3.7957342 -3.806458 -3.9598975 -4.0862174 -4.1730518 -4.235815 -4.2792344 -4.3076839][-4.2392583 -4.2395754 -4.2276125 -4.20829 -4.1773887 -4.1147733 -3.9684641 -3.7876737 -3.8018699 -3.9709718 -4.1020484 -4.1863437 -4.2441516 -4.2873015 -4.3127823][-4.235672 -4.2417364 -4.2387223 -4.2316842 -4.2142806 -4.1652174 -4.0467572 -3.9040661 -3.907526 -4.0348663 -4.1419053 -4.2146173 -4.2640767 -4.3007193 -4.3194022][-4.2363691 -4.2406383 -4.2365313 -4.2365193 -4.2316628 -4.1975479 -4.1134324 -4.021687 -4.0314035 -4.1171408 -4.1920681 -4.2453003 -4.2804723 -4.3066454 -4.3197927][-4.246985 -4.2452044 -4.2338243 -4.2310023 -4.2266893 -4.1967735 -4.1358228 -4.0831332 -4.1070471 -4.1747961 -4.2320151 -4.2656813 -4.2823949 -4.2985148 -4.3118629][-4.2540674 -4.2517905 -4.2387853 -4.2322221 -4.2200212 -4.1872792 -4.1383405 -4.1077766 -4.1411309 -4.1979265 -4.2419639 -4.2625813 -4.2670341 -4.2768664 -4.2897611][-4.2570457 -4.258255 -4.2501783 -4.2455344 -4.2261162 -4.1913104 -4.1549063 -4.1393929 -4.1683526 -4.2074389 -4.2366829 -4.2458892 -4.2404976 -4.2453318 -4.2551064][-4.2568922 -4.2628884 -4.2612777 -4.2596602 -4.2387657 -4.205338 -4.1812277 -4.1775947 -4.2001963 -4.2225533 -4.2377033 -4.2353325 -4.2237434 -4.2233415 -4.2250233][-4.2606854 -4.2734857 -4.2786613 -4.2762957 -4.2569184 -4.2292776 -4.209271 -4.2081809 -4.2216997 -4.2347221 -4.2438545 -4.2384815 -4.2254429 -4.2211738 -4.2135043]]...]
INFO - root - 2017-12-05 11:39:41.379613: step 4510, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.771 sec/batch; 70h:13m:11s remains)
INFO - root - 2017-12-05 11:39:49.883327: step 4520, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 78h:30m:49s remains)
INFO - root - 2017-12-05 11:39:58.310852: step 4530, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 75h:34m:12s remains)
INFO - root - 2017-12-05 11:40:06.922414: step 4540, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.884 sec/batch; 80h:33m:06s remains)
INFO - root - 2017-12-05 11:40:15.437636: step 4550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 78h:16m:37s remains)
INFO - root - 2017-12-05 11:40:23.855198: step 4560, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 79h:02m:33s remains)
INFO - root - 2017-12-05 11:40:32.378266: step 4570, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 76h:48m:30s remains)
INFO - root - 2017-12-05 11:40:40.933398: step 4580, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 78h:40m:54s remains)
INFO - root - 2017-12-05 11:40:49.498479: step 4590, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 77h:16m:38s remains)
INFO - root - 2017-12-05 11:40:58.153753: step 4600, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 77h:30m:49s remains)
2017-12-05 11:40:58.940052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3256755 -4.3127966 -4.2994361 -4.291255 -4.2883739 -4.288012 -4.2894077 -4.2929158 -4.2987337 -4.3045235 -4.3094449 -4.3126922 -4.3165369 -4.3205872 -4.3193545][-4.3176966 -4.2988825 -4.2811117 -4.2709293 -4.26656 -4.2640562 -4.2634878 -4.2672329 -4.2755041 -4.2845726 -4.2913675 -4.2958903 -4.3006854 -4.3057556 -4.3037472][-4.3058748 -4.2819333 -4.2614765 -4.2499733 -4.2434516 -4.237164 -4.2329698 -4.235065 -4.2465758 -4.2606163 -4.270287 -4.2763476 -4.2819562 -4.2878413 -4.2850633][-4.2942085 -4.2677774 -4.2455688 -4.2320337 -4.2223415 -4.2114372 -4.201983 -4.2009826 -4.2157 -4.2374129 -4.2531667 -4.2625909 -4.2703881 -4.2767892 -4.2721758][-4.2857242 -4.2580166 -4.233427 -4.2164617 -4.2017159 -4.1858673 -4.1681781 -4.1593437 -4.1762638 -4.210186 -4.2365375 -4.2523847 -4.265718 -4.2753563 -4.2704272][-4.2838187 -4.2558661 -4.2277803 -4.2056923 -4.1839046 -4.1588035 -4.1255422 -4.0999818 -4.1163597 -4.1690803 -4.2138791 -4.2423453 -4.2660642 -4.2836409 -4.2832174][-4.2870355 -4.2602658 -4.2300682 -4.2010493 -4.1699681 -4.1306777 -4.0710635 -4.0117359 -4.0190859 -4.0962024 -4.1673193 -4.2148328 -4.2554412 -4.2865205 -4.2950158][-4.2929459 -4.2687216 -4.2393842 -4.2060933 -4.1715584 -4.1228719 -4.0355687 -3.9314837 -3.9155846 -4.0092897 -4.1063037 -4.1735997 -4.2289853 -4.2718029 -4.2911677][-4.298543 -4.2775731 -4.2507505 -4.2178583 -4.1888094 -4.146687 -4.0569468 -3.9327335 -3.8862495 -3.9649103 -4.064692 -4.1411462 -4.2027388 -4.2477636 -4.2722936][-4.2994576 -4.281 -4.2589273 -4.2298121 -4.21007 -4.1841841 -4.1169019 -4.0124903 -3.9542186 -3.9923019 -4.0615406 -4.1267543 -4.1843638 -4.2231693 -4.2454681][-4.2976904 -4.2821703 -4.2641773 -4.2402368 -4.2284226 -4.2180767 -4.1771975 -4.1054611 -4.0575204 -4.0689092 -4.1046419 -4.1441107 -4.1839509 -4.2116089 -4.2282667][-4.2923832 -4.2781119 -4.2621751 -4.2429466 -4.23624 -4.2377648 -4.221108 -4.1805797 -4.1519227 -4.1554618 -4.1708255 -4.1860013 -4.2024207 -4.2160721 -4.2269249][-4.2916694 -4.2791481 -4.2658448 -4.2520003 -4.2497349 -4.2571545 -4.256391 -4.2391658 -4.2269869 -4.2293677 -4.2337675 -4.2338815 -4.2336688 -4.2369947 -4.2452855][-4.3007493 -4.2912436 -4.2814965 -4.2729845 -4.2739048 -4.2802472 -4.2809954 -4.2743783 -4.2725697 -4.2767096 -4.2779226 -4.2720609 -4.2634597 -4.2615805 -4.2689395][-4.3174067 -4.3117018 -4.3047342 -4.2998562 -4.3010778 -4.30284 -4.298831 -4.2947226 -4.2986641 -4.3039112 -4.3035116 -4.294682 -4.2834 -4.2788553 -4.2852]]...]
INFO - root - 2017-12-05 11:41:07.325811: step 4610, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.829 sec/batch; 75h:31m:07s remains)
INFO - root - 2017-12-05 11:41:15.737189: step 4620, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 79h:35m:30s remains)
INFO - root - 2017-12-05 11:41:24.327828: step 4630, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 76h:26m:58s remains)
INFO - root - 2017-12-05 11:41:32.833070: step 4640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 77h:20m:33s remains)
INFO - root - 2017-12-05 11:41:41.349301: step 4650, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.855 sec/batch; 77h:51m:09s remains)
INFO - root - 2017-12-05 11:41:50.027257: step 4660, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 77h:43m:01s remains)
INFO - root - 2017-12-05 11:41:58.593343: step 4670, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 79h:23m:48s remains)
INFO - root - 2017-12-05 11:42:07.144157: step 4680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 77h:39m:20s remains)
INFO - root - 2017-12-05 11:42:15.654869: step 4690, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:51m:27s remains)
INFO - root - 2017-12-05 11:42:24.252704: step 4700, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 80h:29m:44s remains)
2017-12-05 11:42:25.011025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1983051 -4.2075777 -4.2279768 -4.2429066 -4.2382159 -4.2221603 -4.20911 -4.2082715 -4.2121024 -4.2228942 -4.2186913 -4.1968842 -4.1840515 -4.1914372 -4.2047396][-4.1880264 -4.2026587 -4.2287087 -4.24654 -4.2406917 -4.2211647 -4.2076979 -4.2092652 -4.2155056 -4.2265048 -4.2248759 -4.2080722 -4.198813 -4.2081857 -4.2251253][-4.1632423 -4.1854086 -4.2216606 -4.2435527 -4.236186 -4.2099857 -4.192924 -4.19894 -4.2140288 -4.229506 -4.2277164 -4.2127619 -4.2074809 -4.2199264 -4.2411432][-4.1527829 -4.1743236 -4.2126307 -4.232305 -4.2113628 -4.1680045 -4.141715 -4.1594987 -4.1937914 -4.2164326 -4.2148995 -4.2013988 -4.2025604 -4.2210741 -4.2489805][-4.150373 -4.1659751 -4.2000089 -4.206665 -4.1590257 -4.0819921 -4.0435433 -4.0875654 -4.1521668 -4.1853609 -4.1813917 -4.1655712 -4.1724844 -4.2019758 -4.2385163][-4.1610012 -4.1729245 -4.1927266 -4.1755672 -4.0981321 -3.9873745 -3.9395208 -4.0069237 -4.0986362 -4.138515 -4.1300058 -4.1138253 -4.1329036 -4.176867 -4.2203422][-4.1858935 -4.1921716 -4.1941814 -4.1587949 -4.0691528 -3.9513295 -3.9073942 -3.9814076 -4.0782375 -4.1154842 -4.1020117 -4.0848222 -4.1137967 -4.16974 -4.2174788][-4.2181234 -4.2184658 -4.2084031 -4.170527 -4.08978 -3.9908 -3.9564815 -4.0183678 -4.0993829 -4.1286163 -4.1113558 -4.0953174 -4.124825 -4.1804271 -4.2269835][-4.2441955 -4.239749 -4.2259474 -4.19417 -4.1308713 -4.05404 -4.0220995 -4.0680633 -4.130621 -4.1503263 -4.1310792 -4.1173019 -4.1460962 -4.1964579 -4.23704][-4.2564211 -4.2485847 -4.23515 -4.2144628 -4.1718655 -4.1194897 -4.0914731 -4.1191769 -4.1615181 -4.1728864 -4.1589403 -4.147471 -4.171885 -4.2144985 -4.2476707][-4.2539234 -4.2471242 -4.238636 -4.2291875 -4.2048993 -4.1737094 -4.1528111 -4.1681652 -4.1957788 -4.2031817 -4.1941977 -4.1831455 -4.1965075 -4.230876 -4.2581663][-4.2534132 -4.2494845 -4.2463441 -4.2459507 -4.2350721 -4.2174335 -4.2026143 -4.2129555 -4.2324476 -4.2363791 -4.2274642 -4.2158766 -4.2238069 -4.2513013 -4.2736344][-4.2639251 -4.2654457 -4.2661424 -4.2686672 -4.2658134 -4.2576265 -4.2498918 -4.2556272 -4.2678127 -4.2706847 -4.2624373 -4.2534962 -4.2588878 -4.2796583 -4.29525][-4.2794657 -4.2840924 -4.2850895 -4.2872095 -4.2878103 -4.2851467 -4.2817922 -4.2848425 -4.2908111 -4.2922215 -4.2861309 -4.281085 -4.283946 -4.2949095 -4.3014555][-4.2849 -4.2890973 -4.29036 -4.2924409 -4.2935071 -4.292275 -4.2900095 -4.2917118 -4.2948952 -4.2968707 -4.2954106 -4.293601 -4.294066 -4.2969041 -4.2961597]]...]
INFO - root - 2017-12-05 11:42:33.393678: step 4710, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 77h:56m:10s remains)
INFO - root - 2017-12-05 11:42:41.848156: step 4720, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.848 sec/batch; 77h:12m:31s remains)
INFO - root - 2017-12-05 11:42:50.258350: step 4730, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.804 sec/batch; 73h:12m:15s remains)
INFO - root - 2017-12-05 11:42:58.808965: step 4740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 77h:34m:19s remains)
INFO - root - 2017-12-05 11:43:07.310464: step 4750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 77h:19m:48s remains)
INFO - root - 2017-12-05 11:43:15.803542: step 4760, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 75h:38m:22s remains)
INFO - root - 2017-12-05 11:43:24.241441: step 4770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 77h:07m:32s remains)
INFO - root - 2017-12-05 11:43:32.749768: step 4780, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 78h:01m:56s remains)
INFO - root - 2017-12-05 11:43:41.408219: step 4790, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 78h:36m:09s remains)
INFO - root - 2017-12-05 11:43:50.000463: step 4800, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.840 sec/batch; 76h:30m:26s remains)
2017-12-05 11:43:50.721711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2330327 -4.2367125 -4.2433848 -4.2487874 -4.2455626 -4.23893 -4.2371874 -4.2444229 -4.2567077 -4.266675 -4.2662058 -4.2605925 -4.262054 -4.2725987 -4.2843323][-4.2396722 -4.2366095 -4.2365961 -4.2398715 -4.2412562 -4.2434998 -4.2495179 -4.2580395 -4.2676873 -4.275423 -4.2770047 -4.275948 -4.2785063 -4.2843943 -4.2894969][-4.2493343 -4.2425408 -4.2394614 -4.2443681 -4.2514396 -4.25977 -4.2675166 -4.2740445 -4.2793403 -4.2802258 -4.2772341 -4.2752953 -4.2765627 -4.2794309 -4.2801938][-4.2598658 -4.2511559 -4.24696 -4.2523971 -4.26054 -4.2669377 -4.2701764 -4.2722635 -4.2740793 -4.2704229 -4.2638907 -4.2587128 -4.2584772 -4.2599759 -4.2571845][-4.2625308 -4.252852 -4.2460742 -4.2493486 -4.2545676 -4.2526269 -4.2428017 -4.2357993 -4.2368803 -4.2342405 -4.2266293 -4.21872 -4.2200871 -4.225523 -4.2255363][-4.2542768 -4.2439418 -4.2318039 -4.2284012 -4.2277732 -4.2094927 -4.1751838 -4.1536655 -4.1628237 -4.1785693 -4.1801515 -4.1750345 -4.1811662 -4.1919804 -4.1952949][-4.2295227 -4.2204409 -4.203145 -4.1907592 -4.1790977 -4.1366158 -4.0688181 -4.0308557 -4.0647678 -4.1171322 -4.1404543 -4.1448154 -4.1571441 -4.1725821 -4.1768441][-4.1950011 -4.1909509 -4.1719255 -4.1517448 -4.1329093 -4.0748644 -3.9831073 -3.9386768 -4.0016513 -4.0837364 -4.1221981 -4.1318269 -4.1453509 -4.162096 -4.1691108][-4.1667433 -4.1667466 -4.1478605 -4.1291261 -4.1227121 -4.082099 -4.0050945 -3.9751408 -4.0395374 -4.1142945 -4.1465111 -4.1512485 -4.1565948 -4.1675367 -4.1746264][-4.1403856 -4.1397758 -4.121592 -4.1160793 -4.1360722 -4.1292915 -4.0907063 -4.0794873 -4.1256771 -4.1750264 -4.1964941 -4.1988215 -4.1976185 -4.2009583 -4.2053666][-4.1273427 -4.1186161 -4.1039257 -4.1160955 -4.153461 -4.1691437 -4.1595531 -4.1597071 -4.1890292 -4.2202153 -4.2382779 -4.2431936 -4.2425227 -4.2431307 -4.2459507][-4.128624 -4.1142969 -4.1105103 -4.13672 -4.1757669 -4.1962457 -4.1992364 -4.2034073 -4.2219987 -4.2442541 -4.2623091 -4.2703619 -4.2728066 -4.2738461 -4.2740812][-4.156147 -4.1458468 -4.1540556 -4.1813583 -4.2073636 -4.2189984 -4.2211909 -4.2240305 -4.237474 -4.2578721 -4.2773485 -4.2887969 -4.2941723 -4.2939668 -4.2905035][-4.1955056 -4.1929727 -4.204186 -4.2214 -4.2331247 -4.236845 -4.2362604 -4.2384381 -4.2487793 -4.2637067 -4.2788615 -4.2894583 -4.294229 -4.2907743 -4.2836647][-4.2124896 -4.214776 -4.22568 -4.2368279 -4.2425423 -4.2439957 -4.2442694 -4.2460904 -4.249424 -4.2513971 -4.2559748 -4.2638559 -4.2666726 -4.2587104 -4.2478251]]...]
INFO - root - 2017-12-05 11:43:59.143485: step 4810, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 79h:58m:15s remains)
INFO - root - 2017-12-05 11:44:07.689905: step 4820, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 76h:18m:08s remains)
INFO - root - 2017-12-05 11:44:16.064318: step 4830, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 74h:37m:07s remains)
INFO - root - 2017-12-05 11:44:24.559710: step 4840, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 78h:02m:01s remains)
INFO - root - 2017-12-05 11:44:33.114444: step 4850, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 76h:20m:11s remains)
INFO - root - 2017-12-05 11:44:41.615610: step 4860, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 77h:11m:55s remains)
INFO - root - 2017-12-05 11:44:50.038010: step 4870, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:31m:39s remains)
INFO - root - 2017-12-05 11:44:58.478554: step 4880, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 80h:12m:37s remains)
INFO - root - 2017-12-05 11:45:06.936219: step 4890, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 79h:30m:25s remains)
INFO - root - 2017-12-05 11:45:15.533469: step 4900, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 77h:16m:37s remains)
2017-12-05 11:45:16.257127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1297231 -4.1928458 -4.2449374 -4.2620282 -4.2459702 -4.2097239 -4.1950526 -4.1904144 -4.198936 -4.22833 -4.2600207 -4.2800059 -4.2872353 -4.2808418 -4.2525444][-4.1307349 -4.1999006 -4.257576 -4.278944 -4.2671018 -4.2317133 -4.2110391 -4.203743 -4.2136731 -4.2468572 -4.2783432 -4.2916207 -4.2941704 -4.2884903 -4.2599449][-4.1244888 -4.1981864 -4.2593021 -4.2839093 -4.275176 -4.235734 -4.2057095 -4.1954861 -4.2140746 -4.2551913 -4.2907109 -4.3016825 -4.3007097 -4.2938991 -4.2667751][-4.1085238 -4.1850672 -4.2490745 -4.27665 -4.2679834 -4.2232852 -4.18214 -4.1676435 -4.1954188 -4.2451768 -4.2882071 -4.304965 -4.3053589 -4.2975636 -4.2708282][-4.0946469 -4.1672077 -4.232336 -4.2629156 -4.2526827 -4.2013016 -4.1459889 -4.1198673 -4.1526 -4.214139 -4.2669425 -4.2963653 -4.3031688 -4.2932663 -4.2629476][-4.09792 -4.1616578 -4.2238946 -4.2569833 -4.2467694 -4.1887383 -4.1158767 -4.0655656 -4.0950093 -4.1702595 -4.2357826 -4.2807541 -4.2967043 -4.2835159 -4.2484994][-4.1232357 -4.1762996 -4.2298512 -4.259697 -4.2517061 -4.19432 -4.1115322 -4.03864 -4.0593715 -4.1446514 -4.2215319 -4.2780905 -4.3023396 -4.2862639 -4.2466707][-4.1540079 -4.1946921 -4.2333508 -4.2499943 -4.2381449 -4.1894522 -4.1160369 -4.049994 -4.0692339 -4.1543636 -4.2335954 -4.2923727 -4.3193707 -4.3034859 -4.2625294][-4.1756582 -4.2038436 -4.224504 -4.2211461 -4.198524 -4.1565351 -4.1038685 -4.068954 -4.098805 -4.1770496 -4.2511692 -4.3037 -4.3289585 -4.3180847 -4.2825274][-4.1762886 -4.1922388 -4.2014761 -4.1858225 -4.15426 -4.12123 -4.09727 -4.0975475 -4.1356792 -4.2028189 -4.264668 -4.3057947 -4.3267932 -4.3195076 -4.291368][-4.166646 -4.1754127 -4.1812992 -4.1634436 -4.1319642 -4.1127319 -4.1150184 -4.1361661 -4.1740003 -4.2286434 -4.2746482 -4.3040667 -4.3191872 -4.3118086 -4.2900562][-4.1754909 -4.1814332 -4.1840968 -4.1661034 -4.1388063 -4.1333847 -4.1514187 -4.1795139 -4.2140884 -4.2577691 -4.2891636 -4.3077717 -4.3161254 -4.3053246 -4.2868414][-4.2113171 -4.2149878 -4.211791 -4.1929808 -4.1713972 -4.1738496 -4.1977811 -4.2263517 -4.25772 -4.2908916 -4.3104596 -4.319984 -4.3210588 -4.307332 -4.2916012][-4.2521949 -4.2551088 -4.2491021 -4.2332897 -4.2174387 -4.2208915 -4.2417369 -4.2673068 -4.292448 -4.314868 -4.3261933 -4.3292475 -4.326036 -4.3121357 -4.299027][-4.2719378 -4.2747278 -4.2702551 -4.2611451 -4.2535272 -4.2564163 -4.2698236 -4.2865739 -4.3018813 -4.3146234 -4.3205991 -4.321651 -4.3180285 -4.306469 -4.2968521]]...]
INFO - root - 2017-12-05 11:45:24.744804: step 4910, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 79h:13m:14s remains)
INFO - root - 2017-12-05 11:45:33.263352: step 4920, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:50m:24s remains)
INFO - root - 2017-12-05 11:45:41.904023: step 4930, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 79h:15m:58s remains)
INFO - root - 2017-12-05 11:45:50.451023: step 4940, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 77h:16m:26s remains)
INFO - root - 2017-12-05 11:45:59.010176: step 4950, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 76h:33m:33s remains)
INFO - root - 2017-12-05 11:46:07.581625: step 4960, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 76h:58m:21s remains)
INFO - root - 2017-12-05 11:46:16.125964: step 4970, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 80h:36m:15s remains)
INFO - root - 2017-12-05 11:46:24.490163: step 4980, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 78h:37m:09s remains)
INFO - root - 2017-12-05 11:46:32.999489: step 4990, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 79h:41m:26s remains)
INFO - root - 2017-12-05 11:46:41.526713: step 5000, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 77h:05m:08s remains)
2017-12-05 11:46:42.258272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1683826 -4.1898413 -4.17434 -4.1212478 -4.0831475 -4.0775971 -4.1035662 -4.1426015 -4.1785378 -4.2153893 -4.2395124 -4.2482848 -4.2569485 -4.2590652 -4.2516046][-4.1737156 -4.2009044 -4.1877265 -4.1360383 -4.0896931 -4.082027 -4.1043887 -4.1343436 -4.1619148 -4.1960664 -4.22479 -4.2433157 -4.2571573 -4.2588758 -4.251544][-4.1842585 -4.21201 -4.1976805 -4.1453848 -4.0957108 -4.08286 -4.1032686 -4.1281428 -4.1465273 -4.1752524 -4.2063127 -4.2302556 -4.2463894 -4.2525663 -4.2511306][-4.1752677 -4.2053161 -4.1961493 -4.1504622 -4.1019125 -4.0803971 -4.0897083 -4.1135559 -4.1349573 -4.1660514 -4.1987672 -4.2250896 -4.2400918 -4.2502766 -4.2565122][-4.1553364 -4.1964116 -4.1981688 -4.16448 -4.1147728 -4.0758114 -4.063714 -4.0844812 -4.1155624 -4.14848 -4.177815 -4.2055826 -4.2249432 -4.2433505 -4.2561378][-4.1503568 -4.2008872 -4.2071013 -4.1759148 -4.1248364 -4.0607314 -4.0147524 -4.0332413 -4.0856652 -4.1262708 -4.1516895 -4.1797805 -4.2076216 -4.2325292 -4.2494979][-4.1684484 -4.2093139 -4.2040229 -4.1633854 -4.0962062 -3.9907887 -3.8989322 -3.9294643 -4.0267291 -4.0938425 -4.1238856 -4.1527705 -4.1910272 -4.2217689 -4.2419553][-4.1728215 -4.1956816 -4.1749091 -4.121531 -4.0350389 -3.8895063 -3.7471147 -3.805418 -3.9671767 -4.0611506 -4.0948997 -4.1165166 -4.1551976 -4.1938753 -4.2217355][-4.175271 -4.184329 -4.1508245 -4.0922852 -4.007225 -3.8662705 -3.7222986 -3.8060775 -3.9876266 -4.0707235 -4.0848184 -4.0810189 -4.1059275 -4.1528831 -4.1892285][-4.192843 -4.1967726 -4.1651006 -4.1150465 -4.0543008 -3.9654911 -3.8781245 -3.9495757 -4.0814981 -4.125061 -4.1146531 -4.086832 -4.0993419 -4.1377597 -4.1652021][-4.219718 -4.2167406 -4.1927872 -4.1593542 -4.1221838 -4.0745244 -4.0272613 -4.0743232 -4.1512265 -4.1655431 -4.1405077 -4.1058812 -4.1136 -4.1419563 -4.1521077][-4.2328877 -4.2292514 -4.2146187 -4.1953378 -4.1739473 -4.15133 -4.1236968 -4.1499243 -4.1882339 -4.1879115 -4.1558161 -4.1199746 -4.1237626 -4.1433344 -4.1385722][-4.2449622 -4.2440748 -4.2384682 -4.2296815 -4.2159405 -4.2046885 -4.1878972 -4.1951718 -4.2121506 -4.2078824 -4.1778941 -4.1457438 -4.1416631 -4.1494813 -4.1388245][-4.2660789 -4.2662339 -4.2627387 -4.2562909 -4.2473168 -4.23933 -4.225101 -4.2240291 -4.2298937 -4.2280526 -4.2101731 -4.1859746 -4.1751328 -4.1738205 -4.1657062][-4.2857485 -4.28455 -4.2793622 -4.2708378 -4.26124 -4.2527733 -4.2411189 -4.2375469 -4.2405047 -4.2433991 -4.2388449 -4.2265005 -4.2157321 -4.2125106 -4.205296]]...]
INFO - root - 2017-12-05 11:46:50.760225: step 5010, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.829 sec/batch; 75h:24m:23s remains)
INFO - root - 2017-12-05 11:46:59.341744: step 5020, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 77h:11m:25s remains)
INFO - root - 2017-12-05 11:47:07.905375: step 5030, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 77h:14m:57s remains)
INFO - root - 2017-12-05 11:47:16.531226: step 5040, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 79h:14m:33s remains)
INFO - root - 2017-12-05 11:47:25.058574: step 5050, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:47m:37s remains)
INFO - root - 2017-12-05 11:47:33.500785: step 5060, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 78h:11m:57s remains)
INFO - root - 2017-12-05 11:47:42.047151: step 5070, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 79h:09m:28s remains)
INFO - root - 2017-12-05 11:47:50.618428: step 5080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 76h:42m:00s remains)
INFO - root - 2017-12-05 11:47:59.213692: step 5090, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 78h:29m:22s remains)
INFO - root - 2017-12-05 11:48:07.767070: step 5100, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 76h:01m:59s remains)
2017-12-05 11:48:08.541766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2738104 -4.2692661 -4.2661428 -4.2617521 -4.2522035 -4.2363653 -4.2274094 -4.2264309 -4.2287264 -4.2251115 -4.2237115 -4.2169018 -4.20626 -4.2082925 -4.2288346][-4.2633171 -4.2539034 -4.2475748 -4.2479525 -4.2473373 -4.2407527 -4.2403045 -4.2430129 -4.2375584 -4.2191296 -4.2046771 -4.1904225 -4.182343 -4.1975732 -4.2287431][-4.2440491 -4.2329035 -4.2303019 -4.24101 -4.2520618 -4.2551141 -4.2563844 -4.2535195 -4.2336884 -4.1996975 -4.1800075 -4.1682477 -4.1702838 -4.196013 -4.2325869][-4.2242785 -4.2134 -4.2161207 -4.2337332 -4.2512507 -4.2626686 -4.2612638 -4.2431297 -4.2102547 -4.1731491 -4.1594787 -4.1597786 -4.1736207 -4.2062383 -4.2417011][-4.2012739 -4.201169 -4.2136245 -4.2332816 -4.246244 -4.2566638 -4.2441053 -4.2098169 -4.1699576 -4.1397047 -4.1390915 -4.1533089 -4.1757932 -4.2090077 -4.2393117][-4.1774449 -4.19532 -4.2210083 -4.2410536 -4.2417502 -4.2316647 -4.1975679 -4.1454992 -4.1001139 -4.0841384 -4.1035633 -4.1355443 -4.1682305 -4.2018228 -4.2260666][-4.1525927 -4.1906314 -4.227531 -4.2418756 -4.22382 -4.1875992 -4.1224389 -4.043407 -3.9874487 -3.993422 -4.0430374 -4.0991745 -4.1461096 -4.1822762 -4.2023883][-4.1390162 -4.1962781 -4.2372427 -4.240169 -4.2028909 -4.1388717 -4.0399585 -3.931747 -3.8658416 -3.8963876 -3.9796488 -4.0574532 -4.116056 -4.1532283 -4.1697011][-4.1518545 -4.2108941 -4.2434912 -4.230433 -4.1789813 -4.0958953 -3.9804003 -3.8670311 -3.8157687 -3.8721447 -3.9681268 -4.0416718 -4.0972295 -4.1292186 -4.1413746][-4.1946144 -4.23421 -4.2455816 -4.2183666 -4.1623607 -4.0834122 -3.9862604 -3.9104409 -3.9024544 -3.9648738 -4.0339251 -4.0766487 -4.1118059 -4.1317263 -4.1368074][-4.2453337 -4.2609744 -4.2512808 -4.2162538 -4.1591477 -4.0954127 -4.0382133 -4.0167427 -4.0424156 -4.0900283 -4.1258354 -4.1436839 -4.1595531 -4.1676779 -4.16669][-4.2860956 -4.2833939 -4.2559743 -4.2115417 -4.1556172 -4.1067252 -4.0847168 -4.0987053 -4.1345634 -4.1710253 -4.191299 -4.19903 -4.20461 -4.2091203 -4.2089119][-4.3052144 -4.2891765 -4.2513213 -4.1962194 -4.1330128 -4.0906868 -4.0916314 -4.1254659 -4.1641126 -4.1960716 -4.2130084 -4.2191148 -4.2254558 -4.2336025 -4.2387929][-4.2952948 -4.2713518 -4.2259212 -4.15672 -4.0819659 -4.0489931 -4.0703068 -4.1183395 -4.1632533 -4.1972356 -4.213634 -4.2201128 -4.2298231 -4.2416062 -4.2519379][-4.2737074 -4.2445889 -4.1886725 -4.10386 -4.0216317 -4.0048375 -4.0470581 -4.1075034 -4.1612887 -4.197454 -4.2127113 -4.2193885 -4.2316008 -4.2454572 -4.2580137]]...]
INFO - root - 2017-12-05 11:48:16.994303: step 5110, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 78h:21m:42s remains)
INFO - root - 2017-12-05 11:48:25.540962: step 5120, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 75h:44m:26s remains)
INFO - root - 2017-12-05 11:48:34.099341: step 5130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 77h:22m:46s remains)
INFO - root - 2017-12-05 11:48:42.625139: step 5140, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 76h:47m:15s remains)
INFO - root - 2017-12-05 11:48:51.106652: step 5150, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 75h:32m:27s remains)
INFO - root - 2017-12-05 11:48:59.685023: step 5160, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 77h:25m:29s remains)
INFO - root - 2017-12-05 11:49:08.189587: step 5170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 76h:53m:23s remains)
INFO - root - 2017-12-05 11:49:16.772770: step 5180, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 75h:33m:37s remains)
INFO - root - 2017-12-05 11:49:25.228797: step 5190, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:31m:55s remains)
INFO - root - 2017-12-05 11:49:33.821458: step 5200, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 78h:18m:36s remains)
2017-12-05 11:49:34.629450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2917895 -4.2815428 -4.2566862 -4.2209673 -4.1835246 -4.1621542 -4.1224561 -4.1002502 -4.132369 -4.1570048 -4.1656642 -4.1838975 -4.2056942 -4.2078929 -4.1803312][-4.2918639 -4.2821031 -4.2576661 -4.2221537 -4.184042 -4.16216 -4.1212053 -4.0997491 -4.1347833 -4.1579609 -4.1684542 -4.1824679 -4.2022271 -4.2058282 -4.1851091][-4.2951412 -4.2859445 -4.2594 -4.22059 -4.1809144 -4.1513386 -4.1061358 -4.0855708 -4.1263795 -4.1532412 -4.164403 -4.1788306 -4.1987462 -4.2057714 -4.1962609][-4.300777 -4.2902956 -4.2579346 -4.2094727 -4.1599026 -4.1177592 -4.06254 -4.04332 -4.1009927 -4.1459827 -4.1656294 -4.1834359 -4.2046552 -4.2167411 -4.2159748][-4.30441 -4.2914438 -4.2526603 -4.1910529 -4.1223054 -4.0593672 -3.987016 -3.9717238 -4.0622425 -4.1348104 -4.1687 -4.1926193 -4.2166266 -4.2310014 -4.2315693][-4.3049531 -4.2905669 -4.24846 -4.176662 -4.0885367 -3.9987702 -3.8978119 -3.8842261 -4.0149155 -4.1176748 -4.1721973 -4.2050204 -4.2307339 -4.2433481 -4.2390518][-4.3059573 -4.2925382 -4.2527943 -4.1815486 -4.0874233 -3.980514 -3.861115 -3.85665 -4.0044279 -4.1203675 -4.1853967 -4.220314 -4.2437553 -4.2487941 -4.2337704][-4.3081808 -4.2979445 -4.2646117 -4.2028356 -4.11935 -4.0205693 -3.9147267 -3.9196017 -4.0449967 -4.145678 -4.2025681 -4.2328291 -4.2519574 -4.2501564 -4.2272015][-4.3075924 -4.3012352 -4.27477 -4.2237344 -4.1551156 -4.0751286 -3.9978912 -4.008214 -4.0973945 -4.1745653 -4.2198482 -4.2441754 -4.2608323 -4.2541618 -4.2263174][-4.3023238 -4.2981377 -4.2783728 -4.2366934 -4.1792836 -4.1181512 -4.066761 -4.0755057 -4.1383238 -4.1945658 -4.23113 -4.2558727 -4.27088 -4.2606297 -4.2317967][-4.2959962 -4.293334 -4.27793 -4.2421446 -4.19296 -4.14712 -4.1108913 -4.1164742 -4.1628742 -4.2039232 -4.23807 -4.2690248 -4.2816296 -4.2688179 -4.243381][-4.2929435 -4.2917075 -4.2760739 -4.2418747 -4.1965008 -4.1598444 -4.1333876 -4.1370516 -4.1733084 -4.2038546 -4.2415185 -4.2768073 -4.283061 -4.2678018 -4.2452269][-4.2927551 -4.2912655 -4.2729697 -4.23618 -4.189178 -4.1567698 -4.1366091 -4.1421704 -4.1748357 -4.204143 -4.2471972 -4.2795138 -4.2742181 -4.2542233 -4.2368288][-4.2938409 -4.2903857 -4.2668595 -4.2223177 -4.1707563 -4.1393528 -4.1207695 -4.1311135 -4.1722674 -4.2119689 -4.2562051 -4.2823858 -4.268424 -4.2430758 -4.2276921][-4.2947598 -4.2882214 -4.2584357 -4.2059035 -4.1492181 -4.1165895 -4.0987 -4.1155672 -4.1711645 -4.2254462 -4.2668719 -4.28213 -4.2604704 -4.2294965 -4.2132998]]...]
INFO - root - 2017-12-05 11:49:43.034247: step 5210, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 76h:58m:44s remains)
INFO - root - 2017-12-05 11:49:51.552136: step 5220, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:20m:58s remains)
INFO - root - 2017-12-05 11:50:00.167083: step 5230, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 78h:41m:19s remains)
INFO - root - 2017-12-05 11:50:08.721518: step 5240, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 75h:41m:56s remains)
INFO - root - 2017-12-05 11:50:17.307606: step 5250, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 74h:58m:23s remains)
INFO - root - 2017-12-05 11:50:25.823206: step 5260, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 79h:13m:18s remains)
INFO - root - 2017-12-05 11:50:34.352201: step 5270, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 75h:12m:40s remains)
INFO - root - 2017-12-05 11:50:42.847691: step 5280, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.915 sec/batch; 83h:11m:37s remains)
INFO - root - 2017-12-05 11:50:51.347312: step 5290, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 75h:13m:15s remains)
INFO - root - 2017-12-05 11:50:59.815388: step 5300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:59m:20s remains)
2017-12-05 11:51:00.531613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2761369 -4.2868786 -4.2929931 -4.2942119 -4.2857871 -4.2742496 -4.2658629 -4.2655077 -4.265378 -4.2629609 -4.2618003 -4.2611542 -4.2700887 -4.2833862 -4.3002748][-4.2407203 -4.2542958 -4.2643218 -4.2681208 -4.25632 -4.2409997 -4.2281413 -4.2254343 -4.2246685 -4.225163 -4.2243185 -4.2228589 -4.2336807 -4.2518997 -4.2754555][-4.1898055 -4.207931 -4.2238765 -4.2307053 -4.2165122 -4.1963716 -4.1776247 -4.1710811 -4.1738963 -4.181036 -4.1813641 -4.177073 -4.1848154 -4.20374 -4.2312765][-4.1422644 -4.1657734 -4.1883516 -4.2004247 -4.1837044 -4.1529431 -4.12105 -4.1072459 -4.1165357 -4.1354923 -4.1442895 -4.1411495 -4.145587 -4.1638002 -4.192718][-4.1055479 -4.1327219 -4.1607342 -4.1775036 -4.1597161 -4.1184454 -4.0745254 -4.0540543 -4.0699234 -4.1025195 -4.1230683 -4.1273212 -4.1331306 -4.1512117 -4.1785135][-4.0782547 -4.1076193 -4.1380358 -4.1537881 -4.1299443 -4.0742412 -4.0162063 -3.9879167 -4.0089884 -4.0593104 -4.0986304 -4.118134 -4.1299648 -4.1488791 -4.1706862][-4.0798349 -4.1168342 -4.1485367 -4.156177 -4.1187696 -4.044507 -3.9705064 -3.9347463 -3.9606366 -4.0264273 -4.0845728 -4.1201382 -4.1384277 -4.15827 -4.174304][-4.0948682 -4.1367459 -4.1678839 -4.1684446 -4.1234274 -4.0427461 -3.9661071 -3.9330771 -3.9614277 -4.0285935 -4.09056 -4.1291075 -4.1477842 -4.1673951 -4.184238][-4.099957 -4.1447749 -4.1767521 -4.1762447 -4.1336555 -4.0595117 -3.9919019 -3.9671419 -3.9931583 -4.0478234 -4.0956831 -4.122529 -4.1359611 -4.1552358 -4.1768384][-4.1046715 -4.1472116 -4.1778049 -4.179358 -4.1440268 -4.0826182 -4.02721 -4.0078249 -4.0262256 -4.0656776 -4.0966792 -4.1119776 -4.1227632 -4.1445441 -4.1719136][-4.1056161 -4.1416 -4.1681414 -4.1713924 -4.1455479 -4.1004448 -4.0588531 -4.0453243 -4.0595512 -4.0891271 -4.1087675 -4.1174474 -4.127893 -4.1507206 -4.1808262][-4.1035161 -4.1335773 -4.1569586 -4.1619816 -4.1456246 -4.1150451 -4.0877051 -4.0829778 -4.097311 -4.1211815 -4.13368 -4.1392059 -4.1478238 -4.1686783 -4.1959672][-4.107832 -4.1316791 -4.1489429 -4.1533642 -4.1418867 -4.11938 -4.1027 -4.1080112 -4.1271081 -4.1493363 -4.1583014 -4.160656 -4.1656971 -4.182301 -4.205337][-4.1193509 -4.1358442 -4.1445088 -4.1465435 -4.1391082 -4.1247687 -4.1178718 -4.131041 -4.1530757 -4.17365 -4.1780987 -4.1761503 -4.1766639 -4.1901188 -4.2096272][-4.1281 -4.1397338 -4.1437235 -4.1453285 -4.142612 -4.1350284 -4.133893 -4.1476946 -4.1675224 -4.1841455 -4.1856303 -4.1840391 -4.1842275 -4.1957054 -4.2120962]]...]
INFO - root - 2017-12-05 11:51:08.780425: step 5310, loss = 2.11, batch loss = 2.05 (9.9 examples/sec; 0.811 sec/batch; 73h:42m:34s remains)
INFO - root - 2017-12-05 11:51:17.279941: step 5320, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 76h:48m:51s remains)
INFO - root - 2017-12-05 11:51:25.802729: step 5330, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 74h:24m:36s remains)
INFO - root - 2017-12-05 11:51:34.297734: step 5340, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 81h:24m:10s remains)
INFO - root - 2017-12-05 11:51:42.773526: step 5350, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 77h:52m:06s remains)
INFO - root - 2017-12-05 11:51:51.269940: step 5360, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 77h:26m:39s remains)
INFO - root - 2017-12-05 11:51:59.774579: step 5370, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 76h:44m:47s remains)
INFO - root - 2017-12-05 11:52:08.245888: step 5380, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 75h:31m:29s remains)
INFO - root - 2017-12-05 11:52:16.780435: step 5390, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 76h:52m:25s remains)
INFO - root - 2017-12-05 11:52:25.242931: step 5400, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 74h:22m:54s remains)
2017-12-05 11:52:25.967058: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3010139 -4.2944655 -4.2918987 -4.29126 -4.290484 -4.2926273 -4.2967954 -4.3024545 -4.3039169 -4.300818 -4.2990236 -4.3005624 -4.3017254 -4.3054304 -4.3133535][-4.2897716 -4.2862067 -4.28721 -4.2882695 -4.2874188 -4.2883987 -4.2949591 -4.3034472 -4.3046818 -4.2967916 -4.29247 -4.2925749 -4.2912316 -4.2930913 -4.3003006][-4.2769661 -4.2743707 -4.2752533 -4.273941 -4.2686257 -4.2670569 -4.27486 -4.2880206 -4.2928257 -4.2840919 -4.2785778 -4.2763815 -4.2736526 -4.2755885 -4.282752][-4.2666583 -4.2607455 -4.2572875 -4.2510233 -4.2408562 -4.2349586 -4.2412691 -4.2578759 -4.2700353 -4.2674017 -4.2637653 -4.2613873 -4.2599859 -4.2624097 -4.2701044][-4.2499847 -4.234056 -4.2188377 -4.2037983 -4.1871138 -4.1721697 -4.173564 -4.1992431 -4.2294245 -4.2446795 -4.2509251 -4.2544971 -4.2585177 -4.2624846 -4.2714167][-4.227232 -4.1991072 -4.1693645 -4.1429992 -4.1152163 -4.08431 -4.0656371 -4.0929871 -4.1509295 -4.1962752 -4.2238846 -4.2412992 -4.2565351 -4.26737 -4.2789392][-4.2101579 -4.1730852 -4.1284246 -4.08626 -4.0391269 -3.9795008 -3.9270923 -3.9425631 -4.0230289 -4.1038551 -4.1634994 -4.2062325 -4.2386818 -4.2633429 -4.2820649][-4.2017107 -4.161953 -4.1110158 -4.0587354 -3.992023 -3.9052618 -3.8184745 -3.8061051 -3.8892131 -3.9944897 -4.0837607 -4.15161 -4.2021565 -4.2424378 -4.27348][-4.219738 -4.1944356 -4.160387 -4.11854 -4.0573273 -3.9758725 -3.8913608 -3.8619356 -3.9148879 -3.998754 -4.0778003 -4.139678 -4.1870327 -4.2260213 -4.2588906][-4.2437978 -4.23448 -4.2231717 -4.204083 -4.1673517 -4.116189 -4.0586238 -4.0274224 -4.0392113 -4.07487 -4.1160808 -4.1513996 -4.1825838 -4.2122765 -4.239182][-4.2468615 -4.2473464 -4.2485003 -4.2457209 -4.2294579 -4.2059817 -4.1763434 -4.1533689 -4.1434493 -4.1398892 -4.1473231 -4.1587858 -4.1719017 -4.1922865 -4.2148743][-4.2426896 -4.2419658 -4.2462039 -4.2502437 -4.2456365 -4.2436204 -4.2383275 -4.2308407 -4.2173624 -4.1932006 -4.1791744 -4.1746788 -4.1764288 -4.1888876 -4.2088146][-4.2428622 -4.2337546 -4.2320409 -4.2334847 -4.2345834 -4.2470827 -4.2608237 -4.2682152 -4.2595177 -4.2315106 -4.21093 -4.2035527 -4.2026329 -4.2123284 -4.2293062][-4.2512965 -4.2359676 -4.2274747 -4.2252331 -4.2283878 -4.2465649 -4.266849 -4.2812772 -4.2812657 -4.2624397 -4.2465782 -4.2416377 -4.2418714 -4.24792 -4.2589569][-4.2651362 -4.2485442 -4.2392583 -4.2369833 -4.2398877 -4.2535019 -4.2686458 -4.2802882 -4.2830343 -4.274601 -4.2671695 -4.2644429 -4.2657838 -4.2712989 -4.2801652]]...]
INFO - root - 2017-12-05 11:52:34.379928: step 5410, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 78h:41m:26s remains)
INFO - root - 2017-12-05 11:52:42.964920: step 5420, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 76h:52m:47s remains)
INFO - root - 2017-12-05 11:52:51.484004: step 5430, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 76h:57m:06s remains)
INFO - root - 2017-12-05 11:52:59.995494: step 5440, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 78h:48m:44s remains)
INFO - root - 2017-12-05 11:53:08.419797: step 5450, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 74h:40m:40s remains)
INFO - root - 2017-12-05 11:53:16.905964: step 5460, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 77h:11m:35s remains)
INFO - root - 2017-12-05 11:53:25.425569: step 5470, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 77h:07m:39s remains)
INFO - root - 2017-12-05 11:53:34.063775: step 5480, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 77h:40m:12s remains)
INFO - root - 2017-12-05 11:53:42.584486: step 5490, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 80h:04m:35s remains)
INFO - root - 2017-12-05 11:53:50.998535: step 5500, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:31m:49s remains)
2017-12-05 11:53:51.733738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1532536 -4.0967712 -4.0634713 -4.0924125 -4.1512175 -4.1906748 -4.2008634 -4.182827 -4.1700673 -4.1818352 -4.1797853 -4.1514878 -4.1117253 -4.0741825 -4.0755515][-4.1431365 -4.0814342 -4.0524278 -4.0880547 -4.1486859 -4.1831307 -4.1905942 -4.1716838 -4.1575818 -4.1639652 -4.1641455 -4.1496997 -4.1272607 -4.1002278 -4.1005807][-4.1363873 -4.0747919 -4.0442047 -4.074861 -4.1302032 -4.1619039 -4.1689324 -4.1495862 -4.1412177 -4.1498451 -4.156354 -4.1541829 -4.143117 -4.1198888 -4.1157737][-4.12197 -4.0581808 -4.0259628 -4.0576663 -4.1133366 -4.1460042 -4.14859 -4.1265693 -4.1264582 -4.1488938 -4.1644936 -4.1675158 -4.1560674 -4.1321959 -4.1272755][-4.1063433 -4.042614 -4.0094209 -4.0462284 -4.1028147 -4.1319561 -4.1248903 -4.0980353 -4.1001353 -4.1346054 -4.1655035 -4.1787291 -4.1679749 -4.1438284 -4.1365185][-4.1068082 -4.0479021 -4.0140347 -4.0401468 -4.0840812 -4.1046038 -4.0842767 -4.0465212 -4.0455437 -4.0937152 -4.146616 -4.1782546 -4.1793652 -4.1630116 -4.1560397][-4.1197648 -4.0682645 -4.032198 -4.0360961 -4.05739 -4.0735 -4.0486975 -3.9953763 -3.9836738 -4.03476 -4.1074491 -4.1614609 -4.1820006 -4.1790419 -4.17691][-4.1382413 -4.0951281 -4.05962 -4.0426693 -4.0415673 -4.0520377 -4.0267773 -3.9713845 -3.9512498 -3.998014 -4.0778456 -4.1394987 -4.1660447 -4.1697431 -4.1751637][-4.1707292 -4.1392965 -4.1100612 -4.0832429 -4.0637374 -4.057899 -4.0327778 -3.9863455 -3.9644113 -4.0035152 -4.075913 -4.1277947 -4.1465306 -4.1450405 -4.1512814][-4.2004876 -4.175024 -4.1550169 -4.1318164 -4.1096849 -4.0944862 -4.0754194 -4.04232 -4.0179272 -4.0411997 -4.1006265 -4.143261 -4.1515021 -4.1370897 -4.1311703][-4.2254515 -4.2051058 -4.1895075 -4.1720147 -4.156127 -4.142426 -4.1309114 -4.1124792 -4.0913858 -4.1010437 -4.1454754 -4.1803546 -4.1831012 -4.1560884 -4.1355348][-4.2512941 -4.2374983 -4.2282143 -4.2173591 -4.2095108 -4.2013927 -4.19456 -4.18722 -4.1752796 -4.1791635 -4.20702 -4.2264953 -4.2197514 -4.1850691 -4.1571951][-4.2729068 -4.26637 -4.26534 -4.2622142 -4.2580662 -4.2525535 -4.247776 -4.2462406 -4.242981 -4.2464666 -4.2593236 -4.2637391 -4.248096 -4.2147474 -4.1885386][-4.2758193 -4.2751508 -4.2805333 -4.2828126 -4.2804003 -4.2756438 -4.2703671 -4.2718983 -4.2775464 -4.2830124 -4.2879691 -4.284018 -4.268126 -4.2436523 -4.2227941][-4.2647905 -4.2667584 -4.2732186 -4.2789063 -4.2809563 -4.2795353 -4.2777758 -4.2825866 -4.2939734 -4.3009953 -4.3027034 -4.2990294 -4.2883177 -4.2737918 -4.259645]]...]
INFO - root - 2017-12-05 11:54:00.392973: step 5510, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.884 sec/batch; 80h:18m:24s remains)
INFO - root - 2017-12-05 11:54:08.960711: step 5520, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 80h:10m:44s remains)
INFO - root - 2017-12-05 11:54:17.583262: step 5530, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 77h:32m:06s remains)
INFO - root - 2017-12-05 11:54:26.188227: step 5540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 78h:27m:52s remains)
INFO - root - 2017-12-05 11:54:34.744980: step 5550, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 81h:17m:45s remains)
INFO - root - 2017-12-05 11:54:43.283561: step 5560, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 76h:26m:32s remains)
INFO - root - 2017-12-05 11:54:51.853716: step 5570, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.883 sec/batch; 80h:11m:10s remains)
INFO - root - 2017-12-05 11:55:00.408970: step 5580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:41m:30s remains)
INFO - root - 2017-12-05 11:55:08.903703: step 5590, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 76h:18m:16s remains)
INFO - root - 2017-12-05 11:55:17.402679: step 5600, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 75h:28m:50s remains)
2017-12-05 11:55:18.237985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2240725 -4.2172985 -4.1955004 -4.1693387 -4.1375122 -4.094789 -4.0497522 -4.0273161 -4.0465651 -4.0760093 -4.10378 -4.1320348 -4.1481175 -4.1476631 -4.1492138][-4.2203569 -4.2159166 -4.198082 -4.17248 -4.138751 -4.099473 -4.06325 -4.0411177 -4.0448279 -4.0623894 -4.096416 -4.1415076 -4.1717939 -4.1787395 -4.1734953][-4.2134337 -4.2102556 -4.1947341 -4.1693439 -4.1363449 -4.1057372 -4.0817504 -4.0606437 -4.0487609 -4.0550547 -4.0942526 -4.1470041 -4.1817307 -4.1890893 -4.1766124][-4.2127147 -4.2098265 -4.1940489 -4.1671271 -4.1387768 -4.1210794 -4.1111169 -4.093277 -4.0703721 -4.0693831 -4.109426 -4.1564188 -4.1852884 -4.1907787 -4.1714792][-4.2199006 -4.2147756 -4.1976147 -4.1728897 -4.1533384 -4.1474886 -4.1460137 -4.130362 -4.1029816 -4.0953717 -4.1249108 -4.1565652 -4.1751666 -4.174468 -4.1546474][-4.2290769 -4.2207193 -4.2022142 -4.1820498 -4.1717572 -4.171258 -4.1696005 -4.1517992 -4.1210189 -4.1054935 -4.1209674 -4.1415777 -4.1527328 -4.1501718 -4.1367588][-4.2376204 -4.2272739 -4.2086272 -4.1921644 -4.183794 -4.1802235 -4.1759992 -4.1595359 -4.1273112 -4.103148 -4.1083875 -4.1250796 -4.1306758 -4.1296725 -4.122992][-4.2408557 -4.23075 -4.2147164 -4.2000284 -4.1887612 -4.1790724 -4.1702294 -4.1567554 -4.1307936 -4.1070628 -4.1104727 -4.127717 -4.1349077 -4.1374383 -4.1347933][-4.2401824 -4.2308536 -4.2158422 -4.1997623 -4.1847868 -4.1707067 -4.1592889 -4.1510596 -4.1387119 -4.1286268 -4.1347671 -4.1490378 -4.1576052 -4.1616511 -4.1588697][-4.2382154 -4.2282157 -4.2111096 -4.1931944 -4.1773777 -4.1624789 -4.1522827 -4.1526833 -4.1554294 -4.1604381 -4.1706285 -4.1813717 -4.1830735 -4.1776171 -4.1696911][-4.2357059 -4.2237482 -4.2059331 -4.1889596 -4.1739984 -4.1600742 -4.1542482 -4.1619744 -4.1746783 -4.1893926 -4.2016897 -4.2071867 -4.1971617 -4.1785426 -4.1625342][-4.235486 -4.2189384 -4.1994433 -4.1833587 -4.1691723 -4.1577864 -4.153872 -4.1628704 -4.1780152 -4.190484 -4.1996746 -4.20181 -4.1861472 -4.1601844 -4.1406422][-4.2380877 -4.2186646 -4.1962543 -4.177784 -4.1637435 -4.1542068 -4.14992 -4.157177 -4.1699657 -4.1780186 -4.1828003 -4.1844 -4.1732178 -4.1505923 -4.1334209][-4.2478104 -4.2315807 -4.2096734 -4.1897554 -4.1762881 -4.1652517 -4.1591578 -4.163106 -4.171979 -4.1742907 -4.1758447 -4.1778574 -4.1755481 -4.1627893 -4.1532722][-4.2664189 -4.2591915 -4.2419796 -4.22115 -4.2048459 -4.1908035 -4.1819243 -4.1814961 -4.1853256 -4.1835642 -4.1839247 -4.1868668 -4.1895123 -4.1860542 -4.1861954]]...]
INFO - root - 2017-12-05 11:55:26.533116: step 5610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 77h:07m:30s remains)
INFO - root - 2017-12-05 11:55:35.030275: step 5620, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:15m:31s remains)
INFO - root - 2017-12-05 11:55:43.578827: step 5630, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 80h:10m:22s remains)
INFO - root - 2017-12-05 11:55:52.126110: step 5640, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 74h:29m:19s remains)
INFO - root - 2017-12-05 11:56:00.631797: step 5650, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 76h:21m:20s remains)
INFO - root - 2017-12-05 11:56:09.254045: step 5660, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 77h:20m:32s remains)
INFO - root - 2017-12-05 11:56:17.812233: step 5670, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 76h:50m:02s remains)
INFO - root - 2017-12-05 11:56:26.418792: step 5680, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 77h:14m:19s remains)
INFO - root - 2017-12-05 11:56:34.954180: step 5690, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 78h:49m:08s remains)
INFO - root - 2017-12-05 11:56:43.688957: step 5700, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 77h:31m:13s remains)
2017-12-05 11:56:44.486222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.267837 -4.2675509 -4.2666459 -4.2683325 -4.274827 -4.285099 -4.2943506 -4.298028 -4.2960711 -4.2917943 -4.28934 -4.2904897 -4.2944756 -4.2994795 -4.3025279][-4.2326736 -4.2310729 -4.2263484 -4.2255607 -4.2328463 -4.2472053 -4.2630053 -4.2730575 -4.2753367 -4.2740169 -4.2730751 -4.2751384 -4.2809281 -4.2871723 -4.289371][-4.2045956 -4.1998043 -4.1887054 -4.1818171 -4.1861916 -4.2022161 -4.2244034 -4.2413993 -4.2478833 -4.2489443 -4.2466888 -4.2463098 -4.2524991 -4.2603874 -4.2617483][-4.1829958 -4.1736608 -4.1562796 -4.14301 -4.142158 -4.1556172 -4.181829 -4.2078085 -4.2218671 -4.2252045 -4.2196426 -4.2148724 -4.218749 -4.2254624 -4.2255163][-4.1635323 -4.1487579 -4.1299772 -4.1126 -4.1039963 -4.1071196 -4.1301908 -4.165206 -4.19023 -4.1998534 -4.1946621 -4.1886215 -4.1889644 -4.1895866 -4.1846933][-4.151855 -4.1348648 -4.1166072 -4.0943546 -4.0745754 -4.0612025 -4.0732517 -4.1136341 -4.1517224 -4.1745367 -4.178319 -4.17587 -4.1698813 -4.1565447 -4.140357][-4.157444 -4.1389556 -4.1165013 -4.0845604 -4.0466857 -4.011189 -4.0043678 -4.0420351 -4.0959148 -4.1423612 -4.165473 -4.1724539 -4.1641269 -4.1410542 -4.1100841][-4.1959519 -4.1728668 -4.1396694 -4.0916624 -4.0265808 -3.9656463 -3.9311295 -3.9499395 -4.0117159 -4.0893326 -4.143477 -4.1700292 -4.1666546 -4.1371932 -4.0893655][-4.2504263 -4.2214293 -4.177743 -4.1152744 -4.0271158 -3.9354603 -3.8598742 -3.8481762 -3.9192932 -4.032218 -4.1181703 -4.1645312 -4.1682038 -4.1379848 -4.0730453][-4.2881269 -4.2594953 -4.2108803 -4.1419539 -4.0424967 -3.9240763 -3.81633 -3.7908592 -3.8778629 -4.0113606 -4.1100864 -4.1621623 -4.1709504 -4.1407552 -4.0628762][-4.2997832 -4.2785382 -4.2330337 -4.1659493 -4.0685191 -3.9493968 -3.8482461 -3.8404589 -3.9329123 -4.0527754 -4.1372023 -4.1749315 -4.1809716 -4.1551857 -4.075407][-4.2948484 -4.2867308 -4.2530289 -4.1993237 -4.1234303 -4.0330324 -3.9671726 -3.9812188 -4.0575747 -4.1413331 -4.1959214 -4.2094445 -4.2019606 -4.1758814 -4.1065841][-4.2810612 -4.2907372 -4.2784114 -4.246459 -4.2017107 -4.1512227 -4.1209207 -4.1392894 -4.1855764 -4.232481 -4.25616 -4.2443151 -4.2191706 -4.1900549 -4.1410432][-4.2634468 -4.2914538 -4.300334 -4.29004 -4.2694688 -4.2468042 -4.2343721 -4.24265 -4.262958 -4.2841411 -4.2844906 -4.25886 -4.2272644 -4.2033052 -4.18052][-4.2367349 -4.2765679 -4.2999516 -4.3061967 -4.3001609 -4.2898068 -4.2826214 -4.2813253 -4.2855787 -4.2916555 -4.2836652 -4.2601337 -4.2371778 -4.2260008 -4.2229476]]...]
INFO - root - 2017-12-05 11:56:52.951288: step 5710, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 77h:32m:46s remains)
INFO - root - 2017-12-05 11:57:01.486046: step 5720, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 76h:17m:06s remains)
INFO - root - 2017-12-05 11:57:09.898035: step 5730, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 76h:35m:38s remains)
INFO - root - 2017-12-05 11:57:18.507102: step 5740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 78h:11m:16s remains)
INFO - root - 2017-12-05 11:57:27.066749: step 5750, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 78h:58m:10s remains)
INFO - root - 2017-12-05 11:57:35.606208: step 5760, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.851 sec/batch; 77h:11m:45s remains)
INFO - root - 2017-12-05 11:57:44.137946: step 5770, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 75h:55m:56s remains)
INFO - root - 2017-12-05 11:57:52.651897: step 5780, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 77h:22m:24s remains)
INFO - root - 2017-12-05 11:58:01.307481: step 5790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 77h:37m:43s remains)
INFO - root - 2017-12-05 11:58:09.922383: step 5800, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 76h:39m:07s remains)
2017-12-05 11:58:10.699488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1675696 -4.1483793 -4.1349735 -4.1422405 -4.1635256 -4.1898022 -4.2116852 -4.2255578 -4.2252145 -4.2084508 -4.1722574 -4.1463146 -4.1360512 -4.1246939 -4.1207533][-4.1508312 -4.1530027 -4.15292 -4.1619587 -4.1797619 -4.2016659 -4.2169995 -4.219655 -4.2131515 -4.2020087 -4.1806045 -4.1621461 -4.1538219 -4.1480093 -4.1534209][-4.1464176 -4.1658373 -4.1773353 -4.1861691 -4.1915512 -4.1980157 -4.2062035 -4.2005305 -4.1922936 -4.192028 -4.18937 -4.1828833 -4.1800203 -4.1786442 -4.1832514][-4.1283607 -4.1537619 -4.1702452 -4.1743088 -4.1639767 -4.1527829 -4.1587071 -4.1579862 -4.1575007 -4.1699815 -4.1829467 -4.1857042 -4.185348 -4.1834617 -4.1805468][-4.0882044 -4.1063876 -4.1232991 -4.1171789 -4.0839067 -4.0614133 -4.0727868 -4.0886288 -4.1036177 -4.1274195 -4.156106 -4.1683564 -4.1662946 -4.1548371 -4.1385884][-4.0306792 -4.0340295 -4.0407281 -4.0222569 -3.9698715 -3.9331503 -3.9528761 -3.99291 -4.0247054 -4.0605044 -4.1063614 -4.1342173 -4.1384983 -4.1216316 -4.0978827][-4.0032411 -3.9787333 -3.9582388 -3.9139049 -3.8293753 -3.7591205 -3.7690244 -3.8419604 -3.91452 -3.973027 -4.0429392 -4.0974169 -4.1244178 -4.1179566 -4.1017256][-4.0176263 -3.9764631 -3.9265924 -3.8570814 -3.7519789 -3.6446567 -3.6134596 -3.6964383 -3.8059092 -3.8866606 -3.972599 -4.0516276 -4.1038733 -4.1151881 -4.1165652][-4.0769892 -4.031086 -3.9733665 -3.9067562 -3.8225281 -3.7290812 -3.6756177 -3.7261088 -3.8187556 -3.8840516 -3.9528728 -4.0267282 -4.0865588 -4.1100245 -4.1263852][-4.1625509 -4.1225519 -4.0698156 -4.0148726 -3.9561172 -3.8938773 -3.8509164 -3.8684211 -3.9203372 -3.9582181 -3.9989462 -4.0460792 -4.0962038 -4.1228113 -4.14576][-4.229578 -4.1999884 -4.1594367 -4.1188517 -4.079143 -4.0425267 -4.0175056 -4.0208969 -4.0476165 -4.0728006 -4.09676 -4.1230192 -4.15992 -4.1811571 -4.1955881][-4.2757635 -4.2558494 -4.2309289 -4.2073588 -4.1850286 -4.1682453 -4.1554618 -4.1545334 -4.1671176 -4.1842337 -4.1977344 -4.2137041 -4.2357988 -4.2484 -4.2508173][-4.2968946 -4.2843709 -4.2720361 -4.2633448 -4.2553153 -4.252327 -4.25129 -4.2518344 -4.2573085 -4.2693334 -4.2785263 -4.2876482 -4.2971926 -4.300725 -4.2950587][-4.3043876 -4.298315 -4.293252 -4.2917666 -4.2904758 -4.292099 -4.2949762 -4.3001156 -4.3061972 -4.314075 -4.3220296 -4.3279428 -4.3316894 -4.329144 -4.3216205][-4.3057256 -4.3019209 -4.3004856 -4.3015585 -4.3031769 -4.3056736 -4.3104348 -4.3160219 -4.3214669 -4.3265648 -4.3324947 -4.336647 -4.3366637 -4.3323054 -4.3267856]]...]
INFO - root - 2017-12-05 11:58:19.250161: step 5810, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 78h:27m:40s remains)
INFO - root - 2017-12-05 11:58:27.804049: step 5820, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 78h:11m:02s remains)
INFO - root - 2017-12-05 11:58:36.264277: step 5830, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 75h:50m:10s remains)
INFO - root - 2017-12-05 11:58:44.860389: step 5840, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 81h:16m:07s remains)
INFO - root - 2017-12-05 11:58:53.336879: step 5850, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 76h:47m:03s remains)
INFO - root - 2017-12-05 11:59:02.039464: step 5860, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 78h:42m:42s remains)
INFO - root - 2017-12-05 11:59:10.495336: step 5870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 77h:28m:41s remains)
INFO - root - 2017-12-05 11:59:19.123999: step 5880, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 79h:04m:00s remains)
INFO - root - 2017-12-05 11:59:27.668239: step 5890, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 78h:07m:16s remains)
INFO - root - 2017-12-05 11:59:36.298204: step 5900, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 79h:52m:54s remains)
2017-12-05 11:59:37.012111: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2255359 -4.2283792 -4.2404861 -4.2617683 -4.2736912 -4.266088 -4.2495437 -4.2413592 -4.2471471 -4.2505636 -4.2526569 -4.2535219 -4.2429037 -4.2264304 -4.212501][-4.1840916 -4.1888452 -4.2037005 -4.2286711 -4.244 -4.2366576 -4.2164078 -4.2067919 -4.21774 -4.2237387 -4.2232423 -4.220695 -4.2094483 -4.1884084 -4.1685643][-4.1474552 -4.1513867 -4.1700525 -4.2002945 -4.2187929 -4.2117538 -4.1874957 -4.1752715 -4.1897745 -4.199317 -4.1993828 -4.1945019 -4.1817608 -4.1552591 -4.1298552][-4.1185174 -4.1207047 -4.14349 -4.1808743 -4.2034369 -4.1935682 -4.162292 -4.1447172 -4.1634378 -4.1762228 -4.176569 -4.1735263 -4.1607804 -4.1300716 -4.0949845][-4.1008849 -4.1054063 -4.131331 -4.1672668 -4.181519 -4.162354 -4.1203265 -4.0927596 -4.1199312 -4.14316 -4.147862 -4.1524792 -4.1485186 -4.1189809 -4.0776873][-4.0995917 -4.1052346 -4.1252027 -4.1501174 -4.1497164 -4.1171184 -4.0630903 -4.0292292 -4.0643225 -4.1035991 -4.1210003 -4.1380429 -4.147995 -4.126946 -4.0867558][-4.114068 -4.1147571 -4.1227841 -4.1338372 -4.1139827 -4.0548725 -3.9820623 -3.9509983 -3.9997547 -4.0610247 -4.10107 -4.1358037 -4.1582375 -4.1450324 -4.1111217][-4.1391854 -4.1283431 -4.1215763 -4.1143637 -4.0703697 -3.973217 -3.8812635 -3.8639612 -3.9304779 -4.012321 -4.0738664 -4.1307154 -4.1661782 -4.1588693 -4.1344838][-4.1550155 -4.1393075 -4.1299691 -4.1205192 -4.0740528 -3.9624882 -3.8693948 -3.8611808 -3.9244258 -3.9989996 -4.0634174 -4.1321874 -4.1704092 -4.1591473 -4.1343617][-4.1449256 -4.1359687 -4.1396694 -4.1444583 -4.1166496 -4.0271573 -3.9519644 -3.9379406 -3.9768252 -4.0261106 -4.07794 -4.1422911 -4.1726246 -4.149735 -4.113399][-4.1342349 -4.1333375 -4.1540117 -4.1743903 -4.1663971 -4.1107922 -4.0564919 -4.0318508 -4.04586 -4.0714459 -4.11073 -4.1623898 -4.1839175 -4.1529565 -4.1057224][-4.1309586 -4.133153 -4.1624541 -4.1936784 -4.2027955 -4.1803908 -4.1521091 -4.130826 -4.1305423 -4.1386385 -4.1620283 -4.1950393 -4.2047896 -4.1701856 -4.1175895][-4.1490068 -4.1552215 -4.1878877 -4.2198353 -4.2374697 -4.2375784 -4.2292423 -4.2194591 -4.2160945 -4.2171249 -4.2262363 -4.2385163 -4.2343273 -4.199347 -4.1508589][-4.1984744 -4.2118111 -4.2405 -4.263021 -4.2777853 -4.2830758 -4.2798705 -4.2793121 -4.2790532 -4.2806573 -4.2835536 -4.2863245 -4.2722917 -4.2376184 -4.1982532][-4.258543 -4.2733612 -4.295198 -4.3065023 -4.3130589 -4.31527 -4.311986 -4.3142409 -4.316699 -4.3205056 -4.3247557 -4.3219256 -4.3047876 -4.2774043 -4.251204]]...]
INFO - root - 2017-12-05 11:59:45.420291: step 5910, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 75h:46m:19s remains)
INFO - root - 2017-12-05 11:59:54.055096: step 5920, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 75h:02m:34s remains)
INFO - root - 2017-12-05 12:00:02.657302: step 5930, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 79h:58m:48s remains)
INFO - root - 2017-12-05 12:00:11.229052: step 5940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 79h:33m:47s remains)
INFO - root - 2017-12-05 12:00:19.808504: step 5950, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 78h:21m:34s remains)
INFO - root - 2017-12-05 12:00:28.380067: step 5960, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 78h:30m:33s remains)
INFO - root - 2017-12-05 12:00:37.019392: step 5970, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 77h:54m:28s remains)
INFO - root - 2017-12-05 12:00:45.632314: step 5980, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.829 sec/batch; 75h:09m:49s remains)
INFO - root - 2017-12-05 12:00:54.154749: step 5990, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 79h:04m:02s remains)
INFO - root - 2017-12-05 12:01:02.717125: step 6000, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 79h:48m:37s remains)
2017-12-05 12:01:03.466866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0792503 -4.1127467 -4.162055 -4.2034016 -4.2238984 -4.2326517 -4.2359023 -4.2415771 -4.238903 -4.227437 -4.2067552 -4.1701784 -4.1269937 -4.0931678 -4.0922141][-4.0548511 -4.0862441 -4.1367269 -4.1819687 -4.2070932 -4.2209625 -4.2283926 -4.2374244 -4.2372646 -4.2269907 -4.2064166 -4.1671147 -4.1184812 -4.0886936 -4.1009135][-4.0565991 -4.0844355 -4.125433 -4.1625624 -4.1844039 -4.1992717 -4.2112441 -4.2244387 -4.22634 -4.2190948 -4.2045045 -4.1717114 -4.1298923 -4.1071987 -4.1254063][-4.0760422 -4.0985332 -4.1219831 -4.140604 -4.15286 -4.1671724 -4.1815257 -4.1971536 -4.2023377 -4.2029829 -4.1974397 -4.1763091 -4.1488752 -4.1349092 -4.1580849][-4.0973997 -4.1142592 -4.1231046 -4.1242285 -4.1228585 -4.134726 -4.1521292 -4.1668911 -4.1742797 -4.1804919 -4.1897225 -4.1845503 -4.1682959 -4.1600161 -4.1825495][-4.1168289 -4.1271586 -4.1263971 -4.1194539 -4.1059551 -4.108973 -4.1252513 -4.1334424 -4.1318617 -4.13793 -4.1685257 -4.1878343 -4.1878953 -4.1867132 -4.2071023][-4.1219964 -4.1305265 -4.1277814 -4.1223693 -4.1066809 -4.1005297 -4.1059465 -4.0986915 -4.0765867 -4.0695152 -4.1147189 -4.1624074 -4.1905794 -4.2100487 -4.2381654][-4.1051865 -4.1223869 -4.1302762 -4.1345272 -4.1250043 -4.11343 -4.1011181 -4.0635953 -4.0109005 -3.9891331 -4.0457015 -4.1234188 -4.1841874 -4.2305446 -4.2676754][-4.0916758 -4.1203914 -4.14595 -4.1615939 -4.1599383 -4.143343 -4.1087589 -4.0390792 -3.9589097 -3.9253292 -3.9883056 -4.086555 -4.1727948 -4.240006 -4.283649][-4.0852737 -4.1321282 -4.1756873 -4.202909 -4.2106862 -4.1879563 -4.1309729 -4.0372295 -3.9400759 -3.9014516 -3.9641883 -4.0694337 -4.1641779 -4.2364683 -4.2793307][-4.0693116 -4.1376691 -4.1931753 -4.227983 -4.2453437 -4.2212114 -4.1546693 -4.0579743 -3.965133 -3.9297023 -3.9871554 -4.0831132 -4.1723356 -4.2358551 -4.2655911][-4.0436225 -4.1266694 -4.1815181 -4.2190318 -4.2431254 -4.2195735 -4.1571064 -4.0786471 -4.0096993 -3.9859025 -4.0312843 -4.1090813 -4.1852226 -4.2352118 -4.2500739][-4.0090971 -4.0869761 -4.1337471 -4.1734147 -4.20517 -4.1877708 -4.1405392 -4.0886064 -4.0495224 -4.0417466 -4.0778604 -4.1358852 -4.1949978 -4.2322211 -4.2366633][-3.9973423 -4.0578704 -4.0947661 -4.1338959 -4.166461 -4.1563396 -4.1268516 -4.1003518 -4.0867009 -4.0904474 -4.1190495 -4.1624231 -4.2048306 -4.23039 -4.2292023][-4.0266447 -4.0714211 -4.0982933 -4.1333671 -4.157763 -4.1486173 -4.1308947 -4.1234541 -4.1288571 -4.1421132 -4.1673064 -4.2010965 -4.2315078 -4.2478323 -4.2438936]]...]
INFO - root - 2017-12-05 12:01:11.926703: step 6010, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.905 sec/batch; 82h:02m:53s remains)
INFO - root - 2017-12-05 12:01:20.529385: step 6020, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 77h:57m:31s remains)
INFO - root - 2017-12-05 12:01:29.122704: step 6030, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 78h:36m:55s remains)
INFO - root - 2017-12-05 12:01:37.723664: step 6040, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 81h:03m:33s remains)
INFO - root - 2017-12-05 12:01:46.199454: step 6050, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 78h:05m:31s remains)
INFO - root - 2017-12-05 12:01:54.868936: step 6060, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 75h:26m:04s remains)
INFO - root - 2017-12-05 12:02:03.542165: step 6070, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 76h:20m:24s remains)
INFO - root - 2017-12-05 12:02:12.054617: step 6080, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 77h:20m:17s remains)
INFO - root - 2017-12-05 12:02:20.675214: step 6090, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 80h:03m:11s remains)
INFO - root - 2017-12-05 12:02:29.226559: step 6100, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 74h:55m:31s remains)
2017-12-05 12:02:30.019730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3156304 -4.3157253 -4.319088 -4.3190384 -4.3153849 -4.30934 -4.30236 -4.2971 -4.2955551 -4.2996187 -4.3056717 -4.3065748 -4.3058243 -4.3028841 -4.3012228][-4.3070955 -4.3035173 -4.3040309 -4.2984195 -4.2907319 -4.2816548 -4.2717953 -4.2678752 -4.2677217 -4.27272 -4.281064 -4.2845135 -4.2837353 -4.2792172 -4.2760653][-4.29991 -4.2926712 -4.289793 -4.2804775 -4.269733 -4.2582235 -4.2433882 -4.2377539 -4.2402196 -4.2472892 -4.2578821 -4.2644606 -4.2681026 -4.26237 -4.2553482][-4.2950945 -4.2853694 -4.2803631 -4.2674861 -4.253799 -4.239325 -4.2206235 -4.2133293 -4.2188497 -4.2288766 -4.2391939 -4.2471642 -4.2580724 -4.252749 -4.2423468][-4.2896447 -4.2777767 -4.2708306 -4.2565088 -4.241262 -4.2244744 -4.2036138 -4.197247 -4.2056184 -4.2185178 -4.2268949 -4.2326231 -4.2457037 -4.2371211 -4.2246618][-4.28469 -4.2703805 -4.261415 -4.2469821 -4.2325454 -4.2153 -4.1925354 -4.1882176 -4.1976271 -4.2129149 -4.2221322 -4.2249861 -4.2326922 -4.216383 -4.1992183][-4.28622 -4.2689872 -4.2565012 -4.2426124 -4.2308979 -4.2137594 -4.1906114 -4.1842761 -4.1901436 -4.2075806 -4.2242956 -4.2290621 -4.228107 -4.2031507 -4.1802554][-4.29057 -4.2715688 -4.2576332 -4.2438564 -4.232542 -4.2140727 -4.1909356 -4.1843634 -4.1890087 -4.2101054 -4.2364111 -4.2471094 -4.2397404 -4.2115335 -4.188797][-4.2962956 -4.2815533 -4.2722054 -4.2603993 -4.2440271 -4.2188091 -4.1904969 -4.1804457 -4.1832509 -4.2035804 -4.2369781 -4.2556553 -4.2511859 -4.2314248 -4.2158704][-4.3027453 -4.2921453 -4.2887273 -4.2802291 -4.2612648 -4.2301021 -4.1953483 -4.179831 -4.1809249 -4.2012024 -4.2362418 -4.2580543 -4.2558088 -4.2471352 -4.2402782][-4.3078594 -4.2995572 -4.298779 -4.2927165 -4.2749777 -4.2435551 -4.2051435 -4.1852279 -4.1871772 -4.208518 -4.2382426 -4.2563996 -4.2572546 -4.2568021 -4.2567906][-4.3125315 -4.3051262 -4.3031197 -4.2956524 -4.2787066 -4.25036 -4.2181878 -4.2014809 -4.2039165 -4.2226667 -4.2443309 -4.2565408 -4.25822 -4.260788 -4.2633533][-4.3153052 -4.3088593 -4.3049603 -4.2952113 -4.2809072 -4.2606125 -4.2418375 -4.2357125 -4.2401986 -4.2547855 -4.2688613 -4.2747955 -4.2738872 -4.2741313 -4.2762241][-4.3166623 -4.3117275 -4.308805 -4.30136 -4.2920017 -4.2807326 -4.2726765 -4.2738323 -4.279016 -4.2896228 -4.2971697 -4.2975349 -4.2934828 -4.2916069 -4.2931409][-4.3183117 -4.3156261 -4.3157644 -4.3126025 -4.3064308 -4.2999296 -4.2966738 -4.3011169 -4.3061576 -4.31099 -4.3123426 -4.3102589 -4.3072186 -4.306035 -4.3067594]]...]
INFO - root - 2017-12-05 12:02:38.491910: step 6110, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 80h:15m:49s remains)
INFO - root - 2017-12-05 12:02:46.925942: step 6120, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.856 sec/batch; 77h:34m:15s remains)
INFO - root - 2017-12-05 12:02:55.463626: step 6130, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 78h:29m:45s remains)
INFO - root - 2017-12-05 12:03:04.008397: step 6140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 79h:39m:19s remains)
INFO - root - 2017-12-05 12:03:12.606343: step 6150, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.789 sec/batch; 71h:32m:53s remains)
INFO - root - 2017-12-05 12:03:21.137209: step 6160, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:23m:28s remains)
INFO - root - 2017-12-05 12:03:29.654845: step 6170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 78h:22m:59s remains)
INFO - root - 2017-12-05 12:03:38.186558: step 6180, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 78h:59m:14s remains)
INFO - root - 2017-12-05 12:03:46.690691: step 6190, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 78h:00m:48s remains)
INFO - root - 2017-12-05 12:03:55.207551: step 6200, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.847 sec/batch; 76h:44m:39s remains)
2017-12-05 12:03:55.949459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1348181 -4.1145759 -4.1060181 -4.1128068 -4.1294746 -4.1440344 -4.1566844 -4.1637669 -4.1531668 -4.1373448 -4.123857 -4.1166339 -4.1204357 -4.1273375 -4.1440916][-4.1189818 -4.0968652 -4.0813537 -4.0862861 -4.1084943 -4.129324 -4.1470647 -4.1602926 -4.1543756 -4.1435747 -4.1357741 -4.1302209 -4.1317081 -4.13261 -4.1396036][-4.0943041 -4.0747705 -4.0546641 -4.0583076 -4.0821538 -4.1083245 -4.1304464 -4.148036 -4.1447845 -4.1326132 -4.1219006 -4.112597 -4.11099 -4.1115594 -4.112144][-4.0786281 -4.062674 -4.0463748 -4.0492129 -4.0606308 -4.0741534 -4.0886807 -4.1056705 -4.111917 -4.1074476 -4.0937762 -4.0768356 -4.06711 -4.0675354 -4.0684142][-4.0770292 -4.0619507 -4.0483918 -4.0503569 -4.0490079 -4.0394282 -4.0348153 -4.0477462 -4.0717025 -4.0884876 -4.0864954 -4.0683947 -4.0463972 -4.0374169 -4.036097][-4.0680366 -4.0580196 -4.0529747 -4.0598927 -4.0510588 -4.0190387 -3.99155 -3.9995351 -4.0401626 -4.07726 -4.0969381 -4.0852065 -4.0576572 -4.0420933 -4.0399275][-4.0677781 -4.0597458 -4.0643721 -4.0757718 -4.0624871 -4.0217147 -3.9786289 -3.9744439 -4.0155091 -4.0701947 -4.1103783 -4.1120062 -4.0929475 -4.0766139 -4.070755][-4.0888534 -4.0802145 -4.0835171 -4.0949588 -4.0827413 -4.0458913 -4.0000458 -3.9829183 -4.0074577 -4.0591564 -4.1091375 -4.1291604 -4.1331339 -4.1339664 -4.1293974][-4.1183772 -4.1123886 -4.1089439 -4.1157985 -4.1110096 -4.0880923 -4.0494595 -4.0246596 -4.02847 -4.0589094 -4.1016788 -4.1335168 -4.1590238 -4.1747556 -4.1735578][-4.1479135 -4.1518893 -4.1494651 -4.1540823 -4.1544685 -4.1400857 -4.1080904 -4.0762348 -4.0579267 -4.06388 -4.0909081 -4.1239576 -4.1582723 -4.1814032 -4.1864295][-4.1561003 -4.1734161 -4.1797094 -4.184411 -4.1815205 -4.1690164 -4.1433678 -4.11044 -4.0830936 -4.0694203 -4.0745583 -4.094502 -4.1271405 -4.1582885 -4.17713][-4.1483417 -4.1713176 -4.1894369 -4.1990066 -4.1964216 -4.1884704 -4.1708822 -4.14529 -4.1164913 -4.0935025 -4.083159 -4.08348 -4.1014638 -4.1315069 -4.1598234][-4.135428 -4.1585865 -4.1799817 -4.1934543 -4.1965084 -4.1943631 -4.1856537 -4.169786 -4.1466103 -4.1262331 -4.1166067 -4.11526 -4.1200342 -4.1357222 -4.1571417][-4.1184535 -4.1321464 -4.1457644 -4.1579309 -4.1669817 -4.1719384 -4.1709123 -4.1642933 -4.153192 -4.1446953 -4.147162 -4.1529584 -4.1536922 -4.1572995 -4.1667156][-4.1130643 -4.1172881 -4.1215658 -4.1270885 -4.1337538 -4.1398191 -4.1434641 -4.143188 -4.1402149 -4.1401625 -4.1485891 -4.1583295 -4.1577363 -4.157558 -4.1634555]]...]
INFO - root - 2017-12-05 12:04:04.452678: step 6210, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 76h:00m:13s remains)
INFO - root - 2017-12-05 12:04:13.030836: step 6220, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 78h:41m:45s remains)
INFO - root - 2017-12-05 12:04:21.590764: step 6230, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 80h:14m:49s remains)
INFO - root - 2017-12-05 12:04:30.142447: step 6240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 76h:44m:10s remains)
INFO - root - 2017-12-05 12:04:38.674025: step 6250, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:48m:58s remains)
INFO - root - 2017-12-05 12:04:47.198416: step 6260, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 78h:34m:27s remains)
INFO - root - 2017-12-05 12:04:55.695639: step 6270, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 78h:08m:51s remains)
INFO - root - 2017-12-05 12:05:04.309446: step 6280, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 79h:22m:43s remains)
INFO - root - 2017-12-05 12:05:12.869349: step 6290, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 76h:38m:16s remains)
INFO - root - 2017-12-05 12:05:21.431292: step 6300, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 76h:24m:29s remains)
2017-12-05 12:05:22.214561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2856531 -4.2460752 -4.1880031 -4.122335 -4.0723519 -4.0518279 -4.0737386 -4.1176991 -4.1414413 -4.15078 -4.1562443 -4.1544547 -4.1563797 -4.1680908 -4.18436][-4.2782516 -4.2367868 -4.178463 -4.1107383 -4.0535879 -4.0185194 -4.0350223 -4.0980988 -4.1431251 -4.1617937 -4.1667233 -4.1658034 -4.171524 -4.1890621 -4.2127008][-4.2747712 -4.2318344 -4.1745381 -4.1067572 -4.0430984 -3.9874003 -3.9890466 -4.0679684 -4.1368876 -4.1669631 -4.1725006 -4.1690412 -4.17611 -4.1994581 -4.2301841][-4.2760816 -4.2339687 -4.1801739 -4.115819 -4.0500274 -3.9785891 -3.9556563 -4.0365162 -4.1222544 -4.1620221 -4.16749 -4.1592412 -4.1645 -4.190824 -4.2261834][-4.2788277 -4.2390914 -4.1887054 -4.1291776 -4.0650525 -3.9864047 -3.9392 -4.011673 -4.1071677 -4.153995 -4.1622515 -4.1514673 -4.15301 -4.1757193 -4.2087059][-4.2820621 -4.245203 -4.1991796 -4.1477365 -4.0887704 -4.0090876 -3.9408267 -3.99265 -4.0897875 -4.1441317 -4.1610856 -4.1512642 -4.1492519 -4.1648893 -4.1862478][-4.2827764 -4.2490106 -4.2076545 -4.1662865 -4.1156626 -4.038085 -3.9556382 -3.9815519 -4.0707097 -4.1341281 -4.1618547 -4.1552935 -4.1498194 -4.1550727 -4.16062][-4.283721 -4.2535505 -4.2195249 -4.189929 -4.1498771 -4.0798349 -3.9939206 -3.9968002 -4.0674672 -4.1348248 -4.1743693 -4.1747441 -4.1660752 -4.1600838 -4.1490993][-4.2873435 -4.2586131 -4.2300711 -4.2100186 -4.1805649 -4.12316 -4.0482321 -4.0375934 -4.0859561 -4.1466718 -4.1974468 -4.2104182 -4.2040696 -4.1926408 -4.1701293][-4.2939796 -4.266655 -4.2424412 -4.2292466 -4.2093625 -4.1670713 -4.1093788 -4.0952759 -4.1228137 -4.1669788 -4.22194 -4.2465153 -4.2456436 -4.2321153 -4.205843][-4.3012733 -4.2775335 -4.2578135 -4.2489209 -4.2353935 -4.207221 -4.1630611 -4.1491489 -4.162159 -4.1919169 -4.2458267 -4.2790861 -4.2852035 -4.2732482 -4.2476439][-4.3101206 -4.288878 -4.2715635 -4.2643814 -4.2533293 -4.2357106 -4.2020235 -4.18693 -4.1901407 -4.2136159 -4.2677836 -4.3065782 -4.3192878 -4.3116584 -4.2912946][-4.3235826 -4.3044739 -4.2873468 -4.2794566 -4.2694163 -4.2567129 -4.2322345 -4.217135 -4.2161565 -4.2398291 -4.2910771 -4.3280263 -4.3431096 -4.3403921 -4.3263173][-4.3354979 -4.3175297 -4.2995429 -4.2886367 -4.2764559 -4.2627983 -4.2461295 -4.234868 -4.2333331 -4.256906 -4.3023767 -4.3357868 -4.3520603 -4.3521028 -4.34229][-4.3384418 -4.3230195 -4.3051004 -4.290978 -4.2776318 -4.2662272 -4.2557545 -4.2502804 -4.2495313 -4.2690172 -4.3042574 -4.3317933 -4.3463058 -4.3461204 -4.3394837]]...]
INFO - root - 2017-12-05 12:05:30.707874: step 6310, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 80h:05m:08s remains)
INFO - root - 2017-12-05 12:05:39.233956: step 6320, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 75h:50m:13s remains)
INFO - root - 2017-12-05 12:05:47.938138: step 6330, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 77h:28m:20s remains)
INFO - root - 2017-12-05 12:05:56.400176: step 6340, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.847 sec/batch; 76h:41m:42s remains)
INFO - root - 2017-12-05 12:06:04.957236: step 6350, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 78h:28m:07s remains)
INFO - root - 2017-12-05 12:06:13.583337: step 6360, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 75h:25m:22s remains)
INFO - root - 2017-12-05 12:06:22.038074: step 6370, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:16m:44s remains)
INFO - root - 2017-12-05 12:06:30.525906: step 6380, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 78h:41m:09s remains)
INFO - root - 2017-12-05 12:06:39.073055: step 6390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 78h:05m:57s remains)
INFO - root - 2017-12-05 12:06:47.612308: step 6400, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 74h:42m:48s remains)
2017-12-05 12:06:48.355136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.218811 -4.2365465 -4.2404361 -4.2370324 -4.2251158 -4.2123194 -4.1955066 -4.178443 -4.1756697 -4.1810951 -4.1877975 -4.1990781 -4.1988811 -4.1819525 -4.1593957][-4.2270236 -4.243434 -4.2442718 -4.2361522 -4.2220659 -4.2062035 -4.1889496 -4.1759133 -4.1785235 -4.1849566 -4.190331 -4.2016916 -4.2008662 -4.1853671 -4.1633911][-4.2252617 -4.2375216 -4.2359138 -4.2234597 -4.2059727 -4.1819358 -4.1578746 -4.1485457 -4.1604605 -4.1765981 -4.184866 -4.1934681 -4.1933217 -4.1803908 -4.1621079][-4.2177606 -4.2286897 -4.2277684 -4.2135053 -4.1898928 -4.1547313 -4.1183472 -4.1097031 -4.1314754 -4.1587996 -4.1706057 -4.1759677 -4.1738887 -4.16454 -4.1547818][-4.2125115 -4.2185969 -4.2160091 -4.1999812 -4.16926 -4.1235485 -4.0808425 -4.077045 -4.1087337 -4.1443024 -4.1556959 -4.1515493 -4.1431818 -4.134594 -4.1337824][-4.2063103 -4.2121282 -4.2120171 -4.19525 -4.1580849 -4.1060014 -4.0631185 -4.0636864 -4.098918 -4.133297 -4.1394825 -4.1263161 -4.113687 -4.1056037 -4.1117868][-4.1960258 -4.2039065 -4.2100573 -4.1987276 -4.1613493 -4.1074195 -4.0669661 -4.067564 -4.09814 -4.1256366 -4.1258259 -4.11349 -4.1059346 -4.1080723 -4.1239181][-4.1922245 -4.1937661 -4.1999125 -4.1942921 -4.1585436 -4.1027803 -4.0647907 -4.0650773 -4.0920897 -4.1141386 -4.1164141 -4.1170077 -4.1257114 -4.1427164 -4.1659727][-4.1961503 -4.1883903 -4.1847653 -4.1773429 -4.1457534 -4.0978055 -4.0638151 -4.0596461 -4.0788679 -4.0982018 -4.108283 -4.1205721 -4.1461639 -4.1812625 -4.213695][-4.2097883 -4.1951804 -4.1795993 -4.1669464 -4.1445785 -4.1130571 -4.0870647 -4.0778861 -4.0865383 -4.1025653 -4.1196995 -4.1402926 -4.1725583 -4.2102861 -4.2432976][-4.2217689 -4.201582 -4.1792579 -4.1662297 -4.1527762 -4.1373506 -4.1215558 -4.1092305 -4.1081281 -4.1192608 -4.1399441 -4.1641688 -4.19371 -4.2190027 -4.2358146][-4.2302394 -4.2114534 -4.1882434 -4.1750493 -4.1673565 -4.1603284 -4.1515784 -4.1386123 -4.1341705 -4.1442032 -4.1650572 -4.1874828 -4.2087264 -4.2182279 -4.21554][-4.2343292 -4.2250795 -4.2064347 -4.1926069 -4.1863203 -4.1850262 -4.1831107 -4.1755829 -4.1774635 -4.1901207 -4.2085023 -4.2172675 -4.22041 -4.2128363 -4.1937933][-4.237534 -4.2320142 -4.2165785 -4.2029915 -4.1941733 -4.1946473 -4.1980848 -4.1966662 -4.205194 -4.219954 -4.2317576 -4.2252245 -4.2152228 -4.2010384 -4.1780739][-4.2466755 -4.2430439 -4.2301874 -4.2162704 -4.2046189 -4.2009277 -4.20327 -4.2028222 -4.2097797 -4.22189 -4.2280869 -4.2177453 -4.204896 -4.1904845 -4.1693549]]...]
INFO - root - 2017-12-05 12:06:56.783624: step 6410, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 78h:59m:35s remains)
INFO - root - 2017-12-05 12:07:05.448286: step 6420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 77h:26m:11s remains)
INFO - root - 2017-12-05 12:07:14.044517: step 6430, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 75h:39m:04s remains)
INFO - root - 2017-12-05 12:07:22.625108: step 6440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 76h:07m:43s remains)
INFO - root - 2017-12-05 12:07:31.248891: step 6450, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 76h:03m:48s remains)
INFO - root - 2017-12-05 12:07:39.791178: step 6460, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 81h:03m:24s remains)
INFO - root - 2017-12-05 12:07:48.248016: step 6470, loss = 2.08, batch loss = 2.03 (10.3 examples/sec; 0.779 sec/batch; 70h:33m:16s remains)
INFO - root - 2017-12-05 12:07:56.693778: step 6480, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.820 sec/batch; 74h:18m:16s remains)
INFO - root - 2017-12-05 12:08:05.274465: step 6490, loss = 2.08, batch loss = 2.02 (8.5 examples/sec; 0.945 sec/batch; 85h:33m:58s remains)
INFO - root - 2017-12-05 12:08:13.777912: step 6500, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 78h:25m:19s remains)
2017-12-05 12:08:14.542792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765713 -4.2456884 -4.2036996 -4.1598492 -4.12671 -4.1064615 -4.1140656 -4.1458387 -4.1794491 -4.2107387 -4.2217007 -4.2145667 -4.1963181 -4.1826921 -4.1705885][-4.2624021 -4.2241106 -4.17043 -4.1174278 -4.0766058 -4.0533686 -4.066051 -4.1058831 -4.1463795 -4.1834803 -4.1961651 -4.1858273 -4.1640306 -4.1481242 -4.1353178][-4.2493458 -4.2047911 -4.1403923 -4.078371 -4.0288739 -4.0050306 -4.0263677 -4.0762811 -4.1245217 -4.1653118 -4.1782331 -4.1636348 -4.1394072 -4.1223059 -4.1066418][-4.2367797 -4.189971 -4.1225586 -4.0582094 -4.0054784 -3.984241 -4.01356 -4.0686851 -4.12204 -4.1617527 -4.1700921 -4.1528234 -4.1279039 -4.1073837 -4.0885568][-4.22383 -4.1736574 -4.102253 -4.0367951 -3.9859631 -3.967345 -3.9967456 -4.0498705 -4.1019087 -4.1365776 -4.14337 -4.1316032 -4.1127963 -4.0897031 -4.0667057][-4.21482 -4.1559463 -4.0741186 -4.003016 -3.953053 -3.9357622 -3.9559894 -3.9989855 -4.0460682 -4.0789351 -4.0930634 -4.0971589 -4.0921116 -4.0733075 -4.0457835][-4.212018 -4.1429815 -4.0490704 -3.9665868 -3.9095755 -3.8881197 -3.8974307 -3.9344349 -3.9831924 -4.0189791 -4.040544 -4.0585246 -4.0670862 -4.0535822 -4.026094][-4.2104683 -4.1342788 -4.0325179 -3.9395039 -3.8727152 -3.8411028 -3.8442779 -3.8800488 -3.931334 -3.9691982 -3.9916372 -4.0183444 -4.0388122 -4.0312996 -4.0064626][-4.2061963 -4.1266956 -4.0227103 -3.9284837 -3.8599119 -3.8208094 -3.8215897 -3.8582428 -3.9104223 -3.9474876 -3.9694636 -3.9994252 -4.0284839 -4.0271769 -4.0039825][-4.2047219 -4.1281557 -4.031034 -3.9486318 -3.8878162 -3.8492606 -3.8448622 -3.8800063 -3.9311209 -3.9671924 -3.9912825 -4.0222216 -4.0543785 -4.0542817 -4.0286865][-4.2082419 -4.1385636 -4.0541224 -3.987808 -3.937794 -3.9052382 -3.9015002 -3.9376378 -3.9834692 -4.0146656 -4.039021 -4.0669346 -4.0942736 -4.0934739 -4.0694327][-4.216795 -4.1568608 -4.0888186 -4.0362272 -3.9945087 -3.9671519 -3.9662724 -4.0022469 -4.0388718 -4.0618954 -4.0834146 -4.1071358 -4.127563 -4.125885 -4.1079974][-4.2306151 -4.1836944 -4.1323652 -4.0920911 -4.0608678 -4.0420437 -4.0465403 -4.0796971 -4.1064939 -4.1185288 -4.1319075 -4.1475148 -4.1598716 -4.1575546 -4.1466589][-4.2552028 -4.2231207 -4.1874571 -4.1589184 -4.136735 -4.1260071 -4.133873 -4.1626148 -4.1819286 -4.1869559 -4.1907444 -4.19728 -4.2025485 -4.2004151 -4.1934314][-4.2832012 -4.2641163 -4.2395191 -4.2184634 -4.2007027 -4.1933994 -4.2011352 -4.223526 -4.239018 -4.2433076 -4.2443604 -4.2462425 -4.2476339 -4.24685 -4.2438226]]...]
INFO - root - 2017-12-05 12:08:22.910360: step 6510, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 75h:58m:34s remains)
INFO - root - 2017-12-05 12:08:31.457093: step 6520, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 77h:00m:41s remains)
INFO - root - 2017-12-05 12:08:40.033003: step 6530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:06m:49s remains)
INFO - root - 2017-12-05 12:08:48.442092: step 6540, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.825 sec/batch; 74h:43m:53s remains)
INFO - root - 2017-12-05 12:08:56.954261: step 6550, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 77h:06m:38s remains)
INFO - root - 2017-12-05 12:09:05.509390: step 6560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 77h:38m:41s remains)
INFO - root - 2017-12-05 12:09:14.137924: step 6570, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.866 sec/batch; 78h:23m:48s remains)
INFO - root - 2017-12-05 12:09:22.636799: step 6580, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 79h:32m:23s remains)
INFO - root - 2017-12-05 12:09:31.105558: step 6590, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 76h:49m:14s remains)
INFO - root - 2017-12-05 12:09:39.616412: step 6600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 78h:37m:53s remains)
2017-12-05 12:09:40.355769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2768931 -4.2295842 -4.1716833 -4.1283417 -4.0993748 -4.0875554 -4.0802145 -4.0634127 -4.0521827 -4.0592475 -4.0802879 -4.08827 -4.0989575 -4.1092486 -4.1097503][-4.2789063 -4.2291303 -4.1683321 -4.12404 -4.0991454 -4.088469 -4.0814476 -4.0676827 -4.0557594 -4.0555162 -4.0665884 -4.0824223 -4.1052179 -4.1194949 -4.112133][-4.280479 -4.2309594 -4.1701484 -4.1244059 -4.1005912 -4.0908103 -4.086915 -4.0829573 -4.07829 -4.0755925 -4.0751824 -4.0850906 -4.1043148 -4.1156611 -4.1016831][-4.2785931 -4.2265081 -4.163034 -4.1133509 -4.088419 -4.0817609 -4.08304 -4.0862 -4.0921922 -4.0995708 -4.0990229 -4.0986934 -4.1040816 -4.1105795 -4.0980635][-4.2741179 -4.2214622 -4.15725 -4.1035943 -4.0782928 -4.0752931 -4.0770216 -4.0785894 -4.0933547 -4.1090946 -4.1110377 -4.1053057 -4.1058745 -4.116147 -4.113677][-4.2642312 -4.2096748 -4.1470795 -4.09351 -4.0667005 -4.0571866 -4.0487504 -4.05036 -4.0752316 -4.1000257 -4.1073842 -4.1030397 -4.0994644 -4.1124563 -4.1236467][-4.2546077 -4.1975212 -4.1361828 -4.0824642 -4.0511522 -4.02528 -3.9996102 -3.9997339 -4.0341859 -4.0695829 -4.0865459 -4.0864868 -4.0791478 -4.0886097 -4.105854][-4.2539563 -4.2009692 -4.1446714 -4.0904169 -4.0550327 -4.0163107 -3.9775326 -3.969619 -3.9992626 -4.0410576 -4.0700784 -4.0780935 -4.0709805 -4.0734162 -4.0862169][-4.2571845 -4.211133 -4.1673045 -4.1216846 -4.0909982 -4.0504832 -4.010745 -3.9926083 -4.0127597 -4.0605631 -4.0971718 -4.1028576 -4.0805359 -4.064065 -4.0646071][-4.2626028 -4.2213736 -4.1866608 -4.1492653 -4.1226315 -4.0879955 -4.0562062 -4.0363488 -4.0486617 -4.0937271 -4.1273713 -4.1229024 -4.0892482 -4.0572672 -4.0489445][-4.2679005 -4.231513 -4.2001276 -4.1648178 -4.1398311 -4.1120658 -4.0922794 -4.0773969 -4.082746 -4.1141825 -4.1372809 -4.127459 -4.0885105 -4.05241 -4.0431137][-4.2743063 -4.2416263 -4.2116909 -4.1794677 -4.1558809 -4.1387706 -4.1292615 -4.1189008 -4.1205263 -4.1373429 -4.1465263 -4.1320257 -4.0951595 -4.0599532 -4.0517249][-4.2841558 -4.2511115 -4.2151694 -4.1811762 -4.1606822 -4.1547222 -4.1579247 -4.1541677 -4.1541557 -4.1600223 -4.1578584 -4.1449084 -4.1164613 -4.08868 -4.0821047][-4.2990355 -4.2673945 -4.2316828 -4.2001243 -4.1858482 -4.1910367 -4.2042871 -4.2045417 -4.2040462 -4.201848 -4.1961646 -4.1871181 -4.170289 -4.1483493 -4.1315613][-4.3205986 -4.2952843 -4.2655978 -4.2435632 -4.2364798 -4.2442737 -4.2542882 -4.2530189 -4.2508588 -4.247705 -4.24232 -4.2378297 -4.2297287 -4.2123895 -4.1915612]]...]
INFO - root - 2017-12-05 12:09:48.811964: step 6610, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 78h:33m:39s remains)
INFO - root - 2017-12-05 12:09:57.281522: step 6620, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.814 sec/batch; 73h:42m:09s remains)
INFO - root - 2017-12-05 12:10:05.804277: step 6630, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:54m:16s remains)
INFO - root - 2017-12-05 12:10:14.326991: step 6640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:47m:02s remains)
INFO - root - 2017-12-05 12:10:22.837627: step 6650, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 78h:17m:13s remains)
INFO - root - 2017-12-05 12:10:31.318419: step 6660, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 77h:01m:49s remains)
INFO - root - 2017-12-05 12:10:39.822817: step 6670, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 75h:32m:34s remains)
INFO - root - 2017-12-05 12:10:48.351196: step 6680, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 76h:39m:54s remains)
INFO - root - 2017-12-05 12:10:56.801817: step 6690, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 79h:51m:02s remains)
INFO - root - 2017-12-05 12:11:05.274835: step 6700, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 76h:42m:56s remains)
2017-12-05 12:11:06.031087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2458858 -4.22709 -4.2009411 -4.1810846 -4.16912 -4.1591687 -4.1514616 -4.1481695 -4.1571088 -4.1637654 -4.1724849 -4.1808562 -4.1928649 -4.1865654 -4.1557956][-4.2408462 -4.2159443 -4.1815643 -4.15024 -4.1336761 -4.1192808 -4.1058054 -4.1009722 -4.1131563 -4.1164947 -4.1124225 -4.120616 -4.137084 -4.1385159 -4.1215081][-4.2297583 -4.1997709 -4.152133 -4.1038103 -4.0812712 -4.0581727 -4.0419784 -4.0409622 -4.0718451 -4.0820441 -4.071485 -4.077374 -4.0941892 -4.0965385 -4.0868497][-4.225832 -4.1946335 -4.1377945 -4.0742669 -4.0421939 -4.0092053 -3.9841146 -3.9882653 -4.0382609 -4.0648127 -4.055871 -4.0668392 -4.0857062 -4.0827155 -4.0728183][-4.2324409 -4.2003241 -4.1409636 -4.0692124 -4.0289783 -3.9933562 -3.9701712 -3.9826751 -4.0359082 -4.0647736 -4.05251 -4.0711188 -4.100657 -4.1012788 -4.0929475][-4.2402663 -4.2099338 -4.1529007 -4.0825181 -4.0399122 -4.0028787 -3.9777961 -3.9882176 -4.037446 -4.0641766 -4.0512457 -4.0744305 -4.1182261 -4.1382871 -4.1408873][-4.2500353 -4.2250962 -4.1741529 -4.1110535 -4.0672288 -4.0170393 -3.9649479 -3.9536495 -3.9893751 -4.0163956 -4.0142922 -4.053071 -4.1249561 -4.1720185 -4.1913881][-4.2586842 -4.2355304 -4.1945605 -4.1424751 -4.0974865 -4.030468 -3.9470994 -3.9071362 -3.9207363 -3.9438097 -3.9495597 -3.9991531 -4.102356 -4.1785688 -4.2174597][-4.265214 -4.2423606 -4.2108765 -4.1714759 -4.1317072 -4.0597363 -3.9618511 -3.9062622 -3.8994145 -3.9033949 -3.8982987 -3.9464979 -4.071445 -4.1724877 -4.2310457][-4.2674255 -4.248467 -4.2244272 -4.1946287 -4.1619153 -4.0991764 -4.0117936 -3.9596672 -3.9479332 -3.9437134 -3.9324751 -3.9695835 -4.0818043 -4.1792316 -4.2388687][-4.264852 -4.2499204 -4.234396 -4.2148342 -4.1905365 -4.1405606 -4.0733671 -4.03478 -4.0325322 -4.0369234 -4.0346489 -4.0648084 -4.1421194 -4.2099032 -4.2528081][-4.2594733 -4.2490993 -4.2385211 -4.2267184 -4.2109108 -4.1760616 -4.1277 -4.102304 -4.104589 -4.1143379 -4.1258378 -4.1559258 -4.2081566 -4.2479935 -4.2705221][-4.2523289 -4.248414 -4.2418008 -4.2333908 -4.22657 -4.205862 -4.17287 -4.1558647 -4.1591306 -4.1699247 -4.1889253 -4.2194953 -4.2561145 -4.2750254 -4.2815623][-4.2454586 -4.245388 -4.240766 -4.2323084 -4.2269268 -4.2134194 -4.191545 -4.1818838 -4.1874366 -4.2012005 -4.2272954 -4.2590537 -4.2823577 -4.2847624 -4.2794151][-4.2452259 -4.2459912 -4.2406564 -4.2303553 -4.224381 -4.2135673 -4.197474 -4.1919045 -4.2011008 -4.2187581 -4.2462196 -4.2752051 -4.287117 -4.2792482 -4.2670426]]...]
INFO - root - 2017-12-05 12:11:14.525145: step 6710, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 77h:37m:39s remains)
INFO - root - 2017-12-05 12:11:23.017796: step 6720, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 75h:18m:21s remains)
INFO - root - 2017-12-05 12:11:31.648985: step 6730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 76h:40m:42s remains)
INFO - root - 2017-12-05 12:11:40.258394: step 6740, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 79h:13m:30s remains)
INFO - root - 2017-12-05 12:11:48.841669: step 6750, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 77h:33m:47s remains)
INFO - root - 2017-12-05 12:11:57.378062: step 6760, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 77h:45m:15s remains)
INFO - root - 2017-12-05 12:12:05.992662: step 6770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 79h:04m:36s remains)
INFO - root - 2017-12-05 12:12:14.639380: step 6780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 77h:08m:37s remains)
INFO - root - 2017-12-05 12:12:23.129070: step 6790, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.782 sec/batch; 70h:46m:32s remains)
INFO - root - 2017-12-05 12:12:31.646105: step 6800, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 76h:25m:06s remains)
2017-12-05 12:12:32.451105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2229996 -4.2175183 -4.2230706 -4.2308187 -4.2303886 -4.1992359 -4.1558871 -4.1394682 -4.1832843 -4.2332435 -4.2408495 -4.2269545 -4.2172904 -4.2189369 -4.2135353][-4.2272105 -4.2231026 -4.2237163 -4.2316685 -4.232904 -4.1985564 -4.1525598 -4.1412168 -4.1884694 -4.2393293 -4.2408752 -4.2207723 -4.20827 -4.2136455 -4.20997][-4.2314668 -4.2268667 -4.2216563 -4.2236505 -4.2196116 -4.1827445 -4.133708 -4.1204581 -4.16407 -4.2158742 -4.2225647 -4.2042274 -4.1906118 -4.1967964 -4.1952558][-4.2352777 -4.2279353 -4.2182536 -4.2121983 -4.195045 -4.15441 -4.1038976 -4.0866356 -4.1258082 -4.1852589 -4.2057366 -4.1963139 -4.1856952 -4.195199 -4.1983671][-4.2423992 -4.2324452 -4.2221518 -4.2117205 -4.1816974 -4.1263485 -4.0614667 -4.0358377 -4.0805807 -4.1572151 -4.1963739 -4.196702 -4.1945863 -4.2098856 -4.2195191][-4.2530665 -4.2437139 -4.2379742 -4.2259822 -4.18353 -4.1080647 -4.0145426 -3.9724448 -4.0238309 -4.1210418 -4.1835089 -4.2025127 -4.2098694 -4.2307372 -4.2456093][-4.2567391 -4.2515116 -4.2535877 -4.244451 -4.1977654 -4.1021361 -3.97453 -3.90581 -3.9644916 -4.0853224 -4.1710334 -4.2085738 -4.2271066 -4.251493 -4.2686124][-4.2624555 -4.2651381 -4.2734947 -4.2670197 -4.2210946 -4.12052 -3.9806271 -3.9008904 -3.9625387 -4.0849185 -4.1732564 -4.218761 -4.246347 -4.2712011 -4.2837753][-4.2662058 -4.2771606 -4.287077 -4.2825961 -4.2444677 -4.15819 -4.039854 -3.9770694 -4.0265226 -4.1213217 -4.1913424 -4.229907 -4.2573752 -4.2807331 -4.2891212][-4.2601056 -4.2780504 -4.2897263 -4.2874727 -4.2601409 -4.1923785 -4.1046367 -4.0633855 -4.0974517 -4.16003 -4.2062893 -4.2352967 -4.25927 -4.2773957 -4.2815032][-4.2489243 -4.2724867 -4.2877979 -4.2889638 -4.2668276 -4.2118812 -4.1463437 -4.1181064 -4.141891 -4.1857371 -4.2206168 -4.2470541 -4.2678151 -4.2752781 -4.2717476][-4.2475886 -4.2728677 -4.2915549 -4.2966008 -4.2750545 -4.2290921 -4.1799045 -4.156981 -4.1743703 -4.2093873 -4.2395072 -4.2638073 -4.2800817 -4.2769151 -4.2650104][-4.241004 -4.2635841 -4.28495 -4.2951083 -4.2797546 -4.24196 -4.2040992 -4.1852441 -4.194181 -4.219532 -4.2450314 -4.2687383 -4.2839527 -4.2753596 -4.2560525][-4.222446 -4.2400894 -4.2655535 -4.2829237 -4.2790737 -4.2526617 -4.2265763 -4.2108216 -4.2047167 -4.2103758 -4.2297325 -4.2592759 -4.2773991 -4.2690759 -4.2443953][-4.1925278 -4.2041178 -4.2347341 -4.259975 -4.2657971 -4.2511115 -4.2387056 -4.2246113 -4.2044711 -4.1905761 -4.2062335 -4.2420015 -4.2624335 -4.2578363 -4.2333779]]...]
INFO - root - 2017-12-05 12:12:40.920085: step 6810, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 79h:38m:15s remains)
INFO - root - 2017-12-05 12:12:49.328007: step 6820, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:45m:13s remains)
INFO - root - 2017-12-05 12:12:57.991730: step 6830, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 79h:07m:56s remains)
INFO - root - 2017-12-05 12:13:06.592280: step 6840, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 78h:21m:45s remains)
INFO - root - 2017-12-05 12:13:15.214207: step 6850, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 77h:20m:35s remains)
INFO - root - 2017-12-05 12:13:23.789619: step 6860, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 76h:46m:02s remains)
INFO - root - 2017-12-05 12:13:32.271537: step 6870, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.806 sec/batch; 72h:52m:01s remains)
INFO - root - 2017-12-05 12:13:40.788754: step 6880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 78h:13m:20s remains)
INFO - root - 2017-12-05 12:13:49.468389: step 6890, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.894 sec/batch; 80h:53m:17s remains)
INFO - root - 2017-12-05 12:13:57.910866: step 6900, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 77h:55m:45s remains)
2017-12-05 12:13:58.637464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1639414 -4.1243839 -4.0629392 -4.012732 -4.001039 -4.016788 -4.0367184 -4.0495663 -4.0509925 -4.0586758 -4.0776038 -4.0906286 -4.10061 -4.1078639 -4.1165609][-4.1427412 -4.1152644 -4.0680604 -4.0240989 -4.0113244 -4.0256076 -4.0457373 -4.052844 -4.0392308 -4.033762 -4.0421462 -4.0528016 -4.0626788 -4.06889 -4.0751615][-4.1095376 -4.0942421 -4.06615 -4.03909 -4.0268631 -4.032712 -4.0465956 -4.0489163 -4.0370955 -4.0299129 -4.0342522 -4.0426755 -4.0516973 -4.0538979 -4.051753][-4.0888629 -4.0824308 -4.0731225 -4.0606413 -4.0454144 -4.0365453 -4.039257 -4.0450668 -4.0465627 -4.0499511 -4.0606556 -4.0659361 -4.0638404 -4.0549221 -4.0457573][-4.0830173 -4.0847039 -4.0890441 -4.0856977 -4.0623369 -4.0376153 -4.0269718 -4.0345883 -4.04998 -4.0694537 -4.0883484 -4.0913272 -4.0774908 -4.0566425 -4.0457673][-4.0813994 -4.0908155 -4.1005235 -4.0981812 -4.0669217 -4.0226946 -3.9903388 -3.9892008 -4.0223107 -4.0667057 -4.0992451 -4.1036716 -4.0817437 -4.055759 -4.0468822][-4.0716109 -4.0863876 -4.0962224 -4.0893617 -4.0447164 -3.9684696 -3.9006908 -3.8824971 -3.9395416 -4.0246463 -4.0849123 -4.0984883 -4.0756922 -4.0478978 -4.0413718][-4.0629873 -4.0818954 -4.0894971 -4.0738873 -4.0141063 -3.9079163 -3.8009193 -3.7638454 -3.8410418 -3.9620011 -4.0489774 -4.0777917 -4.0617442 -4.0347624 -4.0318756][-4.0681095 -4.0872192 -4.0947742 -4.0803914 -4.0266919 -3.9306781 -3.8363922 -3.8064885 -3.8648 -3.9602296 -4.0361557 -4.0682788 -4.0638056 -4.042809 -4.0401678][-4.0692053 -4.0888653 -4.099051 -4.0901642 -4.053144 -3.9897361 -3.9343126 -3.9219837 -3.9556956 -4.010571 -4.0596437 -4.08739 -4.0947323 -4.0872235 -4.0817342][-4.0825977 -4.1018715 -4.1078539 -4.097187 -4.06935 -4.0259929 -3.9959826 -3.9907613 -4.0109806 -4.0483594 -4.0874162 -4.1170306 -4.1313758 -4.1289487 -4.115222][-4.1165414 -4.1268754 -4.1244655 -4.1093497 -4.0848536 -4.0523043 -4.0278015 -4.0175047 -4.0263596 -4.0540342 -4.0911374 -4.1247458 -4.1421852 -4.1381345 -4.1147618][-4.1541648 -4.1536374 -4.1432862 -4.1241603 -4.1030974 -4.0757003 -4.0460892 -4.0186129 -4.0079474 -4.0257258 -4.0637217 -4.1019669 -4.12167 -4.1168227 -4.0927486][-4.18115 -4.1762333 -4.1624889 -4.1423187 -4.1213031 -4.0947123 -4.0609841 -4.02125 -3.9951854 -4.00434 -4.0446696 -4.0892854 -4.1166468 -4.1186776 -4.10033][-4.2016158 -4.1939974 -4.1794157 -4.1573296 -4.1317205 -4.1025615 -4.0697908 -4.0321422 -4.0073962 -4.0192404 -4.0631652 -4.1134863 -4.1401277 -4.1428804 -4.1252136]]...]
INFO - root - 2017-12-05 12:14:07.168963: step 6910, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 78h:14m:28s remains)
INFO - root - 2017-12-05 12:14:15.743534: step 6920, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 77h:24m:43s remains)
INFO - root - 2017-12-05 12:14:24.023390: step 6930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 78h:13m:25s remains)
INFO - root - 2017-12-05 12:14:32.602015: step 6940, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 79h:32m:28s remains)
INFO - root - 2017-12-05 12:14:41.148236: step 6950, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 75h:39m:37s remains)
INFO - root - 2017-12-05 12:14:49.604513: step 6960, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 77h:17m:57s remains)
INFO - root - 2017-12-05 12:14:58.058379: step 6970, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 77h:21m:55s remains)
INFO - root - 2017-12-05 12:15:06.624743: step 6980, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 77h:15m:11s remains)
INFO - root - 2017-12-05 12:15:15.200844: step 6990, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 76h:33m:50s remains)
INFO - root - 2017-12-05 12:15:23.718736: step 7000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 77h:48m:04s remains)
2017-12-05 12:15:24.515808: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2520709 -4.2544222 -4.2514563 -4.2457533 -4.2486444 -4.2532129 -4.2552171 -4.2549729 -4.2511144 -4.2470937 -4.2496109 -4.2587352 -4.2636023 -4.2716618 -4.2870169][-4.2128549 -4.2164974 -4.21436 -4.2053351 -4.2063351 -4.2106848 -4.2104659 -4.2066679 -4.1996579 -4.1946673 -4.2013512 -4.2170572 -4.2258925 -4.2377377 -4.2612834][-4.1674562 -4.1704865 -4.1678128 -4.1574326 -4.1557221 -4.16129 -4.1616139 -4.1562929 -4.1493311 -4.1462369 -4.1565967 -4.1757383 -4.1854782 -4.1996055 -4.2311192][-4.1245222 -4.1254644 -4.1204028 -4.109488 -4.1079993 -4.1162233 -4.1191893 -4.1165257 -4.1142311 -4.1201057 -4.1355114 -4.1531272 -4.1612043 -4.1765823 -4.2153344][-4.0970917 -4.0985694 -4.0921516 -4.0813646 -4.080349 -4.0880747 -4.0895615 -4.091836 -4.0987973 -4.1168318 -4.1376882 -4.15061 -4.1576071 -4.1749368 -4.217495][-4.086144 -4.0884452 -4.0820136 -4.0698724 -4.0661192 -4.0661893 -4.0545917 -4.0546746 -4.0736418 -4.1042347 -4.1312718 -4.14401 -4.1575093 -4.1820059 -4.2261705][-4.0507956 -4.0496902 -4.0423527 -4.0284386 -4.0158615 -3.997797 -3.9605002 -3.9595022 -4.0006065 -4.0468588 -4.0835376 -4.1094537 -4.1376653 -4.1739874 -4.2227216][-3.9893332 -3.98015 -3.9679027 -3.9466605 -3.9205847 -3.8782909 -3.8112278 -3.8184958 -3.8938365 -3.9571972 -4.0048351 -4.054956 -4.1044884 -4.1536245 -4.2096157][-3.9802158 -3.9652772 -3.9500232 -3.9268222 -3.8985143 -3.8555238 -3.7883339 -3.8025663 -3.8858948 -3.9423342 -3.9842465 -4.043364 -4.1001134 -4.1506472 -4.2056637][-4.0491648 -4.0362 -4.0278959 -4.0134926 -3.9936113 -3.968823 -3.9257481 -3.9317293 -3.987973 -4.0173554 -4.0372744 -4.0821004 -4.1279125 -4.1677241 -4.2132044][-4.1360645 -4.1315379 -4.1308804 -4.1254973 -4.1146679 -4.10242 -4.075789 -4.0690842 -4.0981059 -4.1095424 -4.11259 -4.1417942 -4.1770897 -4.2044873 -4.2355361][-4.2136774 -4.2179232 -4.2205553 -4.221458 -4.2177763 -4.2102175 -4.192842 -4.1838474 -4.1971974 -4.2012525 -4.1972451 -4.2147155 -4.2401118 -4.2544909 -4.2687407][-4.2676244 -4.2747712 -4.2766843 -4.2807908 -4.2826357 -4.2776737 -4.2676182 -4.2625279 -4.268641 -4.2683964 -4.2611132 -4.2719493 -4.2908287 -4.2976346 -4.2996373][-4.2810669 -4.2855992 -4.287 -4.29461 -4.3011885 -4.2998357 -4.2969384 -4.2971444 -4.3004336 -4.2991872 -4.2939248 -4.3009958 -4.3149128 -4.3190289 -4.3161416][-4.2727151 -4.2752018 -4.2774143 -4.2844605 -4.291615 -4.2922025 -4.2925243 -4.294363 -4.2966022 -4.2966638 -4.2948513 -4.3003244 -4.3117423 -4.3171692 -4.3172054]]...]
INFO - root - 2017-12-05 12:15:33.002210: step 7010, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 77h:02m:40s remains)
INFO - root - 2017-12-05 12:15:41.583937: step 7020, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 76h:38m:50s remains)
INFO - root - 2017-12-05 12:15:50.272920: step 7030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 76h:42m:03s remains)
INFO - root - 2017-12-05 12:15:58.695693: step 7040, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 77h:09m:37s remains)
INFO - root - 2017-12-05 12:16:07.286115: step 7050, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 77h:14m:50s remains)
INFO - root - 2017-12-05 12:16:15.852303: step 7060, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 76h:31m:18s remains)
INFO - root - 2017-12-05 12:16:24.420372: step 7070, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 79h:30m:16s remains)
INFO - root - 2017-12-05 12:16:33.079620: step 7080, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:22m:40s remains)
INFO - root - 2017-12-05 12:16:41.584446: step 7090, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 75h:13m:52s remains)
INFO - root - 2017-12-05 12:16:50.160994: step 7100, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 77h:29m:35s remains)
2017-12-05 12:16:50.929239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1556396 -4.1516657 -4.1438046 -4.1305022 -4.1266432 -4.1282496 -4.1296506 -4.1341581 -4.12848 -4.1325569 -4.1586952 -4.1867371 -4.1900735 -4.1725416 -4.1664386][-4.1432595 -4.1505656 -4.1529832 -4.1435695 -4.1324573 -4.1283813 -4.13854 -4.1551247 -4.1557755 -4.1534591 -4.163003 -4.1828108 -4.1861181 -4.170578 -4.168066][-4.125073 -4.1463337 -4.1591215 -4.1537633 -4.1368489 -4.128335 -4.1470037 -4.1773705 -4.1878252 -4.1837721 -4.1815605 -4.193624 -4.1986704 -4.1887341 -4.188345][-4.1055021 -4.1342196 -4.1539617 -4.1496353 -4.1271896 -4.113625 -4.131391 -4.1619997 -4.1805525 -4.1862025 -4.1878014 -4.2008667 -4.208571 -4.2028661 -4.2042646][-4.0910845 -4.1175861 -4.13726 -4.1356053 -4.1118259 -4.0873184 -4.0870495 -4.1007047 -4.1206293 -4.1458983 -4.1649747 -4.1855941 -4.1976652 -4.20013 -4.2034025][-4.093884 -4.1126976 -4.1324883 -4.1334634 -4.1081562 -4.0683212 -4.0338936 -4.0083065 -4.0120478 -4.0583344 -4.1098342 -4.1471119 -4.1678796 -4.183732 -4.1987863][-4.1296477 -4.1365447 -4.1490717 -4.149241 -4.1184711 -4.058301 -3.985611 -3.908006 -3.8792624 -3.9425631 -4.0329885 -4.0945187 -4.1254845 -4.1574597 -4.1912041][-4.1755109 -4.1727147 -4.172956 -4.1697674 -4.1411562 -4.0730486 -3.9798617 -3.8707139 -3.8118658 -3.867851 -3.9726279 -4.0462914 -4.0875769 -4.1338115 -4.1825576][-4.1957736 -4.1939907 -4.1888175 -4.1846509 -4.166853 -4.1155558 -4.0431962 -3.961211 -3.9068277 -3.9243586 -3.9803464 -4.0264874 -4.062655 -4.11175 -4.1664796][-4.1695747 -4.1777649 -4.1775594 -4.1779132 -4.1754909 -4.1534104 -4.1192446 -4.0771661 -4.0415769 -4.031951 -4.0337491 -4.0337772 -4.0460629 -4.0809984 -4.1267233][-4.122879 -4.141459 -4.1515412 -4.1594958 -4.1665335 -4.1642694 -4.1575603 -4.1463675 -4.1325808 -4.1189685 -4.0939651 -4.0634613 -4.0486345 -4.0605755 -4.0878787][-4.0922809 -4.1093063 -4.1280589 -4.1386352 -4.1396532 -4.1382542 -4.1430807 -4.1521158 -4.1609731 -4.1633325 -4.1406612 -4.105227 -4.075469 -4.0662012 -4.0776696][-4.0805645 -4.0890393 -4.1122332 -4.1232986 -4.1134138 -4.1004634 -4.1060157 -4.1233969 -4.1506429 -4.173811 -4.1675048 -4.1415095 -4.1110215 -4.0957727 -4.1006408][-4.0906749 -4.097785 -4.1232181 -4.1356273 -4.1210318 -4.0993762 -4.0931454 -4.1039934 -4.1336036 -4.1624093 -4.1685009 -4.152751 -4.1312966 -4.1218972 -4.1266279][-4.1192174 -4.1308947 -4.1537166 -4.1678867 -4.1587849 -4.1408844 -4.1264443 -4.1217456 -4.1305718 -4.141624 -4.1436057 -4.1386514 -4.1324944 -4.1359005 -4.1455717]]...]
INFO - root - 2017-12-05 12:16:59.259561: step 7110, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 78h:31m:47s remains)
INFO - root - 2017-12-05 12:17:07.818560: step 7120, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 78h:39m:40s remains)
INFO - root - 2017-12-05 12:17:16.389312: step 7130, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 78h:14m:44s remains)
INFO - root - 2017-12-05 12:17:25.006950: step 7140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 76h:53m:17s remains)
INFO - root - 2017-12-05 12:17:33.488283: step 7150, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 77h:28m:08s remains)
INFO - root - 2017-12-05 12:17:42.004309: step 7160, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 74h:30m:21s remains)
INFO - root - 2017-12-05 12:17:50.323918: step 7170, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 75h:57m:41s remains)
INFO - root - 2017-12-05 12:17:58.892895: step 7180, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 77h:09m:57s remains)
INFO - root - 2017-12-05 12:18:07.467577: step 7190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 76h:08m:13s remains)
INFO - root - 2017-12-05 12:18:16.022389: step 7200, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 77h:47m:58s remains)
2017-12-05 12:18:16.787427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2245603 -4.2250495 -4.2165585 -4.2177992 -4.2224283 -4.2189441 -4.2103558 -4.2136893 -4.2174459 -4.2143598 -4.2065182 -4.2048492 -4.2118363 -4.2213368 -4.2232342][-4.249897 -4.2411771 -4.224709 -4.2188768 -4.2221537 -4.2200851 -4.2129974 -4.218091 -4.2233396 -4.2189374 -4.2101364 -4.2072721 -4.2112 -4.2151403 -4.2108588][-4.2616496 -4.2455125 -4.2219515 -4.2091012 -4.20786 -4.2049508 -4.1987662 -4.2055922 -4.2160573 -4.22016 -4.2162948 -4.2114043 -4.2076516 -4.2035079 -4.1939306][-4.2626786 -4.2470012 -4.2167826 -4.1909137 -4.1783872 -4.1671753 -4.1585174 -4.1692557 -4.19431 -4.2205615 -4.231606 -4.2260447 -4.21446 -4.2023263 -4.1903472][-4.2611113 -4.250536 -4.2187781 -4.1805224 -4.1482859 -4.1215882 -4.1061759 -4.120461 -4.1650314 -4.2168126 -4.2491574 -4.2519817 -4.2398605 -4.2225418 -4.2095971][-4.2632947 -4.2575092 -4.2196774 -4.1623349 -4.1005263 -4.0389576 -3.9979558 -4.0148616 -4.089313 -4.1753144 -4.2382703 -4.2690048 -4.2720222 -4.2583237 -4.2447581][-4.2750564 -4.2663713 -4.2140589 -4.1313906 -4.0303097 -3.9202271 -3.8382525 -3.8560996 -3.9735289 -4.1028051 -4.2024155 -4.2674856 -4.2947111 -4.2909284 -4.2762232][-4.2894864 -4.2785225 -4.2208252 -4.12555 -4.003469 -3.8597274 -3.7351749 -3.7394116 -3.882231 -4.0424585 -4.1648631 -4.2536168 -4.3006082 -4.3059249 -4.2882743][-4.2924404 -4.2907286 -4.2420425 -4.1578946 -4.0532303 -3.9302161 -3.8105459 -3.7871528 -3.892941 -4.0359607 -4.1535964 -4.2420211 -4.2889409 -4.2954988 -4.2733011][-4.2985663 -4.3012052 -4.2636404 -4.1986618 -4.124896 -4.0506072 -3.9729738 -3.940742 -3.9919302 -4.08881 -4.1810384 -4.2482505 -4.2757277 -4.2710996 -4.2410893][-4.3200493 -4.3217573 -4.2923007 -4.2417207 -4.1909451 -4.1506066 -4.1091604 -4.084291 -4.107151 -4.171102 -4.2366543 -4.2745523 -4.2752709 -4.255343 -4.215147][-4.3470783 -4.3458843 -4.3198109 -4.2754669 -4.2338262 -4.2079716 -4.1864977 -4.1762347 -4.1971889 -4.244658 -4.28879 -4.3015676 -4.2861185 -4.2561846 -4.2071109][-4.3478384 -4.3466229 -4.3275347 -4.2967353 -4.2621484 -4.2414889 -4.2298026 -4.2312212 -4.2535648 -4.29174 -4.3220739 -4.3231311 -4.3037996 -4.2696719 -4.2177739][-4.3322086 -4.3304548 -4.3187995 -4.3040795 -4.284533 -4.2711811 -4.2648954 -4.2671232 -4.2856593 -4.3136358 -4.3335085 -4.3316941 -4.311564 -4.2757273 -4.2279921][-4.3198986 -4.3179717 -4.3109751 -4.3053179 -4.2963662 -4.2875485 -4.283977 -4.2856765 -4.296483 -4.3150191 -4.3324418 -4.3371692 -4.3225636 -4.2922063 -4.2556348]]...]
INFO - root - 2017-12-05 12:18:25.381017: step 7210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 75h:43m:42s remains)
INFO - root - 2017-12-05 12:18:33.832080: step 7220, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 75h:50m:33s remains)
INFO - root - 2017-12-05 12:18:42.321131: step 7230, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 77h:24m:28s remains)
INFO - root - 2017-12-05 12:18:50.844485: step 7240, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.858 sec/batch; 77h:28m:54s remains)
INFO - root - 2017-12-05 12:18:59.343017: step 7250, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.903 sec/batch; 81h:37m:25s remains)
INFO - root - 2017-12-05 12:19:07.704755: step 7260, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 77h:49m:35s remains)
INFO - root - 2017-12-05 12:19:16.119023: step 7270, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.806 sec/batch; 72h:50m:56s remains)
INFO - root - 2017-12-05 12:19:24.662436: step 7280, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 77h:37m:36s remains)
INFO - root - 2017-12-05 12:19:33.131691: step 7290, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 76h:43m:20s remains)
INFO - root - 2017-12-05 12:19:41.698076: step 7300, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 78h:37m:49s remains)
2017-12-05 12:19:42.421854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2233315 -4.2289777 -4.239778 -4.2530308 -4.2643752 -4.2698574 -4.27179 -4.2753677 -4.2740431 -4.2652416 -4.2583094 -4.2581382 -4.2611117 -4.2697296 -4.2899857][-4.1722412 -4.1689076 -4.1832094 -4.2062426 -4.2242441 -4.2315779 -4.2319965 -4.2371216 -4.2367659 -4.225009 -4.2141848 -4.2156239 -4.2225342 -4.2353897 -4.2618327][-4.1342759 -4.1189475 -4.1330209 -4.166707 -4.19447 -4.2063351 -4.2058506 -4.2092657 -4.2087188 -4.198638 -4.1893373 -4.1971989 -4.2105169 -4.2250037 -4.2505245][-4.1269326 -4.1043053 -4.1116886 -4.1466904 -4.174459 -4.1860294 -4.1845827 -4.1913056 -4.1966763 -4.1942205 -4.1921353 -4.2074981 -4.2232628 -4.2364964 -4.2572083][-4.1356077 -4.1163621 -4.1206007 -4.1412 -4.1529946 -4.1483035 -4.1371555 -4.1489124 -4.1728449 -4.1892257 -4.1968975 -4.2141914 -4.2302837 -4.2442932 -4.2614303][-4.1501207 -4.1368589 -4.1388454 -4.1430588 -4.1331935 -4.0971231 -4.0575376 -4.0677404 -4.1253347 -4.1719451 -4.1942253 -4.2138376 -4.2270136 -4.2403555 -4.2543507][-4.1704736 -4.1621284 -4.1626339 -4.155663 -4.1254926 -4.0535922 -3.9630044 -3.9556096 -4.058847 -4.1459427 -4.1874285 -4.2130771 -4.2289863 -4.2432575 -4.2538848][-4.182446 -4.1772003 -4.1795397 -4.172739 -4.1373429 -4.0527005 -3.9354341 -3.9042974 -4.0212688 -4.1231451 -4.1693974 -4.198554 -4.2257962 -4.246079 -4.2556396][-4.1793413 -4.1833072 -4.1941738 -4.1970859 -4.1721344 -4.1105642 -4.0274076 -4.0004816 -4.068089 -4.1372676 -4.16436 -4.1843719 -4.2124219 -4.2367921 -4.2501779][-4.1668024 -4.1749992 -4.1950154 -4.2097631 -4.2037349 -4.1727095 -4.1325755 -4.1208978 -4.1528287 -4.1840992 -4.1872287 -4.1898723 -4.202507 -4.2216654 -4.2386384][-4.156343 -4.1638112 -4.1890087 -4.2082186 -4.2088552 -4.1975088 -4.1841121 -4.1880269 -4.2103543 -4.2268276 -4.2239161 -4.2192779 -4.2167397 -4.227644 -4.2408423][-4.1598887 -4.1653333 -4.185142 -4.2006435 -4.2017422 -4.1994214 -4.2024436 -4.2180076 -4.239718 -4.2552104 -4.2614479 -4.2615433 -4.2546988 -4.2570748 -4.26214][-4.1839962 -4.187788 -4.195684 -4.1962934 -4.1964178 -4.2029338 -4.2127028 -4.2316017 -4.2522311 -4.2673926 -4.2800164 -4.2847433 -4.2797914 -4.27832 -4.2788177][-4.2173581 -4.218823 -4.2168813 -4.2098989 -4.2073231 -4.2139783 -4.2241216 -4.240603 -4.2568274 -4.2675185 -4.2782893 -4.2860074 -4.2853284 -4.2848144 -4.2844987][-4.2465706 -4.2474985 -4.2458 -4.2413454 -4.2413378 -4.2456546 -4.2497997 -4.2591348 -4.2698355 -4.2785215 -4.2852287 -4.29325 -4.2972889 -4.2986665 -4.2950659]]...]
INFO - root - 2017-12-05 12:19:50.906427: step 7310, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 77h:50m:43s remains)
INFO - root - 2017-12-05 12:19:59.555769: step 7320, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:34m:51s remains)
INFO - root - 2017-12-05 12:20:08.069850: step 7330, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 77h:36m:05s remains)
INFO - root - 2017-12-05 12:20:16.660078: step 7340, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 75h:36m:24s remains)
INFO - root - 2017-12-05 12:20:25.155209: step 7350, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 75h:45m:26s remains)
INFO - root - 2017-12-05 12:20:33.688345: step 7360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:30m:26s remains)
INFO - root - 2017-12-05 12:20:42.100962: step 7370, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 76h:03m:52s remains)
INFO - root - 2017-12-05 12:20:50.649782: step 7380, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 76h:55m:35s remains)
INFO - root - 2017-12-05 12:20:59.162948: step 7390, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 78h:06m:20s remains)
INFO - root - 2017-12-05 12:21:07.742672: step 7400, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 77h:54m:50s remains)
2017-12-05 12:21:08.668001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2692552 -4.2708712 -4.2542334 -4.2400904 -4.2182422 -4.1725359 -4.1033063 -4.0442362 -4.0452085 -4.0947781 -4.151576 -4.1891994 -4.20603 -4.2163539 -4.2379713][-4.2662039 -4.2666197 -4.2535572 -4.2430129 -4.2216134 -4.1753173 -4.1065645 -4.049912 -4.0438814 -4.101 -4.1707721 -4.217886 -4.2452283 -4.2578726 -4.2736826][-4.2681775 -4.2589207 -4.2433267 -4.2341323 -4.2134171 -4.1664996 -4.0981426 -4.037 -4.0290403 -4.0935659 -4.1744657 -4.2292252 -4.2659397 -4.2880154 -4.3086782][-4.2773767 -4.2591057 -4.2390232 -4.2223086 -4.1886468 -4.1286306 -4.0445342 -3.9726951 -3.9725568 -4.0539708 -4.147058 -4.2133875 -4.2611904 -4.2948742 -4.3209805][-4.2882104 -4.2662053 -4.2404804 -4.2058496 -4.1435165 -4.0472145 -3.9207897 -3.8365192 -3.8776524 -3.9999945 -4.1134868 -4.1925058 -4.2455187 -4.28463 -4.3136086][-4.2894888 -4.268599 -4.2405639 -4.1869745 -4.0943689 -3.9525697 -3.7719054 -3.6803617 -3.7896559 -3.9628115 -4.0980034 -4.1866622 -4.245132 -4.2900214 -4.3199444][-4.2885032 -4.2710462 -4.2441978 -4.185338 -4.0820289 -3.9234939 -3.7275634 -3.6334889 -3.7637815 -3.9535346 -4.095376 -4.1867032 -4.2485929 -4.2979727 -4.3297458][-4.2909169 -4.2734079 -4.2486978 -4.2008743 -4.1168957 -3.9928327 -3.8392639 -3.7503502 -3.824517 -3.9727612 -4.1001716 -4.1896672 -4.25321 -4.3027534 -4.3329525][-4.2951941 -4.27912 -4.2598257 -4.2287445 -4.1732945 -4.0968566 -3.9909978 -3.9122224 -3.93116 -4.023735 -4.1214004 -4.203218 -4.2660155 -4.3113046 -4.3381319][-4.3012633 -4.2861938 -4.26854 -4.24476 -4.2070718 -4.1606321 -4.0914841 -4.0300016 -4.0293989 -4.0890608 -4.162921 -4.2339153 -4.2879953 -4.3252172 -4.3455238][-4.3101578 -4.2948523 -4.2760692 -4.2542343 -4.2253256 -4.1911287 -4.1434054 -4.102623 -4.1033955 -4.149797 -4.2094917 -4.2674084 -4.3086128 -4.3362536 -4.3501706][-4.3108373 -4.2944474 -4.2752357 -4.2560797 -4.2328587 -4.2046537 -4.1675563 -4.1401806 -4.1449728 -4.1898847 -4.2441568 -4.2916079 -4.3233666 -4.3427796 -4.3516612][-4.3039317 -4.2884197 -4.272294 -4.2549129 -4.2362862 -4.2151318 -4.1884212 -4.1701603 -4.1765604 -4.2148185 -4.2623439 -4.303112 -4.3289051 -4.3429732 -4.3474522][-4.2945838 -4.2815533 -4.2693987 -4.2535563 -4.2390718 -4.2256894 -4.20963 -4.1992035 -4.208488 -4.2401767 -4.2774758 -4.30864 -4.3274317 -4.3369889 -4.3377829][-4.2912731 -4.2829208 -4.2738419 -4.2612891 -4.2493582 -4.241652 -4.2365003 -4.2358503 -4.2475362 -4.2696495 -4.2944345 -4.3132253 -4.323473 -4.3282552 -4.3266382]]...]
INFO - root - 2017-12-05 12:21:17.062296: step 7410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 75h:33m:30s remains)
INFO - root - 2017-12-05 12:21:25.561302: step 7420, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 78h:01m:36s remains)
INFO - root - 2017-12-05 12:21:34.007234: step 7430, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 75h:04m:58s remains)
INFO - root - 2017-12-05 12:21:42.541046: step 7440, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 78h:49m:43s remains)
INFO - root - 2017-12-05 12:21:51.064666: step 7450, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 75h:46m:27s remains)
INFO - root - 2017-12-05 12:21:59.655098: step 7460, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 76h:19m:00s remains)
INFO - root - 2017-12-05 12:22:08.176904: step 7470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 76h:14m:18s remains)
INFO - root - 2017-12-05 12:22:16.662598: step 7480, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 73h:22m:43s remains)
INFO - root - 2017-12-05 12:22:25.149296: step 7490, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:31m:37s remains)
INFO - root - 2017-12-05 12:22:33.709782: step 7500, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 80h:01m:31s remains)
2017-12-05 12:22:34.462638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2865067 -4.2829041 -4.2753406 -4.2712703 -4.2671404 -4.2519336 -4.2366881 -4.2327547 -4.2403431 -4.2473688 -4.2496381 -4.2557578 -4.2608418 -4.2581882 -4.2473564][-4.27625 -4.2723384 -4.2621346 -4.2548933 -4.2425389 -4.2139397 -4.185349 -4.1780944 -4.1946111 -4.20873 -4.2151752 -4.2280531 -4.2351055 -4.22899 -4.2103395][-4.2625184 -4.258049 -4.2464123 -4.2358608 -4.216507 -4.1764555 -4.1340995 -4.1270018 -4.1557245 -4.1843395 -4.2038689 -4.2250071 -4.2304764 -4.2133188 -4.18301][-4.251215 -4.2496228 -4.2388139 -4.2263856 -4.1991 -4.1460018 -4.0886035 -4.0768261 -4.1185117 -4.1651387 -4.2009816 -4.2285051 -4.2328339 -4.2058425 -4.1648479][-4.2495146 -4.24928 -4.2344661 -4.2114897 -4.1703725 -4.1051922 -4.0339389 -4.0156207 -4.07245 -4.13745 -4.1865835 -4.2195559 -4.2221727 -4.1897116 -4.1449823][-4.2534585 -4.2504969 -4.2261248 -4.1874704 -4.130362 -4.0521131 -3.9679391 -3.9314604 -4.0023971 -4.0938039 -4.1576052 -4.1996226 -4.2059107 -4.1718016 -4.1323462][-4.2539554 -4.243124 -4.2117729 -4.1683874 -4.1066594 -4.0148335 -3.9048138 -3.8368332 -3.9189179 -4.040134 -4.1206775 -4.1695104 -4.179873 -4.1516991 -4.1229873][-4.2428789 -4.2232785 -4.1882062 -4.1467953 -4.0932097 -4.003809 -3.8816946 -3.7865515 -3.8579891 -3.9917665 -4.0804787 -4.13043 -4.1442227 -4.123126 -4.1038876][-4.2218127 -4.1983066 -4.1627903 -4.1261597 -4.0947905 -4.0376792 -3.9553378 -3.8889203 -3.9254217 -4.0115819 -4.0747252 -4.1088777 -4.1152782 -4.0938768 -4.0853181][-4.2013278 -4.1820965 -4.150403 -4.1281805 -4.122138 -4.1038074 -4.0637116 -4.02838 -4.0402007 -4.0791097 -4.1090851 -4.1260724 -4.12066 -4.0921817 -4.0822186][-4.1827841 -4.1671739 -4.1461091 -4.1345525 -4.1412153 -4.145596 -4.1317611 -4.11342 -4.1148472 -4.1295362 -4.1384091 -4.1440082 -4.1317515 -4.0992074 -4.0888352][-4.1692429 -4.1617217 -4.1514764 -4.1473937 -4.157486 -4.1714277 -4.1734834 -4.1644983 -4.1663685 -4.175715 -4.1801167 -4.1813011 -4.1683655 -4.1396723 -4.128336][-4.175611 -4.17288 -4.172843 -4.1747231 -4.1862731 -4.2031646 -4.2107968 -4.2101526 -4.2156477 -4.2280426 -4.2355881 -4.2363434 -4.2269011 -4.2030025 -4.1874647][-4.1967955 -4.1995382 -4.2086239 -4.2139325 -4.2238245 -4.2384014 -4.2454281 -4.2472243 -4.2555485 -4.2685862 -4.2798657 -4.2838926 -4.2779393 -4.2616005 -4.2449384][-4.2382259 -4.2406025 -4.2497716 -4.2538118 -4.2579732 -4.2673349 -4.273767 -4.2780361 -4.2864242 -4.2970452 -4.3078156 -4.3135514 -4.3107224 -4.3000016 -4.2866359]]...]
INFO - root - 2017-12-05 12:22:43.011249: step 7510, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 75h:46m:11s remains)
INFO - root - 2017-12-05 12:22:51.538380: step 7520, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 79h:32m:53s remains)
INFO - root - 2017-12-05 12:23:00.042024: step 7530, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 78h:23m:58s remains)
INFO - root - 2017-12-05 12:23:08.541089: step 7540, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 78h:04m:54s remains)
INFO - root - 2017-12-05 12:23:16.980611: step 7550, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 73h:39m:40s remains)
INFO - root - 2017-12-05 12:23:25.590252: step 7560, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 79h:45m:27s remains)
INFO - root - 2017-12-05 12:23:34.247525: step 7570, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 79h:36m:02s remains)
INFO - root - 2017-12-05 12:23:42.704837: step 7580, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 76h:54m:39s remains)
INFO - root - 2017-12-05 12:23:51.083775: step 7590, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 78h:35m:16s remains)
INFO - root - 2017-12-05 12:23:59.507946: step 7600, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 77h:37m:36s remains)
2017-12-05 12:24:00.254493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32507 -4.315444 -4.3060765 -4.3041615 -4.3056107 -4.30525 -4.3076439 -4.313271 -4.3239746 -4.3312798 -4.33307 -4.3399677 -4.3464079 -4.3465576 -4.3408432][-4.304606 -4.28565 -4.2672482 -4.2614079 -4.2619348 -4.256206 -4.2520905 -4.2555251 -4.2722926 -4.2875018 -4.2947683 -4.3058639 -4.3190985 -4.323411 -4.3197775][-4.2803278 -4.25256 -4.2237854 -4.2122135 -4.2100687 -4.1961246 -4.1824207 -4.1808829 -4.2035232 -4.2294569 -4.2450371 -4.2593575 -4.2804332 -4.2912092 -4.2909985][-4.2601418 -4.227551 -4.1925087 -4.1750488 -4.1670976 -4.143837 -4.1187606 -4.1176062 -4.1458569 -4.1768379 -4.1936584 -4.2086468 -4.2345872 -4.2526207 -4.2577052][-4.2487125 -4.2145433 -4.1771731 -4.1517158 -4.1307421 -4.0964427 -4.0637927 -4.0669451 -4.1002078 -4.1315923 -4.1450543 -4.1532235 -4.1778154 -4.2035332 -4.2192264][-4.2437716 -4.2080097 -4.168942 -4.1333504 -4.0897222 -4.033917 -3.9951975 -4.0141816 -4.060658 -4.0944881 -4.1015429 -4.0966411 -4.1136446 -4.1497407 -4.1799951][-4.2417984 -4.2057505 -4.1677957 -4.1256695 -4.0575829 -3.9687834 -3.9184279 -3.9651656 -4.0416436 -4.084291 -4.0816388 -4.0605235 -4.0694103 -4.1149168 -4.1588087][-4.2409368 -4.2081294 -4.1748357 -4.1331296 -4.0561218 -3.944257 -3.8785441 -3.9399209 -4.0363712 -4.0856867 -4.08219 -4.0578365 -4.0598564 -4.1006808 -4.1441326][-4.2462783 -4.2211509 -4.1960464 -4.1632504 -4.1041107 -4.01085 -3.9452384 -3.9824646 -4.0534782 -4.0938139 -4.0991387 -4.0950894 -4.0957966 -4.1143785 -4.1397753][-4.2532473 -4.2352366 -4.2168708 -4.1938252 -4.1568689 -4.0981054 -4.0503836 -4.0652232 -4.1007948 -4.1231427 -4.1350017 -4.15065 -4.1544824 -4.1510949 -4.1533852][-4.2549829 -4.2387404 -4.221489 -4.2026448 -4.1783504 -4.1448874 -4.1195965 -4.1294727 -4.1488585 -4.1614604 -4.1751394 -4.19677 -4.2032351 -4.1923385 -4.1816244][-4.254549 -4.236156 -4.2175956 -4.1966772 -4.1753139 -4.1584687 -4.1544776 -4.1689386 -4.1855655 -4.1988955 -4.2130785 -4.2310696 -4.2356038 -4.2266393 -4.2152319][-4.2581525 -4.2361875 -4.2112617 -4.1842527 -4.1587682 -4.1467743 -4.1554608 -4.1800079 -4.2020354 -4.2212186 -4.2338486 -4.2444744 -4.247786 -4.2421327 -4.2330055][-4.2657642 -4.2410493 -4.2080688 -4.1713729 -4.1374869 -4.1224952 -4.1323767 -4.1642427 -4.1952767 -4.2200928 -4.2319469 -4.2380953 -4.2397375 -4.2359395 -4.2285509][-4.2696118 -4.2429614 -4.2063184 -4.1649594 -4.1255279 -4.1066246 -4.1129413 -4.1445971 -4.1792111 -4.2061105 -4.2186575 -4.224299 -4.2272696 -4.2243953 -4.2192988]]...]
INFO - root - 2017-12-05 12:24:08.684588: step 7610, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 76h:39m:24s remains)
INFO - root - 2017-12-05 12:24:17.266836: step 7620, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 77h:21m:18s remains)
INFO - root - 2017-12-05 12:24:25.689104: step 7630, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 76h:12m:17s remains)
INFO - root - 2017-12-05 12:24:34.207865: step 7640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 76h:51m:10s remains)
INFO - root - 2017-12-05 12:24:42.747262: step 7650, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 78h:04m:08s remains)
INFO - root - 2017-12-05 12:24:51.297827: step 7660, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 78h:28m:29s remains)
INFO - root - 2017-12-05 12:24:59.655497: step 7670, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.819 sec/batch; 73h:52m:19s remains)
INFO - root - 2017-12-05 12:25:08.179526: step 7680, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 77h:32m:26s remains)
INFO - root - 2017-12-05 12:25:16.709476: step 7690, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 76h:05m:08s remains)
INFO - root - 2017-12-05 12:25:25.240406: step 7700, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 75h:01m:40s remains)
2017-12-05 12:25:26.012304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2256188 -4.2353797 -4.2423797 -4.2456136 -4.2435756 -4.243319 -4.2505279 -4.2633152 -4.2838254 -4.3030987 -4.3126817 -4.3095794 -4.2962341 -4.2770305 -4.2643123][-4.1499519 -4.1671386 -4.1826172 -4.1904836 -4.1910686 -4.1929245 -4.2010512 -4.2142735 -4.2365952 -4.2590761 -4.2712555 -4.2701731 -4.2559891 -4.2344246 -4.2180829][-4.0642648 -4.0928192 -4.1220679 -4.1389112 -4.1446857 -4.147222 -4.1554084 -4.169817 -4.1962409 -4.2206049 -4.2348557 -4.2357407 -4.22384 -4.2015667 -4.1825495][-4.0128603 -4.0475025 -4.0835466 -4.10697 -4.1163406 -4.1172323 -4.1237168 -4.1385241 -4.1691236 -4.1953478 -4.2143893 -4.21875 -4.2132187 -4.1966968 -4.1815166][-4.0462766 -4.075633 -4.1011953 -4.1207671 -4.1289449 -4.1211805 -4.1125836 -4.1176047 -4.1441689 -4.1692476 -4.1896248 -4.2009158 -4.2105131 -4.2078223 -4.2041669][-4.1281786 -4.1502018 -4.1544003 -4.1534767 -4.1462369 -4.1201711 -4.0912991 -4.0860071 -4.1083388 -4.1321936 -4.1528411 -4.1703973 -4.1917024 -4.2043805 -4.2197995][-4.1850367 -4.1997647 -4.1851583 -4.1548691 -4.120719 -4.0783205 -4.0382471 -4.0302825 -4.0554991 -4.0862765 -4.1100721 -4.1335711 -4.1601377 -4.1789594 -4.2077656][-4.2110934 -4.217732 -4.1867881 -4.1365848 -4.0821075 -4.0318832 -3.9958484 -3.993268 -4.0197644 -4.0549836 -4.0826569 -4.1016922 -4.124711 -4.1431136 -4.1810589][-4.21531 -4.2243142 -4.1950173 -4.1440611 -4.0883121 -4.0458198 -4.0210838 -4.0170565 -4.0334945 -4.0696235 -4.09976 -4.1106215 -4.1279135 -4.1540384 -4.1988721][-4.2089005 -4.2249284 -4.2062945 -4.1686974 -4.129477 -4.1038151 -4.0866475 -4.0787535 -4.0919452 -4.12823 -4.1582866 -4.1684337 -4.1840591 -4.2099109 -4.2453594][-4.2083721 -4.2269306 -4.2197123 -4.2041516 -4.1920214 -4.1832113 -4.175488 -4.1729832 -4.1863136 -4.2159295 -4.2409134 -4.2486143 -4.2577868 -4.2712116 -4.2899003][-4.2088747 -4.2304559 -4.2338791 -4.23889 -4.2490983 -4.2579155 -4.2642846 -4.2660594 -4.2767215 -4.2961893 -4.3109074 -4.3147411 -4.3164368 -4.32048 -4.3222761][-4.1901393 -4.2164893 -4.2329044 -4.2572522 -4.2838478 -4.3058629 -4.3188481 -4.3223805 -4.3298926 -4.3393893 -4.3442616 -4.3435349 -4.3415871 -4.3394833 -4.3328586][-4.164072 -4.1919332 -4.2174106 -4.2550426 -4.2930956 -4.3200693 -4.3362012 -4.3408723 -4.3440409 -4.3459244 -4.3429551 -4.3393478 -4.33611 -4.332304 -4.3255515][-4.1559019 -4.1791444 -4.203474 -4.2419643 -4.2846365 -4.3144727 -4.3314486 -4.3350215 -4.3350096 -4.3327866 -4.3265524 -4.321146 -4.3165832 -4.3122153 -4.3072062]]...]
INFO - root - 2017-12-05 12:25:34.547985: step 7710, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 78h:50m:50s remains)
INFO - root - 2017-12-05 12:25:43.109330: step 7720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 77h:24m:19s remains)
INFO - root - 2017-12-05 12:25:51.629071: step 7730, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 73h:58m:20s remains)
INFO - root - 2017-12-05 12:26:00.206151: step 7740, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 76h:20m:19s remains)
INFO - root - 2017-12-05 12:26:08.608369: step 7750, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 76h:41m:36s remains)
INFO - root - 2017-12-05 12:26:17.106563: step 7760, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 76h:18m:16s remains)
INFO - root - 2017-12-05 12:26:25.681751: step 7770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 79h:16m:01s remains)
INFO - root - 2017-12-05 12:26:34.225706: step 7780, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 75h:12m:24s remains)
INFO - root - 2017-12-05 12:26:42.699265: step 7790, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 75h:36m:56s remains)
INFO - root - 2017-12-05 12:26:51.259032: step 7800, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 77h:48m:19s remains)
2017-12-05 12:26:51.976360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16861 -4.1689386 -4.1684017 -4.1786346 -4.2146764 -4.2506585 -4.2760653 -4.2903547 -4.296823 -4.2961659 -4.2893729 -4.285181 -4.2880111 -4.2766476 -4.2510324][-4.1897392 -4.18109 -4.1698775 -4.1698837 -4.2026429 -4.2418447 -4.2702842 -4.2863865 -4.2935615 -4.293797 -4.2859564 -4.277729 -4.276165 -4.263876 -4.2439876][-4.1937218 -4.1746044 -4.1540985 -4.145937 -4.1774254 -4.223197 -4.25794 -4.27616 -4.280736 -4.2784433 -4.2682419 -4.2597914 -4.2573104 -4.2487822 -4.2379045][-4.1714 -4.1366034 -4.1040196 -4.0933886 -4.136683 -4.1945524 -4.2359385 -4.253561 -4.2517323 -4.2460642 -4.2396936 -4.2363629 -4.2367563 -4.23697 -4.2386756][-4.13985 -4.0886621 -4.045013 -4.0359073 -4.0907946 -4.15548 -4.1943426 -4.204731 -4.1931047 -4.1858363 -4.1877379 -4.1933084 -4.20246 -4.2182808 -4.23496][-4.1109319 -4.0589519 -4.0169439 -4.009594 -4.06224 -4.1222262 -4.1533465 -4.1511135 -4.1244965 -4.10764 -4.1127949 -4.1271911 -4.1462994 -4.1769195 -4.2046442][-4.0964241 -4.0599914 -4.0368009 -4.0268064 -4.0547566 -4.0927391 -4.1101704 -4.1021638 -4.0668569 -4.0372629 -4.0406094 -4.0649257 -4.0948219 -4.1291518 -4.1587715][-4.1241894 -4.1087661 -4.1005254 -4.0887856 -4.0880084 -4.0920515 -4.09364 -4.0924358 -4.0691752 -4.0407653 -4.0391607 -4.0587544 -4.0819783 -4.1047421 -4.131783][-4.1750093 -4.1763554 -4.1791844 -4.1703506 -4.1577258 -4.141449 -4.1337924 -4.1436858 -4.1384029 -4.120121 -4.1119862 -4.1147738 -4.1174073 -4.1192994 -4.1370692][-4.2268114 -4.240674 -4.2476673 -4.2391415 -4.2281032 -4.21284 -4.2065363 -4.2214246 -4.2255497 -4.2138934 -4.2009668 -4.1872578 -4.1742291 -4.1632333 -4.1712966][-4.2710671 -4.2904253 -4.2977438 -4.2868958 -4.2757564 -4.2684784 -4.2681189 -4.2796869 -4.2847362 -4.2779355 -4.26264 -4.2427435 -4.2252917 -4.21246 -4.2147312][-4.2934861 -4.312644 -4.3171663 -4.3056865 -4.2969828 -4.2950439 -4.2981114 -4.3048644 -4.3062248 -4.3016505 -4.2883811 -4.270164 -4.2542157 -4.2444453 -4.2437611][-4.3018775 -4.3204093 -4.3263049 -4.3185105 -4.3141212 -4.3145614 -4.3170276 -4.319901 -4.3187089 -4.3161278 -4.3083587 -4.2938848 -4.2809834 -4.2747602 -4.2735114][-4.3139768 -4.3266454 -4.330986 -4.3274307 -4.3255453 -4.3272696 -4.329195 -4.3304968 -4.3286257 -4.3268247 -4.322494 -4.3142085 -4.3058634 -4.3016887 -4.3001852][-4.3240423 -4.3281026 -4.3288631 -4.3268161 -4.3269873 -4.3294415 -4.3308992 -4.3312469 -4.3298922 -4.3290896 -4.3275671 -4.3249836 -4.3212228 -4.3191681 -4.3177619]]...]
INFO - root - 2017-12-05 12:27:00.268364: step 7810, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 73h:05m:16s remains)
INFO - root - 2017-12-05 12:27:08.843717: step 7820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 78h:13m:58s remains)
INFO - root - 2017-12-05 12:27:17.385990: step 7830, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 77h:53m:43s remains)
INFO - root - 2017-12-05 12:27:26.011915: step 7840, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.852 sec/batch; 76h:50m:45s remains)
INFO - root - 2017-12-05 12:27:34.488058: step 7850, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 77h:19m:14s remains)
INFO - root - 2017-12-05 12:27:42.878792: step 7860, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 77h:52m:12s remains)
INFO - root - 2017-12-05 12:27:51.405350: step 7870, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 76h:33m:22s remains)
INFO - root - 2017-12-05 12:27:59.850031: step 7880, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 79h:19m:37s remains)
INFO - root - 2017-12-05 12:28:08.390228: step 7890, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 77h:40m:57s remains)
INFO - root - 2017-12-05 12:28:16.971738: step 7900, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 78h:33m:54s remains)
2017-12-05 12:28:17.753786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2603283 -4.2516251 -4.2529144 -4.2688003 -4.2840905 -4.2912645 -4.2920823 -4.290308 -4.2861352 -4.2867775 -4.2942076 -4.3016167 -4.3031378 -4.3010249 -4.3009048][-4.215045 -4.2022324 -4.206624 -4.230444 -4.2494988 -4.2558775 -4.2543635 -4.2519259 -4.2496552 -4.2546244 -4.267766 -4.2801609 -4.2844167 -4.2826519 -4.2810903][-4.1778378 -4.16473 -4.1740732 -4.2049532 -4.2257972 -4.2278619 -4.2215772 -4.2161493 -4.2142377 -4.222959 -4.2414489 -4.2576904 -4.2624307 -4.2583241 -4.2563434][-4.157577 -4.148469 -4.1629086 -4.1965108 -4.2154026 -4.2134666 -4.2029548 -4.1953249 -4.1921411 -4.2011728 -4.223413 -4.2437024 -4.25112 -4.247941 -4.2464895][-4.1505251 -4.1491041 -4.164578 -4.1906343 -4.2035069 -4.1981759 -4.1881261 -4.179554 -4.1747584 -4.181622 -4.2045178 -4.2283764 -4.2410655 -4.2441859 -4.2468276][-4.1651211 -4.1705761 -4.1848764 -4.1992264 -4.202445 -4.1919746 -4.1808925 -4.1688457 -4.1611519 -4.1650257 -4.1866989 -4.2120862 -4.2297225 -4.2406216 -4.2500978][-4.1886106 -4.1979742 -4.2100806 -4.2146649 -4.2091269 -4.1957159 -4.1863279 -4.1748037 -4.167182 -4.171402 -4.1924543 -4.2177386 -4.2352824 -4.2470579 -4.2573032][-4.1974244 -4.20432 -4.2123914 -4.2132359 -4.20519 -4.1924229 -4.18977 -4.1857076 -4.1811194 -4.1861668 -4.207983 -4.2316179 -4.2448387 -4.2502127 -4.2541766][-4.1959176 -4.2002711 -4.2074 -4.2096615 -4.20426 -4.1949563 -4.1971636 -4.1989532 -4.1978865 -4.2022085 -4.2209024 -4.2374411 -4.2391062 -4.2369919 -4.2368436][-4.1890154 -4.1923294 -4.2016916 -4.2097764 -4.210711 -4.2057681 -4.2076607 -4.2114725 -4.2115326 -4.2108293 -4.2185631 -4.2229333 -4.2094889 -4.1998425 -4.2027211][-4.1842508 -4.1873088 -4.19913 -4.2113042 -4.2157273 -4.2106957 -4.2087669 -4.2120943 -4.2118521 -4.2054176 -4.204998 -4.198667 -4.1762257 -4.1619673 -4.1710334][-4.1831007 -4.1869035 -4.2007179 -4.2150702 -4.2218156 -4.2174158 -4.21065 -4.2105136 -4.2053032 -4.1928391 -4.18905 -4.1788592 -4.1582 -4.1465282 -4.1607509][-4.1788073 -4.1852956 -4.2008076 -4.2171836 -4.2267952 -4.2239943 -4.2142525 -4.2065067 -4.1948857 -4.1816425 -4.1810508 -4.17715 -4.1655235 -4.1606479 -4.1765971][-4.1831269 -4.1940818 -4.208837 -4.2239289 -4.2335711 -4.2305303 -4.21989 -4.2060008 -4.1894665 -4.17887 -4.1819363 -4.1855655 -4.1844406 -4.1862097 -4.2037382][-4.2003541 -4.2151556 -4.2278171 -4.2407169 -4.2492313 -4.2476282 -4.23982 -4.2270575 -4.2118015 -4.2040229 -4.2071486 -4.2098174 -4.2112818 -4.2144036 -4.2304082]]...]
INFO - root - 2017-12-05 12:28:26.098018: step 7910, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 77h:22m:57s remains)
INFO - root - 2017-12-05 12:28:34.574784: step 7920, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 74h:04m:05s remains)
INFO - root - 2017-12-05 12:28:43.031656: step 7930, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 76h:14m:33s remains)
INFO - root - 2017-12-05 12:28:51.539574: step 7940, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:58m:39s remains)
INFO - root - 2017-12-05 12:29:00.060053: step 7950, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 80h:12m:51s remains)
INFO - root - 2017-12-05 12:29:08.603672: step 7960, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 77h:45m:58s remains)
INFO - root - 2017-12-05 12:29:17.037381: step 7970, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 74h:27m:14s remains)
INFO - root - 2017-12-05 12:29:25.491588: step 7980, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 77h:37m:38s remains)
INFO - root - 2017-12-05 12:29:33.891464: step 7990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 75h:36m:50s remains)
INFO - root - 2017-12-05 12:29:42.371127: step 8000, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 75h:34m:52s remains)
2017-12-05 12:29:43.081067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1478329 -4.1393032 -4.1461163 -4.1624684 -4.1688862 -4.1657152 -4.1674066 -4.1742949 -4.1838436 -4.2019887 -4.226819 -4.2414827 -4.2442546 -4.2433004 -4.2443151][-4.1475577 -4.1394615 -4.1496692 -4.1644993 -4.1678538 -4.1640406 -4.1665554 -4.1717277 -4.17981 -4.2001238 -4.2324772 -4.254777 -4.2610855 -4.2582912 -4.2519965][-4.1584406 -4.1472144 -4.1539865 -4.1591706 -4.1516519 -4.1368642 -4.1330075 -4.1378884 -4.1499386 -4.1780667 -4.219099 -4.2510076 -4.2640696 -4.2643819 -4.2566118][-4.1600461 -4.1474285 -4.1471548 -4.1409411 -4.1211042 -4.0947847 -4.0804925 -4.0829945 -4.1037126 -4.1448827 -4.1952677 -4.23423 -4.2555032 -4.2641706 -4.261591][-4.1597333 -4.1462116 -4.137434 -4.1202469 -4.0913858 -4.055819 -4.0295038 -4.0314283 -4.0657935 -4.1192222 -4.1696668 -4.2082281 -4.2368083 -4.2521195 -4.2539082][-4.1631289 -4.1474338 -4.13007 -4.10578 -4.0708365 -4.0235472 -3.979234 -3.9783113 -4.0271654 -4.089489 -4.1376047 -4.1763048 -4.2134018 -4.2338624 -4.2355504][-4.1723604 -4.1497269 -4.1200714 -4.0843315 -4.036644 -3.9685097 -3.8958068 -3.8901906 -3.9623036 -4.038518 -4.0926042 -4.1415234 -4.1928 -4.2216945 -4.2268276][-4.1783671 -4.1506152 -4.1107426 -4.0639939 -4.0060453 -3.9253139 -3.8333197 -3.8230669 -3.9117935 -3.9941592 -4.0531511 -4.1106238 -4.1712093 -4.2057986 -4.216094][-4.1813884 -4.1544895 -4.1128116 -4.0684547 -4.0198531 -3.9603963 -3.8941267 -3.8871422 -3.9451351 -3.9972692 -4.0407324 -4.0888939 -4.1420336 -4.1771851 -4.1975584][-4.1819892 -4.1581903 -4.1194234 -4.0811906 -4.0461459 -4.0165696 -3.9852996 -3.9838989 -4.008029 -4.0291519 -4.0503039 -4.0785532 -4.1124873 -4.1444416 -4.17813][-4.1756873 -4.1508636 -4.1116943 -4.0748062 -4.0498247 -4.0442195 -4.0400052 -4.0458779 -4.0553274 -4.0647407 -4.0761862 -4.087039 -4.1037984 -4.1328087 -4.169188][-4.1708479 -4.1429429 -4.1004176 -4.0626335 -4.0481119 -4.0622272 -4.0772724 -4.0855956 -4.0871735 -4.0922685 -4.0968485 -4.0970244 -4.1071954 -4.1358342 -4.1657381][-4.177177 -4.1483164 -4.1063991 -4.073226 -4.0696096 -4.0948534 -4.11617 -4.1192174 -4.11181 -4.1109591 -4.1108222 -4.1079025 -4.1184874 -4.1436095 -4.1580381][-4.1916833 -4.1651397 -4.1285062 -4.1014652 -4.1055117 -4.131556 -4.1452794 -4.1391048 -4.1270285 -4.1232152 -4.12124 -4.118999 -4.133019 -4.1548834 -4.1531553][-4.2084126 -4.1863122 -4.157414 -4.1395144 -4.1473136 -4.1679735 -4.17217 -4.1588197 -4.1454787 -4.1397033 -4.1341085 -4.1329875 -4.1504521 -4.1675916 -4.1582575]]...]
INFO - root - 2017-12-05 12:29:51.545570: step 8010, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:35m:18s remains)
INFO - root - 2017-12-05 12:29:59.996733: step 8020, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 75h:47m:16s remains)
INFO - root - 2017-12-05 12:30:08.399287: step 8030, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.847 sec/batch; 76h:17m:47s remains)
INFO - root - 2017-12-05 12:30:16.726203: step 8040, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 74h:23m:00s remains)
INFO - root - 2017-12-05 12:30:25.304397: step 8050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 78h:34m:37s remains)
INFO - root - 2017-12-05 12:30:33.868232: step 8060, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 79h:13m:26s remains)
INFO - root - 2017-12-05 12:30:42.362091: step 8070, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 78h:28m:41s remains)
INFO - root - 2017-12-05 12:30:51.019367: step 8080, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:16m:17s remains)
INFO - root - 2017-12-05 12:30:59.624722: step 8090, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 77h:05m:35s remains)
INFO - root - 2017-12-05 12:31:08.143233: step 8100, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 75h:12m:40s remains)
2017-12-05 12:31:08.927367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1970205 -4.179759 -4.1725464 -4.1795955 -4.1997724 -4.2184534 -4.2146659 -4.1969066 -4.1948352 -4.1931653 -4.187892 -4.1924787 -4.2126403 -4.2329111 -4.2501173][-4.1952362 -4.1763749 -4.1699715 -4.1751862 -4.1907392 -4.2041249 -4.192369 -4.1664758 -4.1696963 -4.1787872 -4.1835632 -4.1920142 -4.2089853 -4.2249508 -4.2416015][-4.1845279 -4.16857 -4.1637416 -4.1657095 -4.174088 -4.183167 -4.1693282 -4.1425743 -4.1539803 -4.1721144 -4.184093 -4.1963749 -4.2101274 -4.2215266 -4.236691][-4.1750584 -4.1665478 -4.1647067 -4.1637826 -4.1637607 -4.1671 -4.1534619 -4.1305108 -4.1488371 -4.1715164 -4.1867132 -4.2030592 -4.2134752 -4.2206635 -4.2345181][-4.1619072 -4.1602721 -4.1632519 -4.1616244 -4.155107 -4.1508236 -4.133286 -4.1138835 -4.1387558 -4.1668072 -4.1853986 -4.2052135 -4.214098 -4.2189674 -4.23285][-4.1451621 -4.1459932 -4.1511788 -4.148499 -4.1368947 -4.1235681 -4.0960879 -4.0763383 -4.1086216 -4.1471815 -4.17067 -4.197659 -4.211473 -4.2169328 -4.2329664][-4.1333361 -4.1341882 -4.1356044 -4.1304646 -4.1166182 -4.0937648 -4.047574 -4.0183892 -4.0607538 -4.118917 -4.153914 -4.1886263 -4.2089167 -4.219017 -4.2387676][-4.1531863 -4.1468744 -4.1373076 -4.1235967 -4.1072726 -4.0768561 -4.0140786 -3.9710062 -4.0205564 -4.0962124 -4.142282 -4.1793914 -4.2039995 -4.2195516 -4.247025][-4.1910071 -4.1765947 -4.156662 -4.1337461 -4.118289 -4.0936451 -4.0370221 -3.9952068 -4.0387034 -4.1101422 -4.1528354 -4.1827765 -4.2058949 -4.22441 -4.2590137][-4.2263112 -4.2080207 -4.18278 -4.1544213 -4.1437221 -4.1352782 -4.1040888 -4.0805583 -4.1114035 -4.1632705 -4.1928935 -4.2120533 -4.2295742 -4.2455568 -4.2783127][-4.2515903 -4.2340722 -4.2098837 -4.1815639 -4.1759114 -4.1817846 -4.1772814 -4.171536 -4.1899424 -4.2218313 -4.2377753 -4.2458739 -4.25624 -4.2675309 -4.2926555][-4.2589178 -4.2464204 -4.2274561 -4.2033443 -4.2005239 -4.2164083 -4.230803 -4.2359066 -4.2447886 -4.2596741 -4.2674313 -4.2685432 -4.2718868 -4.2776341 -4.2960868][-4.2560511 -4.2471409 -4.2328544 -4.2148266 -4.2136307 -4.2336197 -4.2556152 -4.2643056 -4.2685971 -4.2723522 -4.2748337 -4.2753625 -4.2778697 -4.2814322 -4.2945561][-4.2532296 -4.24408 -4.2332153 -4.2191062 -4.2167573 -4.2326727 -4.2535915 -4.2622919 -4.2658143 -4.2666545 -4.2688913 -4.2717209 -4.2769294 -4.2817693 -4.291183][-4.2611847 -4.25329 -4.2466197 -4.2330379 -4.225121 -4.234323 -4.2519417 -4.2590256 -4.26056 -4.2601833 -4.262414 -4.2656527 -4.2715039 -4.275178 -4.2821918]]...]
INFO - root - 2017-12-05 12:31:17.515789: step 8110, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.859 sec/batch; 77h:25m:27s remains)
INFO - root - 2017-12-05 12:31:26.107450: step 8120, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 78h:04m:43s remains)
INFO - root - 2017-12-05 12:31:34.682107: step 8130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 77h:47m:58s remains)
INFO - root - 2017-12-05 12:31:43.146370: step 8140, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.854 sec/batch; 76h:56m:15s remains)
INFO - root - 2017-12-05 12:31:51.551015: step 8150, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.811 sec/batch; 73h:06m:44s remains)
INFO - root - 2017-12-05 12:32:00.162337: step 8160, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 77h:36m:59s remains)
INFO - root - 2017-12-05 12:32:08.635900: step 8170, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:04m:05s remains)
INFO - root - 2017-12-05 12:32:17.190007: step 8180, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 73h:59m:18s remains)
INFO - root - 2017-12-05 12:32:25.767510: step 8190, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 79h:57m:50s remains)
INFO - root - 2017-12-05 12:32:34.316307: step 8200, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 78h:44m:31s remains)
2017-12-05 12:32:35.038663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3275738 -4.2996788 -4.2545691 -4.2050672 -4.1663547 -4.1333485 -4.1221142 -4.1564879 -4.1830144 -4.1840181 -4.1786456 -4.1789231 -4.1980734 -4.2239747 -4.2481065][-4.3293967 -4.2978873 -4.2473955 -4.1921868 -4.1475487 -4.1095123 -4.1040239 -4.1471257 -4.1842055 -4.1957736 -4.1954875 -4.1985645 -4.2153649 -4.2366281 -4.2543631][-4.3258762 -4.291626 -4.2358112 -4.17433 -4.120666 -4.0787706 -4.0827231 -4.1391973 -4.1902456 -4.2175541 -4.2285757 -4.23711 -4.2523537 -4.2669187 -4.2753439][-4.3196139 -4.2839708 -4.2266989 -4.163918 -4.1064291 -4.0595865 -4.0653706 -4.1285062 -4.1878262 -4.2261076 -4.2477875 -4.2644587 -4.2816949 -4.2934546 -4.2974267][-4.3192806 -4.2854896 -4.2314272 -4.1695304 -4.1065431 -4.0465527 -4.0399823 -4.0978127 -4.1617088 -4.2092381 -4.2408309 -4.26565 -4.2866797 -4.3000703 -4.3044877][-4.3207669 -4.2904167 -4.2388043 -4.1761618 -4.1064138 -4.0285015 -3.997407 -4.0418119 -4.1072841 -4.1659536 -4.2076244 -4.2398014 -4.2664108 -4.2860394 -4.2955246][-4.3205719 -4.2918949 -4.2412519 -4.1762118 -4.0954461 -3.9914062 -3.9235504 -3.9516177 -4.0219641 -4.0954461 -4.1516042 -4.1932473 -4.2274928 -4.2545223 -4.2719][-4.3213272 -4.2920213 -4.2398872 -4.16965 -4.0756984 -3.9408617 -3.8356125 -3.856082 -3.9388375 -4.0247831 -4.093246 -4.1426544 -4.1831803 -4.214848 -4.2373166][-4.3192735 -4.2892294 -4.239953 -4.1733851 -4.0802259 -3.9369664 -3.8241498 -3.8535438 -3.946125 -4.0309234 -4.0929184 -4.1316609 -4.1633964 -4.1888647 -4.208559][-4.3157163 -4.2883425 -4.2477317 -4.1939449 -4.1176281 -3.9943328 -3.9017191 -3.9427485 -4.0310593 -4.1027184 -4.1464305 -4.1648922 -4.178339 -4.1906095 -4.2008123][-4.3146329 -4.2915816 -4.2593803 -4.2189703 -4.1634912 -4.0702362 -4.0047603 -4.0466728 -4.1180253 -4.172708 -4.2023325 -4.2087784 -4.2107058 -4.2096877 -4.2101088][-4.317493 -4.2991185 -4.2744937 -4.2441878 -4.2078886 -4.1453805 -4.1062779 -4.1449814 -4.1966677 -4.2324619 -4.2494907 -4.2514472 -4.2474647 -4.2374034 -4.2305231][-4.321743 -4.3030667 -4.2807808 -4.2574224 -4.2348208 -4.196188 -4.1772308 -4.2121887 -4.2504864 -4.2731309 -4.281116 -4.2797227 -4.2715507 -4.2585115 -4.2488737][-4.3217425 -4.3006349 -4.2744422 -4.2485485 -4.2297606 -4.2061534 -4.2016215 -4.2388511 -4.2741337 -4.2936716 -4.2991004 -4.2946634 -4.2827439 -4.2689862 -4.25896][-4.3217726 -4.2991648 -4.2665291 -4.2338071 -4.2125549 -4.1936593 -4.1950212 -4.2341661 -4.27282 -4.2962055 -4.3041096 -4.300077 -4.2877059 -4.2740254 -4.2629757]]...]
INFO - root - 2017-12-05 12:32:43.437683: step 8210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 78h:05m:32s remains)
INFO - root - 2017-12-05 12:32:51.988945: step 8220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 78h:13m:51s remains)
INFO - root - 2017-12-05 12:33:00.518599: step 8230, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 75h:05m:20s remains)
INFO - root - 2017-12-05 12:33:09.048937: step 8240, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 75h:58m:04s remains)
INFO - root - 2017-12-05 12:33:17.451483: step 8250, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 76h:15m:00s remains)
INFO - root - 2017-12-05 12:33:25.985088: step 8260, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 74h:57m:52s remains)
INFO - root - 2017-12-05 12:33:34.538229: step 8270, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 78h:39m:41s remains)
INFO - root - 2017-12-05 12:33:43.257086: step 8280, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 78h:50m:22s remains)
INFO - root - 2017-12-05 12:33:51.820269: step 8290, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 76h:17m:29s remains)
INFO - root - 2017-12-05 12:34:00.353880: step 8300, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 75h:38m:50s remains)
2017-12-05 12:34:01.124297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19598 -4.2025051 -4.2096477 -4.2230864 -4.2471323 -4.268168 -4.2764854 -4.2723174 -4.2603531 -4.2464461 -4.2321453 -4.2123456 -4.188746 -4.1718626 -4.1636][-4.1764183 -4.17895 -4.18325 -4.1971827 -4.2250252 -4.249619 -4.2601237 -4.2601714 -4.2531343 -4.2431793 -4.2325845 -4.2177744 -4.200953 -4.1880455 -4.1809039][-4.1554193 -4.1550961 -4.1571021 -4.1674786 -4.1903043 -4.211175 -4.2208943 -4.2240515 -4.222332 -4.2191224 -4.2151203 -4.2079029 -4.2005744 -4.1941934 -4.1903667][-4.137754 -4.1372461 -4.1373897 -4.1396108 -4.1488781 -4.1610074 -4.1658578 -4.1672912 -4.1688423 -4.1739855 -4.1788182 -4.179275 -4.1819258 -4.1844358 -4.1872978][-4.1295757 -4.1310291 -4.1340771 -4.1334562 -4.1315331 -4.1339293 -4.1292667 -4.123003 -4.1255856 -4.1357336 -4.14676 -4.1537671 -4.16354 -4.1727509 -4.1795688][-4.1266956 -4.1300082 -4.1398435 -4.1418376 -4.134347 -4.1283665 -4.1121931 -4.0949359 -4.0949354 -4.1080689 -4.1210666 -4.1358871 -4.1589608 -4.1762285 -4.1861954][-4.1144266 -4.1226172 -4.1463208 -4.1593003 -4.1547117 -4.142632 -4.1135325 -4.0830154 -4.0766606 -4.0865664 -4.09909 -4.1211023 -4.161272 -4.1909242 -4.2048278][-4.1122913 -4.1244359 -4.1599612 -4.182323 -4.1812463 -4.1688352 -4.1313157 -4.0870342 -4.06676 -4.064117 -4.0714855 -4.1009021 -4.1568108 -4.2005672 -4.2152958][-4.1172762 -4.1304779 -4.1693358 -4.1951246 -4.1960683 -4.1821976 -4.1407709 -4.0878921 -4.0490794 -4.0269203 -4.0267897 -4.0620675 -4.13097 -4.191092 -4.2151628][-4.127347 -4.1397281 -4.1776652 -4.2007208 -4.2018037 -4.1925864 -4.1570406 -4.1092687 -4.0626483 -4.0245876 -4.0126252 -4.0424066 -4.1099739 -4.1765752 -4.2106][-4.1456852 -4.1578703 -4.1898212 -4.2067275 -4.2065706 -4.1990833 -4.1721272 -4.1388459 -4.1049128 -4.069037 -4.05201 -4.0660477 -4.108551 -4.1579251 -4.1922774][-4.1652055 -4.1791534 -4.1996088 -4.2073474 -4.2026668 -4.1919909 -4.1729951 -4.1577668 -4.1397605 -4.1158819 -4.100615 -4.1039982 -4.1195769 -4.1406069 -4.1632562][-4.1822782 -4.2016835 -4.2167206 -4.22201 -4.2156606 -4.2050729 -4.194334 -4.191021 -4.1860995 -4.1740527 -4.1580248 -4.1521978 -4.14902 -4.1473689 -4.1548691][-4.2004228 -4.224287 -4.2390475 -4.2434459 -4.2391582 -4.2343273 -4.22928 -4.2295947 -4.2320933 -4.2254543 -4.2103295 -4.2022543 -4.1937246 -4.1820941 -4.1784835][-4.1938534 -4.2172117 -4.2302923 -4.2364392 -4.2409492 -4.2463665 -4.2445455 -4.2419891 -4.2437181 -4.2380247 -4.2255592 -4.2198186 -4.2111139 -4.197866 -4.1923089]]...]
INFO - root - 2017-12-05 12:34:09.607238: step 8310, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.881 sec/batch; 79h:21m:35s remains)
INFO - root - 2017-12-05 12:34:18.166371: step 8320, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 73h:42m:14s remains)
INFO - root - 2017-12-05 12:34:26.708581: step 8330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:57m:45s remains)
INFO - root - 2017-12-05 12:34:35.312494: step 8340, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:59m:19s remains)
INFO - root - 2017-12-05 12:34:43.864501: step 8350, loss = 2.11, batch loss = 2.05 (9.2 examples/sec; 0.872 sec/batch; 78h:30m:07s remains)
INFO - root - 2017-12-05 12:34:52.381790: step 8360, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:56m:04s remains)
INFO - root - 2017-12-05 12:35:00.963023: step 8370, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 80h:52m:03s remains)
INFO - root - 2017-12-05 12:35:09.479126: step 8380, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.891 sec/batch; 80h:15m:44s remains)
INFO - root - 2017-12-05 12:35:18.079079: step 8390, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 77h:04m:22s remains)
INFO - root - 2017-12-05 12:35:26.614828: step 8400, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 74h:34m:47s remains)
2017-12-05 12:35:27.375549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.319943 -4.3161216 -4.3086576 -4.3001637 -4.28752 -4.2762752 -4.2638845 -4.2519388 -4.2534838 -4.2642817 -4.2748461 -4.2825627 -4.2872262 -4.2936425 -4.2966471][-4.3169422 -4.31288 -4.3031917 -4.2903581 -4.272563 -4.2543716 -4.2309566 -4.2072315 -4.2104063 -4.2292142 -4.2473149 -4.2582164 -4.2641091 -4.27482 -4.2811351][-4.3176742 -4.3143497 -4.3015242 -4.2813597 -4.2534604 -4.2241907 -4.1881914 -4.1531672 -4.160223 -4.1905813 -4.2176485 -4.2338419 -4.243649 -4.2590756 -4.268414][-4.3199825 -4.316648 -4.2983971 -4.2671461 -4.224504 -4.1856856 -4.142271 -4.1030636 -4.1179137 -4.1578979 -4.193593 -4.217176 -4.2277102 -4.2456641 -4.2594447][-4.3216815 -4.3176765 -4.2938614 -4.2524853 -4.1964145 -4.1483727 -4.1007795 -4.0619564 -4.0828595 -4.1242929 -4.1626277 -4.18842 -4.1960111 -4.21114 -4.2284942][-4.3208251 -4.3169127 -4.2907395 -4.2431884 -4.1786623 -4.1227174 -4.0707951 -4.0324993 -4.0588541 -4.1005707 -4.1370463 -4.1631064 -4.1675277 -4.1746874 -4.1875734][-4.3180418 -4.3141751 -4.2864532 -4.2332191 -4.1643171 -4.1090431 -4.0577288 -4.0162916 -4.0446777 -4.0867887 -4.1195908 -4.1459618 -4.146987 -4.1478949 -4.1557665][-4.3134122 -4.3094192 -4.2794833 -4.2217212 -4.1528864 -4.1058521 -4.0636945 -4.0236754 -4.0487943 -4.084168 -4.1087308 -4.1319842 -4.1301579 -4.1305523 -4.1339679][-4.3093023 -4.3061094 -4.2773166 -4.2205267 -4.1573234 -4.1186523 -4.0799975 -4.0359845 -4.0555182 -4.0856428 -4.10453 -4.1291623 -4.1328321 -4.1354561 -4.1345377][-4.3078771 -4.3052082 -4.27979 -4.2266717 -4.1673603 -4.1282153 -4.0819387 -4.0298047 -4.0428963 -4.0687265 -4.0863266 -4.1116524 -4.123951 -4.1337285 -4.1371675][-4.3081765 -4.3068452 -4.2839293 -4.232254 -4.1702485 -4.1241889 -4.0698175 -4.012435 -4.02172 -4.0503025 -4.0663176 -4.0937662 -4.1121926 -4.1243162 -4.1304331][-4.3081083 -4.3073497 -4.2843151 -4.2317848 -4.1649184 -4.1138835 -4.0594478 -4.0033021 -4.0135946 -4.049861 -4.0655522 -4.0910916 -4.1098452 -4.1224108 -4.1305656][-4.3063717 -4.3023868 -4.2788086 -4.2295847 -4.1643391 -4.1155128 -4.0708437 -4.0245771 -4.0371408 -4.0724192 -4.0814819 -4.0947566 -4.10744 -4.1183515 -4.1275096][-4.3056917 -4.2972436 -4.2740393 -4.2328978 -4.1750426 -4.1296096 -4.0929017 -4.0540624 -4.0651712 -4.0989933 -4.1089063 -4.114994 -4.120707 -4.1287866 -4.1361151][-4.3077288 -4.297852 -4.2771487 -4.24731 -4.20375 -4.164835 -4.1329551 -4.0996861 -4.1079316 -4.1394815 -4.1555471 -4.1587229 -4.1574087 -4.1625171 -4.16846]]...]
INFO - root - 2017-12-05 12:35:35.881567: step 8410, loss = 2.04, batch loss = 1.98 (8.9 examples/sec; 0.903 sec/batch; 81h:16m:45s remains)
INFO - root - 2017-12-05 12:35:44.580100: step 8420, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.876 sec/batch; 78h:52m:57s remains)
INFO - root - 2017-12-05 12:35:53.177867: step 8430, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:16m:28s remains)
INFO - root - 2017-12-05 12:36:01.808489: step 8440, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 78h:11m:57s remains)
INFO - root - 2017-12-05 12:36:10.450429: step 8450, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.872 sec/batch; 78h:28m:55s remains)
INFO - root - 2017-12-05 12:36:18.994367: step 8460, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 78h:45m:25s remains)
INFO - root - 2017-12-05 12:36:27.430969: step 8470, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:19m:54s remains)
INFO - root - 2017-12-05 12:36:36.010118: step 8480, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 77h:26m:29s remains)
INFO - root - 2017-12-05 12:36:44.622066: step 8490, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 77h:23m:05s remains)
INFO - root - 2017-12-05 12:36:53.198188: step 8500, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 77h:48m:15s remains)
2017-12-05 12:36:53.995321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1766391 -4.172626 -4.1740108 -4.1630692 -4.1452746 -4.1416025 -4.1481185 -4.163187 -4.1848 -4.2002878 -4.2079744 -4.2017808 -4.1872182 -4.1697559 -4.1495619][-4.1572843 -4.1612759 -4.1688485 -4.1606627 -4.1447859 -4.140769 -4.1475511 -4.1615963 -4.1762772 -4.1834607 -4.1809349 -4.1682739 -4.1540742 -4.1421776 -4.1298981][-4.1561441 -4.1600127 -4.1631355 -4.15582 -4.1426964 -4.1390843 -4.1438546 -4.156383 -4.1739292 -4.1818881 -4.1726751 -4.1527224 -4.1338444 -4.1209717 -4.1169577][-4.1620932 -4.1595826 -4.1557226 -4.1447597 -4.1276674 -4.1169543 -4.1209331 -4.141541 -4.1665125 -4.1743469 -4.1584625 -4.1326146 -4.1084466 -4.0924315 -4.0919342][-4.1624074 -4.1507778 -4.13366 -4.1109633 -4.0816631 -4.0630755 -4.0697608 -4.0995197 -4.1348224 -4.1489487 -4.1389222 -4.11899 -4.0946984 -4.0754371 -4.0718069][-4.160346 -4.1370134 -4.1028094 -4.0605135 -4.0093908 -3.9798629 -3.9885 -4.0316811 -4.083322 -4.1177287 -4.1293736 -4.1265383 -4.1095257 -4.0874734 -4.0745196][-4.163868 -4.1262946 -4.0737724 -4.0133486 -3.9476829 -3.9096353 -3.9153705 -3.96539 -4.0340428 -4.0908542 -4.1201367 -4.131144 -4.1237593 -4.1061563 -4.089231][-4.1651196 -4.1185842 -4.0576706 -3.9937654 -3.9329019 -3.9033914 -3.9100327 -3.9594197 -4.0346622 -4.1013546 -4.1307812 -4.1419559 -4.1354265 -4.11997 -4.1064968][-4.1711731 -4.1269455 -4.0718904 -4.0195861 -3.9768007 -3.9637752 -3.9714334 -4.0164957 -4.0872521 -4.1472287 -4.1644177 -4.1652431 -4.152102 -4.1370206 -4.1222548][-4.18749 -4.1553631 -4.1137547 -4.0752873 -4.0506124 -4.0483437 -4.0515647 -4.0835886 -4.1388946 -4.1822729 -4.1798477 -4.1638465 -4.1441422 -4.1297197 -4.1166253][-4.2084522 -4.1911206 -4.1600857 -4.1302624 -4.1138453 -4.1112413 -4.1103439 -4.1308303 -4.1717143 -4.1938705 -4.1690836 -4.1329012 -4.1044292 -4.0947385 -4.0959492][-4.2221665 -4.214952 -4.1870403 -4.1563153 -4.1398416 -4.1335926 -4.1323462 -4.1500797 -4.1792746 -4.1844506 -4.145432 -4.103364 -4.0762844 -4.0764804 -4.0965142][-4.2300887 -4.2269297 -4.1963229 -4.1570435 -4.1313062 -4.120378 -4.1215811 -4.1405587 -4.1634021 -4.1594214 -4.1184015 -4.0808287 -4.0662837 -4.083231 -4.11637][-4.2358742 -4.2352777 -4.2034407 -4.1555138 -4.1171041 -4.0959787 -4.0944715 -4.1130157 -4.136797 -4.1328716 -4.0968981 -4.0662513 -4.0644279 -4.0910187 -4.1280642][-4.2346511 -4.2379742 -4.2109528 -4.1612921 -4.1123443 -4.0782537 -4.071919 -4.0921183 -4.1229424 -4.1284318 -4.1079478 -4.0864758 -4.0898914 -4.1168437 -4.1506143]]...]
INFO - root - 2017-12-05 12:37:02.492693: step 8510, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.829 sec/batch; 74h:38m:45s remains)
INFO - root - 2017-12-05 12:37:11.123330: step 8520, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 78h:50m:08s remains)
INFO - root - 2017-12-05 12:37:19.767918: step 8530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 77h:28m:01s remains)
INFO - root - 2017-12-05 12:37:28.316933: step 8540, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 75h:44m:02s remains)
INFO - root - 2017-12-05 12:37:36.894498: step 8550, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 76h:27m:36s remains)
INFO - root - 2017-12-05 12:37:45.350516: step 8560, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 75h:54m:37s remains)
INFO - root - 2017-12-05 12:37:53.897686: step 8570, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 77h:19m:42s remains)
INFO - root - 2017-12-05 12:38:02.449896: step 8580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 77h:49m:46s remains)
INFO - root - 2017-12-05 12:38:10.945487: step 8590, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 78h:17m:01s remains)
INFO - root - 2017-12-05 12:38:19.469355: step 8600, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 75h:07m:24s remains)
2017-12-05 12:38:20.168820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2594514 -4.261363 -4.2602897 -4.2614336 -4.2622056 -4.2611418 -4.2580662 -4.2550526 -4.2537551 -4.2533917 -4.2544937 -4.2557821 -4.2551928 -4.2525864 -4.2436891][-4.2692671 -4.2715387 -4.2730865 -4.277534 -4.2797728 -4.2779903 -4.2728109 -4.2676735 -4.2653093 -4.2649055 -4.266777 -4.2701035 -4.2703366 -4.2668333 -4.256351][-4.27902 -4.2820797 -4.2877626 -4.2969365 -4.3003397 -4.2973151 -4.2892876 -4.282114 -4.2815342 -4.2828221 -4.2855959 -4.28826 -4.2874079 -4.2823515 -4.2716475][-4.2824497 -4.2826123 -4.2876272 -4.297123 -4.2991934 -4.2921968 -4.2782931 -4.2700152 -4.2772455 -4.2890062 -4.2967119 -4.2992978 -4.2965264 -4.2897305 -4.2801328][-4.2619357 -4.2582812 -4.2612824 -4.2666636 -4.2620978 -4.24244 -4.2140875 -4.199131 -4.2191153 -4.2522926 -4.2731953 -4.2800851 -4.2803364 -4.2781816 -4.2742553][-4.2100768 -4.21044 -4.2224164 -4.2336617 -4.2232409 -4.1830215 -4.1252174 -4.087389 -4.1149774 -4.1737795 -4.2119656 -4.2278824 -4.2358022 -4.2408695 -4.2447891][-4.1595464 -4.1603112 -4.1789188 -4.1977997 -4.1859937 -4.1254454 -4.0308909 -3.95527 -3.9823186 -4.0673709 -4.1265292 -4.1561809 -4.1761875 -4.1901097 -4.2020578][-4.1441278 -4.1364484 -4.1504226 -4.1694326 -4.1575289 -4.0904856 -3.9827952 -3.8933377 -3.9148071 -4.00742 -4.0781569 -4.1184406 -4.1479359 -4.1670389 -4.18098][-4.1422787 -4.1310859 -4.1367412 -4.1497703 -4.1478534 -4.1068792 -4.0377483 -3.9818494 -3.9921763 -4.0553889 -4.1086993 -4.1404614 -4.1630554 -4.1764016 -4.1861348][-4.1415482 -4.1301193 -4.1294141 -4.1329107 -4.134172 -4.1229315 -4.0967407 -4.0729027 -4.0755415 -4.1082516 -4.1427031 -4.1618657 -4.1737633 -4.1762815 -4.1815944][-4.1253109 -4.1179624 -4.1147594 -4.1111488 -4.1143394 -4.1220088 -4.1219559 -4.11601 -4.1171794 -4.1372852 -4.1628308 -4.1776938 -4.1840506 -4.1806512 -4.1822643][-4.1069088 -4.1074023 -4.1087303 -4.1039443 -4.1132836 -4.133832 -4.143424 -4.1404214 -4.1407342 -4.1565809 -4.1779723 -4.1949644 -4.2011476 -4.1987267 -4.1990151][-4.1265936 -4.1284895 -4.1310811 -4.1266317 -4.1348567 -4.1545382 -4.1636477 -4.1601777 -4.1602044 -4.1721735 -4.1901803 -4.2051411 -4.2116232 -4.21344 -4.21902][-4.1826534 -4.1801562 -4.1778188 -4.1724129 -4.1770091 -4.1886964 -4.1927533 -4.1862893 -4.1834121 -4.1879392 -4.1978006 -4.2070265 -4.213294 -4.221046 -4.2326026][-4.2327657 -4.2287617 -4.2214813 -4.2153826 -4.2145829 -4.2158794 -4.2131295 -4.2052879 -4.2006536 -4.2007704 -4.2047477 -4.2095833 -4.2167764 -4.2290082 -4.2441278]]...]
INFO - root - 2017-12-05 12:38:28.570939: step 8610, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 78h:44m:12s remains)
INFO - root - 2017-12-05 12:38:36.949617: step 8620, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 75h:08m:41s remains)
INFO - root - 2017-12-05 12:38:45.577211: step 8630, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:55m:09s remains)
INFO - root - 2017-12-05 12:38:54.131013: step 8640, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 77h:04m:24s remains)
INFO - root - 2017-12-05 12:39:02.567132: step 8650, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 76h:05m:15s remains)
INFO - root - 2017-12-05 12:39:11.060918: step 8660, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 76h:10m:25s remains)
INFO - root - 2017-12-05 12:39:19.545471: step 8670, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 77h:37m:51s remains)
INFO - root - 2017-12-05 12:39:28.039502: step 8680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 76h:14m:16s remains)
INFO - root - 2017-12-05 12:39:36.457537: step 8690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 78h:04m:35s remains)
INFO - root - 2017-12-05 12:39:44.942813: step 8700, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.848 sec/batch; 76h:17m:54s remains)
2017-12-05 12:39:45.787228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2500124 -4.2484989 -4.2471714 -4.2469573 -4.24732 -4.24884 -4.2505116 -4.25222 -4.2551527 -4.2584748 -4.2607117 -4.2636032 -4.265481 -4.26484 -4.264101][-4.2277451 -4.2243586 -4.2210307 -4.2209487 -4.221982 -4.22445 -4.2262425 -4.2289844 -4.2346206 -4.2400341 -4.2443848 -4.2499523 -4.2540746 -4.2538433 -4.2540526][-4.2020769 -4.1965361 -4.190691 -4.1877928 -4.1866226 -4.1879444 -4.19052 -4.1955147 -4.2034087 -4.2104321 -4.2179246 -4.2282572 -4.2367578 -4.2388334 -4.2412081][-4.1830897 -4.1751037 -4.1621413 -4.151701 -4.14298 -4.1348562 -4.1328745 -4.1405644 -4.1551356 -4.1713452 -4.1874294 -4.2068176 -4.2237649 -4.229423 -4.2331891][-4.1666927 -4.15165 -4.128624 -4.1071024 -4.0851355 -4.0592871 -4.0481339 -4.0597658 -4.08508 -4.1156964 -4.1469016 -4.1822023 -4.2115946 -4.223001 -4.2290215][-4.1471043 -4.1178384 -4.0827217 -4.04762 -4.0056934 -3.9576855 -3.9377253 -3.9597695 -3.9977219 -4.040091 -4.0884075 -4.1419778 -4.1859646 -4.205575 -4.2187438][-4.1278877 -4.083878 -4.0368323 -3.9880478 -3.9228592 -3.8487644 -3.8261142 -3.8723547 -3.9293509 -3.9808934 -4.0382748 -4.0996814 -4.1504979 -4.1779084 -4.2024088][-4.1068897 -4.0536351 -4.0001693 -3.9466958 -3.8755486 -3.794929 -3.7771831 -3.836565 -3.9026532 -3.9548717 -4.0067749 -4.061377 -4.1080384 -4.1387806 -4.1722589][-4.1053209 -4.0557632 -4.0085936 -3.9679458 -3.9189286 -3.866262 -3.857605 -3.8918772 -3.9276266 -3.9580102 -3.9897356 -4.0242286 -4.0594316 -4.0898256 -4.1319175][-4.1278591 -4.0930734 -4.06 -4.0325847 -4.0027666 -3.9770517 -3.9784946 -3.9902544 -3.9923778 -3.994766 -4.0001531 -4.0088105 -4.0272021 -4.0529122 -4.0993342][-4.1621604 -4.1368623 -4.113656 -4.0974455 -4.0792518 -4.0659118 -4.0725031 -4.0730224 -4.0553741 -4.0318251 -4.0161653 -4.0119991 -4.0229826 -4.0428357 -4.0848708][-4.1955786 -4.1706066 -4.1471462 -4.1314969 -4.1151304 -4.1063013 -4.1132388 -4.1121964 -4.0933542 -4.0596147 -4.0318713 -4.025485 -4.036437 -4.0546532 -4.0877609][-4.2213407 -4.191606 -4.1609421 -4.1391139 -4.1231055 -4.1168342 -4.1231303 -4.1235156 -4.1160226 -4.0922728 -4.0656538 -4.0552249 -4.0587673 -4.0740924 -4.0951018][-4.2280016 -4.1931496 -4.1532683 -4.125073 -4.108005 -4.1032267 -4.1070838 -4.1063771 -4.1102023 -4.1064124 -4.0939441 -4.0824556 -4.0805717 -4.091022 -4.0964627][-4.2142539 -4.1766768 -4.1342487 -4.1041765 -4.087254 -4.0823026 -4.08328 -4.0805264 -4.0858955 -4.0916958 -4.0899286 -4.0864124 -4.0915494 -4.1017995 -4.0917439]]...]
INFO - root - 2017-12-05 12:39:54.073021: step 8710, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.804 sec/batch; 72h:19m:29s remains)
INFO - root - 2017-12-05 12:40:02.498928: step 8720, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 73h:29m:40s remains)
INFO - root - 2017-12-05 12:40:11.009661: step 8730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 75h:24m:29s remains)
INFO - root - 2017-12-05 12:40:19.587209: step 8740, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 76h:26m:26s remains)
INFO - root - 2017-12-05 12:40:28.129445: step 8750, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 77h:11m:02s remains)
INFO - root - 2017-12-05 12:40:36.711473: step 8760, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:55m:49s remains)
INFO - root - 2017-12-05 12:40:45.196737: step 8770, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 73h:10m:07s remains)
INFO - root - 2017-12-05 12:40:53.845849: step 8780, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 76h:10m:39s remains)
INFO - root - 2017-12-05 12:41:02.455255: step 8790, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.872 sec/batch; 78h:22m:59s remains)
INFO - root - 2017-12-05 12:41:10.905020: step 8800, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 76h:35m:14s remains)
2017-12-05 12:41:11.648157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1642165 -4.1896691 -4.1956792 -4.1839437 -4.1636667 -4.1350474 -4.1191378 -4.1224737 -4.139195 -4.1551991 -4.1794624 -4.2109203 -4.2330718 -4.24201 -4.2416539][-4.1472383 -4.1832647 -4.1973577 -4.1965456 -4.186974 -4.163785 -4.1438851 -4.1390514 -4.1486306 -4.1608167 -4.1801629 -4.2041516 -4.2212105 -4.2281165 -4.2297535][-4.1366868 -4.1781149 -4.1995263 -4.2037177 -4.1985431 -4.1792417 -4.1569657 -4.1454864 -4.1477633 -4.1584244 -4.1722746 -4.1884332 -4.2017474 -4.2110105 -4.215034][-4.1426382 -4.182415 -4.2030821 -4.2048388 -4.193902 -4.1715608 -4.14771 -4.1322169 -4.1279163 -4.137187 -4.1485229 -4.1608114 -4.178844 -4.1955552 -4.2028327][-4.1640329 -4.1947103 -4.2071705 -4.1995773 -4.1756005 -4.1423984 -4.1101141 -4.0859041 -4.0772343 -4.0932865 -4.11329 -4.1320057 -4.1597075 -4.1832213 -4.1920919][-4.193954 -4.2179127 -4.2188334 -4.1966662 -4.1540151 -4.1028495 -4.050755 -4.0073886 -3.9965444 -4.0343685 -4.0786805 -4.1161809 -4.1510448 -4.1757789 -4.188755][-4.2179656 -4.2360082 -4.2280049 -4.1960225 -4.1423435 -4.07332 -3.9868953 -3.9084108 -3.8935769 -3.9701705 -4.0576859 -4.1208167 -4.1573429 -4.1769519 -4.1872144][-4.2323804 -4.245183 -4.2320065 -4.1987081 -4.1436095 -4.0608149 -3.9406085 -3.8212311 -3.8085933 -3.9366155 -4.0659976 -4.142591 -4.1746492 -4.187252 -4.1892562][-4.2334394 -4.2444415 -4.2355628 -4.2100906 -4.1597571 -4.0700459 -3.9347479 -3.8090019 -3.8131304 -3.962425 -4.0966768 -4.1667786 -4.1922622 -4.1988978 -4.1952014][-4.2196784 -4.2330275 -4.2381096 -4.2275233 -4.187254 -4.1060863 -3.9873021 -3.8956571 -3.915709 -4.0325475 -4.1327343 -4.1818585 -4.2011123 -4.203784 -4.1982694][-4.1975379 -4.2141695 -4.2342935 -4.2414055 -4.2147393 -4.1529531 -4.0679135 -4.01511 -4.0343227 -4.1068082 -4.1656508 -4.1937237 -4.2044063 -4.2029781 -4.1982317][-4.177659 -4.1944718 -4.2240281 -4.2454176 -4.2355676 -4.195714 -4.1413493 -4.1078711 -4.1182041 -4.1609826 -4.196909 -4.2142444 -4.2166204 -4.2111578 -4.2067533][-4.1673822 -4.1839314 -4.2154722 -4.2434969 -4.2506208 -4.2294369 -4.1934624 -4.1645904 -4.1651368 -4.1934752 -4.2188945 -4.2306805 -4.230257 -4.2264385 -4.2244225][-4.1655846 -4.1834583 -4.2125425 -4.2410851 -4.2588882 -4.2519822 -4.2261958 -4.1975412 -4.1886425 -4.2082582 -4.226325 -4.2337432 -4.2342324 -4.2325215 -4.2293053][-4.1839895 -4.2001915 -4.2227025 -4.2434907 -4.2618771 -4.2638474 -4.2439289 -4.2134585 -4.1958833 -4.2044125 -4.2182717 -4.2258568 -4.2253222 -4.21835 -4.2091274]]...]
INFO - root - 2017-12-05 12:41:20.040610: step 8810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:12m:45s remains)
INFO - root - 2017-12-05 12:41:28.548697: step 8820, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 79h:29m:18s remains)
INFO - root - 2017-12-05 12:41:37.049039: step 8830, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 76h:01m:37s remains)
INFO - root - 2017-12-05 12:41:45.617900: step 8840, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 75h:25m:03s remains)
INFO - root - 2017-12-05 12:41:54.138789: step 8850, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 77h:46m:28s remains)
INFO - root - 2017-12-05 12:42:02.746113: step 8860, loss = 2.04, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 72h:51m:34s remains)
INFO - root - 2017-12-05 12:42:11.155003: step 8870, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.824 sec/batch; 74h:05m:34s remains)
INFO - root - 2017-12-05 12:42:19.707567: step 8880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:56m:48s remains)
INFO - root - 2017-12-05 12:42:28.314852: step 8890, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 75h:28m:55s remains)
INFO - root - 2017-12-05 12:42:36.987287: step 8900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 76h:25m:08s remains)
2017-12-05 12:42:37.738707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1358781 -4.133914 -4.1402106 -4.1502252 -4.1648593 -4.17726 -4.1813688 -4.1692719 -4.1646261 -4.17445 -4.1782069 -4.1762109 -4.1617727 -4.1357183 -4.1307635][-4.0872917 -4.0987177 -4.1256571 -4.1499972 -4.1787686 -4.2021031 -4.2063937 -4.1930356 -4.187212 -4.1938829 -4.19255 -4.17816 -4.1404819 -4.0973835 -4.08613][-4.0229063 -4.0551271 -4.1080809 -4.1588669 -4.1985993 -4.2230697 -4.2176991 -4.1949048 -4.1866188 -4.1894631 -4.1830468 -4.1590734 -4.1081629 -4.059165 -4.0471916][-3.9725657 -4.0279202 -4.1105223 -4.177031 -4.211369 -4.2211809 -4.19225 -4.1485763 -4.1375313 -4.1472516 -4.1456647 -4.1259861 -4.0840607 -4.0505667 -4.046979][-4.0154757 -4.0790167 -4.156354 -4.2020006 -4.20794 -4.188489 -4.1283646 -4.0596075 -4.053946 -4.0861506 -4.1003642 -4.100728 -4.0896397 -4.0825233 -4.091167][-4.1022048 -4.1577168 -4.2049026 -4.216156 -4.1872358 -4.1300807 -4.035943 -3.9493051 -3.9661458 -4.038497 -4.0835972 -4.1081028 -4.1266117 -4.1341295 -4.14769][-4.1505604 -4.1878567 -4.2044353 -4.1803651 -4.10879 -4.0139489 -3.8918724 -3.7956338 -3.8592925 -3.9846106 -4.0642314 -4.1128278 -4.1484089 -4.1562371 -4.1590757][-4.1739964 -4.1878519 -4.1723785 -4.118938 -4.0169287 -3.9065874 -3.7800267 -3.693924 -3.8043509 -3.9630413 -4.0544643 -4.1116447 -4.1540251 -4.1528797 -4.1396275][-4.1954684 -4.1890664 -4.1474328 -4.0803013 -3.984935 -3.8989987 -3.8231037 -3.790272 -3.8902297 -4.0251684 -4.0973725 -4.142251 -4.1781363 -4.1719189 -4.1551838][-4.1904526 -4.1712828 -4.1214185 -4.0580211 -3.9859498 -3.9360774 -3.9096379 -3.9172857 -3.9926751 -4.0920625 -4.1425557 -4.1730757 -4.1990137 -4.1941047 -4.181416][-4.1844473 -4.158967 -4.1110864 -4.0589671 -4.0093379 -3.9862862 -3.991591 -4.023365 -4.0796056 -4.1433454 -4.176569 -4.1941895 -4.2077641 -4.2032647 -4.1943073][-4.2015047 -4.1745152 -4.1379628 -4.109117 -4.0839562 -4.0782871 -4.0986466 -4.1347356 -4.1695685 -4.2032132 -4.22206 -4.2279549 -4.2296581 -4.2266049 -4.22202][-4.2405944 -4.2153478 -4.1913481 -4.1796503 -4.1715593 -4.1744976 -4.1954112 -4.2244382 -4.2419033 -4.2552996 -4.2653408 -4.2677045 -4.2656727 -4.2639346 -4.2644734][-4.2784872 -4.2593827 -4.2438364 -4.240108 -4.2397981 -4.2452645 -4.2632284 -4.28297 -4.2907395 -4.2940636 -4.2984862 -4.298646 -4.2959528 -4.2964258 -4.298][-4.2991347 -4.287601 -4.2797046 -4.2799349 -4.282548 -4.2867064 -4.2975731 -4.3074055 -4.3099341 -4.309948 -4.3114476 -4.3107352 -4.308013 -4.3087573 -4.3095722]]...]
INFO - root - 2017-12-05 12:42:46.112189: step 8910, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 78h:04m:41s remains)
INFO - root - 2017-12-05 12:42:54.645608: step 8920, loss = 2.04, batch loss = 1.98 (10.3 examples/sec; 0.776 sec/batch; 69h:46m:14s remains)
INFO - root - 2017-12-05 12:43:03.229029: step 8930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 77h:42m:37s remains)
INFO - root - 2017-12-05 12:43:11.702635: step 8940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:51m:24s remains)
INFO - root - 2017-12-05 12:43:20.218679: step 8950, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 77h:44m:20s remains)
INFO - root - 2017-12-05 12:43:28.827496: step 8960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 77h:42m:41s remains)
INFO - root - 2017-12-05 12:43:37.453302: step 8970, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:52m:13s remains)
INFO - root - 2017-12-05 12:43:45.941747: step 8980, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:51m:03s remains)
INFO - root - 2017-12-05 12:43:54.551781: step 8990, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 74h:39m:09s remains)
INFO - root - 2017-12-05 12:44:03.175590: step 9000, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 76h:11m:59s remains)
2017-12-05 12:44:03.914575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2039819 -4.1987853 -4.1801643 -4.1624026 -4.1563077 -4.1569862 -4.16172 -4.1664529 -4.1676288 -4.1654458 -4.170023 -4.1695771 -4.1529427 -4.1252437 -4.0964279][-4.1953931 -4.1863523 -4.1685009 -4.1531744 -4.1502671 -4.1507187 -4.1504664 -4.1496525 -4.1498003 -4.1527448 -4.1636891 -4.1706805 -4.1617169 -4.1383615 -4.1115918][-4.1749706 -4.1637988 -4.1466727 -4.1342125 -4.1338167 -4.133317 -4.1316576 -4.1295147 -4.1326928 -4.1442013 -4.1615834 -4.1738267 -4.1699839 -4.1504545 -4.1237364][-4.1483188 -4.1396084 -4.1238127 -4.1137013 -4.114573 -4.1118636 -4.107049 -4.1041083 -4.1133575 -4.1382313 -4.1660991 -4.1832356 -4.1811075 -4.1646037 -4.1419644][-4.1188645 -4.1177626 -4.1054759 -4.0953321 -4.0889206 -4.0760446 -4.0607986 -4.0530982 -4.0719595 -4.1156712 -4.1587892 -4.1828561 -4.184886 -4.1739054 -4.1584363][-4.0725131 -4.0813189 -4.0710812 -4.0532956 -4.0301466 -3.998487 -3.9656789 -3.9544127 -3.9923015 -4.0622697 -4.1286736 -4.1684337 -4.1831717 -4.1815476 -4.1737738][-4.0530086 -4.0603056 -4.0468388 -4.0205421 -3.9823122 -3.9324563 -3.8762572 -3.8505831 -3.8980994 -3.9890859 -4.0743341 -4.1292653 -4.15613 -4.1593018 -4.1545][-4.0868797 -4.0869708 -4.07405 -4.0539327 -4.0202293 -3.9734457 -3.9173384 -3.8838317 -3.9151525 -3.9902625 -4.0635347 -4.1110539 -4.1319852 -4.1257486 -4.1107731][-4.1324816 -4.1284628 -4.1176147 -4.1054087 -4.0862012 -4.0606856 -4.0297856 -4.010746 -4.0308237 -4.0779548 -4.1222868 -4.1501217 -4.1576014 -4.1407905 -4.1118736][-4.16246 -4.1591763 -4.1475267 -4.1403089 -4.1320791 -4.123621 -4.1146574 -4.1130443 -4.1320138 -4.1618047 -4.1853266 -4.2006841 -4.2043147 -4.1894288 -4.161624][-4.1778393 -4.1786957 -4.1701965 -4.16413 -4.1586676 -4.1586027 -4.1637297 -4.1742334 -4.1924095 -4.2109261 -4.221262 -4.2284679 -4.2303257 -4.2198186 -4.1990829][-4.1905127 -4.1944332 -4.1872931 -4.1766782 -4.1665726 -4.1669278 -4.17776 -4.1934052 -4.2087054 -4.2198229 -4.2257237 -4.2309561 -4.23253 -4.2259297 -4.2128506][-4.2123132 -4.2158542 -4.2076521 -4.1940103 -4.1783357 -4.1735644 -4.1821322 -4.1949573 -4.2052193 -4.2096152 -4.2118826 -4.2151818 -4.2150607 -4.2099447 -4.2022405][-4.2287397 -4.2299771 -4.2247024 -4.2160215 -4.2034435 -4.1981196 -4.2025042 -4.2088137 -4.2110572 -4.2083411 -4.2057137 -4.2043986 -4.2007842 -4.1968579 -4.1934166][-4.2308841 -4.2335534 -4.2337785 -4.2319465 -4.2249207 -4.2211285 -4.2220531 -4.2241364 -4.2226419 -4.2185421 -4.2147369 -4.2116604 -4.2095604 -4.2102041 -4.2116137]]...]
INFO - root - 2017-12-05 12:44:12.460183: step 9010, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 75h:24m:33s remains)
INFO - root - 2017-12-05 12:44:20.972213: step 9020, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 77h:32m:52s remains)
INFO - root - 2017-12-05 12:44:29.419189: step 9030, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 75h:49m:50s remains)
INFO - root - 2017-12-05 12:44:37.989231: step 9040, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 77h:17m:52s remains)
INFO - root - 2017-12-05 12:44:46.395067: step 9050, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 75h:16m:27s remains)
INFO - root - 2017-12-05 12:44:55.069666: step 9060, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.883 sec/batch; 79h:21m:20s remains)
INFO - root - 2017-12-05 12:45:03.646252: step 9070, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 75h:40m:08s remains)
INFO - root - 2017-12-05 12:45:12.273159: step 9080, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 77h:14m:37s remains)
INFO - root - 2017-12-05 12:45:20.772821: step 9090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 78h:03m:43s remains)
INFO - root - 2017-12-05 12:45:29.418142: step 9100, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.811 sec/batch; 72h:53m:18s remains)
2017-12-05 12:45:30.131269: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3006406 -4.2855697 -4.2649474 -4.2343979 -4.2034144 -4.1817942 -4.1786494 -4.1816378 -4.195 -4.2133942 -4.2214007 -4.2139359 -4.2041779 -4.2043357 -4.2040796][-4.2983336 -4.2787714 -4.253705 -4.2234478 -4.1949606 -4.1796236 -4.1807022 -4.1848345 -4.1946797 -4.2072425 -4.2139187 -4.2162623 -4.22084 -4.2267904 -4.2248425][-4.2947474 -4.2697697 -4.2387371 -4.2031479 -4.1746273 -4.1605864 -4.1662474 -4.1784225 -4.1925111 -4.2012568 -4.2029247 -4.211504 -4.22684 -4.2375379 -4.2321095][-4.2933593 -4.2652845 -4.2295609 -4.1841908 -4.1517582 -4.1379285 -4.1465745 -4.165453 -4.1870456 -4.1964097 -4.1958094 -4.2013268 -4.2163224 -4.2282195 -4.2270474][-4.2932329 -4.2631488 -4.2238293 -4.1709108 -4.1301188 -4.1119347 -4.1191816 -4.1428857 -4.1735697 -4.1917686 -4.1963224 -4.1993747 -4.2070961 -4.2153206 -4.2216134][-4.2956576 -4.2642813 -4.2208977 -4.1617785 -4.1070032 -4.0768447 -4.0701118 -4.0884233 -4.1335154 -4.1780124 -4.1992984 -4.2038937 -4.2071095 -4.2085938 -4.2126288][-4.3035045 -4.2744441 -4.230628 -4.16899 -4.1013093 -4.049108 -4.0098553 -4.0044551 -4.0669541 -4.1500363 -4.1964531 -4.20843 -4.21181 -4.2091718 -4.2052989][-4.313067 -4.2892904 -4.2500334 -4.1878643 -4.1133451 -4.0397863 -3.959115 -3.9105222 -3.986923 -4.1086483 -4.181108 -4.2074523 -4.21818 -4.2155709 -4.2045455][-4.3207655 -4.3037734 -4.2734814 -4.2160935 -4.1430559 -4.0664992 -3.9762897 -3.9104588 -3.9662423 -4.0815964 -4.1598129 -4.19988 -4.2225795 -4.2235246 -4.2098088][-4.327734 -4.3160992 -4.2957821 -4.2481074 -4.1848388 -4.1239963 -4.0605121 -4.0121164 -4.0278807 -4.0937185 -4.1508193 -4.1933179 -4.2256718 -4.23412 -4.2194963][-4.3303857 -4.3224082 -4.3080254 -4.2670479 -4.2141829 -4.1696744 -4.1323676 -4.1032591 -4.0954714 -4.1213121 -4.1556578 -4.190455 -4.2264094 -4.2428837 -4.2292194][-4.3294749 -4.3221159 -4.3067341 -4.2688446 -4.2231922 -4.1902328 -4.1658292 -4.1471419 -4.1370897 -4.1482992 -4.1686831 -4.1917143 -4.2224565 -4.2411323 -4.228126][-4.3291044 -4.3216767 -4.3039827 -4.2664728 -4.2236571 -4.1940742 -4.1731038 -4.159863 -4.1606522 -4.175148 -4.1909375 -4.2066603 -4.2255349 -4.2337494 -4.2179246][-4.3290305 -4.3231378 -4.3051686 -4.2698355 -4.2316012 -4.2024179 -4.1828003 -4.1757231 -4.1865768 -4.2043133 -4.217319 -4.2294722 -4.2386589 -4.2343388 -4.214613][-4.3290505 -4.3249083 -4.312748 -4.2876272 -4.2575345 -4.2332072 -4.2183628 -4.2150059 -4.227634 -4.2435441 -4.2530055 -4.2620225 -4.265594 -4.2569757 -4.2418]]...]
INFO - root - 2017-12-05 12:45:38.561258: step 9110, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 77h:34m:37s remains)
INFO - root - 2017-12-05 12:45:47.161450: step 9120, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 78h:45m:13s remains)
INFO - root - 2017-12-05 12:45:55.666312: step 9130, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 76h:08m:25s remains)
INFO - root - 2017-12-05 12:46:04.134878: step 9140, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 77h:22m:37s remains)
INFO - root - 2017-12-05 12:46:12.748191: step 9150, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 78h:37m:28s remains)
INFO - root - 2017-12-05 12:46:21.315134: step 9160, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 76h:02m:16s remains)
INFO - root - 2017-12-05 12:46:29.854486: step 9170, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 78h:26m:41s remains)
INFO - root - 2017-12-05 12:46:38.419395: step 9180, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 75h:51m:41s remains)
INFO - root - 2017-12-05 12:46:46.972859: step 9190, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 78h:05m:37s remains)
INFO - root - 2017-12-05 12:46:55.633488: step 9200, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 79h:02m:27s remains)
2017-12-05 12:46:56.361338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.293941 -4.271121 -4.2544088 -4.2433643 -4.2316279 -4.227694 -4.23733 -4.252593 -4.2741933 -4.2857981 -4.2857413 -4.2831049 -4.2796144 -4.2781668 -4.2808337][-4.2865229 -4.2567916 -4.2327642 -4.2098231 -4.1841884 -4.169013 -4.1768112 -4.1986518 -4.2344851 -4.258563 -4.2614179 -4.2592111 -4.2540364 -4.249938 -4.25157][-4.2789912 -4.2405062 -4.2028875 -4.160749 -4.1179986 -4.0891504 -4.0895329 -4.1166544 -4.1715736 -4.2144251 -4.2246375 -4.2262959 -4.2253551 -4.2222085 -4.22865][-4.2709351 -4.2241907 -4.1762547 -4.1258526 -4.0812864 -4.044076 -4.02381 -4.0472021 -4.1162653 -4.1739569 -4.1930571 -4.1974592 -4.1999512 -4.1990528 -4.20973][-4.2660437 -4.2165914 -4.1647749 -4.1169467 -4.0705037 -4.0132895 -3.9579697 -3.9691553 -4.0561476 -4.1285954 -4.1605754 -4.1710691 -4.1751561 -4.1745715 -4.1894245][-4.2647562 -4.2155619 -4.1590948 -4.1075921 -4.0524621 -3.9736416 -3.88455 -3.8781016 -3.97923 -4.0743017 -4.1236587 -4.1433396 -4.152318 -4.1538453 -4.1735811][-4.25986 -4.2088118 -4.1478329 -4.092957 -4.0255146 -3.9195108 -3.7842016 -3.7405884 -3.8630514 -3.9998279 -4.0769267 -4.1100736 -4.1279049 -4.1358614 -4.161901][-4.2500062 -4.1959662 -4.134553 -4.0738087 -3.9854763 -3.852788 -3.6798348 -3.6073182 -3.7557974 -3.9282904 -4.0260835 -4.0719943 -4.1010547 -4.121016 -4.1556525][-4.2396626 -4.1801734 -4.1131811 -4.0441432 -3.9502814 -3.8296909 -3.7040582 -3.6760206 -3.80498 -3.9443088 -4.0249329 -4.063663 -4.0875516 -4.1098514 -4.1455655][-4.23119 -4.1753659 -4.1092281 -4.0450206 -3.9694571 -3.8858156 -3.828532 -3.8424985 -3.9300468 -4.0140371 -4.0643125 -4.0850449 -4.0953593 -4.1122618 -4.1389542][-4.2343779 -4.1897912 -4.1346469 -4.0817442 -4.0251489 -3.966568 -3.9412773 -3.9713557 -4.03161 -4.0749536 -4.099534 -4.1106162 -4.1189351 -4.1281543 -4.143806][-4.2463789 -4.2115836 -4.1662469 -4.12266 -4.0773792 -4.0333538 -4.0243521 -4.0569463 -4.0917349 -4.1078944 -4.1191535 -4.128438 -4.13907 -4.1449146 -4.1548829][-4.267364 -4.2394423 -4.2028322 -4.1664147 -4.1294675 -4.1003962 -4.0993943 -4.1212096 -4.1336441 -4.1338243 -4.1374078 -4.1484709 -4.1643758 -4.1708703 -4.1811585][-4.293807 -4.2725773 -4.2448106 -4.2148829 -4.1882253 -4.1723189 -4.1744337 -4.1882625 -4.1933603 -4.1887221 -4.185719 -4.1914234 -4.20414 -4.2094007 -4.2175603][-4.3164005 -4.2990818 -4.2791533 -4.2587376 -4.2409258 -4.2338243 -4.2399182 -4.2517796 -4.2578168 -4.2547622 -4.2493215 -4.2492709 -4.2549124 -4.2585139 -4.2640395]]...]
INFO - root - 2017-12-05 12:47:04.883677: step 9210, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 75h:57m:06s remains)
INFO - root - 2017-12-05 12:47:13.430649: step 9220, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 76h:57m:20s remains)
INFO - root - 2017-12-05 12:47:21.938911: step 9230, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 76h:00m:21s remains)
INFO - root - 2017-12-05 12:47:30.371926: step 9240, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.820 sec/batch; 73h:35m:27s remains)
INFO - root - 2017-12-05 12:47:38.889541: step 9250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 76h:30m:29s remains)
INFO - root - 2017-12-05 12:47:47.535871: step 9260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:41m:36s remains)
INFO - root - 2017-12-05 12:47:56.189793: step 9270, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 78h:09m:54s remains)
INFO - root - 2017-12-05 12:48:04.682858: step 9280, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 77h:29m:53s remains)
INFO - root - 2017-12-05 12:48:13.210709: step 9290, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 76h:29m:42s remains)
INFO - root - 2017-12-05 12:48:21.768615: step 9300, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 73h:31m:38s remains)
2017-12-05 12:48:22.517672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2270894 -4.2277775 -4.2196531 -4.2089028 -4.1963038 -4.1861095 -4.1791477 -4.1738644 -4.182116 -4.2090659 -4.2328596 -4.2455025 -4.2432384 -4.2320342 -4.2245784][-4.2074771 -4.2040486 -4.193409 -4.1800461 -4.1653495 -4.1544385 -4.149756 -4.1469235 -4.1557746 -4.1856713 -4.2126689 -4.2294226 -4.2319317 -4.2261558 -4.2230473][-4.1772575 -4.1705079 -4.15735 -4.14153 -4.12472 -4.1158128 -4.1186237 -4.1215587 -4.1327376 -4.1655474 -4.1959844 -4.2151217 -4.2218685 -4.2201266 -4.2197142][-4.1528673 -4.1449933 -4.1297526 -4.1122026 -4.095614 -4.09014 -4.0988216 -4.1049089 -4.1166463 -4.1498814 -4.1823707 -4.203969 -4.2135825 -4.2152791 -4.2165108][-4.1423707 -4.1390991 -4.1271586 -4.1126175 -4.1005793 -4.0971951 -4.1038933 -4.1042871 -4.1107211 -4.1411357 -4.1739192 -4.1969247 -4.210259 -4.2156591 -4.2177949][-4.1395488 -4.1404653 -4.1308141 -4.1171989 -4.1042132 -4.0968671 -4.0981364 -4.0922189 -4.0959249 -4.1265287 -4.1627107 -4.1886339 -4.2050304 -4.2141619 -4.2173505][-4.1225996 -4.1285753 -4.1189742 -4.1026516 -4.0860415 -4.0754566 -4.0750427 -4.0700312 -4.0787172 -4.1137381 -4.1529689 -4.1822753 -4.2003794 -4.2110286 -4.2159963][-4.0851717 -4.097496 -4.089179 -4.0698743 -4.0492058 -4.0349016 -4.0350051 -4.0369425 -4.0545287 -4.095058 -4.1382251 -4.1716952 -4.1925535 -4.2054858 -4.2146893][-4.0645041 -4.08384 -4.0792036 -4.05903 -4.0360355 -4.0206556 -4.0211015 -4.0296054 -4.0536318 -4.09607 -4.1396866 -4.1737852 -4.1945853 -4.2071071 -4.2176461][-4.0705295 -4.0930405 -4.0943022 -4.0791917 -4.0591693 -4.0453396 -4.0462437 -4.0568905 -4.0815678 -4.1212511 -4.1620564 -4.1949205 -4.2125 -4.2199855 -4.2265382][-4.0790267 -4.0974951 -4.1004953 -4.0908632 -4.0756912 -4.065712 -4.0678024 -4.0790906 -4.1026549 -4.1403241 -4.1807575 -4.2150478 -4.2311797 -4.2332869 -4.235168][-4.0778837 -4.0922265 -4.0970931 -4.0936618 -4.0860033 -4.0817103 -4.0849729 -4.09419 -4.1138144 -4.1489315 -4.1889648 -4.2228427 -4.2369642 -4.2358685 -4.2357][-4.0717144 -4.0846677 -4.0919003 -4.0914822 -4.08887 -4.0885272 -4.0898266 -4.090291 -4.1017351 -4.1325622 -4.1725793 -4.2082524 -4.2242918 -4.2253175 -4.2268081][-4.0758929 -4.0837073 -4.0865293 -4.0829468 -4.0789227 -4.0776978 -4.0737472 -4.0652833 -4.07054 -4.0986028 -4.1407309 -4.1804738 -4.2023864 -4.2098136 -4.2157369][-4.0888782 -4.0922084 -4.0914845 -4.0858188 -4.0792308 -4.0760589 -4.0717592 -4.0617461 -4.0641618 -4.0900092 -4.1302524 -4.1681752 -4.1909213 -4.20204 -4.210278]]...]
INFO - root - 2017-12-05 12:48:30.945549: step 9310, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 74h:07m:38s remains)
INFO - root - 2017-12-05 12:48:39.413054: step 9320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 75h:29m:03s remains)
INFO - root - 2017-12-05 12:48:48.074712: step 9330, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 78h:14m:02s remains)
INFO - root - 2017-12-05 12:48:56.597670: step 9340, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 76h:29m:19s remains)
INFO - root - 2017-12-05 12:49:05.088209: step 9350, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 77h:52m:04s remains)
INFO - root - 2017-12-05 12:49:13.741427: step 9360, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 80h:26m:07s remains)
INFO - root - 2017-12-05 12:49:22.275510: step 9370, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 0.851 sec/batch; 76h:23m:16s remains)
INFO - root - 2017-12-05 12:49:30.834356: step 9380, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 78h:03m:16s remains)
INFO - root - 2017-12-05 12:49:39.410209: step 9390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 77h:40m:23s remains)
INFO - root - 2017-12-05 12:49:48.000091: step 9400, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 77h:28m:55s remains)
2017-12-05 12:49:48.821416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1644816 -4.1692815 -4.18625 -4.1961679 -4.1907325 -4.1892085 -4.1997175 -4.2219143 -4.2342553 -4.19486 -4.1182365 -4.077704 -4.1031075 -4.1432266 -4.1846981][-4.1630983 -4.1690826 -4.2008691 -4.2244558 -4.2210789 -4.216032 -4.2192407 -4.2378645 -4.249012 -4.20677 -4.1251349 -4.0800257 -4.1090136 -4.1553755 -4.2017674][-4.1844382 -4.1947331 -4.2330446 -4.25404 -4.2479172 -4.2406492 -4.2423916 -4.2554789 -4.2611003 -4.2198467 -4.1378255 -4.0876374 -4.1167507 -4.1663227 -4.2141352][-4.2139134 -4.2218914 -4.2567525 -4.2688732 -4.2598443 -4.2575035 -4.26443 -4.2692013 -4.264915 -4.2246284 -4.14403 -4.0908833 -4.1207333 -4.1759996 -4.2228203][-4.2273941 -4.2359924 -4.2613997 -4.2597065 -4.2457705 -4.2530074 -4.2672868 -4.2672262 -4.2486548 -4.204133 -4.1270885 -4.0768189 -4.1132073 -4.1728449 -4.2141719][-4.2165003 -4.2242603 -4.233139 -4.2103739 -4.1861639 -4.1973529 -4.2227893 -4.224463 -4.1943045 -4.1454353 -4.0779567 -4.0396328 -4.0874791 -4.150414 -4.1854458][-4.1715517 -4.1732831 -4.1657052 -4.1180849 -4.0759954 -4.0836797 -4.1140075 -4.1089568 -4.0607624 -4.0078 -3.9587045 -3.9457493 -4.0111027 -4.0908647 -4.1391864][-4.109848 -4.0999184 -4.0736761 -4.0001912 -3.9346385 -3.93786 -3.9714479 -3.9538441 -3.883966 -3.8350239 -3.8211076 -3.84511 -3.9293246 -4.0257921 -4.0925226][-4.0649714 -4.044908 -4.0062203 -3.9296503 -3.8580422 -3.866724 -3.9081781 -3.8916245 -3.8197932 -3.7873316 -3.8067224 -3.8463049 -3.9202738 -4.00574 -4.0740547][-4.0592346 -4.0416536 -4.0032 -3.9312625 -3.8638904 -3.8747509 -3.9175348 -3.9011583 -3.83418 -3.8246071 -3.8686779 -3.9138174 -3.9765282 -4.0518856 -4.1161385][-4.0812321 -4.0714655 -4.0414138 -3.9860413 -3.9302065 -3.9322999 -3.9586358 -3.9327388 -3.8749681 -3.8862219 -3.9454107 -3.999177 -4.0609012 -4.1312571 -4.1921973][-4.1313376 -4.1335211 -4.1188083 -4.0872035 -4.0506392 -4.0415621 -4.0510836 -4.0254788 -3.9866908 -4.0020647 -4.0490227 -4.0945306 -4.1474338 -4.2059617 -4.2582207][-4.1858706 -4.1994276 -4.1974163 -4.1840196 -4.1660218 -4.1570363 -4.15709 -4.1383758 -4.1169333 -4.1229587 -4.1427135 -4.1719332 -4.2124057 -4.252965 -4.290216][-4.2349625 -4.2468605 -4.2501841 -4.2474179 -4.2418566 -4.2377663 -4.235527 -4.2263966 -4.2149734 -4.2098808 -4.2070909 -4.2187281 -4.2462931 -4.2689624 -4.2923036][-4.269691 -4.2751966 -4.2749176 -4.2737627 -4.27237 -4.2703581 -4.2674971 -4.2641087 -4.2566442 -4.2416344 -4.220799 -4.2194333 -4.2426167 -4.25934 -4.2774677]]...]
INFO - root - 2017-12-05 12:49:57.143945: step 9410, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 77h:09m:41s remains)
INFO - root - 2017-12-05 12:50:05.528006: step 9420, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 78h:47m:29s remains)
INFO - root - 2017-12-05 12:50:14.097443: step 9430, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 76h:57m:21s remains)
INFO - root - 2017-12-05 12:50:22.735260: step 9440, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 77h:49m:29s remains)
INFO - root - 2017-12-05 12:50:31.345949: step 9450, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 75h:29m:19s remains)
INFO - root - 2017-12-05 12:50:39.695463: step 9460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 77h:16m:50s remains)
INFO - root - 2017-12-05 12:50:48.229605: step 9470, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 78h:48m:24s remains)
INFO - root - 2017-12-05 12:50:56.900034: step 9480, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 79h:49m:09s remains)
INFO - root - 2017-12-05 12:51:05.463698: step 9490, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 75h:31m:15s remains)
INFO - root - 2017-12-05 12:51:14.066014: step 9500, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 75h:28m:28s remains)
2017-12-05 12:51:14.833225: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2960548 -4.2981558 -4.3009419 -4.3033433 -4.3050203 -4.306047 -4.3049293 -4.3033247 -4.3026514 -4.3019228 -4.3015203 -4.3023787 -4.3054333 -4.3101606 -4.3144808][-4.2823853 -4.2848516 -4.2868323 -4.2881279 -4.2903562 -4.2928839 -4.2914557 -4.2870655 -4.28428 -4.2811813 -4.2783847 -4.28043 -4.2877874 -4.2982354 -4.3079748][-4.2471647 -4.2523975 -4.2558379 -4.2538285 -4.2558503 -4.2591319 -4.2574353 -4.25141 -4.2462721 -4.2401743 -4.2360682 -4.2431035 -4.2593322 -4.279705 -4.2974086][-4.1993461 -4.20288 -4.2029757 -4.1939096 -4.1942139 -4.194025 -4.1876054 -4.1794333 -4.1732225 -4.1665139 -4.1665821 -4.1855497 -4.2158041 -4.2491264 -4.278017][-4.1694493 -4.1665134 -4.1619134 -4.1490927 -4.1462679 -4.1390977 -4.1223531 -4.1038666 -4.0919833 -4.0863056 -4.0959005 -4.1271472 -4.1688776 -4.2137327 -4.2529635][-4.1552281 -4.1437783 -4.1349063 -4.1236734 -4.1242032 -4.1108389 -4.0769014 -4.0354986 -4.0105414 -4.0096622 -4.0323281 -4.0760932 -4.125411 -4.17956 -4.2275019][-4.1202617 -4.0908918 -4.0724983 -4.0680594 -4.0774779 -4.0599313 -4.0106497 -3.9463117 -3.9097888 -3.9254189 -3.9703143 -4.02828 -4.0831523 -4.1449914 -4.2007675][-4.0995173 -4.0520964 -4.0191174 -4.0138154 -4.0232358 -3.9969504 -3.928339 -3.837847 -3.7851627 -3.8266339 -3.9092164 -3.9923387 -4.0572147 -4.1256509 -4.1885333][-4.1152158 -4.0614777 -4.0210037 -4.0083184 -4.0110807 -3.9802551 -3.904753 -3.8025715 -3.7457404 -3.7992704 -3.8985319 -3.9925673 -4.0634403 -4.1355853 -4.2010989][-4.1353254 -4.0889745 -4.055037 -4.039196 -4.0436234 -4.0229383 -3.9643259 -3.8834627 -3.8457537 -3.889456 -3.966809 -4.0441952 -4.1072483 -4.1705179 -4.227932][-4.1557126 -4.1186604 -4.0915718 -4.0782976 -4.0887923 -4.0790296 -4.0370579 -3.9845688 -3.9665928 -4.0018487 -4.0579433 -4.1172466 -4.1696348 -4.2187705 -4.2597885][-4.1747961 -4.1452336 -4.1234488 -4.1150928 -4.1317749 -4.1367822 -4.1192255 -4.0932341 -4.0874343 -4.1159725 -4.1570449 -4.2026434 -4.2389445 -4.2675719 -4.2894282][-4.198215 -4.1754375 -4.1631513 -4.1658459 -4.1931095 -4.2116685 -4.2131033 -4.2051139 -4.203002 -4.2180986 -4.2421141 -4.2710853 -4.2886567 -4.2999406 -4.3076339][-4.2282619 -4.2142363 -4.2109156 -4.2212443 -4.2515683 -4.2762446 -4.2883639 -4.2892365 -4.2869015 -4.2917089 -4.3026996 -4.3150406 -4.3171844 -4.3170705 -4.3178697][-4.2679377 -4.2618628 -4.2645326 -4.2758765 -4.3002391 -4.3199096 -4.3313427 -4.3337607 -4.3300109 -4.3300767 -4.3339472 -4.3363023 -4.3312411 -4.3256884 -4.3236241]]...]
INFO - root - 2017-12-05 12:51:23.269108: step 9510, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 77h:53m:43s remains)
INFO - root - 2017-12-05 12:51:31.887569: step 9520, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 73h:34m:44s remains)
INFO - root - 2017-12-05 12:51:40.419081: step 9530, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 77h:12m:34s remains)
INFO - root - 2017-12-05 12:51:48.891452: step 9540, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 77h:49m:09s remains)
INFO - root - 2017-12-05 12:51:57.500670: step 9550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 77h:31m:42s remains)
INFO - root - 2017-12-05 12:52:06.152102: step 9560, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 77h:10m:32s remains)
INFO - root - 2017-12-05 12:52:14.524949: step 9570, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 78h:54m:56s remains)
INFO - root - 2017-12-05 12:52:23.068166: step 9580, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 77h:25m:04s remains)
INFO - root - 2017-12-05 12:52:31.567375: step 9590, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 75h:50m:02s remains)
INFO - root - 2017-12-05 12:52:40.202618: step 9600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 76h:50m:00s remains)
2017-12-05 12:52:40.975739: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3263178 -4.3321266 -4.3324285 -4.3228841 -4.3001671 -4.267292 -4.2263269 -4.19659 -4.1839662 -4.1764297 -4.1705041 -4.1652942 -4.158267 -4.1303759 -4.099277][-4.3192992 -4.3245163 -4.3257828 -4.31832 -4.2959557 -4.2566171 -4.20836 -4.1742511 -4.1671696 -4.1664453 -4.1645255 -4.1646647 -4.1636124 -4.1348271 -4.092464][-4.2958016 -4.303699 -4.30927 -4.3073006 -4.2853713 -4.2408209 -4.1913538 -4.1620021 -4.1631579 -4.1677408 -4.1680188 -4.17427 -4.1795793 -4.1555963 -4.1142693][-4.259778 -4.2708392 -4.2826781 -4.2868023 -4.2693057 -4.2264047 -4.1844358 -4.166997 -4.1772623 -4.1843324 -4.184885 -4.1935654 -4.2033677 -4.1859283 -4.15368][-4.204464 -4.2168379 -4.2348876 -4.2502184 -4.244411 -4.2140307 -4.1870418 -4.18292 -4.1972351 -4.2030578 -4.2035117 -4.2151375 -4.2264476 -4.2162461 -4.1943111][-4.140213 -4.1528611 -4.1800404 -4.2094936 -4.2184906 -4.2010121 -4.1864924 -4.18727 -4.1993856 -4.2049055 -4.2084665 -4.2290716 -4.2445593 -4.2424188 -4.2299638][-4.0960412 -4.1124105 -4.1434107 -4.1782951 -4.194869 -4.1854405 -4.174016 -4.1692371 -4.1749697 -4.1813059 -4.1938806 -4.2306781 -4.2549 -4.26392 -4.2628274][-4.0929971 -4.1076527 -4.1283097 -4.1503258 -4.1629391 -4.155478 -4.1393661 -4.1255684 -4.1237698 -4.1352816 -4.1653905 -4.220705 -4.2590485 -4.2794328 -4.2863755][-4.1045442 -4.1100192 -4.1154122 -4.1202073 -4.1155877 -4.1004615 -4.0749879 -4.0556717 -4.0527463 -4.0783038 -4.1277075 -4.1961308 -4.2480178 -4.279284 -4.292726][-4.1168804 -4.1093373 -4.0970821 -4.08333 -4.0592875 -4.0324349 -4.0033793 -3.9902325 -3.9950998 -4.0379562 -4.1014919 -4.1729836 -4.227941 -4.2672062 -4.2860637][-4.1289806 -4.1158876 -4.095891 -4.0729713 -4.0436344 -4.0167947 -3.9962573 -3.9962983 -4.0061727 -4.0504408 -4.1123762 -4.17273 -4.217423 -4.2568398 -4.2789207][-4.1452856 -4.1281219 -4.1097 -4.0892186 -4.0692844 -4.05843 -4.0494065 -4.0576797 -4.0686049 -4.1035881 -4.1537395 -4.1969447 -4.2278576 -4.2588024 -4.2777333][-4.1675839 -4.1478457 -4.1350307 -4.1214662 -4.1150894 -4.11927 -4.1217737 -4.1324368 -4.1421647 -4.1658092 -4.2008982 -4.2283754 -4.247159 -4.2698488 -4.2820458][-4.194191 -4.1759405 -4.1690259 -4.163981 -4.1685944 -4.1838417 -4.1942387 -4.204845 -4.2150059 -4.2294154 -4.2491274 -4.2630229 -4.272593 -4.285923 -4.2890697][-4.226439 -4.2131968 -4.2098927 -4.2116838 -4.2228055 -4.2407217 -4.2519875 -4.2608376 -4.2717018 -4.2790284 -4.285706 -4.2911143 -4.2937689 -4.2970786 -4.2912946]]...]
INFO - root - 2017-12-05 12:52:49.521998: step 9610, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 78h:56m:38s remains)
INFO - root - 2017-12-05 12:52:58.129396: step 9620, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 79h:08m:26s remains)
INFO - root - 2017-12-05 12:53:06.612005: step 9630, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 74h:14m:23s remains)
INFO - root - 2017-12-05 12:53:15.249928: step 9640, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 77h:23m:52s remains)
INFO - root - 2017-12-05 12:53:23.793019: step 9650, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 77h:47m:50s remains)
INFO - root - 2017-12-05 12:53:32.328052: step 9660, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 79h:19m:20s remains)
INFO - root - 2017-12-05 12:53:40.800886: step 9670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:36m:38s remains)
INFO - root - 2017-12-05 12:53:49.396272: step 9680, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 78h:49m:53s remains)
INFO - root - 2017-12-05 12:53:57.994095: step 9690, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:36m:29s remains)
INFO - root - 2017-12-05 12:54:06.570659: step 9700, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 75h:43m:28s remains)
2017-12-05 12:54:07.326432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2017636 -4.2016444 -4.1951795 -4.1860123 -4.1744957 -4.1594429 -4.1598682 -4.18 -4.1972575 -4.2094626 -4.2179523 -4.2137794 -4.1922212 -4.17878 -4.1851349][-4.1935129 -4.1976514 -4.1936707 -4.1842747 -4.1726346 -4.1575441 -4.1546092 -4.1742353 -4.1976452 -4.2158928 -4.2280574 -4.2236676 -4.1950645 -4.1680555 -4.1642871][-4.1862741 -4.1934752 -4.1893992 -4.182879 -4.1697493 -4.1527848 -4.145709 -4.1660061 -4.1989927 -4.223485 -4.2388835 -4.2338357 -4.2008877 -4.166882 -4.1580296][-4.1785727 -4.1899981 -4.1848674 -4.1754642 -4.1580739 -4.1360769 -4.1253657 -4.1473541 -4.1905575 -4.2235284 -4.2415552 -4.2381816 -4.2082448 -4.1778607 -4.1687956][-4.1794548 -4.1979156 -4.194015 -4.1791549 -4.1549478 -4.124929 -4.1027131 -4.1268926 -4.1811652 -4.222815 -4.2428789 -4.2379718 -4.2103863 -4.184237 -4.173964][-4.1898274 -4.2060552 -4.1997223 -4.1821547 -4.1544061 -4.1164608 -4.0815635 -4.1086626 -4.173615 -4.2207518 -4.2413316 -4.2359266 -4.2120466 -4.1894088 -4.1751623][-4.1928892 -4.2018552 -4.1953106 -4.1761808 -4.1441631 -4.0942545 -4.0371456 -4.0680995 -4.1554828 -4.2182755 -4.2475924 -4.2449713 -4.2257981 -4.2052922 -4.1871395][-4.1772552 -4.181005 -4.1768494 -4.1552258 -4.1117878 -4.0397191 -3.9445403 -3.9769676 -4.1042657 -4.1987982 -4.2403092 -4.2424512 -4.2226567 -4.19891 -4.1786933][-4.1513109 -4.1551437 -4.1550245 -4.1364245 -4.0922356 -4.0127788 -3.8917089 -3.9011674 -4.0474072 -4.1619606 -4.2148542 -4.2250652 -4.205266 -4.17588 -4.1551056][-4.1398683 -4.153276 -4.1650329 -4.15886 -4.1271138 -4.0691652 -3.9728608 -3.9564245 -4.0586238 -4.155684 -4.20377 -4.215198 -4.1970267 -4.1714411 -4.152669][-4.1548572 -4.1765366 -4.1990466 -4.2045574 -4.1861744 -4.1504374 -4.0930829 -4.0675192 -4.1123347 -4.1745148 -4.212193 -4.2225652 -4.2115617 -4.1962023 -4.1808643][-4.1743107 -4.1979361 -4.2228007 -4.2310462 -4.2212553 -4.1954365 -4.1581182 -4.1279635 -4.1378393 -4.1743374 -4.2013607 -4.216341 -4.2168641 -4.2099991 -4.1954594][-4.1990304 -4.2196722 -4.2413883 -4.2483521 -4.2395935 -4.2115879 -4.174222 -4.1403494 -4.1358023 -4.1567655 -4.18098 -4.2033234 -4.2144027 -4.2153082 -4.20067][-4.2320781 -4.249692 -4.2656393 -4.26954 -4.2596731 -4.2320461 -4.1955504 -4.1618304 -4.15345 -4.1641412 -4.1848903 -4.2088 -4.2226539 -4.2289467 -4.2200966][-4.2686825 -4.2829928 -4.2921433 -4.2915754 -4.2825522 -4.2610569 -4.2332439 -4.2067041 -4.1990194 -4.20552 -4.2171249 -4.2333059 -4.2437925 -4.2498875 -4.2506294]]...]
INFO - root - 2017-12-05 12:54:15.763427: step 9710, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 77h:30m:04s remains)
INFO - root - 2017-12-05 12:54:24.320100: step 9720, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 73h:41m:55s remains)
INFO - root - 2017-12-05 12:54:32.884596: step 9730, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 78h:19m:50s remains)
INFO - root - 2017-12-05 12:54:41.633484: step 9740, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 76h:36m:50s remains)
INFO - root - 2017-12-05 12:54:50.207174: step 9750, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:29m:32s remains)
INFO - root - 2017-12-05 12:54:58.910746: step 9760, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.855 sec/batch; 76h:38m:29s remains)
INFO - root - 2017-12-05 12:55:07.456816: step 9770, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 73h:49m:22s remains)
INFO - root - 2017-12-05 12:55:15.995954: step 9780, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 78h:04m:42s remains)
INFO - root - 2017-12-05 12:55:24.507539: step 9790, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 74h:27m:14s remains)
INFO - root - 2017-12-05 12:55:33.091257: step 9800, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 76h:49m:57s remains)
2017-12-05 12:55:33.880649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1120076 -4.1146131 -4.1405497 -4.1658936 -4.1844974 -4.1925516 -4.1868591 -4.1657763 -4.137918 -4.1276469 -4.1447315 -4.1702728 -4.1852427 -4.1869059 -4.1788006][-4.1974511 -4.1954436 -4.2027431 -4.2125053 -4.2221355 -4.22371 -4.2120895 -4.18849 -4.1632085 -4.1557331 -4.1716394 -4.1927061 -4.2045279 -4.2005816 -4.1831098][-4.2576842 -4.2510867 -4.2463808 -4.2434783 -4.240387 -4.2294464 -4.2102947 -4.1865931 -4.1695886 -4.1719623 -4.1903324 -4.2079525 -4.2172546 -4.2089005 -4.1839781][-4.2848072 -4.2745442 -4.2620664 -4.2498226 -4.2377267 -4.2154989 -4.1880794 -4.1637416 -4.1514854 -4.1593618 -4.1787724 -4.1978784 -4.2127485 -4.2138791 -4.1957135][-4.2854295 -4.2734108 -4.2545443 -4.231987 -4.2081194 -4.1726122 -4.1351452 -4.112638 -4.1097832 -4.1248827 -4.1502757 -4.179244 -4.2084737 -4.2279458 -4.2243814][-4.2784286 -4.2669673 -4.2420235 -4.2085943 -4.1747007 -4.1317172 -4.0941286 -4.0789652 -4.0888009 -4.1148162 -4.1481524 -4.187429 -4.2268248 -4.2566872 -4.2605977][-4.2686439 -4.2536111 -4.2245469 -4.1872768 -4.1493859 -4.105011 -4.0712528 -4.0670319 -4.0890808 -4.1235142 -4.1612768 -4.2000661 -4.2405643 -4.2707763 -4.2753315][-4.2630339 -4.2443647 -4.2145481 -4.1740556 -4.130652 -4.082943 -4.0477629 -4.0464396 -4.0735493 -4.1120682 -4.1535897 -4.1930904 -4.2322674 -4.2616963 -4.265161][-4.2593412 -4.2427216 -4.2164097 -4.1776886 -4.13067 -4.0794759 -4.0389071 -4.0323548 -4.055747 -4.0947075 -4.1352792 -4.1723433 -4.2066231 -4.2363338 -4.2408628][-4.2425113 -4.2286849 -4.2066331 -4.1730471 -4.1285281 -4.0749192 -4.0304708 -4.0182285 -4.0365982 -4.0704207 -4.1042728 -4.1374621 -4.1699333 -4.2017956 -4.2121773][-4.2188225 -4.2093244 -4.1906414 -4.1614594 -4.1221876 -4.0729933 -4.035089 -4.0259848 -4.0425873 -4.0690989 -4.0917344 -4.1211996 -4.1562934 -4.1898451 -4.2018452][-4.1865292 -4.1819873 -4.1704154 -4.1530862 -4.12955 -4.0967464 -4.0722523 -4.0718331 -4.0881624 -4.1081338 -4.1210089 -4.1415172 -4.1677823 -4.1927052 -4.2004137][-4.1613665 -4.1570468 -4.1529474 -4.1543865 -4.1537352 -4.1381412 -4.124351 -4.1258507 -4.1344752 -4.1442251 -4.1465917 -4.1535535 -4.1664934 -4.1809978 -4.1866536][-4.1453781 -4.1321473 -4.1283526 -4.1447144 -4.1654077 -4.1657453 -4.1590276 -4.1565156 -4.1576414 -4.1609249 -4.1575818 -4.1544075 -4.1570621 -4.1632829 -4.1663222][-4.1318984 -4.1029134 -4.0916929 -4.1151004 -4.1486745 -4.1634541 -4.1667933 -4.1677318 -4.1688852 -4.1708212 -4.1664481 -4.1597509 -4.1542468 -4.1480742 -4.1424942]]...]
INFO - root - 2017-12-05 12:55:42.331393: step 9810, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 73h:43m:12s remains)
INFO - root - 2017-12-05 12:55:50.867200: step 9820, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 77h:22m:20s remains)
INFO - root - 2017-12-05 12:55:59.320015: step 9830, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.811 sec/batch; 72h:43m:10s remains)
INFO - root - 2017-12-05 12:56:07.797187: step 9840, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 75h:21m:51s remains)
INFO - root - 2017-12-05 12:56:16.432568: step 9850, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 77h:09m:03s remains)
INFO - root - 2017-12-05 12:56:25.010338: step 9860, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 76h:35m:59s remains)
INFO - root - 2017-12-05 12:56:33.608039: step 9870, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.827 sec/batch; 74h:08m:46s remains)
INFO - root - 2017-12-05 12:56:42.236113: step 9880, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 79h:06m:52s remains)
INFO - root - 2017-12-05 12:56:50.729388: step 9890, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 75h:13m:30s remains)
INFO - root - 2017-12-05 12:56:59.139755: step 9900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 76h:30m:27s remains)
2017-12-05 12:56:59.886517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2332606 -4.255084 -4.265914 -4.2669549 -4.2690854 -4.2693324 -4.266551 -4.26203 -4.2619519 -4.2640886 -4.2701631 -4.2677293 -4.2620673 -4.2571573 -4.2439837][-4.22923 -4.2528958 -4.2606583 -4.2582307 -4.2564068 -4.2505035 -4.2414985 -4.2350988 -4.236692 -4.241991 -4.2498541 -4.2464108 -4.237123 -4.2288256 -4.2080832][-4.2171931 -4.2383204 -4.2435837 -4.2410617 -4.2368412 -4.2294216 -4.2150712 -4.2046342 -4.2058048 -4.2118788 -4.2204356 -4.2165265 -4.2064724 -4.1936383 -4.1629968][-4.2028508 -4.214395 -4.21818 -4.2189565 -4.2180076 -4.2093244 -4.1885 -4.1695447 -4.1656513 -4.1745338 -4.1889977 -4.1937008 -4.1936522 -4.1807775 -4.1412063][-4.2007356 -4.2056041 -4.2026644 -4.2015209 -4.2003345 -4.1875029 -4.1556258 -4.1242328 -4.1167026 -4.1344924 -4.1578827 -4.1783743 -4.1891704 -4.1756687 -4.1294813][-4.195776 -4.195817 -4.1845584 -4.1757965 -4.1686864 -4.1502528 -4.114656 -4.0774918 -4.0764685 -4.1080236 -4.1382117 -4.1684875 -4.1809592 -4.1641607 -4.1137824][-4.1793947 -4.1811328 -4.1631556 -4.1457539 -4.1293006 -4.1048727 -4.0704865 -4.0434113 -4.0549688 -4.0927315 -4.123776 -4.1541438 -4.1679997 -4.1523404 -4.1067715][-4.1554446 -4.1575365 -4.1386719 -4.1213493 -4.0949235 -4.0620856 -4.0345483 -4.0271473 -4.048439 -4.0809565 -4.1084814 -4.1341491 -4.1485538 -4.1384826 -4.1065941][-4.1348753 -4.1296449 -4.1068292 -4.0878224 -4.0572352 -4.0220127 -4.0044088 -4.0098519 -4.0377316 -4.0719352 -4.095397 -4.114749 -4.1273685 -4.1237206 -4.1041203][-4.1110172 -4.0991378 -4.0713692 -4.0508032 -4.0251136 -3.9979136 -3.9854994 -3.9943206 -4.0302286 -4.0716486 -4.0934815 -4.1097469 -4.1234021 -4.1234636 -4.1056614][-4.0965514 -4.0774665 -4.0448456 -4.027842 -4.01847 -4.0091996 -4.0014987 -4.0070372 -4.0439558 -4.0853109 -4.1047173 -4.1190257 -4.1326046 -4.1342144 -4.1161194][-4.0922575 -4.0664539 -4.0281148 -4.0113339 -4.0173774 -4.0282221 -4.0315065 -4.0400696 -4.0712361 -4.098526 -4.1138091 -4.1305871 -4.1438355 -4.1416416 -4.1210074][-4.0878468 -4.0609179 -4.0240197 -4.0126982 -4.0277987 -4.0522871 -4.0672975 -4.0799546 -4.0971479 -4.1114397 -4.12484 -4.1421132 -4.1520357 -4.1451507 -4.1233749][-4.0807481 -4.0581989 -4.0337219 -4.0318236 -4.0532589 -4.0839782 -4.1073995 -4.1250982 -4.1392541 -4.15299 -4.1644678 -4.1730852 -4.1734748 -4.1602249 -4.1363888][-4.1157179 -4.1028242 -4.0948067 -4.0994086 -4.1193924 -4.1439481 -4.1634369 -4.1786761 -4.1945019 -4.2104311 -4.2186418 -4.2194209 -4.2128487 -4.1993132 -4.1808567]]...]
INFO - root - 2017-12-05 12:57:08.317983: step 9910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:41m:48s remains)
INFO - root - 2017-12-05 12:57:16.805566: step 9920, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 74h:34m:01s remains)
INFO - root - 2017-12-05 12:57:25.406589: step 9930, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 78h:56m:28s remains)
INFO - root - 2017-12-05 12:57:33.963426: step 9940, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 78h:34m:37s remains)
INFO - root - 2017-12-05 12:57:42.498060: step 9950, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 76h:16m:57s remains)
INFO - root - 2017-12-05 12:57:51.079974: step 9960, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 79h:58m:42s remains)
INFO - root - 2017-12-05 12:57:59.608034: step 9970, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 76h:11m:19s remains)
INFO - root - 2017-12-05 12:58:08.221437: step 9980, loss = 2.05, batch loss = 1.99 (8.4 examples/sec; 0.947 sec/batch; 84h:52m:34s remains)
INFO - root - 2017-12-05 12:58:16.703822: step 9990, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 78h:28m:59s remains)
INFO - root - 2017-12-05 12:58:25.264245: step 10000, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 77h:03m:33s remains)
2017-12-05 12:58:26.008082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1865096 -4.2033706 -4.2173662 -4.2282376 -4.2337623 -4.2350812 -4.2329664 -4.2252803 -4.2141137 -4.2023187 -4.1870594 -4.1786561 -4.1777053 -4.1876259 -4.200789][-4.1972618 -4.2183619 -4.2321587 -4.2380872 -4.2397065 -4.23964 -4.2414618 -4.2385759 -4.2341261 -4.2309279 -4.2189851 -4.2069778 -4.2006445 -4.2076545 -4.2198172][-4.2030215 -4.2232132 -4.2320061 -4.2322621 -4.2301388 -4.2272348 -4.2275672 -4.2251644 -4.2248244 -4.2290292 -4.2252655 -4.2173071 -4.2149606 -4.2248287 -4.2345805][-4.200562 -4.215693 -4.2173538 -4.2080445 -4.193676 -4.1813521 -4.1738224 -4.1662221 -4.1703215 -4.1860576 -4.19855 -4.20198 -4.2082591 -4.2238054 -4.2352524][-4.1804457 -4.185751 -4.177947 -4.1575489 -4.1290655 -4.1033959 -4.0740466 -4.049818 -4.0645943 -4.1065836 -4.1449471 -4.1674409 -4.1870403 -4.2126937 -4.2302518][-4.1667576 -4.1551104 -4.129693 -4.094089 -4.0505295 -4.0066838 -3.9435594 -3.8902111 -3.9310312 -4.0129862 -4.0755835 -4.1177177 -4.1585174 -4.2017679 -4.2296586][-4.1655717 -4.1397877 -4.0958242 -4.0366893 -3.9698346 -3.9005332 -3.7967901 -3.7071667 -3.7863979 -3.9131336 -3.9958317 -4.0534296 -4.1153741 -4.1759996 -4.2169952][-4.1745615 -4.1518035 -4.1033134 -4.0387683 -3.9657195 -3.8897417 -3.7916272 -3.7212219 -3.7966745 -3.9043257 -3.9657097 -4.0086875 -4.0720196 -4.1407971 -4.1939254][-4.1745443 -4.1639276 -4.1317883 -4.0889564 -4.0355935 -3.9813018 -3.9225004 -3.88718 -3.9257643 -3.9770114 -3.9956207 -4.0129843 -4.0608072 -4.1240292 -4.1790576][-4.1623297 -4.1650982 -4.152956 -4.135376 -4.1061406 -4.074326 -4.0481868 -4.03331 -4.0472775 -4.0613136 -4.0569606 -4.0637078 -4.0973382 -4.1448026 -4.1864471][-4.1444769 -4.1642976 -4.1703897 -4.1703939 -4.1589465 -4.1434736 -4.1303315 -4.1204691 -4.1137877 -4.1038461 -4.0935197 -4.1054525 -4.1393204 -4.1791596 -4.2106037][-4.1379504 -4.1695809 -4.1895008 -4.19816 -4.1956472 -4.1872072 -4.1742783 -4.1634312 -4.1449509 -4.1247282 -4.1148462 -4.1316085 -4.1678815 -4.2054377 -4.2318444][-4.1569118 -4.1840334 -4.202281 -4.2078118 -4.2021451 -4.187736 -4.1692739 -4.15826 -4.1442928 -4.1322451 -4.1324248 -4.1538515 -4.1854239 -4.2151976 -4.2397685][-4.1660681 -4.1885824 -4.2022042 -4.2035913 -4.1990108 -4.183949 -4.1638064 -4.1569424 -4.149796 -4.1419206 -4.1428094 -4.1585927 -4.1827035 -4.212554 -4.2412028][-4.1603804 -4.1819506 -4.1997714 -4.2091269 -4.2102556 -4.1991124 -4.1793375 -4.1719675 -4.162406 -4.150516 -4.1458325 -4.1559854 -4.1799841 -4.2129321 -4.2461267]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 12:58:35.373770: step 10010, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 77h:45m:57s remains)
INFO - root - 2017-12-05 12:58:44.101780: step 10020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 77h:25m:20s remains)
INFO - root - 2017-12-05 12:58:52.648924: step 10030, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 76h:01m:51s remains)
INFO - root - 2017-12-05 12:59:01.173291: step 10040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 77h:26m:16s remains)
INFO - root - 2017-12-05 12:59:09.684047: step 10050, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:50m:43s remains)
INFO - root - 2017-12-05 12:59:18.288992: step 10060, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 74h:22m:07s remains)
INFO - root - 2017-12-05 12:59:26.893569: step 10070, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 75h:57m:48s remains)
INFO - root - 2017-12-05 12:59:35.455748: step 10080, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 75h:55m:39s remains)
INFO - root - 2017-12-05 12:59:44.054377: step 10090, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.816 sec/batch; 73h:06m:50s remains)
INFO - root - 2017-12-05 12:59:52.634182: step 10100, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 76h:40m:58s remains)
2017-12-05 12:59:53.408441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2052183 -4.200346 -4.2139473 -4.2437406 -4.2701731 -4.2931075 -4.3075905 -4.3091207 -4.298409 -4.2796569 -4.2642016 -4.2622385 -4.2674122 -4.2730093 -4.2780843][-4.1955514 -4.1807566 -4.1805077 -4.2001181 -4.2255936 -4.2531929 -4.2766218 -4.2887516 -4.2858491 -4.2690425 -4.2510061 -4.2438579 -4.2429147 -4.2444334 -4.2520003][-4.1887817 -4.1640835 -4.1519146 -4.1597414 -4.1792383 -4.2020783 -4.2242537 -4.2429767 -4.2521062 -4.2464876 -4.2320995 -4.2211246 -4.2128868 -4.2110543 -4.2218742][-4.1788616 -4.1441574 -4.1220565 -4.1178432 -4.1235819 -4.1320181 -4.1457415 -4.1709228 -4.1970034 -4.2110367 -4.2083788 -4.2001376 -4.1909647 -4.1912084 -4.2029667][-4.1685104 -4.1244755 -4.0914493 -4.071517 -4.0543022 -4.0353136 -4.028759 -4.056447 -4.1077142 -4.1526809 -4.1727915 -4.177381 -4.1803045 -4.1901913 -4.2009044][-4.1711245 -4.1212306 -4.0769706 -4.0366759 -3.9853175 -3.9255719 -3.8832994 -3.9089918 -3.9947882 -4.0824118 -4.1373644 -4.1644135 -4.1845326 -4.2052369 -4.2140927][-4.1811962 -4.1359844 -4.0891142 -4.0334816 -3.9514194 -3.847424 -3.7618451 -3.7763906 -3.8857315 -4.0050273 -4.0913649 -4.1434884 -4.1825643 -4.213006 -4.2259145][-4.1883864 -4.1555429 -4.1195221 -4.0716677 -3.9911997 -3.8834665 -3.7895141 -3.7806659 -3.8632591 -3.9691913 -4.0589805 -4.1237659 -4.1758866 -4.2141509 -4.2326779][-4.1792212 -4.1607866 -4.1420546 -4.1180792 -4.0666127 -3.9897761 -3.9167721 -3.8947959 -3.9341691 -4.0033336 -4.070652 -4.1255245 -4.1722193 -4.2056222 -4.2210026][-4.1470051 -4.1424561 -4.1478467 -4.1564832 -4.1392226 -4.0991955 -4.0554752 -4.032671 -4.0452347 -4.0814233 -4.1169829 -4.1462564 -4.1724687 -4.1869588 -4.1889586][-4.1218696 -4.1237817 -4.148469 -4.1853261 -4.19692 -4.18798 -4.1708422 -4.1566172 -4.1576605 -4.17044 -4.1755061 -4.1743817 -4.1743679 -4.1655374 -4.1462336][-4.122221 -4.1275482 -4.1609144 -4.2095513 -4.2336478 -4.2388439 -4.2343006 -4.225584 -4.2234373 -4.2234826 -4.2113628 -4.192667 -4.1750965 -4.1493583 -4.1162348][-4.1578169 -4.1662312 -4.20115 -4.2490153 -4.2701755 -4.2724113 -4.2627878 -4.2485218 -4.2366066 -4.2253156 -4.2048612 -4.1818352 -4.1606188 -4.1365023 -4.1118951][-4.2164297 -4.2250571 -4.2550068 -4.2918968 -4.3003211 -4.2909789 -4.2707648 -4.2448025 -4.2178307 -4.1904025 -4.16253 -4.1404309 -4.1252027 -4.1166449 -4.1142612][-4.26913 -4.2782035 -4.3012252 -4.3241277 -4.3213582 -4.3064823 -4.2829447 -4.251 -4.2126431 -4.1721835 -4.1377549 -4.1160135 -4.1070776 -4.1117916 -4.1295571]]...]
INFO - root - 2017-12-05 13:00:01.879011: step 10110, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.797 sec/batch; 71h:21m:38s remains)
INFO - root - 2017-12-05 13:00:10.387511: step 10120, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 75h:08m:21s remains)
INFO - root - 2017-12-05 13:00:18.982971: step 10130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:52m:01s remains)
INFO - root - 2017-12-05 13:00:27.518576: step 10140, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 76h:26m:41s remains)
INFO - root - 2017-12-05 13:00:35.927306: step 10150, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 75h:40m:55s remains)
INFO - root - 2017-12-05 13:00:44.498207: step 10160, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 77h:22m:39s remains)
INFO - root - 2017-12-05 13:00:53.002726: step 10170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:24m:56s remains)
INFO - root - 2017-12-05 13:01:01.586983: step 10180, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 78h:57m:04s remains)
INFO - root - 2017-12-05 13:01:10.056794: step 10190, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 74h:31m:37s remains)
INFO - root - 2017-12-05 13:01:18.550924: step 10200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 77h:12m:09s remains)
2017-12-05 13:01:19.266185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2995653 -4.2988229 -4.2919779 -4.2895 -4.2951736 -4.3057141 -4.3159366 -4.3223004 -4.3225632 -4.3154869 -4.3109393 -4.3105745 -4.3132195 -4.3178411 -4.3170776][-4.3186483 -4.3197279 -4.3107662 -4.3059835 -4.3081865 -4.312674 -4.3155818 -4.3135796 -4.3087869 -4.3016515 -4.2991295 -4.3011622 -4.3077526 -4.3161345 -4.3174162][-4.3204241 -4.3225155 -4.3126106 -4.304543 -4.3022833 -4.3010244 -4.2956777 -4.2838178 -4.2757988 -4.2734241 -4.2771025 -4.2850237 -4.2978158 -4.3092875 -4.3118587][-4.310791 -4.3147879 -4.3062811 -4.2984905 -4.2939386 -4.2873058 -4.2712259 -4.2460871 -4.2325921 -4.236948 -4.2499022 -4.2634182 -4.2787237 -4.2890449 -4.290884][-4.3040929 -4.3105874 -4.3032203 -4.294497 -4.2863135 -4.2709627 -4.2395096 -4.19938 -4.1806297 -4.1950893 -4.2194304 -4.2386713 -4.2531438 -4.2582936 -4.2580709][-4.3002596 -4.3098493 -4.30162 -4.2862797 -4.2649026 -4.230732 -4.1778774 -4.1223369 -4.1066785 -4.1422963 -4.1877222 -4.2214661 -4.2372966 -4.236877 -4.231956][-4.2999997 -4.3098369 -4.2955103 -4.2644572 -4.2214437 -4.16189 -4.0836806 -4.0148215 -4.0134454 -4.0802069 -4.1541061 -4.2078786 -4.2295189 -4.2273602 -4.2179184][-4.3007407 -4.3051119 -4.2800279 -4.2348952 -4.1750703 -4.0987339 -4.0053439 -3.9355407 -3.957629 -4.0512104 -4.1457682 -4.2103128 -4.2328825 -4.2263808 -4.211369][-4.3025208 -4.3010516 -4.2693062 -4.2177162 -4.1568036 -4.0869708 -4.0086942 -3.963685 -4.0009427 -4.0925536 -4.1801872 -4.2343326 -4.2446594 -4.2255883 -4.2037349][-4.3161259 -4.3090878 -4.27505 -4.2254996 -4.1750607 -4.1262121 -4.08086 -4.0662646 -4.1012521 -4.1673923 -4.22811 -4.2600489 -4.2528157 -4.2241106 -4.2006841][-4.3404841 -4.3302321 -4.298933 -4.2569394 -4.2182631 -4.1888881 -4.1694083 -4.17108 -4.1955476 -4.2346587 -4.2691655 -4.2817564 -4.2651529 -4.2371969 -4.2184186][-4.3579421 -4.3481965 -4.3242874 -4.2914319 -4.2626352 -4.2458649 -4.2394595 -4.2443118 -4.2596931 -4.280942 -4.2991629 -4.3038211 -4.2897553 -4.2713976 -4.2586393][-4.3563061 -4.3513131 -4.33518 -4.3107266 -4.2902975 -4.2819295 -4.2808895 -4.2860851 -4.2976055 -4.3106923 -4.3219452 -4.3243108 -4.3144765 -4.3031607 -4.2929382][-4.338995 -4.336185 -4.3226581 -4.3040571 -4.291029 -4.2896667 -4.2924061 -4.2979903 -4.3072467 -4.3158913 -4.3246269 -4.3273706 -4.321146 -4.3152966 -4.3099346][-4.3141117 -4.3096676 -4.2944164 -4.2765827 -4.2661715 -4.2679687 -4.2730579 -4.2796168 -4.2866249 -4.2911925 -4.2999334 -4.3085566 -4.3111258 -4.3127236 -4.3124022]]...]
INFO - root - 2017-12-05 13:01:27.793500: step 10210, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 75h:13m:21s remains)
INFO - root - 2017-12-05 13:01:36.192194: step 10220, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.781 sec/batch; 69h:55m:46s remains)
INFO - root - 2017-12-05 13:01:44.678053: step 10230, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.819 sec/batch; 73h:21m:04s remains)
INFO - root - 2017-12-05 13:01:53.275571: step 10240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:44m:33s remains)
INFO - root - 2017-12-05 13:02:01.779884: step 10250, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 79h:08m:46s remains)
INFO - root - 2017-12-05 13:02:10.405550: step 10260, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 78h:34m:22s remains)
INFO - root - 2017-12-05 13:02:19.003533: step 10270, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 78h:36m:57s remains)
INFO - root - 2017-12-05 13:02:27.549887: step 10280, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.827 sec/batch; 74h:00m:17s remains)
INFO - root - 2017-12-05 13:02:36.060597: step 10290, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 74h:12m:46s remains)
INFO - root - 2017-12-05 13:02:44.566043: step 10300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 76h:31m:52s remains)
2017-12-05 13:02:45.334762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2192607 -4.2257776 -4.2296753 -4.2320137 -4.23191 -4.2311978 -4.2315421 -4.2303219 -4.2256021 -4.2162204 -4.2003155 -4.1883221 -4.1834598 -4.1899157 -4.2167568][-4.2185183 -4.2241831 -4.2267447 -4.228416 -4.2285004 -4.2271504 -4.2275038 -4.227962 -4.2264042 -4.2203918 -4.2094617 -4.2019777 -4.2000971 -4.2062979 -4.2298813][-4.2274833 -4.2333694 -4.2375679 -4.2409892 -4.2392955 -4.2340326 -4.2328982 -4.2351952 -4.2392673 -4.2391686 -4.2357254 -4.2314816 -4.2279253 -4.2303591 -4.2481074][-4.2182665 -4.2264185 -4.2346964 -4.2415247 -4.2367072 -4.2233381 -4.2149754 -4.2187228 -4.2341895 -4.2465029 -4.2513022 -4.2507772 -4.2462974 -4.2468429 -4.2599163][-4.17841 -4.1901827 -4.2037559 -4.21256 -4.2025237 -4.1775694 -4.1579003 -4.1630888 -4.1944623 -4.2277484 -4.2464066 -4.2503691 -4.2428617 -4.2384667 -4.2476363][-4.117969 -4.137229 -4.1596079 -4.1727519 -4.1576715 -4.112771 -4.0722508 -4.0733404 -4.1229525 -4.1830263 -4.221602 -4.233211 -4.2235637 -4.2125616 -4.2215242][-4.0567412 -4.090169 -4.1216512 -4.135263 -4.1092138 -4.0403934 -3.9709296 -3.9588904 -4.0243707 -4.1117139 -4.1739879 -4.1989098 -4.1943235 -4.1868739 -4.2026238][-4.0052285 -4.0601406 -4.1123676 -4.1327286 -4.0984583 -4.0113144 -3.9161634 -3.8804529 -3.9436979 -4.0394831 -4.1164293 -4.1606832 -4.1719713 -4.178112 -4.2060161][-3.9712849 -4.0418625 -4.1161523 -4.15289 -4.1291251 -4.0551429 -3.9721286 -3.9280541 -3.960844 -4.0265875 -4.0895557 -4.1392493 -4.1669059 -4.196053 -4.2372046][-3.9836664 -4.0563421 -4.1324329 -4.1733718 -4.1618128 -4.1147456 -4.0669622 -4.0329728 -4.0413723 -4.0720148 -4.1098351 -4.1496606 -4.183115 -4.2259107 -4.2736921][-4.0394559 -4.1025276 -4.1652107 -4.200593 -4.19312 -4.1664643 -4.1453156 -4.1252189 -4.1236067 -4.1298127 -4.14277 -4.1679745 -4.1986051 -4.2444367 -4.2944765][-4.0838122 -4.1411185 -4.200182 -4.2361803 -4.2343073 -4.2184606 -4.2069826 -4.1901417 -4.1772003 -4.1634979 -4.157578 -4.1714478 -4.1968179 -4.2402534 -4.289876][-4.0988345 -4.1552858 -4.2196422 -4.2615247 -4.2701373 -4.2641749 -4.2568383 -4.239615 -4.2209625 -4.1970229 -4.1812229 -4.1853561 -4.2026305 -4.2371054 -4.2803559][-4.0971584 -4.1496453 -4.2156787 -4.2682 -4.2926626 -4.2995553 -4.2988091 -4.2853537 -4.2659154 -4.2376871 -4.2161713 -4.2119584 -4.2203622 -4.242187 -4.2751584][-4.0819879 -4.1254315 -4.1908827 -4.2509661 -4.2892451 -4.3084335 -4.3132958 -4.3015075 -4.2792206 -4.2519217 -4.2300887 -4.2204127 -4.2249851 -4.2422013 -4.2697048]]...]
INFO - root - 2017-12-05 13:02:53.589664: step 10310, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 75h:29m:49s remains)
INFO - root - 2017-12-05 13:03:02.105599: step 10320, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 79h:19m:35s remains)
INFO - root - 2017-12-05 13:03:10.529335: step 10330, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:49m:45s remains)
INFO - root - 2017-12-05 13:03:19.081518: step 10340, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 77h:35m:57s remains)
INFO - root - 2017-12-05 13:03:27.652881: step 10350, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 77h:15m:39s remains)
INFO - root - 2017-12-05 13:03:36.187124: step 10360, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 75h:42m:55s remains)
INFO - root - 2017-12-05 13:03:44.842337: step 10370, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.849 sec/batch; 75h:55m:29s remains)
INFO - root - 2017-12-05 13:03:53.378117: step 10380, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:38m:36s remains)
INFO - root - 2017-12-05 13:04:01.861953: step 10390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:43m:25s remains)
INFO - root - 2017-12-05 13:04:10.376677: step 10400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 78h:03m:22s remains)
2017-12-05 13:04:11.084470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3094206 -4.3032308 -4.2884736 -4.2670021 -4.245121 -4.2301388 -4.23098 -4.2490129 -4.2752757 -4.2981477 -4.3134952 -4.3218322 -4.3230209 -4.3204646 -4.3174567][-4.3051004 -4.2957797 -4.2773361 -4.253943 -4.2323432 -4.2195916 -4.2237906 -4.2449341 -4.2723632 -4.2962775 -4.3133888 -4.3222485 -4.3229828 -4.3205914 -4.3180351][-4.2967887 -4.2832952 -4.25975 -4.2329659 -4.2109175 -4.2014537 -4.2103429 -4.2373071 -4.2686791 -4.2949009 -4.3150349 -4.3248525 -4.3246851 -4.3218536 -4.3199749][-4.2784953 -4.2602262 -4.2285128 -4.1936169 -4.1684203 -4.1618648 -4.1764574 -4.2111163 -4.2498217 -4.2835054 -4.3120489 -4.3266287 -4.3272061 -4.3238106 -4.3219919][-4.2476406 -4.2220631 -4.1796875 -4.1363683 -4.1104283 -4.1069989 -4.125618 -4.1633592 -4.207974 -4.2543106 -4.2971873 -4.3209629 -4.325809 -4.3237743 -4.3226476][-4.1997633 -4.1641989 -4.1104164 -4.0628319 -4.0419636 -4.0443163 -4.065691 -4.1013885 -4.1476703 -4.2086945 -4.269145 -4.305562 -4.3193893 -4.3216367 -4.3222308][-4.1566882 -4.1118045 -4.0527148 -4.0078282 -3.9926827 -3.9975054 -4.0170302 -4.0462818 -4.0893717 -4.1604319 -4.2351747 -4.2835197 -4.309021 -4.3191171 -4.3228621][-4.14012 -4.0974145 -4.0479808 -4.0130162 -3.9990752 -3.9989538 -4.0099173 -4.0285716 -4.0643606 -4.1363959 -4.2145619 -4.2683167 -4.3022137 -4.3190069 -4.3272133][-4.1403708 -4.1074195 -4.0716305 -4.0455394 -4.0306478 -4.0233755 -4.02478 -4.0348382 -4.0662704 -4.1328979 -4.2062278 -4.2600708 -4.2991824 -4.3211746 -4.3328991][-4.1535721 -4.1310468 -4.1065335 -4.0873365 -4.0723963 -4.0617342 -4.0577559 -4.0613918 -4.0875196 -4.1443 -4.206768 -4.2562218 -4.2959695 -4.3209138 -4.334434][-4.1794372 -4.1652708 -4.1492548 -4.1380424 -4.129169 -4.1234326 -4.119576 -4.1192412 -4.1367717 -4.1773605 -4.2245989 -4.264123 -4.2981873 -4.3208861 -4.3331552][-4.2156119 -4.2094412 -4.2023206 -4.20032 -4.1997414 -4.1998482 -4.1973696 -4.1950107 -4.2041478 -4.228323 -4.2600288 -4.2875838 -4.3109207 -4.326324 -4.333724][-4.2537642 -4.25221 -4.2514572 -4.2553887 -4.259027 -4.2629018 -4.2633696 -4.2612705 -4.2634077 -4.2741165 -4.2932434 -4.3111854 -4.3244562 -4.3321643 -4.3346834][-4.2836018 -4.2823353 -4.2829137 -4.2873244 -4.2911263 -4.2957721 -4.2991228 -4.2981353 -4.2965078 -4.3002357 -4.3114953 -4.3226175 -4.3288531 -4.3312192 -4.3312411][-4.3013949 -4.3004127 -4.2995 -4.3003416 -4.3014894 -4.3043928 -4.3080764 -4.30917 -4.3077555 -4.3093061 -4.3156891 -4.3219342 -4.3241997 -4.3244033 -4.3246851]]...]
INFO - root - 2017-12-05 13:04:19.583376: step 10410, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 73h:31m:20s remains)
INFO - root - 2017-12-05 13:04:28.169506: step 10420, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 76h:20m:20s remains)
INFO - root - 2017-12-05 13:04:36.634023: step 10430, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 74h:36m:48s remains)
INFO - root - 2017-12-05 13:04:45.039042: step 10440, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 75h:57m:55s remains)
INFO - root - 2017-12-05 13:04:53.456960: step 10450, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.814 sec/batch; 72h:47m:25s remains)
INFO - root - 2017-12-05 13:05:02.034961: step 10460, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 75h:07m:43s remains)
INFO - root - 2017-12-05 13:05:10.661219: step 10470, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 78h:00m:52s remains)
INFO - root - 2017-12-05 13:05:19.228220: step 10480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:19m:34s remains)
INFO - root - 2017-12-05 13:05:27.800272: step 10490, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 73h:20m:07s remains)
INFO - root - 2017-12-05 13:05:36.366848: step 10500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 74h:55m:12s remains)
2017-12-05 13:05:37.059067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2716475 -4.2599878 -4.2557783 -4.25401 -4.2451553 -4.2198524 -4.1906548 -4.1870103 -4.205513 -4.2250595 -4.2385821 -4.2518611 -4.27415 -4.3053885 -4.3322821][-4.2684479 -4.2505159 -4.2439513 -4.2466764 -4.2434993 -4.2284913 -4.2099323 -4.2099819 -4.2263889 -4.2475533 -4.2604861 -4.2683163 -4.2827168 -4.3091388 -4.3343964][-4.247282 -4.2279506 -4.2166061 -4.2190914 -4.2199955 -4.2196155 -4.2185678 -4.2303538 -4.2513709 -4.2729611 -4.2813191 -4.2808084 -4.2860179 -4.3074131 -4.332253][-4.2176194 -4.2069387 -4.1959195 -4.1965747 -4.2003708 -4.2047238 -4.2114244 -4.2312217 -4.2585559 -4.2828155 -4.2899623 -4.28535 -4.2852626 -4.3032184 -4.3283486][-4.199687 -4.198451 -4.1894774 -4.1888771 -4.1939125 -4.1917853 -4.1907058 -4.2063947 -4.2372956 -4.2677741 -4.27801 -4.2744579 -4.2749825 -4.2944674 -4.3228388][-4.2004104 -4.1995792 -4.1889744 -4.1880574 -4.1911793 -4.1772971 -4.1615176 -4.1681862 -4.1973395 -4.2301273 -4.2435975 -4.246449 -4.2555432 -4.2830338 -4.3166313][-4.2060118 -4.202302 -4.1898861 -4.1896272 -4.1909151 -4.1659393 -4.137361 -4.1379085 -4.1625047 -4.1941838 -4.210619 -4.2237868 -4.2466426 -4.2827363 -4.3177004][-4.2037382 -4.1938477 -4.1823068 -4.1824212 -4.1791353 -4.14976 -4.1203136 -4.1252966 -4.1492653 -4.1784821 -4.1969314 -4.2194858 -4.2532644 -4.2938681 -4.3255334][-4.2012944 -4.187448 -4.1746426 -4.1671896 -4.156414 -4.1312609 -4.116293 -4.1332784 -4.1578078 -4.1830468 -4.2037406 -4.2302589 -4.2662077 -4.3050919 -4.3321934][-4.200871 -4.1870222 -4.1693978 -4.14852 -4.1260023 -4.108798 -4.1145544 -4.1462345 -4.178503 -4.2035136 -4.2257462 -4.2515492 -4.2811913 -4.313623 -4.3361177][-4.2010975 -4.1928692 -4.173995 -4.14335 -4.1123695 -4.1047797 -4.1279588 -4.1697965 -4.2064362 -4.2319107 -4.253334 -4.2740407 -4.2947783 -4.3200507 -4.3385758][-4.2088952 -4.211031 -4.19853 -4.1697874 -4.1400309 -4.1380587 -4.1652069 -4.2051167 -4.2370129 -4.2595325 -4.2778854 -4.292027 -4.3050251 -4.3244967 -4.3401608][-4.2282052 -4.2402287 -4.2359643 -4.2167268 -4.1966619 -4.1957641 -4.2133641 -4.2399125 -4.2620168 -4.2784009 -4.2904086 -4.2983088 -4.3065777 -4.3226552 -4.3387752][-4.2500863 -4.2664003 -4.2649412 -4.2534118 -4.2420053 -4.2370672 -4.2399025 -4.2517428 -4.2657585 -4.2782726 -4.2844453 -4.2878089 -4.2938995 -4.3113208 -4.3330464][-4.2679138 -4.281127 -4.2734747 -4.2640028 -4.25645 -4.2444797 -4.2322865 -4.233386 -4.2436042 -4.2562275 -4.2620225 -4.2649412 -4.2735748 -4.2964778 -4.325613]]...]
INFO - root - 2017-12-05 13:05:45.691595: step 10510, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.863 sec/batch; 77h:11m:28s remains)
INFO - root - 2017-12-05 13:05:54.050951: step 10520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 75h:55m:00s remains)
INFO - root - 2017-12-05 13:06:02.630275: step 10530, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 78h:55m:46s remains)
INFO - root - 2017-12-05 13:06:11.165072: step 10540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 76h:06m:38s remains)
INFO - root - 2017-12-05 13:06:19.592329: step 10550, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 75h:23m:07s remains)
INFO - root - 2017-12-05 13:06:28.125463: step 10560, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 76h:13m:53s remains)
INFO - root - 2017-12-05 13:06:36.598665: step 10570, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 75h:38m:25s remains)
INFO - root - 2017-12-05 13:06:45.063618: step 10580, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 77h:51m:50s remains)
INFO - root - 2017-12-05 13:06:53.735189: step 10590, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 77h:18m:43s remains)
INFO - root - 2017-12-05 13:07:02.298271: step 10600, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 75h:55m:29s remains)
2017-12-05 13:07:03.125101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1031351 -4.058382 -4.002296 -3.9828179 -4.0303903 -4.0692954 -4.0947623 -4.1128931 -4.1140795 -4.113883 -4.1175027 -4.1202536 -4.1301212 -4.1368375 -4.1332154][-4.0806556 -4.0422068 -3.9949827 -3.9844387 -4.0287089 -4.0529089 -4.0621924 -4.0729518 -4.0776134 -4.0861983 -4.1034207 -4.1173067 -4.1357861 -4.1520386 -4.1546082][-4.0804944 -4.048316 -4.0224943 -4.026742 -4.0626383 -4.0698347 -4.0633259 -4.0659218 -4.0723486 -4.0932446 -4.1221709 -4.1407905 -4.1597776 -4.1709728 -4.1658583][-4.1109829 -4.0846143 -4.078805 -4.0953465 -4.1249666 -4.123661 -4.1120296 -4.1084962 -4.1117682 -4.12749 -4.1515718 -4.1653008 -4.1786861 -4.1843553 -4.17127][-4.1496029 -4.1237831 -4.1219192 -4.141129 -4.1641884 -4.1653581 -4.1678925 -4.1798186 -4.1889439 -4.1890659 -4.1895094 -4.1816468 -4.1834693 -4.1871819 -4.1749239][-4.175889 -4.1429634 -4.1313629 -4.1408033 -4.1485047 -4.1478906 -4.1631374 -4.1976666 -4.2230921 -4.2220764 -4.2076139 -4.1825752 -4.1748524 -4.1829448 -4.1845293][-4.1768909 -4.1319866 -4.1046572 -4.09671 -4.0898771 -4.0803885 -4.0967321 -4.1432943 -4.184793 -4.1925292 -4.1778359 -4.1576276 -4.1581745 -4.1855989 -4.2066936][-4.1497955 -4.0952864 -4.0562577 -4.0371084 -4.023613 -4.0025811 -4.0037107 -4.0435314 -4.0871887 -4.1013584 -4.0970416 -4.0999184 -4.1286745 -4.1839576 -4.2233877][-4.110393 -4.0522776 -4.009831 -3.9876871 -3.970928 -3.9406753 -3.9211628 -3.9439914 -3.9841838 -3.9986596 -4.0054779 -4.0433874 -4.10761 -4.180593 -4.22137][-4.081347 -4.0158281 -3.9700892 -3.9496863 -3.9332254 -3.9023304 -3.8765011 -3.8899612 -3.9212716 -3.9319882 -3.9510527 -4.0213575 -4.1108727 -4.1840482 -4.2094207][-4.0816479 -4.0145206 -3.9683266 -3.9515004 -3.9430737 -3.9232023 -3.9133985 -3.922781 -3.9389994 -3.9415655 -3.9643409 -4.0399079 -4.1269908 -4.1802211 -4.1852307][-4.1056824 -4.0465212 -4.0109773 -4.0003705 -3.9947553 -3.9844439 -3.9885092 -3.9987421 -4.0128055 -4.0230865 -4.0437083 -4.0941105 -4.1493425 -4.1710429 -4.1593795][-4.1348772 -4.0903654 -4.0708213 -4.0704889 -4.0735211 -4.0709686 -4.0723834 -4.07991 -4.0965447 -4.1162853 -4.1327863 -4.1537437 -4.1750431 -4.1720452 -4.1475911][-4.1630387 -4.13022 -4.1235704 -4.1358457 -4.1483145 -4.1517525 -4.1521549 -4.1579094 -4.174026 -4.1916804 -4.2045722 -4.2107024 -4.2098575 -4.1918674 -4.158268][-4.1789684 -4.1537743 -4.1557312 -4.1767359 -4.1960034 -4.2042689 -4.2078705 -4.2142305 -4.229588 -4.2429657 -4.2533789 -4.2565784 -4.2483754 -4.224247 -4.1873078]]...]
INFO - root - 2017-12-05 13:07:11.694975: step 10610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:22m:41s remains)
INFO - root - 2017-12-05 13:07:20.309277: step 10620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 78h:27m:37s remains)
INFO - root - 2017-12-05 13:07:28.794292: step 10630, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 75h:06m:54s remains)
INFO - root - 2017-12-05 13:07:37.317973: step 10640, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 75h:26m:48s remains)
INFO - root - 2017-12-05 13:07:45.786615: step 10650, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 77h:07m:02s remains)
INFO - root - 2017-12-05 13:07:54.149663: step 10660, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 73h:34m:34s remains)
INFO - root - 2017-12-05 13:08:02.744978: step 10670, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 77h:28m:32s remains)
INFO - root - 2017-12-05 13:08:11.345132: step 10680, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:37m:29s remains)
INFO - root - 2017-12-05 13:08:19.850699: step 10690, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:13m:53s remains)
INFO - root - 2017-12-05 13:08:28.341406: step 10700, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 78h:28m:49s remains)
2017-12-05 13:08:29.118074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3019018 -4.2880836 -4.273273 -4.2523837 -4.220746 -4.1910648 -4.168149 -4.1448216 -4.149591 -4.1665964 -4.2033014 -4.2465868 -4.2688241 -4.2683864 -4.2382088][-4.3011694 -4.2943506 -4.2791591 -4.2443914 -4.190413 -4.1431231 -4.1046658 -4.0713983 -4.0741291 -4.1030812 -4.1557369 -4.2114906 -4.2425337 -4.2404747 -4.1980844][-4.2997751 -4.2981544 -4.2837543 -4.2393465 -4.1707253 -4.1070819 -4.0494504 -4.0108051 -4.0152073 -4.0579648 -4.1302061 -4.196743 -4.2319455 -4.21974 -4.1558623][-4.2939034 -4.2930765 -4.2753763 -4.2202539 -4.1350794 -4.0537424 -3.975142 -3.9365742 -3.9575832 -4.023603 -4.1167097 -4.1907973 -4.2163606 -4.1760478 -4.0857463][-4.2905116 -4.2890482 -4.2671394 -4.2017722 -4.1016445 -3.9992964 -3.90066 -3.855552 -3.8937802 -3.9785104 -4.0856109 -4.167263 -4.1783328 -4.1041579 -3.9925511][-4.2910028 -4.2880483 -4.2626343 -4.1918311 -4.0865841 -3.9699311 -3.8558121 -3.7987516 -3.8446283 -3.9376473 -4.0543213 -4.14002 -4.13605 -4.0375218 -3.9058905][-4.2898417 -4.2830629 -4.2521486 -4.180028 -4.0747166 -3.9565408 -3.835058 -3.7491698 -3.7801666 -3.8718646 -3.999877 -4.098053 -4.1014595 -3.9991283 -3.865788][-4.2851419 -4.2759266 -4.2411919 -4.1696825 -4.06935 -3.9598005 -3.8397985 -3.7332509 -3.7379177 -3.816 -3.9418752 -4.0488863 -4.0729051 -3.9860604 -3.8734965][-4.2760911 -4.2632794 -4.2254252 -4.15457 -4.0603447 -3.9600134 -3.8538108 -3.7550015 -3.7464159 -3.8074253 -3.92521 -4.0266886 -4.0584955 -4.0005393 -3.9246576][-4.2626133 -4.2427535 -4.2034593 -4.1393366 -4.0545115 -3.9664395 -3.8782239 -3.7996609 -3.787508 -3.8325295 -3.9385369 -4.0272756 -4.0601172 -4.0268908 -3.9823546][-4.2520728 -4.2260222 -4.1882157 -4.13601 -4.0684614 -3.9988124 -3.9345908 -3.884186 -3.8748384 -3.9056113 -3.9907398 -4.062943 -4.0938005 -4.0758495 -4.050601][-4.251821 -4.2237763 -4.1881227 -4.1478634 -4.1006908 -4.0539994 -4.0161572 -3.99502 -4.00272 -4.0278015 -4.0861616 -4.1312027 -4.1503568 -4.139534 -4.1249661][-4.2620196 -4.236711 -4.2061424 -4.1769981 -4.1482425 -4.1222639 -4.1059003 -4.105783 -4.12349 -4.1451921 -4.184422 -4.2097254 -4.2209897 -4.21656 -4.2085862][-4.28186 -4.2629113 -4.2417188 -4.2243085 -4.2094822 -4.1983395 -4.1942978 -4.20285 -4.2215652 -4.2396274 -4.2640615 -4.278295 -4.2842488 -4.2819796 -4.2769642][-4.3071213 -4.2954273 -4.2844534 -4.2766242 -4.2713428 -4.2685747 -4.26909 -4.27967 -4.2961464 -4.3089738 -4.3219891 -4.3292227 -4.3310838 -4.3278513 -4.3226051]]...]
INFO - root - 2017-12-05 13:08:37.720910: step 10710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:37m:13s remains)
INFO - root - 2017-12-05 13:08:46.216956: step 10720, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 77h:12m:14s remains)
INFO - root - 2017-12-05 13:08:54.815467: step 10730, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 77h:27m:11s remains)
INFO - root - 2017-12-05 13:09:03.186873: step 10740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 76h:14m:57s remains)
INFO - root - 2017-12-05 13:09:11.866076: step 10750, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 80h:43m:46s remains)
INFO - root - 2017-12-05 13:09:20.460562: step 10760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 76h:55m:05s remains)
INFO - root - 2017-12-05 13:09:28.947677: step 10770, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 74h:19m:43s remains)
INFO - root - 2017-12-05 13:09:37.598056: step 10780, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 74h:21m:54s remains)
INFO - root - 2017-12-05 13:09:46.241075: step 10790, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.878 sec/batch; 78h:25m:57s remains)
INFO - root - 2017-12-05 13:09:54.874165: step 10800, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 77h:18m:07s remains)
2017-12-05 13:09:55.604116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104854 -4.307785 -4.2993088 -4.2924829 -4.2830896 -4.2728915 -4.2612677 -4.2517734 -4.24227 -4.2373743 -4.2384033 -4.2310686 -4.2149067 -4.1985292 -4.1724782][-4.2881761 -4.2824726 -4.2696338 -4.258791 -4.2463837 -4.236012 -4.2264533 -4.2204957 -4.2130232 -4.2122269 -4.2148862 -4.2059655 -4.1866632 -4.1614728 -4.1243787][-4.261168 -4.2521133 -4.2346869 -4.2232118 -4.2147379 -4.2083182 -4.20529 -4.203475 -4.1992416 -4.202693 -4.2074218 -4.1938486 -4.1687961 -4.1365933 -4.096858][-4.2333732 -4.2235065 -4.2068243 -4.1980133 -4.1924987 -4.1858969 -4.1847634 -4.1845469 -4.1844888 -4.1955762 -4.2080965 -4.20019 -4.1804442 -4.1567059 -4.1275887][-4.2131858 -4.2056484 -4.1902056 -4.1790438 -4.1678238 -4.1492829 -4.1386633 -4.1397386 -4.1465974 -4.166883 -4.1920362 -4.1993184 -4.1948032 -4.1871505 -4.1706848][-4.2064686 -4.1976347 -4.1806622 -4.1622424 -4.1339355 -4.0878415 -4.0526638 -4.0596852 -4.0821185 -4.1134443 -4.1503081 -4.1675692 -4.178791 -4.1895108 -4.1842446][-4.2137609 -4.1958113 -4.1678481 -4.1293135 -4.0726767 -3.9841919 -3.9150805 -3.948035 -4.0059042 -4.0491009 -4.087626 -4.1113153 -4.1430321 -4.17792 -4.1915298][-4.2219882 -4.1925387 -4.147182 -4.0867019 -4.0086603 -3.9030919 -3.8206236 -3.887862 -3.9802957 -4.022234 -4.0526938 -4.0803814 -4.12513 -4.1775026 -4.2098312][-4.2216778 -4.1866593 -4.1301022 -4.0627089 -3.9983864 -3.9341421 -3.8868961 -3.9436 -4.01706 -4.0383434 -4.057497 -4.0907316 -4.1422315 -4.2006989 -4.2419677][-4.2028794 -4.17943 -4.1340322 -4.0826063 -4.0485554 -4.0323997 -4.01608 -4.0381346 -4.0708146 -4.0753336 -4.0918617 -4.1308632 -4.1795192 -4.229702 -4.2652597][-4.1703148 -4.1721597 -4.157187 -4.1313162 -4.1181808 -4.1251616 -4.1200376 -4.1187315 -4.1265259 -4.1233759 -4.1360059 -4.1678405 -4.2058492 -4.2422872 -4.2666526][-4.1525259 -4.1801877 -4.1855593 -4.1763043 -4.1695719 -4.1773844 -4.177525 -4.1719089 -4.1668272 -4.155561 -4.1592565 -4.1771145 -4.2029467 -4.2305908 -4.2499738][-4.1627359 -4.193912 -4.1997514 -4.1949525 -4.1870522 -4.1891136 -4.1914368 -4.1896381 -4.1809154 -4.1670475 -4.1671534 -4.1755772 -4.1957836 -4.2216396 -4.2427444][-4.1736388 -4.1931329 -4.193511 -4.1843104 -4.1721439 -4.1711397 -4.1768394 -4.18165 -4.1804328 -4.1783452 -4.1829486 -4.1914697 -4.2130594 -4.243001 -4.2673049][-4.1717582 -4.1807184 -4.1763291 -4.1646395 -4.1508675 -4.15426 -4.1690497 -4.184164 -4.1980815 -4.2121296 -4.2239032 -4.2352328 -4.255444 -4.2805805 -4.299818]]...]
INFO - root - 2017-12-05 13:10:04.256488: step 10810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:47m:42s remains)
INFO - root - 2017-12-05 13:10:12.826528: step 10820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 77h:59m:52s remains)
INFO - root - 2017-12-05 13:10:21.409340: step 10830, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 78h:55m:40s remains)
INFO - root - 2017-12-05 13:10:29.967532: step 10840, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 78h:36m:41s remains)
INFO - root - 2017-12-05 13:10:38.405846: step 10850, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.853 sec/batch; 76h:10m:37s remains)
INFO - root - 2017-12-05 13:10:46.878844: step 10860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 77h:34m:40s remains)
INFO - root - 2017-12-05 13:10:55.329748: step 10870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:37m:43s remains)
INFO - root - 2017-12-05 13:11:03.791381: step 10880, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 73h:59m:33s remains)
INFO - root - 2017-12-05 13:11:12.240476: step 10890, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 77h:11m:39s remains)
INFO - root - 2017-12-05 13:11:20.829259: step 10900, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 77h:50m:19s remains)
2017-12-05 13:11:21.541672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3227215 -4.3105621 -4.2997017 -4.296021 -4.2994924 -4.3052564 -4.3129816 -4.3212805 -4.3298216 -4.3356996 -4.340673 -4.3458219 -4.3500166 -4.3524585 -4.3521614][-4.3097386 -4.2939496 -4.2819729 -4.2773714 -4.2812181 -4.2847118 -4.2896423 -4.3002367 -4.3175607 -4.328784 -4.336359 -4.3402977 -4.3410239 -4.3449326 -4.3466063][-4.29577 -4.2787313 -4.2658277 -4.2578368 -4.2554531 -4.2501674 -4.2454128 -4.2540483 -4.27525 -4.2933388 -4.3102608 -4.318152 -4.3205552 -4.3265481 -4.3308358][-4.2805381 -4.2591171 -4.2414045 -4.2276607 -4.2142062 -4.194376 -4.1760645 -4.1821756 -4.2098083 -4.2408977 -4.2724328 -4.2878885 -4.2931786 -4.3000755 -4.3076873][-4.2626462 -4.23522 -4.207284 -4.1858664 -4.1602931 -4.117435 -4.0760179 -4.0820785 -4.1345348 -4.1936183 -4.2422075 -4.2683268 -4.2759404 -4.2819109 -4.2900939][-4.2465615 -4.2085538 -4.1658978 -4.1300569 -4.0879507 -4.0129733 -3.9352791 -3.9391217 -4.0287971 -4.124115 -4.1928763 -4.231636 -4.2441216 -4.2523012 -4.2638888][-4.2326522 -4.1823082 -4.1210279 -4.0605726 -3.99417 -3.8825622 -3.7664995 -3.7584968 -3.882952 -4.0223947 -4.1157513 -4.1753793 -4.2070413 -4.2281866 -4.2460642][-4.2221432 -4.1595387 -4.0809383 -3.9969013 -3.9139853 -3.789794 -3.6612871 -3.6424866 -3.7790542 -3.9465892 -4.0606503 -4.1373243 -4.1905756 -4.2256713 -4.2457485][-4.2186437 -4.147635 -4.0571637 -3.9583011 -3.8739233 -3.772208 -3.6796131 -3.6713564 -3.7905571 -3.9501083 -4.0700378 -4.1477733 -4.2018647 -4.2385902 -4.2556596][-4.2255316 -4.1522541 -4.057157 -3.9485059 -3.8535314 -3.7626088 -3.7046978 -3.7160211 -3.8202381 -3.9626524 -4.084033 -4.1631866 -4.2106848 -4.2450175 -4.2624288][-4.2406731 -4.1712737 -4.0783081 -3.9680986 -3.8619947 -3.7696919 -3.72791 -3.7541909 -3.8493388 -3.9729948 -4.0900073 -4.17158 -4.2161736 -4.250289 -4.2715034][-4.2623224 -4.2046309 -4.1252551 -4.032196 -3.9383652 -3.8571715 -3.8214905 -3.8472438 -3.9255457 -4.0234604 -4.1268797 -4.2024198 -4.2419386 -4.2719526 -4.2913566][-4.2845235 -4.2437043 -4.1874781 -4.1255779 -4.0639882 -4.0130172 -3.9920118 -4.0135789 -4.0697045 -4.1342525 -4.2078586 -4.2617807 -4.2887468 -4.3069644 -4.3169541][-4.306088 -4.2822361 -4.2501864 -4.2187 -4.1887774 -4.1668906 -4.1607361 -4.1783843 -4.211349 -4.2463393 -4.2852473 -4.3140512 -4.3270836 -4.3338413 -4.3353782][-4.3199105 -4.3086724 -4.2925711 -4.279706 -4.2685547 -4.2623196 -4.2652431 -4.2791729 -4.2967467 -4.3145437 -4.3318477 -4.3424025 -4.345274 -4.3445835 -4.3420429]]...]
INFO - root - 2017-12-05 13:11:29.993186: step 10910, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.812 sec/batch; 72h:34m:28s remains)
INFO - root - 2017-12-05 13:11:38.591015: step 10920, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 76h:11m:09s remains)
INFO - root - 2017-12-05 13:11:47.161299: step 10930, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 77h:05m:30s remains)
INFO - root - 2017-12-05 13:11:55.684852: step 10940, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 77h:40m:21s remains)
INFO - root - 2017-12-05 13:12:04.096087: step 10950, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:39m:49s remains)
INFO - root - 2017-12-05 13:12:12.554858: step 10960, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 77h:20m:05s remains)
INFO - root - 2017-12-05 13:12:21.041841: step 10970, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 75h:58m:49s remains)
INFO - root - 2017-12-05 13:12:29.566137: step 10980, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 76h:44m:09s remains)
INFO - root - 2017-12-05 13:12:37.926892: step 10990, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 74h:19m:04s remains)
INFO - root - 2017-12-05 13:12:46.308739: step 11000, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.774 sec/batch; 69h:09m:35s remains)
2017-12-05 13:12:47.006060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3230152 -4.3140383 -4.3102674 -4.3100915 -4.3166747 -4.3240581 -4.3259521 -4.3268576 -4.3304915 -4.3330755 -4.3316674 -4.3287525 -4.3334527 -4.3427353 -4.351727][-4.2955456 -4.2809782 -4.275003 -4.2738719 -4.2815924 -4.2903633 -4.2923603 -4.2932267 -4.2998271 -4.305686 -4.3038521 -4.2981071 -4.3043222 -4.3192673 -4.3363891][-4.2626429 -4.2397566 -4.2269425 -4.222815 -4.2295022 -4.2372336 -4.2374954 -4.2390032 -4.24991 -4.2612247 -4.2588677 -4.2470484 -4.2537422 -4.2772412 -4.3059473][-4.2278738 -4.1943707 -4.1723905 -4.1650243 -4.1700206 -4.1707973 -4.1589532 -4.1533346 -4.1750007 -4.19919 -4.2015877 -4.1871586 -4.1955113 -4.2283478 -4.2684612][-4.2009196 -4.1593447 -4.1279655 -4.1160026 -4.1151261 -4.0992384 -4.0602074 -4.0417838 -4.0826941 -4.1343679 -4.150609 -4.1419973 -4.1561165 -4.1948686 -4.2385888][-4.1862121 -4.1389542 -4.0971746 -4.0725665 -4.0529051 -4.0046606 -3.9231787 -3.8880954 -3.9621716 -4.0561018 -4.0961223 -4.1008253 -4.123105 -4.1675258 -4.2106886][-4.1866603 -4.1404667 -4.0963068 -4.059 -4.0194454 -3.9401286 -3.8235135 -3.7818694 -3.8803911 -4.0012736 -4.0627074 -4.0840397 -4.111475 -4.1578212 -4.1982989][-4.20253 -4.1644874 -4.1255455 -4.0867248 -4.044724 -3.9692016 -3.8662026 -3.8363614 -3.9177091 -4.0130882 -4.0698633 -4.0946631 -4.1225328 -4.1674614 -4.2044606][-4.2247491 -4.1978431 -4.1674428 -4.1336451 -4.099174 -4.044548 -3.9773717 -3.9625535 -4.0093851 -4.0647821 -4.1017237 -4.1199174 -4.1456866 -4.1865044 -4.2195206][-4.2429166 -4.2279434 -4.2091002 -4.1850042 -4.1641297 -4.1341925 -4.101162 -4.0956969 -4.1177988 -4.1438293 -4.1598773 -4.16403 -4.1820021 -4.2147841 -4.2391548][-4.2562504 -4.246212 -4.2353969 -4.222384 -4.2181525 -4.2111831 -4.2030253 -4.2047043 -4.2126122 -4.2228069 -4.226306 -4.217772 -4.2268424 -4.2491078 -4.2631631][-4.2698174 -4.2581706 -4.2494817 -4.2447524 -4.2512908 -4.259037 -4.2658286 -4.2744818 -4.2819476 -4.2874346 -4.2857056 -4.273581 -4.2731228 -4.2821412 -4.2887607][-4.2838116 -4.2710891 -4.2637382 -4.2650561 -4.2765188 -4.2893357 -4.3005118 -4.3106627 -4.3193097 -4.3232765 -4.3205414 -4.311203 -4.3078036 -4.3096161 -4.3119164][-4.2991071 -4.286561 -4.2805686 -4.2839231 -4.2952809 -4.3084493 -4.319685 -4.3304195 -4.3401361 -4.343513 -4.3400908 -4.333426 -4.329885 -4.3290672 -4.3292394][-4.3221207 -4.3120904 -4.3065305 -4.3079286 -4.3154545 -4.3244829 -4.3332634 -4.3420892 -4.3506684 -4.3538918 -4.3517776 -4.3480754 -4.3455439 -4.3433762 -4.3417025]]...]
INFO - root - 2017-12-05 13:12:55.636513: step 11010, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:41m:59s remains)
INFO - root - 2017-12-05 13:13:04.131929: step 11020, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 76h:06m:52s remains)
INFO - root - 2017-12-05 13:13:12.594182: step 11030, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 73h:40m:51s remains)
INFO - root - 2017-12-05 13:13:21.170494: step 11040, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 78h:59m:53s remains)
INFO - root - 2017-12-05 13:13:29.703600: step 11050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:20m:02s remains)
INFO - root - 2017-12-05 13:13:38.207975: step 11060, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 76h:24m:29s remains)
INFO - root - 2017-12-05 13:13:46.794310: step 11070, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 77h:30m:42s remains)
INFO - root - 2017-12-05 13:13:55.284856: step 11080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:36m:56s remains)
INFO - root - 2017-12-05 13:14:03.898459: step 11090, loss = 2.10, batch loss = 2.04 (9.0 examples/sec; 0.887 sec/batch; 79h:12m:45s remains)
INFO - root - 2017-12-05 13:14:12.190548: step 11100, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.764 sec/batch; 68h:14m:46s remains)
2017-12-05 13:14:12.888389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2151012 -4.2126732 -4.2117729 -4.2112813 -4.20815 -4.2040854 -4.2010837 -4.1991429 -4.20157 -4.2076793 -4.2099109 -4.2053456 -4.2032847 -4.2094369 -4.2171521][-4.2122531 -4.2149911 -4.2184362 -4.2183719 -4.2107272 -4.1986256 -4.1889644 -4.1825137 -4.1829247 -4.1921597 -4.2019076 -4.204215 -4.2077084 -4.2192192 -4.2296128][-4.2047663 -4.2121029 -4.2177825 -4.2148004 -4.19976 -4.1771054 -4.1605959 -4.1548958 -4.1600976 -4.1776166 -4.1977477 -4.2087874 -4.2181931 -4.2319164 -4.2419333][-4.2100759 -4.2169352 -4.2176905 -4.2055445 -4.1782341 -4.1417227 -4.1167097 -4.112936 -4.1282911 -4.1594172 -4.1940093 -4.2168727 -4.2319107 -4.2465949 -4.2526951][-4.2201147 -4.2191525 -4.2085838 -4.1838074 -4.1428733 -4.08772 -4.0456605 -4.0433617 -4.071691 -4.1242204 -4.1852384 -4.2277923 -4.2500224 -4.2619038 -4.2590518][-4.2260604 -4.2165589 -4.1955242 -4.1584964 -4.1009703 -4.02263 -3.9508398 -3.9390342 -3.9752719 -4.0460682 -4.1338282 -4.2011366 -4.23924 -4.2588468 -4.2542977][-4.2297988 -4.2201271 -4.1966519 -4.1519208 -4.0809011 -3.986717 -3.8907595 -3.8604951 -3.8918929 -3.9682946 -4.0670595 -4.1510043 -4.2051506 -4.2358379 -4.2343636][-4.247787 -4.2458606 -4.2239137 -4.1748829 -4.1004324 -4.0087438 -3.911185 -3.8669288 -3.8822131 -3.9460039 -4.0309234 -4.1139927 -4.1712689 -4.2003236 -4.1946163][-4.2725267 -4.2753215 -4.2563024 -4.2065816 -4.1377859 -4.0654745 -3.9920893 -3.9519432 -3.950829 -3.9901893 -4.0509958 -4.1186647 -4.1637855 -4.1756272 -4.1594191][-4.289959 -4.29191 -4.2728415 -4.2233233 -4.1614752 -4.10951 -4.0768652 -4.0602837 -4.0580416 -4.0770278 -4.1124763 -4.1508851 -4.1687651 -4.1574845 -4.1305323][-4.2808895 -4.2804394 -4.2649221 -4.2199874 -4.1660995 -4.1301756 -4.1298184 -4.1407828 -4.1480455 -4.1556158 -4.1686149 -4.1783552 -4.1700273 -4.1408358 -4.111578][-4.2559924 -4.2512574 -4.2354913 -4.1932726 -4.1443696 -4.1212626 -4.1403637 -4.1709585 -4.1938004 -4.2045445 -4.2095156 -4.2018886 -4.1766195 -4.1392536 -4.1125393][-4.2445588 -4.2363768 -4.2170625 -4.1741638 -4.1270695 -4.1116652 -4.1356821 -4.17193 -4.2056041 -4.2242904 -4.2293677 -4.2163773 -4.1860676 -4.1498222 -4.1303649][-4.2588005 -4.2507396 -4.2303648 -4.1891842 -4.1454859 -4.133101 -4.1512752 -4.1821756 -4.2188821 -4.2438431 -4.2503271 -4.235661 -4.2045388 -4.1709452 -4.1562028][-4.2788277 -4.2694192 -4.2481713 -4.2138872 -4.1800694 -4.1716671 -4.18504 -4.2093387 -4.2431941 -4.2659211 -4.2679262 -4.2470303 -4.2120528 -4.1820011 -4.1747246]]...]
INFO - root - 2017-12-05 13:14:21.481437: step 11110, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 76h:40m:03s remains)
INFO - root - 2017-12-05 13:14:30.090537: step 11120, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 78h:18m:24s remains)
INFO - root - 2017-12-05 13:14:38.645746: step 11130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:59m:38s remains)
INFO - root - 2017-12-05 13:14:47.052708: step 11140, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 73h:47m:11s remains)
INFO - root - 2017-12-05 13:14:55.540737: step 11150, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 72h:54m:49s remains)
INFO - root - 2017-12-05 13:15:04.015065: step 11160, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 76h:23m:48s remains)
INFO - root - 2017-12-05 13:15:12.650196: step 11170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 75h:41m:05s remains)
INFO - root - 2017-12-05 13:15:21.218348: step 11180, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 73h:47m:22s remains)
INFO - root - 2017-12-05 13:15:29.817240: step 11190, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 77h:35m:25s remains)
INFO - root - 2017-12-05 13:15:38.225253: step 11200, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.741 sec/batch; 66h:09m:40s remains)
2017-12-05 13:15:38.985162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2255449 -4.2420878 -4.257689 -4.2668958 -4.2674661 -4.2635136 -4.2647152 -4.2692757 -4.2697935 -4.2632246 -4.2513418 -4.2386308 -4.230937 -4.22955 -4.2327466][-4.2004151 -4.2199373 -4.2357836 -4.2457466 -4.2491417 -4.2488933 -4.2549896 -4.2665858 -4.2734032 -4.2692323 -4.2554312 -4.2380967 -4.225513 -4.2225127 -4.2264452][-4.1783295 -4.1978049 -4.2108417 -4.217382 -4.2201476 -4.2228937 -4.2330427 -4.2544742 -4.2711043 -4.2743688 -4.264215 -4.24859 -4.23631 -4.232029 -4.2328634][-4.175498 -4.1889324 -4.1941419 -4.1910186 -4.1859417 -4.1848745 -4.1955004 -4.22479 -4.2546492 -4.2703166 -4.2716589 -4.2665715 -4.2626033 -4.2605109 -4.2579803][-4.1904373 -4.1992207 -4.19595 -4.1760707 -4.1543384 -4.1408477 -4.1434069 -4.1714787 -4.2111921 -4.2454667 -4.267262 -4.2766609 -4.2837524 -4.2879658 -4.2871122][-4.2107611 -4.2193661 -4.2136269 -4.1802077 -4.1354132 -4.0978312 -4.0778236 -4.0881662 -4.1314583 -4.190279 -4.2379823 -4.2658677 -4.283267 -4.2959585 -4.3022652][-4.2312694 -4.2435174 -4.2380242 -4.1958566 -4.1306362 -4.0621018 -4.0030184 -3.974149 -4.0094008 -4.0973682 -4.1804228 -4.2311296 -4.2635169 -4.2881155 -4.3057914][-4.2441874 -4.2626438 -4.2608409 -4.2191367 -4.1462774 -4.0551214 -3.9570017 -3.8727317 -3.8894563 -4.0031204 -4.11558 -4.1888027 -4.2324247 -4.2644587 -4.2916][-4.2498159 -4.2743487 -4.2812481 -4.2502384 -4.1862154 -4.0936012 -3.98714 -3.8843093 -3.875432 -3.9752209 -4.081377 -4.154 -4.1953607 -4.2227902 -4.251749][-4.252614 -4.2816305 -4.2969565 -4.2794461 -4.2317839 -4.1578937 -4.0732479 -3.9975305 -3.9811504 -4.0334806 -4.0942388 -4.1306434 -4.1413093 -4.1417379 -4.1717362][-4.2539515 -4.2857981 -4.3084006 -4.301517 -4.26977 -4.2183318 -4.159565 -4.1102695 -4.0938988 -4.1082706 -4.1176815 -4.1038742 -4.0591536 -4.0117488 -4.0420823][-4.2530746 -4.2847886 -4.3115811 -4.314467 -4.2965441 -4.2623053 -4.2228217 -4.1908712 -4.1740527 -4.1667042 -4.1434045 -4.0898695 -3.99088 -3.8859491 -3.9144068][-4.253562 -4.2855654 -4.3149877 -4.3253789 -4.3183212 -4.29737 -4.2718735 -4.2485547 -4.2301464 -4.2136211 -4.17978 -4.1139245 -4.0049529 -3.8928835 -3.8986654][-4.2536411 -4.2862573 -4.3184495 -4.3340831 -4.3347988 -4.324039 -4.3097525 -4.2926922 -4.2734547 -4.253993 -4.2218204 -4.1649342 -4.08353 -4.0089068 -4.0011396][-4.2521434 -4.2848248 -4.3178096 -4.335887 -4.3409204 -4.3357067 -4.3273692 -4.3166695 -4.3017163 -4.2840991 -4.256391 -4.2150044 -4.1623592 -4.1184654 -4.1057711]]...]
INFO - root - 2017-12-05 13:15:47.552047: step 11210, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 79h:07m:49s remains)
INFO - root - 2017-12-05 13:15:55.995491: step 11220, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 74h:12m:19s remains)
INFO - root - 2017-12-05 13:16:04.554913: step 11230, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 74h:26m:31s remains)
INFO - root - 2017-12-05 13:16:13.088284: step 11240, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 76h:30m:07s remains)
INFO - root - 2017-12-05 13:16:21.607196: step 11250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 75h:24m:19s remains)
INFO - root - 2017-12-05 13:16:30.153010: step 11260, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:34m:03s remains)
INFO - root - 2017-12-05 13:16:38.466242: step 11270, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 77h:23m:58s remains)
INFO - root - 2017-12-05 13:16:46.986612: step 11280, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:57m:31s remains)
INFO - root - 2017-12-05 13:16:55.462990: step 11290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 76h:36m:20s remains)
INFO - root - 2017-12-05 13:17:03.858135: step 11300, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 73h:29m:04s remains)
2017-12-05 13:17:04.658649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1968431 -4.2165585 -4.2241616 -4.22528 -4.2117276 -4.199368 -4.1909547 -4.1788511 -4.1678677 -4.1527014 -4.130055 -4.085001 -4.0572786 -4.0733318 -4.1042376][-4.1835575 -4.2033391 -4.2083807 -4.210062 -4.1984644 -4.1895733 -4.1868558 -4.1779342 -4.167819 -4.1496582 -4.1231747 -4.0750346 -4.0528378 -4.0767269 -4.1069059][-4.1590738 -4.1790395 -4.1850343 -4.1874976 -4.1791024 -4.1742077 -4.1752605 -4.1711359 -4.1665154 -4.1525393 -4.1281638 -4.0849104 -4.0719128 -4.1006618 -4.1281772][-4.1300955 -4.1521416 -4.1642284 -4.1699886 -4.1603703 -4.1544266 -4.1549859 -4.1497173 -4.1477895 -4.1443539 -4.1342006 -4.1070127 -4.1048031 -4.1346974 -4.15992][-4.1096363 -4.1342716 -4.1493692 -4.1556835 -4.1433792 -4.1341858 -4.1295772 -4.1214061 -4.1207442 -4.1237454 -4.12384 -4.1082854 -4.1168194 -4.1499844 -4.176548][-4.095489 -4.1213417 -4.1395369 -4.1459131 -4.1293545 -4.1120677 -4.0996461 -4.0893955 -4.0871153 -4.0847721 -4.0808315 -4.065743 -4.0792518 -4.1201382 -4.1552014][-4.095355 -4.1217022 -4.1393943 -4.1433005 -4.1193161 -4.0884938 -4.0665956 -4.0560451 -4.052722 -4.0416703 -4.0255523 -4.0020075 -4.0181675 -4.0698566 -4.1121969][-4.1027331 -4.1256757 -4.1351709 -4.1313114 -4.1008348 -4.0654693 -4.043757 -4.0388684 -4.0387387 -4.02631 -4.0045147 -3.9712412 -3.984858 -4.0430908 -4.0878696][-4.1129241 -4.1276593 -4.1257725 -4.1171446 -4.0918512 -4.0647097 -4.0497284 -4.0504756 -4.0566854 -4.0539322 -4.0455151 -4.0203381 -4.0304 -4.080564 -4.1188431][-4.1152582 -4.1221476 -4.1178246 -4.1144443 -4.0991292 -4.0827279 -4.074522 -4.0766964 -4.0871482 -4.0922518 -4.0928297 -4.07631 -4.0838718 -4.125453 -4.1613388][-4.1114316 -4.1156526 -4.1128097 -4.1147542 -4.1074142 -4.0985494 -4.095613 -4.0974965 -4.1049924 -4.1071148 -4.1075177 -4.0951481 -4.1029773 -4.1428142 -4.1823139][-4.0989895 -4.1054626 -4.1047206 -4.110044 -4.1125093 -4.1155038 -4.1187391 -4.1187825 -4.1172018 -4.1083565 -4.1011009 -4.0918903 -4.1035166 -4.144134 -4.18838][-4.0859065 -4.0917039 -4.0946608 -4.1040072 -4.1157985 -4.13365 -4.1477971 -4.1469526 -4.1350942 -4.1215653 -4.1099906 -4.098846 -4.1110668 -4.1499777 -4.1896181][-4.0795264 -4.0854483 -4.09295 -4.1073012 -4.1298561 -4.1585369 -4.1806703 -4.1780877 -4.1617475 -4.1507053 -4.1429224 -4.1317263 -4.136312 -4.1624212 -4.18728][-4.0817981 -4.0926518 -4.107409 -4.1292181 -4.1552181 -4.1812921 -4.2056074 -4.206038 -4.1948047 -4.1899414 -4.1916704 -4.1842542 -4.1800294 -4.1878452 -4.1948586]]...]
INFO - root - 2017-12-05 13:17:13.266355: step 11310, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 79h:18m:29s remains)
INFO - root - 2017-12-05 13:17:21.760497: step 11320, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 73h:11m:48s remains)
INFO - root - 2017-12-05 13:17:30.328309: step 11330, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 75h:48m:28s remains)
INFO - root - 2017-12-05 13:17:38.819615: step 11340, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 76h:42m:22s remains)
INFO - root - 2017-12-05 13:17:47.244189: step 11350, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 74h:33m:58s remains)
INFO - root - 2017-12-05 13:17:55.838965: step 11360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 77h:19m:10s remains)
INFO - root - 2017-12-05 13:18:04.346063: step 11370, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 76h:15m:27s remains)
INFO - root - 2017-12-05 13:18:12.772043: step 11380, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 77h:02m:27s remains)
INFO - root - 2017-12-05 13:18:21.176590: step 11390, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.805 sec/batch; 71h:49m:09s remains)
INFO - root - 2017-12-05 13:18:29.652257: step 11400, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 73h:56m:48s remains)
2017-12-05 13:18:30.469845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3104911 -4.305119 -4.30645 -4.3086119 -4.31012 -4.3151913 -4.3203773 -4.3182712 -4.3107252 -4.305274 -4.3040619 -4.3085375 -4.321352 -4.3353977 -4.3443866][-4.2793136 -4.2696648 -4.2680607 -4.2668118 -4.2671785 -4.2759886 -4.2852039 -4.2858253 -4.2790933 -4.2743397 -4.2753181 -4.2809076 -4.297298 -4.3178916 -4.3339548][-4.2396297 -4.225378 -4.2187519 -4.2138405 -4.2114034 -4.2215848 -4.2357826 -4.2402754 -4.2352552 -4.2302494 -4.2320666 -4.238306 -4.2566948 -4.2852778 -4.3135543][-4.2040448 -4.1830888 -4.1673341 -4.1535349 -4.1430826 -4.1504459 -4.1670752 -4.1717143 -4.1723127 -4.177515 -4.1880794 -4.2000217 -4.2188873 -4.2485452 -4.2847042][-4.1692533 -4.1408129 -4.1151476 -4.0869932 -4.0630569 -4.0535188 -4.0605459 -4.0614824 -4.0754604 -4.1081204 -4.1413336 -4.1671572 -4.1857829 -4.2107139 -4.2478075][-4.1438432 -4.1114078 -4.0772014 -4.0335822 -3.9876697 -3.9460373 -3.9202118 -3.8967395 -3.9237645 -4.0014505 -4.0739 -4.1196694 -4.1400909 -4.1643925 -4.20574][-4.13624 -4.0996704 -4.0606418 -4.0125132 -3.9529665 -3.8714674 -3.7842679 -3.7004454 -3.7251947 -3.8628969 -3.9912808 -4.0705986 -4.1046309 -4.1389275 -4.1898909][-4.1268611 -4.0869136 -4.0537772 -4.0182309 -3.9640458 -3.8637185 -3.7236147 -3.5719478 -3.581259 -3.7640283 -3.9387319 -4.0484257 -4.1001291 -4.147871 -4.2049589][-4.1140766 -4.0745854 -4.0590048 -4.0491195 -4.01705 -3.9349833 -3.808368 -3.6707902 -3.6570897 -3.796792 -3.9565291 -4.0680933 -4.125586 -4.1781297 -4.2327175][-4.0943418 -4.0553441 -4.0563073 -4.0742512 -4.0750132 -4.0344305 -3.9524007 -3.8573394 -3.8350043 -3.9152408 -4.024395 -4.1126738 -4.16238 -4.2078786 -4.2546697][-4.0779757 -4.0385156 -4.044744 -4.080924 -4.1145449 -4.1123347 -4.06535 -4.0055966 -3.987112 -4.0273819 -4.0964594 -4.1596141 -4.2001514 -4.2354779 -4.2707348][-4.0830064 -4.0438328 -4.0471849 -4.0850892 -4.1388226 -4.1639462 -4.1426787 -4.1101418 -4.1009369 -4.1199689 -4.1654043 -4.2131839 -4.2444329 -4.2703543 -4.2932568][-4.1242537 -4.094492 -4.0967336 -4.1215706 -4.1704106 -4.2030892 -4.1995354 -4.1869936 -4.1851945 -4.1950774 -4.2258468 -4.2596183 -4.2819943 -4.3015637 -4.3171954][-4.2011185 -4.1833048 -4.1859732 -4.1988978 -4.2337613 -4.2591357 -4.2609162 -4.2578921 -4.2573605 -4.2609887 -4.2790227 -4.2990923 -4.3121691 -4.3255272 -4.337687][-4.2731681 -4.2626987 -4.2631373 -4.2712684 -4.2932792 -4.3080721 -4.3109241 -4.3124084 -4.3133149 -4.3137612 -4.3236508 -4.3342819 -4.3408141 -4.3486271 -4.3558869]]...]
INFO - root - 2017-12-05 13:18:38.979539: step 11410, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 77h:01m:15s remains)
INFO - root - 2017-12-05 13:18:47.441342: step 11420, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:15m:27s remains)
INFO - root - 2017-12-05 13:18:55.829982: step 11430, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 74h:55m:02s remains)
INFO - root - 2017-12-05 13:19:04.228255: step 11440, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.809 sec/batch; 72h:11m:30s remains)
INFO - root - 2017-12-05 13:19:12.763606: step 11450, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.866 sec/batch; 77h:15m:27s remains)
INFO - root - 2017-12-05 13:19:21.253390: step 11460, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.847 sec/batch; 75h:29m:34s remains)
INFO - root - 2017-12-05 13:19:29.814973: step 11470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 76h:24m:10s remains)
INFO - root - 2017-12-05 13:19:38.453615: step 11480, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 75h:20m:58s remains)
INFO - root - 2017-12-05 13:19:47.139323: step 11490, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 77h:57m:00s remains)
INFO - root - 2017-12-05 13:19:55.522243: step 11500, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 75h:17m:00s remains)
2017-12-05 13:19:56.289405: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2593746 -4.2588434 -4.2427859 -4.2142653 -4.1841011 -4.1636744 -4.1568341 -4.1614752 -4.1716208 -4.1972713 -4.2297387 -4.2565722 -4.2742782 -4.2833343 -4.2935643][-4.2478294 -4.2489748 -4.235754 -4.2066441 -4.1699924 -4.13841 -4.1223769 -4.1228166 -4.1319137 -4.1651545 -4.2105489 -4.2480526 -4.2709403 -4.2823887 -4.2952466][-4.23318 -4.2380004 -4.2280478 -4.2027535 -4.1659856 -4.1230645 -4.0964222 -4.0960026 -4.1041117 -4.1401677 -4.197793 -4.2432942 -4.2671237 -4.2785287 -4.2948737][-4.214335 -4.2238617 -4.2206941 -4.2033653 -4.1731095 -4.12112 -4.0799065 -4.0764728 -4.0833654 -4.1127934 -4.1751289 -4.22736 -4.2561932 -4.2701735 -4.2913456][-4.1944256 -4.2095079 -4.2156425 -4.2077222 -4.1807475 -4.1192746 -4.0618734 -4.0570741 -4.0717244 -4.0966706 -4.1566458 -4.212429 -4.2474184 -4.2640409 -4.2879548][-4.1649461 -4.1872978 -4.1977577 -4.19459 -4.1689377 -4.10058 -4.0275846 -4.0278831 -4.065125 -4.0978832 -4.1582422 -4.213048 -4.2488966 -4.2662644 -4.2876315][-4.1104856 -4.1457253 -4.1639404 -4.1691828 -4.1446705 -4.0650144 -3.9679234 -3.9635358 -4.0267267 -4.0847507 -4.1559281 -4.2180629 -4.2560229 -4.2736454 -4.2884521][-4.0236044 -4.07401 -4.1084533 -4.1272869 -4.1067038 -4.0179172 -3.8889513 -3.8610067 -3.9470322 -4.0441246 -4.1399045 -4.2181864 -4.2679124 -4.28704 -4.2932382][-3.9183011 -3.9735484 -4.0271363 -4.0643468 -4.0570388 -3.9664283 -3.8062634 -3.7348657 -3.8281713 -3.969342 -4.1012759 -4.2054734 -4.2709465 -4.2984533 -4.2993355][-3.8662286 -3.902909 -3.9623594 -4.0211177 -4.0398831 -3.9739375 -3.8161938 -3.7040703 -3.7700868 -3.924094 -4.0761342 -4.1943965 -4.2693825 -4.303997 -4.3054419][-3.9039836 -3.9158914 -3.9614608 -4.0222807 -4.05291 -4.0215354 -3.901557 -3.7925763 -3.8151505 -3.9354138 -4.0754004 -4.1907043 -4.2663507 -4.3044224 -4.3099036][-3.9936652 -3.9864843 -4.0098958 -4.0569372 -4.0891852 -4.0824804 -4.00789 -3.9233167 -3.9166586 -3.9871297 -4.0934443 -4.1932912 -4.2621155 -4.3010616 -4.3123374][-4.1074095 -4.0883741 -4.0928707 -4.1187563 -4.1474724 -4.1581664 -4.1225734 -4.0680242 -4.0490932 -4.0757976 -4.1382294 -4.2109232 -4.2686205 -4.3037367 -4.3164353][-4.2004285 -4.1799564 -4.1725411 -4.1821074 -4.203249 -4.2222614 -4.2099524 -4.1732373 -4.1511822 -4.1536937 -4.1861749 -4.2346411 -4.2779479 -4.3081994 -4.3202682][-4.2593188 -4.2449627 -4.2326961 -4.2328987 -4.2468519 -4.2685289 -4.2700982 -4.2449226 -4.223721 -4.21671 -4.231864 -4.2628303 -4.2908983 -4.3129153 -4.32206]]...]
INFO - root - 2017-12-05 13:20:04.935917: step 11510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:47m:47s remains)
INFO - root - 2017-12-05 13:20:13.545977: step 11520, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 77h:22m:14s remains)
INFO - root - 2017-12-05 13:20:22.093638: step 11530, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 76h:46m:45s remains)
INFO - root - 2017-12-05 13:20:30.573044: step 11540, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 78h:56m:14s remains)
INFO - root - 2017-12-05 13:20:38.984665: step 11550, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 73h:56m:28s remains)
INFO - root - 2017-12-05 13:20:47.604688: step 11560, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:54m:34s remains)
INFO - root - 2017-12-05 13:20:56.181117: step 11570, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 76h:17m:00s remains)
INFO - root - 2017-12-05 13:21:04.730409: step 11580, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 74h:37m:27s remains)
INFO - root - 2017-12-05 13:21:13.298264: step 11590, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 77h:55m:24s remains)
INFO - root - 2017-12-05 13:21:21.763591: step 11600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:52m:23s remains)
2017-12-05 13:21:22.587297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3054142 -4.3063278 -4.3140097 -4.3235064 -4.3296952 -4.3331504 -4.3346071 -4.3348126 -4.3358159 -4.3363194 -4.3338633 -4.3339491 -4.3354635 -4.3384337 -4.3426065][-4.2788196 -4.2777953 -4.28684 -4.2976646 -4.3024206 -4.3068533 -4.3114562 -4.3135166 -4.314723 -4.3156767 -4.3142786 -4.3171973 -4.3195043 -4.3251681 -4.3307414][-4.2444267 -4.2416353 -4.2537251 -4.2646189 -4.2651896 -4.2686739 -4.2754192 -4.2793674 -4.2821832 -4.2862396 -4.2884474 -4.2956591 -4.3019276 -4.312901 -4.3203397][-4.19841 -4.1895528 -4.2027469 -4.2151113 -4.2146459 -4.21752 -4.2250724 -4.2279477 -4.2318945 -4.2444329 -4.2554936 -4.26966 -4.2785378 -4.2939596 -4.3059182][-4.1456308 -4.1276827 -4.1360908 -4.1482553 -4.1474075 -4.1438546 -4.1492562 -4.15166 -4.16352 -4.1912622 -4.2179656 -4.2418427 -4.2547646 -4.2740588 -4.2904315][-4.1016707 -4.07152 -4.0672545 -4.0720954 -4.0610204 -4.04331 -4.0378008 -4.0381074 -4.0658369 -4.1168332 -4.1644549 -4.2000608 -4.2244382 -4.2491527 -4.2695274][-4.0710897 -4.0305495 -4.0118394 -4.0009408 -3.9750681 -3.9381468 -3.9085493 -3.899446 -3.9439909 -4.0177288 -4.0857821 -4.1411977 -4.1833205 -4.2182183 -4.2463875][-4.0664248 -4.0217428 -3.9955134 -3.9733226 -3.9347904 -3.8783569 -3.8164454 -3.7923026 -3.8477111 -3.9350777 -4.0190592 -4.0910449 -4.1468096 -4.1933093 -4.2287288][-4.0799575 -4.04198 -4.0194855 -3.9966898 -3.9595146 -3.9035311 -3.8347311 -3.8075063 -3.853622 -3.9261904 -4.0023327 -4.0731091 -4.1329064 -4.1834483 -4.2191896][-4.0883565 -4.0610418 -4.048574 -4.0347328 -4.0057392 -3.9613354 -3.9078193 -3.88659 -3.9091628 -3.9512782 -4.0106635 -4.0760155 -4.1351795 -4.1834283 -4.2158051][-4.1017332 -4.0790882 -4.0712194 -4.062479 -4.0404963 -4.0089312 -3.9708793 -3.9525926 -3.9591529 -3.9805408 -4.0256228 -4.0844941 -4.1401296 -4.1828752 -4.2135277][-4.1282244 -4.1066751 -4.0980864 -4.0898666 -4.0726233 -4.0489984 -4.0199943 -4.0017262 -4.0026994 -4.0161624 -4.0500717 -4.0983887 -4.14513 -4.1814861 -4.2130141][-4.1823053 -4.1654844 -4.1571116 -4.1495872 -4.1345596 -4.1148071 -4.0894051 -4.0686064 -4.0641594 -4.072156 -4.094913 -4.1303611 -4.1672616 -4.1998916 -4.23035][-4.2462525 -4.23457 -4.2296758 -4.2242947 -4.2124171 -4.1976209 -4.1788588 -4.1615362 -4.1555085 -4.1579461 -4.1702347 -4.1913242 -4.217 -4.2415218 -4.264226][-4.2931395 -4.2845483 -4.2826805 -4.2807994 -4.2744904 -4.2662392 -4.2560287 -4.2467918 -4.2440786 -4.2444558 -4.2485414 -4.2580791 -4.2720175 -4.2867227 -4.2996073]]...]
INFO - root - 2017-12-05 13:21:31.203848: step 11610, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 76h:44m:48s remains)
INFO - root - 2017-12-05 13:21:39.690730: step 11620, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 74h:53m:15s remains)
INFO - root - 2017-12-05 13:21:48.192161: step 11630, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 74h:13m:20s remains)
INFO - root - 2017-12-05 13:21:56.752582: step 11640, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 75h:39m:28s remains)
INFO - root - 2017-12-05 13:22:05.239903: step 11650, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 76h:07m:10s remains)
INFO - root - 2017-12-05 13:22:13.795304: step 11660, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 77h:33m:35s remains)
INFO - root - 2017-12-05 13:22:22.355329: step 11670, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 75h:09m:28s remains)
INFO - root - 2017-12-05 13:22:30.935260: step 11680, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 77h:36m:48s remains)
INFO - root - 2017-12-05 13:22:39.646516: step 11690, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 79h:01m:00s remains)
INFO - root - 2017-12-05 13:22:48.023785: step 11700, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 76h:18m:33s remains)
2017-12-05 13:22:48.775305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1347108 -4.1104522 -4.1112919 -4.1340542 -4.1550617 -4.1610241 -4.1530609 -4.1335044 -4.1311164 -4.139431 -4.1509356 -4.1493006 -4.1331139 -4.1171508 -4.1224146][-4.1398172 -4.1134243 -4.1157432 -4.1421208 -4.1610184 -4.1638322 -4.155519 -4.1400261 -4.1400952 -4.1507082 -4.1672077 -4.173419 -4.1643291 -4.1481633 -4.1469431][-4.1418624 -4.1175423 -4.1241961 -4.1540494 -4.1698685 -4.1723509 -4.1673841 -4.1575184 -4.1588979 -4.1684852 -4.1827517 -4.1879272 -4.1844697 -4.1726594 -4.1682906][-4.1396255 -4.1182652 -4.1238265 -4.1502433 -4.1657457 -4.1654062 -4.1615233 -4.1570272 -4.1682963 -4.1824818 -4.1924458 -4.1932969 -4.1952109 -4.1925488 -4.1898446][-4.1206813 -4.102118 -4.10645 -4.1305223 -4.1438189 -4.135951 -4.1210632 -4.1185966 -4.145505 -4.17339 -4.1860995 -4.1865296 -4.1920495 -4.2011266 -4.2056222][-4.0791559 -4.0612469 -4.0669751 -4.0902472 -4.0964117 -4.0682058 -4.0302043 -4.0291023 -4.0803409 -4.1309094 -4.1577177 -4.1634331 -4.1747856 -4.1923265 -4.2030277][-4.0551581 -4.035593 -4.0398474 -4.0574422 -4.0463624 -3.9873111 -3.9155235 -3.9146781 -3.9955792 -4.0733242 -4.118557 -4.1366253 -4.1528382 -4.17085 -4.1789389][-4.076282 -4.0614157 -4.0666704 -4.0772772 -4.0551772 -3.9821439 -3.8989854 -3.8953366 -3.9766877 -4.0539861 -4.0973043 -4.1192665 -4.1356049 -4.14748 -4.1487193][-4.1171832 -4.1102276 -4.119051 -4.12903 -4.1127415 -4.0552197 -3.9899426 -3.9807527 -4.0320559 -4.0816979 -4.1063232 -4.1238537 -4.1419039 -4.1514673 -4.147541][-4.1500387 -4.1504965 -4.1617918 -4.1752634 -4.1703444 -4.1374826 -4.0931549 -4.0754089 -4.0946512 -4.1185904 -4.129643 -4.1444263 -4.1641188 -4.1746969 -4.1755772][-4.1810122 -4.189549 -4.203012 -4.2167854 -4.2164273 -4.1938744 -4.158318 -4.1308541 -4.1265683 -4.1338243 -4.1409883 -4.1539745 -4.1737847 -4.1854272 -4.1938167][-4.2117267 -4.2205462 -4.2307053 -4.2390823 -4.2357736 -4.2126222 -4.1791072 -4.1510077 -4.1349959 -4.1362705 -4.1451759 -4.1565962 -4.1745653 -4.1869049 -4.1999645][-4.2369304 -4.2391663 -4.2391973 -4.240562 -4.2325 -4.2070136 -4.176909 -4.1561203 -4.1419473 -4.1430297 -4.1537352 -4.162663 -4.1802163 -4.1938415 -4.2076888][-4.2582731 -4.2477088 -4.2351446 -4.2310076 -4.2172594 -4.1880431 -4.1599822 -4.1458149 -4.1378531 -4.1425104 -4.1554527 -4.1668253 -4.1852546 -4.2010012 -4.2133355][-4.2551041 -4.2366786 -4.2204933 -4.2177796 -4.209188 -4.1830268 -4.1586127 -4.1467876 -4.1381903 -4.1423087 -4.1553979 -4.1717548 -4.1878805 -4.2003374 -4.2084985]]...]
INFO - root - 2017-12-05 13:22:57.416444: step 11710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:50m:36s remains)
INFO - root - 2017-12-05 13:23:06.050470: step 11720, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 77h:20m:38s remains)
INFO - root - 2017-12-05 13:23:14.521363: step 11730, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 73h:21m:26s remains)
INFO - root - 2017-12-05 13:23:23.115593: step 11740, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 77h:05m:44s remains)
INFO - root - 2017-12-05 13:23:31.600604: step 11750, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 76h:05m:54s remains)
INFO - root - 2017-12-05 13:23:40.008707: step 11760, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 76h:10m:11s remains)
INFO - root - 2017-12-05 13:23:48.603122: step 11770, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 76h:39m:18s remains)
INFO - root - 2017-12-05 13:23:57.241681: step 11780, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 78h:32m:36s remains)
INFO - root - 2017-12-05 13:24:05.887284: step 11790, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 77h:26m:04s remains)
INFO - root - 2017-12-05 13:24:14.337360: step 11800, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 73h:56m:02s remains)
2017-12-05 13:24:15.176213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2532043 -4.2441154 -4.2326088 -4.23962 -4.2510853 -4.24836 -4.2430015 -4.2407665 -4.2459807 -4.2543812 -4.2664285 -4.2799234 -4.2869916 -4.2867208 -4.2950654][-4.2134376 -4.2029166 -4.1971626 -4.2145715 -4.2315717 -4.2334337 -4.2300663 -4.2286582 -4.2361093 -4.2449226 -4.2547789 -4.2667952 -4.2752042 -4.2780619 -4.2866492][-4.1711264 -4.1535077 -4.1487608 -4.1750307 -4.1993055 -4.2077723 -4.21 -4.2140384 -4.2295218 -4.2424226 -4.2524786 -4.2651625 -4.276577 -4.2797775 -4.2841463][-4.1375933 -4.1121554 -4.0983992 -4.1159382 -4.1351361 -4.1476431 -4.1585531 -4.1734757 -4.2051897 -4.2328739 -4.2509212 -4.2699084 -4.2853723 -4.2883453 -4.2880917][-4.1200976 -4.0927949 -4.07069 -4.0724883 -4.0759172 -4.0823035 -4.0923905 -4.1132445 -4.1584764 -4.2046452 -4.2370887 -4.2657022 -4.2877045 -4.2915459 -4.2902927][-4.1199 -4.09746 -4.0764856 -4.0693316 -4.0626078 -4.0605259 -4.0650358 -4.0791421 -4.1182165 -4.1689053 -4.2060008 -4.2381186 -4.2650304 -4.2768965 -4.2831345][-4.1071396 -4.0883284 -4.0720625 -4.0651717 -4.0609751 -4.0618739 -4.0685968 -4.0788355 -4.1044636 -4.1457624 -4.1799178 -4.2051616 -4.2289128 -4.2460222 -4.2608232][-4.0839505 -4.06419 -4.0504179 -4.0446057 -4.0420532 -4.0474429 -4.0587363 -4.0731177 -4.0952206 -4.1310773 -4.163466 -4.1826105 -4.1980457 -4.2133861 -4.2333803][-4.0812173 -4.0593872 -4.0428486 -4.0340028 -4.0282211 -4.025527 -4.0321655 -4.0466123 -4.0757051 -4.1186709 -4.1560597 -4.1765037 -4.1859884 -4.1951246 -4.2160439][-4.1036377 -4.0829058 -4.0632057 -4.0549779 -4.04861 -4.0365973 -4.0340366 -4.0434175 -4.0696106 -4.1092076 -4.1468716 -4.1699924 -4.1803136 -4.1871691 -4.2103024][-4.1302752 -4.111516 -4.0945559 -4.0910869 -4.087698 -4.0750837 -4.070848 -4.0764289 -4.0955267 -4.1208177 -4.1453519 -4.1593652 -4.1662912 -4.1745133 -4.20372][-4.1472039 -4.1315384 -4.1161981 -4.111567 -4.1092677 -4.1021552 -4.1024833 -4.1090736 -4.1242795 -4.1369853 -4.1495233 -4.1575308 -4.1614041 -4.169486 -4.2018762][-4.1703444 -4.1582236 -4.1460924 -4.1412635 -4.1400309 -4.1375918 -4.141717 -4.1514564 -4.1633005 -4.1670461 -4.1714911 -4.1747103 -4.17456 -4.1789746 -4.2073364][-4.212647 -4.2046642 -4.197484 -4.1949754 -4.194592 -4.1945672 -4.19778 -4.2049565 -4.2114434 -4.2103291 -4.2122345 -4.2143335 -4.2126784 -4.2138109 -4.2322693][-4.2439241 -4.239501 -4.2367229 -4.2360215 -4.2363405 -4.2362003 -4.2360539 -4.2382665 -4.2413335 -4.241931 -4.2454252 -4.2485776 -4.2494864 -4.2520337 -4.2637815]]...]
INFO - root - 2017-12-05 13:24:23.741275: step 11810, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:51m:48s remains)
INFO - root - 2017-12-05 13:24:32.337660: step 11820, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 77h:28m:09s remains)
INFO - root - 2017-12-05 13:24:40.906564: step 11830, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 72h:53m:19s remains)
INFO - root - 2017-12-05 13:24:49.476445: step 11840, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 74h:24m:21s remains)
INFO - root - 2017-12-05 13:24:58.066452: step 11850, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 75h:42m:26s remains)
INFO - root - 2017-12-05 13:25:06.748168: step 11860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 77h:07m:52s remains)
INFO - root - 2017-12-05 13:25:15.329235: step 11870, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 76h:36m:37s remains)
INFO - root - 2017-12-05 13:25:24.088577: step 11880, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 78h:56m:03s remains)
INFO - root - 2017-12-05 13:25:32.629560: step 11890, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 74h:47m:55s remains)
INFO - root - 2017-12-05 13:25:41.103211: step 11900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 77h:30m:01s remains)
2017-12-05 13:25:41.819207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2805271 -4.275454 -4.2603946 -4.2458038 -4.2407656 -4.2368703 -4.2252493 -4.205245 -4.1790848 -4.1477618 -4.1260161 -4.12255 -4.1524997 -4.2103949 -4.2677412][-4.2907262 -4.287652 -4.2689285 -4.2490587 -4.2360415 -4.2259326 -4.208518 -4.1834474 -4.1554356 -4.1263857 -4.1173105 -4.1244264 -4.1550446 -4.2142177 -4.2738795][-4.2818594 -4.2737632 -4.2486706 -4.2274241 -4.2153735 -4.2063427 -4.1868572 -4.1599636 -4.1265812 -4.0996017 -4.1036854 -4.1222892 -4.1573663 -4.2161717 -4.2727089][-4.2651911 -4.2480474 -4.218905 -4.1991034 -4.1900511 -4.1839924 -4.1607533 -4.127542 -4.0853639 -4.064959 -4.0838437 -4.1166234 -4.1580534 -4.2137623 -4.263833][-4.24696 -4.2210164 -4.1879191 -4.1633964 -4.1476154 -4.1334281 -4.0988216 -4.0537181 -4.0053444 -4.0062776 -4.0529761 -4.103334 -4.1529703 -4.2060657 -4.2497025][-4.2319446 -4.2007289 -4.1651039 -4.1338053 -4.1028423 -4.071928 -4.0205097 -3.9541543 -3.8919959 -3.92494 -4.0112143 -4.0801754 -4.1405854 -4.1976728 -4.2406859][-4.2223587 -4.1946692 -4.161675 -4.1262569 -4.0851374 -4.0395045 -3.9671042 -3.8718491 -3.7871969 -3.8460743 -3.9646411 -4.0492392 -4.1228065 -4.1872029 -4.2339854][-4.2184067 -4.1969366 -4.1655078 -4.131237 -4.0896931 -4.0419612 -3.9663649 -3.8640909 -3.7774305 -3.83988 -3.9542542 -4.0356655 -4.1108589 -4.178791 -4.2298717][-4.2254968 -4.2106152 -4.1835556 -4.15647 -4.1227312 -4.0828738 -4.024117 -3.9329119 -3.8541932 -3.8984079 -3.9821262 -4.0460515 -4.1135268 -4.1788096 -4.2314091][-4.2367067 -4.2254434 -4.2016149 -4.1811886 -4.1586833 -4.1275105 -4.0879669 -4.0130458 -3.9418595 -3.9670832 -4.0253716 -4.0741382 -4.1333747 -4.193151 -4.2443037][-4.2495351 -4.2391405 -4.2163334 -4.1998019 -4.1851077 -4.163178 -4.1346231 -4.0774064 -4.0237684 -4.0441175 -4.0914693 -4.1318111 -4.179213 -4.2269974 -4.2683325][-4.2655249 -4.2525883 -4.2320385 -4.2164817 -4.2060933 -4.193222 -4.1731372 -4.1319652 -4.0975156 -4.1170645 -4.1562161 -4.1904321 -4.226079 -4.261004 -4.2890825][-4.2829876 -4.2627683 -4.240675 -4.2257986 -4.2180676 -4.2122612 -4.1995621 -4.1745973 -4.1580434 -4.1748343 -4.2025585 -4.2269955 -4.2508492 -4.2795625 -4.298861][-4.2947927 -4.2720432 -4.2524505 -4.2406759 -4.2364893 -4.2345166 -4.2294197 -4.2177796 -4.2134871 -4.2251759 -4.2403741 -4.2530389 -4.2675591 -4.2893276 -4.303699][-4.3077822 -4.2912545 -4.2770009 -4.2662544 -4.2624769 -4.2621932 -4.2617564 -4.2561688 -4.2545576 -4.2584047 -4.2585473 -4.2565932 -4.26523 -4.2858853 -4.3010931]]...]
INFO - root - 2017-12-05 13:25:50.277989: step 11910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:49m:20s remains)
INFO - root - 2017-12-05 13:25:58.926337: step 11920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:45m:27s remains)
INFO - root - 2017-12-05 13:26:07.530714: step 11930, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 76h:12m:50s remains)
INFO - root - 2017-12-05 13:26:16.112622: step 11940, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 75h:43m:55s remains)
INFO - root - 2017-12-05 13:26:24.752882: step 11950, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 77h:29m:59s remains)
INFO - root - 2017-12-05 13:26:33.151713: step 11960, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.829 sec/batch; 73h:50m:04s remains)
INFO - root - 2017-12-05 13:26:41.775395: step 11970, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 73h:43m:39s remains)
INFO - root - 2017-12-05 13:26:50.169927: step 11980, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.806 sec/batch; 71h:44m:02s remains)
INFO - root - 2017-12-05 13:26:58.676591: step 11990, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 75h:41m:50s remains)
INFO - root - 2017-12-05 13:27:07.242286: step 12000, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 75h:31m:58s remains)
2017-12-05 13:27:08.008074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2259355 -4.22081 -4.2157669 -4.2178445 -4.2245688 -4.2270322 -4.2231746 -4.2162437 -4.2200112 -4.2285881 -4.2331824 -4.2335453 -4.2285647 -4.223413 -4.223774][-4.1966672 -4.1916385 -4.1853204 -4.1884141 -4.196157 -4.1951089 -4.1840935 -4.1691532 -4.1723466 -4.1866741 -4.1961918 -4.1986656 -4.1912513 -4.1811061 -4.1861386][-4.1688714 -4.1640253 -4.1595068 -4.1648884 -4.1696911 -4.1634321 -4.1449189 -4.121551 -4.1254954 -4.1413689 -4.1535945 -4.1595626 -4.1545138 -4.1452818 -4.1554241][-4.1515694 -4.1461468 -4.1420116 -4.1510682 -4.1529083 -4.1406379 -4.1177368 -4.0921383 -4.0938997 -4.1021428 -4.1097479 -4.1218195 -4.1260753 -4.1228633 -4.1356392][-4.1563554 -4.1459475 -4.13469 -4.1419148 -4.1381974 -4.1234732 -4.1016521 -4.0774336 -4.0698962 -4.061698 -4.0604372 -4.0815473 -4.1029997 -4.1127973 -4.127512][-4.1644721 -4.14345 -4.1215281 -4.1236868 -4.117867 -4.10297 -4.0815735 -4.0576959 -4.0348582 -4.0061765 -3.997117 -4.0310483 -4.075129 -4.1053019 -4.122498][-4.1698446 -4.1475077 -4.1248307 -4.1212754 -4.1141205 -4.0974612 -4.0709567 -4.0475416 -4.0118246 -3.96207 -3.9460077 -3.9955263 -4.0585842 -4.10213 -4.1256933][-4.1748624 -4.1601553 -4.1441274 -4.1362829 -4.1259642 -4.1047835 -4.0762234 -4.0559912 -4.0182796 -3.962116 -3.9507639 -4.0035686 -4.0647869 -4.1078014 -4.137588][-4.17657 -4.1665416 -4.1523666 -4.1423569 -4.1344595 -4.1188354 -4.0974665 -4.0819273 -4.0528193 -4.0112863 -4.0136638 -4.0572138 -4.0952444 -4.1240239 -4.1575003][-4.1746407 -4.1555076 -4.1328592 -4.1190763 -4.1226163 -4.1279 -4.1269212 -4.1223211 -4.1054969 -4.0808983 -4.0847373 -4.1083713 -4.1188526 -4.1300125 -4.1589317][-4.1771216 -4.1455164 -4.1085215 -4.088254 -4.1026187 -4.1297 -4.146585 -4.1523142 -4.145298 -4.1315594 -4.1307645 -4.1335416 -4.1231818 -4.1217575 -4.1385288][-4.1923828 -4.1565237 -4.1080618 -4.0779533 -4.0920696 -4.128015 -4.1501975 -4.1626725 -4.1612797 -4.1497011 -4.1437011 -4.1314373 -4.1088891 -4.1070147 -4.1162143][-4.218431 -4.1893969 -4.1393452 -4.1034479 -4.1103373 -4.1429343 -4.15971 -4.1743689 -4.17594 -4.1640277 -4.1527052 -4.1344934 -4.1107268 -4.1151304 -4.1203518][-4.2406969 -4.2179303 -4.1763773 -4.1426625 -4.1457777 -4.1717372 -4.1782904 -4.1897554 -4.1925869 -4.1871023 -4.1808596 -4.1701236 -4.1509695 -4.15664 -4.1573915][-4.2638836 -4.2466564 -4.2148328 -4.1864743 -4.1834269 -4.1932936 -4.1926265 -4.2043347 -4.2117696 -4.2158332 -4.2181864 -4.2143006 -4.2009807 -4.2024031 -4.1958494]]...]
INFO - root - 2017-12-05 13:27:16.582951: step 12010, loss = 2.02, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 76h:01m:42s remains)
INFO - root - 2017-12-05 13:27:24.973865: step 12020, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.804 sec/batch; 71h:35m:00s remains)
INFO - root - 2017-12-05 13:27:33.612014: step 12030, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 75h:52m:35s remains)
INFO - root - 2017-12-05 13:27:42.223343: step 12040, loss = 2.03, batch loss = 1.97 (8.8 examples/sec; 0.906 sec/batch; 80h:39m:35s remains)
INFO - root - 2017-12-05 13:27:50.782380: step 12050, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 77h:56m:16s remains)
INFO - root - 2017-12-05 13:27:59.474603: step 12060, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:26m:06s remains)
INFO - root - 2017-12-05 13:28:07.962879: step 12070, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 73h:36m:25s remains)
INFO - root - 2017-12-05 13:28:16.501527: step 12080, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 77h:06m:13s remains)
INFO - root - 2017-12-05 13:28:24.941491: step 12090, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.826 sec/batch; 73h:28m:49s remains)
INFO - root - 2017-12-05 13:28:33.414782: step 12100, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 75h:37m:00s remains)
2017-12-05 13:28:34.197601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2547135 -4.249876 -4.2334008 -4.2233248 -4.2261248 -4.2352519 -4.2341471 -4.2157631 -4.1853123 -4.1615 -4.1685858 -4.2017426 -4.234333 -4.2530665 -4.2557912][-4.2375965 -4.2264495 -4.2074289 -4.1983309 -4.2043495 -4.2095108 -4.1974182 -4.1672835 -4.1316676 -4.11123 -4.1277246 -4.1698256 -4.206315 -4.225709 -4.2306757][-4.2123795 -4.1939034 -4.1694818 -4.1581917 -4.1649737 -4.1680331 -4.1506119 -4.1176834 -4.0861826 -4.0764108 -4.10134 -4.1460938 -4.1821122 -4.2025242 -4.2117591][-4.1898084 -4.1618953 -4.1280031 -4.1128116 -4.1185317 -4.1205549 -4.1034374 -4.0810747 -4.0675898 -4.0751767 -4.1061726 -4.1464696 -4.1797805 -4.2027535 -4.21473][-4.1713109 -4.1374187 -4.0984249 -4.081768 -4.0844035 -4.0836225 -4.0723133 -4.06793 -4.0790076 -4.1025782 -4.1371522 -4.1726766 -4.2008772 -4.2229781 -4.2342219][-4.1680088 -4.1356111 -4.095705 -4.0729542 -4.0689993 -4.06792 -4.0671005 -4.0811458 -4.1081514 -4.139555 -4.1747332 -4.2043362 -4.2245455 -4.2421379 -4.251008][-4.1716251 -4.1416364 -4.10149 -4.0736814 -4.0678058 -4.0741215 -4.0866938 -4.1151261 -4.1480436 -4.1754541 -4.2019954 -4.2204857 -4.2315931 -4.2445602 -4.2514539][-4.1713428 -4.1447172 -4.10787 -4.0799074 -4.07453 -4.087348 -4.1100082 -4.1459107 -4.1753683 -4.19404 -4.2081985 -4.21538 -4.21972 -4.2266231 -4.2311258][-4.1525178 -4.1356993 -4.1064191 -4.0849528 -4.0810471 -4.0982752 -4.1305661 -4.1679521 -4.1884408 -4.1965823 -4.2022862 -4.2042117 -4.2020092 -4.1983414 -4.1985207][-4.1156292 -4.111064 -4.0979557 -4.0895143 -4.0913539 -4.112639 -4.1509304 -4.1853886 -4.1959805 -4.1975894 -4.2014823 -4.2007761 -4.1905332 -4.1770139 -4.1744604][-4.1001759 -4.0999551 -4.0999718 -4.1038141 -4.11263 -4.1384125 -4.1789947 -4.2068172 -4.2064452 -4.2032137 -4.2056932 -4.1997833 -4.1807823 -4.1605058 -4.1589108][-4.11309 -4.112186 -4.1196213 -4.1289039 -4.1399131 -4.16827 -4.2063928 -4.2248416 -4.2158971 -4.2088647 -4.2069221 -4.1939592 -4.16511 -4.1419573 -4.1479445][-4.1472683 -4.144021 -4.149704 -4.1555653 -4.1634603 -4.1893463 -4.2208614 -4.2319255 -4.2188706 -4.2109981 -4.2049756 -4.1838923 -4.1482439 -4.1292424 -4.1500983][-4.1769395 -4.1707129 -4.1710963 -4.1707544 -4.1761374 -4.1955719 -4.2155313 -4.2150097 -4.2004461 -4.1945114 -4.187911 -4.1629281 -4.129003 -4.1233258 -4.1592655][-4.1817174 -4.1780577 -4.1817069 -4.1812954 -4.184833 -4.1957335 -4.2021732 -4.1913056 -4.1764293 -4.1741428 -4.1685243 -4.1444411 -4.1195087 -4.1293678 -4.1740284]]...]
INFO - root - 2017-12-05 13:28:42.739012: step 12110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:25m:22s remains)
INFO - root - 2017-12-05 13:28:51.139267: step 12120, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.808 sec/batch; 71h:53m:31s remains)
INFO - root - 2017-12-05 13:28:59.840670: step 12130, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 75h:36m:52s remains)
INFO - root - 2017-12-05 13:29:08.447578: step 12140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 75h:11m:52s remains)
INFO - root - 2017-12-05 13:29:17.069610: step 12150, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 73h:32m:45s remains)
INFO - root - 2017-12-05 13:29:25.566858: step 12160, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:31m:43s remains)
INFO - root - 2017-12-05 13:29:34.063489: step 12170, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 76h:33m:23s remains)
INFO - root - 2017-12-05 13:29:42.582584: step 12180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 76h:53m:31s remains)
INFO - root - 2017-12-05 13:29:51.096715: step 12190, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 76h:40m:52s remains)
INFO - root - 2017-12-05 13:29:59.565210: step 12200, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 77h:41m:56s remains)
2017-12-05 13:30:00.399026: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2563562 -4.2510114 -4.2596278 -4.2720394 -4.2866387 -4.3030114 -4.3170228 -4.3291049 -4.3387694 -4.3424673 -4.3421283 -4.3425255 -4.3437686 -4.3454084 -4.3475685][-4.2354441 -4.2339153 -4.2496386 -4.271605 -4.29448 -4.3147688 -4.3298798 -4.3401637 -4.3470144 -4.3486819 -4.346983 -4.3457594 -4.3455071 -4.3463645 -4.3484459][-4.1936927 -4.200542 -4.2266636 -4.2616014 -4.2958198 -4.3220868 -4.3390989 -4.3475637 -4.3511858 -4.3499613 -4.3460588 -4.34273 -4.3408208 -4.340508 -4.3426433][-4.1480293 -4.165247 -4.2010031 -4.2462354 -4.2888312 -4.3194761 -4.3363786 -4.3421268 -4.341145 -4.3365712 -4.3308496 -4.3260217 -4.3226542 -4.3222966 -4.3263221][-4.1544042 -4.1775694 -4.2116504 -4.24961 -4.2839155 -4.3090515 -4.3219547 -4.3227553 -4.3157721 -4.3067741 -4.2978721 -4.2908568 -4.2851496 -4.2851372 -4.29349][-4.2016954 -4.2196984 -4.23921 -4.2569466 -4.2719026 -4.283823 -4.2885246 -4.2825541 -4.2700114 -4.2592812 -4.2493992 -4.2418551 -4.2374663 -4.2408175 -4.2545066][-4.2515626 -4.2570615 -4.2560196 -4.24737 -4.2394681 -4.2390323 -4.2397447 -4.2296767 -4.2107544 -4.1996164 -4.1953082 -4.1944408 -4.19924 -4.2110906 -4.2287979][-4.2692909 -4.2664642 -4.2486396 -4.2152863 -4.18704 -4.1774797 -4.176692 -4.1661119 -4.1474748 -4.1426868 -4.1510477 -4.1661358 -4.1852131 -4.2048473 -4.2213206][-4.2570705 -4.2509179 -4.2253208 -4.1776934 -4.1368437 -4.1227536 -4.121788 -4.1160555 -4.1095877 -4.121336 -4.1456618 -4.1734252 -4.1990952 -4.2187519 -4.2294393][-4.2359905 -4.2317619 -4.2121029 -4.1710019 -4.1335526 -4.1210423 -4.1227221 -4.1235242 -4.1273136 -4.1480913 -4.176661 -4.2063789 -4.2285748 -4.24198 -4.2460837][-4.2367063 -4.2371168 -4.2292237 -4.2069335 -4.1865735 -4.1829395 -4.1890421 -4.1945958 -4.2004027 -4.2159357 -4.2347183 -4.2533379 -4.2655921 -4.2704673 -4.2699757][-4.2493463 -4.2538919 -4.2563581 -4.252172 -4.2482834 -4.2518773 -4.2596607 -4.2655997 -4.2686453 -4.2729788 -4.2774119 -4.2821689 -4.2854476 -4.2860546 -4.2844944][-4.257853 -4.2664819 -4.2741251 -4.2786827 -4.2821684 -4.2866683 -4.2920842 -4.2962389 -4.2955751 -4.2907515 -4.2844906 -4.2803078 -4.27986 -4.2807274 -4.2805314][-4.2397861 -4.2502718 -4.2571197 -4.260663 -4.2626514 -4.2640176 -4.2665362 -4.2702336 -4.2706237 -4.2658243 -4.25885 -4.2543154 -4.2551565 -4.2589736 -4.2612829][-4.1875124 -4.1958103 -4.1997185 -4.2028785 -4.2086959 -4.2150702 -4.2212214 -4.22738 -4.2306614 -4.2280679 -4.2225971 -4.218533 -4.2207708 -4.22684 -4.2302542]]...]
INFO - root - 2017-12-05 13:30:09.039716: step 12210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 77h:17m:40s remains)
INFO - root - 2017-12-05 13:30:17.687935: step 12220, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:48m:16s remains)
INFO - root - 2017-12-05 13:30:26.106072: step 12230, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 75h:15m:32s remains)
INFO - root - 2017-12-05 13:30:34.706201: step 12240, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 76h:27m:44s remains)
INFO - root - 2017-12-05 13:30:43.148232: step 12250, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 74h:09m:00s remains)
INFO - root - 2017-12-05 13:30:51.690177: step 12260, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:11m:51s remains)
INFO - root - 2017-12-05 13:31:00.228015: step 12270, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 75h:05m:01s remains)
INFO - root - 2017-12-05 13:31:08.764030: step 12280, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:46m:33s remains)
INFO - root - 2017-12-05 13:31:17.309054: step 12290, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 76h:12m:26s remains)
INFO - root - 2017-12-05 13:31:25.758629: step 12300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 76h:31m:17s remains)
2017-12-05 13:31:26.584314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2465563 -4.22139 -4.1875057 -4.14672 -4.1251893 -4.1503005 -4.2013679 -4.2475209 -4.2649369 -4.2608809 -4.2378263 -4.2241993 -4.2279654 -4.2440104 -4.264564][-4.2443476 -4.2194004 -4.1761789 -4.120163 -4.0904241 -4.1210608 -4.1749721 -4.2245517 -4.2437449 -4.2474132 -4.2344704 -4.2260361 -4.23655 -4.2596354 -4.2793684][-4.2373838 -4.2104282 -4.1592255 -4.0944519 -4.0700803 -4.1097951 -4.1602583 -4.2002511 -4.2128158 -4.2210255 -4.2213774 -4.2242656 -4.2432046 -4.2706504 -4.2893047][-4.2306185 -4.2027321 -4.1534147 -4.0956211 -4.079751 -4.1145396 -4.1477418 -4.1687346 -4.171298 -4.1796279 -4.193037 -4.2097955 -4.239789 -4.2735777 -4.2932992][-4.2246966 -4.194788 -4.1547546 -4.1125326 -4.1010814 -4.1150589 -4.1232924 -4.1252556 -4.120996 -4.1282949 -4.1548128 -4.1913438 -4.2359304 -4.2754254 -4.2962871][-4.226398 -4.19855 -4.1664457 -4.13514 -4.1192689 -4.1060662 -4.0830412 -4.0671139 -4.0638714 -4.0801334 -4.1284103 -4.1894841 -4.2458429 -4.2852516 -4.3045392][-4.2390103 -4.2196403 -4.1909065 -4.1594982 -4.1248994 -4.0756431 -4.0142465 -3.9943094 -4.0137544 -4.0570288 -4.1296868 -4.2077665 -4.2694964 -4.3031325 -4.3159919][-4.2562361 -4.2434006 -4.2160745 -4.1749153 -4.1093874 -4.0148196 -3.9150562 -3.9060054 -3.9743779 -4.0589614 -4.1538906 -4.2380958 -4.29252 -4.3170609 -4.324492][-4.268013 -4.2570639 -4.2283354 -4.1748152 -4.0859551 -3.960521 -3.8466585 -3.8631957 -3.9757328 -4.087079 -4.1912217 -4.26833 -4.3094473 -4.3268733 -4.32889][-4.2735448 -4.2629776 -4.2327662 -4.1773763 -4.0865259 -3.9602942 -3.8634644 -3.8950663 -4.0135307 -4.124033 -4.2209115 -4.2873077 -4.3168988 -4.3262882 -4.3240967][-4.2751741 -4.2639971 -4.2353911 -4.18348 -4.0997205 -3.9916892 -3.9287651 -3.9690263 -4.070323 -4.166162 -4.2454152 -4.2937517 -4.3081779 -4.3106384 -4.3072367][-4.2800479 -4.2685037 -4.2410522 -4.189877 -4.1111088 -4.0271225 -3.9984934 -4.0446377 -4.1261196 -4.2014723 -4.260324 -4.2879858 -4.2894015 -4.2869864 -4.2855229][-4.2863822 -4.2733517 -4.2470722 -4.1988606 -4.1314468 -4.0731993 -4.0670033 -4.1081095 -4.1686525 -4.2250323 -4.2656064 -4.2766647 -4.2699885 -4.2655268 -4.2688427][-4.294734 -4.2831697 -4.2602978 -4.2192287 -4.1676579 -4.1308374 -4.1328754 -4.1636953 -4.2047029 -4.2420135 -4.2649364 -4.2631931 -4.2505703 -4.2468448 -4.2568564][-4.30468 -4.2957783 -4.2760239 -4.2431989 -4.2079282 -4.1852384 -4.1886997 -4.2093353 -4.2332978 -4.2527161 -4.2597365 -4.2490821 -4.2358022 -4.2362447 -4.2543168]]...]
INFO - root - 2017-12-05 13:31:35.015676: step 12310, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 74h:13m:05s remains)
INFO - root - 2017-12-05 13:31:43.702001: step 12320, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:17m:10s remains)
INFO - root - 2017-12-05 13:31:52.240134: step 12330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 77h:10m:47s remains)
INFO - root - 2017-12-05 13:32:00.700627: step 12340, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 74h:53m:42s remains)
INFO - root - 2017-12-05 13:32:09.283001: step 12350, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 78h:24m:47s remains)
INFO - root - 2017-12-05 13:32:17.830774: step 12360, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 77h:49m:09s remains)
INFO - root - 2017-12-05 13:32:26.580013: step 12370, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 75h:00m:12s remains)
INFO - root - 2017-12-05 13:32:35.190278: step 12380, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 77h:41m:41s remains)
INFO - root - 2017-12-05 13:32:43.714372: step 12390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 76h:00m:04s remains)
INFO - root - 2017-12-05 13:32:52.137040: step 12400, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:18m:51s remains)
2017-12-05 13:32:52.974756: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1141758 -4.1615782 -4.210896 -4.2261562 -4.2176032 -4.2118907 -4.2036104 -4.1836166 -4.1590619 -4.1493211 -4.1548052 -4.16217 -4.1538296 -4.1182332 -4.0570378][-4.0592408 -4.133791 -4.1978812 -4.2125707 -4.2001152 -4.1887121 -4.176609 -4.1622534 -4.1483836 -4.1495161 -4.1580853 -4.1663055 -4.161984 -4.1343341 -4.0713515][-4.026938 -4.1259232 -4.1966829 -4.2063403 -4.1872482 -4.1691475 -4.1570559 -4.1548862 -4.1566887 -4.1667266 -4.1755371 -4.1813173 -4.1787386 -4.1580114 -4.0984769][-4.0120831 -4.1239743 -4.18964 -4.1909027 -4.1660738 -4.1443782 -4.1393404 -4.1530194 -4.1668124 -4.1826596 -4.1921544 -4.1943374 -4.193881 -4.1767545 -4.118464][-4.0301094 -4.1388288 -4.1962471 -4.1880922 -4.1544914 -4.1292377 -4.1298909 -4.1536045 -4.1713166 -4.1920705 -4.2039647 -4.2063289 -4.2088761 -4.1940722 -4.1329317][-4.087584 -4.1757717 -4.21541 -4.1947546 -4.1527152 -4.1218438 -4.120976 -4.1476378 -4.1679649 -4.1921754 -4.207181 -4.2135525 -4.2208223 -4.20695 -4.1466017][-4.1629705 -4.2187524 -4.2394967 -4.2127905 -4.1646252 -4.1270728 -4.1181459 -4.1366563 -4.1509867 -4.1726975 -4.1913304 -4.20591 -4.2198315 -4.2124047 -4.1630154][-4.2283916 -4.2580867 -4.2662358 -4.2411337 -4.1955328 -4.1577787 -4.1405239 -4.1416793 -4.1429353 -4.1547971 -4.1710234 -4.1933136 -4.215095 -4.2158084 -4.1757727][-4.2599139 -4.2720957 -4.2750773 -4.2607436 -4.2299976 -4.2014089 -4.1871696 -4.1775179 -4.1668534 -4.163723 -4.1722951 -4.1948366 -4.2176328 -4.2183952 -4.1825981][-4.2713308 -4.2755961 -4.2789583 -4.2752151 -4.2585278 -4.2435637 -4.2390246 -4.2289925 -4.2148838 -4.2022772 -4.2044291 -4.2197485 -4.2338428 -4.2291951 -4.1979227][-4.2793269 -4.28118 -4.2828126 -4.2800303 -4.2679057 -4.2625127 -4.2676477 -4.2648535 -4.2555642 -4.2421002 -4.2406759 -4.2491665 -4.2549977 -4.2482233 -4.2246881][-4.2841544 -4.2847304 -4.2838326 -4.2793589 -4.2717743 -4.2727184 -4.2824445 -4.2849646 -4.2801867 -4.270946 -4.2703896 -4.276628 -4.2790074 -4.2723484 -4.2526379][-4.2939792 -4.2929249 -4.2912011 -4.2863097 -4.2813735 -4.2840986 -4.294095 -4.2995486 -4.3001146 -4.2988739 -4.3008204 -4.304987 -4.3052979 -4.2988591 -4.2831626][-4.3034258 -4.3019075 -4.3006248 -4.2972808 -4.2949929 -4.2979794 -4.3059564 -4.312604 -4.3160248 -4.3183818 -4.3204784 -4.3222966 -4.321034 -4.3164592 -4.3068705][-4.311235 -4.307755 -4.3060484 -4.3042803 -4.3043737 -4.3075614 -4.3131242 -4.3183384 -4.3213654 -4.323184 -4.3243093 -4.324614 -4.323719 -4.3213449 -4.3174982]]...]
INFO - root - 2017-12-05 13:33:01.546577: step 12410, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 77h:20m:34s remains)
INFO - root - 2017-12-05 13:33:10.058244: step 12420, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 75h:09m:40s remains)
INFO - root - 2017-12-05 13:33:18.580621: step 12430, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 72h:20m:38s remains)
INFO - root - 2017-12-05 13:33:27.054837: step 12440, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 73h:48m:42s remains)
INFO - root - 2017-12-05 13:33:35.630598: step 12450, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 74h:09m:37s remains)
INFO - root - 2017-12-05 13:33:44.176417: step 12460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 76h:28m:32s remains)
INFO - root - 2017-12-05 13:33:52.719566: step 12470, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 72h:46m:45s remains)
INFO - root - 2017-12-05 13:34:01.325979: step 12480, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 77h:32m:49s remains)
INFO - root - 2017-12-05 13:34:09.971435: step 12490, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 78h:12m:02s remains)
INFO - root - 2017-12-05 13:34:18.390498: step 12500, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 75h:58m:41s remains)
2017-12-05 13:34:19.184048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1229944 -4.1526523 -4.1667109 -4.1696944 -4.1529131 -4.1216235 -4.1060753 -4.1275392 -4.1700311 -4.1997418 -4.2169652 -4.2337942 -4.2397647 -4.2311478 -4.2163692][-4.1026039 -4.1288881 -4.1385088 -4.1481972 -4.146533 -4.1275015 -4.1163621 -4.1348338 -4.1676989 -4.1892524 -4.2024355 -4.2162995 -4.2238364 -4.2216878 -4.2082262][-4.0983696 -4.121284 -4.1285315 -4.1452718 -4.1571479 -4.1507406 -4.1425762 -4.1549459 -4.1778221 -4.1918111 -4.1977205 -4.2039647 -4.2077627 -4.2100744 -4.196888][-4.1121817 -4.1282849 -4.1346803 -4.1562276 -4.1790342 -4.1782179 -4.1673512 -4.1725054 -4.1924582 -4.2017326 -4.201602 -4.2016106 -4.2006426 -4.2002411 -4.1791081][-4.1460619 -4.1484423 -4.1490049 -4.1673374 -4.1839285 -4.1790791 -4.1630125 -4.163064 -4.1835284 -4.1946521 -4.1953077 -4.1972051 -4.1997786 -4.1994414 -4.1742163][-4.19226 -4.1852236 -4.1765828 -4.1778851 -4.1712065 -4.14682 -4.116364 -4.1117768 -4.1460071 -4.1756439 -4.1895742 -4.2003322 -4.2062044 -4.2082438 -4.1847038][-4.2077827 -4.1959472 -4.1792974 -4.1617532 -4.127111 -4.076736 -4.0188465 -4.00494 -4.0674777 -4.1343431 -4.1738071 -4.201364 -4.2144923 -4.2212982 -4.2031622][-4.1761832 -4.1601458 -4.1346512 -4.0992212 -4.0452137 -3.9718351 -3.8820934 -3.8576765 -3.9639041 -4.0788918 -4.1507945 -4.2001381 -4.2244887 -4.2338057 -4.2223415][-4.1317325 -4.1173515 -4.0864782 -4.0417938 -3.9851739 -3.9092131 -3.8077931 -3.7704632 -3.9004645 -4.0394039 -4.1282206 -4.1936054 -4.2276087 -4.2341928 -4.2250776][-4.1217079 -4.1117344 -4.0846992 -4.046174 -4.0062914 -3.9553573 -3.8817937 -3.847342 -3.9478202 -4.0626297 -4.1362405 -4.1962266 -4.2282186 -4.2288446 -4.2155533][-4.1534963 -4.1487765 -4.1307907 -4.1051745 -4.08381 -4.0567832 -4.0148435 -3.9923697 -4.0522656 -4.1234264 -4.16924 -4.2112589 -4.230782 -4.2200732 -4.2008486][-4.2036424 -4.2018719 -4.1933274 -4.1765475 -4.1612482 -4.1435366 -4.1173291 -4.1002126 -4.1348338 -4.1765356 -4.2004519 -4.2252569 -4.2292337 -4.20587 -4.1810484][-4.2471538 -4.2446637 -4.2400069 -4.2295346 -4.2152333 -4.1999688 -4.1782618 -4.1632733 -4.1857028 -4.2131052 -4.2251282 -4.2368093 -4.22677 -4.1952085 -4.1657143][-4.2691178 -4.2662406 -4.2615194 -4.2539582 -4.2426615 -4.22887 -4.2088094 -4.1994495 -4.2200646 -4.2417507 -4.2501397 -4.2535763 -4.2353096 -4.2011681 -4.1690822][-4.2805696 -4.2735682 -4.2664623 -4.2615743 -4.2567372 -4.2455635 -4.2279682 -4.2249637 -4.2439923 -4.2637868 -4.2695103 -4.2677031 -4.2497954 -4.2174673 -4.1863246]]...]
INFO - root - 2017-12-05 13:34:27.749912: step 12510, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:29m:02s remains)
INFO - root - 2017-12-05 13:34:36.298573: step 12520, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 74h:33m:33s remains)
INFO - root - 2017-12-05 13:34:44.788315: step 12530, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 75h:31m:32s remains)
INFO - root - 2017-12-05 13:34:53.322343: step 12540, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 76h:18m:21s remains)
INFO - root - 2017-12-05 13:35:01.803386: step 12550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 75h:13m:13s remains)
INFO - root - 2017-12-05 13:35:10.270299: step 12560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 74h:25m:13s remains)
INFO - root - 2017-12-05 13:35:18.828851: step 12570, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 76h:48m:12s remains)
INFO - root - 2017-12-05 13:35:27.321757: step 12580, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:53m:42s remains)
INFO - root - 2017-12-05 13:35:35.970931: step 12590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:54m:05s remains)
INFO - root - 2017-12-05 13:35:44.506882: step 12600, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.806 sec/batch; 71h:36m:52s remains)
2017-12-05 13:35:45.282246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3062234 -4.3145018 -4.3198395 -4.324657 -4.3264723 -4.3258915 -4.3238297 -4.3126597 -4.2973886 -4.2864118 -4.2880626 -4.3023543 -4.3131595 -4.3100557 -4.3047585][-4.2958336 -4.3041077 -4.3156133 -4.3279576 -4.33397 -4.334888 -4.3330121 -4.32189 -4.3023009 -4.2892365 -4.292871 -4.3093624 -4.3196239 -4.3149095 -4.3074512][-4.26479 -4.277813 -4.2974019 -4.3147907 -4.3214121 -4.3216925 -4.3210545 -4.3127971 -4.2946792 -4.2871146 -4.2976584 -4.3184061 -4.3296614 -4.326479 -4.3182344][-4.2053022 -4.2250028 -4.25168 -4.2723784 -4.278152 -4.2768869 -4.2776775 -4.2755728 -4.2660146 -4.2712688 -4.2928371 -4.3202338 -4.334157 -4.3364644 -4.3323593][-4.1258688 -4.1485953 -4.1812806 -4.20451 -4.2086596 -4.2000895 -4.1959066 -4.1983042 -4.2006359 -4.2237606 -4.2620106 -4.3023672 -4.3237057 -4.3347716 -4.3383174][-4.0671611 -4.08415 -4.113493 -4.1331773 -4.129076 -4.1076155 -4.0944 -4.098547 -4.1120768 -4.1522212 -4.2069883 -4.2617636 -4.2947316 -4.318038 -4.3316483][-4.0708637 -4.0804381 -4.0975976 -4.1043873 -4.0866547 -4.0507212 -4.0259676 -4.0233049 -4.0330296 -4.0745845 -4.1369429 -4.2027545 -4.250802 -4.28822 -4.3125415][-4.1356449 -4.1403222 -4.1422205 -4.1376 -4.1171927 -4.085743 -4.0608072 -4.043716 -4.0284548 -4.046639 -4.0988016 -4.1645956 -4.2220187 -4.2684884 -4.2988262][-4.2110791 -4.2114997 -4.2000623 -4.1868668 -4.17038 -4.1545153 -4.1414223 -4.1222587 -4.0919981 -4.08635 -4.1208272 -4.1785326 -4.2342429 -4.2781019 -4.3048549][-4.2727928 -4.2690783 -4.24997 -4.2324967 -4.2189107 -4.2105618 -4.2053852 -4.1907406 -4.1595335 -4.1453075 -4.1701465 -4.2186117 -4.2647929 -4.2983961 -4.3157206][-4.3175945 -4.3137479 -4.2953506 -4.2805958 -4.2693987 -4.2635188 -4.2598953 -4.2489309 -4.22288 -4.2056117 -4.2217274 -4.2577944 -4.2904539 -4.3119764 -4.3209548][-4.3400545 -4.3408151 -4.3290129 -4.3190475 -4.3110857 -4.3061943 -4.3020744 -4.2933359 -4.2744479 -4.2575855 -4.2640266 -4.2858653 -4.303782 -4.3142447 -4.3163137][-4.34619 -4.3515792 -4.3476 -4.3428588 -4.3377242 -4.3334432 -4.3292427 -4.3235612 -4.3114367 -4.2986431 -4.2984276 -4.306601 -4.3098912 -4.3106833 -4.3069625][-4.3452487 -4.352253 -4.352591 -4.3494182 -4.3438039 -4.3400211 -4.3368831 -4.3340058 -4.3292046 -4.324409 -4.3227792 -4.3219981 -4.3160176 -4.3096952 -4.3003592][-4.3411021 -4.3478637 -4.3494864 -4.3454866 -4.3390541 -4.3357434 -4.3348041 -4.3343196 -4.3356395 -4.3371592 -4.3347335 -4.3298411 -4.3206377 -4.3126073 -4.3004231]]...]
INFO - root - 2017-12-05 13:35:53.860391: step 12610, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 76h:49m:07s remains)
INFO - root - 2017-12-05 13:36:02.376948: step 12620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 78h:20m:50s remains)
INFO - root - 2017-12-05 13:36:11.013981: step 12630, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 78h:49m:08s remains)
INFO - root - 2017-12-05 13:36:19.463878: step 12640, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 76h:36m:27s remains)
INFO - root - 2017-12-05 13:36:28.063723: step 12650, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.806 sec/batch; 71h:34m:52s remains)
INFO - root - 2017-12-05 13:36:36.539297: step 12660, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 74h:56m:45s remains)
INFO - root - 2017-12-05 13:36:45.112803: step 12670, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 75h:49m:51s remains)
INFO - root - 2017-12-05 13:36:53.583931: step 12680, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 75h:47m:04s remains)
INFO - root - 2017-12-05 13:37:02.206912: step 12690, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 75h:18m:27s remains)
INFO - root - 2017-12-05 13:37:10.696797: step 12700, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 77h:58m:48s remains)
2017-12-05 13:37:11.450602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1751523 -4.171051 -4.1723752 -4.1815419 -4.2059846 -4.2433839 -4.28171 -4.3049703 -4.3035884 -4.2794461 -4.2445045 -4.216126 -4.2282634 -4.2539239 -4.2675381][-4.1911321 -4.189208 -4.1805267 -4.1740756 -4.1928129 -4.2323289 -4.2779727 -4.3121939 -4.3179064 -4.3000283 -4.2653751 -4.2354412 -4.2421174 -4.2638278 -4.2808952][-4.2031736 -4.2026277 -4.186255 -4.1652322 -4.1733136 -4.2080679 -4.2534771 -4.2971973 -4.3152046 -4.3085537 -4.2824397 -4.2565465 -4.257998 -4.2738824 -4.2872791][-4.2137871 -4.2157331 -4.1965418 -4.163815 -4.1578641 -4.180759 -4.2183828 -4.2671967 -4.298924 -4.3066039 -4.2952871 -4.281239 -4.2830896 -4.2952566 -4.3020296][-4.2343855 -4.23777 -4.2180681 -4.1799865 -4.1561394 -4.1555982 -4.1744246 -4.2220178 -4.2688751 -4.2942748 -4.3022871 -4.3054552 -4.3120537 -4.3187213 -4.3169441][-4.2543783 -4.2649579 -4.2512627 -4.2138147 -4.1731272 -4.1397147 -4.1216116 -4.1587658 -4.2215571 -4.2692194 -4.2975507 -4.31729 -4.3294234 -4.3337255 -4.3262229][-4.254632 -4.2790275 -4.2766571 -4.2427731 -4.1873727 -4.1173797 -4.0537128 -4.0759096 -4.1594057 -4.23312 -4.2819424 -4.3135204 -4.3292427 -4.3316875 -4.3210373][-4.2419872 -4.2806954 -4.2881722 -4.2582412 -4.1930103 -4.0908222 -3.9821906 -3.9888258 -4.0955176 -4.1918716 -4.2552037 -4.29167 -4.303658 -4.3009839 -4.2927423][-4.2222376 -4.2725081 -4.2904663 -4.2645817 -4.1967773 -4.0854692 -3.9612644 -3.9586251 -4.0695448 -4.17151 -4.2369003 -4.2672462 -4.265903 -4.2544441 -4.2529917][-4.2029452 -4.2606463 -4.2854762 -4.2616615 -4.1981382 -4.1020379 -4.0020928 -3.9983523 -4.0890784 -4.1798983 -4.23802 -4.2534127 -4.2305994 -4.2043948 -4.2065525][-4.1818504 -4.2446451 -4.2755013 -4.2552586 -4.1996746 -4.121911 -4.0494714 -4.0469766 -4.1148086 -4.189888 -4.233964 -4.2321405 -4.1894479 -4.1496491 -4.1529832][-4.17084 -4.2346668 -4.2662835 -4.2493715 -4.200768 -4.1378441 -4.0846071 -4.0842237 -4.1339788 -4.1937246 -4.2215142 -4.204524 -4.1539407 -4.1085129 -4.1091485][-4.1752558 -4.2335362 -4.2621846 -4.2484665 -4.2047167 -4.1527586 -4.11209 -4.1101818 -4.1455188 -4.192656 -4.2108722 -4.1918974 -4.1450577 -4.1016078 -4.1004953][-4.18713 -4.2388439 -4.2633815 -4.2529483 -4.215795 -4.17126 -4.137464 -4.1307921 -4.1554437 -4.1958327 -4.2169228 -4.2049913 -4.1681924 -4.1332273 -4.1326623][-4.2049894 -4.2474461 -4.2684669 -4.2615781 -4.2313833 -4.1908374 -4.1549268 -4.1395564 -4.1568165 -4.1967058 -4.2280483 -4.2260318 -4.1993613 -4.1707888 -4.1704183]]...]
INFO - root - 2017-12-05 13:37:19.950454: step 12710, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 77h:15m:49s remains)
INFO - root - 2017-12-05 13:37:28.599465: step 12720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 76h:24m:29s remains)
INFO - root - 2017-12-05 13:37:37.108567: step 12730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 76h:25m:00s remains)
INFO - root - 2017-12-05 13:37:45.619939: step 12740, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 76h:11m:52s remains)
INFO - root - 2017-12-05 13:37:54.195304: step 12750, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 75h:51m:02s remains)
INFO - root - 2017-12-05 13:38:02.583605: step 12760, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 78h:02m:22s remains)
INFO - root - 2017-12-05 13:38:11.107785: step 12770, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:32m:47s remains)
INFO - root - 2017-12-05 13:38:19.703647: step 12780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 75h:24m:15s remains)
INFO - root - 2017-12-05 13:38:28.217837: step 12790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 75h:50m:27s remains)
INFO - root - 2017-12-05 13:38:36.737118: step 12800, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 78h:06m:52s remains)
2017-12-05 13:38:37.544167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1708727 -4.1670647 -4.1663928 -4.1711297 -4.192286 -4.2210684 -4.242734 -4.25127 -4.2379417 -4.2185364 -4.2102895 -4.2032704 -4.1881695 -4.1689072 -4.1662483][-4.1540308 -4.1524315 -4.1512427 -4.1513038 -4.1671357 -4.1927843 -4.2084284 -4.2150669 -4.2071071 -4.1995077 -4.2044048 -4.2039256 -4.1838212 -4.1527023 -4.1439729][-4.1553526 -4.1538577 -4.14948 -4.1446495 -4.1520586 -4.168704 -4.1731777 -4.1704555 -4.1670694 -4.1765914 -4.1978421 -4.2078934 -4.1863718 -4.1462765 -4.1366663][-4.1716704 -4.1670294 -4.1573253 -4.147325 -4.1458907 -4.147573 -4.1340146 -4.1129513 -4.1146312 -4.1506028 -4.1940794 -4.2168274 -4.1965694 -4.1507077 -4.1394882][-4.1936765 -4.1860132 -4.1710458 -4.1534948 -4.1378908 -4.1209774 -4.087081 -4.0427008 -4.0492191 -4.1191378 -4.18888 -4.2253966 -4.2111216 -4.1626091 -4.1495862][-4.2101049 -4.2005119 -4.1821637 -4.154747 -4.1202326 -4.0840731 -4.0297003 -3.9620168 -3.9720204 -4.0763211 -4.1698804 -4.2181716 -4.2100792 -4.1594172 -4.146872][-4.2225795 -4.2158031 -4.1972671 -4.1648765 -4.1190062 -4.0690546 -3.9983449 -3.9140255 -3.9233222 -4.0465994 -4.1506863 -4.2049718 -4.1995554 -4.1487074 -4.139442][-4.2379942 -4.233851 -4.2198329 -4.1932454 -4.1508126 -4.0970783 -4.025507 -3.9428833 -3.9497156 -4.0594306 -4.1534634 -4.2037907 -4.1967978 -4.1518984 -4.1487732][-4.2531214 -4.2497306 -4.2401657 -4.2250147 -4.1953731 -4.1500869 -4.0867567 -4.0154357 -4.0185418 -4.0961857 -4.167707 -4.209846 -4.2019858 -4.1660857 -4.1656427][-4.2641273 -4.256494 -4.2467971 -4.2388244 -4.2215786 -4.1884613 -4.1387348 -4.0800805 -4.074523 -4.1221027 -4.1737447 -4.2121983 -4.2090507 -4.1810241 -4.1806278][-4.2661362 -4.252851 -4.2367482 -4.228065 -4.2177181 -4.1955066 -4.1616454 -4.1168103 -4.1077876 -4.13738 -4.1747713 -4.2101536 -4.2122254 -4.1907263 -4.1879768][-4.2660789 -4.2502217 -4.2296119 -4.2210927 -4.2166228 -4.2017736 -4.1804066 -4.1505694 -4.1454043 -4.1639981 -4.1886396 -4.2172122 -4.2212515 -4.204 -4.1985588][-4.268383 -4.2539887 -4.2343841 -4.2281313 -4.2280335 -4.2195735 -4.2078686 -4.1904387 -4.1894255 -4.20073 -4.2132368 -4.2319303 -4.2361431 -4.2225351 -4.2151332][-4.2746234 -4.26352 -4.2467666 -4.2414517 -4.2429667 -4.2388883 -4.2347226 -4.2269888 -4.22836 -4.2354417 -4.2403364 -4.2510524 -4.2541528 -4.2436886 -4.2355814][-4.2863297 -4.2775249 -4.263072 -4.2566481 -4.2574539 -4.2553415 -4.2553506 -4.2547 -4.2584329 -4.2641273 -4.2651052 -4.2702727 -4.2735524 -4.2670345 -4.2580967]]...]
INFO - root - 2017-12-05 13:38:46.055884: step 12810, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 77h:25m:00s remains)
INFO - root - 2017-12-05 13:38:54.588728: step 12820, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 76h:40m:36s remains)
INFO - root - 2017-12-05 13:39:03.204010: step 12830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 76h:43m:21s remains)
INFO - root - 2017-12-05 13:39:11.671540: step 12840, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 73h:55m:03s remains)
INFO - root - 2017-12-05 13:39:20.202027: step 12850, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 77h:44m:22s remains)
INFO - root - 2017-12-05 13:39:28.606584: step 12860, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 75h:39m:43s remains)
INFO - root - 2017-12-05 13:39:37.071253: step 12870, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 74h:32m:19s remains)
INFO - root - 2017-12-05 13:39:45.652569: step 12880, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 73h:58m:30s remains)
INFO - root - 2017-12-05 13:39:54.169558: step 12890, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 76h:00m:04s remains)
INFO - root - 2017-12-05 13:40:02.712699: step 12900, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 75h:46m:03s remains)
2017-12-05 13:40:03.529419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2028494 -4.2152443 -4.2294931 -4.2380877 -4.2392564 -4.2325606 -4.2190881 -4.1999745 -4.1888275 -4.18814 -4.1971517 -4.2099934 -4.2220955 -4.2345486 -4.234941][-4.2012835 -4.201262 -4.206769 -4.2103133 -4.2072854 -4.1976309 -4.1807775 -4.1595125 -4.1544085 -4.1618595 -4.1745696 -4.190989 -4.2057004 -4.2189565 -4.2223325][-4.186368 -4.1801486 -4.17964 -4.1779366 -4.1715312 -4.161386 -4.1437392 -4.1242661 -4.1291342 -4.1441922 -4.1578851 -4.1744547 -4.186008 -4.1987977 -4.2054625][-4.1801977 -4.1739435 -4.1702795 -4.163918 -4.1467113 -4.127502 -4.1045303 -4.0890155 -4.1016021 -4.1219344 -4.1392393 -4.1613455 -4.1744909 -4.1845355 -4.1881704][-4.1902919 -4.1899319 -4.1852841 -4.1708961 -4.1357007 -4.0929508 -4.0553422 -4.0450783 -4.0706553 -4.1040039 -4.1291838 -4.1570244 -4.170886 -4.17447 -4.174684][-4.1845512 -4.1912289 -4.1898494 -4.1744671 -4.1265817 -4.0580487 -3.9998691 -3.9936953 -4.0436893 -4.0965748 -4.1295662 -4.1603632 -4.1757665 -4.1722674 -4.1663051][-4.1703691 -4.1808152 -4.1855607 -4.1737204 -4.115829 -4.0147872 -3.9194715 -3.917861 -4.0089521 -4.08965 -4.1362638 -4.1747375 -4.1971507 -4.1930971 -4.180933][-4.1544967 -4.1697311 -4.17443 -4.1611996 -4.0901914 -3.9557953 -3.8184173 -3.8348057 -3.9787242 -4.0883327 -4.1507707 -4.1965241 -4.2229948 -4.2198844 -4.2029219][-4.145669 -4.1622505 -4.1638393 -4.1461482 -4.0701652 -3.927155 -3.7914841 -3.8408833 -4.0002294 -4.1096125 -4.1696615 -4.216455 -4.2460012 -4.2488737 -4.2302003][-4.1336827 -4.1507883 -4.1612492 -4.1514492 -4.0919571 -3.986587 -3.9028809 -3.9498856 -4.0661335 -4.1479125 -4.1990261 -4.2420921 -4.266377 -4.2654395 -4.2486992][-4.1233449 -4.1445217 -4.1637564 -4.1620607 -4.1229038 -4.0554805 -4.0117068 -4.0419717 -4.1123037 -4.1691895 -4.2116113 -4.2473507 -4.2653708 -4.2626157 -4.2480221][-4.1252375 -4.1445866 -4.1638908 -4.1634688 -4.1350818 -4.0939016 -4.0706306 -4.0922537 -4.1347313 -4.1777344 -4.2140942 -4.2392178 -4.2505946 -4.248807 -4.2383847][-4.1308494 -4.1450887 -4.1617527 -4.1590805 -4.1369076 -4.1087365 -4.0970855 -4.1179795 -4.1460013 -4.1777244 -4.2056103 -4.2215652 -4.2306805 -4.2304111 -4.22683][-4.1359158 -4.1458535 -4.1594195 -4.15295 -4.1253166 -4.096489 -4.0928078 -4.1197929 -4.1421933 -4.16346 -4.1829538 -4.1970015 -4.2096024 -4.21135 -4.21531][-4.1464777 -4.1541586 -4.1631455 -4.1490178 -4.1137104 -4.0819292 -4.0866528 -4.1190925 -4.1440029 -4.1659384 -4.1847329 -4.1939244 -4.2034988 -4.20282 -4.2074847]]...]
INFO - root - 2017-12-05 13:40:11.958373: step 12910, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 73h:21m:27s remains)
INFO - root - 2017-12-05 13:40:20.391448: step 12920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:56m:19s remains)
INFO - root - 2017-12-05 13:40:29.022850: step 12930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:25m:43s remains)
INFO - root - 2017-12-05 13:40:37.572633: step 12940, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 76h:55m:36s remains)
INFO - root - 2017-12-05 13:40:46.105565: step 12950, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:11m:29s remains)
INFO - root - 2017-12-05 13:40:54.698671: step 12960, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 77h:28m:49s remains)
INFO - root - 2017-12-05 13:41:03.102262: step 12970, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 73h:13m:43s remains)
INFO - root - 2017-12-05 13:41:11.579524: step 12980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 76h:10m:50s remains)
INFO - root - 2017-12-05 13:41:20.092330: step 12990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 75h:31m:38s remains)
INFO - root - 2017-12-05 13:41:28.461301: step 13000, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 76h:41m:53s remains)
2017-12-05 13:41:29.188797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3135128 -4.3166957 -4.3168845 -4.31659 -4.3135514 -4.3016782 -4.2922459 -4.2863994 -4.2726154 -4.2609334 -4.2551646 -4.254703 -4.261219 -4.2701316 -4.2842903][-4.3102098 -4.3130441 -4.3124061 -4.3107681 -4.30845 -4.2996984 -4.293292 -4.2899933 -4.277287 -4.2683544 -4.2656693 -4.2668858 -4.2706351 -4.2730951 -4.2818046][-4.2934508 -4.2967596 -4.2940884 -4.287703 -4.2851324 -4.2812963 -4.2795987 -4.2803636 -4.2713647 -4.2653742 -4.2662358 -4.2685471 -4.2706742 -4.2703686 -4.2776337][-4.2550931 -4.2551975 -4.2469797 -4.2363224 -4.2335954 -4.236948 -4.2426081 -4.2503104 -4.2490416 -4.2473631 -4.2538981 -4.2590947 -4.2642612 -4.2660375 -4.2754912][-4.1910577 -4.1912403 -4.1812453 -4.1690211 -4.1639009 -4.168026 -4.1737814 -4.1845374 -4.1913404 -4.1997557 -4.2177439 -4.2290134 -4.2412376 -4.2531438 -4.2711115][-4.1256828 -4.1305938 -4.1235366 -4.1102448 -4.0963359 -4.0898895 -4.0859404 -4.096611 -4.1115279 -4.1329217 -4.1650591 -4.1855917 -4.2098203 -4.2357607 -4.2646995][-4.099102 -4.1125021 -4.1111569 -4.0977745 -4.0746107 -4.0463777 -4.0234909 -4.0325332 -4.0544505 -4.08737 -4.1323047 -4.1633296 -4.1959133 -4.229404 -4.2630644][-4.121654 -4.1406927 -4.1423335 -4.1320505 -4.10678 -4.059041 -4.0200114 -4.031517 -4.0563731 -4.0885525 -4.1325779 -4.164587 -4.1998587 -4.231822 -4.2637014][-4.1698327 -4.1847525 -4.1819158 -4.1701369 -4.1521716 -4.1089993 -4.07367 -4.0903006 -4.1098304 -4.1311369 -4.1650825 -4.1904325 -4.2209616 -4.2457643 -4.2700715][-4.1931443 -4.2046447 -4.20524 -4.1985345 -4.190764 -4.1625037 -4.1372242 -4.1576014 -4.1737404 -4.187408 -4.2109857 -4.2280288 -4.2511821 -4.2654667 -4.2791729][-4.207747 -4.21817 -4.2226987 -4.2187724 -4.2172589 -4.2019205 -4.1866241 -4.2045155 -4.2159495 -4.2253795 -4.2405057 -4.2535009 -4.271235 -4.2799716 -4.2864351][-4.2276978 -4.2346277 -4.2388792 -4.2356243 -4.2374539 -4.23247 -4.2234678 -4.2323027 -4.2347436 -4.2400732 -4.2532868 -4.2644181 -4.2788777 -4.2868476 -4.29228][-4.2539048 -4.2542391 -4.2534175 -4.2468228 -4.2476482 -4.2499523 -4.249383 -4.2536573 -4.2499857 -4.2503781 -4.2609324 -4.268055 -4.2788405 -4.2882605 -4.2966123][-4.27849 -4.27531 -4.2697744 -4.2629185 -4.2620897 -4.26696 -4.2706394 -4.2724204 -4.2650743 -4.2602096 -4.2661633 -4.269455 -4.2804289 -4.2922931 -4.3029294][-4.275104 -4.2748322 -4.2732949 -4.2698464 -4.2688341 -4.2693634 -4.2713594 -4.2746787 -4.27051 -4.2660184 -4.2708549 -4.2741551 -4.2851648 -4.2978363 -4.307085]]...]
INFO - root - 2017-12-05 13:41:37.777170: step 13010, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 73h:59m:07s remains)
INFO - root - 2017-12-05 13:41:46.316893: step 13020, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 78h:01m:11s remains)
INFO - root - 2017-12-05 13:41:54.927259: step 13030, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 75h:13m:50s remains)
INFO - root - 2017-12-05 13:42:03.399735: step 13040, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 75h:57m:05s remains)
INFO - root - 2017-12-05 13:42:11.975978: step 13050, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 74h:20m:29s remains)
INFO - root - 2017-12-05 13:42:20.551125: step 13060, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 74h:23m:33s remains)
INFO - root - 2017-12-05 13:42:29.071724: step 13070, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 75h:19m:03s remains)
INFO - root - 2017-12-05 13:42:37.499413: step 13080, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 75h:30m:44s remains)
INFO - root - 2017-12-05 13:42:46.030272: step 13090, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:47m:24s remains)
INFO - root - 2017-12-05 13:42:54.407483: step 13100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 76h:04m:30s remains)
2017-12-05 13:42:55.182462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3134136 -4.2965603 -4.28502 -4.2755451 -4.272985 -4.2701597 -4.2668476 -4.2676487 -4.2736297 -4.2790055 -4.282033 -4.2884765 -4.3003368 -4.3124537 -4.319787][-4.2884784 -4.2626691 -4.2428451 -4.226028 -4.2170372 -4.2066946 -4.1997819 -4.2013464 -4.2156959 -4.2270412 -4.2308316 -4.2402596 -4.2520723 -4.2625341 -4.2697716][-4.2580762 -4.221056 -4.1884465 -4.1608748 -4.1424751 -4.1205983 -4.1080256 -4.1136785 -4.1382766 -4.1527338 -4.1567173 -4.1692519 -4.1802382 -4.1862688 -4.1955228][-4.2270131 -4.1765909 -4.1254606 -4.0847006 -4.0492358 -4.0144014 -3.9995294 -4.0087152 -4.0391426 -4.0552869 -4.0568924 -4.0704103 -4.0817409 -4.0882549 -4.109633][-4.2013979 -4.139977 -4.0697532 -4.0141935 -3.9629636 -3.9134777 -3.8903456 -3.9025831 -3.9356377 -3.9465468 -3.9458971 -3.9679 -3.9911375 -4.0032697 -4.0375237][-4.1858253 -4.1129942 -4.0275884 -3.9577043 -3.8882267 -3.8129487 -3.7731171 -3.7845802 -3.8247938 -3.837049 -3.8462539 -3.8936229 -3.941056 -3.9651084 -4.0043025][-4.1749816 -4.089756 -3.9916451 -3.90723 -3.8148029 -3.7104311 -3.652555 -3.6807125 -3.756424 -3.7927396 -3.8143935 -3.876766 -3.9429669 -3.9755611 -4.0051465][-4.163053 -4.0668106 -3.9645476 -3.8832278 -3.7962594 -3.6989629 -3.6548605 -3.7110839 -3.8145263 -3.8662879 -3.8872216 -3.9447174 -4.0058012 -4.0316749 -4.0484662][-4.157567 -4.0665164 -3.9786704 -3.9186854 -3.8630204 -3.8069019 -3.7881048 -3.8398151 -3.9261253 -3.9741759 -3.9911094 -4.0365615 -4.0846276 -4.106225 -4.1176939][-4.1722031 -4.0977635 -4.0300245 -3.98703 -3.9564481 -3.9292953 -3.9206486 -3.9567704 -4.0186758 -4.0527005 -4.0702877 -4.1108994 -4.1511512 -4.169796 -4.1800551][-4.2078948 -4.1548772 -4.105226 -4.07287 -4.052753 -4.0367112 -4.0303078 -4.0542288 -4.0978894 -4.1231713 -4.1399264 -4.1705008 -4.2000856 -4.2162781 -4.2240772][-4.2465062 -4.2121444 -4.1790481 -4.1543946 -4.1375456 -4.1280146 -4.1253157 -4.1442246 -4.1808162 -4.2054148 -4.2208743 -4.240622 -4.2540603 -4.2602339 -4.2611523][-4.274631 -4.2500873 -4.2298784 -4.2133603 -4.2001362 -4.1978211 -4.2043123 -4.2254448 -4.2566404 -4.2781687 -4.2870398 -4.2927108 -4.2956944 -4.2952666 -4.2918963][-4.2965951 -4.278986 -4.2652831 -4.253418 -4.2457042 -4.248611 -4.2604585 -4.2810311 -4.3043156 -4.3186483 -4.3215218 -4.3211288 -4.3191919 -4.3172469 -4.3151259][-4.3182421 -4.3039346 -4.2923841 -4.2822485 -4.2765746 -4.2797508 -4.2899723 -4.3046293 -4.3191919 -4.3281512 -4.3302608 -4.3294764 -4.3283596 -4.3272023 -4.3264518]]...]
INFO - root - 2017-12-05 13:43:03.664855: step 13110, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 74h:16m:40s remains)
INFO - root - 2017-12-05 13:43:12.242028: step 13120, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.863 sec/batch; 76h:31m:45s remains)
INFO - root - 2017-12-05 13:43:20.729762: step 13130, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 73h:50m:09s remains)
INFO - root - 2017-12-05 13:43:29.309357: step 13140, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 75h:00m:54s remains)
INFO - root - 2017-12-05 13:43:37.788453: step 13150, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 73h:27m:15s remains)
INFO - root - 2017-12-05 13:43:46.366169: step 13160, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 73h:43m:09s remains)
INFO - root - 2017-12-05 13:43:54.882315: step 13170, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 76h:35m:44s remains)
INFO - root - 2017-12-05 13:44:03.532646: step 13180, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 75h:09m:34s remains)
INFO - root - 2017-12-05 13:44:11.977554: step 13190, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 77h:54m:54s remains)
INFO - root - 2017-12-05 13:44:20.408257: step 13200, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 75h:26m:16s remains)
2017-12-05 13:44:21.174307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2566838 -4.2521396 -4.2456141 -4.2396894 -4.2375493 -4.23296 -4.2218695 -4.216466 -4.2213058 -4.2389288 -4.2557211 -4.2701216 -4.2811871 -4.2848411 -4.2853112][-4.2545447 -4.2505445 -4.2421074 -4.232511 -4.2234349 -4.2088385 -4.187098 -4.1772046 -4.1877384 -4.212225 -4.2368121 -4.2636275 -4.2848573 -4.2931733 -4.2956057][-4.2566638 -4.2525692 -4.2416434 -4.2268167 -4.2075114 -4.1791687 -4.1419358 -4.1219568 -4.1348619 -4.1666684 -4.2019682 -4.2453713 -4.2783804 -4.2936096 -4.2970905][-4.2557669 -4.2510304 -4.2376828 -4.2186937 -4.1919346 -4.1485019 -4.0887794 -4.0509996 -4.0635724 -4.1081896 -4.1574445 -4.2169023 -4.2594485 -4.2808933 -4.2859864][-4.2472124 -4.2425842 -4.2298031 -4.2120042 -4.1813974 -4.1212564 -4.0304303 -3.9662781 -3.9812679 -4.0454044 -4.1130495 -4.1879783 -4.238214 -4.2661443 -4.2739382][-4.2392707 -4.2364297 -4.2263584 -4.2118278 -4.175787 -4.0933266 -3.9618306 -3.865171 -3.8944402 -3.9887512 -4.0801029 -4.1697693 -4.2250466 -4.2573347 -4.2688494][-4.2374611 -4.237453 -4.227664 -4.2095633 -4.164412 -4.0582218 -3.8848186 -3.7536068 -3.8159428 -3.951524 -4.0676756 -4.1675849 -4.2238107 -4.2565308 -4.2743878][-4.2456665 -4.2438192 -4.2284865 -4.2007794 -4.149878 -4.0341163 -3.8415718 -3.6986389 -3.8031812 -3.9597144 -4.0840559 -4.182313 -4.2349186 -4.2653689 -4.2881079][-4.2599897 -4.2482562 -4.2247629 -4.1922541 -4.1470551 -4.0479 -3.8831439 -3.7688839 -3.8834171 -4.0183668 -4.122324 -4.20616 -4.2526026 -4.2818065 -4.3070765][-4.2694993 -4.2466421 -4.21924 -4.1908832 -4.1568513 -4.0825958 -3.9607079 -3.8862643 -3.9850633 -4.0899191 -4.1653805 -4.2296028 -4.268846 -4.296628 -4.3203106][-4.2675576 -4.2376676 -4.2080226 -4.1861873 -4.1651816 -4.1157589 -4.033814 -3.9914391 -4.067553 -4.1482553 -4.202971 -4.2485275 -4.2793946 -4.3046517 -4.3227587][-4.2590628 -4.2294536 -4.2013364 -4.1892204 -4.1794181 -4.1533461 -4.1082897 -4.0898094 -4.1440449 -4.205513 -4.2422323 -4.2678061 -4.2882476 -4.3081689 -4.3198037][-4.2482672 -4.222374 -4.2004595 -4.1986446 -4.1960669 -4.184227 -4.1657376 -4.1659555 -4.207232 -4.2542067 -4.2760634 -4.28821 -4.2999868 -4.3134131 -4.3183842][-4.2499881 -4.2284508 -4.2125859 -4.21527 -4.2184186 -4.2152872 -4.212019 -4.2253323 -4.2577944 -4.2914314 -4.304409 -4.3100061 -4.3151851 -4.3193836 -4.3168707][-4.2651119 -4.2498484 -4.2378864 -4.239089 -4.2455649 -4.2487493 -4.2547274 -4.2719021 -4.2963858 -4.3183632 -4.3266821 -4.3278074 -4.3254561 -4.3223915 -4.315824]]...]
INFO - root - 2017-12-05 13:44:29.672762: step 13210, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 75h:16m:47s remains)
INFO - root - 2017-12-05 13:44:38.182798: step 13220, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 77h:34m:14s remains)
INFO - root - 2017-12-05 13:44:46.684561: step 13230, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 74h:52m:04s remains)
INFO - root - 2017-12-05 13:44:55.159505: step 13240, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 75h:29m:11s remains)
INFO - root - 2017-12-05 13:45:03.822530: step 13250, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 76h:04m:25s remains)
INFO - root - 2017-12-05 13:45:12.255624: step 13260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 74h:21m:17s remains)
INFO - root - 2017-12-05 13:45:20.733838: step 13270, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 74h:11m:43s remains)
INFO - root - 2017-12-05 13:45:29.240642: step 13280, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 76h:40m:33s remains)
INFO - root - 2017-12-05 13:45:37.666448: step 13290, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 74h:56m:53s remains)
INFO - root - 2017-12-05 13:45:45.917372: step 13300, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 76h:02m:45s remains)
2017-12-05 13:45:46.648388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1585646 -4.1612983 -4.1664805 -4.1584992 -4.1335731 -4.1073012 -4.0839124 -4.0519314 -4.0082912 -3.9670978 -3.9651415 -3.9944963 -4.0183668 -4.0245891 -4.0125151][-4.17675 -4.1853042 -4.1907067 -4.1818261 -4.1583862 -4.1350017 -4.1134553 -4.08858 -4.0540342 -4.0235324 -4.022788 -4.0394435 -4.05571 -4.0578289 -4.0436831][-4.20468 -4.2201257 -4.2216444 -4.2078114 -4.1785893 -4.1545429 -4.1413021 -4.120872 -4.090806 -4.0695171 -4.06575 -4.0661936 -4.0681653 -4.0618377 -4.0595527][-4.215889 -4.2308278 -4.2285991 -4.2056932 -4.1697755 -4.1482716 -4.14347 -4.1234174 -4.0938673 -4.07257 -4.0646219 -4.0635724 -4.0620947 -4.0558028 -4.0647392][-4.2298484 -4.238225 -4.2267618 -4.193399 -4.15056 -4.1226897 -4.1171083 -4.0928755 -4.0605149 -4.0390587 -4.0317326 -4.0435638 -4.057054 -4.0614591 -4.0734825][-4.221787 -4.2188497 -4.1987686 -4.1594381 -4.1082764 -4.0731692 -4.0685215 -4.0454817 -4.0107388 -3.9858944 -3.9719977 -3.9976485 -4.0348363 -4.0517235 -4.0671883][-4.1825924 -4.16936 -4.1492252 -4.1104107 -4.0569959 -4.0289521 -4.0368118 -4.0253019 -3.992188 -3.9585781 -3.9351053 -3.9621599 -4.0066776 -4.0248046 -4.045392][-4.1337953 -4.1176825 -4.0991707 -4.0616913 -4.0132961 -4.0028143 -4.0263286 -4.0203366 -3.9925194 -3.9591012 -3.9347878 -3.9537969 -3.9860611 -3.999438 -4.0247526][-4.08741 -4.0768938 -4.064714 -4.0342464 -3.9982462 -4.005671 -4.0357003 -4.0335445 -4.0188518 -3.9962797 -3.9766693 -3.983886 -3.9925611 -3.9950509 -4.0195489][-4.0654707 -4.0588489 -4.0502853 -4.027957 -4.00726 -4.0291624 -4.06289 -4.0697641 -4.0707889 -4.0662322 -4.0560718 -4.0536523 -4.0399976 -4.0245948 -4.0343771][-4.0630159 -4.0601025 -4.0555048 -4.0361829 -4.0233245 -4.051383 -4.0805836 -4.0843859 -4.0930052 -4.1061335 -4.1078792 -4.1018038 -4.079432 -4.0537238 -4.052413][-4.083827 -4.0839319 -4.0751619 -4.0524831 -4.042613 -4.0651393 -4.07958 -4.073235 -4.0823064 -4.1061831 -4.1157846 -4.10847 -4.0892024 -4.0700378 -4.0677652][-4.1185884 -4.1164322 -4.10349 -4.0802579 -4.0691695 -4.0763793 -4.0787711 -4.0675573 -4.07487 -4.1008439 -4.1080484 -4.0969162 -4.0832705 -4.0768828 -4.080039][-4.1507282 -4.145216 -4.1303229 -4.1111107 -4.0944262 -4.0869474 -4.0811419 -4.073329 -4.0795784 -4.1006508 -4.1017752 -4.0891595 -4.0793958 -4.0800762 -4.0870872][-4.1773448 -4.1672668 -4.1521912 -4.138618 -4.1211538 -4.1053329 -4.0958524 -4.09002 -4.0948 -4.110158 -4.11019 -4.1000638 -4.0928903 -4.0936704 -4.099071]]...]
INFO - root - 2017-12-05 13:45:55.163387: step 13310, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 76h:46m:27s remains)
INFO - root - 2017-12-05 13:46:03.793654: step 13320, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 76h:04m:39s remains)
INFO - root - 2017-12-05 13:46:12.388736: step 13330, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 75h:28m:06s remains)
INFO - root - 2017-12-05 13:46:20.858684: step 13340, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 74h:49m:16s remains)
INFO - root - 2017-12-05 13:46:29.360843: step 13350, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 76h:38m:33s remains)
INFO - root - 2017-12-05 13:46:37.900744: step 13360, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 74h:42m:46s remains)
INFO - root - 2017-12-05 13:46:46.501507: step 13370, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 75h:18m:40s remains)
INFO - root - 2017-12-05 13:46:54.887726: step 13380, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 75h:27m:44s remains)
INFO - root - 2017-12-05 13:47:03.376809: step 13390, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 74h:43m:44s remains)
INFO - root - 2017-12-05 13:47:11.696472: step 13400, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 74h:10m:26s remains)
2017-12-05 13:47:12.466417: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.320375 -4.3046713 -4.2811804 -4.2480073 -4.2176528 -4.2009006 -4.183392 -4.1643348 -4.14404 -4.1255307 -4.1071854 -4.1128473 -4.1611357 -4.2161784 -4.2613788][-4.3173547 -4.299129 -4.2702589 -4.2346439 -4.2051644 -4.19101 -4.1722612 -4.1486764 -4.1250496 -4.1178083 -4.1171656 -4.1380172 -4.1943078 -4.2480059 -4.2876306][-4.3135314 -4.2921057 -4.2601905 -4.2243724 -4.1967611 -4.1818523 -4.1635532 -4.1332388 -4.1049771 -4.1092296 -4.1249895 -4.15885 -4.2204776 -4.2768364 -4.3116775][-4.3080058 -4.285378 -4.2533603 -4.2180333 -4.1905608 -4.1696768 -4.1494236 -4.1123252 -4.0841537 -4.0987844 -4.1281362 -4.1684461 -4.2303176 -4.2899556 -4.3222013][-4.3037343 -4.2814021 -4.2497883 -4.2115946 -4.1773009 -4.1440015 -4.1143546 -4.069078 -4.0421658 -4.0681624 -4.11207 -4.1616836 -4.2263341 -4.2894068 -4.3226647][-4.3025255 -4.2803106 -4.2469697 -4.2012196 -4.14999 -4.0997267 -4.0542603 -4.0001583 -3.9823713 -4.0247831 -4.0848784 -4.1491823 -4.2188678 -4.2826967 -4.3187013][-4.3012681 -4.2764583 -4.238441 -4.180459 -4.1064091 -4.032361 -3.9691005 -3.9195724 -3.9335032 -4.0031514 -4.0780406 -4.15204 -4.2207189 -4.2795687 -4.3175869][-4.3027911 -4.2751985 -4.2324781 -4.1611495 -4.0671124 -3.9715633 -3.8945122 -3.8576872 -3.9098203 -4.0029078 -4.0862145 -4.163547 -4.2306919 -4.285758 -4.3221965][-4.308671 -4.2821441 -4.2397685 -4.1666551 -4.0638475 -3.958226 -3.8747635 -3.850492 -3.9232249 -4.0259914 -4.1103182 -4.1829205 -4.2469225 -4.296453 -4.3296113][-4.3126841 -4.2897806 -4.2547889 -4.1925154 -4.0959144 -3.9928064 -3.911613 -3.8961737 -3.9706602 -4.070765 -4.1521659 -4.2186484 -4.27354 -4.3118496 -4.3375158][-4.3163891 -4.2971859 -4.2700415 -4.2201457 -4.1376204 -4.051744 -3.9865954 -3.9775493 -4.0405421 -4.1273823 -4.2032361 -4.260478 -4.3043947 -4.3303261 -4.3463173][-4.3258119 -4.3091578 -4.286428 -4.2464018 -4.1792445 -4.1122146 -4.0707421 -4.0713911 -4.1173434 -4.1848006 -4.2452531 -4.288795 -4.3234286 -4.3425817 -4.3533936][-4.3345466 -4.3176603 -4.2949548 -4.2602549 -4.204237 -4.1533937 -4.1317649 -4.143537 -4.1812716 -4.23244 -4.2784023 -4.3100247 -4.3344512 -4.3488731 -4.3581042][-4.3375821 -4.3200421 -4.2973509 -4.2658787 -4.2187982 -4.1809778 -4.1742563 -4.19528 -4.2284703 -4.2683625 -4.3030028 -4.3262525 -4.3432851 -4.3529339 -4.35966][-4.33847 -4.3233609 -4.3049464 -4.2785811 -4.2410989 -4.2127452 -4.2121792 -4.2342424 -4.2627778 -4.2942948 -4.31944 -4.336535 -4.3489246 -4.3536625 -4.3565764]]...]
INFO - root - 2017-12-05 13:47:20.904190: step 13410, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 74h:07m:38s remains)
INFO - root - 2017-12-05 13:47:29.420482: step 13420, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 76h:34m:04s remains)
INFO - root - 2017-12-05 13:47:38.045454: step 13430, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 77h:37m:48s remains)
INFO - root - 2017-12-05 13:47:46.480976: step 13440, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 73h:38m:07s remains)
INFO - root - 2017-12-05 13:47:54.985142: step 13450, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 75h:36m:15s remains)
INFO - root - 2017-12-05 13:48:03.477929: step 13460, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 77h:54m:27s remains)
INFO - root - 2017-12-05 13:48:11.924001: step 13470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.847 sec/batch; 75h:01m:12s remains)
INFO - root - 2017-12-05 13:48:20.350158: step 13480, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 76h:08m:57s remains)
INFO - root - 2017-12-05 13:48:28.889201: step 13490, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 75h:05m:23s remains)
INFO - root - 2017-12-05 13:48:37.260895: step 13500, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 74h:04m:19s remains)
2017-12-05 13:48:38.017027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.254425 -4.2407141 -4.2309213 -4.2256722 -4.2248392 -4.2252426 -4.2305684 -4.246047 -4.2630191 -4.27341 -4.2807097 -4.2885976 -4.288156 -4.2806854 -4.2681031][-4.2554502 -4.2399063 -4.2294884 -4.2212653 -4.2173672 -4.2163167 -4.2141581 -4.2243004 -4.2439485 -4.2618308 -4.2736425 -4.2791729 -4.2759547 -4.2684336 -4.2541184][-4.255013 -4.2429037 -4.2353497 -4.2237139 -4.2128687 -4.2055125 -4.19291 -4.1953588 -4.2154384 -4.2419086 -4.2594109 -4.2660131 -4.2619429 -4.2526808 -4.2361145][-4.2578979 -4.2515559 -4.2433262 -4.2262039 -4.2097859 -4.1980677 -4.1744971 -4.16528 -4.1815467 -4.2178726 -4.2438474 -4.2550716 -4.2539444 -4.24513 -4.2277293][-4.257719 -4.2546244 -4.2417917 -4.2179956 -4.1940084 -4.1768541 -4.1437826 -4.1235476 -4.1313996 -4.1751561 -4.2169366 -4.242558 -4.2508707 -4.2469339 -4.2287683][-4.2566886 -4.2529082 -4.2364078 -4.2062774 -4.1694221 -4.1381283 -4.0961752 -4.0698576 -4.0690842 -4.113255 -4.1713319 -4.2183318 -4.2435908 -4.2473092 -4.2301426][-4.2529745 -4.2492809 -4.232192 -4.1967654 -4.1451235 -4.096643 -4.0460496 -4.0118494 -4.0058594 -4.0474176 -4.1172194 -4.1852674 -4.2295747 -4.2428408 -4.2291083][-4.2570453 -4.2530551 -4.2364569 -4.1981778 -4.136878 -4.0726905 -4.0115261 -3.9678106 -3.959172 -3.998904 -4.0768819 -4.1573076 -4.2130857 -4.2346263 -4.2263622][-4.2636342 -4.2563195 -4.2352066 -4.1955948 -4.1364322 -4.0708623 -4.0062022 -3.9625196 -3.9573894 -3.9953308 -4.0738883 -4.1529756 -4.2078719 -4.2323661 -4.23126][-4.2678695 -4.2594256 -4.2362943 -4.200191 -4.1503272 -4.0914574 -4.0324354 -4.0001345 -4.0071359 -4.0455494 -4.1131082 -4.1710467 -4.2097316 -4.2281246 -4.2298927][-4.2653432 -4.2593613 -4.2382913 -4.2086692 -4.1712551 -4.1260033 -4.0807548 -4.0622993 -4.0822468 -4.1225581 -4.1704082 -4.1972547 -4.2078485 -4.2085814 -4.2053351][-4.2583389 -4.2544875 -4.2390466 -4.218904 -4.1960392 -4.166503 -4.1388736 -4.1338282 -4.1600947 -4.1944008 -4.2180772 -4.2142038 -4.1938963 -4.170372 -4.1549411][-4.2381415 -4.2379384 -4.2297478 -4.2177873 -4.2050662 -4.1910248 -4.1822343 -4.1928153 -4.2219763 -4.246851 -4.2497363 -4.2245145 -4.1813951 -4.1353283 -4.0999031][-4.2059083 -4.2099404 -4.2069554 -4.2009335 -4.1929464 -4.1850891 -4.1887956 -4.2147117 -4.2499027 -4.2692785 -4.2606339 -4.2245874 -4.1693025 -4.1112175 -4.0621185][-4.1835756 -4.1874075 -4.1822791 -4.171463 -4.162837 -4.1565409 -4.1658473 -4.2042527 -4.2483821 -4.2663088 -4.2540526 -4.2151566 -4.1559763 -4.0935397 -4.0410519]]...]
INFO - root - 2017-12-05 13:48:46.414013: step 13510, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.797 sec/batch; 70h:36m:36s remains)
INFO - root - 2017-12-05 13:48:54.958839: step 13520, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 76h:02m:27s remains)
INFO - root - 2017-12-05 13:49:03.611306: step 13530, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 75h:39m:43s remains)
INFO - root - 2017-12-05 13:49:12.139519: step 13540, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 75h:47m:08s remains)
INFO - root - 2017-12-05 13:49:20.732169: step 13550, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 78h:58m:38s remains)
INFO - root - 2017-12-05 13:49:29.355689: step 13560, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 74h:36m:46s remains)
INFO - root - 2017-12-05 13:49:37.868805: step 13570, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 75h:51m:28s remains)
INFO - root - 2017-12-05 13:49:46.398780: step 13580, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 74h:40m:03s remains)
INFO - root - 2017-12-05 13:49:54.905831: step 13590, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 73h:34m:23s remains)
INFO - root - 2017-12-05 13:50:03.281231: step 13600, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 74h:19m:53s remains)
2017-12-05 13:50:04.113162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1535683 -4.1439614 -4.1412768 -4.1256866 -4.1065807 -4.087316 -4.0696607 -4.0544567 -4.03529 -4.00445 -3.98457 -3.9886348 -4.0003772 -4.013545 -4.0361876][-4.1498141 -4.1399179 -4.1294236 -4.1140652 -4.0922585 -4.068469 -4.0503058 -4.0313044 -4.0163112 -4.0015721 -3.9912953 -3.9931598 -3.9937057 -3.9892993 -3.9958639][-4.14817 -4.13533 -4.114316 -4.0963826 -4.0705209 -4.0387325 -4.010427 -3.9865961 -3.9845676 -3.9978995 -4.0098219 -4.0140676 -4.0045218 -3.9876304 -3.9860117][-4.1425972 -4.1314373 -4.1089697 -4.0849142 -4.0504427 -4.0090613 -3.9645059 -3.9307685 -3.9433279 -3.9903238 -4.0242605 -4.0395947 -4.0256124 -3.9981511 -3.9879382][-4.1376233 -4.1329703 -4.11399 -4.0848236 -4.0436687 -3.991997 -3.9235389 -3.8709788 -3.8974519 -3.9735806 -4.0301166 -4.0590372 -4.0512385 -4.0195422 -3.9940164][-4.1327949 -4.1301789 -4.1144147 -4.08381 -4.0419645 -3.978894 -3.8760707 -3.7939403 -3.8359723 -3.9451339 -4.0258546 -4.0702133 -4.07512 -4.0515814 -4.0221958][-4.1251388 -4.1210375 -4.112329 -4.0857329 -4.03922 -3.9650888 -3.8288574 -3.7040062 -3.760329 -3.90538 -4.0094509 -4.0726037 -4.0920978 -4.0782814 -4.0532379][-4.1278982 -4.1260648 -4.1239853 -4.1000624 -4.0467114 -3.9656131 -3.8164873 -3.6712608 -3.7283611 -3.8813717 -3.9936035 -4.0661225 -4.09799 -4.0998597 -4.0854287][-4.1349635 -4.1367311 -4.1378469 -4.1119037 -4.0615883 -3.9926574 -3.8772447 -3.7727363 -3.8031988 -3.9057429 -3.992095 -4.0575647 -4.0958295 -4.1143985 -4.1116424][-4.1401968 -4.1416039 -4.1370611 -4.1088524 -4.0713286 -4.0286441 -3.9620266 -3.9046578 -3.9095848 -3.9565067 -4.0078335 -4.0541906 -4.0896106 -4.1185379 -4.1245723][-4.1433005 -4.1413851 -4.1341877 -4.1071715 -4.0801435 -4.0603476 -4.02381 -3.9922259 -3.9800661 -3.9926212 -4.0233684 -4.052207 -4.0811806 -4.1114526 -4.122376][-4.1349373 -4.1343851 -4.1274042 -4.1025839 -4.0827 -4.0790129 -4.0611081 -4.0398359 -4.0207677 -4.0185881 -4.0351906 -4.0532546 -4.0748596 -4.0956483 -4.1076756][-4.1083059 -4.114089 -4.1071019 -4.07992 -4.0680194 -4.0806117 -4.0803127 -4.0653114 -4.0456142 -4.0440741 -4.05203 -4.0613604 -4.0725927 -4.0798054 -4.0891294][-4.063571 -4.0752077 -4.0715194 -4.0466905 -4.0466342 -4.0718017 -4.0831861 -4.0688062 -4.0501394 -4.0535374 -4.0654945 -4.0726528 -4.0757065 -4.071558 -4.0694923][-4.0337996 -4.0506186 -4.0529208 -4.0318265 -4.0309825 -4.052948 -4.0690536 -4.0526648 -4.0374508 -4.0451574 -4.0646811 -4.0828104 -4.0890579 -4.0839763 -4.0729709]]...]
INFO - root - 2017-12-05 13:50:12.553779: step 13610, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 75h:10m:56s remains)
INFO - root - 2017-12-05 13:50:20.957382: step 13620, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.776 sec/batch; 68h:44m:45s remains)
INFO - root - 2017-12-05 13:50:29.416191: step 13630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:42m:08s remains)
INFO - root - 2017-12-05 13:50:37.960570: step 13640, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:19m:24s remains)
INFO - root - 2017-12-05 13:50:46.535384: step 13650, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 76h:15m:43s remains)
INFO - root - 2017-12-05 13:50:55.104261: step 13660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 75h:57m:51s remains)
INFO - root - 2017-12-05 13:51:03.586343: step 13670, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 74h:22m:38s remains)
INFO - root - 2017-12-05 13:51:12.090786: step 13680, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 76h:44m:10s remains)
INFO - root - 2017-12-05 13:51:20.578245: step 13690, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 73h:34m:26s remains)
INFO - root - 2017-12-05 13:51:28.927691: step 13700, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 75h:30m:25s remains)
2017-12-05 13:51:29.686342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1329293 -4.1352348 -4.1384373 -4.1535573 -4.1723032 -4.1841545 -4.1953835 -4.2046585 -4.206079 -4.1999259 -4.1972656 -4.1916986 -4.1907969 -4.192904 -4.1993685][-4.1284347 -4.1439209 -4.1552081 -4.171896 -4.1824417 -4.1842937 -4.1909504 -4.1975965 -4.1969104 -4.1871152 -4.1797233 -4.1731853 -4.1752424 -4.1808243 -4.1932149][-4.1332159 -4.16082 -4.1787195 -4.1957035 -4.2024503 -4.1942716 -4.1878247 -4.1870966 -4.186667 -4.1801186 -4.174386 -4.1652532 -4.1670527 -4.1726465 -4.1830006][-4.1327662 -4.1683011 -4.1922326 -4.2088361 -4.213943 -4.2043772 -4.1863394 -4.1674061 -4.1597753 -4.15596 -4.1540289 -4.149436 -4.1524096 -4.1584015 -4.1721067][-4.1300082 -4.1625838 -4.1898375 -4.2045941 -4.2048531 -4.1897626 -4.1563005 -4.1230707 -4.1095819 -4.1068468 -4.1097355 -4.1166525 -4.1293921 -4.142385 -4.1636157][-4.1288853 -4.151722 -4.1754785 -4.1822777 -4.1698546 -4.1348171 -4.0715561 -4.0222068 -4.0314312 -4.0570025 -4.0737858 -4.09138 -4.1065903 -4.1247916 -4.153306][-4.1137104 -4.128262 -4.1479363 -4.1466632 -4.1221194 -4.0627551 -3.9642189 -3.9078908 -3.9628334 -4.0345321 -4.0706091 -4.0879836 -4.0952563 -4.1090226 -4.1346245][-4.1045446 -4.1113214 -4.1262736 -4.11999 -4.0936351 -4.0419445 -3.957058 -3.9183455 -3.9907854 -4.0717592 -4.1096721 -4.1190176 -4.1155667 -4.1170888 -4.1316991][-4.1217294 -4.1261597 -4.1359367 -4.1287026 -4.1107454 -4.0892434 -4.0531745 -4.0421128 -4.0901651 -4.1404986 -4.1612883 -4.1561327 -4.140924 -4.1330318 -4.1368618][-4.1575828 -4.1645894 -4.1703157 -4.1629014 -4.1512232 -4.1428242 -4.1329532 -4.1344366 -4.1606464 -4.1850915 -4.1914868 -4.1779275 -4.1547408 -4.141551 -4.1411991][-4.1930666 -4.1953588 -4.1942015 -4.1874323 -4.178895 -4.1717515 -4.16679 -4.1677442 -4.1788111 -4.1885791 -4.1912217 -4.1804271 -4.1583982 -4.1408234 -4.139503][-4.2146282 -4.2063189 -4.1958852 -4.1891141 -4.1845584 -4.1780105 -4.1711097 -4.1692266 -4.1707458 -4.1761808 -4.184298 -4.184062 -4.1691346 -4.1497107 -4.1409545][-4.2158661 -4.1993456 -4.1831741 -4.1756268 -4.1732931 -4.1674428 -4.1632509 -4.1607523 -4.1561465 -4.1604085 -4.174057 -4.1846709 -4.1825123 -4.1680622 -4.1565776][-4.2050276 -4.1860995 -4.1707926 -4.16188 -4.1589451 -4.157752 -4.156961 -4.1547894 -4.1496735 -4.151381 -4.1635284 -4.177268 -4.1860733 -4.1811347 -4.1724076][-4.1953468 -4.1799841 -4.17101 -4.1644449 -4.1652074 -4.1647224 -4.1612477 -4.154892 -4.1511407 -4.1500492 -4.1604414 -4.1783609 -4.1945229 -4.1959591 -4.1898584]]...]
INFO - root - 2017-12-05 13:51:38.139464: step 13710, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 76h:20m:54s remains)
INFO - root - 2017-12-05 13:51:46.524422: step 13720, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 72h:13m:13s remains)
INFO - root - 2017-12-05 13:51:54.997736: step 13730, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.763 sec/batch; 67h:32m:26s remains)
INFO - root - 2017-12-05 13:52:03.402423: step 13740, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 73h:29m:35s remains)
INFO - root - 2017-12-05 13:52:11.966668: step 13750, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 76h:17m:27s remains)
INFO - root - 2017-12-05 13:52:20.466886: step 13760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 75h:53m:41s remains)
INFO - root - 2017-12-05 13:52:29.013272: step 13770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 77h:17m:11s remains)
INFO - root - 2017-12-05 13:52:37.552486: step 13780, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 77h:40m:44s remains)
INFO - root - 2017-12-05 13:52:46.078520: step 13790, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 75h:59m:54s remains)
INFO - root - 2017-12-05 13:52:54.485762: step 13800, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 76h:13m:21s remains)
2017-12-05 13:52:55.260974: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2877512 -4.2793131 -4.270432 -4.2690907 -4.2673774 -4.2594466 -4.2511511 -4.2478671 -4.2463627 -4.2528305 -4.2633877 -4.2759461 -4.2949076 -4.3184457 -4.3397527][-4.2490072 -4.238441 -4.2277641 -4.22822 -4.2289228 -4.2154212 -4.199594 -4.1970859 -4.1969414 -4.206841 -4.2224789 -4.2411122 -4.2660646 -4.2962723 -4.3245606][-4.1941433 -4.182714 -4.1735487 -4.1777821 -4.181 -4.15751 -4.1328735 -4.1319785 -4.1354327 -4.1524744 -4.1802387 -4.2121944 -4.2450333 -4.2791767 -4.3114676][-4.1282058 -4.11831 -4.1139669 -4.1251478 -4.1311522 -4.0960383 -4.059691 -4.0650535 -4.0776978 -4.101944 -4.1419578 -4.191021 -4.2337022 -4.27138 -4.3052659][-4.0578747 -4.0533376 -4.0516653 -4.0620732 -4.0608411 -4.0090222 -3.9608784 -3.99059 -4.0297532 -4.0652952 -4.1087613 -4.1694326 -4.22421 -4.2684531 -4.30439][-4.0035143 -4.0006933 -3.9968 -3.996038 -3.9717178 -3.8889654 -3.8261933 -3.9065862 -3.9929628 -4.0439334 -4.0859208 -4.152801 -4.2182727 -4.2691 -4.30585][-3.993134 -3.9843273 -3.9692507 -3.9498353 -3.9058585 -3.8040481 -3.7320726 -3.8474669 -3.9688075 -4.031486 -4.0738497 -4.1449013 -4.2184634 -4.27226 -4.3082151][-4.0158725 -4.0052776 -3.9814687 -3.9522896 -3.9169633 -3.8426137 -3.7863984 -3.8728185 -3.9764574 -4.027842 -4.0658126 -4.1397729 -4.2198882 -4.2757893 -4.3106604][-4.0584216 -4.0534449 -4.0307302 -4.0049419 -3.9873126 -3.9486508 -3.9079537 -3.9535394 -4.0195193 -4.0511012 -4.0811329 -4.1514931 -4.2282915 -4.2796378 -4.3116326][-4.0999961 -4.1058359 -4.0952692 -4.0774994 -4.0623865 -4.0338235 -3.997669 -4.0208874 -4.0619125 -4.0835371 -4.1092038 -4.1727467 -4.2442145 -4.2887077 -4.3155107][-4.1207385 -4.1299624 -4.1309328 -4.1221 -4.1066837 -4.0786991 -4.0516419 -4.0667009 -4.0959435 -4.1117854 -4.1365447 -4.1924996 -4.2575336 -4.2963915 -4.3196988][-4.1252966 -4.13302 -4.1370587 -4.1321144 -4.12144 -4.0983019 -4.080162 -4.095921 -4.1190038 -4.1330166 -4.1576519 -4.2089911 -4.2675118 -4.3006334 -4.3224807][-4.1418915 -4.14429 -4.1497293 -4.1470861 -4.1352139 -4.114326 -4.100142 -4.11879 -4.1449203 -4.16442 -4.1884742 -4.2317252 -4.2793636 -4.3060846 -4.3253064][-4.1782422 -4.179791 -4.1837363 -4.1828122 -4.1716285 -4.1531 -4.1408768 -4.1566267 -4.1811323 -4.20619 -4.2306185 -4.2626586 -4.2947688 -4.3130794 -4.3291674][-4.2259378 -4.2304397 -4.2338161 -4.2337618 -4.2271209 -4.2147956 -4.2044482 -4.2163076 -4.2357244 -4.2579036 -4.2791529 -4.2986403 -4.3162937 -4.3266821 -4.3379459]]...]
INFO - root - 2017-12-05 13:53:03.749707: step 13810, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 74h:46m:29s remains)
INFO - root - 2017-12-05 13:53:12.258460: step 13820, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.785 sec/batch; 69h:27m:25s remains)
INFO - root - 2017-12-05 13:53:20.814337: step 13830, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 77h:12m:07s remains)
INFO - root - 2017-12-05 13:53:29.265511: step 13840, loss = 2.05, batch loss = 1.99 (11.1 examples/sec; 0.722 sec/batch; 63h:55m:36s remains)
INFO - root - 2017-12-05 13:53:37.833176: step 13850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 75h:18m:58s remains)
INFO - root - 2017-12-05 13:53:46.408939: step 13860, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 76h:55m:57s remains)
INFO - root - 2017-12-05 13:53:55.014205: step 13870, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 77h:35m:10s remains)
INFO - root - 2017-12-05 13:54:03.536528: step 13880, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 75h:00m:45s remains)
INFO - root - 2017-12-05 13:54:12.077339: step 13890, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 76h:23m:31s remains)
INFO - root - 2017-12-05 13:54:20.645515: step 13900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 76h:51m:58s remains)
2017-12-05 13:54:21.506556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2209439 -4.2242026 -4.225832 -4.2267675 -4.2368908 -4.2541795 -4.2638974 -4.2646818 -4.26221 -4.2610617 -4.2568932 -4.2558355 -4.2679815 -4.278266 -4.2801619][-4.21021 -4.2110839 -4.2129331 -4.2114959 -4.2188005 -4.2356782 -4.2466106 -4.2484488 -4.2484889 -4.2499266 -4.2492533 -4.2522197 -4.2656755 -4.2768593 -4.27903][-4.2144389 -4.2134008 -4.2113323 -4.2028632 -4.20317 -4.218936 -4.2343 -4.2422833 -4.2483048 -4.2517567 -4.253902 -4.2585907 -4.268918 -4.276948 -4.2789087][-4.2289658 -4.2270484 -4.2164211 -4.1930017 -4.1793156 -4.1891069 -4.2048078 -4.2185826 -4.2359819 -4.2520475 -4.2641606 -4.2730737 -4.2809076 -4.2852197 -4.2870903][-4.2385244 -4.2314382 -4.2036672 -4.1577153 -4.1221323 -4.1139407 -4.1170864 -4.1296086 -4.15962 -4.1974587 -4.2332544 -4.2577844 -4.2709246 -4.27661 -4.2800941][-4.2508311 -4.23764 -4.1936488 -4.1286693 -4.0713139 -4.03415 -4.0120506 -4.0124278 -4.04664 -4.1098924 -4.1781034 -4.2267218 -4.2535 -4.2663517 -4.2727718][-4.2669859 -4.2548161 -4.2096696 -4.1429811 -4.0764 -4.0158396 -3.9658785 -3.943239 -3.9662049 -4.0373354 -4.1234632 -4.1917868 -4.2322745 -4.2545338 -4.2659273][-4.27337 -4.2700019 -4.2399616 -4.1937532 -4.1436133 -4.0884838 -4.0304937 -3.9892423 -3.9893262 -4.03821 -4.1091571 -4.1737356 -4.2140408 -4.2375484 -4.2537456][-4.2721782 -4.2816243 -4.2744312 -4.2562094 -4.2341051 -4.203752 -4.1571608 -4.1097312 -4.0879488 -4.1032219 -4.1435413 -4.1863732 -4.2131448 -4.2311907 -4.2502203][-4.2608118 -4.2788458 -4.2865429 -4.2854581 -4.2816968 -4.2741427 -4.2492285 -4.21419 -4.1888094 -4.185112 -4.20147 -4.2210331 -4.2303004 -4.2382293 -4.2519116][-4.2446232 -4.2612329 -4.2717562 -4.2720647 -4.273109 -4.2832508 -4.2802062 -4.2667265 -4.2528591 -4.2473059 -4.2523136 -4.2543745 -4.25001 -4.2490788 -4.2528839][-4.2389822 -4.2447667 -4.2465968 -4.2410955 -4.2406249 -4.2576346 -4.2677207 -4.2685404 -4.2658887 -4.2656174 -4.268455 -4.265573 -4.2569323 -4.2518005 -4.249176][-4.2473259 -4.2441311 -4.238112 -4.2291808 -4.226686 -4.2419395 -4.252933 -4.2568655 -4.2586646 -4.2619848 -4.2664824 -4.2656045 -4.2584805 -4.2523913 -4.2483649][-4.2708683 -4.2697496 -4.2633681 -4.2547474 -4.2514472 -4.2613873 -4.2685151 -4.270318 -4.2704492 -4.2730732 -4.2775736 -4.2774572 -4.2716036 -4.26686 -4.2639484][-4.2998672 -4.30753 -4.3059945 -4.2981339 -4.291976 -4.2941475 -4.2953281 -4.2941823 -4.29211 -4.2934012 -4.2970138 -4.2981319 -4.2956047 -4.2935638 -4.2924418]]...]
INFO - root - 2017-12-05 13:54:30.016185: step 13910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 75h:22m:49s remains)
INFO - root - 2017-12-05 13:54:38.623086: step 13920, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 77h:38m:45s remains)
INFO - root - 2017-12-05 13:54:47.227916: step 13930, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 77h:22m:30s remains)
INFO - root - 2017-12-05 13:54:55.837878: step 13940, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:18m:47s remains)
INFO - root - 2017-12-05 13:55:04.340806: step 13950, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.796 sec/batch; 70h:28m:18s remains)
INFO - root - 2017-12-05 13:55:12.953828: step 13960, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.874 sec/batch; 77h:22m:23s remains)
INFO - root - 2017-12-05 13:55:21.417684: step 13970, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 76h:09m:58s remains)
INFO - root - 2017-12-05 13:55:29.799443: step 13980, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 73h:50m:35s remains)
INFO - root - 2017-12-05 13:55:38.409207: step 13990, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 76h:59m:25s remains)
INFO - root - 2017-12-05 13:55:47.017076: step 14000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 76h:27m:40s remains)
2017-12-05 13:55:47.762116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2197943 -4.2148843 -4.2183027 -4.2244153 -4.2309155 -4.239563 -4.2464371 -4.2389121 -4.2179313 -4.1783586 -4.1362753 -4.1186686 -4.1345539 -4.1621866 -4.1819224][-4.20876 -4.2011418 -4.2043338 -4.2100167 -4.2153239 -4.2199917 -4.2155852 -4.1987739 -4.1669879 -4.1123438 -4.0585413 -4.0453172 -4.0789824 -4.12573 -4.1626859][-4.1947045 -4.1880479 -4.1946387 -4.2049031 -4.212626 -4.2076368 -4.1895919 -4.1576152 -4.1056118 -4.03563 -3.9879332 -3.9967542 -4.04584 -4.102869 -4.1492205][-4.1931276 -4.1901383 -4.1974077 -4.2081194 -4.2130561 -4.1957722 -4.1600685 -4.1091933 -4.0438352 -3.9850187 -3.9737711 -4.0110931 -4.0638475 -4.1159678 -4.1579785][-4.1989555 -4.19377 -4.1940827 -4.1942291 -4.1884918 -4.1531777 -4.0916634 -4.0185103 -3.9582829 -3.9440775 -3.9840043 -4.0428739 -4.0976691 -4.1456127 -4.1770887][-4.2053709 -4.1891065 -4.1750569 -4.157084 -4.1247268 -4.0515156 -3.9526079 -3.8663533 -3.8419356 -3.8986363 -3.9831097 -4.0579133 -4.1151323 -4.1564889 -4.1772885][-4.2044668 -4.1719604 -4.1365962 -4.0955129 -4.0338144 -3.9259033 -3.8064551 -3.7558112 -3.8023329 -3.9057925 -4.0001483 -4.0689406 -4.1134324 -4.1386342 -4.1517367][-4.2219768 -4.1840591 -4.1392488 -4.0897055 -4.0203581 -3.9135966 -3.8306432 -3.8368893 -3.9107208 -3.9979923 -4.0648489 -4.1087542 -4.1348209 -4.1458206 -4.1542578][-4.2500091 -4.21701 -4.1747608 -4.124609 -4.0637379 -3.9922283 -3.9595182 -3.9881465 -4.044838 -4.1017618 -4.1452031 -4.170681 -4.1864944 -4.1898022 -4.1923423][-4.2764 -4.2501855 -4.2113738 -4.1665025 -4.1223931 -4.0882921 -4.0885229 -4.1164651 -4.15218 -4.188375 -4.2109942 -4.2234583 -4.2362018 -4.2379088 -4.2362952][-4.2997475 -4.2779546 -4.2441239 -4.2076564 -4.1796312 -4.1702647 -4.1837592 -4.2060914 -4.22775 -4.2460718 -4.253921 -4.2580609 -4.2663479 -4.268681 -4.2670245][-4.3099546 -4.2907019 -4.2646093 -4.2376275 -4.2211604 -4.2243328 -4.2390485 -4.2546411 -4.2656088 -4.2720957 -4.2735028 -4.2734842 -4.278656 -4.281426 -4.278276][-4.3093281 -4.29326 -4.27556 -4.2575331 -4.2474775 -4.2511845 -4.2624712 -4.27192 -4.27685 -4.2763195 -4.2734847 -4.272306 -4.2743287 -4.275146 -4.27147][-4.2995834 -4.2854128 -4.2729559 -4.2613106 -4.2547307 -4.2564597 -4.2625804 -4.2666116 -4.2680631 -4.26554 -4.2619548 -4.2609606 -4.261879 -4.2617645 -4.2598743][-4.2973185 -4.2852635 -4.2749753 -4.2668366 -4.2623887 -4.2626014 -4.2655911 -4.2677021 -4.2691283 -4.2686195 -4.2687387 -4.2707176 -4.2721062 -4.2722707 -4.2720251]]...]
INFO - root - 2017-12-05 13:55:56.396005: step 14010, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 76h:24m:11s remains)
INFO - root - 2017-12-05 13:56:04.862690: step 14020, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.883 sec/batch; 78h:06m:11s remains)
INFO - root - 2017-12-05 13:56:13.430155: step 14030, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 76h:47m:06s remains)
INFO - root - 2017-12-05 13:56:21.952992: step 14040, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 76h:32m:38s remains)
INFO - root - 2017-12-05 13:56:30.520324: step 14050, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 79h:33m:53s remains)
INFO - root - 2017-12-05 13:56:39.004734: step 14060, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.747 sec/batch; 66h:06m:00s remains)
INFO - root - 2017-12-05 13:56:47.534139: step 14070, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 74h:33m:59s remains)
INFO - root - 2017-12-05 13:56:56.036056: step 14080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 74h:08m:31s remains)
INFO - root - 2017-12-05 13:57:04.701286: step 14090, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 75h:43m:43s remains)
INFO - root - 2017-12-05 13:57:13.154586: step 14100, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 75h:17m:08s remains)
2017-12-05 13:57:13.947925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2889462 -4.281981 -4.2769566 -4.2725515 -4.2703056 -4.2721429 -4.2783737 -4.2848511 -4.289413 -4.2922812 -4.2936039 -4.2942796 -4.2944784 -4.2946768 -4.2956376][-4.2662735 -4.2571154 -4.2491903 -4.24069 -4.2361069 -4.2381725 -4.2484927 -4.2609639 -4.270236 -4.2774148 -4.281951 -4.2827792 -4.2797751 -4.2757034 -4.2723937][-4.2409329 -4.2267971 -4.2124825 -4.1958179 -4.1850681 -4.1844945 -4.1976023 -4.2187138 -4.2382193 -4.2550716 -4.2644129 -4.2648997 -4.2584434 -4.2530179 -4.24864][-4.216042 -4.1949944 -4.1687841 -4.1346087 -4.1094804 -4.10139 -4.116323 -4.1468143 -4.1816945 -4.2157221 -4.2337332 -4.2365336 -4.2301841 -4.2265048 -4.2263021][-4.1939378 -4.1694489 -4.134172 -4.0854096 -4.0463457 -4.0324116 -4.0456405 -4.0750895 -4.1175985 -4.1671815 -4.1963115 -4.2062378 -4.2084026 -4.2142105 -4.2224336][-4.1702976 -4.1452713 -4.1042662 -4.0502882 -4.00885 -3.9898281 -3.9884102 -3.9939852 -4.0267472 -4.0899153 -4.1407 -4.16722 -4.1836667 -4.2030911 -4.2213039][-4.1503382 -4.1238275 -4.0794687 -4.0253072 -3.9787385 -3.9374754 -3.8962026 -3.8503473 -3.860301 -3.9516144 -4.0436344 -4.0962219 -4.1266146 -4.1591744 -4.1908846][-4.1203303 -4.0944691 -4.0485549 -3.9887695 -3.9278557 -3.8522954 -3.753401 -3.6345139 -3.6105213 -3.7524509 -3.9059863 -3.9951277 -4.045177 -4.0949469 -4.1414852][-4.1060739 -4.0838766 -4.0379338 -3.9750178 -3.9162335 -3.8475842 -3.7521498 -3.6236897 -3.5777562 -3.7110043 -3.8654976 -3.9551251 -4.0105562 -4.0701938 -4.121747][-4.1179042 -4.0933962 -4.045702 -3.9872012 -3.9521148 -3.927022 -3.8922744 -3.8300197 -3.8005228 -3.8683395 -3.9526889 -3.9968472 -4.0350857 -4.0947094 -4.1456857][-4.1367826 -4.1041455 -4.0483613 -3.9918432 -3.9764609 -3.9914694 -4.0019455 -3.980438 -3.9606252 -3.9871585 -4.0202289 -4.0298271 -4.0586352 -4.125299 -4.1780248][-4.1756029 -4.1418977 -4.08579 -4.0355058 -4.0339293 -4.0698285 -4.0975657 -4.093647 -4.0814681 -4.0869665 -4.0952158 -4.0880265 -4.1072626 -4.1651216 -4.2134728][-4.2223167 -4.1913495 -4.144722 -4.1084781 -4.1166325 -4.1549673 -4.183372 -4.1849051 -4.1774292 -4.1793985 -4.1827149 -4.1750326 -4.1856194 -4.2232776 -4.2539477][-4.2586946 -4.2322068 -4.1997581 -4.1801 -4.19288 -4.2232313 -4.2425714 -4.2424831 -4.23869 -4.242692 -4.2501922 -4.249958 -4.2575231 -4.2769833 -4.2916207][-4.2804022 -4.2587385 -4.240912 -4.2351127 -4.2457309 -4.2624674 -4.2717342 -4.271389 -4.2692995 -4.2742882 -4.2828379 -4.2879643 -4.2953777 -4.3046007 -4.3102551]]...]
INFO - root - 2017-12-05 13:57:22.461818: step 14110, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:18m:44s remains)
INFO - root - 2017-12-05 13:57:30.981855: step 14120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 74h:18m:58s remains)
INFO - root - 2017-12-05 13:57:39.632464: step 14130, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 80h:05m:12s remains)
INFO - root - 2017-12-05 13:57:48.128636: step 14140, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.788 sec/batch; 69h:39m:00s remains)
INFO - root - 2017-12-05 13:57:56.664686: step 14150, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 74h:14m:51s remains)
INFO - root - 2017-12-05 13:58:05.171768: step 14160, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 74h:46m:53s remains)
INFO - root - 2017-12-05 13:58:13.564580: step 14170, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 73h:48m:07s remains)
INFO - root - 2017-12-05 13:58:22.121192: step 14180, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 73h:13m:17s remains)
INFO - root - 2017-12-05 13:58:30.752757: step 14190, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 74h:30m:04s remains)
INFO - root - 2017-12-05 13:58:39.210363: step 14200, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 77h:10m:28s remains)
2017-12-05 13:58:39.935002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3281336 -4.3276634 -4.33015 -4.330965 -4.3288336 -4.3272634 -4.3276958 -4.3285112 -4.3301845 -4.3302298 -4.330389 -4.3303804 -4.3301082 -4.3322554 -4.3358111][-4.3168697 -4.3151612 -4.3171544 -4.3167663 -4.3141284 -4.3139315 -4.3164678 -4.31928 -4.3229504 -4.3255992 -4.3270426 -4.3260479 -4.3246307 -4.3270707 -4.3321161][-4.2977171 -4.2945776 -4.2939754 -4.2920866 -4.28988 -4.2891 -4.2917848 -4.293612 -4.2956185 -4.3000231 -4.3057151 -4.3083425 -4.3104782 -4.3149452 -4.321866][-4.2667322 -4.2613497 -4.2588243 -4.256259 -4.2518463 -4.2486334 -4.2509551 -4.2518744 -4.2505865 -4.2533951 -4.26177 -4.2714438 -4.2813339 -4.2890592 -4.2971][-4.22206 -4.2140822 -4.2118316 -4.2091188 -4.202878 -4.1965694 -4.1959071 -4.194778 -4.190887 -4.1955671 -4.2105265 -4.2264838 -4.242682 -4.2536631 -4.2607722][-4.1840677 -4.1741142 -4.1703973 -4.162673 -4.1493731 -4.1336174 -4.1201 -4.1089487 -4.1024723 -4.1201067 -4.1508822 -4.1756206 -4.1965685 -4.2100754 -4.2163258][-4.1756968 -4.164597 -4.1543665 -4.1334734 -4.1040568 -4.0694127 -4.0333395 -4.0047641 -3.9900553 -4.0188456 -4.0691652 -4.1055589 -4.1339684 -4.1549768 -4.1644907][-4.1884127 -4.1784506 -4.165741 -4.1379881 -4.0982165 -4.0432134 -3.9771497 -3.9196622 -3.8909588 -3.9185979 -3.9808381 -4.0298533 -4.0681839 -4.102881 -4.1190882][-4.204154 -4.1974626 -4.188252 -4.1682973 -4.1353793 -4.07602 -3.9968643 -3.9218161 -3.8788562 -3.8943846 -3.9489176 -4.0007582 -4.0439487 -4.0862632 -4.1053414][-4.2091355 -4.207283 -4.2040124 -4.1987109 -4.1850214 -4.1420016 -4.0772114 -4.0132623 -3.9703085 -3.9701078 -4.0002742 -4.0368 -4.0740666 -4.1115813 -4.1247597][-4.2024622 -4.2035403 -4.207263 -4.2156558 -4.2204132 -4.201098 -4.1605506 -4.1178083 -4.0842781 -4.077065 -4.0884161 -4.1096921 -4.1350355 -4.1582842 -4.1578298][-4.1919327 -4.1908588 -4.1975975 -4.2150016 -4.2327 -4.231204 -4.2125692 -4.1903358 -4.1703534 -4.165338 -4.172246 -4.1834598 -4.1946912 -4.1999431 -4.1849523][-4.1912532 -4.1838531 -4.1898022 -4.2098656 -4.2315969 -4.2380242 -4.2311182 -4.2210741 -4.2117076 -4.2135077 -4.2216978 -4.2280884 -4.2281609 -4.2189746 -4.1912775][-4.196279 -4.1810632 -4.1822162 -4.1979666 -4.2160773 -4.2241697 -4.2189155 -4.2116833 -4.2076054 -4.2141538 -4.2235594 -4.2282276 -4.2258706 -4.2125249 -4.1767535][-4.2061849 -4.1865683 -4.1825166 -4.1904526 -4.19957 -4.2046142 -4.1962919 -4.1886554 -4.1872821 -4.1944547 -4.2024536 -4.2074027 -4.20925 -4.1968837 -4.1573167]]...]
INFO - root - 2017-12-05 13:58:48.409352: step 14210, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 76h:56m:34s remains)
INFO - root - 2017-12-05 13:58:56.836667: step 14220, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 76h:42m:41s remains)
INFO - root - 2017-12-05 13:59:05.242699: step 14230, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 72h:18m:47s remains)
INFO - root - 2017-12-05 13:59:13.825810: step 14240, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:57m:35s remains)
INFO - root - 2017-12-05 13:59:22.333917: step 14250, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 76h:21m:21s remains)
INFO - root - 2017-12-05 13:59:31.032056: step 14260, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:48m:49s remains)
INFO - root - 2017-12-05 13:59:39.560880: step 14270, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 76h:42m:30s remains)
INFO - root - 2017-12-05 13:59:47.987270: step 14280, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 74h:55m:03s remains)
INFO - root - 2017-12-05 13:59:56.606007: step 14290, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 73h:00m:58s remains)
INFO - root - 2017-12-05 14:00:05.018325: step 14300, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 73h:58m:11s remains)
2017-12-05 14:00:05.815343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2382135 -4.2406297 -4.2468295 -4.2433991 -4.2450962 -4.2498116 -4.2475996 -4.2376485 -4.2295957 -4.2227588 -4.2251525 -4.2376084 -4.2542248 -4.2719541 -4.2886][-4.2242465 -4.2314339 -4.2382379 -4.232306 -4.229342 -4.2282619 -4.2184143 -4.2051182 -4.2000313 -4.200181 -4.2112012 -4.2301083 -4.2509151 -4.2709103 -4.2891316][-4.2028408 -4.2119823 -4.219275 -4.214386 -4.2092638 -4.1981821 -4.1776023 -4.1648245 -4.1688533 -4.1805396 -4.2015271 -4.2256875 -4.2494864 -4.2714529 -4.2898712][-4.1924186 -4.1947203 -4.2001367 -4.1976938 -4.1899848 -4.1654229 -4.1356225 -4.1315513 -4.1529326 -4.1749597 -4.2018127 -4.2298985 -4.2554188 -4.2738304 -4.2876911][-4.1922083 -4.1885614 -4.1939425 -4.1922917 -4.1774545 -4.1385231 -4.1013889 -4.1096716 -4.1501718 -4.1800737 -4.2088575 -4.2413244 -4.2679377 -4.28024 -4.2862473][-4.1777949 -4.1740203 -4.182313 -4.1736593 -4.1434488 -4.0894542 -4.0472469 -4.0701728 -4.1283021 -4.1701126 -4.2042255 -4.2419682 -4.26989 -4.2798352 -4.2843714][-4.141602 -4.1387715 -4.1481924 -4.1292963 -4.077857 -4.0001011 -3.9514914 -3.9962094 -4.0763583 -4.1400523 -4.191051 -4.2366381 -4.2648425 -4.2757049 -4.2825971][-4.0995154 -4.0960116 -4.1031446 -4.0750809 -4.00323 -3.9026821 -3.8562865 -3.9305108 -4.03635 -4.1198864 -4.1850953 -4.2400565 -4.2674232 -4.2767134 -4.2845049][-4.0954866 -4.0841289 -4.0882826 -4.0657015 -3.9951632 -3.898809 -3.8706455 -3.9572666 -4.058372 -4.1372013 -4.19644 -4.2471848 -4.2708116 -4.2801046 -4.2879386][-4.123847 -4.1105824 -4.1213837 -4.1166043 -4.0666142 -3.9963703 -3.9878397 -4.0516229 -4.1153541 -4.1694674 -4.2117319 -4.246985 -4.2662711 -4.2789292 -4.2866554][-4.1598105 -4.148901 -4.1663952 -4.1739826 -4.1444411 -4.1008759 -4.0984306 -4.1303396 -4.1551876 -4.184473 -4.2167482 -4.2429042 -4.2632051 -4.2794957 -4.2863531][-4.1994386 -4.1909523 -4.20748 -4.2134275 -4.194694 -4.1688848 -4.1633348 -4.1660781 -4.1657958 -4.184546 -4.2165217 -4.2373776 -4.2603416 -4.2804222 -4.2883453][-4.2408409 -4.2344933 -4.2458553 -4.2468586 -4.2318029 -4.2123656 -4.200839 -4.1824794 -4.1707368 -4.1884546 -4.2224727 -4.2419877 -4.2651615 -4.2881622 -4.2973208][-4.2751188 -4.2689533 -4.2734704 -4.2720633 -4.2624273 -4.2494206 -4.233572 -4.2054396 -4.1890392 -4.206522 -4.2382526 -4.2559175 -4.2764573 -4.2982912 -4.3084226][-4.29701 -4.292232 -4.2938323 -4.2946353 -4.2893333 -4.2792315 -4.2635174 -4.238802 -4.2250285 -4.2388854 -4.2643309 -4.2788682 -4.2955027 -4.3108063 -4.3172207]]...]
INFO - root - 2017-12-05 14:00:14.366484: step 14310, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 76h:39m:40s remains)
INFO - root - 2017-12-05 14:00:22.980132: step 14320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:54m:37s remains)
INFO - root - 2017-12-05 14:00:31.575722: step 14330, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 77h:37m:12s remains)
INFO - root - 2017-12-05 14:00:40.238954: step 14340, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 75h:58m:41s remains)
INFO - root - 2017-12-05 14:00:48.790594: step 14350, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 75h:19m:46s remains)
INFO - root - 2017-12-05 14:00:57.239509: step 14360, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 73h:35m:06s remains)
INFO - root - 2017-12-05 14:01:05.776962: step 14370, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 73h:43m:11s remains)
INFO - root - 2017-12-05 14:01:14.315854: step 14380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:03m:27s remains)
INFO - root - 2017-12-05 14:01:22.792139: step 14390, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:09m:14s remains)
INFO - root - 2017-12-05 14:01:31.248769: step 14400, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 74h:53m:24s remains)
2017-12-05 14:01:32.065015: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3128471 -4.2876887 -4.2626176 -4.2514648 -4.2487922 -4.2434587 -4.2458358 -4.2659907 -4.2867274 -4.288394 -4.2866478 -4.2865119 -4.2885919 -4.2867517 -4.2801681][-4.3130336 -4.2890115 -4.2639952 -4.2542148 -4.2470417 -4.231503 -4.2245364 -4.2438407 -4.2681208 -4.2760134 -4.2795768 -4.284059 -4.2870455 -4.2834482 -4.2737889][-4.313879 -4.2936859 -4.2700849 -4.2578788 -4.2454643 -4.2180243 -4.19542 -4.2054958 -4.2338095 -4.2532868 -4.2668118 -4.2779651 -4.2832661 -4.2798119 -4.2660832][-4.3040671 -4.2831697 -4.2577186 -4.2407 -4.2208567 -4.1840925 -4.149591 -4.1523962 -4.1869893 -4.2248726 -4.2512197 -4.2647762 -4.26784 -4.261476 -4.2432103][-4.2871094 -4.2604342 -4.2327228 -4.2146845 -4.1948643 -4.1571531 -4.1131563 -4.1058741 -4.1426458 -4.19684 -4.232151 -4.2430916 -4.2399273 -4.2285867 -4.2048497][-4.2755003 -4.2451897 -4.2166142 -4.2016425 -4.1870728 -4.1501513 -4.09321 -4.0661988 -4.0981984 -4.1698833 -4.2163734 -4.2257271 -4.2147441 -4.1976123 -4.1655889][-4.2649107 -4.2290425 -4.1950936 -4.1755915 -4.1575184 -4.1083269 -4.0278144 -3.9703567 -3.9985013 -4.0949335 -4.1668544 -4.1858945 -4.1752229 -4.1602764 -4.125658][-4.2512221 -4.2093096 -4.166265 -4.1339741 -4.0994644 -4.0270367 -3.9078205 -3.8032641 -3.824141 -3.9557824 -4.0649581 -4.1103935 -4.1164355 -4.1143527 -4.0867743][-4.2451539 -4.2029386 -4.1586037 -4.1224337 -4.0852842 -4.0102558 -3.8809414 -3.7543969 -3.7654459 -3.9006939 -4.0200348 -4.0783129 -4.0926485 -4.0941844 -4.068161][-4.2502365 -4.2113638 -4.171164 -4.1415243 -4.1177139 -4.0675392 -3.9745433 -3.8778059 -3.8869305 -3.9867125 -4.0773692 -4.1234117 -4.1335673 -4.1263394 -4.0913758][-4.2597408 -4.2241907 -4.1867595 -4.1599579 -4.1438413 -4.1117187 -4.0509963 -3.9855409 -3.9939654 -4.0619631 -4.1243176 -4.1581326 -4.1663022 -4.1565413 -4.1222758][-4.2667217 -4.2315245 -4.1929522 -4.1636214 -4.1452107 -4.118156 -4.0767455 -4.035337 -4.0456119 -4.0931034 -4.1365776 -4.162169 -4.1710544 -4.16594 -4.1405191][-4.2688832 -4.2306967 -4.1881866 -4.1545014 -4.1335907 -4.1131716 -4.092062 -4.076211 -4.0909314 -4.124917 -4.1535063 -4.1698875 -4.1751246 -4.1695504 -4.1498675][-4.2696753 -4.2298732 -4.1847754 -4.1470928 -4.1252384 -4.1129017 -4.1116967 -4.1181827 -4.1378694 -4.161768 -4.178175 -4.1839051 -4.1836681 -4.1771159 -4.1626039][-4.2758951 -4.2382431 -4.1963196 -4.1599464 -4.1391392 -4.132936 -4.1454821 -4.1652441 -4.1846237 -4.1998539 -4.2063012 -4.2053533 -4.2021241 -4.1956229 -4.1840978]]...]
INFO - root - 2017-12-05 14:01:40.587957: step 14410, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 74h:36m:52s remains)
INFO - root - 2017-12-05 14:01:49.094929: step 14420, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 73h:56m:56s remains)
INFO - root - 2017-12-05 14:01:57.606491: step 14430, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:14m:16s remains)
INFO - root - 2017-12-05 14:02:06.177055: step 14440, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 72h:16m:31s remains)
INFO - root - 2017-12-05 14:02:14.706782: step 14450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 74h:04m:42s remains)
INFO - root - 2017-12-05 14:02:23.214920: step 14460, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.773 sec/batch; 68h:19m:25s remains)
INFO - root - 2017-12-05 14:02:31.826978: step 14470, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 74h:56m:39s remains)
INFO - root - 2017-12-05 14:02:40.380597: step 14480, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 75h:57m:39s remains)
INFO - root - 2017-12-05 14:02:49.000453: step 14490, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 78h:43m:42s remains)
INFO - root - 2017-12-05 14:02:57.391145: step 14500, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:26m:57s remains)
2017-12-05 14:02:58.076895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2105284 -4.2130361 -4.19653 -4.1563406 -4.1087503 -4.0681281 -4.0616274 -4.0883369 -4.1215124 -4.1438494 -4.1552868 -4.1739974 -4.1769452 -4.1510077 -4.130713][-4.2393651 -4.2341065 -4.212934 -4.172718 -4.1215506 -4.0678258 -4.0438142 -4.061584 -4.0914354 -4.1107163 -4.1221027 -4.1464 -4.1551809 -4.133172 -4.1143656][-4.2558951 -4.2434974 -4.2230124 -4.1900887 -4.137805 -4.0709248 -4.0274682 -4.0297027 -4.0544329 -4.07119 -4.0844865 -4.1181812 -4.1373906 -4.1211848 -4.1045742][-4.2517028 -4.2362313 -4.2190914 -4.1921024 -4.1448503 -4.0731993 -4.010407 -3.9940913 -4.0160384 -4.0366383 -4.0548663 -4.0984726 -4.1293235 -4.1211753 -4.1083927][-4.2372775 -4.2232995 -4.2097573 -4.1878948 -4.1498303 -4.0744119 -3.994559 -3.9659276 -3.9884605 -4.0143518 -4.0352917 -4.0810213 -4.1213422 -4.1269031 -4.1238055][-4.2245517 -4.2109346 -4.1979427 -4.1791663 -4.1501927 -4.0711422 -3.9750066 -3.9390118 -3.9751258 -4.0110412 -4.0263524 -4.0627837 -4.1109915 -4.1296082 -4.1360226][-4.2049308 -4.1924381 -4.1773696 -4.1576262 -4.1344061 -4.0584617 -3.95008 -3.9132628 -3.975358 -4.0258875 -4.0254235 -4.040328 -4.0842719 -4.1085815 -4.1201725][-4.1808977 -4.1705947 -4.1541233 -4.1337252 -4.1136093 -4.0442653 -3.927141 -3.8893344 -3.9744241 -4.0399251 -4.0293293 -4.0206652 -4.0495262 -4.0747819 -4.0930519][-4.1663761 -4.1648841 -4.1535287 -4.135251 -4.1146917 -4.0517211 -3.9304864 -3.8842249 -3.9712703 -4.040556 -4.0281978 -4.0058012 -4.0228515 -4.0480995 -4.0762057][-4.1743274 -4.1788 -4.172225 -4.1571937 -4.1361418 -4.0816264 -3.9696894 -3.9135332 -3.9809108 -4.0385575 -4.0290437 -4.0041776 -4.017684 -4.0434432 -4.079113][-4.188611 -4.1908722 -4.1815605 -4.1699114 -4.1528163 -4.1097817 -4.0173087 -3.960175 -4.0013189 -4.0412869 -4.0313935 -4.0107117 -4.02815 -4.0544353 -4.0916905][-4.2034335 -4.199451 -4.1832757 -4.1716433 -4.1595411 -4.1288838 -4.0582891 -4.0059085 -4.0261464 -4.0529537 -4.0441909 -4.0283113 -4.047626 -4.0715895 -4.10372][-4.2200289 -4.2070746 -4.1816225 -4.16574 -4.1579027 -4.1399827 -4.0909448 -4.0506086 -4.0576596 -4.0746856 -4.0684547 -4.0584731 -4.0770135 -4.0967245 -4.1244783][-4.2389259 -4.2193556 -4.1876645 -4.1662111 -4.1597705 -4.1523972 -4.1242356 -4.0979981 -4.09642 -4.1042514 -4.0956931 -4.0844588 -4.1005363 -4.1197104 -4.1473379][-4.2574954 -4.2378221 -4.20603 -4.1798658 -4.1693974 -4.1648879 -4.1500568 -4.1328249 -4.1267047 -4.1245556 -4.1110067 -4.0950241 -4.1066036 -4.1296268 -4.1579194]]...]
INFO - root - 2017-12-05 14:03:06.655353: step 14510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 74h:47m:41s remains)
INFO - root - 2017-12-05 14:03:15.175282: step 14520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 75h:43m:36s remains)
INFO - root - 2017-12-05 14:03:23.786929: step 14530, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 74h:52m:16s remains)
INFO - root - 2017-12-05 14:03:32.330103: step 14540, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 73h:53m:45s remains)
INFO - root - 2017-12-05 14:03:40.931066: step 14550, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.821 sec/batch; 72h:32m:54s remains)
INFO - root - 2017-12-05 14:03:49.460237: step 14560, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 75h:45m:22s remains)
INFO - root - 2017-12-05 14:03:57.871938: step 14570, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 74h:16m:32s remains)
INFO - root - 2017-12-05 14:04:06.392726: step 14580, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 73h:26m:29s remains)
INFO - root - 2017-12-05 14:04:15.010620: step 14590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:23m:40s remains)
INFO - root - 2017-12-05 14:04:23.441031: step 14600, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 77h:37m:02s remains)
2017-12-05 14:04:24.258391: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1785579 -4.2112045 -4.2335849 -4.221282 -4.185668 -4.1397781 -4.0995979 -4.0790868 -4.08713 -4.1159506 -4.1413655 -4.1519151 -4.1601119 -4.1659908 -4.18044][-4.2077923 -4.2338195 -4.2491455 -4.2352476 -4.2011423 -4.1560769 -4.1197796 -4.0929956 -4.0772552 -4.0811133 -4.09558 -4.1024613 -4.112103 -4.13059 -4.1589627][-4.2369914 -4.2543583 -4.2578664 -4.2373724 -4.2000351 -4.1521373 -4.1138372 -4.0867634 -4.0670762 -4.063344 -4.0720286 -4.076869 -4.0888324 -4.1185994 -4.1528788][-4.25564 -4.2667217 -4.2596059 -4.2292886 -4.1807351 -4.1229329 -4.0814834 -4.0642443 -4.0686212 -4.0789952 -4.0876107 -4.0900764 -4.1023049 -4.1314344 -4.1564236][-4.2639422 -4.2707896 -4.2539811 -4.2080407 -4.1426158 -4.0746226 -4.0318451 -4.0260978 -4.06816 -4.1123424 -4.1301556 -4.134913 -4.1479411 -4.1666789 -4.17798][-4.2690377 -4.2709346 -4.2457809 -4.1780295 -4.088841 -4.0021253 -3.9488568 -3.9472148 -4.0249081 -4.1120272 -4.1511388 -4.1701956 -4.1927686 -4.2073135 -4.2121058][-4.2717304 -4.2662067 -4.2335014 -4.1466236 -4.0329576 -3.9173267 -3.8360128 -3.8164778 -3.913507 -4.0412879 -4.112133 -4.1593804 -4.2042251 -4.2315893 -4.2428689][-4.2564149 -4.2455735 -4.2090859 -4.1194 -3.999969 -3.8731468 -3.772083 -3.733376 -3.8262103 -3.9717736 -4.0689125 -4.1408916 -4.2032919 -4.2445087 -4.2678375][-4.217896 -4.2155514 -4.19513 -4.1321206 -4.0426517 -3.9458621 -3.865464 -3.831085 -3.8971386 -4.0124192 -4.0975585 -4.1643553 -4.221982 -4.2612977 -4.2851667][-4.1626048 -4.1834579 -4.1941485 -4.1705828 -4.123415 -4.0705619 -4.0253 -4.0044794 -4.0403576 -4.1111197 -4.1673832 -4.2114367 -4.2503586 -4.2787867 -4.2943482][-4.1113248 -4.1588292 -4.1971407 -4.2047334 -4.194314 -4.1772919 -4.158443 -4.1498446 -4.169054 -4.2082534 -4.2407265 -4.2626562 -4.2815928 -4.2964215 -4.30325][-4.0844522 -4.14979 -4.205956 -4.2340412 -4.2463408 -4.251081 -4.2468176 -4.242476 -4.25017 -4.2701621 -4.2868147 -4.2951465 -4.3008065 -4.3051429 -4.3057208][-4.1073489 -4.1724138 -4.2295961 -4.2619147 -4.2840343 -4.29686 -4.2971678 -4.2935424 -4.2954812 -4.3054013 -4.3141537 -4.3169103 -4.3168406 -4.3162465 -4.3130875][-4.1771317 -4.2250361 -4.2665806 -4.2902441 -4.3086371 -4.3200903 -4.3210506 -4.3188448 -4.3196092 -4.3242049 -4.3277726 -4.3291368 -4.3282123 -4.3262734 -4.322278][-4.2446136 -4.2746592 -4.2966719 -4.3068295 -4.3147039 -4.3186913 -4.31816 -4.3174109 -4.3184385 -4.320796 -4.3226624 -4.3242168 -4.3245587 -4.3235006 -4.3205347]]...]
INFO - root - 2017-12-05 14:04:32.741529: step 14610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 75h:39m:04s remains)
INFO - root - 2017-12-05 14:04:41.293326: step 14620, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 74h:48m:32s remains)
INFO - root - 2017-12-05 14:04:49.886788: step 14630, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 72h:58m:23s remains)
INFO - root - 2017-12-05 14:04:58.465183: step 14640, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 76h:06m:11s remains)
INFO - root - 2017-12-05 14:05:07.028548: step 14650, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 76h:06m:10s remains)
INFO - root - 2017-12-05 14:05:15.654847: step 14660, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 76h:29m:38s remains)
INFO - root - 2017-12-05 14:05:24.247670: step 14670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 76h:01m:33s remains)
INFO - root - 2017-12-05 14:05:32.680521: step 14680, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 73h:35m:03s remains)
INFO - root - 2017-12-05 14:05:41.196545: step 14690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 76h:50m:56s remains)
INFO - root - 2017-12-05 14:05:49.665521: step 14700, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 77h:40m:50s remains)
2017-12-05 14:05:50.396693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3137169 -4.3054767 -4.2943544 -4.2879429 -4.2867937 -4.28969 -4.2894831 -4.2899504 -4.2955303 -4.3021717 -4.3038507 -4.2993808 -4.2931328 -4.2873392 -4.2841625][-4.3114157 -4.2948494 -4.2763472 -4.2646856 -4.2581162 -4.25303 -4.2418728 -4.2377372 -4.2454748 -4.2545681 -4.2616768 -4.268085 -4.2734065 -4.27713 -4.2807403][-4.3073659 -4.2905474 -4.2725034 -4.2595258 -4.2480903 -4.2330804 -4.2068748 -4.192718 -4.1984258 -4.2102962 -4.2269192 -4.244956 -4.2604527 -4.2722669 -4.2815986][-4.2938695 -4.2821741 -4.26573 -4.2513494 -4.2336864 -4.205215 -4.1621847 -4.1355877 -4.1387863 -4.1601286 -4.1967344 -4.2289476 -4.2533741 -4.2715135 -4.2844625][-4.2711182 -4.2609673 -4.2404127 -4.2202759 -4.1917439 -4.1451197 -4.0805864 -4.0376711 -4.0362439 -4.0762868 -4.1432185 -4.1949711 -4.2345357 -4.2644787 -4.2836666][-4.2525725 -4.2379541 -4.2083974 -4.1770496 -4.1356173 -4.0700312 -3.9840217 -3.9199705 -3.9127233 -3.9780941 -4.0788465 -4.1526947 -4.2090597 -4.2523918 -4.2784796][-4.25151 -4.2339091 -4.1981087 -4.1567106 -4.108849 -4.03804 -3.9429152 -3.8681161 -3.8633826 -3.9493144 -4.0661 -4.1476712 -4.2092905 -4.2559071 -4.28058][-4.2628007 -4.2478685 -4.2127476 -4.1714826 -4.129571 -4.0706286 -3.9900351 -3.9323823 -3.9468017 -4.0314913 -4.1277885 -4.19534 -4.2454543 -4.281939 -4.29568][-4.2659454 -4.2614884 -4.2365642 -4.2018147 -4.1673117 -4.1237111 -4.0698977 -4.040854 -4.0707736 -4.1409888 -4.2073555 -4.2557683 -4.2898107 -4.3120513 -4.3161678][-4.2477856 -4.2558956 -4.2433653 -4.217454 -4.1905403 -4.1610808 -4.1320214 -4.1250677 -4.1597519 -4.2138934 -4.2596931 -4.29579 -4.3180542 -4.3295665 -4.3291979][-4.2151227 -4.2365651 -4.2348323 -4.2217507 -4.2083521 -4.1947856 -4.1830239 -4.1894145 -4.2221193 -4.2612457 -4.2906041 -4.3145051 -4.325192 -4.3293042 -4.3297386][-4.1930542 -4.2253327 -4.2322445 -4.2317152 -4.2314992 -4.226275 -4.2184596 -4.2248383 -4.249042 -4.2767825 -4.2986054 -4.3128982 -4.3146324 -4.31601 -4.3207169][-4.2006717 -4.235774 -4.2486215 -4.2556314 -4.2605643 -4.2553806 -4.2414107 -4.2392678 -4.2534118 -4.2729268 -4.2910089 -4.3015637 -4.2998066 -4.3022561 -4.311902][-4.2211843 -4.2553558 -4.2723632 -4.2833991 -4.2871242 -4.2784123 -4.2569675 -4.2470694 -4.2549553 -4.2687917 -4.2845907 -4.2938304 -4.291966 -4.2945452 -4.3054352][-4.252943 -4.2807183 -4.2929559 -4.2988038 -4.2957053 -4.2832251 -4.2599525 -4.2484503 -4.2546291 -4.2666173 -4.2804661 -4.2889028 -4.2882824 -4.2905855 -4.3019805]]...]
INFO - root - 2017-12-05 14:05:58.941062: step 14710, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 77h:41m:03s remains)
INFO - root - 2017-12-05 14:06:07.489602: step 14720, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 75h:17m:53s remains)
INFO - root - 2017-12-05 14:06:16.029911: step 14730, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 74h:16m:02s remains)
INFO - root - 2017-12-05 14:06:24.648579: step 14740, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 78h:06m:16s remains)
INFO - root - 2017-12-05 14:06:33.095986: step 14750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 76h:30m:50s remains)
INFO - root - 2017-12-05 14:06:41.834729: step 14760, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 76h:59m:55s remains)
INFO - root - 2017-12-05 14:06:50.463046: step 14770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 75h:22m:28s remains)
INFO - root - 2017-12-05 14:06:58.880407: step 14780, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.812 sec/batch; 71h:37m:13s remains)
INFO - root - 2017-12-05 14:07:07.434240: step 14790, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:46m:35s remains)
INFO - root - 2017-12-05 14:07:15.778950: step 14800, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 75h:12m:18s remains)
2017-12-05 14:07:16.501567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1444426 -4.2175293 -4.2645459 -4.2886672 -4.2970157 -4.298913 -4.3020597 -4.3070364 -4.3169093 -4.3244467 -4.329679 -4.3376493 -4.3438964 -4.34625 -4.3472762][-4.1464267 -4.2162066 -4.2653 -4.290699 -4.3004284 -4.3029752 -4.3080363 -4.318954 -4.3358989 -4.3476982 -4.3529444 -4.3579407 -4.3622923 -4.3616018 -4.3555384][-4.1739249 -4.2303858 -4.27131 -4.2890706 -4.2920132 -4.2922649 -4.2982068 -4.3126431 -4.3324571 -4.3436475 -4.3480406 -4.3522525 -4.3568325 -4.3554587 -4.3464932][-4.1998806 -4.2352977 -4.2594757 -4.2618723 -4.25326 -4.2502532 -4.2543349 -4.2688074 -4.2927337 -4.308804 -4.3215542 -4.3360004 -4.3482914 -4.3499465 -4.3386993][-4.1915183 -4.2035537 -4.2044406 -4.1872029 -4.1655712 -4.1583815 -4.15983 -4.1720333 -4.2049537 -4.234889 -4.2668152 -4.3022432 -4.3288589 -4.3395042 -4.3308048][-4.1524692 -4.1511045 -4.133656 -4.0979218 -4.0608854 -4.0414405 -4.0323563 -4.0364833 -4.0792303 -4.1289678 -4.1866589 -4.2477126 -4.2940216 -4.3216124 -4.3260908][-4.1104107 -4.1032953 -4.0741625 -4.0270896 -3.9772897 -3.9334033 -3.8886695 -3.8699603 -3.9267015 -4.0081029 -4.0973883 -4.1824689 -4.2457385 -4.2929468 -4.3176632][-4.1059365 -4.0990429 -4.063139 -4.007431 -3.9508364 -3.8833582 -3.7981994 -3.7490625 -3.8164783 -3.920033 -4.0254917 -4.1189122 -4.187335 -4.2460961 -4.2878189][-4.1432948 -4.1439304 -4.1194105 -4.0715952 -4.0169091 -3.9479494 -3.8606091 -3.8055804 -3.8521323 -3.9280207 -4.0057912 -4.0789676 -4.1382885 -4.1983237 -4.2471919][-4.2064586 -4.2135348 -4.2028341 -4.1669569 -4.1165333 -4.0554628 -3.9888015 -3.9487505 -3.9669647 -3.997911 -4.0285993 -4.0649776 -4.103653 -4.1572366 -4.2069044][-4.2828417 -4.2925897 -4.2835951 -4.2474561 -4.1970696 -4.1438107 -4.0967226 -4.069941 -4.0760307 -4.0814519 -4.0830445 -4.0884228 -4.1019578 -4.1380153 -4.1768751][-4.3350029 -4.346261 -4.3370042 -4.3032694 -4.2593994 -4.2202959 -4.1893058 -4.172493 -4.1785264 -4.178967 -4.1742368 -4.1657906 -4.1577487 -4.169662 -4.1829925][-4.3603134 -4.3722548 -4.3648934 -4.3383884 -4.3030219 -4.2762203 -4.2578459 -4.2521911 -4.2661858 -4.27303 -4.2747016 -4.2674303 -4.2528539 -4.2431183 -4.2299037][-4.3662395 -4.3771849 -4.3723521 -4.353919 -4.3269138 -4.3091774 -4.2982759 -4.3010521 -4.3218975 -4.3373284 -4.34776 -4.3484821 -4.3377385 -4.3187842 -4.2916055][-4.3609962 -4.3703732 -4.3675594 -4.3535628 -4.3300457 -4.3099208 -4.2954731 -4.2989836 -4.3242211 -4.3512974 -4.3738861 -4.38546 -4.3834429 -4.3666906 -4.3397412]]...]
INFO - root - 2017-12-05 14:07:25.121432: step 14810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 75h:07m:11s remains)
INFO - root - 2017-12-05 14:07:33.628131: step 14820, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 76h:08m:39s remains)
INFO - root - 2017-12-05 14:07:42.076384: step 14830, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 76h:16m:19s remains)
INFO - root - 2017-12-05 14:07:50.691603: step 14840, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 77h:52m:15s remains)
INFO - root - 2017-12-05 14:07:59.251362: step 14850, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 74h:16m:30s remains)
INFO - root - 2017-12-05 14:08:07.782191: step 14860, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 78h:53m:50s remains)
INFO - root - 2017-12-05 14:08:16.322925: step 14870, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 76h:02m:06s remains)
INFO - root - 2017-12-05 14:08:24.947988: step 14880, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.903 sec/batch; 79h:41m:40s remains)
INFO - root - 2017-12-05 14:08:33.264993: step 14890, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 78h:31m:00s remains)
INFO - root - 2017-12-05 14:08:41.807775: step 14900, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 75h:51m:34s remains)
2017-12-05 14:08:42.702205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2432723 -4.2137165 -4.1558557 -4.1127777 -4.1184645 -4.1595807 -4.2019978 -4.2066741 -4.1657381 -4.1336985 -4.1425695 -4.1605992 -4.1651773 -4.1608639 -4.1704707][-4.2337222 -4.19752 -4.1373687 -4.1016107 -4.1290755 -4.1822968 -4.22296 -4.2206154 -4.1713414 -4.1344576 -4.1409459 -4.1618528 -4.171226 -4.1680484 -4.1738153][-4.209866 -4.1639242 -4.1050186 -4.0844741 -4.132237 -4.1862803 -4.2184982 -4.2135253 -4.1781921 -4.1566749 -4.1650043 -4.1805205 -4.1886759 -4.1860962 -4.1866374][-4.1878514 -4.1368809 -4.0855541 -4.0821986 -4.1332154 -4.1726131 -4.1888285 -4.1858292 -4.1747017 -4.1796565 -4.2016683 -4.213408 -4.2128377 -4.2089648 -4.2057242][-4.1713657 -4.12772 -4.089757 -4.0899777 -4.1149077 -4.1191473 -4.1163058 -4.1241922 -4.1509576 -4.189918 -4.2278652 -4.2372956 -4.22948 -4.2236738 -4.218411][-4.1806979 -4.1495318 -4.1186552 -4.1014848 -4.0827665 -4.0331116 -4.0008922 -4.0253954 -4.1001391 -4.1804075 -4.2334146 -4.2410269 -4.2353592 -4.2336779 -4.228332][-4.2104592 -4.1916642 -4.160223 -4.1238561 -4.0620861 -3.9587517 -3.8836427 -3.9198909 -4.0420027 -4.1570625 -4.2208219 -4.2304859 -4.2336059 -4.24399 -4.24699][-4.2327332 -4.2236729 -4.198092 -4.1586022 -4.0837069 -3.9570732 -3.8447611 -3.8648636 -4.0013709 -4.1266613 -4.1930704 -4.2104459 -4.2233562 -4.2428946 -4.2578454][-4.2385321 -4.23898 -4.2244606 -4.1954255 -4.1342106 -4.0239339 -3.9026909 -3.8801308 -3.9793658 -4.0940857 -4.1635261 -4.1876264 -4.2070336 -4.2294893 -4.2495537][-4.2368255 -4.2415371 -4.233655 -4.2119489 -4.1705914 -4.0926905 -3.990963 -3.9367225 -3.9813528 -4.0709734 -4.142395 -4.1724095 -4.192678 -4.2148738 -4.2335377][-4.2315173 -4.2376709 -4.2339735 -4.2151718 -4.1888871 -4.141552 -4.0704341 -4.0114183 -4.0149379 -4.073771 -4.1398668 -4.1716428 -4.1861567 -4.2022095 -4.2154112][-4.2180219 -4.219727 -4.2226782 -4.2134132 -4.2010942 -4.1781578 -4.1346312 -4.08638 -4.0718489 -4.1013212 -4.1524992 -4.1803055 -4.1884122 -4.19794 -4.20694][-4.2026558 -4.1988091 -4.2011108 -4.2014289 -4.1981416 -4.1911564 -4.1729631 -4.1414795 -4.1246734 -4.1388474 -4.1762519 -4.195972 -4.198565 -4.2016449 -4.2030568][-4.1934094 -4.1837692 -4.1749177 -4.1745706 -4.1744618 -4.1756968 -4.1762314 -4.1636171 -4.1542044 -4.1677833 -4.1977739 -4.2124043 -4.2081971 -4.2017074 -4.1944151][-4.1915464 -4.18067 -4.1584349 -4.1452241 -4.1378312 -4.1409621 -4.1508374 -4.1494575 -4.14989 -4.1694441 -4.2011008 -4.2173734 -4.2119522 -4.1969008 -4.17915]]...]
INFO - root - 2017-12-05 14:08:51.269346: step 14910, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 75h:42m:40s remains)
INFO - root - 2017-12-05 14:08:59.977070: step 14920, loss = 2.02, batch loss = 1.96 (9.0 examples/sec; 0.890 sec/batch; 78h:29m:03s remains)
INFO - root - 2017-12-05 14:09:08.707253: step 14930, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 74h:25m:52s remains)
INFO - root - 2017-12-05 14:09:17.177942: step 14940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 76h:07m:42s remains)
INFO - root - 2017-12-05 14:09:25.710145: step 14950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 76h:55m:28s remains)
INFO - root - 2017-12-05 14:09:34.269947: step 14960, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 76h:23m:34s remains)
INFO - root - 2017-12-05 14:09:42.834601: step 14970, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 75h:07m:29s remains)
INFO - root - 2017-12-05 14:09:51.365247: step 14980, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 77h:55m:24s remains)
INFO - root - 2017-12-05 14:09:59.863829: step 14990, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 73h:30m:20s remains)
INFO - root - 2017-12-05 14:10:08.344249: step 15000, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 73h:16m:49s remains)
2017-12-05 14:10:09.113515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2403808 -4.2427044 -4.2394648 -4.2330132 -4.2241225 -4.2178817 -4.2145672 -4.2144437 -4.2190785 -4.2240052 -4.2176037 -4.20575 -4.2014446 -4.2060122 -4.2126341][-4.244895 -4.2493072 -4.2488518 -4.2453065 -4.2389259 -4.2340612 -4.2305059 -4.2278237 -4.2295227 -4.2299914 -4.2157121 -4.195097 -4.1876216 -4.1960187 -4.2081552][-4.2472363 -4.2521811 -4.2531037 -4.2526312 -4.2492361 -4.2473321 -4.2444086 -4.2425165 -4.2448139 -4.2434077 -4.2252083 -4.1990647 -4.1871481 -4.1940475 -4.2059364][-4.234674 -4.2372117 -4.2371993 -4.2369823 -4.2353435 -4.2368212 -4.2382421 -4.2415051 -4.2469392 -4.2484136 -4.2350907 -4.2123489 -4.198123 -4.2004447 -4.2092609][-4.2176366 -4.2159514 -4.2129025 -4.2088318 -4.2018576 -4.2018814 -4.2057734 -4.2128215 -4.2237144 -4.2320886 -4.2290826 -4.2157664 -4.2063618 -4.2082882 -4.2172227][-4.1979537 -4.1888003 -4.1783948 -4.1659479 -4.1496072 -4.1407943 -4.1427507 -4.1568551 -4.1813707 -4.2042217 -4.215075 -4.2124085 -4.2082005 -4.2129903 -4.2218084][-4.1891665 -4.1733084 -4.1545219 -4.1312022 -4.0981436 -4.063798 -4.0440288 -4.0534725 -4.0931764 -4.1363821 -4.1653342 -4.1769772 -4.1792426 -4.1864047 -4.1981597][-4.1771622 -4.1575408 -4.1318874 -4.0986853 -4.0467572 -3.9745507 -3.9133615 -3.9080417 -3.9676623 -4.0392003 -4.0914745 -4.1201448 -4.1298923 -4.1404753 -4.15697][-4.1640735 -4.145236 -4.1175575 -4.0818844 -4.0215635 -3.9294937 -3.8452582 -3.8342066 -3.9089382 -3.9979019 -4.0635672 -4.0968437 -4.1020846 -4.1071105 -4.120851][-4.1714468 -4.1587558 -4.1412024 -4.1212611 -4.0786076 -4.0073004 -3.9438374 -3.9355807 -3.9871211 -4.0552645 -4.1095271 -4.1330552 -4.1235414 -4.1148453 -4.1193871][-4.1932158 -4.1835384 -4.1756458 -4.1723981 -4.1569662 -4.1214995 -4.0858297 -4.0764575 -4.0990186 -4.1394806 -4.17598 -4.190093 -4.1764693 -4.1610689 -4.15866][-4.2188148 -4.2090383 -4.2053504 -4.2114215 -4.2144213 -4.2051454 -4.187964 -4.1796732 -4.1869521 -4.2067132 -4.2276387 -4.2348232 -4.2240071 -4.2104368 -4.2068167][-4.2333689 -4.2263684 -4.2254782 -4.2353354 -4.2453308 -4.247509 -4.24288 -4.2395687 -4.2419333 -4.2498384 -4.2573948 -4.2588711 -4.2516041 -4.2436595 -4.2418356][-4.2218657 -4.2168865 -4.2187257 -4.2311516 -4.2439976 -4.2517862 -4.2543144 -4.2557206 -4.2577934 -4.2603703 -4.2627273 -4.2642927 -4.2626395 -4.2578521 -4.2538352][-4.1804838 -4.1817183 -4.1911678 -4.2102957 -4.2291117 -4.2406373 -4.2448716 -4.2460003 -4.2452173 -4.2463803 -4.2499294 -4.2557788 -4.2600536 -4.2584257 -4.25362]]...]
INFO - root - 2017-12-05 14:10:17.744623: step 15010, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 76h:28m:58s remains)
INFO - root - 2017-12-05 14:10:26.329785: step 15020, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 73h:41m:18s remains)
INFO - root - 2017-12-05 14:10:34.813171: step 15030, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 75h:21m:12s remains)
INFO - root - 2017-12-05 14:10:43.314129: step 15040, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 72h:15m:01s remains)
INFO - root - 2017-12-05 14:10:51.917316: step 15050, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 75h:31m:27s remains)
INFO - root - 2017-12-05 14:11:00.476950: step 15060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 75h:36m:02s remains)
INFO - root - 2017-12-05 14:11:08.948055: step 15070, loss = 2.12, batch loss = 2.06 (9.7 examples/sec; 0.826 sec/batch; 72h:49m:40s remains)
INFO - root - 2017-12-05 14:11:17.516273: step 15080, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 76h:50m:45s remains)
INFO - root - 2017-12-05 14:11:26.124199: step 15090, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 77h:29m:58s remains)
INFO - root - 2017-12-05 14:11:34.588364: step 15100, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 76h:36m:39s remains)
2017-12-05 14:11:35.356901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3244562 -4.30523 -4.2802677 -4.2558584 -4.2330837 -4.2123189 -4.1981645 -4.2023277 -4.22946 -4.2603822 -4.2768459 -4.2862296 -4.2918639 -4.28656 -4.2756295][-4.3047729 -4.2795486 -4.2514687 -4.2250624 -4.1933265 -4.1603818 -4.1350393 -4.1404271 -4.1802163 -4.2254639 -4.2483506 -4.2570148 -4.2578659 -4.2477188 -4.233634][-4.2841096 -4.2548971 -4.2256441 -4.2000709 -4.1599135 -4.107204 -4.0679379 -4.080915 -4.1382155 -4.1984725 -4.2328081 -4.2458344 -4.2421303 -4.2253795 -4.2072072][-4.2595797 -4.2256622 -4.1935945 -4.1690545 -4.1234989 -4.0525889 -3.9969623 -4.0210409 -4.0969653 -4.1691108 -4.2166705 -4.2393684 -4.2388897 -4.2200055 -4.1989541][-4.2305722 -4.1921158 -4.1542282 -4.1253552 -4.0766978 -3.9882195 -3.9059358 -3.9297893 -4.025599 -4.1175709 -4.1823974 -4.2208991 -4.2319794 -4.221734 -4.203752][-4.2061253 -4.166069 -4.1251307 -4.0891919 -4.0337124 -3.9279077 -3.8157969 -3.8261392 -3.940238 -4.057735 -4.1421294 -4.1982956 -4.2206545 -4.2228074 -4.214314][-4.1917214 -4.153389 -4.1130877 -4.069787 -4.002903 -3.8825824 -3.7500906 -3.7409973 -3.8584132 -3.9906516 -4.0917716 -4.1634321 -4.1986079 -4.2136722 -4.2194257][-4.1865911 -4.1531191 -4.1209879 -4.0786691 -4.0051351 -3.8844066 -3.7613378 -3.7407997 -3.8292933 -3.9470453 -4.0473528 -4.1253181 -4.1716127 -4.2001352 -4.2183161][-4.1910605 -4.16646 -4.1481247 -4.1181846 -4.058908 -3.9652345 -3.8787498 -3.8613608 -3.9044216 -3.9809542 -4.0587697 -4.1273036 -4.1719851 -4.2054095 -4.2265635][-4.2024217 -4.188159 -4.1793175 -4.1605458 -4.1170945 -4.0516443 -3.9972494 -3.9888365 -4.0086579 -4.0534825 -4.1098356 -4.1645527 -4.2027483 -4.2326303 -4.2491145][-4.2193265 -4.2131257 -4.2090235 -4.1971345 -4.1613355 -4.1095 -4.069994 -4.0709472 -4.0875659 -4.1203208 -4.1668081 -4.2137074 -4.2472897 -4.26935 -4.2776227][-4.2367616 -4.234633 -4.2336259 -4.230299 -4.2010612 -4.1529222 -4.1236658 -4.1339517 -4.15288 -4.1758027 -4.2152777 -4.2575746 -4.2889266 -4.3048749 -4.3063636][-4.2470407 -4.2450337 -4.2453637 -4.2447171 -4.2211022 -4.1805525 -4.1624632 -4.1782455 -4.1971269 -4.2121758 -4.2454967 -4.2870235 -4.3166881 -4.32819 -4.3257709][-4.2523775 -4.2487383 -4.2477145 -4.2478828 -4.231081 -4.2037239 -4.1943846 -4.2098317 -4.2264261 -4.2400861 -4.2679963 -4.3026352 -4.3257656 -4.3318844 -4.32848][-4.2659359 -4.2587681 -4.2548623 -4.2550383 -4.2464519 -4.230484 -4.2236314 -4.2330217 -4.247292 -4.2620177 -4.2858853 -4.3090582 -4.3221993 -4.3240833 -4.3217125]]...]
INFO - root - 2017-12-05 14:11:43.898041: step 15110, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 76h:13m:47s remains)
INFO - root - 2017-12-05 14:11:52.475625: step 15120, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.834 sec/batch; 73h:31m:56s remains)
INFO - root - 2017-12-05 14:12:00.977710: step 15130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 74h:32m:39s remains)
INFO - root - 2017-12-05 14:12:09.572702: step 15140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:15m:28s remains)
INFO - root - 2017-12-05 14:12:18.196040: step 15150, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 74h:12m:35s remains)
INFO - root - 2017-12-05 14:12:26.721880: step 15160, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:15m:38s remains)
INFO - root - 2017-12-05 14:12:35.200373: step 15170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:44m:51s remains)
INFO - root - 2017-12-05 14:12:43.646863: step 15180, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 75h:39m:36s remains)
INFO - root - 2017-12-05 14:12:52.069105: step 15190, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 73h:20m:53s remains)
INFO - root - 2017-12-05 14:13:00.482968: step 15200, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 76h:19m:40s remains)
2017-12-05 14:13:01.229187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2988844 -4.2758541 -4.2418289 -4.2239914 -4.2171288 -4.2155156 -4.2137284 -4.2094707 -4.2060127 -4.2154174 -4.2306509 -4.2484083 -4.2711082 -4.2929721 -4.3033247][-4.2996192 -4.2781196 -4.2479386 -4.2319579 -4.2234225 -4.218359 -4.2137971 -4.2100377 -4.2082477 -4.2210493 -4.238595 -4.2546082 -4.2705274 -4.2883735 -4.2981386][-4.2827106 -4.2735248 -4.2507463 -4.233696 -4.2245064 -4.2162013 -4.2079973 -4.2004476 -4.1986852 -4.2102466 -4.2274022 -4.2414069 -4.2564683 -4.2764792 -4.29098][-4.2576065 -4.2624321 -4.2486544 -4.2316141 -4.2197614 -4.2069821 -4.1914363 -4.1781993 -4.1737671 -4.182292 -4.1958475 -4.2096395 -4.2335935 -4.26273 -4.2853661][-4.2264009 -4.2400489 -4.2351213 -4.2207346 -4.2073221 -4.190331 -4.1663976 -4.1455593 -4.1307907 -4.1309667 -4.1378622 -4.1601024 -4.2046971 -4.2513061 -4.2829041][-4.1979179 -4.2154794 -4.2203512 -4.2123184 -4.20101 -4.1784997 -4.144196 -4.1106925 -4.0819974 -4.072238 -4.0784879 -4.1157045 -4.1852469 -4.2498207 -4.28613][-4.1735482 -4.1913147 -4.2028775 -4.2022152 -4.1968527 -4.1753039 -4.1358008 -4.0898485 -4.0448971 -4.0300622 -4.0445018 -4.1030788 -4.1888 -4.2589989 -4.2926779][-4.1493282 -4.17052 -4.1882486 -4.1932526 -4.1933889 -4.17255 -4.1287718 -4.0731945 -4.0216331 -4.0082469 -4.0392337 -4.117146 -4.206924 -4.2716851 -4.2989988][-4.1346331 -4.161665 -4.1845608 -4.1930046 -4.1899219 -4.1646366 -4.1134496 -4.0551581 -4.00999 -4.0110717 -4.0624886 -4.1480565 -4.228745 -4.2830911 -4.3046546][-4.150516 -4.1795378 -4.2021794 -4.2038941 -4.1891203 -4.157691 -4.1096306 -4.0622635 -4.0388632 -4.0636163 -4.1220288 -4.1935105 -4.2532225 -4.293982 -4.308795][-4.1894608 -4.2162051 -4.2325253 -4.220643 -4.191617 -4.1536112 -4.1143012 -4.0859971 -4.0873637 -4.1275349 -4.184411 -4.2373934 -4.2795258 -4.306385 -4.3134279][-4.2308879 -4.2488575 -4.2528424 -4.2310228 -4.2020569 -4.1710353 -4.1432781 -4.1268167 -4.1388564 -4.1849437 -4.235981 -4.2755537 -4.3054204 -4.3190136 -4.3187981][-4.2777467 -4.2816048 -4.2729855 -4.2498126 -4.2306185 -4.2132235 -4.1960454 -4.186141 -4.1997819 -4.2428732 -4.2837963 -4.312089 -4.329423 -4.3311391 -4.3240423][-4.3112245 -4.3025403 -4.285769 -4.264782 -4.2526908 -4.2444825 -4.2381687 -4.239697 -4.2588086 -4.2966766 -4.3255444 -4.3412018 -4.3455672 -4.33775 -4.3258982][-4.327311 -4.3135796 -4.2982473 -4.2820382 -4.272459 -4.269413 -4.2704291 -4.279417 -4.29988 -4.32737 -4.34384 -4.3495669 -4.3458776 -4.335494 -4.3237896]]...]
INFO - root - 2017-12-05 14:13:09.616445: step 15210, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 74h:39m:23s remains)
INFO - root - 2017-12-05 14:13:18.228475: step 15220, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 75h:52m:37s remains)
INFO - root - 2017-12-05 14:13:26.779796: step 15230, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 76h:26m:57s remains)
INFO - root - 2017-12-05 14:13:35.403398: step 15240, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 74h:35m:55s remains)
INFO - root - 2017-12-05 14:13:43.951913: step 15250, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:44m:40s remains)
INFO - root - 2017-12-05 14:13:52.437825: step 15260, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 76h:20m:07s remains)
INFO - root - 2017-12-05 14:14:00.836422: step 15270, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 71h:42m:34s remains)
INFO - root - 2017-12-05 14:14:09.378119: step 15280, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 76h:58m:06s remains)
INFO - root - 2017-12-05 14:14:18.021555: step 15290, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 73h:32m:53s remains)
INFO - root - 2017-12-05 14:14:26.466887: step 15300, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 75h:47m:01s remains)
2017-12-05 14:14:27.245201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1817951 -4.171526 -4.179038 -4.1922975 -4.2121987 -4.2208824 -4.2064958 -4.1768112 -4.1486807 -4.1239882 -4.0947471 -4.0653319 -4.0633287 -4.0870628 -4.0922337][-4.1621943 -4.1574392 -4.1703725 -4.1855788 -4.207952 -4.2217426 -4.2042255 -4.1765971 -4.1501193 -4.1225796 -4.0970931 -4.0776887 -4.0855269 -4.1169982 -4.1362362][-4.1496363 -4.1506596 -4.1601233 -4.1714106 -4.191926 -4.2039261 -4.1812115 -4.1573114 -4.1379519 -4.1146727 -4.0923429 -4.0793948 -4.0865564 -4.1180964 -4.148932][-4.1348586 -4.1360087 -4.1384544 -4.1480346 -4.1712694 -4.1824217 -4.1580939 -4.1384764 -4.1331158 -4.1167083 -4.091682 -4.0741987 -4.0764365 -4.1041141 -4.14128][-4.0997982 -4.1005549 -4.1043253 -4.1231918 -4.1580324 -4.1691608 -4.1409249 -4.1232376 -4.1305857 -4.1280694 -4.1093283 -4.0869126 -4.0831709 -4.1060162 -4.1448841][-4.0638876 -4.0591826 -4.0686374 -4.1006246 -4.1404653 -4.1467686 -4.1132145 -4.0966611 -4.1125212 -4.1288748 -4.1263871 -4.1109986 -4.108253 -4.1315374 -4.1719565][-4.0443988 -4.037725 -4.0467987 -4.0808616 -4.1143084 -4.1097865 -4.0695934 -4.049356 -4.0726557 -4.113112 -4.1360283 -4.1380777 -4.1429148 -4.1663165 -4.2068233][-4.0291181 -4.024076 -4.0271258 -4.0509806 -4.0750532 -4.0634971 -4.0183706 -3.9893711 -4.0218606 -4.0875244 -4.1367922 -4.1556854 -4.1664853 -4.1863341 -4.2262435][-4.0167956 -4.0081849 -4.0008392 -4.0093188 -4.0250764 -4.0124049 -3.9710813 -3.9402163 -3.9692922 -4.0427279 -4.1093922 -4.1452579 -4.1609864 -4.1760449 -4.213913][-4.0072813 -3.9925342 -3.9751685 -3.9728482 -3.983098 -3.9758375 -3.949579 -3.9320223 -3.95597 -4.0214043 -4.0906892 -4.1349554 -4.1509328 -4.1592236 -4.1900558][-4.0377283 -4.0186419 -3.9903774 -3.9757869 -3.9790292 -3.9790025 -3.9695907 -3.9710059 -3.996098 -4.0498123 -4.1086965 -4.1454992 -4.1469278 -4.1415057 -4.1589823][-4.10089 -4.0823355 -4.0537348 -4.0344343 -4.0353475 -4.0403991 -4.0403566 -4.0514822 -4.07602 -4.1166124 -4.1570268 -4.17477 -4.1545539 -4.1332331 -4.1353159][-4.1638041 -4.1538739 -4.1358089 -4.122716 -4.1251884 -4.1298141 -4.1303735 -4.1409326 -4.1606536 -4.1878939 -4.2118883 -4.2125044 -4.1803851 -4.152616 -4.147069][-4.200284 -4.199132 -4.1925669 -4.1887722 -4.1936336 -4.1968746 -4.1987648 -4.2099094 -4.2273207 -4.2446914 -4.2570968 -4.2505689 -4.2213979 -4.1987815 -4.1932826][-4.2143183 -4.2151895 -4.2165656 -4.2205229 -4.2284818 -4.235383 -4.2417607 -4.2538443 -4.2693686 -4.2817793 -4.2880764 -4.28145 -4.2609444 -4.2460709 -4.2427177]]...]
INFO - root - 2017-12-05 14:14:35.777102: step 15310, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 75h:20m:37s remains)
INFO - root - 2017-12-05 14:14:44.163362: step 15320, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 75h:58m:16s remains)
INFO - root - 2017-12-05 14:14:52.762297: step 15330, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.865 sec/batch; 76h:11m:49s remains)
INFO - root - 2017-12-05 14:15:01.369923: step 15340, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:13m:00s remains)
INFO - root - 2017-12-05 14:15:09.939252: step 15350, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 75h:47m:33s remains)
INFO - root - 2017-12-05 14:15:18.606803: step 15360, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 77h:21m:36s remains)
INFO - root - 2017-12-05 14:15:27.087778: step 15370, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 77h:42m:14s remains)
INFO - root - 2017-12-05 14:15:35.563419: step 15380, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 76h:03m:35s remains)
INFO - root - 2017-12-05 14:15:44.029530: step 15390, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 74h:48m:56s remains)
INFO - root - 2017-12-05 14:15:52.538929: step 15400, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 75h:23m:10s remains)
2017-12-05 14:15:53.266692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3491049 -4.3453417 -4.33921 -4.3323407 -4.3255377 -4.3202767 -4.3165531 -4.3173628 -4.32394 -4.331409 -4.3331623 -4.3295121 -4.3192978 -4.2934933 -4.24544][-4.3453808 -4.3392253 -4.3315334 -4.323072 -4.3150859 -4.3045182 -4.2932587 -4.2911444 -4.299448 -4.3089805 -4.3138294 -4.31464 -4.3036661 -4.2653837 -4.1902509][-4.341177 -4.3342628 -4.3260841 -4.3157287 -4.3027191 -4.2793446 -4.2562361 -4.2512679 -4.2627287 -4.2750549 -4.2857394 -4.2921796 -4.2829275 -4.231688 -4.1224051][-4.3379631 -4.3310313 -4.319993 -4.3052826 -4.2803888 -4.2357421 -4.192996 -4.18224 -4.2003193 -4.2259884 -4.2493429 -4.2630205 -4.2487979 -4.1757522 -4.0268087][-4.3335166 -4.3237033 -4.3077264 -4.2860446 -4.2468519 -4.1766386 -4.1068764 -4.0879364 -4.1254749 -4.1776829 -4.221365 -4.2407312 -4.2198095 -4.1318159 -3.9698734][-4.325541 -4.3112469 -4.2892461 -4.2592416 -4.2031078 -4.0995531 -3.9917805 -3.9732332 -4.0551138 -4.1484342 -4.2097607 -4.2332807 -4.2140326 -4.1384878 -4.02031][-4.3146715 -4.2941713 -4.2647748 -4.21948 -4.136549 -3.9937081 -3.8431284 -3.8397441 -3.9784641 -4.1155457 -4.1987014 -4.2317138 -4.2220054 -4.1689749 -4.1035714][-4.3063865 -4.2798409 -4.2421312 -4.1835785 -4.0868559 -3.9403181 -3.7955258 -3.8126872 -3.9658918 -4.1136818 -4.2019644 -4.2367191 -4.2335773 -4.2019353 -4.1682978][-4.3084092 -4.2773032 -4.2347836 -4.1776767 -4.099431 -3.9977102 -3.9181714 -3.9448531 -4.0525751 -4.1604161 -4.2290311 -4.2577949 -4.2591591 -4.235641 -4.2119174][-4.3152051 -4.2850237 -4.2486668 -4.2021766 -4.1443954 -4.0859475 -4.05851 -4.0886393 -4.1469932 -4.2062883 -4.2515507 -4.2745671 -4.2803254 -4.25194 -4.2173958][-4.32639 -4.3025966 -4.2745934 -4.2371621 -4.1935697 -4.160615 -4.1586671 -4.1859565 -4.2137604 -4.2437191 -4.2719679 -4.2900214 -4.2843637 -4.2349825 -4.1779032][-4.3389444 -4.3249006 -4.3034806 -4.2722893 -4.2342391 -4.2118049 -4.2207708 -4.2451057 -4.2617559 -4.2798624 -4.2994604 -4.3031278 -4.2652092 -4.1773887 -4.0920887][-4.3419485 -4.33587 -4.3185997 -4.2828474 -4.23955 -4.2196808 -4.2355514 -4.2633266 -4.2815223 -4.2974553 -4.3086982 -4.2868543 -4.2056541 -4.0657845 -3.9472103][-4.3291264 -4.3281617 -4.3091245 -4.2590351 -4.1988025 -4.1691575 -4.1881886 -4.2283783 -4.2623458 -4.285224 -4.2886276 -4.2446213 -4.1272755 -3.9488151 -3.8153028][-4.3034382 -4.3043122 -4.2791443 -4.2082129 -4.1151385 -4.067616 -4.1027408 -4.1728792 -4.2281213 -4.2607822 -4.2632933 -4.2153625 -4.106998 -3.9654627 -3.8905003]]...]
INFO - root - 2017-12-05 14:16:01.898901: step 15410, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 75h:17m:34s remains)
INFO - root - 2017-12-05 14:16:10.595144: step 15420, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 77h:24m:59s remains)
INFO - root - 2017-12-05 14:16:19.184930: step 15430, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 75h:18m:33s remains)
INFO - root - 2017-12-05 14:16:27.807894: step 15440, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 75h:00m:32s remains)
INFO - root - 2017-12-05 14:16:36.411487: step 15450, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 75h:13m:54s remains)
INFO - root - 2017-12-05 14:16:44.884513: step 15460, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 74h:48m:14s remains)
INFO - root - 2017-12-05 14:16:53.380903: step 15470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 75h:28m:30s remains)
INFO - root - 2017-12-05 14:17:01.864946: step 15480, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.806 sec/batch; 70h:57m:20s remains)
INFO - root - 2017-12-05 14:17:10.376071: step 15490, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 74h:52m:38s remains)
INFO - root - 2017-12-05 14:17:18.822168: step 15500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 73h:42m:32s remains)
2017-12-05 14:17:19.635088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0898576 -4.0953159 -4.0937271 -4.1036582 -4.1135144 -4.1287379 -4.1675081 -4.1895714 -4.1937127 -4.2061343 -4.2258878 -4.2326827 -4.2209544 -4.2116766 -4.2042093][-4.0696144 -4.0762372 -4.0899725 -4.1211758 -4.14674 -4.1689324 -4.2054081 -4.2202306 -4.2117562 -4.2089772 -4.2221241 -4.2289467 -4.2169218 -4.2094331 -4.2066445][-4.0935326 -4.1090322 -4.1374693 -4.1773844 -4.2035236 -4.2169704 -4.2355266 -4.2356715 -4.2158051 -4.2052379 -4.2162256 -4.220758 -4.2115579 -4.210113 -4.21153][-4.1473885 -4.171288 -4.19815 -4.2249217 -4.2329273 -4.2230611 -4.2124615 -4.1981816 -4.1819515 -4.1794305 -4.1955442 -4.19738 -4.1893535 -4.195116 -4.2061739][-4.2081523 -4.225225 -4.2385345 -4.2452493 -4.2248259 -4.1769986 -4.12985 -4.1081114 -4.107584 -4.1223516 -4.1518841 -4.1643076 -4.1671138 -4.1863246 -4.2105875][-4.2495604 -4.2469087 -4.237246 -4.2182307 -4.1642461 -4.0693555 -3.9716647 -3.9512305 -3.991843 -4.0370755 -4.082387 -4.1202312 -4.14829 -4.1898332 -4.2320576][-4.2606392 -4.2389812 -4.2077785 -4.164227 -4.0814118 -3.9411886 -3.7867055 -3.7781434 -3.8797219 -3.9679427 -4.0341711 -4.0988183 -4.1503067 -4.2084212 -4.2626114][-4.2566943 -4.2262354 -4.1783476 -4.1126928 -4.019011 -3.8791034 -3.7408483 -3.7579448 -3.8795545 -3.9814782 -4.0610046 -4.1390362 -4.1974678 -4.2533321 -4.2989564][-4.251627 -4.2162967 -4.16441 -4.0941734 -4.0206566 -3.9312868 -3.8572903 -3.8891869 -3.9864125 -4.0691175 -4.14513 -4.221087 -4.2743335 -4.3119783 -4.333436][-4.2524252 -4.2144308 -4.1666737 -4.1074777 -4.0614638 -4.02238 -4.006722 -4.0492659 -4.120584 -4.1818419 -4.2425609 -4.2996883 -4.3334646 -4.34865 -4.3454785][-4.2438164 -4.1941476 -4.1489277 -4.1026983 -4.0821562 -4.0791 -4.1033812 -4.1626649 -4.2242889 -4.2667418 -4.3019133 -4.3288875 -4.340941 -4.3402405 -4.3257694][-4.2045631 -4.1402388 -4.0862732 -4.0541677 -4.0642209 -4.0952148 -4.1476212 -4.2166882 -4.2716503 -4.2986188 -4.3082094 -4.3080549 -4.3011985 -4.2904596 -4.2772894][-4.1517529 -4.080205 -4.0226221 -4.0102954 -4.0529828 -4.1122665 -4.1767054 -4.238595 -4.2780523 -4.2911639 -4.28108 -4.2573175 -4.2332788 -4.2187138 -4.2113328][-4.1288404 -4.0660877 -4.0164542 -4.0175743 -4.0738583 -4.1441116 -4.2033753 -4.2449474 -4.262217 -4.256834 -4.2315693 -4.1921997 -4.1593618 -4.1536551 -4.15868][-4.1661448 -4.1218295 -4.0838647 -4.0822663 -4.1264024 -4.1878858 -4.2307472 -4.2479234 -4.2458019 -4.2248869 -4.1910739 -4.1485648 -4.1200566 -4.1287789 -4.14594]]...]
INFO - root - 2017-12-05 14:17:28.249487: step 15510, loss = 2.04, batch loss = 1.99 (9.9 examples/sec; 0.811 sec/batch; 71h:22m:07s remains)
INFO - root - 2017-12-05 14:17:36.750043: step 15520, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 72h:04m:21s remains)
INFO - root - 2017-12-05 14:17:45.302756: step 15530, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 75h:12m:58s remains)
INFO - root - 2017-12-05 14:17:53.910503: step 15540, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 76h:41m:12s remains)
INFO - root - 2017-12-05 14:18:02.432310: step 15550, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.810 sec/batch; 71h:20m:31s remains)
INFO - root - 2017-12-05 14:18:11.087221: step 15560, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 76h:30m:21s remains)
INFO - root - 2017-12-05 14:18:19.699417: step 15570, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 77h:12m:25s remains)
INFO - root - 2017-12-05 14:18:28.229424: step 15580, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 76h:59m:17s remains)
INFO - root - 2017-12-05 14:18:36.793375: step 15590, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 75h:40m:03s remains)
INFO - root - 2017-12-05 14:18:45.244027: step 15600, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 74h:48m:59s remains)
2017-12-05 14:18:45.998938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23575 -4.2344279 -4.23252 -4.2154331 -4.1872234 -4.1851511 -4.2183051 -4.2572622 -4.2717023 -4.2631922 -4.2473531 -4.2345047 -4.2325454 -4.2411876 -4.266573][-4.2275572 -4.2268515 -4.2229724 -4.2022028 -4.1658359 -4.1566582 -4.1917081 -4.2356081 -4.2564793 -4.2548475 -4.2454734 -4.2377377 -4.2391376 -4.2491593 -4.2704153][-4.235713 -4.2370315 -4.2285519 -4.1981039 -4.1513691 -4.13165 -4.16403 -4.2096291 -4.2345018 -4.2400694 -4.2404709 -4.2414608 -4.247499 -4.2588577 -4.2778969][-4.253685 -4.2591643 -4.2464342 -4.2038989 -4.1444635 -4.1109309 -4.1316662 -4.1694403 -4.1939578 -4.2074041 -4.2211342 -4.2366333 -4.2514138 -4.2675424 -4.2868328][-4.2717829 -4.2833476 -4.2715921 -4.2221808 -4.1511087 -4.0990844 -4.0932965 -4.1080265 -4.1243296 -4.1447206 -4.1746292 -4.2075047 -4.2380328 -4.2667208 -4.2913318][-4.2920737 -4.3073921 -4.2995129 -4.2530184 -4.180717 -4.1108332 -4.0642271 -4.0386024 -4.0370426 -4.0607376 -4.1084056 -4.1623926 -4.2108226 -4.2533045 -4.2868347][-4.3096905 -4.3264861 -4.3223181 -4.28501 -4.2211485 -4.1448011 -4.0658078 -3.9997923 -3.9745603 -3.996582 -4.0573206 -4.1288428 -4.1908607 -4.2414174 -4.279686][-4.3192315 -4.3353167 -4.3335934 -4.3079443 -4.2599583 -4.1954823 -4.1185856 -4.044734 -4.0099344 -4.0224609 -4.0743918 -4.1395855 -4.1978359 -4.2485175 -4.2862644][-4.31727 -4.3321376 -4.3318553 -4.3150477 -4.2806625 -4.2324834 -4.1764402 -4.1242743 -4.1006622 -4.1068788 -4.1353178 -4.1740241 -4.2160444 -4.2596607 -4.2954698][-4.3055353 -4.3197861 -4.3208332 -4.3091264 -4.284236 -4.2485714 -4.2115746 -4.183733 -4.1803584 -4.1898537 -4.20282 -4.2171268 -4.2375288 -4.2675877 -4.2993059][-4.2943549 -4.3049645 -4.3029647 -4.2927394 -4.2724547 -4.2424464 -4.2155418 -4.2048035 -4.2187538 -4.2372155 -4.2480845 -4.2537236 -4.2622676 -4.2804503 -4.3050652][-4.2897472 -4.2927709 -4.2827773 -4.26617 -4.2431664 -4.2138205 -4.1906257 -4.1890593 -4.2164388 -4.2460823 -4.2617908 -4.2675548 -4.2747536 -4.2897806 -4.3094559][-4.2923617 -4.2865777 -4.2650204 -4.236321 -4.2063427 -4.1729436 -4.1459274 -4.1439428 -4.1799488 -4.223556 -4.2498989 -4.2602744 -4.2710958 -4.2864413 -4.30369][-4.2916441 -4.2779608 -4.2434449 -4.1986341 -4.1555624 -4.1157022 -4.0856557 -4.0832868 -4.1268716 -4.1860089 -4.2257495 -4.2422066 -4.2541094 -4.2691283 -4.2871876][-4.2741776 -4.254859 -4.21301 -4.1574464 -4.1057086 -4.0665426 -4.0395975 -4.0370603 -4.0831141 -4.152235 -4.2028356 -4.2232094 -4.2357321 -4.2496514 -4.2713356]]...]
INFO - root - 2017-12-05 14:18:54.530014: step 15610, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:36m:36s remains)
INFO - root - 2017-12-05 14:19:03.079976: step 15620, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 75h:30m:21s remains)
INFO - root - 2017-12-05 14:19:11.546597: step 15630, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 75h:07m:15s remains)
INFO - root - 2017-12-05 14:19:20.015730: step 15640, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 75h:49m:52s remains)
INFO - root - 2017-12-05 14:19:28.476870: step 15650, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 74h:56m:04s remains)
INFO - root - 2017-12-05 14:19:36.986081: step 15660, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 72h:45m:54s remains)
INFO - root - 2017-12-05 14:19:45.531462: step 15670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 75h:54m:47s remains)
INFO - root - 2017-12-05 14:19:54.000764: step 15680, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 75h:02m:08s remains)
INFO - root - 2017-12-05 14:20:02.448735: step 15690, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 72h:19m:15s remains)
INFO - root - 2017-12-05 14:20:10.915268: step 15700, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 74h:18m:53s remains)
2017-12-05 14:20:11.726881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3155179 -4.3111534 -4.3085012 -4.3069448 -4.3067112 -4.3074989 -4.3098249 -4.3105931 -4.3123274 -4.3135695 -4.3137822 -4.3156571 -4.3185077 -4.3231058 -4.32795][-4.3084011 -4.3004665 -4.2961745 -4.2931833 -4.2908978 -4.2912617 -4.2955608 -4.2959828 -4.296454 -4.297092 -4.2954955 -4.2939215 -4.2954707 -4.3030314 -4.3116474][-4.2991929 -4.2869143 -4.2810674 -4.2746968 -4.266902 -4.2643185 -4.26922 -4.270802 -4.2730513 -4.2752528 -4.2691813 -4.2608337 -4.2606463 -4.272212 -4.2842855][-4.2888608 -4.2715049 -4.259088 -4.24466 -4.225625 -4.2157478 -4.2174125 -4.2202554 -4.2293906 -4.239429 -4.2345848 -4.2236996 -4.2238851 -4.2367687 -4.2501836][-4.2807674 -4.2582326 -4.2414007 -4.2189565 -4.1860695 -4.1634045 -4.1502724 -4.1472635 -4.1694145 -4.2024369 -4.211864 -4.2063518 -4.2084522 -4.2194133 -4.2299905][-4.2733459 -4.244832 -4.2234082 -4.1923256 -4.1412268 -4.09292 -4.043582 -4.0160713 -4.05138 -4.1191788 -4.161181 -4.1770854 -4.1909447 -4.2092233 -4.2211394][-4.2626967 -4.2262774 -4.1934638 -4.1480618 -4.0753479 -3.9899249 -3.8901572 -3.8225849 -3.8744504 -3.9856946 -4.0712929 -4.1218343 -4.1551294 -4.1887393 -4.2072725][-4.2460561 -4.2022519 -4.1582022 -4.1023488 -4.0130148 -3.8973839 -3.7537322 -3.6405675 -3.7096574 -3.8583572 -3.9800069 -4.0580597 -4.1085258 -4.1572814 -4.1847715][-4.2268405 -4.1768041 -4.1242828 -4.0682478 -3.9901838 -3.8907402 -3.7647538 -3.6570697 -3.7033443 -3.8293564 -3.9399402 -4.01628 -4.0725794 -4.1291203 -4.163455][-4.1891317 -4.1374598 -4.0862751 -4.0437932 -3.9965689 -3.9459929 -3.8835802 -3.8262258 -3.8436668 -3.9117074 -3.9820423 -4.0336227 -4.0756903 -4.1253824 -4.1608138][-4.1437755 -4.0935373 -4.0546789 -4.0338521 -4.0227079 -4.0158076 -3.9997985 -3.9798648 -3.9834976 -4.0092392 -4.04574 -4.0737171 -4.10036 -4.1425743 -4.1782756][-4.1198659 -4.0717964 -4.04226 -4.0409431 -4.0586777 -4.0790949 -4.0860572 -4.0838728 -4.0817556 -4.0869665 -4.1032314 -4.1199079 -4.1429625 -4.1779695 -4.2110386][-4.1305475 -4.0929418 -4.0771122 -4.0894413 -4.1187754 -4.1453457 -4.1606989 -4.1641679 -4.1631913 -4.1664124 -4.1787758 -4.1947384 -4.214026 -4.2391753 -4.26079][-4.1803579 -4.1568503 -4.1511884 -4.1677928 -4.1955547 -4.2206678 -4.2379317 -4.2447085 -4.2466831 -4.2490988 -4.2575932 -4.2698689 -4.28301 -4.2961917 -4.3044062][-4.2366734 -4.2210674 -4.2195334 -4.2333918 -4.253242 -4.2703352 -4.28322 -4.2912359 -4.296978 -4.3002591 -4.3058491 -4.3129516 -4.3205929 -4.326694 -4.3287826]]...]
INFO - root - 2017-12-05 14:20:20.238590: step 15710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 77h:01m:29s remains)
INFO - root - 2017-12-05 14:20:28.788090: step 15720, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 75h:59m:36s remains)
INFO - root - 2017-12-05 14:20:37.309914: step 15730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 75h:58m:06s remains)
INFO - root - 2017-12-05 14:20:45.923035: step 15740, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 76h:08m:18s remains)
INFO - root - 2017-12-05 14:20:54.325896: step 15750, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:35m:34s remains)
INFO - root - 2017-12-05 14:21:02.921209: step 15760, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.903 sec/batch; 79h:28m:51s remains)
INFO - root - 2017-12-05 14:21:11.551799: step 15770, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 75h:32m:49s remains)
INFO - root - 2017-12-05 14:21:20.151925: step 15780, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 76h:04m:09s remains)
INFO - root - 2017-12-05 14:21:28.719270: step 15790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 75h:07m:50s remains)
INFO - root - 2017-12-05 14:21:37.154285: step 15800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 74h:12m:07s remains)
2017-12-05 14:21:37.939206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1542034 -4.1512055 -4.1619124 -4.1803789 -4.1999087 -4.2233806 -4.2470913 -4.265995 -4.2787261 -4.2782111 -4.2703896 -4.248601 -4.215086 -4.1952105 -4.192255][-4.1744838 -4.1695442 -4.1726656 -4.1827135 -4.1969967 -4.2211914 -4.24698 -4.2620139 -4.2671547 -4.2635918 -4.2530603 -4.2316437 -4.2043266 -4.1948824 -4.198061][-4.1987195 -4.1896396 -4.1837988 -4.1866388 -4.1985459 -4.2239337 -4.2475176 -4.2558894 -4.2521734 -4.2422061 -4.22962 -4.2128053 -4.1973906 -4.1979141 -4.2075877][-4.2197962 -4.211853 -4.2066083 -4.2071834 -4.2138352 -4.2324066 -4.244895 -4.241519 -4.2297912 -4.2190971 -4.21567 -4.215517 -4.2132821 -4.2193985 -4.226779][-4.2305269 -4.2293053 -4.2312236 -4.2344456 -4.2351422 -4.2371097 -4.2289915 -4.2096362 -4.19101 -4.184339 -4.1976104 -4.2181587 -4.2338367 -4.2478838 -4.254703][-4.2231908 -4.2315936 -4.2413363 -4.2469 -4.2415485 -4.2295623 -4.2045507 -4.1682458 -4.1389647 -4.1370435 -4.1712141 -4.2172546 -4.2532935 -4.2751818 -4.2790079][-4.2118373 -4.2232795 -4.2334347 -4.2348027 -4.2225766 -4.1987262 -4.159914 -4.1121159 -4.0812044 -4.0955305 -4.1542463 -4.220962 -4.2692394 -4.2946005 -4.2939248][-4.2075753 -4.2108645 -4.2109222 -4.2015996 -4.1807 -4.148416 -4.10802 -4.0696068 -4.05915 -4.0976915 -4.1703649 -4.2372036 -4.28 -4.3003092 -4.2943554][-4.2126942 -4.2004442 -4.1841497 -4.1621342 -4.1333876 -4.1013756 -4.0743914 -4.061264 -4.076036 -4.1222234 -4.1851692 -4.2345653 -4.263103 -4.2740026 -4.2675161][-4.2260075 -4.197505 -4.1631074 -4.12892 -4.0991859 -4.0760674 -4.0657907 -4.0694809 -4.0914354 -4.1284609 -4.1734161 -4.2084851 -4.2273669 -4.2321038 -4.2263331][-4.2345033 -4.1999364 -4.1623383 -4.1292658 -4.10418 -4.0881176 -4.0847263 -4.0887628 -4.1027756 -4.1279759 -4.1580119 -4.1807618 -4.1927843 -4.194272 -4.1869617][-4.2217007 -4.197794 -4.1751313 -4.156743 -4.1432829 -4.1335969 -4.1309156 -4.130475 -4.1354618 -4.1518641 -4.1711755 -4.18361 -4.1874571 -4.1812644 -4.1683025][-4.2055178 -4.2014928 -4.1972179 -4.191689 -4.1849675 -4.178494 -4.1763496 -4.176002 -4.17969 -4.19346 -4.2073026 -4.2130308 -4.210485 -4.1982288 -4.1799083][-4.2039208 -4.2141171 -4.2191043 -4.217 -4.2112441 -4.2057834 -4.204143 -4.2052307 -4.2101979 -4.2232976 -4.2346625 -4.2374582 -4.2312212 -4.2173891 -4.2006989][-4.2219348 -4.2324386 -4.2339249 -4.2276073 -4.2191434 -4.2134433 -4.2122808 -4.215446 -4.2222009 -4.2337904 -4.2423096 -4.242301 -4.2341971 -4.2212219 -4.2096686]]...]
INFO - root - 2017-12-05 14:21:46.464047: step 15810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 73h:49m:03s remains)
INFO - root - 2017-12-05 14:21:54.915328: step 15820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 75h:33m:29s remains)
INFO - root - 2017-12-05 14:22:03.407342: step 15830, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 78h:13m:50s remains)
INFO - root - 2017-12-05 14:22:12.017015: step 15840, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 73h:57m:10s remains)
INFO - root - 2017-12-05 14:22:20.573559: step 15850, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 74h:13m:44s remains)
INFO - root - 2017-12-05 14:22:29.102675: step 15860, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 75h:48m:07s remains)
INFO - root - 2017-12-05 14:22:37.594361: step 15870, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 74h:55m:07s remains)
INFO - root - 2017-12-05 14:22:46.101732: step 15880, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 74h:58m:07s remains)
INFO - root - 2017-12-05 14:22:54.689263: step 15890, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 75h:50m:50s remains)
INFO - root - 2017-12-05 14:23:03.118090: step 15900, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 73h:22m:22s remains)
2017-12-05 14:23:03.944479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2382174 -4.2307119 -4.2224784 -4.2130094 -4.1984138 -4.1848826 -4.1722364 -4.1707373 -4.1749959 -4.1633687 -4.1620674 -4.1684046 -4.1700649 -4.1590681 -4.1384068][-4.2523913 -4.2528243 -4.2546525 -4.2524133 -4.2404943 -4.2295213 -4.2197685 -4.217186 -4.2174544 -4.2025647 -4.1991096 -4.2097631 -4.2205892 -4.215117 -4.190454][-4.26535 -4.2691875 -4.2751193 -4.2750521 -4.2647252 -4.2551179 -4.2468066 -4.2415533 -4.2354155 -4.2213426 -4.2194262 -4.2306695 -4.2452669 -4.2489362 -4.2293062][-4.2710748 -4.2731123 -4.2793503 -4.2821407 -4.2755432 -4.265758 -4.255754 -4.2461977 -4.2292223 -4.2141833 -4.2147703 -4.2302871 -4.2493029 -4.25802 -4.2442722][-4.2687125 -4.2692537 -4.2774687 -4.2815003 -4.276226 -4.2627683 -4.2458415 -4.2251687 -4.1940217 -4.1732912 -4.1758108 -4.2003436 -4.223237 -4.2314825 -4.217186][-4.2681208 -4.2676711 -4.2742591 -4.2738996 -4.2615533 -4.2371678 -4.2076712 -4.1671743 -4.1147289 -4.0819988 -4.0900979 -4.1304049 -4.1618676 -4.1710577 -4.1562996][-4.2667322 -4.2618408 -4.2575941 -4.2438431 -4.220973 -4.1877875 -4.1479235 -4.0923157 -4.0218153 -3.9792418 -3.9910817 -4.0473657 -4.0918965 -4.1098409 -4.1038651][-4.2572241 -4.2423105 -4.2216706 -4.1914697 -4.1561322 -4.1119008 -4.0633121 -4.0053306 -3.9435749 -3.9153023 -3.945581 -4.01663 -4.0710883 -4.0980372 -4.0950365][-4.2396913 -4.2119474 -4.1798019 -4.1409597 -4.0988808 -4.0492573 -4.0005875 -3.9614608 -3.9332423 -3.9402781 -3.9866872 -4.0563607 -4.1060691 -4.1241126 -4.1102757][-4.2293782 -4.1912241 -4.152669 -4.1137552 -4.0750442 -4.0315342 -3.9978061 -3.988493 -3.9923458 -4.02289 -4.0657949 -4.1169229 -4.1518126 -4.15572 -4.1341372][-4.2487097 -4.2085123 -4.169929 -4.131144 -4.0964847 -4.0669265 -4.0539126 -4.066781 -4.0825906 -4.1149826 -4.1439734 -4.1742945 -4.1962547 -4.1949925 -4.1764231][-4.2902231 -4.2571979 -4.2240987 -4.1912031 -4.1665373 -4.1512504 -4.1488519 -4.1662073 -4.1792717 -4.2021494 -4.22044 -4.2380695 -4.2515411 -4.2503576 -4.2395377][-4.3241611 -4.3034387 -4.2833538 -4.2642217 -4.2508564 -4.242404 -4.2422976 -4.2539725 -4.2599211 -4.2738376 -4.2861595 -4.2955036 -4.3001752 -4.2996664 -4.2938647][-4.3407865 -4.3297749 -4.3200073 -4.3121877 -4.30764 -4.304812 -4.3056846 -4.3112178 -4.3148689 -4.322691 -4.3282366 -4.3312273 -4.3309779 -4.3295488 -4.3264046][-4.343132 -4.3360686 -4.3310595 -4.3279314 -4.3279204 -4.3294048 -4.3307009 -4.3317995 -4.3318195 -4.3325157 -4.3326912 -4.332624 -4.3319659 -4.3316426 -4.3308277]]...]
INFO - root - 2017-12-05 14:23:12.447694: step 15910, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 74h:25m:39s remains)
INFO - root - 2017-12-05 14:23:20.892811: step 15920, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 74h:21m:43s remains)
INFO - root - 2017-12-05 14:23:29.284747: step 15930, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 74h:11m:34s remains)
INFO - root - 2017-12-05 14:23:37.845960: step 15940, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 74h:45m:25s remains)
INFO - root - 2017-12-05 14:23:46.367913: step 15950, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 74h:22m:27s remains)
INFO - root - 2017-12-05 14:23:54.831531: step 15960, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 77h:15m:06s remains)
INFO - root - 2017-12-05 14:24:03.376238: step 15970, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 76h:30m:42s remains)
INFO - root - 2017-12-05 14:24:11.993380: step 15980, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 76h:41m:16s remains)
INFO - root - 2017-12-05 14:24:20.518966: step 15990, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 76h:11m:50s remains)
INFO - root - 2017-12-05 14:24:28.985309: step 16000, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 78h:55m:03s remains)
2017-12-05 14:24:29.747576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1515188 -4.1490412 -4.1661325 -4.1869087 -4.2057753 -4.2135177 -4.2171888 -4.2247887 -4.2313657 -4.2357883 -4.2249427 -4.2060633 -4.1968508 -4.1915903 -4.183567][-4.14831 -4.148591 -4.1694283 -4.1955452 -4.2152271 -4.2208 -4.2248664 -4.2336144 -4.2349329 -4.2314754 -4.2167916 -4.1917968 -4.1764684 -4.174808 -4.1769013][-4.1538291 -4.1591234 -4.1822019 -4.2102647 -4.2279949 -4.232532 -4.2334752 -4.2360516 -4.2290916 -4.2229686 -4.2088852 -4.1776543 -4.1534796 -4.1485443 -4.1597366][-4.1799726 -4.1830616 -4.2001076 -4.2224555 -4.2357225 -4.235445 -4.2257714 -4.2099433 -4.1935897 -4.1876736 -4.180891 -4.1593332 -4.1389441 -4.1292386 -4.1379061][-4.1978021 -4.1968074 -4.2082114 -4.2235875 -4.2212253 -4.2026653 -4.1717167 -4.1345973 -4.1125183 -4.11503 -4.1281767 -4.1306314 -4.1204462 -4.1063533 -4.1059132][-4.1850276 -4.1848154 -4.1982546 -4.209023 -4.1932349 -4.1487594 -4.086453 -4.0185122 -3.984159 -4.0062428 -4.0539622 -4.084321 -4.0920095 -4.0810385 -4.0713754][-4.1253614 -4.117732 -4.1420708 -4.1686163 -4.155057 -4.0928636 -4.0060606 -3.9096763 -3.8525922 -3.89797 -3.9826846 -4.0352311 -4.0559592 -4.0468583 -4.0336165][-4.0289092 -4.0105958 -4.0462775 -4.1005869 -4.1104326 -4.052721 -3.9631271 -3.8665824 -3.8140421 -3.8788292 -3.9743066 -4.0298305 -4.0524116 -4.04172 -4.0272441][-3.9428267 -3.9148128 -3.9548764 -4.0314364 -4.072782 -4.0461173 -3.9848042 -3.9243345 -3.9051156 -3.9631264 -4.02966 -4.0662479 -4.0831327 -4.0730233 -4.061645][-3.9678991 -3.9430962 -3.9733043 -4.0353165 -4.0816755 -4.0816274 -4.0514483 -4.0258188 -4.0282397 -4.0692506 -4.10526 -4.1215148 -4.1262608 -4.115561 -4.10904][-4.0601697 -4.0551319 -4.0735073 -4.1009307 -4.1258268 -4.1318345 -4.1197357 -4.111187 -4.1225133 -4.1516156 -4.1714296 -4.1755252 -4.1678171 -4.1511879 -4.1481833][-4.1259279 -4.1378665 -4.1510181 -4.154717 -4.1574478 -4.1573219 -4.1502967 -4.1514487 -4.1726866 -4.1976862 -4.2141075 -4.2149258 -4.2005796 -4.1831579 -4.1801538][-4.1563573 -4.1698918 -4.1770382 -4.1712966 -4.1639504 -4.1623006 -4.1618195 -4.1689138 -4.1958318 -4.2218614 -4.2399874 -4.2429142 -4.2318492 -4.2175961 -4.2126212][-4.1737804 -4.1784258 -4.1798291 -4.1746869 -4.1715288 -4.1805792 -4.1902986 -4.1976223 -4.2164497 -4.231391 -4.241868 -4.245193 -4.2391186 -4.2286081 -4.2243781][-4.191896 -4.1913543 -4.1905971 -4.18958 -4.194478 -4.2127194 -4.2259874 -4.2279382 -4.2306881 -4.2280178 -4.226438 -4.2281375 -4.2248759 -4.2167573 -4.2145891]]...]
INFO - root - 2017-12-05 14:24:38.215050: step 16010, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 0.815 sec/batch; 71h:41m:31s remains)
INFO - root - 2017-12-05 14:24:46.752650: step 16020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 74h:06m:25s remains)
INFO - root - 2017-12-05 14:24:55.354364: step 16030, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 74h:02m:05s remains)
INFO - root - 2017-12-05 14:25:03.854728: step 16040, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 77h:39m:21s remains)
INFO - root - 2017-12-05 14:25:12.467072: step 16050, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 74h:12m:47s remains)
INFO - root - 2017-12-05 14:25:21.005943: step 16060, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 73h:21m:51s remains)
INFO - root - 2017-12-05 14:25:29.356121: step 16070, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 72h:20m:03s remains)
INFO - root - 2017-12-05 14:25:37.813695: step 16080, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 71h:33m:54s remains)
INFO - root - 2017-12-05 14:25:46.284147: step 16090, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 74h:16m:35s remains)
INFO - root - 2017-12-05 14:25:54.670849: step 16100, loss = 2.02, batch loss = 1.97 (9.3 examples/sec; 0.859 sec/batch; 75h:28m:41s remains)
2017-12-05 14:25:55.417980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2918253 -4.2955518 -4.2970405 -4.3007584 -4.3043914 -4.30869 -4.3130989 -4.314497 -4.3125939 -4.3073459 -4.2996993 -4.2943912 -4.2961063 -4.3079286 -4.3213224][-4.2685637 -4.2733979 -4.2754507 -4.2816606 -4.2904615 -4.2999196 -4.3089027 -4.3135343 -4.3141441 -4.3084512 -4.2980504 -4.2883315 -4.2860427 -4.2975736 -4.3128109][-4.24388 -4.2425022 -4.2393909 -4.2436872 -4.2552066 -4.2673063 -4.2799029 -4.290596 -4.2991896 -4.2963295 -4.2836623 -4.2700076 -4.2650509 -4.276751 -4.2961464][-4.2102919 -4.1965594 -4.1835852 -4.1817255 -4.189055 -4.1955047 -4.2058368 -4.227284 -4.2511792 -4.259367 -4.2503448 -4.2354174 -4.22927 -4.244504 -4.2707186][-4.1676021 -4.1399159 -4.1155167 -4.1054344 -4.1016345 -4.0881753 -4.0843973 -4.1171265 -4.1610856 -4.1860471 -4.1875224 -4.1806188 -4.1801057 -4.2027874 -4.2365875][-4.1245546 -4.0816436 -4.0434852 -4.0213957 -3.9967203 -3.950757 -3.9267216 -3.9770639 -4.0473495 -4.09268 -4.1115494 -4.1201739 -4.1305056 -4.1614728 -4.2018986][-4.1132045 -4.0605879 -4.007031 -3.9671273 -3.914891 -3.8287489 -3.7792156 -3.8492956 -3.9410348 -3.9998941 -4.0379133 -4.0683222 -4.0933552 -4.1320229 -4.1781354][-4.14556 -4.1009021 -4.0498729 -4.0016727 -3.9354312 -3.8340945 -3.7672718 -3.8229141 -3.9050229 -3.9595757 -4.0047135 -4.0530539 -4.0963974 -4.1429191 -4.1903028][-4.193264 -4.1707125 -4.1395693 -4.10583 -4.0524411 -3.97134 -3.9145434 -3.9374063 -3.9778461 -4.0039587 -4.0392685 -4.0861216 -4.1333084 -4.1815357 -4.2258749][-4.2209811 -4.2244644 -4.21873 -4.2103143 -4.183836 -4.1367459 -4.0968204 -4.0949221 -4.09864 -4.0968227 -4.1107497 -4.14134 -4.1785526 -4.219461 -4.2573347][-4.2062445 -4.2270269 -4.2431345 -4.2612171 -4.2651248 -4.2524495 -4.2345085 -4.223958 -4.2092872 -4.1896887 -4.1815166 -4.1912317 -4.2119374 -4.2411923 -4.2698421][-4.1671839 -4.1940675 -4.2214842 -4.2596354 -4.2881036 -4.3001442 -4.3003278 -4.2930927 -4.2752752 -4.2473125 -4.2236362 -4.216248 -4.2225676 -4.2427139 -4.2652206][-4.1595478 -4.1764865 -4.2000394 -4.2460823 -4.2873735 -4.3110819 -4.3198447 -4.3176928 -4.3047023 -4.2795134 -4.2530823 -4.2367554 -4.2336273 -4.2458315 -4.2635136][-4.2031403 -4.2064295 -4.22042 -4.2596469 -4.2974691 -4.3201537 -4.32907 -4.3292961 -4.3218932 -4.3053756 -4.286222 -4.2721977 -4.2659764 -4.2704926 -4.2818875][-4.2673631 -4.2646542 -4.2702479 -4.2935629 -4.3169913 -4.3310513 -4.3360972 -4.3361168 -4.3334794 -4.3265224 -4.3171754 -4.3092175 -4.3044686 -4.3050923 -4.3104839]]...]
INFO - root - 2017-12-05 14:26:03.939685: step 16110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 75h:57m:58s remains)
INFO - root - 2017-12-05 14:26:12.530912: step 16120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 76h:00m:05s remains)
INFO - root - 2017-12-05 14:26:20.993193: step 16130, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 75h:14m:35s remains)
INFO - root - 2017-12-05 14:26:29.615326: step 16140, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 73h:44m:25s remains)
INFO - root - 2017-12-05 14:26:38.131566: step 16150, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 75h:33m:02s remains)
INFO - root - 2017-12-05 14:26:46.641698: step 16160, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 76h:09m:42s remains)
INFO - root - 2017-12-05 14:26:55.186209: step 16170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 74h:01m:13s remains)
INFO - root - 2017-12-05 14:27:03.673851: step 16180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 74h:47m:12s remains)
INFO - root - 2017-12-05 14:27:12.200855: step 16190, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 76h:26m:58s remains)
INFO - root - 2017-12-05 14:27:20.781664: step 16200, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:19m:01s remains)
2017-12-05 14:27:21.610630: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1273851 -4.1304913 -4.1398787 -4.1571622 -4.1777663 -4.1804237 -4.1729913 -4.1489921 -4.130331 -4.1280346 -4.1311421 -4.1399827 -4.1534233 -4.1666889 -4.1743207][-4.1240921 -4.1205664 -4.1210709 -4.1359744 -4.1589532 -4.1673026 -4.1663556 -4.1498671 -4.1382313 -4.1371956 -4.1404576 -4.150279 -4.1634765 -4.1723928 -4.1812491][-4.1265411 -4.1223116 -4.1123242 -4.1185961 -4.1387534 -4.1529379 -4.158906 -4.14809 -4.1458058 -4.1540289 -4.1648984 -4.1764927 -4.1826115 -4.1827106 -4.1890211][-4.1246643 -4.1288776 -4.1182523 -4.1197872 -4.1351695 -4.1444707 -4.1461291 -4.134356 -4.1372309 -4.1544285 -4.1736808 -4.1833057 -4.1830235 -4.1796937 -4.1849589][-4.1191325 -4.1358762 -4.1364651 -4.140532 -4.14711 -4.139214 -4.117178 -4.0938206 -4.1047196 -4.133286 -4.1585474 -4.1661429 -4.1639977 -4.1594119 -4.1615491][-4.1057911 -4.1313224 -4.1481671 -4.1583304 -4.1511207 -4.1169786 -4.0611372 -4.029326 -4.0576739 -4.1064234 -4.1435814 -4.1554008 -4.153583 -4.1431394 -4.1362634][-4.0850606 -4.1152916 -4.1438456 -4.157136 -4.134315 -4.0708795 -3.9829295 -3.9510338 -4.0100203 -4.084991 -4.1318054 -4.1464481 -4.1446772 -4.13095 -4.11747][-4.0797324 -4.1139493 -4.1431818 -4.1503305 -4.1117697 -4.0265694 -3.9257469 -3.9029827 -3.9867628 -4.0772367 -4.1277094 -4.138629 -4.1345091 -4.1234922 -4.109755][-4.0912519 -4.1229978 -4.1436911 -4.13926 -4.0974274 -4.0199285 -3.9407792 -3.9331887 -4.0088835 -4.082655 -4.1225038 -4.1292167 -4.1254148 -4.1155453 -4.1024108][-4.1121726 -4.13242 -4.1383333 -4.1240034 -4.0910912 -4.0409789 -3.9987218 -4.0061936 -4.058948 -4.1023755 -4.1275487 -4.132329 -4.1292486 -4.1205626 -4.1114774][-4.1369262 -4.1476808 -4.1421165 -4.122622 -4.1013355 -4.0788574 -4.0664868 -4.0814991 -4.1163735 -4.1381097 -4.1505585 -4.1531715 -4.147943 -4.1409583 -4.1406341][-4.1576176 -4.161521 -4.1497583 -4.132967 -4.1240172 -4.1215487 -4.1250925 -4.1420956 -4.1657615 -4.1751308 -4.1793747 -4.1785593 -4.1728215 -4.1683335 -4.1726418][-4.1775193 -4.1739478 -4.1603546 -4.1473742 -4.1428566 -4.147398 -4.1577859 -4.1767426 -4.1962342 -4.2025552 -4.2035375 -4.2011342 -4.1972141 -4.1959238 -4.2009392][-4.1968384 -4.189465 -4.1790729 -4.1724744 -4.1722317 -4.1764622 -4.1868157 -4.2028503 -4.2183394 -4.2240009 -4.2254696 -4.2261043 -4.2253094 -4.2255583 -4.22887][-4.2114635 -4.2044215 -4.197866 -4.1966696 -4.1997552 -4.2016659 -4.2080989 -4.2183318 -4.226789 -4.2306 -4.2319651 -4.2343907 -4.2354164 -4.2357564 -4.2364316]]...]
INFO - root - 2017-12-05 14:27:30.138925: step 16210, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 75h:07m:18s remains)
INFO - root - 2017-12-05 14:27:38.710244: step 16220, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 75h:36m:40s remains)
INFO - root - 2017-12-05 14:27:47.136865: step 16230, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 73h:43m:00s remains)
INFO - root - 2017-12-05 14:27:55.611951: step 16240, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 75h:25m:45s remains)
INFO - root - 2017-12-05 14:28:04.160422: step 16250, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 78h:30m:00s remains)
INFO - root - 2017-12-05 14:28:12.663060: step 16260, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 73h:13m:29s remains)
INFO - root - 2017-12-05 14:28:21.191280: step 16270, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 75h:16m:58s remains)
INFO - root - 2017-12-05 14:28:29.733986: step 16280, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 73h:17m:51s remains)
INFO - root - 2017-12-05 14:28:38.269980: step 16290, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 74h:39m:44s remains)
INFO - root - 2017-12-05 14:28:46.753029: step 16300, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 74h:03m:07s remains)
2017-12-05 14:28:47.559979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3198595 -4.3154597 -4.3106322 -4.3073206 -4.3053322 -4.3050265 -4.3087273 -4.3155737 -4.3220439 -4.3248067 -4.3213277 -4.3113275 -4.299015 -4.294558 -4.301033][-4.3036771 -4.29523 -4.2849655 -4.277637 -4.2729468 -4.2707963 -4.2730122 -4.2812614 -4.2923865 -4.3001933 -4.2984953 -4.283905 -4.2661037 -4.2609496 -4.2746134][-4.2730937 -4.262969 -4.2505178 -4.2406907 -4.232563 -4.2253656 -4.2215672 -4.2291818 -4.2481689 -4.26354 -4.2619171 -4.2389746 -4.21117 -4.2043319 -4.2266097][-4.2406726 -4.2325916 -4.223176 -4.2143912 -4.200232 -4.1815085 -4.1670876 -4.1729918 -4.2022858 -4.2290287 -4.2302718 -4.1989713 -4.1578646 -4.1448126 -4.171392][-4.2149639 -4.2105446 -4.2082844 -4.2023926 -4.1806197 -4.1459761 -4.11337 -4.1136427 -4.15747 -4.2058768 -4.2191544 -4.1874113 -4.1378055 -4.113018 -4.1337032][-4.1963353 -4.1967959 -4.201817 -4.1981544 -4.1700745 -4.1166615 -4.0558891 -4.0420074 -4.1033874 -4.1812406 -4.2177062 -4.1987534 -4.15135 -4.1140242 -4.1235557][-4.1816339 -4.189826 -4.2011871 -4.1979623 -4.1625619 -4.0908818 -3.9998882 -3.9661303 -4.0434618 -4.1523838 -4.2173753 -4.2234039 -4.1883492 -4.141397 -4.1357627][-4.1632266 -4.1790023 -4.1953411 -4.1925654 -4.1542978 -4.071312 -3.9588459 -3.9027357 -3.9851444 -4.1156745 -4.2044997 -4.2365088 -4.218513 -4.1717825 -4.152472][-4.1439624 -4.1665473 -4.1931076 -4.2002649 -4.1723766 -4.0979652 -3.9914541 -3.9240425 -3.9803817 -4.1029577 -4.1995754 -4.2462111 -4.2433715 -4.2057161 -4.1762371][-4.1462307 -4.1685958 -4.2018924 -4.2217083 -4.2140565 -4.1677103 -4.0908756 -4.030901 -4.05107 -4.1358247 -4.2169528 -4.259212 -4.2602949 -4.2296672 -4.1949153][-4.1617527 -4.1749172 -4.2060413 -4.23473 -4.2471805 -4.2306032 -4.1828957 -4.1346569 -4.1306219 -4.1762633 -4.23319 -4.264339 -4.2640724 -4.2404623 -4.2079787][-4.1876793 -4.1889434 -4.2112942 -4.24236 -4.2670593 -4.2713013 -4.2480021 -4.2118421 -4.1976905 -4.214437 -4.2477169 -4.2704167 -4.2728105 -4.2583117 -4.2322273][-4.2204857 -4.2142577 -4.22816 -4.2546487 -4.2808514 -4.2939296 -4.2884626 -4.2671413 -4.2539043 -4.2545047 -4.2681775 -4.281775 -4.2860255 -4.2786961 -4.259912][-4.2559853 -4.24772 -4.2520103 -4.2674589 -4.2869925 -4.3008552 -4.306953 -4.3018565 -4.2953153 -4.2898812 -4.29227 -4.2991104 -4.3025427 -4.2984591 -4.2854047][-4.2861271 -4.2795386 -4.2795534 -4.2859468 -4.2967715 -4.3071918 -4.3163514 -4.3210964 -4.3216033 -4.3170571 -4.3153038 -4.3181334 -4.3197947 -4.3171153 -4.30849]]...]
INFO - root - 2017-12-05 14:28:56.177601: step 16310, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 76h:25m:25s remains)
INFO - root - 2017-12-05 14:29:04.668969: step 16320, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 73h:56m:37s remains)
INFO - root - 2017-12-05 14:29:13.348213: step 16330, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 73h:20m:14s remains)
INFO - root - 2017-12-05 14:29:21.855185: step 16340, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 73h:28m:15s remains)
INFO - root - 2017-12-05 14:29:30.531541: step 16350, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.894 sec/batch; 78h:33m:13s remains)
INFO - root - 2017-12-05 14:29:39.121548: step 16360, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.860 sec/batch; 75h:33m:05s remains)
INFO - root - 2017-12-05 14:29:47.632980: step 16370, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 76h:50m:06s remains)
INFO - root - 2017-12-05 14:29:56.090418: step 16380, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 72h:03m:41s remains)
INFO - root - 2017-12-05 14:30:04.501487: step 16390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 75h:33m:46s remains)
INFO - root - 2017-12-05 14:30:12.903988: step 16400, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 72h:30m:18s remains)
2017-12-05 14:30:13.649951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1870389 -4.1844945 -4.1789255 -4.1671157 -4.1333675 -4.0793042 -4.03159 -4.0138912 -4.0440693 -4.093019 -4.141336 -4.191432 -4.2311578 -4.2536683 -4.2761822][-4.1877642 -4.1874681 -4.1859651 -4.1789265 -4.1477714 -4.0927663 -4.038918 -4.0206375 -4.0486355 -4.08774 -4.128159 -4.1786504 -4.2240925 -4.2509494 -4.2757025][-4.1697321 -4.1691003 -4.1723146 -4.1823049 -4.1765795 -4.1348152 -4.0750237 -4.04225 -4.0532241 -4.0752764 -4.1059079 -4.1558166 -4.2074738 -4.2424917 -4.2750573][-4.1307359 -4.1312137 -4.1418133 -4.1694417 -4.1859975 -4.1585288 -4.0902257 -4.0341535 -4.0268674 -4.0485559 -4.0807853 -4.1371312 -4.1947155 -4.2383094 -4.277329][-4.0920162 -4.0952549 -4.1097822 -4.1424456 -4.1637864 -4.1375704 -4.0565681 -3.9685869 -3.9485509 -3.9929662 -4.0505495 -4.123848 -4.1881781 -4.2410374 -4.2817254][-4.0597806 -4.0624952 -4.0811095 -4.114337 -4.125742 -4.0872211 -3.9780879 -3.8440213 -3.8165925 -3.9054556 -4.0103941 -4.1088624 -4.1827173 -4.2452407 -4.2869134][-4.0389266 -4.0447812 -4.0677438 -4.0922561 -4.0884933 -4.0312219 -3.8877149 -3.708827 -3.686136 -3.8349404 -3.9860759 -4.1001377 -4.1807294 -4.2475629 -4.2903509][-4.0575161 -4.0647726 -4.0817709 -4.0923982 -4.0736032 -3.9961369 -3.8393931 -3.6668773 -3.6742458 -3.8517213 -4.0086222 -4.1151295 -4.1875606 -4.250761 -4.2907996][-4.1073494 -4.1132736 -4.1187682 -4.1161523 -4.0908675 -4.0193834 -3.9049025 -3.8069541 -3.8377857 -3.9670577 -4.0781622 -4.15346 -4.2082381 -4.2605886 -4.2952795][-4.1564703 -4.1645651 -4.1647935 -4.1554289 -4.130034 -4.0733418 -4.0036879 -3.9522171 -3.976809 -4.0507245 -4.1222792 -4.1762309 -4.2236037 -4.2713156 -4.3006477][-4.1845388 -4.1930962 -4.1899323 -4.1790128 -4.1544333 -4.109036 -4.0662274 -4.0371208 -4.0531464 -4.09295 -4.1426783 -4.1901274 -4.2392168 -4.2827911 -4.3046823][-4.192461 -4.2037959 -4.1990695 -4.1884532 -4.1631384 -4.1263237 -4.0980139 -4.0831223 -4.090981 -4.1113763 -4.148839 -4.198041 -4.2485814 -4.2862844 -4.3034468][-4.18116 -4.19462 -4.1905179 -4.1787591 -4.1547952 -4.1234946 -4.099184 -4.0844617 -4.0858231 -4.1001511 -4.1372972 -4.1935472 -4.2495947 -4.28772 -4.3037548][-4.1693277 -4.1791644 -4.1742711 -4.1625834 -4.14868 -4.128643 -4.1059661 -4.084837 -4.082016 -4.0979071 -4.1375661 -4.1997852 -4.2586994 -4.2960081 -4.3109746][-4.1642842 -4.1671209 -4.15858 -4.1545424 -4.1563263 -4.1511259 -4.1356211 -4.1155305 -4.1116042 -4.1313224 -4.1730609 -4.2318473 -4.2819467 -4.31049 -4.3211532]]...]
INFO - root - 2017-12-05 14:30:22.170234: step 16410, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 77h:13m:12s remains)
INFO - root - 2017-12-05 14:30:30.767729: step 16420, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 73h:25m:49s remains)
INFO - root - 2017-12-05 14:30:39.326906: step 16430, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 74h:00m:54s remains)
INFO - root - 2017-12-05 14:30:47.865321: step 16440, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 77h:03m:38s remains)
INFO - root - 2017-12-05 14:30:56.342934: step 16450, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 72h:21m:10s remains)
INFO - root - 2017-12-05 14:31:04.837975: step 16460, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 74h:14m:53s remains)
INFO - root - 2017-12-05 14:31:13.368350: step 16470, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 75h:28m:58s remains)
INFO - root - 2017-12-05 14:31:21.668369: step 16480, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.800 sec/batch; 70h:14m:40s remains)
INFO - root - 2017-12-05 14:31:30.115425: step 16490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 74h:05m:26s remains)
INFO - root - 2017-12-05 14:31:38.560921: step 16500, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.882 sec/batch; 77h:25m:24s remains)
2017-12-05 14:31:39.363718: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2343011 -4.2308249 -4.2245054 -4.2215075 -4.2248116 -4.2332888 -4.2446895 -4.2553368 -4.2612939 -4.2624216 -4.2617612 -4.2598996 -4.25585 -4.2489109 -4.2402039][-4.231451 -4.2291236 -4.2192297 -4.2103081 -4.205821 -4.2079115 -4.2164035 -4.2288027 -4.2395539 -4.2468348 -4.2533879 -4.2589011 -4.2598143 -4.2560167 -4.2485795][-4.2173524 -4.2175579 -4.2033105 -4.1867356 -4.1720381 -4.1632538 -4.1632352 -4.1728978 -4.1874442 -4.2027555 -4.2208781 -4.2382526 -4.2480841 -4.2500987 -4.2430367][-4.1872897 -4.1898661 -4.1728978 -4.1513462 -4.1296496 -4.1110058 -4.1009836 -4.1027265 -4.1136675 -4.1293492 -4.1542368 -4.1843781 -4.2076354 -4.2196679 -4.2145576][-4.1417665 -4.1494575 -4.1352258 -4.1183214 -4.1010981 -4.0822172 -4.0660739 -4.0564823 -4.0524454 -4.0559678 -4.0751638 -4.1066055 -4.1389771 -4.1620584 -4.1657104][-4.1039681 -4.1139531 -4.10331 -4.0934544 -4.0849948 -4.0693893 -4.0524526 -4.034863 -4.0135469 -4.0002346 -4.0057907 -4.0276675 -4.0597916 -4.0898628 -4.1075726][-4.0812316 -4.0916028 -4.0845222 -4.0792718 -4.074635 -4.059433 -4.0414834 -4.0167127 -3.9817572 -3.9575348 -3.9510646 -3.9556606 -3.9772344 -4.0086403 -4.0381217][-4.0641551 -4.0753121 -4.0734835 -4.071516 -4.0690637 -4.0601177 -4.0482206 -4.0244884 -3.9872451 -3.9595318 -3.9418225 -3.9274862 -3.9333909 -3.9625194 -3.9966183][-4.0552325 -4.0620923 -4.0618587 -4.0609837 -4.0606031 -4.0608163 -4.0636749 -4.05094 -4.0239224 -4.0036378 -3.9883099 -3.9724026 -3.9708834 -3.9814696 -4.0006185][-4.0637708 -4.0642381 -4.0628672 -4.0589924 -4.0537395 -4.0556259 -4.0678782 -4.071969 -4.0621877 -4.0537162 -4.047195 -4.0391459 -4.0352507 -4.0266147 -4.0191684][-4.0813179 -4.0745091 -4.0698233 -4.0622945 -4.0489364 -4.0453749 -4.0592623 -4.0714011 -4.0716505 -4.070641 -4.0723433 -4.0738997 -4.0721745 -4.0536189 -4.0371051][-4.1018109 -4.0928507 -4.0874519 -4.0778866 -4.0598345 -4.0503578 -4.0573807 -4.069191 -4.0689383 -4.0700269 -4.0782237 -4.0897279 -4.0936246 -4.0784211 -4.06872][-4.1212544 -4.1192245 -4.1181655 -4.1100092 -4.0881457 -4.0708222 -4.0692625 -4.07739 -4.0744472 -4.0747705 -4.0842333 -4.1010981 -4.1143966 -4.1105742 -4.11697][-4.136147 -4.1419182 -4.1449919 -4.1383734 -4.1128778 -4.0913782 -4.0846515 -4.0875039 -4.0814371 -4.0812016 -4.0922933 -4.1117754 -4.1315689 -4.1432486 -4.1681914][-4.1413012 -4.14659 -4.1489682 -4.1469336 -4.1292477 -4.1106825 -4.1019816 -4.1020331 -4.0968819 -4.095974 -4.1035504 -4.1190476 -4.1370935 -4.1584516 -4.1947818]]...]
INFO - root - 2017-12-05 14:31:47.981343: step 16510, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 77h:11m:56s remains)
INFO - root - 2017-12-05 14:31:56.561787: step 16520, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 76h:29m:55s remains)
INFO - root - 2017-12-05 14:32:05.088953: step 16530, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 74h:14m:49s remains)
INFO - root - 2017-12-05 14:32:13.549188: step 16540, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 73h:51m:13s remains)
INFO - root - 2017-12-05 14:32:21.984633: step 16550, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 75h:18m:47s remains)
INFO - root - 2017-12-05 14:32:30.537592: step 16560, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 74h:13m:19s remains)
INFO - root - 2017-12-05 14:32:39.052812: step 16570, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 75h:10m:37s remains)
INFO - root - 2017-12-05 14:32:47.546995: step 16580, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 72h:36m:27s remains)
INFO - root - 2017-12-05 14:32:55.944888: step 16590, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 75h:22m:35s remains)
INFO - root - 2017-12-05 14:33:04.312603: step 16600, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.843 sec/batch; 74h:00m:22s remains)
2017-12-05 14:33:05.071006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.319994 -4.3196921 -4.3213558 -4.3239202 -4.3222032 -4.3189268 -4.31075 -4.3026094 -4.295682 -4.2942877 -4.3050246 -4.3179407 -4.3289032 -4.3380127 -4.3438163][-4.3154025 -4.3134422 -4.309185 -4.30445 -4.2957172 -4.2862358 -4.2725067 -4.2611322 -4.2546449 -4.258954 -4.2792449 -4.3005643 -4.3200116 -4.3373652 -4.347403][-4.3044462 -4.293962 -4.2752361 -4.2585707 -4.2417607 -4.2275314 -4.2094932 -4.1933613 -4.1880622 -4.2047381 -4.2438469 -4.2775569 -4.3069439 -4.3333158 -4.3485842][-4.2784219 -4.2554092 -4.2204146 -4.1912894 -4.169909 -4.1530538 -4.1337485 -4.1163306 -4.1208057 -4.1557975 -4.2160907 -4.2611928 -4.2952843 -4.3285561 -4.3486691][-4.2367659 -4.2005954 -4.1497307 -4.1062779 -4.0757132 -4.0539246 -4.0314283 -4.0172677 -4.0403876 -4.1004205 -4.1798282 -4.2341466 -4.2766509 -4.3181663 -4.3452334][-4.18412 -4.1391478 -4.0757055 -4.0136619 -3.9632611 -3.9259644 -3.9016664 -3.8929622 -3.939142 -4.0259109 -4.1139388 -4.1765728 -4.2348614 -4.295331 -4.3354845][-4.1294336 -4.0821524 -4.0138397 -3.9356306 -3.8598504 -3.8042536 -3.7847195 -3.7868092 -3.8525658 -3.9518664 -4.0317841 -4.103982 -4.1801929 -4.2593918 -4.3140354][-4.0903716 -4.0482006 -3.9812975 -3.8931491 -3.7920973 -3.7245972 -3.7143991 -3.7332747 -3.8051004 -3.889159 -3.9583411 -4.0476627 -4.1382351 -4.2231684 -4.2860003][-4.0737591 -4.0426507 -3.9873323 -3.9006267 -3.7947035 -3.7316184 -3.7355309 -3.7595682 -3.811259 -3.861115 -3.9153452 -4.0146918 -4.107357 -4.188611 -4.257144][-4.0863008 -4.0655608 -4.0223289 -3.9479749 -3.8573251 -3.8138106 -3.8199692 -3.8321416 -3.8604603 -3.8876357 -3.9356523 -4.0267329 -4.1055923 -4.178834 -4.2474771][-4.1290379 -4.1155415 -4.0860662 -4.029779 -3.9639 -3.9379177 -3.9355135 -3.9287512 -3.9385924 -3.9614608 -4.0105915 -4.0843143 -4.1440673 -4.2096319 -4.2700691][-4.1978755 -4.19109 -4.1735616 -4.1357064 -4.0958176 -4.083487 -4.0763125 -4.0612655 -4.0624294 -4.0825124 -4.1219864 -4.173574 -4.2158742 -4.2687316 -4.3116517][-4.2586527 -4.2576585 -4.2495847 -4.2290473 -4.2103639 -4.206079 -4.20239 -4.1921954 -4.1897664 -4.2027793 -4.2283144 -4.2598205 -4.2889037 -4.3219466 -4.3441081][-4.2964363 -4.2979708 -4.2959957 -4.2874718 -4.2820678 -4.282917 -4.2843647 -4.2807713 -4.2780752 -4.2820787 -4.2939291 -4.3102355 -4.3290782 -4.3452983 -4.3532424][-4.3168359 -4.3171577 -4.3159065 -4.3113346 -4.3097854 -4.3118968 -4.3140912 -4.3143978 -4.31409 -4.3144259 -4.3174934 -4.3249116 -4.3354 -4.3418951 -4.3437843]]...]
INFO - root - 2017-12-05 14:33:13.630007: step 16610, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 75h:19m:43s remains)
INFO - root - 2017-12-05 14:33:22.280877: step 16620, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 73h:44m:27s remains)
INFO - root - 2017-12-05 14:33:30.859979: step 16630, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 74h:33m:10s remains)
INFO - root - 2017-12-05 14:33:39.310739: step 16640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 74h:23m:32s remains)
INFO - root - 2017-12-05 14:33:47.802432: step 16650, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 74h:30m:24s remains)
INFO - root - 2017-12-05 14:33:56.360753: step 16660, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 72h:56m:00s remains)
INFO - root - 2017-12-05 14:34:05.067508: step 16670, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 75h:31m:06s remains)
INFO - root - 2017-12-05 14:34:13.543232: step 16680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 74h:42m:03s remains)
INFO - root - 2017-12-05 14:34:22.017791: step 16690, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 74h:34m:04s remains)
INFO - root - 2017-12-05 14:34:30.474623: step 16700, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 76h:32m:19s remains)
2017-12-05 14:34:31.268888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3241763 -4.3124695 -4.30691 -4.3030934 -4.3002009 -4.3006949 -4.30389 -4.3069358 -4.3082533 -4.305769 -4.3005333 -4.2920842 -4.2802358 -4.2687817 -4.2596292][-4.318378 -4.3039374 -4.2970929 -4.2938323 -4.2917681 -4.2919354 -4.2954755 -4.2994332 -4.301898 -4.3006105 -4.2970638 -4.2910848 -4.2826471 -4.274075 -4.2657394][-4.3178639 -4.3036752 -4.2951608 -4.2898712 -4.2876077 -4.2872376 -4.291141 -4.2963495 -4.2998896 -4.2989078 -4.29699 -4.2942562 -4.2904229 -4.2856255 -4.2777133][-4.3148 -4.2977138 -4.2838612 -4.2735348 -4.2672338 -4.2629142 -4.2628007 -4.2658 -4.2689342 -4.2678256 -4.26714 -4.2680073 -4.2706256 -4.2733254 -4.268466][-4.2984657 -4.273149 -4.2503242 -4.2307239 -4.21372 -4.1967711 -4.1794467 -4.1713581 -4.18041 -4.1852822 -4.1872091 -4.1944404 -4.2066569 -4.2220311 -4.2276831][-4.2647009 -4.2316675 -4.2021689 -4.172472 -4.1358781 -4.0941944 -4.0387859 -4.0151477 -4.0511904 -4.079865 -4.0921741 -4.110229 -4.1313334 -4.157649 -4.1752663][-4.2307348 -4.1940045 -4.164712 -4.1302361 -4.076827 -4.0069408 -3.9103453 -3.8797774 -3.9572761 -4.018434 -4.04806 -4.0736594 -4.0952139 -4.1234465 -4.1458387][-4.2202654 -4.186213 -4.1631923 -4.1351042 -4.0861559 -4.0224152 -3.9372835 -3.9127898 -3.9865496 -4.0418515 -4.06503 -4.0825133 -4.0941091 -4.1117134 -4.130589][-4.228785 -4.1979141 -4.1808352 -4.1634789 -4.1308193 -4.0889454 -4.0363269 -4.0206723 -4.0646605 -4.0928741 -4.102212 -4.108139 -4.1112528 -4.1215649 -4.1381264][-4.2412586 -4.2134266 -4.2022929 -4.1956224 -4.1784272 -4.1545243 -4.1261091 -4.1165891 -4.1357355 -4.1466732 -4.1495948 -4.1543903 -4.1583529 -4.1671515 -4.1831055][-4.2558193 -4.2319722 -4.2267661 -4.2282844 -4.2210569 -4.2099271 -4.1981535 -4.1931143 -4.2008972 -4.2046714 -4.2064314 -4.2137485 -4.2205806 -4.2291007 -4.2414289][-4.2720828 -4.2530856 -4.2520485 -4.257381 -4.2582521 -4.258347 -4.2593422 -4.25928 -4.2645974 -4.2702155 -4.2748342 -4.2809753 -4.2873664 -4.2934551 -4.2995577][-4.2912807 -4.2783132 -4.2800412 -4.288157 -4.2950096 -4.3008108 -4.3060579 -4.3084974 -4.3139877 -4.3204741 -4.325695 -4.3310804 -4.3371806 -4.3429484 -4.3478084][-4.3100505 -4.3023667 -4.3059826 -4.3154006 -4.32402 -4.3283129 -4.331274 -4.3320479 -4.3341351 -4.3384428 -4.3442116 -4.3516364 -4.3588581 -4.3655448 -4.3712683][-4.3248534 -4.3184624 -4.3195338 -4.3251195 -4.3303866 -4.33157 -4.331233 -4.32964 -4.3293815 -4.3320589 -4.3375998 -4.3454437 -4.3528733 -4.35974 -4.3652325]]...]
INFO - root - 2017-12-05 14:34:39.608963: step 16710, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 75h:14m:53s remains)
INFO - root - 2017-12-05 14:34:48.213654: step 16720, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 77h:16m:08s remains)
INFO - root - 2017-12-05 14:34:56.751023: step 16730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 74h:39m:51s remains)
INFO - root - 2017-12-05 14:35:05.142500: step 16740, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 75h:30m:51s remains)
INFO - root - 2017-12-05 14:35:13.745402: step 16750, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 75h:50m:19s remains)
INFO - root - 2017-12-05 14:35:22.183869: step 16760, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.810 sec/batch; 71h:01m:56s remains)
INFO - root - 2017-12-05 14:35:30.653842: step 16770, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 73h:53m:15s remains)
INFO - root - 2017-12-05 14:35:39.201445: step 16780, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 76h:30m:50s remains)
INFO - root - 2017-12-05 14:35:47.710680: step 16790, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 0.789 sec/batch; 69h:12m:11s remains)
INFO - root - 2017-12-05 14:35:56.143565: step 16800, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 75h:06m:27s remains)
2017-12-05 14:35:56.911786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.283 -4.2683282 -4.2598886 -4.2465835 -4.2266183 -4.2087312 -4.20238 -4.2040682 -4.2143865 -4.2343431 -4.2475848 -4.2505488 -4.2508874 -4.2486038 -4.2485394][-4.272501 -4.2573094 -4.2485971 -4.2327447 -4.2067761 -4.1787629 -4.1615491 -4.1589289 -4.1720715 -4.2034225 -4.2271814 -4.2349944 -4.2351928 -4.2300224 -4.228579][-4.2603359 -4.2469578 -4.2380404 -4.2246394 -4.2018952 -4.1695061 -4.1441317 -4.1397238 -4.1560917 -4.1940789 -4.2255259 -4.2381487 -4.2383952 -4.2317686 -4.2271428][-4.2436156 -4.2308817 -4.2211628 -4.2154665 -4.2051253 -4.1807156 -4.1531606 -4.1459746 -4.1651754 -4.2035775 -4.231904 -4.2392712 -4.2405534 -4.236805 -4.2321768][-4.2241454 -4.2055364 -4.19164 -4.1937461 -4.195909 -4.1743407 -4.1376715 -4.1259518 -4.1552005 -4.2013326 -4.224093 -4.2191815 -4.2206435 -4.2250104 -4.2265449][-4.2026 -4.1768732 -4.1551223 -4.1582723 -4.1564779 -4.1197453 -4.0583735 -4.033608 -4.0894227 -4.1617308 -4.1905651 -4.18322 -4.1870055 -4.197475 -4.2055411][-4.1862278 -4.155148 -4.1227574 -4.1170564 -4.1027083 -4.0340228 -3.9271708 -3.8702035 -3.9642653 -4.0867372 -4.1408997 -4.1460991 -4.1593027 -4.174437 -4.18965][-4.1899529 -4.1591234 -4.123857 -4.1124907 -4.082922 -3.9969151 -3.8639793 -3.7813165 -3.8962333 -4.0524216 -4.1219788 -4.1352162 -4.1514988 -4.1715865 -4.1966457][-4.2241716 -4.199985 -4.1694818 -4.1530871 -4.1185012 -4.0480452 -3.949163 -3.8975096 -3.9939022 -4.1167626 -4.1697326 -4.1729093 -4.1801171 -4.1998816 -4.2285805][-4.2546415 -4.2336211 -4.2091837 -4.1946855 -4.1669188 -4.1231332 -4.0690017 -4.0484495 -4.1144013 -4.1936383 -4.2284474 -4.2258673 -4.225821 -4.2414331 -4.2647657][-4.2668357 -4.2471461 -4.2312069 -4.2225852 -4.2092323 -4.1846046 -4.1526065 -4.1425457 -4.1873579 -4.2432313 -4.2719097 -4.2728209 -4.2701144 -4.2785597 -4.2920012][-4.2677951 -4.2523952 -4.2420807 -4.23748 -4.2304997 -4.2137241 -4.1913896 -4.1862321 -4.2205167 -4.262567 -4.2885537 -4.2926173 -4.2883368 -4.2904978 -4.2996621][-4.2636137 -4.2496395 -4.243855 -4.2416768 -4.2332468 -4.2187176 -4.2053676 -4.206203 -4.2347426 -4.2683191 -4.291729 -4.2974243 -4.2921748 -4.2893524 -4.29457][-4.2614312 -4.249826 -4.2493086 -4.2509403 -4.2456632 -4.2326722 -4.2209086 -4.221612 -4.2451987 -4.2728233 -4.2927508 -4.3001752 -4.2995892 -4.2965469 -4.2965631][-4.2716269 -4.2628369 -4.26277 -4.2656069 -4.2649736 -4.2560954 -4.2476954 -4.2477922 -4.263937 -4.2843938 -4.2981315 -4.304358 -4.3074985 -4.3071613 -4.3047781]]...]
INFO - root - 2017-12-05 14:36:05.382921: step 16810, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 74h:08m:10s remains)
INFO - root - 2017-12-05 14:36:13.933328: step 16820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 74h:26m:29s remains)
INFO - root - 2017-12-05 14:36:22.457190: step 16830, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 76h:25m:41s remains)
INFO - root - 2017-12-05 14:36:30.982610: step 16840, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.838 sec/batch; 73h:26m:19s remains)
INFO - root - 2017-12-05 14:36:39.361370: step 16850, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 73h:44m:46s remains)
INFO - root - 2017-12-05 14:36:47.920401: step 16860, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 75h:22m:42s remains)
INFO - root - 2017-12-05 14:36:56.352979: step 16870, loss = 2.05, batch loss = 2.00 (9.9 examples/sec; 0.804 sec/batch; 70h:29m:54s remains)
INFO - root - 2017-12-05 14:37:04.874550: step 16880, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 74h:35m:58s remains)
INFO - root - 2017-12-05 14:37:13.327341: step 16890, loss = 2.08, batch loss = 2.02 (11.0 examples/sec; 0.727 sec/batch; 63h:45m:10s remains)
INFO - root - 2017-12-05 14:37:21.802117: step 16900, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 72h:56m:43s remains)
2017-12-05 14:37:22.556387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1308231 -4.1359081 -4.1479 -4.1657424 -4.1769543 -4.1768928 -4.171999 -4.1698122 -4.145349 -4.0998712 -4.07122 -4.0726943 -4.104229 -4.1392708 -4.1623492][-4.1218505 -4.1350956 -4.1480355 -4.1625719 -4.169682 -4.1673837 -4.1619883 -4.1645174 -4.1449027 -4.0974813 -4.0649853 -4.065465 -4.1045632 -4.1507063 -4.1794605][-4.1134434 -4.1318245 -4.1398454 -4.1479192 -4.1533103 -4.1535654 -4.1524935 -4.1636219 -4.156868 -4.114511 -4.0741515 -4.0720696 -4.1165533 -4.1729908 -4.2043629][-4.1085806 -4.1291704 -4.1290956 -4.13396 -4.1441422 -4.1512413 -4.1492038 -4.1662869 -4.1735721 -4.144074 -4.1052284 -4.0938506 -4.124949 -4.175704 -4.2070646][-4.0947838 -4.1147666 -4.1094007 -4.1140614 -4.1291804 -4.1348958 -4.1174555 -4.1271133 -4.1526618 -4.1476874 -4.1246214 -4.11457 -4.1290159 -4.156909 -4.1777725][-4.0774636 -4.0958433 -4.0886021 -4.0887341 -4.097785 -4.0863757 -4.0345993 -4.0188556 -4.0717545 -4.1095123 -4.1139393 -4.1143479 -4.1249647 -4.1310706 -4.13326][-4.06957 -4.0861592 -4.0764589 -4.0665574 -4.0537019 -3.9999573 -3.8849959 -3.8066311 -3.8857632 -3.9998806 -4.0632453 -4.08745 -4.1014462 -4.1023664 -4.092176][-4.0913291 -4.1033916 -4.0919342 -4.0688062 -4.0421286 -3.9735439 -3.8341792 -3.7030666 -3.7578743 -3.9106925 -4.0178161 -4.0656948 -4.0866528 -4.0949368 -4.0913277][-4.126442 -4.136971 -4.1356993 -4.1194744 -4.1009474 -4.0609126 -3.9777837 -3.8821232 -3.88549 -3.9792397 -4.059279 -4.1005096 -4.113133 -4.1184964 -4.1162677][-4.1612377 -4.1709127 -4.1775618 -4.1713257 -4.1646957 -4.1494489 -4.1115251 -4.0666237 -4.0624104 -4.1041241 -4.14223 -4.1624 -4.1560721 -4.152597 -4.145083][-4.1864433 -4.1921482 -4.2004189 -4.20013 -4.2013817 -4.1972609 -4.1793714 -4.1625557 -4.1686635 -4.1878476 -4.2029881 -4.2088194 -4.1909313 -4.1807523 -4.1765442][-4.1960673 -4.1982789 -4.2026525 -4.2007484 -4.199038 -4.1957979 -4.1848321 -4.1809649 -4.1969151 -4.2169266 -4.2258711 -4.2230344 -4.2060351 -4.1987171 -4.1996894][-4.2051811 -4.1982541 -4.1938643 -4.1844049 -4.174808 -4.1660519 -4.15646 -4.1544623 -4.1740465 -4.1982694 -4.2079039 -4.2034516 -4.1928716 -4.1941023 -4.2031946][-4.2333441 -4.2166829 -4.2032442 -4.1823988 -4.1613069 -4.1464543 -4.13484 -4.1260056 -4.1365967 -4.1511321 -4.15704 -4.1561494 -4.1557741 -4.16399 -4.1774759][-4.2748141 -4.2567163 -4.2365556 -4.207396 -4.1771727 -4.1579576 -4.1434641 -4.1267085 -4.1171179 -4.1121593 -4.1058569 -4.1024561 -4.1059566 -4.1163745 -4.130229]]...]
INFO - root - 2017-12-05 14:37:31.100538: step 16910, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 75h:45m:25s remains)
INFO - root - 2017-12-05 14:37:39.453065: step 16920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 73h:33m:01s remains)
INFO - root - 2017-12-05 14:37:47.996043: step 16930, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 73h:58m:16s remains)
INFO - root - 2017-12-05 14:37:56.457322: step 16940, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 73h:53m:03s remains)
INFO - root - 2017-12-05 14:38:04.902011: step 16950, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 74h:36m:10s remains)
INFO - root - 2017-12-05 14:38:13.409222: step 16960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:09m:43s remains)
INFO - root - 2017-12-05 14:38:21.936824: step 16970, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 75h:38m:42s remains)
INFO - root - 2017-12-05 14:38:30.601638: step 16980, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 76h:00m:52s remains)
INFO - root - 2017-12-05 14:38:39.001750: step 16990, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.761 sec/batch; 66h:42m:57s remains)
INFO - root - 2017-12-05 14:38:47.483350: step 17000, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 74h:01m:45s remains)
2017-12-05 14:38:48.201986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2897854 -4.2890911 -4.2905307 -4.2909608 -4.2914019 -4.2967362 -4.3010063 -4.3017354 -4.3046703 -4.3075809 -4.3107123 -4.314157 -4.3185611 -4.320755 -4.32141][-4.2739148 -4.272912 -4.272284 -4.2697649 -4.2699771 -4.2747879 -4.2767878 -4.2769122 -4.2859383 -4.2949047 -4.3006744 -4.3072877 -4.3138509 -4.3170271 -4.3177094][-4.250689 -4.2463307 -4.240169 -4.2337289 -4.2332387 -4.2386975 -4.2375069 -4.2386403 -4.2558227 -4.2746706 -4.2863331 -4.297328 -4.3098021 -4.3157558 -4.3161788][-4.224328 -4.2133284 -4.1990142 -4.1867247 -4.1832047 -4.188292 -4.18573 -4.1925936 -4.2226887 -4.2533503 -4.27225 -4.2881112 -4.3060756 -4.3167405 -4.3161893][-4.1932845 -4.1729436 -4.147121 -4.1264253 -4.1149993 -4.1119332 -4.1067858 -4.1269646 -4.1820335 -4.23187 -4.2618141 -4.2819815 -4.3040595 -4.3179789 -4.3162608][-4.1460071 -4.1123843 -4.0722036 -4.0406294 -4.0178585 -3.9966946 -3.9744616 -4.007659 -4.1011066 -4.1820931 -4.2305489 -4.263195 -4.291894 -4.309546 -4.309092][-4.1042113 -4.0636988 -4.0114732 -3.9634485 -3.9206836 -3.8628032 -3.793817 -3.8282449 -3.9620571 -4.0850477 -4.1642966 -4.2184134 -4.2610679 -4.2877836 -4.2954397][-4.0767393 -4.0349131 -3.9795108 -3.9233162 -3.8650918 -3.7696736 -3.6406198 -3.6584868 -3.8159554 -3.975595 -4.0897317 -4.1666851 -4.2237368 -4.2620378 -4.2816515][-4.0955853 -4.0615292 -4.0201712 -3.974601 -3.9255755 -3.8318443 -3.6927173 -3.6718934 -3.7950187 -3.9444177 -4.0653419 -4.1480064 -4.20823 -4.2536869 -4.2804313][-4.1396074 -4.1148672 -4.0911913 -4.06447 -4.03408 -3.9709392 -3.8684394 -3.8324273 -3.8986835 -4.0059652 -4.1057458 -4.1764321 -4.2284851 -4.2689915 -4.2918873][-4.1835418 -4.1602244 -4.14797 -4.1325254 -4.11312 -4.0701656 -4.0027709 -3.9733238 -4.0088463 -4.0857172 -4.1630898 -4.2180867 -4.2598104 -4.2901812 -4.3037577][-4.2376823 -4.2172413 -4.2097683 -4.20064 -4.1856394 -4.1506805 -4.1012092 -4.0757928 -4.0949192 -4.1515818 -4.210865 -4.2538805 -4.2867241 -4.3073168 -4.3130074][-4.2787547 -4.2642837 -4.2594266 -4.2543111 -4.2447176 -4.2196455 -4.1835537 -4.1600628 -4.1699281 -4.2086234 -4.2527866 -4.2849693 -4.3078823 -4.3207455 -4.3228726][-4.2999859 -4.2925773 -4.2904749 -4.2874961 -4.2842455 -4.2714787 -4.2511148 -4.2345409 -4.2390947 -4.26375 -4.2941575 -4.3142667 -4.3251719 -4.3313088 -4.3320007][-4.3071756 -4.3034081 -4.3031392 -4.3031349 -4.3056741 -4.3055897 -4.300498 -4.2931657 -4.2946434 -4.3085747 -4.325264 -4.3350377 -4.3376288 -4.3379884 -4.3360105]]...]
INFO - root - 2017-12-05 14:38:56.675754: step 17010, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 75h:07m:50s remains)
INFO - root - 2017-12-05 14:39:05.287960: step 17020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 73h:43m:22s remains)
INFO - root - 2017-12-05 14:39:13.604254: step 17030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 75h:43m:41s remains)
INFO - root - 2017-12-05 14:39:22.075096: step 17040, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 76h:30m:59s remains)
INFO - root - 2017-12-05 14:39:30.573989: step 17050, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 76h:19m:06s remains)
INFO - root - 2017-12-05 14:39:39.102146: step 17060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 75h:29m:44s remains)
INFO - root - 2017-12-05 14:39:47.620277: step 17070, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:05m:07s remains)
INFO - root - 2017-12-05 14:39:56.197661: step 17080, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 74h:08m:29s remains)
INFO - root - 2017-12-05 14:40:04.625985: step 17090, loss = 2.09, batch loss = 2.03 (10.7 examples/sec; 0.746 sec/batch; 65h:20m:09s remains)
INFO - root - 2017-12-05 14:40:13.151720: step 17100, loss = 2.02, batch loss = 1.96 (9.3 examples/sec; 0.863 sec/batch; 75h:37m:27s remains)
2017-12-05 14:40:13.930290: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2663803 -4.2646027 -4.2612166 -4.258534 -4.2598906 -4.2637949 -4.2708621 -4.2758031 -4.2800045 -4.2814956 -4.2788134 -4.2755442 -4.2755919 -4.276752 -4.2780075][-4.2442751 -4.2328959 -4.2247124 -4.2173376 -4.2151365 -4.2160888 -4.2248716 -4.237164 -4.2501521 -4.2624025 -4.2685804 -4.2679605 -4.2678938 -4.2698817 -4.272521][-4.2165427 -4.1993203 -4.1839542 -4.1676016 -4.1576281 -4.15462 -4.166275 -4.1886263 -4.2167053 -4.2414632 -4.2577367 -4.2619076 -4.2610278 -4.2598896 -4.2612424][-4.1970782 -4.1802249 -4.157547 -4.1301336 -4.1095686 -4.0975223 -4.102963 -4.1322327 -4.1763039 -4.2124829 -4.2377667 -4.2476931 -4.2470155 -4.2403431 -4.2374439][-4.1939349 -4.1808348 -4.1524715 -4.1156435 -4.074964 -4.0373244 -4.0218372 -4.0524483 -4.1124473 -4.164978 -4.2019849 -4.2177587 -4.2147727 -4.2005515 -4.1928258][-4.1901841 -4.1814737 -4.1483307 -4.0968223 -4.0334325 -3.9658208 -3.9191527 -3.9477282 -4.0310807 -4.1028671 -4.1502604 -4.16976 -4.1617889 -4.1424494 -4.1340003][-4.1739345 -4.1701837 -4.1373329 -4.07815 -4.000175 -3.9125605 -3.8323302 -3.8480744 -3.9529402 -4.0437703 -4.1005306 -4.1214023 -4.1112404 -4.0958757 -4.0968628][-4.152339 -4.153976 -4.1335988 -4.0828562 -4.0080047 -3.9145143 -3.8069072 -3.7934408 -3.9060967 -4.0150867 -4.0788288 -4.0973825 -4.08633 -4.0724478 -4.0772104][-4.1497498 -4.1508751 -4.1456566 -4.1183844 -4.0614047 -3.9866133 -3.8812034 -3.829536 -3.9173856 -4.0277781 -4.0911794 -4.1069903 -4.0956411 -4.078023 -4.0761585][-4.1724648 -4.1715717 -4.1715841 -4.1646743 -4.1345673 -4.0900078 -4.0159478 -3.9551353 -3.9991968 -4.084291 -4.1348896 -4.1419258 -4.1219869 -4.0950732 -4.0869732][-4.1864643 -4.1838412 -4.1860428 -4.1847038 -4.1738114 -4.1582842 -4.1174312 -4.0747929 -4.0928326 -4.1452413 -4.1786408 -4.1740928 -4.1415215 -4.102798 -4.0869255][-4.1829128 -4.181324 -4.1852951 -4.1841145 -4.1812925 -4.1835237 -4.1670537 -4.144876 -4.1535139 -4.1818137 -4.1975517 -4.184361 -4.1455436 -4.1046171 -4.0855556][-4.1685419 -4.1717758 -4.1786141 -4.1807208 -4.1815963 -4.1871247 -4.1844316 -4.1788826 -4.1852713 -4.2001162 -4.20628 -4.1928439 -4.1633434 -4.1352468 -4.1237416][-4.1723728 -4.1824436 -4.195787 -4.204041 -4.207962 -4.2123437 -4.2157917 -4.2177773 -4.2229514 -4.2327871 -4.2351265 -4.22258 -4.2000155 -4.1816583 -4.17584][-4.1853247 -4.2001395 -4.2172236 -4.2312078 -4.2382412 -4.2423768 -4.2462091 -4.2474375 -4.2511172 -4.2593875 -4.2609563 -4.2524424 -4.2367458 -4.2256861 -4.2261]]...]
INFO - root - 2017-12-05 14:40:22.408437: step 17110, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 75h:17m:19s remains)
INFO - root - 2017-12-05 14:40:30.812937: step 17120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 76h:07m:55s remains)
INFO - root - 2017-12-05 14:40:39.283174: step 17130, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 75h:09m:46s remains)
INFO - root - 2017-12-05 14:40:47.688662: step 17140, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.871 sec/batch; 76h:16m:26s remains)
INFO - root - 2017-12-05 14:40:56.110454: step 17150, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 76h:15m:12s remains)
INFO - root - 2017-12-05 14:41:04.702373: step 17160, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 73h:47m:11s remains)
INFO - root - 2017-12-05 14:41:13.133135: step 17170, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 74h:12m:23s remains)
INFO - root - 2017-12-05 14:41:21.643024: step 17180, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 74h:47m:25s remains)
INFO - root - 2017-12-05 14:41:30.140780: step 17190, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 76h:45m:08s remains)
INFO - root - 2017-12-05 14:41:38.641986: step 17200, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 74h:19m:11s remains)
2017-12-05 14:41:39.488588: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.228961 -4.2259083 -4.2158294 -4.1991925 -4.1779623 -4.1635432 -4.1684761 -4.1899934 -4.2164297 -4.2388911 -4.2517605 -4.2589288 -4.2648525 -4.268198 -4.2693038][-4.2279639 -4.2232318 -4.2102838 -4.1878648 -4.1573443 -4.1342344 -4.1357622 -4.1614256 -4.1980596 -4.232481 -4.2547541 -4.2657652 -4.2710342 -4.2721934 -4.2716308][-4.2268724 -4.2213912 -4.20727 -4.1812496 -4.1437392 -4.1120811 -4.1053052 -4.1275983 -4.1698232 -4.215857 -4.2486949 -4.2657356 -4.2728949 -4.273551 -4.2727866][-4.2259965 -4.2213535 -4.2082419 -4.1816416 -4.1404939 -4.1019492 -4.0834417 -4.0953755 -4.1377664 -4.1939411 -4.2375617 -4.2621551 -4.272871 -4.2741246 -4.2734694][-4.22395 -4.2217264 -4.2112565 -4.1869016 -4.1465569 -4.1047869 -4.0758095 -4.0742135 -4.112195 -4.1750393 -4.2276254 -4.2589164 -4.2729912 -4.2747941 -4.274013][-4.2209592 -4.22133 -4.2133 -4.1929817 -4.1567225 -4.1159158 -4.0820484 -4.0687227 -4.0986791 -4.1624441 -4.2209973 -4.2577205 -4.27395 -4.2757392 -4.2747378][-4.223022 -4.2232518 -4.2146921 -4.1976056 -4.1668382 -4.1292777 -4.0945745 -4.0743012 -4.09538 -4.1552119 -4.2157893 -4.2566581 -4.2753983 -4.2776017 -4.2758446][-4.2299128 -4.2288666 -4.2182288 -4.2032571 -4.177866 -4.1433687 -4.1091232 -4.0865684 -4.1005454 -4.1528 -4.2116413 -4.2551131 -4.2770314 -4.2805181 -4.2781172][-4.23787 -4.2369957 -4.2260132 -4.2135406 -4.1928849 -4.1611686 -4.1268897 -4.1026731 -4.1084371 -4.1496258 -4.2041321 -4.2492433 -4.2755079 -4.2820959 -4.2801256][-4.2459078 -4.2465591 -4.2375331 -4.2285676 -4.2127757 -4.184598 -4.1500311 -4.1214218 -4.1160975 -4.1440964 -4.1914024 -4.2369738 -4.2682571 -4.2798152 -4.2799315][-4.2498174 -4.2525425 -4.2476826 -4.2437277 -4.2346811 -4.2130218 -4.1798816 -4.1466007 -4.1299324 -4.14463 -4.1833367 -4.2268 -4.2603736 -4.2751217 -4.2772679][-4.2470636 -4.2525487 -4.2515759 -4.251894 -4.2496305 -4.2365022 -4.2083216 -4.1748781 -4.151227 -4.1544747 -4.1835647 -4.2223296 -4.2541981 -4.269979 -4.27373][-4.2391109 -4.247304 -4.2486014 -4.2513008 -4.253612 -4.2491379 -4.2299137 -4.2023892 -4.1781859 -4.1740222 -4.1932645 -4.2247777 -4.2525134 -4.2672973 -4.2717581][-4.234911 -4.2429347 -4.2440562 -4.2471547 -4.2515969 -4.2534871 -4.2439108 -4.226006 -4.2071304 -4.2017288 -4.2131915 -4.2356319 -4.2568893 -4.2682023 -4.2716713][-4.2372408 -4.2416811 -4.241003 -4.2429662 -4.2473278 -4.2517309 -4.2489271 -4.239531 -4.2288947 -4.2265291 -4.2351327 -4.2506418 -4.2649446 -4.2717161 -4.2726736]]...]
INFO - root - 2017-12-05 14:41:48.022085: step 17210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 75h:23m:10s remains)
INFO - root - 2017-12-05 14:41:56.549350: step 17220, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 77h:33m:10s remains)
INFO - root - 2017-12-05 14:42:04.972376: step 17230, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 72h:52m:08s remains)
INFO - root - 2017-12-05 14:42:13.318638: step 17240, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 74h:14m:09s remains)
INFO - root - 2017-12-05 14:42:21.817750: step 17250, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 76h:19m:56s remains)
INFO - root - 2017-12-05 14:42:30.347901: step 17260, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 74h:23m:04s remains)
INFO - root - 2017-12-05 14:42:38.802465: step 17270, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 74h:28m:23s remains)
INFO - root - 2017-12-05 14:42:47.225340: step 17280, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 73h:13m:02s remains)
INFO - root - 2017-12-05 14:42:55.731125: step 17290, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 73h:26m:22s remains)
INFO - root - 2017-12-05 14:43:04.261535: step 17300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 75h:35m:37s remains)
2017-12-05 14:43:05.046964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2366714 -4.2553673 -4.2682853 -4.2685266 -4.2643571 -4.2766986 -4.2904005 -4.2895255 -4.2829223 -4.2793679 -4.2721243 -4.2653527 -4.2681108 -4.2701707 -4.2626433][-4.2546391 -4.2660327 -4.2751732 -4.2760892 -4.2720203 -4.2837009 -4.2981029 -4.295764 -4.2822008 -4.2692347 -4.253006 -4.2401466 -4.238637 -4.2370796 -4.2292032][-4.2581687 -4.2626729 -4.2686133 -4.2681413 -4.2601008 -4.2666688 -4.2814174 -4.2836614 -4.273653 -4.2631369 -4.2438335 -4.2267289 -4.2215209 -4.2183037 -4.2123652][-4.2493887 -4.2474952 -4.2513914 -4.2513447 -4.2412133 -4.2398868 -4.2513986 -4.2596221 -4.2578959 -4.2548213 -4.2384233 -4.2225184 -4.2167277 -4.2129645 -4.2101884][-4.2259517 -4.2175117 -4.2195134 -4.2223039 -4.2159324 -4.2129421 -4.2214723 -4.2342949 -4.2409191 -4.246109 -4.2386808 -4.2288628 -4.223455 -4.218051 -4.2148137][-4.1855726 -4.1675768 -4.161088 -4.1614003 -4.1589332 -4.1612616 -4.1701961 -4.1867647 -4.202034 -4.2172074 -4.2216582 -4.2216682 -4.2213945 -4.2175155 -4.2147293][-4.1445351 -4.1114955 -4.0899296 -4.0789843 -4.0718164 -4.0746908 -4.0814185 -4.1000566 -4.1236887 -4.1511574 -4.1707692 -4.1848488 -4.1959076 -4.2029333 -4.2091537][-4.1175876 -4.0704126 -4.0332646 -4.0116358 -3.9963191 -3.9963202 -3.9989378 -4.0137568 -4.0438967 -4.0912228 -4.1361628 -4.1699276 -4.1969595 -4.217207 -4.2311273][-4.1148481 -4.0637527 -4.0193605 -3.9930949 -3.9751053 -3.9703214 -3.9722548 -3.9807608 -4.0060911 -4.0637546 -4.1261735 -4.1749935 -4.2144847 -4.2426996 -4.259902][-4.1466017 -4.1040826 -4.0678687 -4.048337 -4.0344434 -4.0297856 -4.0364275 -4.042788 -4.0539846 -4.0974283 -4.1515322 -4.1970067 -4.2319374 -4.2566175 -4.2733335][-4.1853552 -4.1574326 -4.1359611 -4.1276464 -4.1224914 -4.1253872 -4.1414065 -4.1496048 -4.1523833 -4.1731119 -4.2047787 -4.2332869 -4.2513371 -4.2628465 -4.2736912][-4.2023654 -4.1879416 -4.1778884 -4.1772089 -4.1799679 -4.1920252 -4.2180753 -4.2308178 -4.2296419 -4.2335777 -4.2416611 -4.2501779 -4.251575 -4.2511024 -4.254776][-4.181169 -4.1765418 -4.1761036 -4.1840186 -4.1979618 -4.2180214 -4.2512403 -4.2678075 -4.2676206 -4.2639127 -4.2574081 -4.2513967 -4.240509 -4.2316532 -4.2304349][-4.14647 -4.1495566 -4.152782 -4.162643 -4.1790247 -4.20007 -4.2328343 -4.2519565 -4.2561145 -4.2523179 -4.2419424 -4.2305713 -4.2151117 -4.2040348 -4.1998096][-4.1033659 -4.1116638 -4.1154413 -4.1278496 -4.1429896 -4.1636105 -4.1934748 -4.2133141 -4.2190862 -4.2155886 -4.203999 -4.1927843 -4.1788034 -4.1703343 -4.1653733]]...]
INFO - root - 2017-12-05 14:43:13.546137: step 17310, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 76h:42m:16s remains)
INFO - root - 2017-12-05 14:43:22.100484: step 17320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 74h:03m:08s remains)
INFO - root - 2017-12-05 14:43:30.582621: step 17330, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 75h:28m:33s remains)
INFO - root - 2017-12-05 14:43:39.107679: step 17340, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 75h:05m:10s remains)
INFO - root - 2017-12-05 14:43:47.575861: step 17350, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 76h:01m:32s remains)
INFO - root - 2017-12-05 14:43:55.974613: step 17360, loss = 2.06, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 79h:15m:58s remains)
INFO - root - 2017-12-05 14:44:04.542902: step 17370, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 75h:08m:44s remains)
INFO - root - 2017-12-05 14:44:13.034863: step 17380, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 71h:46m:19s remains)
INFO - root - 2017-12-05 14:44:21.333077: step 17390, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 74h:05m:25s remains)
INFO - root - 2017-12-05 14:44:29.809278: step 17400, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 73h:51m:26s remains)
2017-12-05 14:44:30.608759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1394053 -4.1207724 -4.0926113 -4.0739012 -4.08998 -4.1341643 -4.1811595 -4.2149367 -4.2458658 -4.2616005 -4.2581615 -4.2489281 -4.2467866 -4.2474804 -4.2460527][-4.1485219 -4.1260662 -4.0966191 -4.0791492 -4.0931482 -4.1344295 -4.1815104 -4.2174792 -4.2465191 -4.2639728 -4.2615476 -4.250917 -4.2466054 -4.2449603 -4.2424331][-4.1800056 -4.1552587 -4.1270361 -4.1106658 -4.1163216 -4.1427445 -4.1786203 -4.2080584 -4.22935 -4.2465177 -4.2474995 -4.2368445 -4.2309775 -4.2296987 -4.2290711][-4.207778 -4.1815972 -4.1563072 -4.1397858 -4.1332994 -4.13772 -4.152019 -4.1721849 -4.1929474 -4.2157941 -4.22299 -4.2121263 -4.2038507 -4.1999025 -4.2009068][-4.21308 -4.1858239 -4.1662989 -4.1482344 -4.1257977 -4.1014977 -4.0908046 -4.1027565 -4.1321216 -4.1637812 -4.1787481 -4.1702733 -4.1599984 -4.1553144 -4.1598063][-4.1906381 -4.161025 -4.1427078 -4.1177907 -4.0712023 -4.0090303 -3.9625032 -3.9723792 -4.0344834 -4.0923352 -4.1222763 -4.1195927 -4.1033664 -4.096405 -4.1038704][-4.1470251 -4.1091166 -4.086071 -4.0514741 -3.981926 -3.875 -3.7775853 -3.7979128 -3.9265375 -4.0266471 -4.0723467 -4.0724845 -4.0503821 -4.0343442 -4.0501409][-4.1187944 -4.0757513 -4.047029 -4.0102334 -3.9337862 -3.7966256 -3.6491778 -3.6821396 -3.8672652 -3.9924057 -4.0452728 -4.0475063 -4.0262623 -4.0117068 -4.0388465][-4.135128 -4.0980821 -4.0702038 -4.0449295 -3.9903033 -3.8764374 -3.7509508 -3.7851958 -3.9352255 -4.029223 -4.062789 -4.057394 -4.0435977 -4.0415716 -4.0707946][-4.161016 -4.1374464 -4.1207404 -4.113821 -4.0876656 -4.016407 -3.9381871 -3.9600325 -4.0436206 -4.094934 -4.1042957 -4.0828996 -4.0751495 -4.0885978 -4.1149817][-4.1714678 -4.1611891 -4.1602578 -4.1689758 -4.1606503 -4.1242824 -4.081471 -4.0858335 -4.1184869 -4.1387525 -4.1321993 -4.1043415 -4.1018186 -4.1258364 -4.1464491][-4.1749444 -4.1694078 -4.1755095 -4.1924539 -4.1983528 -4.185041 -4.1639023 -4.1602912 -4.1681809 -4.1721029 -4.1595807 -4.1364565 -4.1395149 -4.1654158 -4.18139][-4.18305 -4.1812234 -4.1854253 -4.1995215 -4.2113123 -4.2126474 -4.2064009 -4.20159 -4.201695 -4.203352 -4.1956706 -4.1844864 -4.1867876 -4.205801 -4.2165632][-4.19591 -4.1978226 -4.2004981 -4.2089381 -4.2201543 -4.2275424 -4.2294531 -4.2264676 -4.2247496 -4.2296004 -4.2305889 -4.2282104 -4.2330713 -4.2450376 -4.2501259][-4.2227459 -4.2242742 -4.2242308 -4.2287807 -4.236445 -4.24466 -4.2525396 -4.2519293 -4.2491007 -4.2544503 -4.2596922 -4.2633219 -4.2692966 -4.2747726 -4.275351]]...]
INFO - root - 2017-12-05 14:44:39.159886: step 17410, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 75h:13m:51s remains)
INFO - root - 2017-12-05 14:44:47.722202: step 17420, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 73h:52m:11s remains)
INFO - root - 2017-12-05 14:44:56.272467: step 17430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 75h:06m:10s remains)
INFO - root - 2017-12-05 14:45:04.839350: step 17440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 73h:34m:46s remains)
INFO - root - 2017-12-05 14:45:13.242638: step 17450, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 71h:43m:24s remains)
INFO - root - 2017-12-05 14:45:21.786557: step 17460, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 74h:14m:55s remains)
INFO - root - 2017-12-05 14:45:30.260320: step 17470, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 77h:32m:40s remains)
INFO - root - 2017-12-05 14:45:38.723739: step 17480, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 75h:06m:39s remains)
INFO - root - 2017-12-05 14:45:47.071835: step 17490, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 73h:31m:51s remains)
INFO - root - 2017-12-05 14:45:55.618761: step 17500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 75h:23m:58s remains)
2017-12-05 14:45:56.400285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2006598 -4.2014952 -4.2050238 -4.2144523 -4.2187424 -4.2069383 -4.1909761 -4.1841431 -4.1858082 -4.1931758 -4.1995616 -4.2007022 -4.1989975 -4.19311 -4.197845][-4.1684971 -4.1801543 -4.190361 -4.2028251 -4.2041507 -4.1811457 -4.1511889 -4.1341586 -4.1349115 -4.1495361 -4.1634178 -4.1659722 -4.15989 -4.1523976 -4.1587133][-4.1396666 -4.1619058 -4.18011 -4.1970215 -4.1960235 -4.1589894 -4.108604 -4.0825286 -4.094841 -4.1225095 -4.1426353 -4.1400666 -4.1247587 -4.1126003 -4.1228051][-4.1105542 -4.1425071 -4.1709828 -4.1881895 -4.1762676 -4.1176658 -4.043622 -4.0115376 -4.046185 -4.0977559 -4.1288462 -4.1230145 -4.0951886 -4.0718403 -4.0896716][-4.106513 -4.1442428 -4.1763172 -4.188416 -4.1572485 -4.0661931 -3.9531007 -3.9136314 -3.9848986 -4.0715971 -4.1215744 -4.1174712 -4.0831261 -4.0506763 -4.0761614][-4.1293488 -4.1671782 -4.1949635 -4.1972976 -4.1409235 -4.0026774 -3.8350313 -3.8022654 -3.9281082 -4.052701 -4.116251 -4.1148524 -4.0752392 -4.0359406 -4.0653872][-4.1624331 -4.1917834 -4.2032638 -4.1868539 -4.0996966 -3.9105172 -3.6852388 -3.6930161 -3.8884566 -4.0443358 -4.1145144 -4.1169095 -4.0727978 -4.0269842 -4.0556092][-4.1909046 -4.210567 -4.2028651 -4.1633968 -4.0508103 -3.8301764 -3.5929046 -3.6644349 -3.8972516 -4.0518565 -4.11195 -4.1119156 -4.0670247 -4.0201674 -4.0524549][-4.20298 -4.2140088 -4.1965165 -4.1454148 -4.0347791 -3.8556023 -3.7123122 -3.799768 -3.9752197 -4.08228 -4.1179271 -4.106451 -4.0568728 -4.0165863 -4.0554571][-4.1854873 -4.1942325 -4.174438 -4.1217971 -4.0360184 -3.9361002 -3.8936505 -3.9662673 -4.0652418 -4.1199141 -4.1286259 -4.1044817 -4.0535398 -4.021986 -4.0629282][-4.1518121 -4.1653132 -4.1519175 -4.1068354 -4.0537682 -4.0192585 -4.0236082 -4.0687642 -4.1199517 -4.14495 -4.1379113 -4.1031756 -4.0549092 -4.0309424 -4.0727987][-4.1204667 -4.140408 -4.1414695 -4.1164746 -4.0923448 -4.0877724 -4.0990171 -4.1227851 -4.1445332 -4.1508169 -4.1357241 -4.0991149 -4.0601788 -4.0461421 -4.0892839][-4.1025658 -4.1288633 -4.1386647 -4.1344037 -4.1321573 -4.1335912 -4.1386189 -4.1495256 -4.1531863 -4.1456146 -4.1260858 -4.0961227 -4.0701137 -4.0661755 -4.1098719][-4.1138391 -4.141468 -4.1609545 -4.1720705 -4.1782622 -4.177834 -4.1805878 -4.1846194 -4.1770916 -4.1597576 -4.1376038 -4.1194072 -4.1097345 -4.1098833 -4.1474423][-4.1371069 -4.16481 -4.1908717 -4.2103715 -4.2214389 -4.2218227 -4.2250485 -4.2264 -4.2135553 -4.1927714 -4.1761847 -4.1721215 -4.1714797 -4.1706357 -4.19427]]...]
INFO - root - 2017-12-05 14:46:04.951073: step 17510, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 72h:30m:45s remains)
INFO - root - 2017-12-05 14:46:13.485074: step 17520, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 73h:10m:31s remains)
INFO - root - 2017-12-05 14:46:22.139194: step 17530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 76h:10m:51s remains)
INFO - root - 2017-12-05 14:46:30.812060: step 17540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 75h:27m:10s remains)
INFO - root - 2017-12-05 14:46:39.278033: step 17550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 74h:21m:50s remains)
INFO - root - 2017-12-05 14:46:47.756856: step 17560, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 76h:06m:52s remains)
INFO - root - 2017-12-05 14:46:56.264339: step 17570, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 75h:35m:27s remains)
INFO - root - 2017-12-05 14:47:04.868787: step 17580, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 77h:25m:44s remains)
INFO - root - 2017-12-05 14:47:13.439631: step 17590, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 74h:51m:44s remains)
INFO - root - 2017-12-05 14:47:22.004971: step 17600, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 75h:07m:27s remains)
2017-12-05 14:47:22.744865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.262249 -4.2624474 -4.2573848 -4.2448516 -4.2279272 -4.2222691 -4.2283363 -4.2341237 -4.2398148 -4.2477508 -4.2476659 -4.2388091 -4.2302942 -4.2324963 -4.2561297][-4.244019 -4.2451797 -4.2377415 -4.2181149 -4.18839 -4.1720114 -4.1794839 -4.1950846 -4.2096796 -4.2200527 -4.2187443 -4.2051253 -4.1897984 -4.1912 -4.2220874][-4.2273741 -4.2289782 -4.2211895 -4.1973796 -4.1556277 -4.1227584 -4.1284804 -4.1572738 -4.1844835 -4.1996202 -4.19899 -4.1827679 -4.161839 -4.1620526 -4.1978006][-4.2016935 -4.2034836 -4.1944246 -4.1643591 -4.1113696 -4.0634413 -4.0672269 -4.1121974 -4.1542935 -4.1777849 -4.1823158 -4.1673565 -4.1433115 -4.1419263 -4.1809659][-4.1732807 -4.170702 -4.1569309 -4.1173263 -4.0510044 -3.990593 -3.99893 -4.0637059 -4.1212673 -4.1549845 -4.1675167 -4.1540232 -4.1310225 -4.1315384 -4.1729355][-4.1660738 -4.1577883 -4.1366796 -4.08802 -4.0089869 -3.9351161 -3.949899 -4.0303512 -4.1017704 -4.1463103 -4.1656041 -4.1534724 -4.1332045 -4.1369104 -4.1778126][-4.1724629 -4.160686 -4.137229 -4.0855775 -3.9968 -3.9130256 -3.9280698 -4.0148311 -4.0938463 -4.1464071 -4.1706839 -4.1628265 -4.1471267 -4.1520782 -4.1886334][-4.1737714 -4.1646571 -4.1493845 -4.1045828 -4.0222845 -3.9448948 -3.9553585 -4.0297718 -4.10441 -4.1584759 -4.184063 -4.1768413 -4.1616969 -4.16528 -4.1977663][-4.1829104 -4.1783743 -4.1704512 -4.135407 -4.0656614 -4.0011597 -4.0105891 -4.0684586 -4.1299543 -4.1778607 -4.1976624 -4.1868467 -4.1707959 -4.1742606 -4.2061205][-4.205616 -4.2039485 -4.1990671 -4.1715155 -4.1113424 -4.05682 -4.0651784 -4.1071496 -4.1538286 -4.1932149 -4.2061477 -4.1914191 -4.1762648 -4.1819153 -4.2152929][-4.2260976 -4.2263961 -4.2243814 -4.2034893 -4.1540651 -4.110703 -4.1155753 -4.1397018 -4.1691594 -4.2019582 -4.2094283 -4.1946616 -4.1823344 -4.1915293 -4.2264414][-4.2400289 -4.2439961 -4.2475462 -4.2335625 -4.1979604 -4.1655097 -4.166163 -4.1755824 -4.1921625 -4.2178836 -4.223166 -4.2118979 -4.2036262 -4.2158809 -4.2490587][-4.2514529 -4.2592583 -4.2659411 -4.2552671 -4.2294755 -4.205214 -4.2015009 -4.2023897 -4.2126331 -4.2331519 -4.2420235 -4.2381 -4.2366753 -4.2519913 -4.2794089][-4.2625551 -4.2724075 -4.2779064 -4.2670422 -4.2456579 -4.22684 -4.2191343 -4.2157526 -4.2251258 -4.2440114 -4.2589474 -4.2626143 -4.2671609 -4.2826614 -4.3039393][-4.2742081 -4.280879 -4.2844467 -4.277142 -4.2629104 -4.2507071 -4.2447896 -4.2411985 -4.245955 -4.26035 -4.2753181 -4.2829776 -4.2910938 -4.3053575 -4.3211923]]...]
INFO - root - 2017-12-05 14:47:31.394677: step 17610, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 76h:04m:02s remains)
INFO - root - 2017-12-05 14:47:39.916510: step 17620, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.827 sec/batch; 72h:19m:33s remains)
INFO - root - 2017-12-05 14:47:48.448073: step 17630, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 74h:40m:58s remains)
INFO - root - 2017-12-05 14:47:57.009908: step 17640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:52m:18s remains)
INFO - root - 2017-12-05 14:48:05.586744: step 17650, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 74h:53m:44s remains)
INFO - root - 2017-12-05 14:48:14.248679: step 17660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:57m:56s remains)
INFO - root - 2017-12-05 14:48:22.792211: step 17670, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 74h:58m:08s remains)
INFO - root - 2017-12-05 14:48:31.372538: step 17680, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 71h:41m:26s remains)
INFO - root - 2017-12-05 14:48:39.934503: step 17690, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 76h:37m:58s remains)
INFO - root - 2017-12-05 14:48:48.612241: step 17700, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 78h:01m:56s remains)
2017-12-05 14:48:49.405200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.188014 -4.1935062 -4.1902857 -4.1663561 -4.15373 -4.1691394 -4.1961842 -4.2222629 -4.2319365 -4.219944 -4.207047 -4.1997857 -4.204977 -4.22489 -4.2353063][-4.1763496 -4.1827025 -4.1771522 -4.1541243 -4.1459036 -4.1668062 -4.1947575 -4.211987 -4.2109966 -4.1963491 -4.1927676 -4.1942992 -4.2035818 -4.2249928 -4.2374258][-4.1627378 -4.1682868 -4.1582713 -4.1384726 -4.13864 -4.162467 -4.1859121 -4.189857 -4.1796141 -4.1701603 -4.1782217 -4.1887493 -4.2025504 -4.2252731 -4.2389607][-4.1494317 -4.1574039 -4.1460786 -4.12636 -4.1309824 -4.1501441 -4.1632977 -4.1557713 -4.1438518 -4.1459055 -4.1655974 -4.1874576 -4.2034025 -4.2205882 -4.227356][-4.1371331 -4.1512 -4.142323 -4.12206 -4.1219764 -4.1371403 -4.1457157 -4.1348453 -4.1290331 -4.1408386 -4.16453 -4.1890936 -4.2046614 -4.2132678 -4.2095308][-4.1375723 -4.1512022 -4.1391234 -4.1132803 -4.1058569 -4.1173687 -4.1258698 -4.1186004 -4.1226134 -4.1437349 -4.1618576 -4.1765447 -4.1928535 -4.2077956 -4.2066789][-4.134944 -4.1395369 -4.1232915 -4.0954685 -4.08472 -4.0902605 -4.0967717 -4.09703 -4.1123004 -4.1365576 -4.1479096 -4.1540127 -4.1757612 -4.2036381 -4.2097049][-4.1204205 -4.11737 -4.1008668 -4.0779214 -4.0655737 -4.0634565 -4.0700016 -4.07798 -4.1013875 -4.1292729 -4.1403756 -4.14819 -4.1749878 -4.2080493 -4.2150412][-4.1100326 -4.1018939 -4.0926991 -4.0785751 -4.0695491 -4.0633855 -4.066329 -4.0794115 -4.1067362 -4.1350336 -4.1473794 -4.1576319 -4.175427 -4.199554 -4.2038884][-4.1154284 -4.1059012 -4.1002564 -4.0923281 -4.0872488 -4.0824447 -4.0806241 -4.0894256 -4.1128955 -4.1371303 -4.1480303 -4.1564231 -4.1644378 -4.1801548 -4.1835003][-4.1323514 -4.1265912 -4.1197958 -4.1088085 -4.1035814 -4.10432 -4.103488 -4.1027584 -4.1201825 -4.1418076 -4.1507974 -4.1546173 -4.1594405 -4.1721935 -4.1731849][-4.1440649 -4.1478114 -4.1392088 -4.1268973 -4.1193633 -4.1232405 -4.1257463 -4.1202378 -4.127984 -4.1433039 -4.1505013 -4.1535654 -4.1527796 -4.1573577 -4.1522317][-4.1322522 -4.1520767 -4.1505079 -4.1419735 -4.136363 -4.1409049 -4.14385 -4.1340685 -4.1304579 -4.1349845 -4.13916 -4.1403112 -4.1312265 -4.1205254 -4.1085491][-4.1037178 -4.1384206 -4.1483531 -4.1446428 -4.1446347 -4.1509356 -4.1542511 -4.1416612 -4.1286621 -4.1213331 -4.1210785 -4.1219511 -4.1098814 -4.0878644 -4.0665445][-4.0809746 -4.1173038 -4.1294646 -4.1301351 -4.1353159 -4.1458426 -4.1492996 -4.1344781 -4.115407 -4.0975642 -4.0962744 -4.1053519 -4.099824 -4.0800643 -4.0544662]]...]
INFO - root - 2017-12-05 14:48:58.053398: step 17710, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 76h:29m:31s remains)
INFO - root - 2017-12-05 14:49:06.635036: step 17720, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 75h:13m:57s remains)
INFO - root - 2017-12-05 14:49:15.243159: step 17730, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 73h:37m:02s remains)
INFO - root - 2017-12-05 14:49:23.687680: step 17740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 74h:06m:43s remains)
INFO - root - 2017-12-05 14:49:32.394033: step 17750, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 76h:32m:53s remains)
INFO - root - 2017-12-05 14:49:41.011640: step 17760, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 75h:40m:50s remains)
INFO - root - 2017-12-05 14:49:49.476205: step 17770, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 74h:10m:24s remains)
INFO - root - 2017-12-05 14:49:58.042503: step 17780, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 75h:17m:25s remains)
INFO - root - 2017-12-05 14:50:06.409723: step 17790, loss = 2.08, batch loss = 2.02 (11.3 examples/sec; 0.709 sec/batch; 62h:00m:03s remains)
INFO - root - 2017-12-05 14:50:14.931941: step 17800, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 74h:01m:23s remains)
2017-12-05 14:50:15.693434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2884278 -4.3014874 -4.3102264 -4.3126531 -4.3074346 -4.2970986 -4.2875848 -4.2848649 -4.288579 -4.2958045 -4.302309 -4.3069205 -4.3091908 -4.3101435 -4.3109751][-4.2852893 -4.2958355 -4.3031034 -4.3051338 -4.2998948 -4.2894912 -4.2796631 -4.2772837 -4.2821212 -4.2913136 -4.3003645 -4.3078051 -4.3119903 -4.3138347 -4.3152146][-4.283977 -4.2919459 -4.2984037 -4.3009968 -4.2965183 -4.2863703 -4.2760334 -4.2733479 -4.2782187 -4.287694 -4.2972574 -4.3050904 -4.3094988 -4.3110509 -4.3123593][-4.28006 -4.2882414 -4.2963524 -4.3005829 -4.2957234 -4.282547 -4.2681384 -4.2624059 -4.266058 -4.2768803 -4.2905836 -4.3010769 -4.3055863 -4.3053155 -4.3055978][-4.2670946 -4.2793274 -4.2909513 -4.2961073 -4.2882609 -4.2669325 -4.2423115 -4.2281752 -4.2313051 -4.2503915 -4.2763014 -4.2954507 -4.3028383 -4.30111 -4.2991829][-4.2425723 -4.260993 -4.2756577 -4.2790341 -4.2633038 -4.2247796 -4.1778011 -4.1476984 -4.1528568 -4.1926408 -4.2450953 -4.2836113 -4.29916 -4.2973027 -4.292511][-4.2201633 -4.2409892 -4.2558112 -4.254209 -4.2253971 -4.1601272 -4.0774822 -4.0204434 -4.0293641 -4.101963 -4.1938224 -4.2595096 -4.2870412 -4.2877803 -4.2812529][-4.2170992 -4.2330375 -4.2432947 -4.233604 -4.1897855 -4.0957317 -3.9743826 -3.8847153 -3.8962445 -4.0063648 -4.139204 -4.2317843 -4.2710829 -4.2761135 -4.2695537][-4.2375093 -4.244041 -4.2479973 -4.2311697 -4.178628 -4.0729938 -3.9381049 -3.8349509 -3.847681 -3.9764044 -4.1254249 -4.2264929 -4.2681003 -4.27324 -4.2653904][-4.2639408 -4.2619281 -4.2616372 -4.2464128 -4.2020416 -4.1128521 -4.001884 -3.922163 -3.9374018 -4.0445008 -4.1666241 -4.2479053 -4.2777586 -4.2776203 -4.2681251][-4.2797303 -4.2732658 -4.2726173 -4.2659445 -4.2411604 -4.1866469 -4.1190319 -4.0746169 -4.0895762 -4.1571693 -4.2322497 -4.2785387 -4.2896237 -4.2822919 -4.2716455][-4.28566 -4.2779455 -4.2776856 -4.2780094 -4.2706547 -4.2488184 -4.2199039 -4.2029495 -4.2148757 -4.2489634 -4.2838707 -4.3006468 -4.2969937 -4.2848063 -4.2740054][-4.2910037 -4.2837329 -4.282774 -4.2846727 -4.2847371 -4.2794719 -4.2711911 -4.267797 -4.2763233 -4.2913051 -4.3043709 -4.3060584 -4.2971539 -4.2847805 -4.2751346][-4.3003044 -4.2947545 -4.2930455 -4.2934184 -4.2931376 -4.2911444 -4.28841 -4.2880936 -4.2924557 -4.298233 -4.3023839 -4.3003182 -4.2927179 -4.2824917 -4.2741432][-4.3082285 -4.3044734 -4.3019514 -4.30066 -4.2988858 -4.2964067 -4.2942243 -4.2941127 -4.2951989 -4.295331 -4.294313 -4.2917895 -4.286366 -4.277504 -4.2701244]]...]
INFO - root - 2017-12-05 14:50:24.380071: step 17810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 76h:11m:16s remains)
INFO - root - 2017-12-05 14:50:32.940075: step 17820, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 76h:12m:58s remains)
INFO - root - 2017-12-05 14:50:41.483991: step 17830, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 74h:16m:05s remains)
INFO - root - 2017-12-05 14:50:50.087497: step 17840, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 73h:49m:44s remains)
INFO - root - 2017-12-05 14:50:58.643413: step 17850, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 76h:09m:42s remains)
INFO - root - 2017-12-05 14:51:07.084571: step 17860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 73h:15m:40s remains)
INFO - root - 2017-12-05 14:51:15.593967: step 17870, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 74h:27m:41s remains)
INFO - root - 2017-12-05 14:51:24.154268: step 17880, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 78h:27m:08s remains)
INFO - root - 2017-12-05 14:51:32.604140: step 17890, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 73h:58m:02s remains)
INFO - root - 2017-12-05 14:51:41.123155: step 17900, loss = 2.07, batch loss = 2.01 (10.1 examples/sec; 0.792 sec/batch; 69h:11m:53s remains)
2017-12-05 14:51:41.802546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1987133 -4.2046585 -4.2258611 -4.2453685 -4.24506 -4.2344542 -4.2274885 -4.2198982 -4.20519 -4.184998 -4.1625247 -4.15561 -4.1705604 -4.2026634 -4.23718][-4.21792 -4.2181206 -4.2336016 -4.2516642 -4.2528229 -4.2438235 -4.2405071 -4.2354183 -4.2209268 -4.2016587 -4.1827035 -4.1711307 -4.1812625 -4.2116175 -4.242795][-4.245389 -4.2393517 -4.2466583 -4.257555 -4.2586942 -4.2572465 -4.2596779 -4.2581844 -4.2473254 -4.2319074 -4.2165971 -4.2014651 -4.2040906 -4.2285848 -4.2523303][-4.2665892 -4.2632031 -4.2678485 -4.2700582 -4.2672405 -4.267983 -4.2701988 -4.2726073 -4.2708354 -4.2666259 -4.2592421 -4.2438579 -4.2402086 -4.2559643 -4.2689347][-4.2570276 -4.2617612 -4.2698259 -4.2740431 -4.2703323 -4.2688022 -4.2685313 -4.2730308 -4.2805877 -4.2870579 -4.2930083 -4.2884874 -4.2841926 -4.2917743 -4.2938819][-4.2117405 -4.2246461 -4.2395411 -4.2462497 -4.2445545 -4.2453594 -4.2455845 -4.2492266 -4.2610087 -4.2754316 -4.2955484 -4.3073053 -4.3118253 -4.3162618 -4.3140779][-4.155694 -4.1735229 -4.1938519 -4.1997542 -4.1961279 -4.2003717 -4.2061062 -4.2130156 -4.2297482 -4.2516713 -4.2788181 -4.3022985 -4.3100967 -4.3090878 -4.3076816][-4.1170058 -4.1333275 -4.153594 -4.1527681 -4.1445112 -4.1489482 -4.1572237 -4.1675754 -4.1897521 -4.2137103 -4.2434945 -4.2728043 -4.2835765 -4.2854362 -4.2918615][-4.1204338 -4.1282945 -4.1353774 -4.1214476 -4.107204 -4.1115265 -4.117619 -4.1253915 -4.1482139 -4.1719165 -4.2020597 -4.2337837 -4.2488575 -4.2565503 -4.2684011][-4.1657376 -4.163218 -4.1583691 -4.135931 -4.1169872 -4.1157722 -4.1154237 -4.1165242 -4.1300964 -4.1443877 -4.1680436 -4.1949368 -4.2112293 -4.2251754 -4.2436409][-4.1992464 -4.1958666 -4.1899352 -4.1716137 -4.1576495 -4.15423 -4.1522136 -4.1501222 -4.1529236 -4.1524763 -4.1550932 -4.1609979 -4.1714325 -4.1909585 -4.2161617][-4.1768384 -4.1833234 -4.1884441 -4.1841125 -4.1811018 -4.1798706 -4.1764774 -4.1697993 -4.1609154 -4.1462035 -4.127038 -4.1118832 -4.1128244 -4.1376071 -4.1727862][-4.1555686 -4.1689482 -4.1770215 -4.1787448 -4.1815329 -4.1807604 -4.1751618 -4.1652684 -4.1510019 -4.1308775 -4.1034966 -4.0788131 -4.0744028 -4.0999846 -4.1387415][-4.1822314 -4.1917319 -4.1950994 -4.1963716 -4.1992412 -4.1986475 -4.19322 -4.1856532 -4.175571 -4.1564689 -4.13143 -4.1055574 -4.099473 -4.1171241 -4.1465836][-4.2012138 -4.2060614 -4.2054133 -4.2041812 -4.2046237 -4.2031689 -4.1985288 -4.1930966 -4.187109 -4.1712685 -4.1499481 -4.1293592 -4.1269565 -4.1419287 -4.159235]]...]
INFO - root - 2017-12-05 14:51:50.247335: step 17910, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 72h:58m:35s remains)
INFO - root - 2017-12-05 14:51:58.769077: step 17920, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 73h:49m:45s remains)
INFO - root - 2017-12-05 14:52:07.261262: step 17930, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 76h:36m:37s remains)
INFO - root - 2017-12-05 14:52:15.825423: step 17940, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 73h:24m:12s remains)
INFO - root - 2017-12-05 14:52:24.457584: step 17950, loss = 2.06, batch loss = 2.00 (8.6 examples/sec; 0.929 sec/batch; 81h:11m:43s remains)
INFO - root - 2017-12-05 14:52:33.019224: step 17960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 75h:26m:18s remains)
INFO - root - 2017-12-05 14:52:41.648958: step 17970, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:41m:27s remains)
INFO - root - 2017-12-05 14:52:50.193590: step 17980, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.815 sec/batch; 71h:11m:25s remains)
INFO - root - 2017-12-05 14:52:58.592247: step 17990, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 71h:31m:52s remains)
INFO - root - 2017-12-05 14:53:07.146506: step 18000, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 74h:58m:59s remains)
2017-12-05 14:53:07.965761: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32648 -4.3161016 -4.3019385 -4.2861357 -4.2668405 -4.2471914 -4.2415094 -4.2499576 -4.2644334 -4.2808928 -4.2932935 -4.3012915 -4.3028684 -4.2904925 -4.268611][-4.323503 -4.309546 -4.2890749 -4.2674365 -4.2447491 -4.2229347 -4.2181544 -4.2284508 -4.2481227 -4.2691593 -4.2817793 -4.2892194 -4.2906027 -4.2774339 -4.2534103][-4.320055 -4.3027525 -4.2766838 -4.2491069 -4.2231474 -4.199955 -4.1947727 -4.2064648 -4.2324004 -4.2576189 -4.2729106 -4.2803984 -4.2814541 -4.2682676 -4.2458644][-4.3171649 -4.2975984 -4.2671618 -4.2340822 -4.2036576 -4.179318 -4.1738253 -4.1860433 -4.2152538 -4.2443757 -4.2637286 -4.2751937 -4.278729 -4.2688279 -4.2501521][-4.3141279 -4.2938228 -4.2629142 -4.2287636 -4.1970611 -4.1725745 -4.1660156 -4.1742349 -4.1977243 -4.2291832 -4.2537107 -4.2713156 -4.2799988 -4.2724915 -4.2569561][-4.3118205 -4.2928534 -4.2634254 -4.2334785 -4.2019987 -4.1743827 -4.1627808 -4.160748 -4.1718841 -4.20011 -4.2336955 -4.2617555 -4.2770605 -4.2755027 -4.2663574][-4.3101382 -4.292944 -4.2655163 -4.2385592 -4.2057028 -4.1752815 -4.1536827 -4.1378279 -4.1380372 -4.1652956 -4.2101827 -4.25175 -4.2763929 -4.2802219 -4.2747827][-4.3091574 -4.2927732 -4.2664738 -4.237431 -4.1996012 -4.1589551 -4.1222525 -4.088654 -4.0811653 -4.116189 -4.1776648 -4.2385092 -4.2802982 -4.2886066 -4.2828918][-4.3091769 -4.2936244 -4.2676859 -4.2354474 -4.1895514 -4.1352506 -4.0796843 -4.031527 -4.0206227 -4.0675745 -4.1430211 -4.2160325 -4.2685513 -4.2827997 -4.2768669][-4.3104086 -4.2959452 -4.2711182 -4.2346845 -4.1799316 -4.112679 -4.0425 -3.9856844 -3.9740882 -4.0307384 -4.1207418 -4.2004094 -4.2531052 -4.2683287 -4.260016][-4.3117623 -4.2980018 -4.273385 -4.2369609 -4.1830816 -4.1190939 -4.0524297 -3.9977622 -3.983917 -4.0401163 -4.1316576 -4.2070508 -4.2532544 -4.2679935 -4.2566714][-4.3123684 -4.2968464 -4.2709336 -4.2365203 -4.1910391 -4.1416855 -4.0920734 -4.0500445 -4.0384059 -4.0851774 -4.1609106 -4.2232852 -4.2623506 -4.27658 -4.2637362][-4.3116446 -4.2928338 -4.2640095 -4.2301388 -4.1922736 -4.1548676 -4.1215115 -4.0977693 -4.0962529 -4.1328058 -4.1873217 -4.2354755 -4.2668242 -4.2800555 -4.26894][-4.3104095 -4.2868114 -4.2540278 -4.2185063 -4.1856041 -4.1592283 -4.1399555 -4.1341882 -4.1470022 -4.1815205 -4.2224994 -4.2574825 -4.2788692 -4.2854815 -4.2745867][-4.3113132 -4.2848992 -4.2495389 -4.2138453 -4.1868248 -4.1713419 -4.1682019 -4.1762295 -4.1970954 -4.2294149 -4.2605033 -4.282825 -4.291687 -4.2887836 -4.2755117]]...]
INFO - root - 2017-12-05 14:53:16.450328: step 18010, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.809 sec/batch; 70h:40m:47s remains)
INFO - root - 2017-12-05 14:53:25.084916: step 18020, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 76h:31m:05s remains)
INFO - root - 2017-12-05 14:53:33.786759: step 18030, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.892 sec/batch; 77h:54m:37s remains)
INFO - root - 2017-12-05 14:53:42.497839: step 18040, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.858 sec/batch; 74h:54m:13s remains)
INFO - root - 2017-12-05 14:53:51.172040: step 18050, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 76h:32m:21s remains)
INFO - root - 2017-12-05 14:53:59.731289: step 18060, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 74h:14m:56s remains)
INFO - root - 2017-12-05 14:54:08.345671: step 18070, loss = 2.06, batch loss = 2.01 (8.5 examples/sec; 0.936 sec/batch; 81h:45m:32s remains)
INFO - root - 2017-12-05 14:54:16.979544: step 18080, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 76h:57m:28s remains)
INFO - root - 2017-12-05 14:54:25.361525: step 18090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 74h:48m:12s remains)
INFO - root - 2017-12-05 14:54:34.072245: step 18100, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 74h:33m:55s remains)
2017-12-05 14:54:34.847318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2894125 -4.2891154 -4.2900748 -4.2911716 -4.2887111 -4.2854319 -4.2845421 -4.2852902 -4.2882886 -4.2920647 -4.2920818 -4.2896776 -4.2897625 -4.2908578 -4.2910233][-4.2484016 -4.2485294 -4.2490792 -4.247077 -4.2414351 -4.2382808 -4.2374415 -4.23874 -4.2437048 -4.2496915 -4.2501559 -4.2469935 -4.24498 -4.2431846 -4.2391839][-4.2192674 -4.220902 -4.2226443 -4.2190995 -4.211278 -4.209404 -4.2072558 -4.2038212 -4.2056608 -4.2099142 -4.2129703 -4.2122493 -4.2086692 -4.2055979 -4.2005944][-4.2124805 -4.2134962 -4.216064 -4.2140985 -4.2076826 -4.20662 -4.1996593 -4.1862793 -4.1781487 -4.1784611 -4.1837716 -4.1886935 -4.1904926 -4.1905341 -4.1885538][-4.215147 -4.2151527 -4.2146335 -4.2107439 -4.202702 -4.1995769 -4.1882358 -4.1684971 -4.154985 -4.1498442 -4.154767 -4.16825 -4.1816649 -4.1880422 -4.1871343][-4.2108221 -4.2092304 -4.2039785 -4.1962237 -4.1847706 -4.17733 -4.1659269 -4.1493707 -4.1337695 -4.1235776 -4.1256828 -4.1427388 -4.1631808 -4.1735959 -4.1730151][-4.1863432 -4.183929 -4.1754417 -4.1637874 -4.1509509 -4.140636 -4.1269407 -4.1099052 -4.0888391 -4.0735693 -4.0756235 -4.0950222 -4.116147 -4.1253395 -4.1247616][-4.132421 -4.1273251 -4.1177192 -4.1046848 -4.0929317 -4.082305 -4.0651455 -4.0420246 -4.0150657 -3.9974046 -4.0040727 -4.0273986 -4.048574 -4.0579 -4.057024][-4.0966005 -4.0866261 -4.0775652 -4.0626035 -4.0511322 -4.0423889 -4.0244236 -3.9993744 -3.9728918 -3.9611645 -3.9769077 -4.0041814 -4.0274653 -4.0366316 -4.0357003][-4.1080165 -4.0981574 -4.0886812 -4.073339 -4.0606394 -4.0529385 -4.0368223 -4.0183468 -4.0034838 -3.9998956 -4.0146513 -4.0377321 -4.0579996 -4.0640321 -4.0643225][-4.1460714 -4.1429582 -4.1352744 -4.1265812 -4.1195459 -4.114368 -4.1003351 -4.0842738 -4.0747895 -4.0729051 -4.0779181 -4.0885615 -4.099051 -4.1029844 -4.1028781][-4.1707959 -4.17618 -4.1746974 -4.1747627 -4.1744637 -4.17615 -4.1682081 -4.155293 -4.1475787 -4.1428585 -4.1362324 -4.1314273 -4.1341987 -4.1378522 -4.1381989][-4.1614652 -4.1738729 -4.181757 -4.1911654 -4.1975532 -4.2051764 -4.2058053 -4.1990128 -4.1944489 -4.1887531 -4.1744809 -4.1614027 -4.1546631 -4.1491995 -4.1437945][-4.1468277 -4.1611466 -4.1714692 -4.1812749 -4.1863832 -4.1928024 -4.1980071 -4.1990848 -4.1992221 -4.1940274 -4.1807265 -4.1687374 -4.1593566 -4.1477518 -4.1383085][-4.1515646 -4.1596837 -4.1626406 -4.1645575 -4.1637044 -4.1643577 -4.16757 -4.1739073 -4.1818333 -4.1826711 -4.1779227 -4.1741838 -4.1668882 -4.1516161 -4.1382933]]...]
INFO - root - 2017-12-05 14:54:43.347241: step 18110, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 75h:32m:43s remains)
INFO - root - 2017-12-05 14:54:51.893517: step 18120, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 0.782 sec/batch; 68h:16m:17s remains)
INFO - root - 2017-12-05 14:55:00.375682: step 18130, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.823 sec/batch; 71h:49m:34s remains)
INFO - root - 2017-12-05 14:55:08.911275: step 18140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 75h:07m:31s remains)
INFO - root - 2017-12-05 14:55:17.569234: step 18150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 75h:53m:40s remains)
INFO - root - 2017-12-05 14:55:26.174721: step 18160, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 78h:44m:57s remains)
INFO - root - 2017-12-05 14:55:34.717216: step 18170, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 74h:15m:53s remains)
INFO - root - 2017-12-05 14:55:43.319127: step 18180, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 75h:20m:53s remains)
INFO - root - 2017-12-05 14:55:51.793667: step 18190, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 75h:47m:26s remains)
INFO - root - 2017-12-05 14:56:00.293842: step 18200, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 73h:36m:35s remains)
2017-12-05 14:56:01.026650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2866831 -4.2710795 -4.2626004 -4.2648559 -4.2688494 -4.2768183 -4.2871346 -4.2992582 -4.30581 -4.3093767 -4.3158321 -4.3211 -4.3223896 -4.3202453 -4.31567][-4.2556825 -4.2358508 -4.2280254 -4.2309504 -4.2347507 -4.2417669 -4.2529364 -4.2713313 -4.2815776 -4.2884426 -4.299695 -4.3073516 -4.3124714 -4.313477 -4.3075519][-4.2192469 -4.1973257 -4.1880774 -4.1866469 -4.1863728 -4.18914 -4.19854 -4.222261 -4.2353539 -4.2500648 -4.2716742 -4.2847214 -4.2918968 -4.2980504 -4.29452][-4.1820259 -4.1559396 -4.1404705 -4.1297731 -4.1248794 -4.122129 -4.1269894 -4.15094 -4.1656952 -4.1911159 -4.2244234 -4.2435694 -4.2562904 -4.2702146 -4.2740293][-4.1502757 -4.1187758 -4.0941405 -4.0710711 -4.0618691 -4.0561047 -4.0506835 -4.0632119 -4.0798473 -4.1187339 -4.1622238 -4.1874943 -4.2096729 -4.2389717 -4.254396][-4.1269217 -4.0872545 -4.0476379 -4.0047536 -3.9834442 -3.9790976 -3.9726171 -3.9701271 -3.981858 -4.0355778 -4.0905347 -4.1228681 -4.1572218 -4.1995959 -4.2308517][-4.1190877 -4.0723162 -4.017241 -3.9542682 -3.9226677 -3.9237056 -3.9227095 -3.9074342 -3.9047163 -3.9604728 -4.0210819 -4.0554605 -4.0981708 -4.1490259 -4.1938128][-4.1397986 -4.0934286 -4.0309134 -3.962455 -3.9277763 -3.9267986 -3.9224854 -3.8856292 -3.8612614 -3.9099331 -3.9718537 -4.0045719 -4.0524955 -4.1115031 -4.1654434][-4.1775565 -4.1380725 -4.0814967 -4.0219126 -3.9847274 -3.9718432 -3.9576046 -3.9137988 -3.8740959 -3.9069188 -3.9527798 -3.9769356 -4.0258026 -4.0877872 -4.1466203][-4.2067451 -4.1758966 -4.1364632 -4.0958691 -4.0687094 -4.051971 -4.0363846 -3.9984083 -3.955827 -3.959105 -3.9732888 -3.9807112 -4.0199795 -4.0736923 -4.12978][-4.2139421 -4.1869473 -4.1603889 -4.1380415 -4.1287885 -4.1189041 -4.104063 -4.0734186 -4.0401468 -4.0285268 -4.01488 -4.0018234 -4.0227108 -4.0632353 -4.1099305][-4.2096505 -4.1822429 -4.1594791 -4.1460938 -4.1477265 -4.1482806 -4.1411176 -4.1238518 -4.1072769 -4.0954008 -4.0655661 -4.0330143 -4.0300975 -4.054831 -4.0934577][-4.2043753 -4.17949 -4.1584167 -4.1482487 -4.1525664 -4.1592174 -4.1636577 -4.1585727 -4.1563849 -4.1502819 -4.1207523 -4.0815072 -4.0623512 -4.0721064 -4.097805][-4.2106647 -4.1906333 -4.1743913 -4.1664987 -4.1674442 -4.1724215 -4.1806574 -4.18236 -4.1869259 -4.1900544 -4.174119 -4.1399188 -4.116508 -4.1170664 -4.1308656][-4.2297916 -4.2165155 -4.2064657 -4.2020621 -4.2032757 -4.2084813 -4.216979 -4.2198238 -4.2233109 -4.2290921 -4.2222843 -4.2022481 -4.1862369 -4.1864614 -4.1924977]]...]
INFO - root - 2017-12-05 14:56:09.582402: step 18210, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 72h:56m:51s remains)
INFO - root - 2017-12-05 14:56:18.105319: step 18220, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 74h:58m:58s remains)
INFO - root - 2017-12-05 14:56:26.872166: step 18230, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.785 sec/batch; 68h:29m:20s remains)
INFO - root - 2017-12-05 14:56:35.484030: step 18240, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 73h:40m:50s remains)
INFO - root - 2017-12-05 14:56:44.114081: step 18250, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 74h:23m:15s remains)
INFO - root - 2017-12-05 14:56:52.781755: step 18260, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 74h:29m:41s remains)
INFO - root - 2017-12-05 14:57:01.225122: step 18270, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 74h:14m:24s remains)
INFO - root - 2017-12-05 14:57:09.754602: step 18280, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 75h:46m:59s remains)
INFO - root - 2017-12-05 14:57:18.340303: step 18290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 75h:10m:44s remains)
INFO - root - 2017-12-05 14:57:26.801601: step 18300, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 76h:52m:41s remains)
2017-12-05 14:57:27.700944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2601061 -4.2411432 -4.2284112 -4.2229033 -4.2252908 -4.2379117 -4.2563324 -4.2773952 -4.2851872 -4.2859044 -4.2852154 -4.2855191 -4.289803 -4.2962384 -4.3051376][-4.2335844 -4.2028208 -4.1822057 -4.1743188 -4.1761184 -4.1895208 -4.20799 -4.2317824 -4.2432375 -4.247272 -4.2507672 -4.2551961 -4.2598782 -4.2647052 -4.2775688][-4.2298865 -4.1974912 -4.1748571 -4.1665149 -4.1646314 -4.1677437 -4.1759138 -4.1933937 -4.2033958 -4.2102809 -4.2149935 -4.2207832 -4.224524 -4.2282491 -4.2427416][-4.2218552 -4.1918068 -4.1703935 -4.1618867 -4.1573043 -4.153985 -4.1539288 -4.164506 -4.1717834 -4.1797218 -4.1842742 -4.1894107 -4.18978 -4.1895223 -4.205863][-4.2007809 -4.1710424 -4.1544309 -4.1457 -4.1344833 -4.1216269 -4.11314 -4.119298 -4.1334949 -4.154501 -4.1660814 -4.1685219 -4.1676893 -4.1653976 -4.1808076][-4.1601276 -4.1277313 -4.1179008 -4.110548 -4.0896597 -4.057436 -4.0281148 -4.019052 -4.0407414 -4.0847187 -4.1133909 -4.1269641 -4.1334958 -4.1379032 -4.1558275][-4.1150551 -4.0784464 -4.069447 -4.0551691 -4.0123596 -3.9498734 -3.879199 -3.8313594 -3.8573558 -3.9349351 -3.9947565 -4.0326767 -4.0657005 -4.0881639 -4.117209][-4.1075239 -4.0684972 -4.0506253 -4.02321 -3.9607623 -3.8780348 -3.77917 -3.6965058 -3.7151415 -3.8156147 -3.8978903 -3.9585779 -4.0187397 -4.0600867 -4.102325][-4.1522532 -4.1207619 -4.1022983 -4.0768881 -4.0282121 -3.9637971 -3.8967113 -3.8388309 -3.8411226 -3.9043157 -3.9611382 -4.0065584 -4.0574427 -4.0937977 -4.1362453][-4.2152414 -4.2003212 -4.1899605 -4.175139 -4.1473403 -4.109158 -4.074945 -4.04542 -4.0390048 -4.0685196 -4.0976286 -4.1191244 -4.1479979 -4.1704693 -4.200027][-4.26856 -4.2671218 -4.2633519 -4.254055 -4.2372651 -4.2142229 -4.1932597 -4.1776285 -4.1720304 -4.1852126 -4.1992021 -4.2050381 -4.2185311 -4.2318926 -4.2487116][-4.2938595 -4.2951384 -4.2952628 -4.290597 -4.2802453 -4.2650084 -4.2492557 -4.2394323 -4.2364535 -4.2421794 -4.2506318 -4.2526889 -4.2596555 -4.2667637 -4.2753654][-4.3063569 -4.306283 -4.3088613 -4.3084292 -4.3042512 -4.2969794 -4.2882371 -4.2831779 -4.2808237 -4.2824922 -4.2861695 -4.2866945 -4.2894416 -4.2926683 -4.2959352][-4.3148246 -4.3129182 -4.31507 -4.3158212 -4.3143673 -4.3117166 -4.3088431 -4.3071036 -4.3067279 -4.3072515 -4.3077488 -4.3054562 -4.3040862 -4.3036785 -4.303885][-4.3193855 -4.3166547 -4.3165298 -4.3160858 -4.3152785 -4.3145266 -4.3143249 -4.3136683 -4.3133636 -4.313602 -4.313448 -4.3117304 -4.3109756 -4.3102007 -4.3097477]]...]
INFO - root - 2017-12-05 14:57:36.133313: step 18310, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 76h:45m:19s remains)
INFO - root - 2017-12-05 14:57:44.755451: step 18320, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 76h:13m:27s remains)
INFO - root - 2017-12-05 14:57:53.332138: step 18330, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 74h:47m:45s remains)
INFO - root - 2017-12-05 14:58:01.833934: step 18340, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.794 sec/batch; 69h:18m:37s remains)
INFO - root - 2017-12-05 14:58:10.463350: step 18350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 76h:11m:08s remains)
INFO - root - 2017-12-05 14:58:19.096286: step 18360, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 75h:13m:54s remains)
INFO - root - 2017-12-05 14:58:27.679903: step 18370, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 73h:38m:59s remains)
INFO - root - 2017-12-05 14:58:36.321068: step 18380, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 75h:25m:26s remains)
INFO - root - 2017-12-05 14:58:44.955577: step 18390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 76h:03m:57s remains)
INFO - root - 2017-12-05 14:58:53.576566: step 18400, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 73h:46m:09s remains)
2017-12-05 14:58:54.365628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1638837 -4.1714582 -4.1940722 -4.2249765 -4.2421474 -4.2459011 -4.2312436 -4.218164 -4.2095442 -4.1926494 -4.1650677 -4.1257114 -4.0903378 -4.0871062 -4.1352396][-4.1780677 -4.1903329 -4.2130146 -4.2396774 -4.2584887 -4.2698441 -4.2639537 -4.2535477 -4.2397113 -4.214972 -4.170558 -4.116436 -4.0737853 -4.0798559 -4.140244][-4.1917419 -4.2032042 -4.2208095 -4.2430053 -4.2629986 -4.2789974 -4.2791429 -4.2663879 -4.2495213 -4.222846 -4.1783824 -4.1281185 -4.0914564 -4.1059709 -4.1629682][-4.1947231 -4.2030425 -4.2148671 -4.2266474 -4.2406816 -4.2540183 -4.2553673 -4.2426953 -4.2288942 -4.2120247 -4.185936 -4.1552763 -4.1354423 -4.1496549 -4.1867132][-4.1836419 -4.1906629 -4.1964817 -4.1961064 -4.1989045 -4.1990128 -4.1921682 -4.1772776 -4.1733403 -4.1780076 -4.1752715 -4.1683178 -4.163826 -4.1709456 -4.1821852][-4.1552758 -4.1604543 -4.1646595 -4.1571474 -4.1415086 -4.1206021 -4.0940742 -4.0690475 -4.0793653 -4.1073627 -4.1289167 -4.1444221 -4.1488013 -4.1463485 -4.1380792][-4.1166759 -4.1225424 -4.12755 -4.1145959 -4.0793548 -4.0263004 -3.9582794 -3.9066081 -3.9428802 -4.0118761 -4.0641208 -4.1015553 -4.113904 -4.1055145 -4.0884733][-4.0838428 -4.090178 -4.093257 -4.0750594 -4.0255809 -3.9454699 -3.8384078 -3.7561224 -3.8280425 -3.9451516 -4.0243745 -4.0794163 -4.0998273 -4.0927415 -4.0784793][-4.0678444 -4.0753918 -4.0793533 -4.0660048 -4.0254965 -3.9640009 -3.8857555 -3.8327465 -3.8946986 -3.9909048 -4.0544314 -4.0956178 -4.1080689 -4.1019521 -4.0926313][-4.0821919 -4.0957146 -4.10492 -4.0984473 -4.0775847 -4.0495253 -4.0153346 -3.9940481 -4.0292964 -4.0794778 -4.1089668 -4.1236892 -4.1230116 -4.1188722 -4.1150765][-4.1139007 -4.136857 -4.1496696 -4.1460705 -4.1375332 -4.1317539 -4.121798 -4.1182075 -4.1368318 -4.153079 -4.1554289 -4.1529489 -4.1467528 -4.1449561 -4.1422176][-4.1344519 -4.1669555 -4.1907783 -4.1926165 -4.1869664 -4.18684 -4.1847878 -4.1836309 -4.1931477 -4.1964297 -4.1906877 -4.184268 -4.1751671 -4.170012 -4.1583548][-4.144454 -4.18156 -4.210516 -4.2207665 -4.2148137 -4.2128716 -4.2133641 -4.2099648 -4.2137456 -4.2198648 -4.2183909 -4.2118063 -4.2046094 -4.1962638 -4.1774564][-4.1722317 -4.20112 -4.2261844 -4.2376895 -4.2331538 -4.2311769 -4.231699 -4.2267828 -4.22841 -4.236629 -4.2363763 -4.2322016 -4.2339554 -4.2294874 -4.2111788][-4.2014656 -4.215013 -4.2300129 -4.2372117 -4.2356954 -4.2372351 -4.237246 -4.2291803 -4.2259722 -4.2314029 -4.2332468 -4.2351642 -4.24451 -4.2484465 -4.2397437]]...]
INFO - root - 2017-12-05 14:59:02.833778: step 18410, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 74h:35m:21s remains)
INFO - root - 2017-12-05 14:59:11.472110: step 18420, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 75h:19m:07s remains)
INFO - root - 2017-12-05 14:59:20.007299: step 18430, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 75h:00m:00s remains)
INFO - root - 2017-12-05 14:59:28.453678: step 18440, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 73h:14m:10s remains)
INFO - root - 2017-12-05 14:59:36.912931: step 18450, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.745 sec/batch; 65h:02m:03s remains)
INFO - root - 2017-12-05 14:59:45.505881: step 18460, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 74h:58m:33s remains)
INFO - root - 2017-12-05 14:59:54.149660: step 18470, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:52m:42s remains)
INFO - root - 2017-12-05 15:00:02.702693: step 18480, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 75h:13m:28s remains)
INFO - root - 2017-12-05 15:00:11.183297: step 18490, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 76h:56m:16s remains)
INFO - root - 2017-12-05 15:00:19.753354: step 18500, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 78h:12m:25s remains)
2017-12-05 15:00:20.634677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2705417 -4.2800984 -4.2890544 -4.2963371 -4.3011804 -4.305357 -4.30883 -4.3098321 -4.307725 -4.3036132 -4.2995806 -4.2945685 -4.2832842 -4.264349 -4.2454162][-4.2643247 -4.2763848 -4.287827 -4.2980332 -4.3051944 -4.3116989 -4.3183727 -4.3223567 -4.3220968 -4.3177414 -4.3119545 -4.30514 -4.292068 -4.2711792 -4.2501926][-4.2566881 -4.2657762 -4.2755675 -4.286818 -4.2972913 -4.3076844 -4.3192835 -4.3289175 -4.3332043 -4.3305554 -4.3235397 -4.3149943 -4.3004913 -4.2776566 -4.2540922][-4.24436 -4.2430048 -4.2447453 -4.2538338 -4.2664547 -4.2792621 -4.2957234 -4.3138776 -4.3272581 -4.3309569 -4.3266072 -4.3193617 -4.305964 -4.2825484 -4.2569108][-4.2245045 -4.2070336 -4.1947842 -4.1969242 -4.20877 -4.2210035 -4.2385588 -4.2650938 -4.2916489 -4.307251 -4.310555 -4.3086972 -4.3003092 -4.2796216 -4.2545471][-4.20273 -4.1696773 -4.141572 -4.132143 -4.1374779 -4.1428947 -4.1526957 -4.1812048 -4.2207284 -4.2519193 -4.2671328 -4.2750068 -4.276031 -4.2632318 -4.2435813][-4.1809683 -4.1380978 -4.0976839 -4.0757828 -4.0687566 -4.0613251 -4.0543671 -4.075633 -4.125845 -4.1749177 -4.205421 -4.225481 -4.2388725 -4.2379603 -4.2275825][-4.1638913 -4.1213412 -4.0773892 -4.0458059 -4.0237432 -3.9963624 -3.9666576 -3.9738526 -4.0314775 -4.0981841 -4.1453052 -4.17759 -4.2018976 -4.2127867 -4.2121396][-4.1486816 -4.1132574 -4.0736413 -4.0396953 -4.0081978 -3.9662471 -3.9211442 -3.9141803 -3.9719958 -4.0493922 -4.1095915 -4.1498713 -4.1798263 -4.1974788 -4.2017188][-4.1364503 -4.1073809 -4.0744915 -4.0448895 -4.0156994 -3.974611 -3.9310586 -3.9194727 -3.967947 -4.0427532 -4.1053677 -4.1464281 -4.1752295 -4.1933961 -4.1984158][-4.1381207 -4.1123571 -4.0861592 -4.0641584 -4.0441289 -4.0148506 -3.9834034 -3.9747515 -4.0112238 -4.0738211 -4.1274309 -4.1617002 -4.1836772 -4.1976495 -4.2006254][-4.1662383 -4.1435876 -4.1231966 -4.1085815 -4.0977292 -4.0807858 -4.0613031 -4.0561709 -4.0813117 -4.1265111 -4.1646662 -4.18765 -4.2004857 -4.2076173 -4.2059321][-4.219101 -4.201602 -4.1861935 -4.1771708 -4.1719527 -4.1625915 -4.150013 -4.1453362 -4.1587005 -4.1852489 -4.2077041 -4.2193942 -4.2227044 -4.2209072 -4.21113][-4.2738895 -4.2663198 -4.2570157 -4.2498107 -4.2443376 -4.2363133 -4.22615 -4.2195749 -4.2233763 -4.2351861 -4.2451491 -4.2475715 -4.2440505 -4.23393 -4.21372][-4.3000479 -4.3056498 -4.3046832 -4.2998996 -4.2930727 -4.2841196 -4.2738304 -4.2656083 -4.2626195 -4.2639594 -4.2645917 -4.2601476 -4.2519011 -4.2348475 -4.2044492]]...]
INFO - root - 2017-12-05 15:00:29.177106: step 18510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 73h:56m:59s remains)
INFO - root - 2017-12-05 15:00:37.707709: step 18520, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 75h:56m:59s remains)
INFO - root - 2017-12-05 15:00:46.337977: step 18530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 74h:15m:12s remains)
INFO - root - 2017-12-05 15:00:54.896771: step 18540, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 74h:27m:23s remains)
INFO - root - 2017-12-05 15:01:03.516629: step 18550, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 74h:08m:37s remains)
INFO - root - 2017-12-05 15:01:11.954248: step 18560, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 77h:01m:03s remains)
INFO - root - 2017-12-05 15:01:20.528221: step 18570, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 75h:01m:43s remains)
INFO - root - 2017-12-05 15:01:29.042910: step 18580, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 73h:06m:31s remains)
INFO - root - 2017-12-05 15:01:37.542347: step 18590, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 74h:10m:06s remains)
INFO - root - 2017-12-05 15:01:46.179785: step 18600, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 73h:38m:01s remains)
2017-12-05 15:01:46.922925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2048054 -4.21665 -4.2331257 -4.2414069 -4.2293248 -4.2056956 -4.1881371 -4.1776938 -4.165113 -4.1494632 -4.1401234 -4.1566224 -4.1875091 -4.2256379 -4.2652245][-4.2056174 -4.218997 -4.2363973 -4.251287 -4.2453732 -4.2268691 -4.2089429 -4.1960073 -4.1820931 -4.1685042 -4.1605072 -4.1685672 -4.189887 -4.2232213 -4.2602248][-4.2126703 -4.2297821 -4.2462397 -4.2608314 -4.2521687 -4.2343822 -4.2211194 -4.2091718 -4.2012138 -4.1982875 -4.1907926 -4.1847806 -4.19196 -4.2144437 -4.2473474][-4.2237644 -4.24309 -4.2538629 -4.2646141 -4.2508116 -4.2302303 -4.2134004 -4.1934319 -4.1968675 -4.2186556 -4.2188406 -4.2042937 -4.1992078 -4.2121048 -4.2439966][-4.2358646 -4.2540722 -4.2586784 -4.2645912 -4.2450576 -4.2137232 -4.17374 -4.1285286 -4.1427503 -4.2005162 -4.2273498 -4.2215695 -4.2171893 -4.2279778 -4.2569275][-4.2525129 -4.2650127 -4.2622676 -4.2629442 -4.2341228 -4.1835785 -4.1040578 -4.01541 -4.0438209 -4.1446891 -4.2039313 -4.2194963 -4.2273312 -4.2455459 -4.272244][-4.2678828 -4.272409 -4.2613854 -4.257328 -4.2216797 -4.1513643 -4.039124 -3.9122777 -3.952251 -4.0843987 -4.1670294 -4.202682 -4.2219143 -4.2492809 -4.2773871][-4.2666574 -4.2652984 -4.2479491 -4.2462792 -4.2142916 -4.1432838 -4.0327816 -3.9141257 -3.9530897 -4.0678697 -4.1399441 -4.1773119 -4.2080107 -4.2422237 -4.2721996][-4.25774 -4.254324 -4.2361755 -4.2412825 -4.2216215 -4.1679497 -4.0881562 -4.0125332 -4.0418053 -4.1095357 -4.1462274 -4.16933 -4.1970882 -4.2292237 -4.2576284][-4.2412133 -4.2388854 -4.2255564 -4.2374125 -4.2298098 -4.193027 -4.1381035 -4.0962076 -4.1227908 -4.1618757 -4.1723771 -4.1797714 -4.1954937 -4.2183018 -4.2462783][-4.2262874 -4.222014 -4.2105331 -4.2267752 -4.2307515 -4.2061687 -4.1629453 -4.1386781 -4.1631508 -4.1888804 -4.1930981 -4.1973629 -4.2069192 -4.222034 -4.2498455][-4.2264652 -4.212853 -4.1996264 -4.218399 -4.22861 -4.2114396 -4.1723061 -4.1593561 -4.1825924 -4.1993275 -4.2031622 -4.2118454 -4.2188921 -4.2292914 -4.2547035][-4.2434196 -4.2232838 -4.2114863 -4.2293835 -4.2386856 -4.222187 -4.1860209 -4.1822076 -4.2030253 -4.212079 -4.2186284 -4.2328496 -4.24155 -4.2515144 -4.2689438][-4.2654943 -4.2456636 -4.2382722 -4.2520747 -4.2567973 -4.2391024 -4.2098908 -4.2121639 -4.2267985 -4.2319841 -4.238492 -4.2535114 -4.2665958 -4.2768145 -4.2881575][-4.2738013 -4.2582321 -4.2544332 -4.2623153 -4.26234 -4.2470121 -4.2279768 -4.233777 -4.2433848 -4.2459378 -4.2497692 -4.2601438 -4.2722912 -4.2825608 -4.2923956]]...]
INFO - root - 2017-12-05 15:01:55.551594: step 18610, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 76h:10m:37s remains)
INFO - root - 2017-12-05 15:02:04.088900: step 18620, loss = 2.04, batch loss = 1.98 (10.6 examples/sec; 0.753 sec/batch; 65h:38m:19s remains)
INFO - root - 2017-12-05 15:02:12.602491: step 18630, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 76h:08m:07s remains)
INFO - root - 2017-12-05 15:02:21.182154: step 18640, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 75h:06m:26s remains)
INFO - root - 2017-12-05 15:02:29.870646: step 18650, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 76h:50m:57s remains)
INFO - root - 2017-12-05 15:02:38.639922: step 18660, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.977 sec/batch; 85h:08m:49s remains)
INFO - root - 2017-12-05 15:02:47.086361: step 18670, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 73h:24m:52s remains)
INFO - root - 2017-12-05 15:02:55.691078: step 18680, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.836 sec/batch; 72h:54m:19s remains)
INFO - root - 2017-12-05 15:03:04.159875: step 18690, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 72h:23m:50s remains)
INFO - root - 2017-12-05 15:03:12.743154: step 18700, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 72h:25m:16s remains)
2017-12-05 15:03:13.565778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1950526 -4.1782837 -4.1499748 -4.118072 -4.110733 -4.1516728 -4.1975346 -4.2236633 -4.2253017 -4.2063608 -4.1826749 -4.1503377 -4.1237216 -4.12499 -4.1283274][-4.172452 -4.15447 -4.1198859 -4.0786047 -4.0543451 -4.0851679 -4.1367507 -4.17411 -4.1906104 -4.1901507 -4.1831412 -4.1620665 -4.141911 -4.1427135 -4.1455226][-4.1584353 -4.1402125 -4.1017218 -4.0506039 -4.0089269 -4.0227489 -4.0701375 -4.1143064 -4.144948 -4.1682329 -4.1816931 -4.1736212 -4.1606026 -4.1593504 -4.1589489][-4.1717491 -4.1497984 -4.1034694 -4.0458984 -3.988373 -3.979373 -4.0133395 -4.0657682 -4.1156163 -4.1591415 -4.1862073 -4.1893511 -4.1821513 -4.1780939 -4.1741123][-4.1989489 -4.1696248 -4.1180239 -4.0588531 -3.9947481 -3.9633653 -3.9815412 -4.0384693 -4.1040406 -4.1588535 -4.1902232 -4.2006731 -4.1987534 -4.1949677 -4.1934953][-4.2225428 -4.1862469 -4.1286597 -4.068007 -4.0069904 -3.9718723 -3.9822414 -4.0326862 -4.0921011 -4.1411057 -4.1707048 -4.1895008 -4.199894 -4.2057867 -4.2092738][-4.2282543 -4.1904831 -4.1301293 -4.0732388 -4.0218177 -3.9952838 -4.0033612 -4.031116 -4.0576935 -4.0759711 -4.0957642 -4.1318488 -4.1700687 -4.1948628 -4.205699][-4.2204 -4.189352 -4.1418867 -4.099206 -4.0628448 -4.0415058 -4.0327659 -4.02273 -4.0028596 -3.9802036 -3.9867535 -4.0400381 -4.1074009 -4.1557956 -4.1780519][-4.2132277 -4.1928864 -4.1691246 -4.1553645 -4.143508 -4.1279597 -4.0984097 -4.0498352 -3.9858713 -3.9293308 -3.9227433 -3.9797573 -4.0576229 -4.1201587 -4.1513333][-4.2128983 -4.20843 -4.210475 -4.2195539 -4.2250762 -4.2155294 -4.176681 -4.1133242 -4.0373955 -3.9763088 -3.966917 -4.0092616 -4.0694094 -4.122592 -4.148953][-4.2178698 -4.2251792 -4.2383947 -4.2529593 -4.2619987 -4.2540674 -4.2186465 -4.1622171 -4.0987973 -4.0532455 -4.0510764 -4.0865541 -4.1279836 -4.1602592 -4.1727333][-4.237196 -4.2478814 -4.2576947 -4.2670364 -4.2746143 -4.2671638 -4.2362041 -4.1917067 -4.1455746 -4.1208234 -4.1294188 -4.1625714 -4.1926584 -4.2113042 -4.2138638][-4.2648826 -4.2754092 -4.2778835 -4.28046 -4.2861924 -4.2796249 -4.2575283 -4.2289839 -4.2010159 -4.1891351 -4.2006311 -4.2250376 -4.2423215 -4.2489586 -4.2475481][-4.28632 -4.2977467 -4.2981153 -4.2974858 -4.3013835 -4.2986093 -4.2889776 -4.2758222 -4.261816 -4.254519 -4.2602072 -4.2724113 -4.2796226 -4.279707 -4.2769032][-4.3111014 -4.3180652 -4.3166265 -4.3133068 -4.314847 -4.316009 -4.3139696 -4.3102608 -4.30441 -4.2990847 -4.3010812 -4.3069787 -4.3088236 -4.3065939 -4.3038936]]...]
INFO - root - 2017-12-05 15:03:22.167925: step 18710, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 75h:46m:14s remains)
INFO - root - 2017-12-05 15:03:30.731045: step 18720, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 75h:23m:27s remains)
INFO - root - 2017-12-05 15:03:39.372540: step 18730, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 74h:35m:41s remains)
INFO - root - 2017-12-05 15:03:47.866047: step 18740, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 74h:38m:45s remains)
INFO - root - 2017-12-05 15:03:56.350662: step 18750, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 73h:39m:33s remains)
INFO - root - 2017-12-05 15:04:04.875918: step 18760, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 73h:56m:25s remains)
INFO - root - 2017-12-05 15:04:13.335135: step 18770, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 73h:36m:34s remains)
INFO - root - 2017-12-05 15:04:21.805711: step 18780, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 72h:22m:11s remains)
INFO - root - 2017-12-05 15:04:30.304002: step 18790, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 74h:31m:51s remains)
INFO - root - 2017-12-05 15:04:38.928694: step 18800, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 73h:56m:45s remains)
2017-12-05 15:04:39.744721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2608871 -4.2685819 -4.2724752 -4.2674379 -4.2555208 -4.2391734 -4.2202735 -4.198554 -4.1711621 -4.1356206 -4.0803146 -4.02764 -4.0265155 -4.0593562 -4.1108718][-4.2586193 -4.2631931 -4.263979 -4.2594738 -4.2524424 -4.2432384 -4.2316446 -4.2156324 -4.195786 -4.1720023 -4.1327734 -4.0962009 -4.0953197 -4.1199169 -4.1606874][-4.2644129 -4.2608876 -4.2525787 -4.2429442 -4.2343154 -4.2265062 -4.2185397 -4.2107582 -4.2082386 -4.2098575 -4.1992488 -4.1858311 -4.1874056 -4.2015524 -4.2252378][-4.2657595 -4.2508936 -4.2302356 -4.2100105 -4.19271 -4.1770163 -4.1660857 -4.1653829 -4.1858945 -4.2174449 -4.2377872 -4.2473435 -4.2556272 -4.2624831 -4.2710514][-4.2443466 -4.212028 -4.1751747 -4.1429896 -4.1111131 -4.0780344 -4.055048 -4.0591812 -4.1052485 -4.17114 -4.2230396 -4.2559514 -4.2757659 -4.2831426 -4.2848845][-4.2094588 -4.1617103 -4.1141782 -4.0764685 -4.03274 -3.976058 -3.921406 -3.91077 -3.9752116 -4.0764637 -4.1615009 -4.220192 -4.2563434 -4.2711973 -4.27451][-4.1817794 -4.1312861 -4.0837622 -4.0428934 -3.9889266 -3.9068086 -3.8174791 -3.7832091 -3.8554876 -3.9760354 -4.0771852 -4.1498194 -4.2012343 -4.2310276 -4.2453337][-4.1819229 -4.1415029 -4.1010394 -4.0608711 -4.0040693 -3.9159002 -3.8146319 -3.7685196 -3.8333666 -3.9472041 -4.0420113 -4.109231 -4.1616011 -4.198236 -4.2207966][-4.2064228 -4.1835494 -4.1592298 -4.1337729 -4.0941625 -4.0288458 -3.9507279 -3.9076719 -3.9444523 -4.0189877 -4.0806661 -4.1233878 -4.1608768 -4.1878085 -4.2066445][-4.2327476 -4.2257419 -4.2192297 -4.213428 -4.1988335 -4.1647158 -4.1182303 -4.0887733 -4.1016541 -4.135397 -4.1603355 -4.1734343 -4.1877446 -4.1948633 -4.1991653][-4.24816 -4.2479219 -4.2526164 -4.2646604 -4.2722673 -4.2597427 -4.2332592 -4.2145624 -4.2150235 -4.2236915 -4.2276855 -4.2238922 -4.2225871 -4.2148647 -4.2067404][-4.2486062 -4.2470717 -4.2546244 -4.2757521 -4.2962818 -4.2967796 -4.2823873 -4.2707567 -4.2677236 -4.2695556 -4.2687345 -4.2608252 -4.2566772 -4.2448831 -4.2319775][-4.2346163 -4.2271128 -4.2312117 -4.2524552 -4.2763538 -4.2841949 -4.2784729 -4.2747746 -4.27564 -4.2782068 -4.277575 -4.2725143 -4.2735858 -4.2655611 -4.25527][-4.2248249 -4.2097163 -4.2073445 -4.2227473 -4.2420611 -4.2513561 -4.2533913 -4.2579708 -4.2661982 -4.2734294 -4.2752028 -4.2741241 -4.2796378 -4.275465 -4.2689052][-4.2414107 -4.2236104 -4.2158713 -4.2218866 -4.2296848 -4.2337165 -4.2394452 -4.2488756 -4.262 -4.2741656 -4.2797771 -4.2822862 -4.2883062 -4.2840319 -4.2780762]]...]
INFO - root - 2017-12-05 15:04:48.284836: step 18810, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 75h:06m:08s remains)
INFO - root - 2017-12-05 15:04:56.802907: step 18820, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 74h:33m:42s remains)
INFO - root - 2017-12-05 15:05:05.393122: step 18830, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 74h:52m:37s remains)
INFO - root - 2017-12-05 15:05:13.920498: step 18840, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 77h:19m:06s remains)
INFO - root - 2017-12-05 15:05:22.589296: step 18850, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 74h:20m:09s remains)
INFO - root - 2017-12-05 15:05:31.151156: step 18860, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 72h:56m:45s remains)
INFO - root - 2017-12-05 15:05:39.726162: step 18870, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 76h:31m:09s remains)
INFO - root - 2017-12-05 15:05:48.262109: step 18880, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 72h:52m:09s remains)
INFO - root - 2017-12-05 15:05:56.805819: step 18890, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 73h:45m:26s remains)
INFO - root - 2017-12-05 15:06:05.350584: step 18900, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 75h:29m:48s remains)
2017-12-05 15:06:06.075074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2872634 -4.2842522 -4.2736974 -4.2657208 -4.2634811 -4.2622795 -4.2576108 -4.2507076 -4.2349277 -4.2071576 -4.1871157 -4.1976738 -4.2233195 -4.2475605 -4.2637653][-4.2666717 -4.26236 -4.2536774 -4.2539768 -4.26117 -4.2680097 -4.2674131 -4.2599454 -4.2416978 -4.2146974 -4.1959214 -4.2046065 -4.2301278 -4.25503 -4.2706923][-4.2569647 -4.2605457 -4.2579112 -4.2616396 -4.2671547 -4.2674518 -4.2593331 -4.2481132 -4.2305775 -4.2087154 -4.1957397 -4.207891 -4.2378039 -4.2663836 -4.2823067][-4.2594547 -4.2639213 -4.2583175 -4.2578878 -4.258122 -4.2504625 -4.2305121 -4.212286 -4.1969666 -4.18911 -4.1939497 -4.2166171 -4.2481332 -4.2738118 -4.2887974][-4.2515125 -4.255476 -4.249167 -4.2464991 -4.2381864 -4.2171922 -4.1745334 -4.1443019 -4.1379871 -4.1523657 -4.1768675 -4.2064729 -4.2377386 -4.26616 -4.2869668][-4.2399287 -4.2467194 -4.24215 -4.23416 -4.2114253 -4.1653686 -4.0887856 -4.0341244 -4.0339541 -4.0784388 -4.134069 -4.1837497 -4.2271 -4.2614965 -4.2834435][-4.2344046 -4.2363434 -4.230351 -4.2135406 -4.176064 -4.1030478 -3.9840498 -3.8889067 -3.9040313 -3.9997704 -4.1047988 -4.1807222 -4.2360559 -4.2708216 -4.2878184][-4.2282438 -4.2224679 -4.2149572 -4.1948605 -4.1523414 -4.067 -3.9279306 -3.8226619 -3.8648176 -3.9928489 -4.1152921 -4.1930451 -4.24847 -4.2826691 -4.2970791][-4.2173028 -4.2097931 -4.207386 -4.1957822 -4.1631861 -4.0960803 -3.9877 -3.9160564 -3.9633656 -4.0686488 -4.1648226 -4.2239466 -4.2690573 -4.29836 -4.30746][-4.2122693 -4.2133021 -4.2181878 -4.21663 -4.2033844 -4.1642652 -4.0946727 -4.0534658 -4.0913439 -4.1600809 -4.2244005 -4.2663913 -4.2992177 -4.3200994 -4.3212357][-4.2153621 -4.2278438 -4.2392139 -4.24557 -4.2446971 -4.2273488 -4.1891351 -4.1690025 -4.1947684 -4.2313728 -4.2689462 -4.296226 -4.3186507 -4.3334856 -4.33131][-4.2298017 -4.2433219 -4.2555 -4.2630625 -4.2671127 -4.2653646 -4.2512922 -4.2450438 -4.2618141 -4.279233 -4.3028507 -4.3196912 -4.3334112 -4.3431697 -4.3394017][-4.2506838 -4.2628736 -4.2710638 -4.2741895 -4.2781143 -4.2850523 -4.2843008 -4.2842298 -4.2929516 -4.3011847 -4.317101 -4.3302183 -4.33922 -4.3454933 -4.3413482][-4.2671952 -4.2780323 -4.2808185 -4.2789893 -4.2787595 -4.2874479 -4.2927508 -4.2944913 -4.296761 -4.298234 -4.308238 -4.3206024 -4.3307223 -4.3362594 -4.33421][-4.2699919 -4.2773194 -4.2755589 -4.2685671 -4.26255 -4.2662888 -4.2702489 -4.2720428 -4.2721858 -4.2702036 -4.2761588 -4.2938752 -4.3107462 -4.3199968 -4.3233366]]...]
INFO - root - 2017-12-05 15:06:14.722947: step 18910, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 73h:25m:04s remains)
INFO - root - 2017-12-05 15:06:23.332383: step 18920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 74h:15m:53s remains)
INFO - root - 2017-12-05 15:06:31.883160: step 18930, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 74h:08m:03s remains)
INFO - root - 2017-12-05 15:06:40.388080: step 18940, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.767 sec/batch; 66h:47m:58s remains)
INFO - root - 2017-12-05 15:06:48.920896: step 18950, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.841 sec/batch; 73h:15m:49s remains)
INFO - root - 2017-12-05 15:06:57.459136: step 18960, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 71h:29m:45s remains)
INFO - root - 2017-12-05 15:07:05.964258: step 18970, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 75h:37m:08s remains)
INFO - root - 2017-12-05 15:07:14.559302: step 18980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 75h:54m:58s remains)
INFO - root - 2017-12-05 15:07:23.179048: step 18990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 74h:16m:22s remains)
INFO - root - 2017-12-05 15:07:31.743878: step 19000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 74h:48m:35s remains)
2017-12-05 15:07:32.488640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.272203 -4.2340746 -4.1845517 -4.1337352 -4.0920324 -4.069797 -4.0651293 -4.0733681 -4.0982022 -4.1283879 -4.1522336 -4.1805363 -4.21453 -4.2457547 -4.2765231][-4.2822361 -4.2433991 -4.1926055 -4.1398039 -4.0997443 -4.0802536 -4.0769238 -4.078115 -4.0907702 -4.1141396 -4.1400676 -4.1776252 -4.2177391 -4.251822 -4.2834129][-4.2994995 -4.2637963 -4.2131934 -4.1644011 -4.1284842 -4.111908 -4.1005411 -4.086163 -4.0852084 -4.1036186 -4.1331754 -4.18055 -4.2252321 -4.25957 -4.2900968][-4.3042817 -4.2716236 -4.2247949 -4.1806889 -4.1486263 -4.1271071 -4.0981855 -4.0655861 -4.0586467 -4.0797009 -4.11923 -4.17829 -4.2269464 -4.260334 -4.2915425][-4.2980995 -4.2670565 -4.22037 -4.1755204 -4.1383991 -4.0999541 -4.0447583 -3.9923165 -3.9923971 -4.0373316 -4.1019292 -4.1734657 -4.2237096 -4.2558236 -4.2896051][-4.2811818 -4.2464266 -4.1956997 -4.1450582 -4.0957713 -4.0342121 -3.9436889 -3.8630581 -3.8929424 -3.9935975 -4.092237 -4.1700149 -4.2194633 -4.2515726 -4.2864504][-4.2647533 -4.2227092 -4.1649857 -4.1082282 -4.0488267 -3.9632466 -3.8351166 -3.7350051 -3.8230345 -3.9803143 -4.0939369 -4.1665955 -4.2099919 -4.2431879 -4.2807121][-4.2570715 -4.2082477 -4.1473007 -4.0886149 -4.0283842 -3.9434614 -3.832649 -3.7726841 -3.884933 -4.0272956 -4.117074 -4.1686525 -4.2053928 -4.2411051 -4.2822552][-4.2547283 -4.2042513 -4.1471663 -4.0946689 -4.0483408 -3.9999957 -3.951499 -3.9418483 -4.0230174 -4.1105218 -4.1618071 -4.191608 -4.2213683 -4.2567563 -4.2956667][-4.25362 -4.2072315 -4.1608977 -4.1246486 -4.1029682 -4.0893631 -4.0758543 -4.0767894 -4.1255255 -4.1763959 -4.2065468 -4.2250371 -4.2501278 -4.2823038 -4.3129005][-4.2607846 -4.2224689 -4.1875525 -4.1650796 -4.1558151 -4.1497254 -4.1365824 -4.13247 -4.1654162 -4.2076945 -4.2342768 -4.2508459 -4.2735529 -4.3006825 -4.3232532][-4.2726007 -4.2386851 -4.2072725 -4.1858544 -4.1737804 -4.1632605 -4.1485648 -4.1466761 -4.1787319 -4.22166 -4.2491112 -4.2668157 -4.28804 -4.3103232 -4.3287172][-4.2822704 -4.24831 -4.2143273 -4.1873422 -4.1696234 -4.1603918 -4.1523261 -4.1592178 -4.1929832 -4.2340012 -4.2605228 -4.2782922 -4.2972593 -4.3166509 -4.3334575][-4.2911267 -4.2564659 -4.2214737 -4.1927924 -4.1738677 -4.1691465 -4.1720591 -4.1887865 -4.2219305 -4.2570424 -4.280127 -4.2950792 -4.3099136 -4.3261032 -4.3405724][-4.3056984 -4.2759938 -4.2471552 -4.224731 -4.2103839 -4.2094307 -4.2179227 -4.2348924 -4.2604461 -4.286859 -4.3035893 -4.3148761 -4.3270159 -4.3396363 -4.3498082]]...]
INFO - root - 2017-12-05 15:07:41.089258: step 19010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 74h:30m:25s remains)
INFO - root - 2017-12-05 15:07:49.599882: step 19020, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 72h:56m:02s remains)
INFO - root - 2017-12-05 15:07:58.085231: step 19030, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 74h:16m:37s remains)
INFO - root - 2017-12-05 15:08:06.692670: step 19040, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 72h:14m:24s remains)
INFO - root - 2017-12-05 15:08:15.201734: step 19050, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 73h:19m:42s remains)
INFO - root - 2017-12-05 15:08:23.772266: step 19060, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 73h:53m:36s remains)
INFO - root - 2017-12-05 15:08:32.320356: step 19070, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 73h:26m:51s remains)
INFO - root - 2017-12-05 15:08:40.890094: step 19080, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 72h:55m:54s remains)
INFO - root - 2017-12-05 15:08:49.255488: step 19090, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 76h:11m:02s remains)
INFO - root - 2017-12-05 15:08:57.824614: step 19100, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 75h:13m:31s remains)
2017-12-05 15:08:58.617573: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2899146 -4.2874908 -4.2884536 -4.2910781 -4.2972775 -4.3044972 -4.3087621 -4.3106441 -4.3138328 -4.3193083 -4.3246055 -4.3279433 -4.3304939 -4.3343091 -4.3390269][-4.2641511 -4.2596617 -4.257916 -4.2581205 -4.2660027 -4.2761965 -4.2813792 -4.2823248 -4.2878551 -4.2985196 -4.3101182 -4.3190155 -4.3245573 -4.3291597 -4.3344865][-4.2297063 -4.2212768 -4.2151566 -4.2123585 -4.2211056 -4.2328377 -4.2363982 -4.2349162 -4.2430596 -4.2591748 -4.2784457 -4.295711 -4.3087139 -4.3171616 -4.3252964][-4.1980081 -4.1829047 -4.1723104 -4.167212 -4.1730933 -4.1794763 -4.1736526 -4.165442 -4.1757259 -4.198319 -4.2292209 -4.2606058 -4.2860646 -4.3029938 -4.3155265][-4.1706133 -4.1513653 -4.1384583 -4.1312227 -4.1317973 -4.1282544 -4.1085553 -4.0922217 -4.1081762 -4.1409211 -4.1887584 -4.2368631 -4.273994 -4.2984433 -4.3115244][-4.1417904 -4.1165972 -4.099329 -4.0918736 -4.0899572 -4.0774021 -4.0430861 -4.0184965 -4.0456176 -4.0989242 -4.1630874 -4.2215109 -4.2629862 -4.2901397 -4.3044147][-4.1185622 -4.0775337 -4.0431151 -4.0301924 -4.0283675 -4.0076857 -3.9599335 -3.9335015 -3.9825687 -4.0568662 -4.1319866 -4.1941943 -4.2413664 -4.2746158 -4.292809][-4.1197481 -4.0635877 -4.0098515 -3.9805362 -3.9686651 -3.9372234 -3.8897386 -3.8823116 -3.9530003 -4.038918 -4.1155972 -4.1777596 -4.2268806 -4.2654505 -4.2886953][-4.1442132 -4.0868521 -4.0244946 -3.9854128 -3.9697766 -3.9361067 -3.8993993 -3.9082761 -3.9804831 -4.0612545 -4.1317472 -4.1882524 -4.2357421 -4.276845 -4.2988715][-4.17034 -4.1173735 -4.05866 -4.026176 -4.0181966 -3.9930596 -3.969749 -3.9803073 -4.0394583 -4.1092062 -4.1686373 -4.217782 -4.2601833 -4.2983432 -4.31511][-4.2002583 -4.1574049 -4.1148772 -4.0990624 -4.0986657 -4.0831528 -4.0705824 -4.0749645 -4.1154575 -4.1666031 -4.2094116 -4.2479587 -4.2827888 -4.3133545 -4.324842][-4.2360411 -4.2098026 -4.1844125 -4.1816006 -4.1864848 -4.1830463 -4.1804757 -4.1806993 -4.2028322 -4.23286 -4.2577128 -4.2821813 -4.3036656 -4.3233113 -4.3301497][-4.2745214 -4.2598443 -4.2468 -4.2510695 -4.2609935 -4.2693939 -4.2752466 -4.272769 -4.2782497 -4.2897696 -4.2997117 -4.3105226 -4.3205938 -4.3311481 -4.33568][-4.3042331 -4.2974887 -4.2919254 -4.2957177 -4.3056431 -4.3159695 -4.323874 -4.3228512 -4.3212514 -4.3225503 -4.3254685 -4.3288465 -4.332233 -4.3381057 -4.3404284][-4.3197436 -4.3169489 -4.3143516 -4.316021 -4.3218126 -4.330297 -4.3384142 -4.3401241 -4.3380947 -4.3366861 -4.33625 -4.3373661 -4.337503 -4.3388948 -4.339736]]...]
INFO - root - 2017-12-05 15:09:07.110353: step 19110, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 77h:50m:47s remains)
INFO - root - 2017-12-05 15:09:15.733200: step 19120, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 75h:52m:46s remains)
INFO - root - 2017-12-05 15:09:24.233841: step 19130, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 73h:04m:10s remains)
INFO - root - 2017-12-05 15:09:32.739192: step 19140, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 71h:12m:11s remains)
INFO - root - 2017-12-05 15:09:41.224857: step 19150, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:35m:02s remains)
INFO - root - 2017-12-05 15:09:49.664080: step 19160, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.807 sec/batch; 70h:14m:29s remains)
INFO - root - 2017-12-05 15:09:58.223525: step 19170, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 73h:40m:01s remains)
INFO - root - 2017-12-05 15:10:06.824597: step 19180, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 72h:38m:24s remains)
INFO - root - 2017-12-05 15:10:15.376342: step 19190, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 76h:17m:29s remains)
INFO - root - 2017-12-05 15:10:23.920812: step 19200, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 74h:02m:17s remains)
2017-12-05 15:10:24.713713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2672524 -4.2646194 -4.2557516 -4.2445836 -4.2415886 -4.2352676 -4.2162628 -4.1950545 -4.1966786 -4.2108531 -4.2292786 -4.2587285 -4.2935266 -4.3195806 -4.3335485][-4.2533283 -4.2467775 -4.234386 -4.2188854 -4.2133164 -4.2021022 -4.1787357 -4.15408 -4.1558805 -4.179915 -4.2104239 -4.250319 -4.2903624 -4.3172092 -4.3326941][-4.24323 -4.2334723 -4.217927 -4.2003655 -4.1922927 -4.1768651 -4.149168 -4.1229243 -4.1273065 -4.16219 -4.20316 -4.25228 -4.2964029 -4.3215528 -4.3346539][-4.2279077 -4.2172871 -4.2042575 -4.1890683 -4.1787934 -4.1597137 -4.1291256 -4.104177 -4.1100206 -4.153203 -4.2003369 -4.2542462 -4.3015671 -4.3265123 -4.3375363][-4.2113495 -4.205225 -4.2018442 -4.1943727 -4.1847715 -4.1597004 -4.124835 -4.0976992 -4.1011491 -4.1477804 -4.1986246 -4.2561827 -4.3053632 -4.3298054 -4.3393922][-4.1923022 -4.1961575 -4.2076178 -4.2120318 -4.205821 -4.1765828 -4.1396589 -4.102078 -4.0955558 -4.1383963 -4.1930571 -4.2550411 -4.3067584 -4.3311152 -4.3395882][-4.1763444 -4.1881552 -4.2121344 -4.2269554 -4.2268715 -4.1939073 -4.152133 -4.10182 -4.0833993 -4.1196985 -4.1795592 -4.24759 -4.3040004 -4.3302994 -4.3392744][-4.157855 -4.18051 -4.2141476 -4.23504 -4.2407589 -4.2074494 -4.1578074 -4.0940619 -4.0654707 -4.0979233 -4.1642985 -4.2414322 -4.3032126 -4.3314009 -4.3419933][-4.1325927 -4.1650171 -4.2041044 -4.2257628 -4.2338514 -4.2036777 -4.14841 -4.0727444 -4.0380158 -4.0717177 -4.1464224 -4.2326522 -4.2994723 -4.3330226 -4.3462563][-4.0954313 -4.13387 -4.1781659 -4.2019329 -4.2119188 -4.1843424 -4.1280212 -4.0477767 -4.0095963 -4.0508308 -4.1350465 -4.2271838 -4.2979369 -4.335794 -4.3506174][-4.0603976 -4.0977449 -4.1436033 -4.1704082 -4.1797752 -4.1566997 -4.10501 -4.0283809 -3.9983706 -4.050035 -4.1389604 -4.2296715 -4.3007336 -4.3402267 -4.3545918][-4.051898 -4.07983 -4.1160507 -4.1408496 -4.1514835 -4.1377234 -4.0974011 -4.0370464 -4.0256133 -4.0808377 -4.1620131 -4.2416053 -4.3076825 -4.3451548 -4.3577495][-4.0656147 -4.0827742 -4.1091404 -4.1288605 -4.142312 -4.1378613 -4.1120224 -4.0751953 -4.0799394 -4.1298184 -4.194418 -4.2591081 -4.3177676 -4.3502679 -4.3594408][-4.0889568 -4.0984116 -4.1174583 -4.1300926 -4.1400175 -4.1404567 -4.126996 -4.1098061 -4.1247673 -4.1659565 -4.2167306 -4.271769 -4.3248086 -4.3527851 -4.3594661][-4.1058 -4.1152186 -4.1311007 -4.1380444 -4.1379552 -4.1392808 -4.1339836 -4.1278439 -4.1454573 -4.1790628 -4.2240386 -4.2766027 -4.3275857 -4.3542976 -4.3600755]]...]
INFO - root - 2017-12-05 15:10:33.314357: step 19210, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 73h:50m:08s remains)
INFO - root - 2017-12-05 15:10:41.783670: step 19220, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 75h:15m:21s remains)
INFO - root - 2017-12-05 15:10:50.317534: step 19230, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 72h:45m:21s remains)
INFO - root - 2017-12-05 15:10:59.010634: step 19240, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 75h:25m:29s remains)
INFO - root - 2017-12-05 15:11:07.598840: step 19250, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 74h:50m:56s remains)
INFO - root - 2017-12-05 15:11:16.141044: step 19260, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.752 sec/batch; 65h:24m:16s remains)
INFO - root - 2017-12-05 15:11:24.718625: step 19270, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 73h:19m:36s remains)
INFO - root - 2017-12-05 15:11:33.333054: step 19280, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 74h:32m:00s remains)
INFO - root - 2017-12-05 15:11:41.930136: step 19290, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 77h:04m:20s remains)
INFO - root - 2017-12-05 15:11:50.478364: step 19300, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 74h:32m:22s remains)
2017-12-05 15:11:51.283815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2173419 -4.2267356 -4.21725 -4.2023382 -4.1882496 -4.1803222 -4.18113 -4.1900873 -4.201396 -4.2102833 -4.2102923 -4.2046595 -4.1990895 -4.1950107 -4.1916022][-4.20705 -4.2252488 -4.2153335 -4.194674 -4.1706939 -4.152277 -4.1480513 -4.1609073 -4.1822119 -4.2018743 -4.2093673 -4.2066259 -4.1997552 -4.1913843 -4.1831708][-4.1900048 -4.2156215 -4.2066522 -4.178184 -4.1407895 -4.1068211 -4.093049 -4.1099191 -4.1448364 -4.1786847 -4.1970129 -4.2018495 -4.1971073 -4.1871023 -4.1748371][-4.1889319 -4.2157078 -4.2043509 -4.1661592 -4.1095967 -4.0514112 -4.0226154 -4.0436316 -4.0953054 -4.1465793 -4.1800103 -4.1966276 -4.1984272 -4.1907353 -4.1803126][-4.2026339 -4.2233453 -4.20429 -4.1504874 -4.069922 -3.982712 -3.936836 -3.9647291 -4.0355034 -4.10623 -4.15712 -4.188118 -4.1997252 -4.1977253 -4.1938972][-4.2182326 -4.2351408 -4.2083378 -4.1417894 -4.0408063 -3.931808 -3.875412 -3.9100852 -3.9912882 -4.0708413 -4.1312938 -4.1710196 -4.1892672 -4.1916375 -4.1948366][-4.2201848 -4.2395806 -4.2114954 -4.1413188 -4.036324 -3.9274583 -3.8760724 -3.909431 -3.9847922 -4.0545416 -4.1094909 -4.1473317 -4.1678143 -4.17374 -4.1809287][-4.2115021 -4.2362518 -4.2154522 -4.1548986 -4.0638638 -3.9736714 -3.9335437 -3.9586711 -4.0129848 -4.0602946 -4.0968556 -4.1251049 -4.142312 -4.1469369 -4.152245][-4.2171702 -4.2466884 -4.2366295 -4.19191 -4.1233425 -4.0574121 -4.027729 -4.0415177 -4.0721116 -4.0958385 -4.1138182 -4.1316323 -4.143157 -4.1405468 -4.1352644][-4.229126 -4.2593679 -4.2597141 -4.2315183 -4.1827917 -4.1341696 -4.1120825 -4.1155529 -4.1279163 -4.1370158 -4.14198 -4.1513252 -4.1565051 -4.1473651 -4.1295686][-4.2475724 -4.2747917 -4.28099 -4.263217 -4.2291026 -4.1918583 -4.1727743 -4.170764 -4.1732821 -4.1744952 -4.1737008 -4.1812754 -4.1870685 -4.1769428 -4.1535935][-4.2662568 -4.2890463 -4.298471 -4.2884693 -4.2634664 -4.2335835 -4.2166843 -4.211741 -4.2112761 -4.21024 -4.2090588 -4.2203879 -4.231667 -4.2268434 -4.2063613][-4.283433 -4.303535 -4.3143568 -4.3112321 -4.290925 -4.2622752 -4.2436285 -4.236392 -4.2356253 -4.2348151 -4.2350216 -4.2497354 -4.2657785 -4.2674561 -4.253705][-4.3022032 -4.3197708 -4.3312774 -4.331718 -4.3132672 -4.2846479 -4.2635732 -4.2542386 -4.2527809 -4.2515593 -4.2501206 -4.2618065 -4.277523 -4.2838597 -4.2797537][-4.3181305 -4.3318486 -4.3423891 -4.3431454 -4.3272343 -4.303648 -4.2854185 -4.2761745 -4.2734938 -4.269866 -4.2640238 -4.2682643 -4.2771873 -4.2829151 -4.2850218]]...]
INFO - root - 2017-12-05 15:11:59.896062: step 19310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 75h:31m:10s remains)
INFO - root - 2017-12-05 15:12:08.513264: step 19320, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 75h:51m:27s remains)
INFO - root - 2017-12-05 15:12:16.932640: step 19330, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:26m:12s remains)
INFO - root - 2017-12-05 15:12:25.474753: step 19340, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 77h:15m:29s remains)
INFO - root - 2017-12-05 15:12:34.035406: step 19350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 75h:03m:29s remains)
INFO - root - 2017-12-05 15:12:42.649781: step 19360, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 74h:08m:07s remains)
INFO - root - 2017-12-05 15:12:51.013689: step 19370, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 73h:31m:40s remains)
INFO - root - 2017-12-05 15:12:59.580504: step 19380, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 74h:05m:03s remains)
INFO - root - 2017-12-05 15:13:07.822755: step 19390, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 74h:54m:51s remains)
INFO - root - 2017-12-05 15:13:16.409672: step 19400, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 73h:33m:33s remains)
2017-12-05 15:13:17.246462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1604209 -4.16875 -4.1874204 -4.2073183 -4.2071114 -4.1936164 -4.1726642 -4.1426797 -4.1188679 -4.1204085 -4.1375623 -4.1481528 -4.166142 -4.1914511 -4.2146568][-4.1504831 -4.1605287 -4.1836739 -4.21057 -4.2180648 -4.2086315 -4.1911459 -4.1619468 -4.1373162 -4.1329789 -4.1378388 -4.1412115 -4.160449 -4.1900821 -4.2124381][-4.1330981 -4.146029 -4.1740861 -4.2061825 -4.2197971 -4.2107882 -4.1956091 -4.1740632 -4.1553221 -4.1479077 -4.1454549 -4.1485376 -4.1645212 -4.1874785 -4.2030182][-4.1079931 -4.1292853 -4.1655407 -4.2024755 -4.2191682 -4.2088056 -4.1945562 -4.1756597 -4.162931 -4.1587806 -4.1560369 -4.1646361 -4.1772628 -4.1896944 -4.1890349][-4.0888376 -4.1141386 -4.1588144 -4.20282 -4.2244997 -4.2153106 -4.1986437 -4.1760697 -4.1640849 -4.1608763 -4.1628523 -4.1826425 -4.2043414 -4.2136722 -4.199533][-4.1078281 -4.1252246 -4.159483 -4.1957555 -4.2154055 -4.205605 -4.1872296 -4.1661816 -4.1551714 -4.1481938 -4.15427 -4.1887026 -4.2235069 -4.2393475 -4.2271924][-4.1500659 -4.1570983 -4.16864 -4.1818504 -4.1876531 -4.1663527 -4.1408706 -4.1243114 -4.1179729 -4.1143193 -4.127799 -4.1733809 -4.21584 -4.2417464 -4.2421732][-4.1796589 -4.1825604 -4.1791296 -4.1718216 -4.15923 -4.1211472 -4.0838208 -4.0653911 -4.0642323 -4.0720725 -4.0957456 -4.1443005 -4.1902127 -4.224102 -4.2390962][-4.1931748 -4.1935921 -4.1834884 -4.1656928 -4.1398025 -4.0869894 -4.0284376 -3.9932652 -3.9923007 -4.0163994 -4.05634 -4.1105075 -4.1563177 -4.1947651 -4.2183628][-4.1926184 -4.1901441 -4.1777291 -4.1579657 -4.128109 -4.0683613 -3.9917402 -3.9354012 -3.9281211 -3.9651949 -4.0231085 -4.0862274 -4.1373296 -4.1789379 -4.2031612][-4.2065244 -4.1976433 -4.1840343 -4.17118 -4.150775 -4.102026 -4.0344691 -3.9824898 -3.9707246 -4.0045218 -4.0611792 -4.1212325 -4.1726255 -4.212678 -4.2331972][-4.239923 -4.2243953 -4.210772 -4.2071457 -4.2007914 -4.17163 -4.1274953 -4.0959759 -4.0905247 -4.1142807 -4.1539917 -4.1994033 -4.2433462 -4.2766862 -4.2911654][-4.2793808 -4.261869 -4.2487907 -4.2497158 -4.2516456 -4.2374773 -4.2140079 -4.198287 -4.1971622 -4.2122893 -4.2385283 -4.2718296 -4.3077059 -4.3335576 -4.3432155][-4.3181205 -4.3032603 -4.2916422 -4.2922187 -4.2956595 -4.2888255 -4.2775378 -4.2712889 -4.2727809 -4.2826266 -4.2999969 -4.32391 -4.3493476 -4.3662944 -4.372304][-4.3421545 -4.3311777 -4.3216629 -4.318305 -4.3183155 -4.3145289 -4.3100429 -4.3100543 -4.3147068 -4.3234015 -4.3368487 -4.3537016 -4.3694396 -4.3787971 -4.3814907]]...]
INFO - root - 2017-12-05 15:13:25.851470: step 19410, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 74h:36m:07s remains)
INFO - root - 2017-12-05 15:13:34.470534: step 19420, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:57m:27s remains)
INFO - root - 2017-12-05 15:13:43.020970: step 19430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 75h:10m:56s remains)
INFO - root - 2017-12-05 15:13:51.576470: step 19440, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:54m:32s remains)
INFO - root - 2017-12-05 15:14:00.114557: step 19450, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.803 sec/batch; 69h:47m:06s remains)
INFO - root - 2017-12-05 15:14:08.678731: step 19460, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 73h:16m:31s remains)
INFO - root - 2017-12-05 15:14:17.284023: step 19470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 74h:56m:05s remains)
INFO - root - 2017-12-05 15:14:25.682966: step 19480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 75h:31m:14s remains)
INFO - root - 2017-12-05 15:14:34.226136: step 19490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 74h:38m:07s remains)
INFO - root - 2017-12-05 15:14:42.814946: step 19500, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 73h:38m:28s remains)
2017-12-05 15:14:43.531430: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.345336 -4.3466816 -4.3470688 -4.3462553 -4.3443713 -4.3424325 -4.34124 -4.3403978 -4.3402114 -4.3406305 -4.3413448 -4.341814 -4.3423247 -4.3427453 -4.3424597][-4.344439 -4.3454514 -4.3449965 -4.3432198 -4.340909 -4.3391776 -4.3389273 -4.3395076 -4.3405452 -4.3413572 -4.3410525 -4.3401303 -4.3393836 -4.3394432 -4.339983][-4.34274 -4.3419862 -4.3387022 -4.3336148 -4.3288627 -4.3268528 -4.3286467 -4.3330011 -4.33808 -4.340981 -4.341074 -4.3398676 -4.3387032 -4.3387733 -4.3398981][-4.3353119 -4.3306556 -4.3222094 -4.3122272 -4.3030248 -4.298552 -4.3010259 -4.3102651 -4.3220577 -4.3308096 -4.3352594 -4.3368225 -4.3371568 -4.3381009 -4.3402672][-4.3170247 -4.30841 -4.2945051 -4.2782731 -4.2620587 -4.2505774 -4.2485604 -4.2595911 -4.2794981 -4.299233 -4.3140597 -4.3234978 -4.3291707 -4.3336143 -4.3376808][-4.2836313 -4.2717867 -4.2545319 -4.2338476 -4.2092834 -4.18695 -4.1769609 -4.18619 -4.2095265 -4.2393665 -4.2678747 -4.2916703 -4.3079486 -4.3192034 -4.3271942][-4.2318072 -4.2197018 -4.200572 -4.174983 -4.1403246 -4.1060233 -4.090292 -4.0994048 -4.1262875 -4.1630597 -4.1994305 -4.2329407 -4.26143 -4.2850623 -4.3025556][-4.1660037 -4.1564994 -4.1384387 -4.1090889 -4.0626049 -4.0138054 -3.992245 -3.9995129 -4.0258994 -4.0653653 -4.10813 -4.1506963 -4.19091 -4.2287631 -4.26059][-4.0923853 -4.0876389 -4.0780244 -4.0545807 -4.0117979 -3.9624634 -3.9342561 -3.9307752 -3.9451818 -3.9757442 -4.0173 -4.065887 -4.1165285 -4.167182 -4.213356][-4.053483 -4.0498252 -4.04721 -4.0366778 -4.0150218 -3.9890375 -3.9701803 -3.9621246 -3.9651873 -3.9796643 -4.0076127 -4.0494208 -4.0996647 -4.153007 -4.2031703][-4.1052785 -4.0955753 -4.087605 -4.0792375 -4.0716715 -4.0710397 -4.0757833 -4.0841765 -4.0938931 -4.1042752 -4.118185 -4.141264 -4.1728692 -4.2085042 -4.2436795][-4.2112222 -4.19851 -4.185514 -4.175334 -4.1726403 -4.1838264 -4.2019749 -4.2222271 -4.2400522 -4.2517796 -4.2598534 -4.2689972 -4.2803764 -4.2932849 -4.3063693][-4.3018875 -4.2922678 -4.2830491 -4.2771745 -4.2781491 -4.2906742 -4.3102455 -4.330965 -4.3461704 -4.35348 -4.3558035 -4.3561945 -4.3556795 -4.3547087 -4.3532448][-4.3548656 -4.35044 -4.3459339 -4.3439751 -4.3474784 -4.3574767 -4.3701854 -4.3819995 -4.3897471 -4.3923059 -4.3911448 -4.387598 -4.3827171 -4.3769355 -4.3702469][-4.3729467 -4.3710647 -4.3694773 -4.3694615 -4.3723378 -4.3772531 -4.3817258 -4.3849115 -4.3860526 -4.3853583 -4.3832941 -4.3796906 -4.3749557 -4.3695316 -4.3634086]]...]
INFO - root - 2017-12-05 15:14:52.045828: step 19510, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 72h:13m:03s remains)
INFO - root - 2017-12-05 15:15:00.641755: step 19520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 75h:12m:58s remains)
INFO - root - 2017-12-05 15:15:09.151654: step 19530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 73h:34m:53s remains)
INFO - root - 2017-12-05 15:15:17.709948: step 19540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 74h:56m:57s remains)
INFO - root - 2017-12-05 15:15:26.109974: step 19550, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 74h:30m:15s remains)
INFO - root - 2017-12-05 15:15:34.551124: step 19560, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 74h:11m:40s remains)
INFO - root - 2017-12-05 15:15:43.090005: step 19570, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 76h:00m:05s remains)
INFO - root - 2017-12-05 15:15:51.658596: step 19580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 75h:21m:32s remains)
INFO - root - 2017-12-05 15:16:00.062332: step 19590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 74h:15m:04s remains)
INFO - root - 2017-12-05 15:16:08.618680: step 19600, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 74h:17m:29s remains)
2017-12-05 15:16:09.425054: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2944813 -4.2891808 -4.2851672 -4.28313 -4.2802973 -4.2798977 -4.2763958 -4.2664108 -4.2593389 -4.2620769 -4.2666368 -4.2675219 -4.269248 -4.2734504 -4.2815328][-4.2865515 -4.2771335 -4.271297 -4.2704372 -4.2689557 -4.2717657 -4.2691989 -4.2526455 -4.2419939 -4.2473197 -4.2533722 -4.2460723 -4.2411337 -4.2471938 -4.2624774][-4.2750258 -4.2594204 -4.2502222 -4.2501273 -4.2506924 -4.2545948 -4.2506061 -4.228765 -4.2159853 -4.2251725 -4.2365475 -4.2233286 -4.2126226 -4.221704 -4.2425265][-4.2520704 -4.2291861 -4.213552 -4.21317 -4.2149892 -4.2195973 -4.2162762 -4.19408 -4.1783209 -4.1897974 -4.2095866 -4.198905 -4.1866536 -4.1974478 -4.2236466][-4.2123508 -4.179131 -4.1525292 -4.1501279 -4.1550989 -4.1601391 -4.1564 -4.1322203 -4.1180921 -4.146441 -4.1818209 -4.1772695 -4.1628671 -4.1761026 -4.2029195][-4.1615038 -4.1160641 -4.0742888 -4.0639782 -4.0705342 -4.0710669 -4.0594063 -4.0246391 -4.0166454 -4.0813518 -4.1478415 -4.1555891 -4.147296 -4.166182 -4.1912923][-4.1247768 -4.0666456 -4.0033689 -3.9704211 -3.9603386 -3.9427578 -3.9122305 -3.8617468 -3.8699446 -3.9915867 -4.1067753 -4.137701 -4.1441078 -4.1688218 -4.1892514][-4.1057596 -4.0440664 -3.9676113 -3.910316 -3.8723617 -3.8250515 -3.7602139 -3.6860244 -3.7067227 -3.8777668 -4.0365281 -4.0993652 -4.1258116 -4.1574864 -4.1820621][-4.1147003 -4.0645757 -3.9939775 -3.9303248 -3.8744211 -3.8065119 -3.7166638 -3.6267629 -3.6433265 -3.8053384 -3.9616282 -4.0414982 -4.0868349 -4.1268616 -4.1650143][-4.1611485 -4.1262746 -4.070909 -4.0141935 -3.9593046 -3.8891058 -3.7971768 -3.7103746 -3.7041986 -3.8047857 -3.9177666 -3.9922705 -4.0465069 -4.0979986 -4.1547561][-4.2230206 -4.1995339 -4.15849 -4.1136603 -4.0720787 -4.01585 -3.939038 -3.8608174 -3.8327804 -3.8759267 -3.9435725 -4.0018568 -4.0535316 -4.1098638 -4.176127][-4.2736139 -4.2565708 -4.2289114 -4.199996 -4.1744823 -4.138135 -4.08528 -4.0294461 -4.0006437 -4.012218 -4.0478415 -4.0861492 -4.1274877 -4.1755905 -4.2284827][-4.3054013 -4.2949886 -4.2788515 -4.2631469 -4.2498164 -4.2296748 -4.1972628 -4.1632743 -4.1462703 -4.1510625 -4.1716905 -4.1959286 -4.2231 -4.252841 -4.2813306][-4.3176517 -4.3125014 -4.304215 -4.296207 -4.2895689 -4.2792721 -4.26059 -4.2413011 -4.232059 -4.2361031 -4.2517595 -4.2692838 -4.2852893 -4.2994475 -4.3107271][-4.317812 -4.3153691 -4.3118582 -4.30883 -4.3061728 -4.3008761 -4.2902036 -4.2800679 -4.2751913 -4.2778158 -4.2880454 -4.3002067 -4.3094587 -4.3155408 -4.3191361]]...]
INFO - root - 2017-12-05 15:16:17.814756: step 19610, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 74h:09m:47s remains)
INFO - root - 2017-12-05 15:16:26.289366: step 19620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 76h:17m:40s remains)
INFO - root - 2017-12-05 15:16:34.924577: step 19630, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 72h:31m:11s remains)
INFO - root - 2017-12-05 15:16:43.449023: step 19640, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 73h:13m:38s remains)
INFO - root - 2017-12-05 15:16:52.103827: step 19650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 76h:15m:47s remains)
INFO - root - 2017-12-05 15:17:00.486564: step 19660, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.838 sec/batch; 72h:48m:38s remains)
INFO - root - 2017-12-05 15:17:09.048490: step 19670, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 76h:34m:32s remains)
INFO - root - 2017-12-05 15:17:17.640736: step 19680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 73h:40m:54s remains)
INFO - root - 2017-12-05 15:17:25.979858: step 19690, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 75h:15m:00s remains)
INFO - root - 2017-12-05 15:17:34.538027: step 19700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:23m:32s remains)
2017-12-05 15:17:35.313385: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3587284 -4.355279 -4.3516645 -4.3477306 -4.3435569 -4.3400407 -4.3372717 -4.3358955 -4.3347425 -4.3326554 -4.3321586 -4.3335586 -4.334198 -4.3353229 -4.3408175][-4.3504515 -4.3425822 -4.3351693 -4.3284736 -4.321959 -4.3165774 -4.3132305 -4.3110342 -4.3074694 -4.3020787 -4.2991757 -4.3000884 -4.3012247 -4.3041511 -4.3132334][-4.3373537 -4.3229942 -4.3114133 -4.3034992 -4.2964282 -4.2889562 -4.2836781 -4.2769523 -4.2662663 -4.2540622 -4.2476254 -4.250494 -4.2586432 -4.2662072 -4.2793303][-4.3202467 -4.2999845 -4.2859979 -4.2786956 -4.272666 -4.265224 -4.256948 -4.2404585 -4.2174916 -4.1974583 -4.1899447 -4.197886 -4.217279 -4.2325225 -4.2499185][-4.3016191 -4.27715 -4.264276 -4.2597184 -4.2559724 -4.2468472 -4.2339592 -4.2075753 -4.1651783 -4.1314096 -4.1269603 -4.1471338 -4.1810117 -4.2076406 -4.2284522][-4.2845278 -4.2574897 -4.2461987 -4.2433457 -4.2399707 -4.2254539 -4.2036424 -4.1585908 -4.0808992 -4.0242906 -4.0394149 -4.0904889 -4.1456456 -4.1870809 -4.2127004][-4.2717881 -4.2437568 -4.2318668 -4.2272477 -4.2207069 -4.1984472 -4.1652026 -4.0948925 -3.9693189 -3.881036 -3.936239 -4.0361915 -4.1192627 -4.1751909 -4.1992078][-4.2662206 -4.23731 -4.2233214 -4.2151947 -4.2038665 -4.1792941 -4.1419106 -4.0639987 -3.9249861 -3.8288143 -3.902204 -4.0183015 -4.1100359 -4.172627 -4.19666][-4.2658463 -4.2354236 -4.2215681 -4.2143312 -4.2043824 -4.1845603 -4.1537528 -4.0951629 -3.9973993 -3.9309175 -3.9791563 -4.0643015 -4.1353498 -4.1885834 -4.2105618][-4.268899 -4.2385378 -4.2267914 -4.2237439 -4.2181926 -4.2038612 -4.186563 -4.1604276 -4.1104264 -4.064558 -4.0850854 -4.1381288 -4.1815672 -4.2139606 -4.2293119][-4.2732711 -4.2451916 -4.2375007 -4.2367435 -4.2327814 -4.2237749 -4.220942 -4.2168918 -4.1903081 -4.150629 -4.1532264 -4.187603 -4.2175026 -4.2357621 -4.2437487][-4.2769203 -4.2522449 -4.2487974 -4.2505736 -4.2460027 -4.2398071 -4.2427745 -4.244524 -4.2234626 -4.183352 -4.1724043 -4.1945786 -4.2228713 -4.2418838 -4.2527113][-4.2856011 -4.2646322 -4.2623076 -4.2637191 -4.2589207 -4.2533097 -4.2548637 -4.2534256 -4.2313366 -4.195292 -4.1804357 -4.1953034 -4.2250986 -4.2514 -4.2707629][-4.3042636 -4.2880645 -4.2851448 -4.2845039 -4.2797318 -4.275022 -4.2738094 -4.2677765 -4.2476296 -4.2225628 -4.2147126 -4.2300568 -4.2570381 -4.2830362 -4.3042426][-4.3283873 -4.3183866 -4.3161039 -4.3141088 -4.3120556 -4.3113065 -4.3098736 -4.3046675 -4.2919979 -4.2784023 -4.2773237 -4.2879443 -4.3029943 -4.3192716 -4.3362813]]...]
INFO - root - 2017-12-05 15:17:43.855587: step 19710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 76h:38m:16s remains)
INFO - root - 2017-12-05 15:17:52.363418: step 19720, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 77h:25m:00s remains)
INFO - root - 2017-12-05 15:18:00.884828: step 19730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 73h:37m:36s remains)
INFO - root - 2017-12-05 15:18:09.433668: step 19740, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:49m:29s remains)
INFO - root - 2017-12-05 15:18:18.015942: step 19750, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 76h:43m:47s remains)
INFO - root - 2017-12-05 15:18:26.624039: step 19760, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 74h:53m:05s remains)
INFO - root - 2017-12-05 15:18:35.026132: step 19770, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 72h:19m:23s remains)
INFO - root - 2017-12-05 15:18:43.557901: step 19780, loss = 2.11, batch loss = 2.05 (9.5 examples/sec; 0.840 sec/batch; 72h:55m:32s remains)
INFO - root - 2017-12-05 15:18:52.122447: step 19790, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 76h:01m:21s remains)
INFO - root - 2017-12-05 15:19:00.438587: step 19800, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.850 sec/batch; 73h:51m:55s remains)
2017-12-05 15:19:01.186136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2726097 -4.2549706 -4.2299409 -4.1879721 -4.1525674 -4.1330323 -4.111073 -4.0969524 -4.1180291 -4.1496792 -4.1763029 -4.2014785 -4.2137704 -4.218564 -4.225575][-4.2743163 -4.2559052 -4.2320318 -4.1940732 -4.1646523 -4.1504893 -4.1300216 -4.1142731 -4.1367068 -4.1671872 -4.1880746 -4.2097206 -4.2186584 -4.2222929 -4.2307591][-4.2791882 -4.2624345 -4.2398639 -4.2091322 -4.1874814 -4.1767673 -4.1581974 -4.1446171 -4.165731 -4.1891327 -4.1990004 -4.2102509 -4.211565 -4.2122927 -4.2249646][-4.2798162 -4.2652278 -4.2423959 -4.2165718 -4.198432 -4.1896453 -4.1754265 -4.1658287 -4.186429 -4.2047539 -4.2030478 -4.2002864 -4.1945338 -4.1965823 -4.2150769][-4.2769313 -4.2624564 -4.2378488 -4.209866 -4.1859069 -4.1696248 -4.1477461 -4.1340637 -4.15511 -4.180738 -4.1825914 -4.1798191 -4.1733956 -4.1763706 -4.1936817][-4.2794905 -4.2633762 -4.2325912 -4.1943007 -4.1561065 -4.1163697 -4.0685492 -4.0388765 -4.0693603 -4.11826 -4.1413116 -4.154923 -4.1580162 -4.1656303 -4.1810403][-4.2847385 -4.2650981 -4.2256837 -4.1765294 -4.1206551 -4.0501456 -3.9660926 -3.9152982 -3.9684155 -4.0577779 -4.1125469 -4.1475458 -4.1604857 -4.1683645 -4.1766653][-4.2829394 -4.2645159 -4.2270436 -4.180758 -4.1238256 -4.0449262 -3.94768 -3.8895617 -3.9467607 -4.0505772 -4.1213226 -4.1692286 -4.1872091 -4.1876297 -4.1834407][-4.2802606 -4.2713857 -4.2495804 -4.2195735 -4.17709 -4.1125731 -4.0389152 -3.9938416 -4.0289631 -4.1056466 -4.1631093 -4.2055469 -4.2186255 -4.2112727 -4.2007918][-4.2816873 -4.28519 -4.28173 -4.2691631 -4.2419434 -4.1957893 -4.1481943 -4.1171436 -4.1321516 -4.1781082 -4.2157269 -4.24531 -4.2507877 -4.2340584 -4.2195354][-4.2856731 -4.2970333 -4.3041487 -4.3039875 -4.2906532 -4.2620521 -4.2342725 -4.211947 -4.2166195 -4.2432146 -4.2647433 -4.2822061 -4.2810287 -4.2587171 -4.2428021][-4.288013 -4.3013377 -4.3128586 -4.3189888 -4.3149195 -4.298583 -4.2835631 -4.2692389 -4.2715287 -4.2861304 -4.2953892 -4.303865 -4.2999563 -4.277802 -4.2664423][-4.2889957 -4.2969055 -4.3055749 -4.3137255 -4.3156123 -4.3082209 -4.2983842 -4.2871275 -4.2877431 -4.2946334 -4.2990046 -4.3053813 -4.3041062 -4.2875967 -4.280911][-4.2937107 -4.2936182 -4.2954626 -4.3007989 -4.3031321 -4.3000097 -4.2925992 -4.2811675 -4.2807245 -4.2848954 -4.2893362 -4.2975225 -4.3010449 -4.2924991 -4.2893977][-4.3030157 -4.2956905 -4.2902069 -4.2895627 -4.2892265 -4.287508 -4.2826018 -4.2741613 -4.2746358 -4.2794042 -4.2845869 -4.2921357 -4.2963166 -4.2915826 -4.2884774]]...]
INFO - root - 2017-12-05 15:19:09.775016: step 19810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:22m:37s remains)
INFO - root - 2017-12-05 15:19:18.324189: step 19820, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 73h:47m:40s remains)
INFO - root - 2017-12-05 15:19:26.969461: step 19830, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 76h:13m:48s remains)
INFO - root - 2017-12-05 15:19:35.554214: step 19840, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 73h:16m:12s remains)
INFO - root - 2017-12-05 15:19:44.143595: step 19850, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 72h:25m:40s remains)
INFO - root - 2017-12-05 15:19:52.832742: step 19860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 75h:27m:34s remains)
INFO - root - 2017-12-05 15:20:01.424062: step 19870, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 74h:21m:12s remains)
INFO - root - 2017-12-05 15:20:09.904549: step 19880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 75h:14m:25s remains)
INFO - root - 2017-12-05 15:20:18.406603: step 19890, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 73h:56m:13s remains)
INFO - root - 2017-12-05 15:20:27.000172: step 19900, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 74h:13m:16s remains)
2017-12-05 15:20:27.709794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0094209 -3.9708779 -3.9754527 -4.0163741 -4.06448 -4.0867257 -4.1058888 -4.1140327 -4.1145496 -4.1206579 -4.1296682 -4.13728 -4.13195 -4.1231441 -4.12297][-4.0257673 -4.0066466 -4.0206532 -4.0527573 -4.0880346 -4.1004562 -4.1109214 -4.1131153 -4.1092758 -4.1109943 -4.1120324 -4.1159019 -4.1114268 -4.1065922 -4.115407][-4.0475969 -4.0446382 -4.0594 -4.0792284 -4.1080155 -4.11675 -4.118834 -4.1096377 -4.0994382 -4.093173 -4.0779181 -4.0678906 -4.0609574 -4.0621319 -4.0778551][-4.0740123 -4.0774021 -4.0941925 -4.1104193 -4.1329708 -4.1374707 -4.1306343 -4.1081953 -4.0829535 -4.0632076 -4.0314808 -4.0033016 -3.9961922 -4.0064192 -4.0275078][-4.0870633 -4.0903859 -4.1108932 -4.1279283 -4.1413479 -4.135778 -4.1194272 -4.0887971 -4.0528035 -4.0248222 -3.9899015 -3.951844 -3.9431159 -3.9601707 -3.9844761][-4.0819321 -4.0798588 -4.1022367 -4.1219707 -4.1252079 -4.1057868 -4.0791645 -4.0475035 -4.0096226 -3.9805913 -3.9545898 -3.9242463 -3.9206393 -3.9366512 -3.9559145][-4.0739007 -4.0643454 -4.082521 -4.0955653 -4.0856462 -4.0503955 -4.0124655 -3.9862287 -3.9642375 -3.9507556 -3.9429221 -3.9313376 -3.9375486 -3.9510202 -3.9649873][-4.0606575 -4.0414658 -4.0476036 -4.0505271 -4.0326581 -3.9916692 -3.9518075 -3.9403398 -3.9438989 -3.954051 -3.9674797 -3.9697411 -3.9775805 -3.9877157 -3.9976041][-4.0329981 -4.0058622 -4.0072608 -4.0118122 -3.9994118 -3.972877 -3.9491704 -3.9540405 -3.9690268 -3.982512 -3.9992752 -4.00382 -4.0107746 -4.0174589 -4.0225005][-4.00733 -3.9848454 -3.9915786 -4.0041347 -4.0035629 -3.9953651 -3.9876134 -3.9981148 -4.0051866 -4.0054946 -4.0135813 -4.015676 -4.022471 -4.0257945 -4.0306187][-4.0027442 -3.9912152 -4.0044785 -4.0227857 -4.0280948 -4.0272961 -4.0257592 -4.0334678 -4.0320334 -4.022634 -4.0210457 -4.021297 -4.0304174 -4.0359812 -4.0432634][-4.0336013 -4.0344782 -4.0555916 -4.0766754 -4.0825315 -4.081861 -4.0807571 -4.0834947 -4.0797291 -4.068119 -4.0607052 -4.0588088 -4.069458 -4.0763965 -4.0831847][-4.1058292 -4.1156292 -4.1406302 -4.1603541 -4.1643462 -4.1601777 -4.1540322 -4.15245 -4.1481833 -4.1382279 -4.1302404 -4.1275887 -4.1367912 -4.14274 -4.1496348][-4.1893792 -4.2026391 -4.2251024 -4.2420125 -4.2420592 -4.2344522 -4.2253947 -4.2211881 -4.2169166 -4.2087293 -4.2040114 -4.2032933 -4.2092905 -4.2140164 -4.2200208][-4.2607493 -4.2731061 -4.2907248 -4.3019848 -4.2977977 -4.2896147 -4.2806525 -4.2735124 -4.2696352 -4.2648425 -4.264708 -4.2667603 -4.2720752 -4.2759109 -4.2798762]]...]
INFO - root - 2017-12-05 15:20:36.213358: step 19910, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 75h:19m:13s remains)
INFO - root - 2017-12-05 15:20:44.784965: step 19920, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 72h:58m:44s remains)
INFO - root - 2017-12-05 15:20:53.325111: step 19930, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 73h:15m:34s remains)
INFO - root - 2017-12-05 15:21:01.880359: step 19940, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 74h:38m:12s remains)
INFO - root - 2017-12-05 15:21:10.604058: step 19950, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 74h:24m:17s remains)
INFO - root - 2017-12-05 15:21:19.215340: step 19960, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 73h:41m:32s remains)
INFO - root - 2017-12-05 15:21:27.751079: step 19970, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 73h:24m:49s remains)
INFO - root - 2017-12-05 15:21:36.331474: step 19980, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 71h:03m:31s remains)
INFO - root - 2017-12-05 15:21:44.745993: step 19990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 74h:54m:41s remains)
INFO - root - 2017-12-05 15:21:53.324734: step 20000, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 73h:58m:19s remains)
2017-12-05 15:21:54.041870: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2475743 -4.2469668 -4.25869 -4.2730312 -4.2874742 -4.2899427 -4.2825413 -4.2770052 -4.27479 -4.267766 -4.2527676 -4.2368832 -4.240798 -4.2542863 -4.2669592][-4.1982541 -4.1939588 -4.2063813 -4.2243271 -4.2443051 -4.2452116 -4.23195 -4.2245359 -4.2220144 -4.2133751 -4.1959753 -4.1783524 -4.1835461 -4.2029023 -4.2229252][-4.1558294 -4.1478491 -4.1610074 -4.1800513 -4.1945763 -4.1890244 -4.1665821 -4.1558495 -4.1554585 -4.1514311 -4.1397276 -4.1261497 -4.1341071 -4.1592488 -4.187274][-4.1317787 -4.1206045 -4.1336584 -4.1465178 -4.1474938 -4.132678 -4.0978179 -4.077733 -4.082078 -4.0919018 -4.0913782 -4.0864334 -4.104919 -4.1394925 -4.1726847][-4.1176481 -4.1055036 -4.1140337 -4.10922 -4.0945272 -4.0666471 -4.0105305 -3.9690695 -3.9819083 -4.0156145 -4.0289087 -4.040688 -4.0809717 -4.1296711 -4.169404][-4.1145797 -4.0956464 -4.089922 -4.056437 -4.0168848 -3.9681909 -3.8713112 -3.7945716 -3.8350949 -3.9081404 -3.9396081 -3.9766545 -4.0466447 -4.110888 -4.1602397][-4.1430483 -4.1138992 -4.0852442 -4.0099525 -3.9338059 -3.8561816 -3.7053835 -3.5896502 -3.6863012 -3.8107533 -3.8579571 -3.91167 -4.0114756 -4.0900764 -4.1485991][-4.179913 -4.1476746 -4.1022654 -4.0017977 -3.9140856 -3.8415685 -3.6913304 -3.5774515 -3.6944265 -3.8244376 -3.8576617 -3.8998804 -4.0054975 -4.0851984 -4.1445827][-4.2200408 -4.1876063 -4.1409349 -4.0528469 -3.9903793 -3.9504387 -3.8478029 -3.7642269 -3.8414235 -3.9233155 -3.9297636 -3.951097 -4.0366712 -4.1064997 -4.159472][-4.2489133 -4.2203565 -4.1800585 -4.1222534 -4.0878892 -4.0731239 -4.0105762 -3.951479 -3.9911642 -4.03484 -4.0315313 -4.0358071 -4.0958104 -4.1523385 -4.1939073][-4.2663145 -4.2468424 -4.2205067 -4.1925349 -4.1806526 -4.1793242 -4.1431794 -4.1018395 -4.1181655 -4.1367617 -4.1280174 -4.1221247 -4.157692 -4.200305 -4.2301631][-4.2850571 -4.2766728 -4.2635174 -4.2564678 -4.2582054 -4.2608795 -4.2399635 -4.2125435 -4.2139106 -4.2171197 -4.2083368 -4.2016735 -4.2189507 -4.2443962 -4.2607336][-4.2933512 -4.29018 -4.2871089 -4.2905493 -4.2969875 -4.3001986 -4.2903862 -4.2765703 -4.2709641 -4.2653627 -4.260077 -4.2563915 -4.2617035 -4.2735834 -4.281837][-4.2963352 -4.2926159 -4.2928944 -4.2963696 -4.3011336 -4.3033829 -4.3013468 -4.2979932 -4.29415 -4.2900043 -4.2863455 -4.2842441 -4.2865353 -4.2929049 -4.3002644][-4.3094654 -4.3048263 -4.3035126 -4.30424 -4.30553 -4.3072524 -4.30823 -4.3091569 -4.3092356 -4.310472 -4.3097086 -4.3074741 -4.3083262 -4.313138 -4.320745]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 15:22:03.271045: step 20010, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 75h:42m:46s remains)
INFO - root - 2017-12-05 15:22:11.778383: step 20020, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 74h:58m:39s remains)
INFO - root - 2017-12-05 15:22:20.305930: step 20030, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 73h:34m:39s remains)
INFO - root - 2017-12-05 15:22:28.911885: step 20040, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 75h:08m:58s remains)
INFO - root - 2017-12-05 15:22:37.557061: step 20050, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 76h:05m:37s remains)
INFO - root - 2017-12-05 15:22:46.111924: step 20060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 74h:17m:16s remains)
INFO - root - 2017-12-05 15:22:54.731052: step 20070, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 74h:03m:17s remains)
INFO - root - 2017-12-05 15:23:03.316172: step 20080, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 74h:16m:27s remains)
INFO - root - 2017-12-05 15:23:11.812717: step 20090, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:20m:11s remains)
INFO - root - 2017-12-05 15:23:20.273515: step 20100, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 76h:12m:43s remains)
2017-12-05 15:23:20.988066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2521086 -4.253377 -4.2604213 -4.2684221 -4.2718897 -4.2723489 -4.2683935 -4.2656522 -4.2660542 -4.2668681 -4.2736673 -4.2828712 -4.2943721 -4.3092303 -4.3201504][-4.1958113 -4.19552 -4.2082119 -4.2244577 -4.2335324 -4.2318096 -4.2205334 -4.2113442 -4.2133365 -4.2223525 -4.2398586 -4.2569785 -4.2707577 -4.2881837 -4.304913][-4.1467175 -4.1384606 -4.1489511 -4.1715093 -4.1841245 -4.1823792 -4.171032 -4.1570783 -4.1561971 -4.1711712 -4.2007308 -4.22871 -4.249794 -4.2712936 -4.2927117][-4.115531 -4.096209 -4.1007581 -4.1292439 -4.1464009 -4.1441927 -4.138598 -4.127851 -4.1244035 -4.1414132 -4.1797786 -4.2155747 -4.2435718 -4.26939 -4.2923965][-4.0934854 -4.0642266 -4.0635104 -4.0916572 -4.1042061 -4.1014061 -4.1042013 -4.1053753 -4.1096373 -4.1329327 -4.1759734 -4.2131686 -4.2413282 -4.2699127 -4.2957144][-4.0742078 -4.046422 -4.0546532 -4.0779557 -4.0774617 -4.0707345 -4.0723629 -4.0834632 -4.0989141 -4.1269917 -4.1742673 -4.2153568 -4.2432165 -4.2708097 -4.298285][-4.0761056 -4.0485148 -4.0623608 -4.0810761 -4.070601 -4.0570636 -4.0483189 -4.0594172 -4.0863051 -4.1176038 -4.1664953 -4.214849 -4.2459731 -4.2719283 -4.2963514][-4.090107 -4.0617561 -4.07022 -4.0857921 -4.0755711 -4.0577874 -4.0365596 -4.0425992 -4.0760512 -4.109364 -4.1561561 -4.2071934 -4.2428737 -4.2665458 -4.2868567][-4.0880561 -4.0653238 -4.0767965 -4.0964627 -4.0872035 -4.0633788 -4.0319481 -4.0340829 -4.0710931 -4.104785 -4.1539803 -4.2067828 -4.2443376 -4.2653689 -4.2827182][-4.07393 -4.0592184 -4.073226 -4.0926266 -4.0913429 -4.07904 -4.0579123 -4.0645785 -4.1018491 -4.1328106 -4.1783061 -4.2252617 -4.2599463 -4.276916 -4.2887192][-4.066184 -4.051558 -4.060369 -4.0722718 -4.0796528 -4.0873713 -4.0839353 -4.0987759 -4.1379428 -4.1705637 -4.209548 -4.2482686 -4.2785487 -4.296658 -4.3019853][-4.0883312 -4.0716138 -4.0743856 -4.0777807 -4.0859737 -4.1019459 -4.11116 -4.1302876 -4.1663446 -4.1998434 -4.2326064 -4.2649837 -4.296638 -4.320549 -4.3229985][-4.136054 -4.1223912 -4.1224837 -4.1202087 -4.1210093 -4.1317458 -4.140799 -4.1549964 -4.1862717 -4.2193642 -4.2504754 -4.2793331 -4.3119168 -4.3396659 -4.3430495][-4.1715827 -4.16729 -4.168045 -4.1664557 -4.1632624 -4.1670613 -4.1714096 -4.1816645 -4.2043319 -4.2326684 -4.261694 -4.2878428 -4.3185883 -4.3443365 -4.3494015][-4.1972723 -4.1984062 -4.2014942 -4.2019386 -4.2017078 -4.2010283 -4.2016268 -4.2101502 -4.2275167 -4.2480159 -4.2690849 -4.2911086 -4.3173676 -4.3388505 -4.3450909]]...]
INFO - root - 2017-12-05 15:23:29.656179: step 20110, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 71h:25m:01s remains)
INFO - root - 2017-12-05 15:23:38.114012: step 20120, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 74h:51m:34s remains)
INFO - root - 2017-12-05 15:23:46.634815: step 20130, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 74h:53m:25s remains)
INFO - root - 2017-12-05 15:23:55.289687: step 20140, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:43m:21s remains)
INFO - root - 2017-12-05 15:24:03.868159: step 20150, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 75h:08m:41s remains)
INFO - root - 2017-12-05 15:24:12.448874: step 20160, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 74h:01m:43s remains)
INFO - root - 2017-12-05 15:24:20.962951: step 20170, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.801 sec/batch; 69h:30m:24s remains)
INFO - root - 2017-12-05 15:24:29.448614: step 20180, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.769 sec/batch; 66h:43m:09s remains)
INFO - root - 2017-12-05 15:24:38.001687: step 20190, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 76h:10m:09s remains)
INFO - root - 2017-12-05 15:24:46.581432: step 20200, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 75h:06m:19s remains)
2017-12-05 15:24:47.441977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31471 -4.3028388 -4.2942739 -4.2898965 -4.2897997 -4.2921648 -4.2934852 -4.2923417 -4.2912388 -4.2906175 -4.2897272 -4.2878561 -4.2847075 -4.2807465 -4.2791882][-4.3056731 -4.29464 -4.2866364 -4.2827926 -4.2845187 -4.290442 -4.2962418 -4.29993 -4.3027334 -4.3050489 -4.3053975 -4.3036089 -4.2998271 -4.2947307 -4.2914119][-4.2982349 -4.2903662 -4.2848716 -4.2824092 -4.2847724 -4.2904773 -4.2955046 -4.2981915 -4.3003182 -4.3035741 -4.3063474 -4.3062906 -4.3031182 -4.2982545 -4.2947903][-4.2985044 -4.2939 -4.2903237 -4.288094 -4.2877307 -4.2880054 -4.2857695 -4.2816982 -4.2796183 -4.28264 -4.2884078 -4.2906332 -4.28962 -4.2865825 -4.2851133][-4.2930555 -4.2886147 -4.2836008 -4.27873 -4.2737384 -4.266273 -4.2535071 -4.2366791 -4.2272983 -4.2329588 -4.2450619 -4.2514439 -4.2548866 -4.2579226 -4.2635021][-4.2670045 -4.2583427 -4.2478018 -4.2387462 -4.2287154 -4.2107239 -4.1821651 -4.1480379 -4.1300716 -4.1444993 -4.1716623 -4.1916103 -4.2061381 -4.2167039 -4.2279983][-4.2162743 -4.2000575 -4.1831465 -4.1730657 -4.1632524 -4.1368909 -4.0935535 -4.0415382 -4.0142474 -4.0440702 -4.0989947 -4.1424961 -4.1713123 -4.1881318 -4.1994905][-4.1520419 -4.1289487 -4.1068931 -4.1001215 -4.0999317 -4.0788527 -4.0380363 -3.9867969 -3.962429 -4.0036893 -4.071085 -4.1219993 -4.1559582 -4.1779432 -4.193665][-4.1067319 -4.0787568 -4.0556231 -4.05335 -4.0672135 -4.0680108 -4.0566826 -4.0394993 -4.0336766 -4.0654964 -4.1104484 -4.1460009 -4.1771545 -4.2042356 -4.225421][-4.1074595 -4.0775747 -4.0567684 -4.0577383 -4.0815506 -4.1061907 -4.1269131 -4.1433325 -4.1551509 -4.1745262 -4.1926794 -4.2090712 -4.2311573 -4.2550311 -4.273859][-4.1564822 -4.1316833 -4.1164632 -4.118309 -4.1422729 -4.1751528 -4.2098808 -4.2374167 -4.2538362 -4.2640276 -4.26878 -4.2749929 -4.289278 -4.3050594 -4.3156667][-4.2304864 -4.2147236 -4.2038217 -4.2034769 -4.220612 -4.2475772 -4.2760229 -4.2966175 -4.3068409 -4.3092265 -4.30735 -4.3099322 -4.3191247 -4.3278561 -4.330677][-4.2819529 -4.2748113 -4.2677946 -4.2653584 -4.2731304 -4.288033 -4.3017888 -4.3080597 -4.3089805 -4.3057547 -4.3018103 -4.3031163 -4.3091016 -4.314486 -4.315134][-4.2897568 -4.2885695 -4.2851453 -4.2809191 -4.2791719 -4.2813463 -4.2843132 -4.2853384 -4.2845736 -4.2820792 -4.28056 -4.2818732 -4.2846708 -4.2875772 -4.289794][-4.2717366 -4.2731748 -4.2710023 -4.2633977 -4.2540717 -4.2492046 -4.2502813 -4.2560291 -4.2626057 -4.2668524 -4.2699003 -4.2718573 -4.2721543 -4.2736173 -4.2779655]]...]
INFO - root - 2017-12-05 15:24:55.826922: step 20210, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 73h:48m:57s remains)
INFO - root - 2017-12-05 15:25:04.204318: step 20220, loss = 2.05, batch loss = 1.99 (10.4 examples/sec; 0.772 sec/batch; 66h:55m:58s remains)
INFO - root - 2017-12-05 15:25:12.830066: step 20230, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 75h:53m:30s remains)
INFO - root - 2017-12-05 15:25:21.329569: step 20240, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 71h:27m:56s remains)
INFO - root - 2017-12-05 15:25:29.929988: step 20250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 74h:04m:50s remains)
INFO - root - 2017-12-05 15:25:38.470771: step 20260, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 74h:02m:22s remains)
INFO - root - 2017-12-05 15:25:46.995904: step 20270, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 74h:17m:03s remains)
INFO - root - 2017-12-05 15:25:55.399012: step 20280, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.744 sec/batch; 64h:31m:05s remains)
INFO - root - 2017-12-05 15:26:04.064584: step 20290, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 75h:38m:24s remains)
INFO - root - 2017-12-05 15:26:12.681648: step 20300, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 74h:24m:02s remains)
2017-12-05 15:26:13.443208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181478 -4.22469 -4.2212939 -4.2131724 -4.2007136 -4.1889586 -4.1867933 -4.1927381 -4.1991596 -4.1952648 -4.1890287 -4.1849394 -4.1914473 -4.20121 -4.2065868][-4.2175794 -4.2269979 -4.2267962 -4.221292 -4.20894 -4.1960034 -4.1917825 -4.1933813 -4.1927695 -4.180584 -4.172205 -4.1733465 -4.1847391 -4.1967907 -4.2030034][-4.1990476 -4.2146258 -4.2217841 -4.2209449 -4.2096272 -4.1944938 -4.1836395 -4.1784945 -4.1784706 -4.1738834 -4.175334 -4.1818709 -4.1906533 -4.202888 -4.2104015][-4.18627 -4.2051063 -4.2157154 -4.21245 -4.2023754 -4.1847296 -4.1657672 -4.156209 -4.16466 -4.1763468 -4.1904435 -4.1996889 -4.2006946 -4.2080722 -4.2139716][-4.1870537 -4.2042794 -4.210156 -4.196198 -4.1825628 -4.1620021 -4.1319265 -4.1139507 -4.13148 -4.1686916 -4.201405 -4.2134171 -4.2107649 -4.208797 -4.20808][-4.1944456 -4.2052727 -4.2014713 -4.1792107 -4.1584849 -4.126708 -4.0739131 -4.034586 -4.05844 -4.1281304 -4.1869574 -4.2102442 -4.2131891 -4.2104158 -4.2059817][-4.1929369 -4.1927238 -4.1800847 -4.1552949 -4.12922 -4.0808387 -3.9975586 -3.9238675 -3.9573517 -4.0674677 -4.1569057 -4.194972 -4.2055888 -4.2081323 -4.2056131][-4.1925983 -4.184772 -4.1693811 -4.1464825 -4.1141567 -4.0507355 -3.9420238 -3.8391056 -3.8877525 -4.0273471 -4.1348715 -4.1792207 -4.1935077 -4.1975012 -4.1947107][-4.1987281 -4.1941481 -4.1860485 -4.1676025 -4.132318 -4.066988 -3.9665561 -3.878895 -3.9271483 -4.0495772 -4.1362233 -4.1688361 -4.1783805 -4.1803145 -4.1785131][-4.2142568 -4.2200594 -4.2155957 -4.1974258 -4.1623611 -4.1068673 -4.0362992 -3.9862876 -4.0244751 -4.1070046 -4.1577916 -4.1740937 -4.1785531 -4.1783547 -4.1782188][-4.2303658 -4.2438788 -4.2409754 -4.2226291 -4.1902337 -4.14651 -4.1042347 -4.0832753 -4.1111379 -4.1589656 -4.1845379 -4.1933546 -4.1992235 -4.2007341 -4.2001948][-4.2326069 -4.2459488 -4.2436972 -4.229219 -4.1997852 -4.1639881 -4.137773 -4.1344271 -4.1566038 -4.185709 -4.2027879 -4.2122569 -4.2184448 -4.2208872 -4.2203989][-4.2246928 -4.2330756 -4.23114 -4.218401 -4.1914263 -4.1606388 -4.1458426 -4.1528025 -4.1726303 -4.19089 -4.2070293 -4.2233105 -4.232326 -4.233582 -4.23127][-4.2121062 -4.2158647 -4.2153206 -4.206409 -4.1841383 -4.158452 -4.1470594 -4.1554852 -4.1703291 -4.1791816 -4.1923056 -4.2103925 -4.2182174 -4.2174459 -4.2154336][-4.21965 -4.2172818 -4.2107296 -4.1987891 -4.1784687 -4.15957 -4.1534929 -4.1608167 -4.1669297 -4.164288 -4.1677089 -4.1800876 -4.1832151 -4.178761 -4.177309]]...]
INFO - root - 2017-12-05 15:26:22.016341: step 20310, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 74h:46m:32s remains)
INFO - root - 2017-12-05 15:26:30.556225: step 20320, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 77h:23m:30s remains)
INFO - root - 2017-12-05 15:26:38.963419: step 20330, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 76h:55m:28s remains)
INFO - root - 2017-12-05 15:26:47.538742: step 20340, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 75h:26m:11s remains)
INFO - root - 2017-12-05 15:26:56.141268: step 20350, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 76h:07m:24s remains)
INFO - root - 2017-12-05 15:27:04.764470: step 20360, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.866 sec/batch; 75h:04m:58s remains)
INFO - root - 2017-12-05 15:27:13.332107: step 20370, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 75h:01m:13s remains)
INFO - root - 2017-12-05 15:27:21.746474: step 20380, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.762 sec/batch; 66h:03m:58s remains)
INFO - root - 2017-12-05 15:27:30.294209: step 20390, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 74h:01m:33s remains)
INFO - root - 2017-12-05 15:27:38.802705: step 20400, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 73h:04m:51s remains)
2017-12-05 15:27:39.549277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1808453 -4.2049756 -4.2173905 -4.2207179 -4.2186346 -4.2222352 -4.2224846 -4.2190633 -4.2164307 -4.215529 -4.2212343 -4.2329359 -4.24132 -4.2446961 -4.2487464][-4.1538668 -4.1741257 -4.1876273 -4.1904106 -4.1884985 -4.1889868 -4.1852374 -4.1800437 -4.1755843 -4.1704464 -4.1725259 -4.1844096 -4.2012572 -4.2226648 -4.2479496][-4.1243024 -4.1362882 -4.1471357 -4.1476274 -4.14587 -4.1469564 -4.147058 -4.1470509 -4.1453962 -4.1375093 -4.1310778 -4.1348982 -4.1520958 -4.18283 -4.2228155][-4.1073985 -4.1163988 -4.1307106 -4.13593 -4.1388345 -4.14262 -4.145442 -4.1445127 -4.1371479 -4.1211257 -4.1046677 -4.1006508 -4.1150079 -4.1480975 -4.198019][-4.1283903 -4.1369286 -4.152885 -4.1630707 -4.1703997 -4.1737785 -4.1709647 -4.1583142 -4.134131 -4.103529 -4.0800676 -4.0820575 -4.1060386 -4.1462374 -4.2012777][-4.1863122 -4.1939535 -4.2046018 -4.2130771 -4.2176104 -4.21298 -4.1965256 -4.1648812 -4.1213851 -4.0796652 -4.0600553 -4.0818467 -4.1254854 -4.1747293 -4.2295389][-4.2457223 -4.2516141 -4.257647 -4.2629447 -4.2613034 -4.2465844 -4.2148881 -4.1662788 -4.1095271 -4.0647597 -4.05847 -4.1042061 -4.1654925 -4.2181735 -4.2644587][-4.2856278 -4.2880135 -4.2897983 -4.29057 -4.2829437 -4.2615824 -4.2238789 -4.1727705 -4.11935 -4.0838242 -4.0925093 -4.1492772 -4.2144418 -4.2631555 -4.2963977][-4.299458 -4.3008108 -4.3007622 -4.2981381 -4.2867684 -4.2659588 -4.2343173 -4.1957369 -4.1581745 -4.137579 -4.1527839 -4.2030673 -4.2560349 -4.2922354 -4.3108621][-4.280828 -4.2837772 -4.2870078 -4.2861576 -4.2784696 -4.2666364 -4.2494106 -4.2298093 -4.2115173 -4.2041526 -4.2176127 -4.2503576 -4.2821412 -4.3018365 -4.307128][-4.2451587 -4.2533607 -4.2632842 -4.2691879 -4.2701464 -4.2696424 -4.2677503 -4.2660642 -4.2635851 -4.2624183 -4.2682796 -4.2815366 -4.2934718 -4.2994123 -4.2977128][-4.2228575 -4.236371 -4.2516913 -4.26326 -4.2709913 -4.2790933 -4.2870164 -4.2948027 -4.299614 -4.3006759 -4.3010416 -4.3018293 -4.3005748 -4.2976527 -4.2934151][-4.2262936 -4.2417912 -4.2582703 -4.2700043 -4.2779326 -4.28686 -4.2959471 -4.3034577 -4.3074336 -4.3070841 -4.30534 -4.3027663 -4.2970576 -4.2919245 -4.2891378][-4.2411709 -4.2555213 -4.2697062 -4.2778392 -4.2815347 -4.2853789 -4.2885318 -4.2896409 -4.2886972 -4.2859244 -4.2840896 -4.2831821 -4.2804813 -4.2796254 -4.2818375][-4.2584205 -4.2686572 -4.2787881 -4.2831135 -4.2827682 -4.2810769 -4.2779741 -4.2727594 -4.2662091 -4.2604527 -4.2583365 -4.2597947 -4.2611594 -4.2648497 -4.2718291]]...]
INFO - root - 2017-12-05 15:27:48.073956: step 20410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 74h:11m:04s remains)
INFO - root - 2017-12-05 15:27:56.645677: step 20420, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.879 sec/batch; 76h:11m:28s remains)
INFO - root - 2017-12-05 15:28:05.035616: step 20430, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 72h:14m:28s remains)
INFO - root - 2017-12-05 15:28:13.473346: step 20440, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 75h:17m:27s remains)
INFO - root - 2017-12-05 15:28:21.945733: step 20450, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 75h:25m:40s remains)
INFO - root - 2017-12-05 15:28:30.549948: step 20460, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 73h:44m:28s remains)
INFO - root - 2017-12-05 15:28:39.138036: step 20470, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:15m:24s remains)
INFO - root - 2017-12-05 15:28:47.612086: step 20480, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.771 sec/batch; 66h:49m:13s remains)
INFO - root - 2017-12-05 15:28:56.091840: step 20490, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 73h:11m:22s remains)
INFO - root - 2017-12-05 15:29:04.602623: step 20500, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 74h:02m:06s remains)
2017-12-05 15:29:05.337143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3299322 -4.3299403 -4.3323665 -4.3419919 -4.3540874 -4.362627 -4.367012 -4.3717647 -4.3757877 -4.3755021 -4.3707881 -4.3655362 -4.3625269 -4.361805 -4.3632021][-4.2735705 -4.2710514 -4.2742386 -4.2878757 -4.3073325 -4.3249717 -4.336297 -4.3483214 -4.3599119 -4.3650746 -4.3629589 -4.3583245 -4.3546929 -4.3529124 -4.3528762][-4.2040515 -4.1967659 -4.200367 -4.2156706 -4.2410197 -4.2665396 -4.2842231 -4.3037162 -4.3270774 -4.3428178 -4.3458624 -4.3418145 -4.3370156 -4.3323154 -4.3304567][-4.1407776 -4.1254063 -4.1248159 -4.138741 -4.1640835 -4.1916361 -4.2104497 -4.2352309 -4.2714162 -4.3025579 -4.3139133 -4.3146491 -4.3122 -4.3051167 -4.2995152][-4.1018291 -4.0788717 -4.0694351 -4.0748839 -4.0899529 -4.1065984 -4.1179976 -4.1447134 -4.196702 -4.2463222 -4.2692137 -4.2760077 -4.2791014 -4.2748542 -4.2688723][-4.0723939 -4.0389013 -4.014677 -4.0049314 -4.0017266 -3.9998209 -3.9978144 -4.0280318 -4.0995331 -4.1714077 -4.2112818 -4.227694 -4.2392173 -4.2425036 -4.24151][-4.0573635 -4.0119171 -3.9746652 -3.9495566 -3.9308956 -3.9098988 -3.8893204 -3.9156778 -4.001121 -4.0898952 -4.1473203 -4.1757584 -4.1971416 -4.2137041 -4.224843][-4.0671763 -4.0195389 -3.9825063 -3.9594078 -3.9431446 -3.9199405 -3.8887048 -3.9017167 -3.9754677 -4.0581064 -4.1167617 -4.1488471 -4.1729188 -4.1970243 -4.2195587][-4.0950246 -4.0537395 -4.0238047 -4.0102677 -4.0068288 -3.9933295 -3.9710073 -3.9782631 -4.0301628 -4.0908809 -4.1353369 -4.1574516 -4.1755915 -4.1990275 -4.2242084][-4.1310287 -4.0984144 -4.0726237 -4.0660548 -4.0711823 -4.0648723 -4.0516071 -4.0587835 -4.0956254 -4.1399193 -4.1714044 -4.1801248 -4.1883278 -4.2053909 -4.2280293][-4.1541128 -4.1288548 -4.10587 -4.1036263 -4.1132226 -4.1115856 -4.1041775 -4.1099877 -4.1371808 -4.1684446 -4.1891131 -4.1855588 -4.1841106 -4.1922207 -4.2106285][-4.1714931 -4.1515818 -4.1336465 -4.1345048 -4.1465597 -4.1481614 -4.144752 -4.1501966 -4.169673 -4.1897626 -4.1994085 -4.1865792 -4.1772223 -4.1833696 -4.2038174][-4.1912832 -4.17654 -4.1637392 -4.1644182 -4.1747 -4.1775756 -4.1756268 -4.1779423 -4.1889477 -4.1998219 -4.1973581 -4.1760287 -4.1597538 -4.1659851 -4.1896453][-4.223516 -4.21403 -4.2045164 -4.20066 -4.2038822 -4.2027626 -4.197669 -4.1940312 -4.1945677 -4.1965246 -4.1851721 -4.1585727 -4.1344843 -4.1364131 -4.1592488][-4.2613454 -4.2567205 -4.2497206 -4.2429409 -4.2399044 -4.2345581 -4.2269173 -4.2199092 -4.2143335 -4.20954 -4.1928988 -4.163763 -4.1363883 -4.1318927 -4.14719]]...]
INFO - root - 2017-12-05 15:29:13.897674: step 20510, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 75h:26m:32s remains)
INFO - root - 2017-12-05 15:29:22.307244: step 20520, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 71h:50m:48s remains)
INFO - root - 2017-12-05 15:29:30.987839: step 20530, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 72h:40m:30s remains)
INFO - root - 2017-12-05 15:29:39.392702: step 20540, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.784 sec/batch; 67h:55m:32s remains)
INFO - root - 2017-12-05 15:29:47.941242: step 20550, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 73h:30m:37s remains)
INFO - root - 2017-12-05 15:29:56.405962: step 20560, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 73h:26m:58s remains)
INFO - root - 2017-12-05 15:30:04.869148: step 20570, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 70h:46m:18s remains)
INFO - root - 2017-12-05 15:30:13.284047: step 20580, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.778 sec/batch; 67h:25m:33s remains)
INFO - root - 2017-12-05 15:30:21.787657: step 20590, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 75h:24m:16s remains)
INFO - root - 2017-12-05 15:30:30.325837: step 20600, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 75h:40m:16s remains)
2017-12-05 15:30:31.124897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2544336 -4.2580738 -4.2695751 -4.2810249 -4.2845116 -4.2844248 -4.2826562 -4.284307 -4.2972875 -4.3183427 -4.3302674 -4.3217192 -4.2973857 -4.2678781 -4.2556386][-4.2467661 -4.2497425 -4.2636414 -4.2786336 -4.2835455 -4.279398 -4.2691655 -4.263855 -4.2760277 -4.3007345 -4.3170533 -4.3126359 -4.2916689 -4.266748 -4.2575827][-4.220459 -4.2222705 -4.2400341 -4.2654409 -4.2796488 -4.274128 -4.2560024 -4.2439656 -4.2581582 -4.2876287 -4.30907 -4.3087626 -4.2915835 -4.2708831 -4.2626629][-4.17663 -4.174768 -4.1975603 -4.2375889 -4.2648158 -4.261157 -4.2377534 -4.22111 -4.2368274 -4.2706413 -4.2993321 -4.3056731 -4.2937031 -4.275733 -4.264441][-4.1153984 -4.1093163 -4.1412244 -4.19764 -4.2370868 -4.2353873 -4.2062945 -4.1816235 -4.1958003 -4.2351136 -4.27525 -4.2931056 -4.2865381 -4.2720337 -4.2572169][-4.0471234 -4.0417385 -4.0880342 -4.1592507 -4.2045555 -4.1996012 -4.1598787 -4.1236982 -4.1351547 -4.1829581 -4.2368665 -4.2677231 -4.2679653 -4.2574534 -4.24114][-4.0120697 -4.0160236 -4.0719705 -4.1423345 -4.1777506 -4.1598563 -4.1038227 -4.0547423 -4.0664644 -4.1271524 -4.197722 -4.2425842 -4.2532406 -4.2474041 -4.2309947][-4.0562658 -4.0727344 -4.1150041 -4.1573582 -4.1664882 -4.1325626 -4.066443 -4.0165091 -4.0364962 -4.1063433 -4.1815515 -4.2296715 -4.245779 -4.2433462 -4.2276368][-4.1202297 -4.1364789 -4.157279 -4.1711178 -4.1626377 -4.1336036 -4.0885015 -4.0605173 -4.0805726 -4.1343708 -4.1902122 -4.2266817 -4.2424021 -4.2422533 -4.2280307][-4.1518321 -4.1601653 -4.1661882 -4.1644492 -4.1536913 -4.1411905 -4.1273928 -4.1216154 -4.135314 -4.1678023 -4.2012172 -4.2236991 -4.2351289 -4.235611 -4.2219057][-4.1638784 -4.1611676 -4.1555595 -4.1428418 -4.1305127 -4.1293292 -4.134644 -4.141778 -4.1550579 -4.1806803 -4.2067347 -4.2244582 -4.2318993 -4.230691 -4.2158766][-4.176331 -4.1652436 -4.1504626 -4.129612 -4.11372 -4.1165438 -4.1277194 -4.1388812 -4.1532454 -4.1781111 -4.2047887 -4.2231297 -4.2288947 -4.225594 -4.2096391][-4.1922765 -4.1771851 -4.1607413 -4.1391072 -4.121438 -4.1204128 -4.126924 -4.1360512 -4.1493769 -4.172811 -4.2003789 -4.2191615 -4.2253981 -4.2236557 -4.2085781][-4.2065988 -4.1933284 -4.1816607 -4.1648588 -4.1478915 -4.1422386 -4.14333 -4.1514106 -4.1628976 -4.1825571 -4.2085314 -4.22705 -4.2339997 -4.2343392 -4.2210312][-4.2158279 -4.2065787 -4.2020078 -4.1942258 -4.1840577 -4.1793108 -4.1790395 -4.1872725 -4.1968369 -4.2109175 -4.2309575 -4.2451749 -4.250917 -4.251699 -4.23888]]...]
INFO - root - 2017-12-05 15:30:39.739922: step 20610, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 74h:00m:49s remains)
INFO - root - 2017-12-05 15:30:48.253090: step 20620, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 73h:58m:44s remains)
INFO - root - 2017-12-05 15:30:56.878278: step 20630, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 76h:28m:20s remains)
INFO - root - 2017-12-05 15:31:05.392044: step 20640, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 75h:14m:11s remains)
INFO - root - 2017-12-05 15:31:13.758652: step 20650, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 74h:20m:15s remains)
INFO - root - 2017-12-05 15:31:22.319985: step 20660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 76h:09m:33s remains)
INFO - root - 2017-12-05 15:31:30.832143: step 20670, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 74h:04m:57s remains)
INFO - root - 2017-12-05 15:31:39.314099: step 20680, loss = 2.06, batch loss = 2.00 (10.6 examples/sec; 0.754 sec/batch; 65h:20m:06s remains)
INFO - root - 2017-12-05 15:31:47.856203: step 20690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 74h:51m:23s remains)
INFO - root - 2017-12-05 15:31:56.427630: step 20700, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 72h:20m:36s remains)
2017-12-05 15:31:57.235951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2530341 -4.2274423 -4.2052779 -4.1905384 -4.1889768 -4.2060347 -4.2392526 -4.2740817 -4.30095 -4.3150482 -4.3148522 -4.3091598 -4.3083186 -4.3130641 -4.3189044][-4.2657795 -4.238306 -4.2129679 -4.1965261 -4.1949668 -4.2094398 -4.240057 -4.272974 -4.3008261 -4.3177986 -4.3222728 -4.3205295 -4.3184795 -4.3194871 -4.3214574][-4.278183 -4.2392068 -4.2018089 -4.1757159 -4.1671739 -4.1772351 -4.20898 -4.2446694 -4.2744288 -4.2948976 -4.3047957 -4.30815 -4.3077607 -4.3084416 -4.3107939][-4.2671142 -4.2112427 -4.15586 -4.1147442 -4.0938392 -4.09523 -4.1315517 -4.1790652 -4.2181592 -4.2459173 -4.2664504 -4.2791443 -4.2834897 -4.2867146 -4.2900929][-4.234704 -4.1646376 -4.0932579 -4.0379615 -4.0009389 -3.9873447 -4.022347 -4.0772161 -4.1258354 -4.1631041 -4.196734 -4.22588 -4.244369 -4.2576656 -4.2671518][-4.201252 -4.1233306 -4.0407085 -3.9677017 -3.907444 -3.8726368 -3.8982306 -3.9565527 -4.0127439 -4.0578709 -4.1049886 -4.1510496 -4.1902318 -4.2226272 -4.2443042][-4.189291 -4.1168432 -4.0304546 -3.9415174 -3.8566551 -3.7958708 -3.8018804 -3.8561192 -3.9166374 -3.9691024 -4.0292597 -4.094223 -4.154489 -4.2045326 -4.2378][-4.2071953 -4.1492367 -4.0725579 -3.9879241 -3.8974032 -3.8208003 -3.7960143 -3.8244615 -3.87639 -3.9325845 -4.0020289 -4.0796666 -4.1507521 -4.2084818 -4.2496405][-4.2451463 -4.2070594 -4.1513543 -4.0875106 -4.0165367 -3.9454389 -3.8942077 -3.8851151 -3.9166255 -3.965661 -4.031496 -4.1068192 -4.1741486 -4.2287931 -4.267447][-4.2831769 -4.2630553 -4.2243686 -4.1784053 -4.12993 -4.0768771 -4.0240378 -3.9998152 -4.0164971 -4.0494871 -4.0999374 -4.1629066 -4.2181482 -4.2598691 -4.2866068][-4.3076692 -4.3003416 -4.2757454 -4.2430944 -4.2087173 -4.1721473 -4.1352453 -4.1178465 -4.126009 -4.1443872 -4.1796632 -4.225152 -4.2617321 -4.2842627 -4.2965307][-4.3163209 -4.3150306 -4.2985687 -4.2739782 -4.24827 -4.2256866 -4.2069016 -4.2017655 -4.2094259 -4.2231054 -4.2472568 -4.275125 -4.2915 -4.2953124 -4.2939916][-4.3134966 -4.3102713 -4.2981324 -4.2813592 -4.2663474 -4.2583528 -4.2537889 -4.2577333 -4.2676044 -4.2780333 -4.2903557 -4.3023467 -4.3043756 -4.2950115 -4.283123][-4.2997551 -4.2897186 -4.2815485 -4.2767763 -4.2766657 -4.2819242 -4.2890048 -4.3007154 -4.3138385 -4.3223128 -4.3249726 -4.3238063 -4.3152261 -4.2975945 -4.2769909][-4.2834086 -4.2632308 -4.2555261 -4.2611217 -4.2741103 -4.2906408 -4.3066549 -4.3230958 -4.3383622 -4.3438272 -4.3398242 -4.3307357 -4.3163543 -4.295969 -4.2715263]]...]
INFO - root - 2017-12-05 15:32:05.721586: step 20710, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 72h:53m:04s remains)
INFO - root - 2017-12-05 15:32:14.248135: step 20720, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 74h:00m:28s remains)
INFO - root - 2017-12-05 15:32:22.744803: step 20730, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 70h:34m:24s remains)
INFO - root - 2017-12-05 15:32:31.191208: step 20740, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 75h:04m:11s remains)
INFO - root - 2017-12-05 15:32:39.757764: step 20750, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 77h:27m:26s remains)
INFO - root - 2017-12-05 15:32:48.084008: step 20760, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 72h:39m:06s remains)
INFO - root - 2017-12-05 15:32:56.635396: step 20770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 73h:38m:37s remains)
INFO - root - 2017-12-05 15:33:05.067641: step 20780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 73h:59m:44s remains)
INFO - root - 2017-12-05 15:33:13.663004: step 20790, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.875 sec/batch; 75h:46m:35s remains)
INFO - root - 2017-12-05 15:33:22.172630: step 20800, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 73h:14m:26s remains)
2017-12-05 15:33:22.916187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2612472 -4.2634006 -4.2642989 -4.2662716 -4.266223 -4.2650809 -4.2673426 -4.2736139 -4.2818584 -4.2884216 -4.2895489 -4.2869592 -4.2818947 -4.2777405 -4.2769408][-4.2605824 -4.2647276 -4.2667007 -4.2653875 -4.2578759 -4.2494392 -4.2490039 -4.2577929 -4.2725148 -4.28709 -4.2934446 -4.29249 -4.28514 -4.2775278 -4.274353][-4.2471609 -4.2529807 -4.2539277 -4.2446718 -4.2239146 -4.2034583 -4.1990361 -4.213522 -4.2418294 -4.271997 -4.2914305 -4.2977037 -4.2909045 -4.2798452 -4.2729511][-4.2160568 -4.2240181 -4.2228069 -4.202311 -4.1622086 -4.1227674 -4.1111722 -4.1343155 -4.1829696 -4.2375278 -4.2762718 -4.2936573 -4.2905278 -4.2770634 -4.2661405][-4.17647 -4.1877656 -4.1851106 -4.1537747 -4.0920672 -4.0281181 -4.0055013 -4.0377107 -4.1079988 -4.1894145 -4.2494779 -4.2791972 -4.2813478 -4.2677431 -4.2538476][-4.1575842 -4.17009 -4.1652479 -4.1237621 -4.0408878 -3.9508224 -3.9137666 -3.9514961 -4.0399694 -4.1450143 -4.2234488 -4.2633557 -4.2703843 -4.2570295 -4.2410331][-4.1796846 -4.1884484 -4.1785693 -4.1307893 -4.0387621 -3.9353304 -3.889168 -3.9251411 -4.0176325 -4.1306148 -4.2161369 -4.2593803 -4.2689853 -4.2560062 -4.2391868][-4.2192907 -4.2219806 -4.2092972 -4.166553 -4.0863347 -3.9945881 -3.9529107 -3.9807587 -4.0567255 -4.1538053 -4.2287321 -4.2683277 -4.2785759 -4.2680511 -4.2530546][-4.2499352 -4.2496147 -4.2387967 -4.2105393 -4.1578531 -4.0959873 -4.0687637 -4.0879855 -4.1371908 -4.2025509 -4.2539992 -4.281281 -4.2875104 -4.2786727 -4.2668185][-4.2650089 -4.2652283 -4.2595816 -4.2465043 -4.222116 -4.1917815 -4.180759 -4.1938825 -4.21955 -4.2528229 -4.2783332 -4.2902145 -4.2901521 -4.2822638 -4.2740493][-4.26755 -4.2693672 -4.2680573 -4.2649293 -4.25876 -4.2486386 -4.2469025 -4.2554593 -4.2670417 -4.2797384 -4.2880831 -4.290256 -4.2874541 -4.2818193 -4.27723][-4.2656446 -4.2680421 -4.2694054 -4.2710748 -4.2721663 -4.2707057 -4.2716269 -4.2762032 -4.2812057 -4.284791 -4.2860684 -4.2853088 -4.2833524 -4.2807837 -4.2790728][-4.2659054 -4.2673674 -4.269104 -4.2717071 -4.2745738 -4.27591 -4.2774816 -4.2798595 -4.2818503 -4.2819982 -4.2813978 -4.2808061 -4.2803211 -4.279871 -4.2798514][-4.2692342 -4.26905 -4.2696524 -4.2716179 -4.2741017 -4.2759743 -4.2775097 -4.2790651 -4.28002 -4.2797322 -4.279057 -4.2787066 -4.2785959 -4.2785387 -4.2787819][-4.2748289 -4.2735758 -4.2728715 -4.2738132 -4.2754326 -4.2767591 -4.2779417 -4.2791519 -4.2801557 -4.2806835 -4.280725 -4.280427 -4.2797565 -4.2786736 -4.2778034]]...]
INFO - root - 2017-12-05 15:33:31.621562: step 20810, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 74h:11m:02s remains)
INFO - root - 2017-12-05 15:33:40.121556: step 20820, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 75h:25m:35s remains)
INFO - root - 2017-12-05 15:33:48.593794: step 20830, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 73h:21m:30s remains)
INFO - root - 2017-12-05 15:33:57.134424: step 20840, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 72h:25m:47s remains)
INFO - root - 2017-12-05 15:34:05.614138: step 20850, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 73h:59m:04s remains)
INFO - root - 2017-12-05 15:34:13.927475: step 20860, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.788 sec/batch; 68h:12m:54s remains)
INFO - root - 2017-12-05 15:34:22.380605: step 20870, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 74h:54m:25s remains)
INFO - root - 2017-12-05 15:34:30.894032: step 20880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 74h:29m:52s remains)
INFO - root - 2017-12-05 15:34:39.399693: step 20890, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 74h:26m:14s remains)
INFO - root - 2017-12-05 15:34:47.946972: step 20900, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 73h:15m:00s remains)
2017-12-05 15:34:48.685214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1765203 -4.2378178 -4.2774291 -4.2942543 -4.2898974 -4.2758508 -4.2568941 -4.2371244 -4.2247372 -4.2203865 -4.2163086 -4.2132983 -4.2118516 -4.2104645 -4.209363][-4.1031489 -4.1847091 -4.2415509 -4.275219 -4.2832355 -4.2789292 -4.2686958 -4.2582464 -4.252018 -4.2497659 -4.2453136 -4.2398252 -4.2352409 -4.2323594 -4.2327981][-4.0822663 -4.1693978 -4.2298293 -4.2658877 -4.2767663 -4.2745953 -4.2666631 -4.2610192 -4.2589564 -4.2559242 -4.2473588 -4.2386894 -4.2296906 -4.2236462 -4.2253962][-4.1322689 -4.2002873 -4.2422285 -4.2653952 -4.2707205 -4.2637854 -4.2518063 -4.2484612 -4.2553453 -4.2567983 -4.2473936 -4.23647 -4.2216363 -4.2077165 -4.2040229][-4.1931705 -4.2330213 -4.2481856 -4.2499247 -4.2421675 -4.2269812 -4.2140431 -4.2176318 -4.2387357 -4.2530475 -4.2513161 -4.244648 -4.2300038 -4.2122788 -4.2046957][-4.2373471 -4.2473588 -4.2342272 -4.2122593 -4.1884766 -4.1629806 -4.1512046 -4.1693273 -4.2090468 -4.2421732 -4.2545195 -4.2578998 -4.248219 -4.2294741 -4.2181559][-4.2527652 -4.2333426 -4.1886258 -4.136178 -4.0877895 -4.0486369 -4.0338578 -4.0665679 -4.1343365 -4.1978116 -4.2365584 -4.2596955 -4.2635422 -4.2521882 -4.2414217][-4.2486858 -4.2054014 -4.1319366 -4.0454211 -3.9669106 -3.9023268 -3.8721125 -3.9136364 -4.0134869 -4.1149969 -4.1836152 -4.2304606 -4.2568245 -4.2599831 -4.2536912][-4.2479177 -4.1980553 -4.1161165 -4.0143914 -3.9113181 -3.812983 -3.7499127 -3.7848461 -3.9085877 -4.0383892 -4.1279311 -4.19365 -4.2393346 -4.2558079 -4.2527995][-4.2682233 -4.2292113 -4.1637797 -4.0775032 -3.981221 -3.8826842 -3.8051126 -3.8099051 -3.9093432 -4.0291 -4.1203165 -4.1907 -4.2400103 -4.2582736 -4.2531981][-4.2958245 -4.2719483 -4.2294183 -4.1714597 -4.1040788 -4.0337954 -3.9722254 -3.9554074 -4.0094776 -4.0939512 -4.167201 -4.2251096 -4.2641339 -4.2742186 -4.264267][-4.3130465 -4.3015814 -4.2784786 -4.2465768 -4.2081485 -4.1648583 -4.1217513 -4.0987496 -4.1210642 -4.1743884 -4.228189 -4.2699137 -4.2944975 -4.296432 -4.2840862][-4.3153391 -4.3119144 -4.3034306 -4.2910743 -4.2745438 -4.2529664 -4.2266259 -4.20778 -4.2162385 -4.2482238 -4.2831492 -4.3063893 -4.3180361 -4.31371 -4.3018045][-4.3116789 -4.3107338 -4.3093977 -4.3077378 -4.30304 -4.2947869 -4.2822175 -4.2720804 -4.2768841 -4.2956195 -4.3137517 -4.32312 -4.3246737 -4.3170819 -4.3069358][-4.3084345 -4.307879 -4.3084884 -4.3108993 -4.3119321 -4.3107791 -4.3078213 -4.3042531 -4.3077245 -4.3180327 -4.325726 -4.3265896 -4.3224444 -4.3146181 -4.3079438]]...]
INFO - root - 2017-12-05 15:34:57.289608: step 20910, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 74h:26m:00s remains)
INFO - root - 2017-12-05 15:35:05.769609: step 20920, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 74h:50m:33s remains)
INFO - root - 2017-12-05 15:35:14.332022: step 20930, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 73h:26m:51s remains)
INFO - root - 2017-12-05 15:35:22.870132: step 20940, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 72h:38m:35s remains)
INFO - root - 2017-12-05 15:35:31.348095: step 20950, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 72h:56m:28s remains)
INFO - root - 2017-12-05 15:35:39.769503: step 20960, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 73h:40m:26s remains)
INFO - root - 2017-12-05 15:35:48.299990: step 20970, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 77h:57m:57s remains)
INFO - root - 2017-12-05 15:35:56.691550: step 20980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 75h:02m:56s remains)
INFO - root - 2017-12-05 15:36:05.332794: step 20990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 74h:06m:39s remains)
INFO - root - 2017-12-05 15:36:13.955937: step 21000, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 73h:04m:19s remains)
2017-12-05 15:36:14.709481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3032985 -4.280715 -4.2599564 -4.2472119 -4.2326345 -4.2189455 -4.21581 -4.2188215 -4.2220836 -4.2284117 -4.2328811 -4.2369075 -4.233582 -4.2183332 -4.2044749][-4.2989736 -4.2753592 -4.2524366 -4.2309384 -4.2045522 -4.1792173 -4.1682749 -4.1690331 -4.1778774 -4.1986108 -4.2166171 -4.2283635 -4.2313643 -4.220953 -4.2105837][-4.2975388 -4.2744942 -4.2494259 -4.2174888 -4.1771884 -4.1375828 -4.1148925 -4.1087 -4.1200771 -4.1531239 -4.1871457 -4.2104745 -4.2247081 -4.2236571 -4.2201247][-4.3007946 -4.2811847 -4.255969 -4.2162714 -4.1648083 -4.1119137 -4.0760317 -4.0598135 -4.0684333 -4.1116152 -4.1631551 -4.1987052 -4.2209892 -4.2259588 -4.2255716][-4.3070459 -4.292522 -4.2688923 -4.2255011 -4.1667881 -4.1035838 -4.0545611 -4.0251803 -4.026875 -4.0772896 -4.1450167 -4.1932282 -4.2201829 -4.2261195 -4.2245312][-4.3136544 -4.3036537 -4.2807317 -4.2346544 -4.1714811 -4.099525 -4.0381794 -3.9954035 -3.9874794 -4.038415 -4.118907 -4.1792979 -4.2094355 -4.2128711 -4.206429][-4.3192768 -4.311358 -4.2882075 -4.2412105 -4.1777916 -4.1020918 -4.0309734 -3.9793212 -3.967392 -4.0157242 -4.0988803 -4.1643791 -4.1919661 -4.1894116 -4.1770296][-4.3227358 -4.3143868 -4.2904539 -4.2461863 -4.1879668 -4.1148729 -4.0409994 -3.9876227 -3.9801958 -4.0306053 -4.1079569 -4.1665869 -4.1834912 -4.1671438 -4.1449618][-4.322576 -4.3119621 -4.28953 -4.2546716 -4.21071 -4.1501102 -4.0843945 -4.03859 -4.0401731 -4.0863667 -4.1439223 -4.1785445 -4.1751924 -4.1433887 -4.1106062][-4.3199148 -4.30682 -4.2861156 -4.2615671 -4.2325268 -4.1866317 -4.1348548 -4.1042829 -4.1185255 -4.1575575 -4.1889348 -4.1938658 -4.1681972 -4.1220765 -4.08385][-4.3164659 -4.3011336 -4.2799764 -4.2593436 -4.2372146 -4.2018957 -4.160768 -4.1424775 -4.168499 -4.2049518 -4.2225232 -4.2109337 -4.1733909 -4.12242 -4.0866528][-4.3129435 -4.2964482 -4.2729273 -4.2508578 -4.2299061 -4.2001576 -4.1680775 -4.1594863 -4.1921973 -4.2294459 -4.2449145 -4.2282672 -4.1870003 -4.1378312 -4.1078672][-4.3071561 -4.2877307 -4.2600131 -4.2314711 -4.2055197 -4.1774893 -4.1542697 -4.1573572 -4.1984539 -4.237196 -4.2521806 -4.2335529 -4.1948166 -4.15252 -4.1269875][-4.3035111 -4.2802191 -4.24734 -4.2087941 -4.16997 -4.1367807 -4.1183729 -4.1339068 -4.1860857 -4.2271194 -4.242918 -4.2260962 -4.1924343 -4.1562691 -4.1351767][-4.3045115 -4.2802672 -4.2448459 -4.1988525 -4.1468415 -4.1041636 -4.0867915 -4.1120448 -4.1708794 -4.2129021 -4.2310133 -4.220252 -4.1945987 -4.164279 -4.1471763]]...]
INFO - root - 2017-12-05 15:36:23.227924: step 21010, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 72h:23m:54s remains)
INFO - root - 2017-12-05 15:36:31.833101: step 21020, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:05m:56s remains)
INFO - root - 2017-12-05 15:36:40.385906: step 21030, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.900 sec/batch; 77h:54m:18s remains)
INFO - root - 2017-12-05 15:36:48.947461: step 21040, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:43m:32s remains)
INFO - root - 2017-12-05 15:36:57.490563: step 21050, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 76h:08m:15s remains)
INFO - root - 2017-12-05 15:37:05.965684: step 21060, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 72h:53m:58s remains)
INFO - root - 2017-12-05 15:37:14.557129: step 21070, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 75h:14m:34s remains)
INFO - root - 2017-12-05 15:37:22.875312: step 21080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 73h:48m:43s remains)
INFO - root - 2017-12-05 15:37:31.356658: step 21090, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 74h:39m:25s remains)
INFO - root - 2017-12-05 15:37:39.902041: step 21100, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 76h:14m:46s remains)
2017-12-05 15:37:40.581970: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.086133 -4.0525804 -4.0329289 -4.043613 -4.0607886 -4.0811324 -4.0983243 -4.1078458 -4.0946321 -4.0734882 -4.0631847 -4.0679593 -4.0815616 -4.1068006 -4.1535654][-4.0583863 -4.0315619 -4.0257592 -4.0408249 -4.05847 -4.0735469 -4.0855031 -4.0967517 -4.0942521 -4.0867 -4.0821757 -4.0901728 -4.1101394 -4.1385508 -4.1820188][-4.0682087 -4.0449147 -4.0521255 -4.0684609 -4.0846763 -4.0956187 -4.1045704 -4.1186762 -4.1275063 -4.1287751 -4.123713 -4.1264029 -4.1414461 -4.1648989 -4.1992807][-4.0914359 -4.0761733 -4.0830564 -4.0950356 -4.1083374 -4.116004 -4.1172037 -4.1277456 -4.1434693 -4.1511745 -4.147543 -4.1465797 -4.1540389 -4.1704807 -4.1987028][-4.1007719 -4.0946736 -4.1004763 -4.1078992 -4.1157293 -4.1108289 -4.0988607 -4.1035562 -4.1233726 -4.1388116 -4.1409249 -4.138382 -4.1414957 -4.1564569 -4.1851244][-4.0763946 -4.0751762 -4.083714 -4.0880246 -4.0853066 -4.0654774 -4.0390387 -4.0351052 -4.0566206 -4.0801868 -4.0942483 -4.10101 -4.112143 -4.1370487 -4.171309][-4.0157161 -4.0141354 -4.0273433 -4.0303783 -4.0144038 -3.9695845 -3.9160128 -3.8947277 -3.9228466 -3.966136 -4.003129 -4.0320711 -4.064002 -4.1096387 -4.1575589][-3.9367797 -3.9423797 -3.9711788 -3.982053 -3.9569843 -3.8839743 -3.7917581 -3.7421331 -3.778357 -3.8455987 -3.909483 -3.9667871 -4.0234518 -4.0916886 -4.1542988][-3.8927164 -3.9116893 -3.963232 -3.9912004 -3.9730179 -3.9008262 -3.8007905 -3.7412591 -3.7697442 -3.8303823 -3.8921072 -3.9528453 -4.0191689 -4.09945 -4.1674023][-3.9125173 -3.9423437 -4.005106 -4.0424681 -4.0368028 -3.9850209 -3.9096944 -3.8650024 -3.8833506 -3.9192333 -3.9597578 -4.0035076 -4.0590987 -4.1321135 -4.1920471][-3.9436102 -3.9813738 -4.0484586 -4.0895863 -4.0944915 -4.0626807 -4.014915 -3.9886975 -4.0027862 -4.024056 -4.0502005 -4.0802336 -4.1211548 -4.1764488 -4.2216268][-3.9991722 -4.0356512 -4.094707 -4.1319633 -4.1429381 -4.126174 -4.1006489 -4.0891929 -4.0981016 -4.111474 -4.1277027 -4.1470914 -4.1777849 -4.2182 -4.2507491][-4.0789819 -4.1047888 -4.1442084 -4.1710262 -4.1835332 -4.1766262 -4.1637583 -4.1593943 -4.1620283 -4.1678309 -4.1751728 -4.1885886 -4.2141523 -4.2465758 -4.2721105][-4.14359 -4.1604271 -4.1816783 -4.1974597 -4.2050953 -4.2001061 -4.1896586 -4.1847653 -4.1834168 -4.18468 -4.188035 -4.1995916 -4.2238212 -4.2542272 -4.2784824][-4.17092 -4.1815481 -4.191144 -4.1979632 -4.200428 -4.1943307 -4.1841445 -4.1778378 -4.1761127 -4.1757884 -4.1775832 -4.1881104 -4.2112393 -4.2411928 -4.2677264]]...]
INFO - root - 2017-12-05 15:37:49.140942: step 21110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 74h:09m:33s remains)
INFO - root - 2017-12-05 15:37:57.900081: step 21120, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 73h:37m:40s remains)
INFO - root - 2017-12-05 15:38:06.487591: step 21130, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 71h:07m:52s remains)
INFO - root - 2017-12-05 15:38:15.128870: step 21140, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 76h:08m:42s remains)
INFO - root - 2017-12-05 15:38:23.779610: step 21150, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 74h:13m:18s remains)
INFO - root - 2017-12-05 15:38:32.358708: step 21160, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 76h:23m:29s remains)
INFO - root - 2017-12-05 15:38:40.942106: step 21170, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 75h:33m:25s remains)
INFO - root - 2017-12-05 15:38:49.237592: step 21180, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 72h:24m:33s remains)
INFO - root - 2017-12-05 15:38:57.836969: step 21190, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 77h:30m:58s remains)
INFO - root - 2017-12-05 15:39:06.417876: step 21200, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 74h:17m:26s remains)
2017-12-05 15:39:07.120792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1786194 -4.1469069 -4.1203976 -4.1155095 -4.1331229 -4.1574593 -4.1723118 -4.1744022 -4.1743422 -4.1724997 -4.1777058 -4.2044339 -4.2307529 -4.2386289 -4.2400336][-4.1629381 -4.1306953 -4.1071048 -4.1055608 -4.1242504 -4.1576986 -4.1822214 -4.1886125 -4.1916666 -4.1860237 -4.1894455 -4.21464 -4.2372236 -4.2395229 -4.2359838][-4.1554489 -4.1278324 -4.1111407 -4.1132584 -4.1274018 -4.1582246 -4.1811619 -4.1880245 -4.1941133 -4.1917272 -4.1968918 -4.21713 -4.2309556 -4.2269597 -4.2215576][-4.1535883 -4.1391573 -4.1309662 -4.1323395 -4.134481 -4.1502357 -4.1592126 -4.1579 -4.1654634 -4.1736975 -4.18751 -4.203011 -4.2043777 -4.1937342 -4.1882076][-4.1570883 -4.1599417 -4.1601248 -4.1575294 -4.145195 -4.1379757 -4.1203485 -4.1017714 -4.1106339 -4.1402254 -4.174952 -4.1924062 -4.1847143 -4.1667781 -4.153966][-4.1808109 -4.1947365 -4.1957583 -4.1811013 -4.1461658 -4.1046605 -4.0493741 -4.0066481 -4.0186744 -4.0771704 -4.14414 -4.1729827 -4.1638603 -4.1443768 -4.1192822][-4.2193851 -4.2362776 -4.2321234 -4.2052722 -4.1441884 -4.0600271 -3.9643841 -3.9009836 -3.92419 -4.015275 -4.1115584 -4.1502175 -4.1405172 -4.1203194 -4.085691][-4.2547936 -4.2718596 -4.2639527 -4.2292814 -4.1498652 -4.0305676 -3.9035065 -3.8263493 -3.8595042 -3.9673097 -4.077373 -4.1252613 -4.1179576 -4.096293 -4.0568638][-4.2767596 -4.2895455 -4.2817812 -4.2485919 -4.1724668 -4.0539145 -3.932169 -3.864352 -3.8944385 -3.9826467 -4.0773678 -4.1242375 -4.11495 -4.086071 -4.0429306][-4.2869234 -4.295083 -4.28743 -4.2612195 -4.2044735 -4.1157556 -4.0322337 -3.9986379 -4.0232897 -4.0700364 -4.1250772 -4.1590915 -4.1428347 -4.1042361 -4.0536752][-4.2882304 -4.2933154 -4.2842069 -4.2621536 -4.2228303 -4.16162 -4.110991 -4.1030116 -4.1283813 -4.1480083 -4.1710172 -4.190362 -4.1669827 -4.122654 -4.0664644][-4.2819433 -4.285181 -4.2736549 -4.2525673 -4.2205763 -4.1768689 -4.1433921 -4.1468549 -4.1704764 -4.1811504 -4.1884608 -4.1969337 -4.1693821 -4.1196628 -4.0605259][-4.2810287 -4.2865415 -4.2751622 -4.2542882 -4.2258654 -4.1953478 -4.1760755 -4.1880655 -4.2059851 -4.2065544 -4.2032065 -4.1999927 -4.1685424 -4.1147647 -4.0601859][-4.2910728 -4.2992215 -4.2890921 -4.2684226 -4.2440434 -4.2247548 -4.2171645 -4.2349353 -4.246861 -4.2381845 -4.2251015 -4.2097797 -4.1747789 -4.123014 -4.0765605][-4.2940459 -4.3034039 -4.2938113 -4.2757339 -4.2560024 -4.2421317 -4.2404838 -4.26103 -4.2725163 -4.2640653 -4.2456293 -4.2204046 -4.18002 -4.1262932 -4.0798726]]...]
INFO - root - 2017-12-05 15:39:15.771679: step 21210, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 74h:54m:28s remains)
INFO - root - 2017-12-05 15:39:24.507337: step 21220, loss = 2.03, batch loss = 1.97 (8.9 examples/sec; 0.902 sec/batch; 77h:59m:15s remains)
INFO - root - 2017-12-05 15:39:33.125574: step 21230, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 73h:59m:09s remains)
INFO - root - 2017-12-05 15:39:41.741463: step 21240, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 75h:54m:39s remains)
INFO - root - 2017-12-05 15:39:50.257609: step 21250, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 75h:19m:35s remains)
INFO - root - 2017-12-05 15:39:58.785218: step 21260, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 75h:02m:08s remains)
INFO - root - 2017-12-05 15:40:07.446390: step 21270, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 75h:25m:19s remains)
INFO - root - 2017-12-05 15:40:15.843471: step 21280, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 72h:55m:30s remains)
INFO - root - 2017-12-05 15:40:24.268455: step 21290, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 72h:02m:45s remains)
INFO - root - 2017-12-05 15:40:32.871176: step 21300, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 76h:19m:12s remains)
2017-12-05 15:40:33.583069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1581421 -4.1779537 -4.1973538 -4.2064581 -4.1873732 -4.1564689 -4.1223993 -4.11842 -4.158289 -4.2110591 -4.2596755 -4.2943697 -4.3060756 -4.3101969 -4.3205013][-4.1308928 -4.1583805 -4.1807742 -4.1892962 -4.1674728 -4.1336818 -4.0947943 -4.0860329 -4.1341677 -4.2012777 -4.2582669 -4.2914934 -4.3044615 -4.3105655 -4.3210053][-4.1054215 -4.1402187 -4.1631932 -4.1666479 -4.1385083 -4.1001339 -4.0615544 -4.0552878 -4.1119437 -4.1925821 -4.2572546 -4.2921267 -4.3055563 -4.3124304 -4.3217893][-4.0953803 -4.1297851 -4.1467757 -4.1476264 -4.1240048 -4.0911236 -4.054328 -4.0480094 -4.1024179 -4.184135 -4.2548056 -4.29396 -4.3078651 -4.3137484 -4.3221335][-4.1119361 -4.139111 -4.1458745 -4.1422167 -4.1303267 -4.1088123 -4.0775194 -4.0622797 -4.1056962 -4.1824694 -4.2571177 -4.2980804 -4.3114018 -4.315475 -4.3226004][-4.1400385 -4.1467791 -4.1364188 -4.1277962 -4.1269331 -4.1193829 -4.1012931 -4.0867767 -4.122138 -4.19389 -4.264699 -4.3025551 -4.3148375 -4.3170943 -4.3229275][-4.1267624 -4.1110759 -4.0853934 -4.0746431 -4.0885153 -4.1035862 -4.103333 -4.098855 -4.1304922 -4.1979542 -4.2651548 -4.302628 -4.3148956 -4.3170643 -4.32302][-4.0885806 -4.0637832 -4.028501 -4.0227094 -4.0534458 -4.0872154 -4.0959444 -4.0931644 -4.1218328 -4.1874032 -4.2555971 -4.2957296 -4.3099728 -4.3154759 -4.3227835][-4.0567317 -4.0454254 -4.0187445 -4.0266132 -4.06584 -4.1019754 -4.1145177 -4.1091194 -4.1297827 -4.185636 -4.2524662 -4.2932768 -4.3064179 -4.3141508 -4.3226185][-4.0502863 -4.0671086 -4.0593605 -4.0731816 -4.11261 -4.1401043 -4.1507912 -4.1429796 -4.1531787 -4.19431 -4.2559967 -4.2957072 -4.3064651 -4.3130655 -4.3219972][-4.0925627 -4.1271706 -4.1290717 -4.1376276 -4.1660008 -4.1831203 -4.1828513 -4.172904 -4.176095 -4.2057424 -4.260076 -4.2979183 -4.3080573 -4.3127508 -4.3213186][-4.1528792 -4.1808085 -4.18563 -4.1910315 -4.2097597 -4.2176991 -4.2098627 -4.2014151 -4.2038121 -4.2252455 -4.2725792 -4.3064966 -4.3120842 -4.3132744 -4.32111][-4.2007365 -4.2188497 -4.2205973 -4.2196956 -4.2242756 -4.2258286 -4.219336 -4.2169185 -4.2196655 -4.234355 -4.27808 -4.3080525 -4.3132372 -4.3135777 -4.3214426][-4.2273197 -4.2368736 -4.2356677 -4.2294822 -4.2202444 -4.2121859 -4.2110891 -4.2138433 -4.2192931 -4.2298303 -4.2692051 -4.2990646 -4.3078036 -4.3121324 -4.321979][-4.2497387 -4.2575459 -4.2567525 -4.2418394 -4.21723 -4.2000618 -4.1980691 -4.200232 -4.2090192 -4.2247376 -4.2638321 -4.2937407 -4.30349 -4.3104162 -4.3216205]]...]
INFO - root - 2017-12-05 15:40:42.166356: step 21310, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 73h:15m:10s remains)
INFO - root - 2017-12-05 15:40:50.685779: step 21320, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 73h:20m:31s remains)
INFO - root - 2017-12-05 15:40:59.269252: step 21330, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 71h:49m:45s remains)
INFO - root - 2017-12-05 15:41:07.880594: step 21340, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.825 sec/batch; 71h:16m:12s remains)
INFO - root - 2017-12-05 15:41:16.466630: step 21350, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 74h:19m:55s remains)
INFO - root - 2017-12-05 15:41:24.935001: step 21360, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 72h:16m:39s remains)
INFO - root - 2017-12-05 15:41:33.509647: step 21370, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 73h:26m:26s remains)
INFO - root - 2017-12-05 15:41:41.907103: step 21380, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 73h:02m:26s remains)
INFO - root - 2017-12-05 15:41:50.491145: step 21390, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 75h:05m:32s remains)
INFO - root - 2017-12-05 15:41:58.897897: step 21400, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 74h:19m:23s remains)
2017-12-05 15:41:59.659518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3020649 -4.3082871 -4.3130975 -4.3142428 -4.3141727 -4.3139663 -4.3139091 -4.3110576 -4.3154888 -4.319284 -4.3132997 -4.308423 -4.3106689 -4.3126297 -4.3128104][-4.274096 -4.2807522 -4.2887192 -4.2919216 -4.2908039 -4.2854943 -4.2831388 -4.282649 -4.2933965 -4.3021021 -4.2955842 -4.2836537 -4.2794356 -4.2828479 -4.2864637][-4.2481103 -4.2538781 -4.26511 -4.2712555 -4.2693186 -4.2570539 -4.248867 -4.2495909 -4.2664003 -4.2801061 -4.2773209 -4.260777 -4.2508106 -4.2538891 -4.2597928][-4.2268949 -4.2323427 -4.2476425 -4.2579446 -4.2517467 -4.2299318 -4.21263 -4.2104988 -4.2325692 -4.2576776 -4.263731 -4.2480965 -4.2347283 -4.2334671 -4.2379727][-4.2000871 -4.2066851 -4.2250533 -4.2393441 -4.22633 -4.1914353 -4.1620636 -4.1525249 -4.1806951 -4.2235231 -4.2399564 -4.2295547 -4.2166896 -4.2093468 -4.2133327][-4.1695342 -4.1781011 -4.2003484 -4.21637 -4.1927605 -4.1372461 -4.0869269 -4.0653105 -4.1030354 -4.1691952 -4.2021337 -4.2030258 -4.1937051 -4.182878 -4.1880813][-4.1519666 -4.1639371 -4.1873322 -4.20171 -4.1671281 -4.0886812 -4.0053592 -3.9589911 -4.0028915 -4.0948997 -4.1499481 -4.1706953 -4.1766148 -4.1716042 -4.1790228][-4.1448569 -4.1572452 -4.1784158 -4.1919818 -4.1582093 -4.0737691 -3.9692228 -3.8972764 -3.9386709 -4.0435739 -4.1114645 -4.1466732 -4.1695633 -4.1765022 -4.1832738][-4.1459923 -4.1544127 -4.1696415 -4.1831784 -4.1612425 -4.0985584 -4.0107479 -3.9490581 -3.9761367 -4.0537481 -4.1056485 -4.1365929 -4.1643305 -4.1802092 -4.1878538][-4.1407843 -4.143909 -4.1571174 -4.1733818 -4.1676197 -4.1376152 -4.0869441 -4.0491891 -4.0584807 -4.0929003 -4.1154952 -4.1304221 -4.1515956 -4.1735067 -4.1860585][-4.1327219 -4.1311135 -4.1423831 -4.1614451 -4.168119 -4.1629548 -4.1460404 -4.1250248 -4.1173797 -4.1233106 -4.1272316 -4.1287117 -4.1386075 -4.158659 -4.1753983][-4.1391287 -4.1340022 -4.1397443 -4.1535568 -4.1637287 -4.174509 -4.1790481 -4.167737 -4.1515412 -4.1449003 -4.1393461 -4.1332364 -4.1356931 -4.1517391 -4.171041][-4.1506629 -4.1480546 -4.15522 -4.1658692 -4.176805 -4.1944666 -4.2084188 -4.2030258 -4.1883759 -4.1795311 -4.1681304 -4.1538672 -4.1478596 -4.1577907 -4.1777749][-4.1811838 -4.1884241 -4.2026896 -4.2144432 -4.2240286 -4.2383828 -4.2512646 -4.2486658 -4.2396994 -4.2328029 -4.2177982 -4.1974607 -4.1841431 -4.186327 -4.2032723][-4.244432 -4.2567854 -4.2716403 -4.280468 -4.2841387 -4.2909017 -4.2975297 -4.2948184 -4.2892241 -4.2843647 -4.2725487 -4.2562633 -4.2439251 -4.2421083 -4.2533393]]...]
INFO - root - 2017-12-05 15:42:08.089113: step 21410, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.801 sec/batch; 69h:13m:35s remains)
INFO - root - 2017-12-05 15:42:16.678343: step 21420, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 75h:08m:16s remains)
INFO - root - 2017-12-05 15:42:25.227900: step 21430, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:38m:29s remains)
INFO - root - 2017-12-05 15:42:33.765050: step 21440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 73h:01m:18s remains)
INFO - root - 2017-12-05 15:42:42.420337: step 21450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 74h:11m:22s remains)
INFO - root - 2017-12-05 15:42:50.892477: step 21460, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.827 sec/batch; 71h:28m:09s remains)
INFO - root - 2017-12-05 15:42:59.464473: step 21470, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.808 sec/batch; 69h:49m:36s remains)
INFO - root - 2017-12-05 15:43:07.901725: step 21480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 73h:04m:55s remains)
INFO - root - 2017-12-05 15:43:16.368382: step 21490, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 72h:00m:09s remains)
INFO - root - 2017-12-05 15:43:24.961948: step 21500, loss = 2.08, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 77h:03m:36s remains)
2017-12-05 15:43:25.750299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1898232 -4.2025194 -4.2192397 -4.2266703 -4.2202077 -4.2146926 -4.2073584 -4.2049022 -4.2052221 -4.209146 -4.2128491 -4.2155752 -4.2156277 -4.2111654 -4.205348][-4.207315 -4.2181768 -4.230762 -4.2394266 -4.2367606 -4.2322154 -4.2261872 -4.2228761 -4.2183847 -4.2153096 -4.2130065 -4.2122011 -4.2089286 -4.20692 -4.2083778][-4.2204161 -4.2348523 -4.2465768 -4.25281 -4.2504816 -4.2439079 -4.2346807 -4.2278342 -4.2190804 -4.2103262 -4.2037897 -4.2035227 -4.2023592 -4.2034531 -4.2081385][-4.2172914 -4.2331424 -4.2456255 -4.252512 -4.2500052 -4.2378774 -4.2197113 -4.2024193 -4.1853423 -4.1724253 -4.1714692 -4.1803093 -4.1868634 -4.1903229 -4.1930108][-4.2183309 -4.2258148 -4.2324815 -4.2336249 -4.223423 -4.2010589 -4.1714587 -4.1444077 -4.1252127 -4.1168766 -4.1274767 -4.1511111 -4.1681337 -4.1722288 -4.169776][-4.2079487 -4.2044592 -4.199162 -4.1908131 -4.1702394 -4.1374788 -4.1003814 -4.0708861 -4.0603976 -4.067203 -4.0919981 -4.1268287 -4.1517367 -4.1599469 -4.1581831][-4.1676488 -4.1557674 -4.1412721 -4.1258364 -4.0964346 -4.0569015 -4.0167117 -3.9977806 -4.0076532 -4.0333643 -4.0661478 -4.1054597 -4.1332588 -4.146708 -4.1482592][-4.112042 -4.0911674 -4.0706744 -4.0483208 -4.0054011 -3.9569507 -3.9208984 -3.9259319 -3.9680538 -4.01469 -4.0526943 -4.0908 -4.1232128 -4.1415462 -4.1417542][-4.0817857 -4.063972 -4.0410166 -4.0050569 -3.94361 -3.887815 -3.8674345 -3.9031971 -3.9641418 -4.01326 -4.0512376 -4.0881138 -4.1214485 -4.1426959 -4.1434979][-4.0876751 -4.0815182 -4.0579176 -4.0099478 -3.9415896 -3.8990889 -3.9028225 -3.9489224 -4.00397 -4.04533 -4.0794616 -4.1097217 -4.137598 -4.1561527 -4.1551738][-4.1100426 -4.1108713 -4.0839372 -4.0313106 -3.9801373 -3.9609513 -3.9727812 -4.0092163 -4.0516658 -4.08571 -4.111836 -4.1286645 -4.1441298 -4.1543837 -4.1522021][-4.1446381 -4.1534123 -4.1297989 -4.0871058 -4.0540919 -4.044909 -4.0519176 -4.0757113 -4.1103077 -4.1372061 -4.1470628 -4.1447635 -4.1421032 -4.1399431 -4.1360226][-4.1889811 -4.2028079 -4.1855583 -4.1536293 -4.1315832 -4.1273594 -4.1330905 -4.1528368 -4.1832757 -4.2023797 -4.1953039 -4.1734672 -4.1539459 -4.1428 -4.13683][-4.2277412 -4.2407584 -4.228837 -4.2064209 -4.1911392 -4.1912112 -4.1990008 -4.2160115 -4.2394919 -4.24907 -4.2297969 -4.1963367 -4.1715012 -4.1608958 -4.1561022][-4.2472391 -4.2599087 -4.2530208 -4.2384167 -4.226541 -4.2273874 -4.2359819 -4.2517014 -4.2667294 -4.2664266 -4.2400227 -4.2038417 -4.1826606 -4.177845 -4.1772065]]...]
INFO - root - 2017-12-05 15:43:34.259259: step 21510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 73h:45m:08s remains)
INFO - root - 2017-12-05 15:43:42.734548: step 21520, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 0.784 sec/batch; 67h:44m:37s remains)
INFO - root - 2017-12-05 15:43:51.290401: step 21530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 74h:32m:24s remains)
INFO - root - 2017-12-05 15:43:59.870995: step 21540, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 73h:44m:36s remains)
INFO - root - 2017-12-05 15:44:08.461527: step 21550, loss = 2.04, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 77h:31m:05s remains)
INFO - root - 2017-12-05 15:44:16.998189: step 21560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 74h:18m:46s remains)
INFO - root - 2017-12-05 15:44:25.752421: step 21570, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 74h:01m:35s remains)
INFO - root - 2017-12-05 15:44:34.127688: step 21580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 75h:15m:39s remains)
INFO - root - 2017-12-05 15:44:42.727577: step 21590, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 74h:51m:40s remains)
INFO - root - 2017-12-05 15:44:51.183958: step 21600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:36m:27s remains)
2017-12-05 15:44:51.919768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2370114 -4.2449503 -4.2475033 -4.2439137 -4.2306056 -4.2140265 -4.20543 -4.2040849 -4.2099962 -4.2199397 -4.2329922 -4.2482796 -4.2652707 -4.2840009 -4.2998033][-4.2413578 -4.2496958 -4.253098 -4.2511854 -4.2382293 -4.2212267 -4.2108727 -4.20887 -4.2148418 -4.226192 -4.2423677 -4.2576957 -4.2705913 -4.2846384 -4.2979922][-4.2261372 -4.2292295 -4.2293487 -4.2260427 -4.2113175 -4.1917944 -4.1806145 -4.1805592 -4.1907496 -4.2096887 -4.2343626 -4.2544308 -4.2667775 -4.2786512 -4.2912936][-4.191093 -4.1884055 -4.1842194 -4.1776896 -4.1609321 -4.1403546 -4.1327276 -4.1403947 -4.160181 -4.1895533 -4.2232265 -4.2484474 -4.2633886 -4.2755136 -4.2874193][-4.13303 -4.1308289 -4.1265941 -4.1200914 -4.10377 -4.084734 -4.0837688 -4.1022863 -4.1331491 -4.1697545 -4.2078462 -4.2373185 -4.2564526 -4.271625 -4.2847447][-4.078999 -4.0827851 -4.0835528 -4.0801377 -4.0668855 -4.0487523 -4.0499115 -4.0721478 -4.1081338 -4.1478958 -4.1895652 -4.2239995 -4.2487688 -4.269136 -4.2847991][-4.0558572 -4.0615549 -4.0645556 -4.0632725 -4.0504966 -4.0301681 -4.0295677 -4.0514064 -4.0883822 -4.1301804 -4.1753025 -4.2136025 -4.2426939 -4.2675095 -4.2856469][-4.0574646 -4.0620017 -4.0655127 -4.06464 -4.0522103 -4.0308347 -4.0281196 -4.0454559 -4.0778112 -4.117465 -4.1634216 -4.2036233 -4.2359138 -4.2643166 -4.285347][-4.0631285 -4.0649881 -4.06628 -4.0622373 -4.0492444 -4.0286288 -4.0218015 -4.0320668 -4.0578637 -4.095346 -4.1425142 -4.186202 -4.2240257 -4.2572994 -4.28283][-4.0673404 -4.0664649 -4.0645351 -4.0610213 -4.051446 -4.0369506 -4.0308828 -4.0372581 -4.0582924 -4.0900817 -4.1327925 -4.1763287 -4.2167344 -4.2522645 -4.2806358][-4.0913367 -4.0916891 -4.0900497 -4.0882626 -4.081975 -4.0710812 -4.0633221 -4.0636282 -4.0756507 -4.0984488 -4.1334724 -4.1733446 -4.2141757 -4.2508421 -4.2809467][-4.1245236 -4.1255636 -4.1238918 -4.1229224 -4.1197472 -4.1108208 -4.1021924 -4.0999694 -4.10744 -4.1247187 -4.1536365 -4.1882796 -4.2247291 -4.2581296 -4.2858691][-4.1719522 -4.1716909 -4.1674733 -4.1648026 -4.1622005 -4.1551971 -4.148941 -4.1480169 -4.1540976 -4.1678529 -4.189764 -4.2160997 -4.2444391 -4.27128 -4.2934217][-4.2084203 -4.2098408 -4.2067943 -4.2039514 -4.199326 -4.1918283 -4.1855044 -4.1846218 -4.1898746 -4.2015681 -4.2190495 -4.2400951 -4.2630739 -4.284255 -4.3008904][-4.2036228 -4.2067919 -4.2055197 -4.2024183 -4.1957016 -4.18807 -4.1839843 -4.1867113 -4.1973443 -4.2139149 -4.233602 -4.2550545 -4.2762923 -4.2945323 -4.3073916]]...]
INFO - root - 2017-12-05 15:45:00.381066: step 21610, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:38m:25s remains)
INFO - root - 2017-12-05 15:45:08.927615: step 21620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:40m:13s remains)
INFO - root - 2017-12-05 15:45:17.331238: step 21630, loss = 2.05, batch loss = 1.99 (10.3 examples/sec; 0.775 sec/batch; 66h:57m:12s remains)
INFO - root - 2017-12-05 15:45:25.841762: step 21640, loss = 2.10, batch loss = 2.05 (9.8 examples/sec; 0.817 sec/batch; 70h:30m:41s remains)
INFO - root - 2017-12-05 15:45:34.456774: step 21650, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.865 sec/batch; 74h:41m:18s remains)
INFO - root - 2017-12-05 15:45:43.029086: step 21660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 74h:13m:08s remains)
INFO - root - 2017-12-05 15:45:51.622548: step 21670, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 74h:31m:26s remains)
INFO - root - 2017-12-05 15:46:00.086598: step 21680, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 75h:55m:02s remains)
INFO - root - 2017-12-05 15:46:08.547623: step 21690, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 70h:22m:30s remains)
INFO - root - 2017-12-05 15:46:17.123414: step 21700, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 72h:11m:12s remains)
2017-12-05 15:46:17.952733: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1924386 -4.2013755 -4.1900835 -4.1810169 -4.1746917 -4.1596394 -4.1404457 -4.11859 -4.1047678 -4.1114697 -4.1304393 -4.1512175 -4.1709352 -4.1928525 -4.2137952][-4.1635308 -4.1751347 -4.1650157 -4.1548786 -4.144598 -4.125514 -4.1016197 -4.066741 -4.0478477 -4.0579481 -4.0797296 -4.1044388 -4.1295385 -4.1622167 -4.1910768][-4.1481409 -4.1654353 -4.1600542 -4.1506557 -4.1352344 -4.1146574 -4.0857763 -4.0409422 -4.0151539 -4.0333304 -4.0610051 -4.0886106 -4.1165819 -4.1540232 -4.1854215][-4.1456394 -4.1592793 -4.1590548 -4.15342 -4.1332893 -4.112803 -4.0802526 -4.0292711 -4.0015965 -4.0264182 -4.0624852 -4.0930862 -4.1199188 -4.1567664 -4.1864381][-4.1316934 -4.1309834 -4.1351466 -4.132216 -4.1094642 -4.0853243 -4.0448012 -3.9835908 -3.9588103 -4.0034466 -4.059319 -4.0953851 -4.120554 -4.1540422 -4.1816626][-4.1095486 -4.0970645 -4.1031675 -4.1019683 -4.0717406 -4.0273552 -3.9635222 -3.8735151 -3.8442674 -3.9252076 -4.0218325 -4.0791817 -4.1144705 -4.1516309 -4.1803541][-4.1001849 -4.0852332 -4.0872574 -4.0776591 -4.0345778 -3.9602528 -3.8475034 -3.7074978 -3.6726463 -3.8076546 -3.9567471 -4.0522504 -4.109375 -4.1577621 -4.1914787][-4.1203547 -4.111712 -4.1086025 -4.0895591 -4.0416336 -3.9549365 -3.8153129 -3.6438577 -3.6045196 -3.7623506 -3.9328086 -4.0463057 -4.1183062 -4.1755662 -4.2110767][-4.1435637 -4.1430311 -4.1377449 -4.119668 -4.0817995 -4.0137267 -3.9048262 -3.7693725 -3.7304392 -3.8476057 -3.9910226 -4.0883441 -4.1514912 -4.2002983 -4.2305446][-4.171742 -4.17527 -4.17224 -4.1585989 -4.1259212 -4.07999 -4.0137048 -3.9163027 -3.878943 -3.9534063 -4.0609407 -4.1398387 -4.190609 -4.2237067 -4.2426348][-4.2081757 -4.2101583 -4.2074022 -4.1929116 -4.1613455 -4.12576 -4.0756235 -3.9967039 -3.9593503 -4.0081854 -4.0929289 -4.1649919 -4.2133241 -4.2393026 -4.2510543][-4.2366309 -4.2338133 -4.223845 -4.2047858 -4.1749883 -4.1415257 -4.0931211 -4.0239682 -3.993964 -4.0339894 -4.1063819 -4.1739035 -4.2244911 -4.2512231 -4.264379][-4.2480888 -4.2393355 -4.2236328 -4.2037778 -4.17813 -4.1481586 -4.1092968 -4.0594497 -4.041544 -4.0749774 -4.1351724 -4.1911874 -4.235414 -4.2585754 -4.274457][-4.2599511 -4.2481914 -4.2334323 -4.2148824 -4.1892786 -4.1601448 -4.1294 -4.0966978 -4.0940013 -4.12558 -4.1707897 -4.2136722 -4.2470379 -4.2649908 -4.2801957][-4.2470675 -4.2397146 -4.2321563 -4.2172232 -4.1939368 -4.1669569 -4.1423054 -4.117837 -4.119947 -4.1460991 -4.177866 -4.212265 -4.2420416 -4.2593961 -4.2757678]]...]
INFO - root - 2017-12-05 15:46:26.522782: step 21710, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 74h:26m:17s remains)
INFO - root - 2017-12-05 15:46:35.070048: step 21720, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 73h:13m:24s remains)
INFO - root - 2017-12-05 15:46:43.662933: step 21730, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 74h:19m:23s remains)
INFO - root - 2017-12-05 15:46:52.103672: step 21740, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.820 sec/batch; 70h:44m:37s remains)
INFO - root - 2017-12-05 15:47:00.662839: step 21750, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 73h:08m:57s remains)
INFO - root - 2017-12-05 15:47:09.209321: step 21760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 74h:30m:35s remains)
INFO - root - 2017-12-05 15:47:17.710628: step 21770, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 72h:37m:49s remains)
INFO - root - 2017-12-05 15:47:26.221463: step 21780, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 74h:24m:41s remains)
INFO - root - 2017-12-05 15:47:34.855254: step 21790, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 74h:06m:32s remains)
INFO - root - 2017-12-05 15:47:43.263147: step 21800, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 75h:25m:20s remains)
2017-12-05 15:47:44.046543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3356743 -4.3330135 -4.3312459 -4.3308721 -4.3281169 -4.3178678 -4.3006568 -4.2813606 -4.2673807 -4.2696495 -4.2795396 -4.2874889 -4.3015218 -4.3158355 -4.31471][-4.333272 -4.327477 -4.3221436 -4.3180003 -4.3112254 -4.29598 -4.2762518 -4.2607322 -4.2528415 -4.2596855 -4.2679424 -4.2756367 -4.2950544 -4.3159051 -4.3164845][-4.3333068 -4.3242965 -4.3136044 -4.3037629 -4.2913718 -4.2686238 -4.2399373 -4.2229075 -4.2254553 -4.2423148 -4.2512441 -4.2597957 -4.287024 -4.3157592 -4.3186474][-4.3299522 -4.3175683 -4.3024464 -4.285862 -4.266408 -4.2338934 -4.1923976 -4.1707468 -4.1850719 -4.2170177 -4.2325234 -4.2454486 -4.2801352 -4.3158231 -4.3201923][-4.3195996 -4.3018689 -4.2804303 -4.2555428 -4.2284164 -4.1876435 -4.13098 -4.1010041 -4.1269732 -4.177568 -4.2098012 -4.233593 -4.2768717 -4.3170319 -4.3211184][-4.3077693 -4.2862015 -4.2569942 -4.2210016 -4.1841135 -4.1336985 -4.055892 -4.0075412 -4.0415726 -4.1184387 -4.17813 -4.2189641 -4.2713184 -4.3154154 -4.3204432][-4.301321 -4.2797222 -4.2463174 -4.2015219 -4.1521463 -4.087213 -3.9818881 -3.9013054 -3.9372988 -4.0498915 -4.1475072 -4.2084937 -4.2646022 -4.3096209 -4.3180718][-4.30281 -4.2832961 -4.2502909 -4.2046533 -4.1467505 -4.0669403 -3.946866 -3.8462172 -3.8835051 -4.0203738 -4.139564 -4.2086854 -4.262145 -4.303762 -4.3161807][-4.3076496 -4.2918363 -4.2624011 -4.220304 -4.1657724 -4.0915017 -3.9925878 -3.9168677 -3.9478097 -4.0592332 -4.1600866 -4.2183948 -4.2619014 -4.2995539 -4.3143888][-4.3146391 -4.302484 -4.278954 -4.2439322 -4.2015934 -4.1500521 -4.0922146 -4.0493927 -4.064043 -4.127645 -4.1915035 -4.2306533 -4.2643056 -4.2980332 -4.3124614][-4.3227053 -4.3179379 -4.3050408 -4.2824554 -4.2544022 -4.2230954 -4.1937351 -4.1676006 -4.1627851 -4.1891432 -4.2241063 -4.247622 -4.2720022 -4.3004217 -4.3131852][-4.3243885 -4.3282208 -4.3286142 -4.32023 -4.3065095 -4.29002 -4.2746105 -4.2528911 -4.2386885 -4.2454824 -4.2623873 -4.2717762 -4.2854805 -4.3069439 -4.3167849][-4.3182597 -4.3283882 -4.3363938 -4.3368139 -4.3347664 -4.3309927 -4.3269496 -4.3114166 -4.2935562 -4.29021 -4.2959661 -4.2936621 -4.2983932 -4.3126292 -4.3190227][-4.3033652 -4.3146396 -4.3244634 -4.3309288 -4.3374133 -4.3436651 -4.3470144 -4.3362741 -4.3192096 -4.3119082 -4.3129215 -4.3049288 -4.3042545 -4.3136215 -4.3177853][-4.2809443 -4.2906561 -4.3018031 -4.31394 -4.3276191 -4.3409672 -4.3483214 -4.3394737 -4.3217015 -4.313313 -4.3141861 -4.3058543 -4.3023467 -4.30948 -4.3133793]]...]
INFO - root - 2017-12-05 15:47:52.544166: step 21810, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 73h:13m:14s remains)
INFO - root - 2017-12-05 15:48:01.067659: step 21820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 74h:34m:31s remains)
INFO - root - 2017-12-05 15:48:09.628208: step 21830, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 73h:19m:01s remains)
INFO - root - 2017-12-05 15:48:18.164119: step 21840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 73h:46m:44s remains)
INFO - root - 2017-12-05 15:48:26.665493: step 21850, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.752 sec/batch; 64h:54m:20s remains)
INFO - root - 2017-12-05 15:48:35.272531: step 21860, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 72h:55m:55s remains)
INFO - root - 2017-12-05 15:48:43.871266: step 21870, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 72h:15m:07s remains)
INFO - root - 2017-12-05 15:48:52.324234: step 21880, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 73h:26m:55s remains)
INFO - root - 2017-12-05 15:49:00.831660: step 21890, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 71h:33m:21s remains)
INFO - root - 2017-12-05 15:49:09.416062: step 21900, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 76h:47m:58s remains)
2017-12-05 15:49:10.170520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0899982 -4.0641165 -4.0678639 -4.086175 -4.1176968 -4.1555696 -4.1679249 -4.1525564 -4.1438971 -4.1547775 -4.17048 -4.1773739 -4.1746922 -4.162775 -4.1441689][-4.0982761 -4.0633678 -4.0640697 -4.0865192 -4.1197691 -4.1539197 -4.1667728 -4.1621633 -4.1674776 -4.1865 -4.2055659 -4.2126393 -4.2091579 -4.2074041 -4.2022634][-4.1060438 -4.0737996 -4.0698414 -4.0874233 -4.1152353 -4.1419573 -4.1505737 -4.1528835 -4.1660981 -4.1878824 -4.2084651 -4.2167535 -4.2174678 -4.2268453 -4.2370439][-4.0988154 -4.0890059 -4.0904179 -4.1031094 -4.1191025 -4.1357851 -4.1376438 -4.1333828 -4.1405067 -4.1580825 -4.1792579 -4.1895237 -4.2007303 -4.2196865 -4.2406511][-4.0667171 -4.0822105 -4.0948448 -4.1121697 -4.1277509 -4.1384368 -4.13425 -4.1199503 -4.1164579 -4.1301613 -4.1540627 -4.1674194 -4.1855826 -4.2096372 -4.2319841][-4.034061 -4.0601134 -4.0748997 -4.0984573 -4.1266007 -4.1407022 -4.127636 -4.101747 -4.0859976 -4.0932422 -4.1250215 -4.1541405 -4.181118 -4.2040777 -4.2213359][-4.0221252 -4.0326824 -4.029387 -4.0431542 -4.076673 -4.1010022 -4.0891037 -4.06258 -4.0490885 -4.0579481 -4.0993166 -4.1490474 -4.1866751 -4.2050748 -4.2065182][-4.0083919 -3.9963024 -3.9692488 -3.964154 -3.999095 -4.0434446 -4.050684 -4.044024 -4.0438747 -4.0578842 -4.1045895 -4.165638 -4.2061267 -4.2104964 -4.1916628][-3.9912357 -3.9707439 -3.9409888 -3.9358416 -3.9755542 -4.0402684 -4.0724015 -4.0855742 -4.0952897 -4.1143613 -4.1540265 -4.2067447 -4.2360992 -4.224762 -4.1941729][-4.0305157 -4.0172663 -3.9931521 -3.9862595 -4.0173516 -4.0805292 -4.12474 -4.1479158 -4.1599126 -4.1783481 -4.2053041 -4.2406974 -4.2562275 -4.2373309 -4.2053003][-4.0970259 -4.0918837 -4.0708718 -4.057497 -4.073369 -4.1228337 -4.1664953 -4.18844 -4.192687 -4.2060971 -4.2258215 -4.2529035 -4.2627168 -4.2452121 -4.21317][-4.1529684 -4.1505527 -4.1268721 -4.1077862 -4.1133661 -4.1479936 -4.1791525 -4.1890211 -4.1853108 -4.197814 -4.2203183 -4.24667 -4.2549219 -4.2403464 -4.2066145][-4.1772037 -4.1692429 -4.1433783 -4.1220183 -4.122467 -4.1481862 -4.1710682 -4.1692677 -4.1590042 -4.171566 -4.1998858 -4.2252555 -4.23242 -4.2191 -4.1888804][-4.1821804 -4.1708241 -4.1518354 -4.1370335 -4.1362915 -4.1571832 -4.1752667 -4.1662354 -4.1455231 -4.1491022 -4.176949 -4.1983032 -4.2017179 -4.1879315 -4.1622529][-4.1693506 -4.1618352 -4.1569405 -4.1546168 -4.157794 -4.1739326 -4.1894865 -4.182755 -4.1590872 -4.1534805 -4.1769505 -4.1938434 -4.1869054 -4.1644807 -4.1387658]]...]
INFO - root - 2017-12-05 15:49:18.680947: step 21910, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.808 sec/batch; 69h:40m:07s remains)
INFO - root - 2017-12-05 15:49:27.233404: step 21920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 72h:53m:42s remains)
INFO - root - 2017-12-05 15:49:35.638639: step 21930, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 75h:14m:45s remains)
INFO - root - 2017-12-05 15:49:44.081300: step 21940, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.843 sec/batch; 72h:45m:50s remains)
INFO - root - 2017-12-05 15:49:52.681255: step 21950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 75h:19m:57s remains)
INFO - root - 2017-12-05 15:50:01.217202: step 21960, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.798 sec/batch; 68h:49m:58s remains)
INFO - root - 2017-12-05 15:50:09.737187: step 21970, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 73h:56m:01s remains)
INFO - root - 2017-12-05 15:50:18.220301: step 21980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 72h:01m:31s remains)
INFO - root - 2017-12-05 15:50:26.627770: step 21990, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 70h:52m:56s remains)
INFO - root - 2017-12-05 15:50:35.281117: step 22000, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 75h:58m:46s remains)
2017-12-05 15:50:36.171937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009963 -4.2902546 -4.2809348 -4.2702775 -4.2620773 -4.2540855 -4.2499352 -4.2456112 -4.2439466 -4.2533517 -4.2630653 -4.2711663 -4.2771115 -4.2778149 -4.2673035][-4.29121 -4.2749567 -4.259769 -4.2420316 -4.234067 -4.2251916 -4.216753 -4.2041049 -4.1968732 -4.211771 -4.229764 -4.2459741 -4.2588387 -4.2620945 -4.2501841][-4.2845244 -4.2639308 -4.2433057 -4.2201624 -4.2146683 -4.2065535 -4.1900978 -4.1624045 -4.1388583 -4.1538725 -4.1805024 -4.2063384 -4.2351165 -4.2471104 -4.2395043][-4.2817392 -4.259419 -4.2356992 -4.2098517 -4.2049828 -4.1964378 -4.1686482 -4.1183143 -4.0718665 -4.0830245 -4.117702 -4.1542659 -4.2048779 -4.2314239 -4.2269573][-4.2814064 -4.2585669 -4.2316928 -4.2038717 -4.1999955 -4.1875143 -4.1447043 -4.0678492 -4.0020113 -4.0176125 -4.0660777 -4.1168613 -4.1865349 -4.2261696 -4.2265468][-4.278954 -4.2564497 -4.2287912 -4.1997938 -4.1876254 -4.1597881 -4.0904365 -3.9826672 -3.9072237 -3.9481463 -4.02945 -4.1011066 -4.1853476 -4.2331061 -4.239974][-4.2756996 -4.2549953 -4.2318735 -4.2061086 -4.185564 -4.1369748 -4.0387206 -3.9010053 -3.8201261 -3.8867893 -3.9986205 -4.0873704 -4.1725035 -4.2184834 -4.2313366][-4.2750177 -4.2590661 -4.2417345 -4.2189016 -4.1890097 -4.1282816 -4.0223589 -3.8854868 -3.805867 -3.8755548 -3.9922361 -4.0784235 -4.1468043 -4.1848183 -4.2040529][-4.2732482 -4.2600403 -4.2468991 -4.2275915 -4.1937838 -4.1340775 -4.0386925 -3.931787 -3.8708856 -3.9310157 -4.0339832 -4.10148 -4.1428485 -4.1665339 -4.186523][-4.2692704 -4.2595553 -4.2519417 -4.2355909 -4.20147 -4.1453214 -4.0619164 -3.9854937 -3.9452305 -3.9922528 -4.0693655 -4.1175761 -4.1433806 -4.1612611 -4.1803565][-4.2662292 -4.2578421 -4.2523565 -4.2391186 -4.2091937 -4.157774 -4.0891538 -4.0369477 -4.0123644 -4.0465579 -4.0986972 -4.1290936 -4.147408 -4.1635761 -4.1822276][-4.2657132 -4.2557454 -4.2489371 -4.2417531 -4.2251425 -4.1874366 -4.1399307 -4.1120486 -4.1035919 -4.1294818 -4.1592832 -4.173233 -4.1882019 -4.2029014 -4.2170491][-4.2697325 -4.2582231 -4.2507067 -4.2497082 -4.248395 -4.228621 -4.2006559 -4.1881151 -4.1890326 -4.2113538 -4.2322421 -4.2414074 -4.2575111 -4.2703953 -4.276628][-4.281106 -4.27123 -4.2664232 -4.2689915 -4.2752056 -4.2682953 -4.2556076 -4.2528505 -4.2595854 -4.276166 -4.2886462 -4.2935634 -4.3054056 -4.3147378 -4.31481][-4.2991762 -4.2919035 -4.2894354 -4.2914119 -4.2983623 -4.2967758 -4.2902 -4.2898507 -4.2975149 -4.3080091 -4.3137693 -4.3140626 -4.31862 -4.3249569 -4.3248692]]...]
INFO - root - 2017-12-05 15:50:44.652974: step 22010, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 75h:09m:34s remains)
INFO - root - 2017-12-05 15:50:53.155303: step 22020, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 72h:37m:04s remains)
INFO - root - 2017-12-05 15:51:01.712850: step 22030, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.818 sec/batch; 70h:34m:24s remains)
INFO - root - 2017-12-05 15:51:10.134828: step 22040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 74h:11m:06s remains)
INFO - root - 2017-12-05 15:51:18.710625: step 22050, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 72h:39m:22s remains)
INFO - root - 2017-12-05 15:51:27.235035: step 22060, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 75h:04m:17s remains)
INFO - root - 2017-12-05 15:51:35.615287: step 22070, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 71h:05m:18s remains)
INFO - root - 2017-12-05 15:51:43.947517: step 22080, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 72h:09m:07s remains)
INFO - root - 2017-12-05 15:51:52.602571: step 22090, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.864 sec/batch; 74h:30m:55s remains)
INFO - root - 2017-12-05 15:52:01.091993: step 22100, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.879 sec/batch; 75h:44m:54s remains)
2017-12-05 15:52:01.857188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2761316 -4.2662807 -4.2638516 -4.2679892 -4.2718954 -4.2758441 -4.2817645 -4.2824044 -4.2952518 -4.3040023 -4.3069267 -4.3080039 -4.3064642 -4.3057828 -4.3046889][-4.2535496 -4.23519 -4.2240338 -4.2216239 -4.2190986 -4.2185955 -4.2215724 -4.2226567 -4.2438159 -4.2601767 -4.2708631 -4.2798967 -4.2864437 -4.2876592 -4.2850318][-4.2318311 -4.2110896 -4.1941619 -4.1826239 -4.1672506 -4.1545463 -4.1479335 -4.1449547 -4.1752853 -4.1995635 -4.2213192 -4.2382836 -4.2497311 -4.2538013 -4.2493258][-4.2142873 -4.1918178 -4.1688919 -4.1446104 -4.112412 -4.0858431 -4.0770397 -4.0798874 -4.114944 -4.1407251 -4.1608229 -4.1790347 -4.1905165 -4.1945276 -4.1850491][-4.2020006 -4.1748147 -4.142942 -4.1026855 -4.0500522 -4.0127864 -4.0102911 -4.01917 -4.0517087 -4.078156 -4.0930004 -4.1103497 -4.1242318 -4.1278744 -4.1098371][-4.198544 -4.1650305 -4.120244 -4.0664072 -3.9942884 -3.946682 -3.9473143 -3.9592657 -3.9823425 -4.0087147 -4.0163136 -4.0358324 -4.050487 -4.049737 -4.0253487][-4.1910491 -4.1514244 -4.0938425 -4.0331984 -3.9607198 -3.9048724 -3.9019005 -3.9116018 -3.9304686 -3.9660687 -3.9672742 -3.9811985 -3.9822681 -3.9690986 -3.9341822][-4.170558 -4.1237588 -4.0575032 -4.00105 -3.9471142 -3.9054968 -3.9009321 -3.900511 -3.9094095 -3.9517589 -3.9495268 -3.95651 -3.9439204 -3.9162178 -3.8729663][-4.15065 -4.0993123 -4.0337939 -3.9831572 -3.9472885 -3.9279206 -3.9307261 -3.9284515 -3.9340138 -3.9812036 -3.9846416 -3.9890692 -3.9711187 -3.9323196 -3.8846354][-4.1481423 -4.0984259 -4.0396433 -3.9992032 -3.9785676 -3.9780042 -3.9914942 -3.992758 -3.9980471 -4.0439863 -4.0550838 -4.059886 -4.0387139 -3.9939055 -3.9465623][-4.1727014 -4.1337471 -4.0899887 -4.0628843 -4.0529723 -4.0605378 -4.0800104 -4.0820632 -4.0858068 -4.1202588 -4.1297207 -4.1313472 -4.1101794 -4.0713558 -4.0344734][-4.2028251 -4.176055 -4.1499596 -4.1403484 -4.1428785 -4.1541605 -4.1752439 -4.1765752 -4.1783533 -4.1953192 -4.1992364 -4.1997037 -4.1855474 -4.1589441 -4.1351933][-4.2259173 -4.2098207 -4.1990428 -4.2006326 -4.212009 -4.2253208 -4.242775 -4.2411876 -4.238709 -4.2436943 -4.2425165 -4.2420049 -4.2337461 -4.2150807 -4.1989293][-4.2467537 -4.2393146 -4.2380042 -4.2425089 -4.2556691 -4.2682266 -4.2793493 -4.2707605 -4.2660527 -4.2669692 -4.2600522 -4.2542415 -4.2484913 -4.2377439 -4.2295766][-4.2642584 -4.25944 -4.2605848 -4.2633553 -4.2719989 -4.2772141 -4.2799249 -4.267622 -4.2635312 -4.2651353 -4.2579184 -4.252243 -4.2487931 -4.2437606 -4.2405567]]...]
INFO - root - 2017-12-05 15:52:10.436778: step 22110, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 73h:00m:45s remains)
INFO - root - 2017-12-05 15:52:18.960060: step 22120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 74h:17m:55s remains)
INFO - root - 2017-12-05 15:52:27.607905: step 22130, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 73h:59m:36s remains)
INFO - root - 2017-12-05 15:52:36.008933: step 22140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 72h:38m:00s remains)
INFO - root - 2017-12-05 15:52:44.575308: step 22150, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 76h:02m:41s remains)
INFO - root - 2017-12-05 15:52:53.093322: step 22160, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 73h:06m:02s remains)
INFO - root - 2017-12-05 15:53:01.604292: step 22170, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 73h:21m:50s remains)
INFO - root - 2017-12-05 15:53:09.991471: step 22180, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 70h:38m:02s remains)
INFO - root - 2017-12-05 15:53:18.477260: step 22190, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 74h:50m:39s remains)
INFO - root - 2017-12-05 15:53:27.008358: step 22200, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.842 sec/batch; 72h:32m:36s remains)
2017-12-05 15:53:27.756806: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.126008 -4.1194367 -4.0878863 -4.0688396 -4.0778937 -4.1195669 -4.1839457 -4.2406821 -4.28376 -4.3103724 -4.3181214 -4.3022537 -4.2806892 -4.2704377 -4.2738481][-4.0832157 -4.0927982 -4.0720158 -4.0582123 -4.0742846 -4.1193709 -4.1838312 -4.23593 -4.2762146 -4.3090596 -4.3242316 -4.3114734 -4.29116 -4.2818565 -4.2866707][-4.0450826 -4.0709314 -4.06441 -4.0561848 -4.0782018 -4.1281791 -4.1902785 -4.232758 -4.2655249 -4.3017359 -4.3245287 -4.3183236 -4.302866 -4.293117 -4.2940884][-4.0393462 -4.0718684 -4.07746 -4.0736217 -4.095365 -4.1428246 -4.1931453 -4.2212973 -4.2447171 -4.280858 -4.3095174 -4.3104329 -4.3005624 -4.2895808 -4.2838016][-4.07274 -4.1017051 -4.1090727 -4.1070137 -4.125494 -4.1593847 -4.1885934 -4.1967058 -4.2073636 -4.241396 -4.2758737 -4.2849588 -4.2804027 -4.2685308 -4.2572083][-4.129981 -4.1506753 -4.1540861 -4.1493096 -4.1594815 -4.1740389 -4.1756878 -4.1594114 -4.1575084 -4.1940689 -4.238358 -4.2588525 -4.2622089 -4.2500749 -4.2335529][-4.1899266 -4.2017455 -4.1983576 -4.186286 -4.18183 -4.1753044 -4.1495681 -4.1076336 -4.0909033 -4.1332555 -4.1907978 -4.224247 -4.2375827 -4.2288685 -4.2102089][-4.2386293 -4.2451539 -4.2375345 -4.2202067 -4.2028713 -4.1754727 -4.1224141 -4.0520868 -4.0137444 -4.059617 -4.1317935 -4.1793733 -4.203414 -4.2023845 -4.1874528][-4.2660246 -4.27424 -4.2679915 -4.25049 -4.2251339 -4.1812634 -4.1059318 -4.0104403 -3.9490135 -3.9918177 -4.0759368 -4.1372242 -4.1732736 -4.1836085 -4.1763225][-4.2636266 -4.2780924 -4.2788267 -4.268683 -4.245914 -4.1999311 -4.1214252 -4.0218983 -3.9513223 -3.9797082 -4.0573921 -4.1224785 -4.1681066 -4.1897326 -4.187839][-4.2468185 -4.2666745 -4.2755957 -4.275785 -4.2644949 -4.2321086 -4.1683083 -4.0842628 -4.0223255 -4.0336571 -4.0885277 -4.1424365 -4.1891994 -4.2174478 -4.2174206][-4.2401209 -4.2594709 -4.2733617 -4.2833982 -4.2847266 -4.2693968 -4.2255549 -4.1606569 -4.1121955 -4.11387 -4.1485929 -4.1876493 -4.227438 -4.2546616 -4.2517085][-4.2605338 -4.2747684 -4.2890825 -4.3032312 -4.3116908 -4.3076777 -4.2802463 -4.2328844 -4.1959333 -4.1916327 -4.2109785 -4.2374868 -4.2693176 -4.2918739 -4.2863903][-4.2945938 -4.3031168 -4.3137584 -4.3251023 -4.3327518 -4.3339667 -4.3205576 -4.2911115 -4.264987 -4.2572737 -4.2651181 -4.28109 -4.3060737 -4.3258967 -4.3201785][-4.3240876 -4.3270283 -4.33274 -4.340035 -4.3455682 -4.3509245 -4.3479104 -4.3330469 -4.3154197 -4.3062267 -4.3069749 -4.31602 -4.3343248 -4.3487349 -4.3426886]]...]
INFO - root - 2017-12-05 15:53:36.351163: step 22210, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 75h:33m:24s remains)
INFO - root - 2017-12-05 15:53:44.892133: step 22220, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 71h:53m:52s remains)
INFO - root - 2017-12-05 15:53:53.513521: step 22230, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 73h:54m:38s remains)
INFO - root - 2017-12-05 15:54:02.035252: step 22240, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 73h:22m:15s remains)
INFO - root - 2017-12-05 15:54:10.471018: step 22250, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 71h:50m:18s remains)
INFO - root - 2017-12-05 15:54:19.001686: step 22260, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 71h:22m:48s remains)
INFO - root - 2017-12-05 15:54:27.561385: step 22270, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 73h:50m:05s remains)
INFO - root - 2017-12-05 15:54:36.063171: step 22280, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 76h:26m:05s remains)
INFO - root - 2017-12-05 15:54:44.652996: step 22290, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.808 sec/batch; 69h:37m:44s remains)
INFO - root - 2017-12-05 15:54:53.252860: step 22300, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 72h:37m:07s remains)
2017-12-05 15:54:54.044971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1797452 -4.2082744 -4.2291389 -4.2259293 -4.2081337 -4.1856823 -4.1606197 -4.1462507 -4.1511865 -4.1666222 -4.19056 -4.2234559 -4.2520971 -4.2681589 -4.2703857][-4.1822066 -4.218955 -4.2484617 -4.2475696 -4.2257524 -4.1962166 -4.1618052 -4.141026 -4.1489615 -4.1756849 -4.2129054 -4.2566295 -4.2885857 -4.2971535 -4.2857423][-4.18619 -4.2254119 -4.2604809 -4.2619605 -4.2354503 -4.1958656 -4.1505156 -4.1246376 -4.1364961 -4.1751337 -4.2262435 -4.2765064 -4.3077574 -4.3096871 -4.2881951][-4.1911421 -4.2284412 -4.265656 -4.2669277 -4.2326617 -4.1785603 -4.120904 -4.0948739 -4.1166148 -4.1692891 -4.2347827 -4.2909579 -4.3199744 -4.3182445 -4.2934227][-4.2027578 -4.2318459 -4.2631474 -4.2582011 -4.2128415 -4.1425357 -4.076715 -4.0593019 -4.0976639 -4.1657166 -4.2438498 -4.3058372 -4.3348737 -4.3327384 -4.3089957][-4.222434 -4.2421312 -4.2623358 -4.2455153 -4.1834006 -4.0934849 -4.0207796 -4.01968 -4.0789886 -4.160841 -4.2472343 -4.3114104 -4.3427186 -4.3450804 -4.325974][-4.2524915 -4.2638388 -4.27273 -4.2421055 -4.1583233 -4.0436392 -3.9649563 -3.9848738 -4.0690417 -4.1633415 -4.2518182 -4.311914 -4.3405986 -4.345335 -4.3329792][-4.2772646 -4.2827959 -4.2845597 -4.2417936 -4.1371474 -4.0005317 -3.9214673 -3.9627194 -4.068368 -4.1712351 -4.2544026 -4.3053837 -4.3279071 -4.332818 -4.3285227][-4.2835822 -4.2851505 -4.2844524 -4.2377853 -4.1274343 -3.9949727 -3.9324639 -3.987721 -4.0937924 -4.1892686 -4.2598414 -4.2993631 -4.3158073 -4.3213654 -4.3223014][-4.2680044 -4.2721324 -4.277554 -4.2391248 -4.1414719 -4.0317955 -3.9897997 -4.0449772 -4.1364722 -4.2141919 -4.26845 -4.2951875 -4.3056278 -4.3117266 -4.317555][-4.2375393 -4.2491932 -4.265337 -4.2414417 -4.1641159 -4.0790172 -4.0511084 -4.0989137 -4.1727958 -4.2316723 -4.2681227 -4.2806063 -4.2845912 -4.2915955 -4.3034835][-4.2101574 -4.2276812 -4.2500196 -4.2373428 -4.1814966 -4.11866 -4.1018071 -4.1423512 -4.2003827 -4.2419968 -4.2598195 -4.2570634 -4.2528133 -4.260222 -4.2778249][-4.191134 -4.211978 -4.2355318 -4.229517 -4.1903934 -4.1425085 -4.1317377 -4.1678128 -4.2160592 -4.242806 -4.2441974 -4.2284012 -4.219409 -4.2299161 -4.2535224][-4.1769233 -4.1994576 -4.2264752 -4.2256923 -4.1954665 -4.1545033 -4.1458325 -4.1766438 -4.2159657 -4.2309704 -4.222167 -4.2017174 -4.1958847 -4.21276 -4.2397485][-4.1783075 -4.2014127 -4.2306476 -4.2314882 -4.2022109 -4.1611218 -4.1494808 -4.169744 -4.1956697 -4.2035971 -4.1944079 -4.1819997 -4.1889396 -4.2114673 -4.2375388]]...]
INFO - root - 2017-12-05 15:55:02.636418: step 22310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 74h:32m:57s remains)
INFO - root - 2017-12-05 15:55:11.204271: step 22320, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 70h:21m:17s remains)
INFO - root - 2017-12-05 15:55:19.778745: step 22330, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 74h:04m:40s remains)
INFO - root - 2017-12-05 15:55:28.367813: step 22340, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 72h:04m:46s remains)
INFO - root - 2017-12-05 15:55:36.912855: step 22350, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 72h:53m:53s remains)
INFO - root - 2017-12-05 15:55:45.431117: step 22360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 75h:17m:46s remains)
INFO - root - 2017-12-05 15:55:54.051678: step 22370, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 74h:22m:42s remains)
INFO - root - 2017-12-05 15:56:02.491763: step 22380, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 72h:45m:48s remains)
INFO - root - 2017-12-05 15:56:10.943058: step 22390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 73h:43m:31s remains)
INFO - root - 2017-12-05 15:56:19.358609: step 22400, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.763 sec/batch; 65h:41m:38s remains)
2017-12-05 15:56:20.128890: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3121667 -4.3129935 -4.3132672 -4.3057718 -4.2937603 -4.2857037 -4.2789083 -4.2742009 -4.2813058 -4.2908254 -4.2924218 -4.2908249 -4.29209 -4.2937961 -4.2988229][-4.3028398 -4.30071 -4.2977681 -4.2863941 -4.2722917 -4.2653718 -4.2609019 -4.2589264 -4.2694921 -4.2811737 -4.2797103 -4.274981 -4.2749667 -4.2754164 -4.2794547][-4.2945957 -4.2895646 -4.2816725 -4.2648296 -4.2493796 -4.2421718 -4.2388468 -4.2390518 -4.2534356 -4.268383 -4.2631612 -4.2542424 -4.2518387 -4.2480807 -4.2460179][-4.2873683 -4.2780962 -4.2621737 -4.2382751 -4.2211232 -4.2129254 -4.2072339 -4.2082815 -4.2273755 -4.2436433 -4.23305 -4.22022 -4.2200294 -4.2108245 -4.1992984][-4.2864437 -4.2730746 -4.2498245 -4.2217073 -4.2006273 -4.1841755 -4.16755 -4.1642489 -4.1824379 -4.1972175 -4.1792641 -4.1603727 -4.1604886 -4.1489863 -4.1337948][-4.2906189 -4.2736893 -4.2463651 -4.2145181 -4.185822 -4.1534405 -4.1157665 -4.1005526 -4.1206512 -4.1409655 -4.1212482 -4.0969486 -4.0969148 -4.0878015 -4.0808754][-4.2911658 -4.2719474 -4.2416019 -4.2006884 -4.1565795 -4.1039653 -4.0471544 -4.023047 -4.0516434 -4.0895967 -4.0802169 -4.0571012 -4.0572557 -4.0537748 -4.0604658][-4.2853794 -4.2639985 -4.2294955 -4.1793652 -4.1192966 -4.0468836 -3.9793382 -3.9571555 -3.9969838 -4.0532017 -4.0604548 -4.0398617 -4.0376987 -4.0426908 -4.0615153][-4.2737908 -4.2524986 -4.2190895 -4.1710095 -4.1133962 -4.0425386 -3.9753776 -3.9490979 -3.9813535 -4.0365348 -4.0513897 -4.0353088 -4.0340509 -4.046607 -4.0737686][-4.2555866 -4.2345243 -4.2092814 -4.1733322 -4.1323471 -4.0777435 -4.0185256 -3.9845285 -3.9974923 -4.0357718 -4.0490003 -4.0407281 -4.0485048 -4.0688624 -4.0963736][-4.2363563 -4.2135935 -4.1970186 -4.1765194 -4.1553869 -4.122263 -4.0788803 -4.044065 -4.0404735 -4.0624752 -4.0699267 -4.0658 -4.0761533 -4.0925083 -4.1109176][-4.226532 -4.1978612 -4.183548 -4.1730165 -4.1662912 -4.1530108 -4.1281543 -4.1013618 -4.0920358 -4.1037612 -4.1014609 -4.0920353 -4.095417 -4.107101 -4.1188483][-4.2278991 -4.192379 -4.1755219 -4.1688704 -4.1693425 -4.1688209 -4.1599441 -4.1452432 -4.1393905 -4.1472521 -4.1391039 -4.1233306 -4.1192484 -4.1249475 -4.1318316][-4.2391386 -4.201973 -4.184515 -4.1804638 -4.1840467 -4.1894836 -4.189364 -4.183495 -4.1821084 -4.1872473 -4.1768069 -4.1576786 -4.1499333 -4.1553111 -4.1638193][-4.2578106 -4.2244158 -4.2091417 -4.2081151 -4.2135305 -4.2193713 -4.2206106 -4.2179103 -4.2180786 -4.2214742 -4.2118082 -4.1933565 -4.18582 -4.1945176 -4.2092519]]...]
INFO - root - 2017-12-05 15:56:28.580034: step 22410, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 73h:45m:39s remains)
INFO - root - 2017-12-05 15:56:37.127784: step 22420, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 72h:57m:39s remains)
INFO - root - 2017-12-05 15:56:45.693563: step 22430, loss = 2.02, batch loss = 1.96 (9.6 examples/sec; 0.835 sec/batch; 71h:52m:52s remains)
INFO - root - 2017-12-05 15:56:54.149346: step 22440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 72h:49m:34s remains)
INFO - root - 2017-12-05 15:57:02.609328: step 22450, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 71h:40m:56s remains)
INFO - root - 2017-12-05 15:57:11.110659: step 22460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 74h:05m:52s remains)
INFO - root - 2017-12-05 15:57:19.713004: step 22470, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 74h:30m:19s remains)
INFO - root - 2017-12-05 15:57:28.194904: step 22480, loss = 2.12, batch loss = 2.06 (9.3 examples/sec; 0.857 sec/batch; 73h:46m:59s remains)
INFO - root - 2017-12-05 15:57:36.716777: step 22490, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 73h:39m:32s remains)
INFO - root - 2017-12-05 15:57:45.199435: step 22500, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 74h:02m:31s remains)
2017-12-05 15:57:46.076552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275034 -4.274384 -4.2734027 -4.2726555 -4.2726064 -4.2732334 -4.273798 -4.2745595 -4.2762485 -4.2775583 -4.276793 -4.2722154 -4.2664313 -4.2591562 -4.2496486][-4.2842135 -4.2829404 -4.2806177 -4.2777677 -4.2755971 -4.2748728 -4.2760816 -4.2785883 -4.2811279 -4.2807722 -4.2763095 -4.2690458 -4.26325 -4.2605705 -4.2577844][-4.2904496 -4.2882562 -4.2841024 -4.2779942 -4.2717824 -4.2670445 -4.2666459 -4.2695665 -4.2714682 -4.2674041 -4.260252 -4.2549424 -4.2535305 -4.2560682 -4.25964][-4.2939544 -4.2909427 -4.2852082 -4.2755547 -4.2641163 -4.2535777 -4.2487831 -4.2482905 -4.2470551 -4.24016 -4.2353864 -4.2378764 -4.2448449 -4.2522678 -4.2604685][-4.2975807 -4.2941074 -4.2871137 -4.2743154 -4.2574916 -4.2402506 -4.2287712 -4.2223306 -4.2159853 -4.2075005 -4.2063494 -4.2174382 -4.2326231 -4.2446532 -4.2548347][-4.3025308 -4.2987609 -4.2908835 -4.2768307 -4.2568922 -4.2349763 -4.2176175 -4.2061691 -4.1954818 -4.1845684 -4.1837411 -4.1979241 -4.2173777 -4.2324409 -4.2421312][-4.3030467 -4.2992916 -4.2916732 -4.2796316 -4.2609138 -4.2399225 -4.2209821 -4.2061234 -4.192081 -4.17575 -4.16833 -4.17784 -4.197607 -4.2157078 -4.2252989][-4.30073 -4.2978263 -4.2916551 -4.2823009 -4.2652521 -4.2464886 -4.2280855 -4.2122045 -4.1939349 -4.1705513 -4.1543303 -4.1576643 -4.178 -4.1990881 -4.2087693][-4.2957678 -4.2940488 -4.2892032 -4.2799258 -4.2615128 -4.2420845 -4.223259 -4.2050209 -4.1821251 -4.15476 -4.1338453 -4.1360168 -4.1614151 -4.1868253 -4.196454][-4.2875214 -4.2867837 -4.281939 -4.269917 -4.2474189 -4.2273364 -4.2107224 -4.1912236 -4.1668754 -4.1405568 -4.1187091 -4.1209278 -4.1516619 -4.1796646 -4.1899652][-4.2776527 -4.2788596 -4.2740469 -4.2585988 -4.2325182 -4.2147088 -4.2040854 -4.18737 -4.1633959 -4.1379042 -4.1155128 -4.1156712 -4.1477432 -4.1756134 -4.1875463][-4.2668457 -4.2721429 -4.2680497 -4.2490597 -4.2214561 -4.2044697 -4.1991639 -4.188889 -4.1666675 -4.1408911 -4.11542 -4.1116614 -4.1411834 -4.1685095 -4.183763][-4.2590942 -4.266809 -4.2628222 -4.2421017 -4.2155571 -4.1981292 -4.1941886 -4.1916814 -4.1775284 -4.15682 -4.1322188 -4.1226811 -4.143219 -4.1658487 -4.1813231][-4.2450147 -4.2513371 -4.247436 -4.2300611 -4.2084136 -4.1953831 -4.1967406 -4.2049165 -4.2042232 -4.190917 -4.1675129 -4.14879 -4.1555347 -4.1695867 -4.1805668][-4.2290812 -4.2310939 -4.2281175 -4.2190719 -4.2094 -4.2061067 -4.2122107 -4.2250643 -4.2319779 -4.2244868 -4.2020121 -4.178268 -4.1728125 -4.1760635 -4.1814251]]...]
INFO - root - 2017-12-05 15:57:54.499639: step 22510, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 74h:25m:44s remains)
INFO - root - 2017-12-05 15:58:03.094452: step 22520, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 74h:09m:51s remains)
INFO - root - 2017-12-05 15:58:11.763062: step 22530, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:24m:08s remains)
INFO - root - 2017-12-05 15:58:20.268298: step 22540, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 72h:59m:40s remains)
INFO - root - 2017-12-05 15:58:28.861294: step 22550, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 72h:55m:29s remains)
INFO - root - 2017-12-05 15:58:37.474577: step 22560, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:31m:32s remains)
INFO - root - 2017-12-05 15:58:45.898003: step 22570, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.880 sec/batch; 75h:43m:25s remains)
INFO - root - 2017-12-05 15:58:54.440421: step 22580, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 74h:55m:33s remains)
INFO - root - 2017-12-05 15:59:03.032555: step 22590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 74h:13m:08s remains)
INFO - root - 2017-12-05 15:59:11.614940: step 22600, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 75h:15m:40s remains)
2017-12-05 15:59:12.348341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2705026 -4.2600265 -4.2517762 -4.2476497 -4.2491951 -4.258585 -4.2667089 -4.275063 -4.2880526 -4.294733 -4.2977009 -4.3035932 -4.3083067 -4.3078294 -4.3061042][-4.2506456 -4.230412 -4.2115641 -4.1971269 -4.1923103 -4.2006426 -4.2050247 -4.21527 -4.2369051 -4.2478471 -4.2522006 -4.2616639 -4.2706165 -4.2723002 -4.271574][-4.2283964 -4.197401 -4.16595 -4.141644 -4.1305704 -4.1350183 -4.1336536 -4.145113 -4.1788783 -4.20108 -4.20504 -4.2135248 -4.2239308 -4.2291327 -4.2365146][-4.2066045 -4.1630564 -4.115787 -4.0836282 -4.0687871 -4.0621452 -4.0451217 -4.0423665 -4.08961 -4.1316991 -4.1420774 -4.1527128 -4.1644349 -4.1802807 -4.2004786][-4.1890697 -4.1284838 -4.0627227 -4.0201306 -3.9929938 -3.9695292 -3.929204 -3.8849847 -3.9436526 -4.0250745 -4.0465751 -4.0687943 -4.0930867 -4.1264296 -4.1643419][-4.1697116 -4.0851011 -3.9890323 -3.9279037 -3.8799934 -3.8303266 -3.7410927 -3.6228852 -3.7080355 -3.8535066 -3.8984118 -3.9377654 -3.992439 -4.0570583 -4.1113396][-4.1579065 -4.05975 -3.9398396 -3.8562934 -3.7934771 -3.7254379 -3.5975082 -3.4165375 -3.5405202 -3.7461975 -3.8187118 -3.8712678 -3.9509618 -4.0260053 -4.0815525][-4.16816 -4.0810666 -3.9734423 -3.9034793 -3.8575635 -3.8074784 -3.71349 -3.5921712 -3.6954761 -3.8475287 -3.8985572 -3.9452064 -4.0153751 -4.0662479 -4.1048555][-4.1907587 -4.1241636 -4.0452356 -3.9994342 -3.971046 -3.9376955 -3.8840058 -3.8281293 -3.8948743 -3.9732416 -3.9925244 -4.0229392 -4.0733328 -4.1058049 -4.1401649][-4.2110729 -4.1601548 -4.1005745 -4.0627089 -4.0300231 -3.9954081 -3.9629133 -3.9435852 -3.9902461 -4.0271611 -4.0328197 -4.0552006 -4.0894804 -4.1182175 -4.1553483][-4.2344923 -4.1960511 -4.1495047 -4.1144667 -4.0791879 -4.04453 -4.0245624 -4.0283928 -4.0654182 -4.0855403 -4.0873327 -4.1020284 -4.1276283 -4.1551 -4.189167][-4.2642322 -4.237361 -4.2073355 -4.1844063 -4.1615672 -4.13576 -4.1229224 -4.1366911 -4.1643028 -4.1725492 -4.1701374 -4.1834598 -4.20555 -4.2277222 -4.2517295][-4.2962933 -4.281055 -4.2665358 -4.2547388 -4.2433076 -4.2289562 -4.224555 -4.2335033 -4.2475433 -4.2512093 -4.2461209 -4.2546678 -4.2692928 -4.284399 -4.2984176][-4.3116369 -4.3038254 -4.2980528 -4.2920561 -4.2849431 -4.2778783 -4.278182 -4.2838097 -4.2901678 -4.2917442 -4.2872581 -4.2913623 -4.3003564 -4.3101659 -4.3169432][-4.317379 -4.3130879 -4.3102522 -4.3074431 -4.3032532 -4.2997003 -4.3005276 -4.3036609 -4.3066545 -4.3069453 -4.305861 -4.308311 -4.3116021 -4.3154988 -4.3182058]]...]
INFO - root - 2017-12-05 15:59:20.887631: step 22610, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 71h:34m:23s remains)
INFO - root - 2017-12-05 15:59:29.268964: step 22620, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.829 sec/batch; 71h:23m:47s remains)
INFO - root - 2017-12-05 15:59:37.827848: step 22630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 74h:07m:59s remains)
INFO - root - 2017-12-05 15:59:46.322211: step 22640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 73h:38m:25s remains)
INFO - root - 2017-12-05 15:59:54.836601: step 22650, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 73h:10m:18s remains)
INFO - root - 2017-12-05 16:00:03.397475: step 22660, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 73h:07m:11s remains)
INFO - root - 2017-12-05 16:00:11.951518: step 22670, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 74h:44m:58s remains)
INFO - root - 2017-12-05 16:00:20.304014: step 22680, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 73h:30m:07s remains)
INFO - root - 2017-12-05 16:00:28.998045: step 22690, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 73h:07m:28s remains)
INFO - root - 2017-12-05 16:00:37.522219: step 22700, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 74h:52m:35s remains)
2017-12-05 16:00:38.276020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3333445 -4.3332682 -4.3298283 -4.3244753 -4.31987 -4.3178353 -4.3221135 -4.3294706 -4.3342471 -4.3344803 -4.3316264 -4.326354 -4.3227539 -4.3232651 -4.3260202][-4.3350739 -4.3325219 -4.3257489 -4.3170495 -4.3095493 -4.3062243 -4.3149195 -4.3288579 -4.3361392 -4.33475 -4.3309822 -4.3239346 -4.3187909 -4.3202724 -4.3251581][-4.3230162 -4.3156271 -4.3059182 -4.2949042 -4.2849326 -4.2815647 -4.29554 -4.3172822 -4.326611 -4.3246064 -4.3240271 -4.3204703 -4.3160148 -4.3193359 -4.3269792][-4.3002582 -4.2865129 -4.2729487 -4.2577882 -4.2442684 -4.2392697 -4.2552247 -4.2835588 -4.2945013 -4.2931309 -4.3004551 -4.3066545 -4.3068113 -4.3148317 -4.3263888][-4.2712154 -4.2507663 -4.2343655 -4.2172456 -4.2023225 -4.1919203 -4.2040992 -4.2307291 -4.238379 -4.2386389 -4.2573938 -4.2769618 -4.2872863 -4.3040748 -4.3214][-4.2340732 -4.2049508 -4.1871252 -4.170629 -4.1532416 -4.1331644 -4.1374836 -4.1574035 -4.161068 -4.1647849 -4.1970439 -4.23377 -4.2598772 -4.28995 -4.3156819][-4.1967726 -4.1615014 -4.1411481 -4.1239848 -4.102365 -4.0760922 -4.0724273 -4.0832844 -4.080039 -4.0872726 -4.132432 -4.1869264 -4.2325482 -4.2771306 -4.312417][-4.1820378 -4.1456542 -4.1247845 -4.1081443 -4.0863214 -4.0558352 -4.0385981 -4.0325065 -4.0208678 -4.0316715 -4.0875416 -4.156074 -4.2163682 -4.2697949 -4.310564][-4.1953583 -4.1670756 -4.1501141 -4.1359229 -4.1149478 -4.0825081 -4.0528851 -4.0348692 -4.0188446 -4.0313511 -4.0883961 -4.1553426 -4.2129221 -4.2649326 -4.3065524][-4.2188439 -4.2013936 -4.1856275 -4.1696105 -4.1522636 -4.1276441 -4.0999913 -4.0830569 -4.0673027 -4.0744882 -4.1210952 -4.1765318 -4.2247415 -4.2698307 -4.3067393][-4.24779 -4.2369976 -4.2215323 -4.2037611 -4.1896696 -4.1730852 -4.1537728 -4.142818 -4.1281509 -4.1271124 -4.1615109 -4.2069712 -4.2469463 -4.2835441 -4.3118143][-4.2691722 -4.2585659 -4.241858 -4.2244606 -4.2153692 -4.2084012 -4.2012234 -4.1966095 -4.1844797 -4.179934 -4.2039261 -4.2405729 -4.272747 -4.3000283 -4.3201461][-4.2844706 -4.2735825 -4.2573586 -4.2435169 -4.2377644 -4.2373929 -4.2401152 -4.2430382 -4.2362013 -4.2319584 -4.2480893 -4.2753167 -4.298604 -4.3164511 -4.3286252][-4.293963 -4.2826653 -4.2694926 -4.2603188 -4.2582521 -4.2632394 -4.2734385 -4.282073 -4.2818193 -4.2794065 -4.2877631 -4.3032622 -4.3168964 -4.3264246 -4.332706][-4.3082738 -4.29904 -4.2898264 -4.2838631 -4.2832527 -4.2880721 -4.29816 -4.3074665 -4.3113956 -4.3113379 -4.3145943 -4.3209248 -4.3269582 -4.3313913 -4.3342142]]...]
INFO - root - 2017-12-05 16:00:46.865896: step 22710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 74h:43m:24s remains)
INFO - root - 2017-12-05 16:00:55.326863: step 22720, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 73h:12m:05s remains)
INFO - root - 2017-12-05 16:01:03.866800: step 22730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 72h:52m:03s remains)
INFO - root - 2017-12-05 16:01:12.375424: step 22740, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 72h:59m:14s remains)
INFO - root - 2017-12-05 16:01:20.875051: step 22750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 72h:18m:43s remains)
INFO - root - 2017-12-05 16:01:29.551691: step 22760, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.852 sec/batch; 73h:18m:16s remains)
INFO - root - 2017-12-05 16:01:38.150425: step 22770, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 73h:39m:25s remains)
INFO - root - 2017-12-05 16:01:46.514950: step 22780, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 71h:49m:40s remains)
INFO - root - 2017-12-05 16:01:55.239281: step 22790, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 74h:22m:00s remains)
INFO - root - 2017-12-05 16:02:03.758669: step 22800, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 72h:13m:13s remains)
2017-12-05 16:02:04.562009: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.199801 -4.1953058 -4.1864052 -4.1819386 -4.1878457 -4.1964235 -4.2004957 -4.2120275 -4.22259 -4.2294106 -4.2381668 -4.2500858 -4.2574763 -4.2597208 -4.2602768][-4.1960907 -4.1970258 -4.1819625 -4.1699729 -4.1820679 -4.1980557 -4.2025781 -4.2108479 -4.2246485 -4.2354784 -4.2451038 -4.2547741 -4.2581654 -4.2556796 -4.2529068][-4.197382 -4.1990347 -4.1747913 -4.1490417 -4.1625018 -4.1849489 -4.1934881 -4.2007031 -4.2217822 -4.2404809 -4.2518959 -4.2590389 -4.2592778 -4.2515388 -4.2445374][-4.1834865 -4.1869545 -4.1627212 -4.1270056 -4.1319489 -4.1487617 -4.1556911 -4.1683702 -4.2030544 -4.2360682 -4.2553706 -4.2633333 -4.2614584 -4.2495131 -4.2388482][-4.1597128 -4.1636872 -4.143435 -4.1016254 -4.0892062 -4.0881991 -4.0849552 -4.1015196 -4.1499324 -4.2044454 -4.245265 -4.2630844 -4.2668352 -4.2551513 -4.2402344][-4.1367803 -4.1370654 -4.1186252 -4.0781074 -4.05694 -4.0406609 -4.0221372 -4.0337873 -4.0870681 -4.1618185 -4.2257376 -4.2579966 -4.270143 -4.2647815 -4.248353][-4.1323137 -4.1324883 -4.1201396 -4.0896873 -4.0668783 -4.0363283 -3.9942183 -3.9885685 -4.0370426 -4.12385 -4.2048063 -4.2502408 -4.2709575 -4.2729783 -4.257935][-4.1420546 -4.1474094 -4.1468697 -4.1339116 -4.1140914 -4.0760818 -4.0156484 -3.986908 -4.0138574 -4.0918207 -4.1828766 -4.2412748 -4.2697883 -4.2790937 -4.2685986][-4.1681337 -4.1775193 -4.18328 -4.1835055 -4.1699882 -4.1359949 -4.0791206 -4.0355673 -4.0357051 -4.0913396 -4.1775227 -4.2382889 -4.2678704 -4.2828112 -4.2808738][-4.2065454 -4.2148075 -4.2216091 -4.2268014 -4.2165461 -4.1959939 -4.1596522 -4.1170068 -4.09943 -4.1277728 -4.1939893 -4.2491093 -4.2747498 -4.2892861 -4.2929735][-4.2474418 -4.2516408 -4.2541943 -4.2561908 -4.247508 -4.2372985 -4.2216349 -4.1915741 -4.1724072 -4.1797557 -4.219265 -4.2639585 -4.286911 -4.2998881 -4.3058968][-4.2782497 -4.2789016 -4.2752271 -4.2701936 -4.2592807 -4.2521319 -4.2502542 -4.2347116 -4.2210865 -4.2168446 -4.2360086 -4.2681332 -4.2901769 -4.3017631 -4.3087487][-4.2763281 -4.27547 -4.26735 -4.259789 -4.2495551 -4.2435708 -4.2489786 -4.2453775 -4.2409859 -4.2337441 -4.2393475 -4.2610159 -4.281126 -4.2912869 -4.2989135][-4.2588224 -4.2584572 -4.2510438 -4.2473059 -4.2387762 -4.2343903 -4.2434716 -4.2484322 -4.2494435 -4.2428403 -4.2415328 -4.25414 -4.2705784 -4.2805114 -4.2881918][-4.2468972 -4.2476916 -4.2436266 -4.2433305 -4.2379909 -4.2342348 -4.2412653 -4.2495494 -4.2545304 -4.2542386 -4.251555 -4.2574468 -4.268487 -4.2773657 -4.2860742]]...]
INFO - root - 2017-12-05 16:02:13.013553: step 22810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 73h:00m:38s remains)
INFO - root - 2017-12-05 16:02:21.650836: step 22820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:21m:53s remains)
INFO - root - 2017-12-05 16:02:30.134107: step 22830, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.824 sec/batch; 70h:54m:08s remains)
INFO - root - 2017-12-05 16:02:38.560657: step 22840, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 71h:48m:02s remains)
INFO - root - 2017-12-05 16:02:47.241583: step 22850, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 74h:57m:30s remains)
INFO - root - 2017-12-05 16:02:55.786495: step 22860, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.858 sec/batch; 73h:45m:25s remains)
INFO - root - 2017-12-05 16:03:04.481955: step 22870, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.883 sec/batch; 75h:58m:39s remains)
INFO - root - 2017-12-05 16:03:13.008754: step 22880, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 75h:44m:23s remains)
INFO - root - 2017-12-05 16:03:21.532295: step 22890, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.866 sec/batch; 74h:26m:12s remains)
INFO - root - 2017-12-05 16:03:30.127664: step 22900, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.849 sec/batch; 73h:01m:53s remains)
2017-12-05 16:03:30.947218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1513419 -4.1867685 -4.1684184 -4.1281533 -4.1110697 -4.1384516 -4.1566238 -4.1416059 -4.1183224 -4.1245432 -4.1644773 -4.1963267 -4.1977673 -4.1794543 -4.1779246][-4.152009 -4.1887636 -4.1769485 -4.141397 -4.1250477 -4.1499252 -4.16689 -4.1580276 -4.1416831 -4.1489043 -4.1746249 -4.1922083 -4.1870766 -4.1689892 -4.1659756][-4.1549044 -4.1918378 -4.1876745 -4.1578989 -4.1378222 -4.1523809 -4.167757 -4.1727695 -4.1750355 -4.1829448 -4.1904855 -4.1975231 -4.1896191 -4.1735229 -4.1624069][-4.1478457 -4.1819367 -4.182878 -4.159543 -4.1338153 -4.128952 -4.1320815 -4.1454821 -4.1740317 -4.1964231 -4.1991758 -4.2056518 -4.2040648 -4.1900892 -4.1710286][-4.1368227 -4.1640658 -4.1571636 -4.1259127 -4.0832486 -4.0522351 -4.0303497 -4.038692 -4.1010327 -4.1590662 -4.1823716 -4.1949883 -4.2001753 -4.1888714 -4.16362][-4.1397123 -4.1453075 -4.1162887 -4.0631475 -3.9851613 -3.9103522 -3.8369629 -3.8225088 -3.9287536 -4.0472879 -4.1167088 -4.1549497 -4.1771078 -4.1749663 -4.1548562][-4.1517034 -4.1365757 -4.0854797 -4.0154281 -3.9117768 -3.7876816 -3.64465 -3.5917151 -3.7294216 -3.9060674 -4.0309839 -4.1045203 -4.14874 -4.1607456 -4.15548][-4.1807289 -4.1588883 -4.1070004 -4.0443549 -3.9505081 -3.8270042 -3.6736832 -3.5946789 -3.7037168 -3.871212 -4.0056205 -4.08802 -4.14242 -4.1667671 -4.177465][-4.2141776 -4.1909256 -4.1467366 -4.0966487 -4.0276074 -3.9385569 -3.8272402 -3.7656677 -3.82822 -3.9423368 -4.0476117 -4.1146331 -4.1630244 -4.1942949 -4.2174716][-4.2403173 -4.2198958 -4.1866755 -4.1478176 -4.0995741 -4.0414386 -3.9728444 -3.9346046 -3.9682112 -4.0391951 -4.1109495 -4.157908 -4.1972094 -4.2294517 -4.2571754][-4.25142 -4.2336664 -4.2131276 -4.1889358 -4.1644163 -4.1312695 -4.0923357 -4.0719028 -4.0852418 -4.1230073 -4.1656456 -4.1967692 -4.2272115 -4.2587028 -4.2858071][-4.2602706 -4.243825 -4.2319107 -4.2206035 -4.2122049 -4.2010136 -4.185111 -4.1794453 -4.1827469 -4.1924019 -4.2098713 -4.2268982 -4.2468767 -4.27322 -4.2960091][-4.2821479 -4.268229 -4.2588964 -4.2520537 -4.24866 -4.2473903 -4.2465916 -4.2505383 -4.2504029 -4.2476792 -4.25119 -4.2561255 -4.2665792 -4.2854018 -4.2996311][-4.3026028 -4.29022 -4.279737 -4.2700014 -4.2657418 -4.2678857 -4.2741804 -4.2823296 -4.2841744 -4.2817655 -4.2813263 -4.28211 -4.2876339 -4.2986093 -4.3060875][-4.3104887 -4.3001461 -4.290328 -4.2807288 -4.2775512 -4.2816129 -4.2891665 -4.2971158 -4.3007135 -4.3004284 -4.3000188 -4.3005481 -4.3030438 -4.3079329 -4.3113375]]...]
INFO - root - 2017-12-05 16:03:39.544107: step 22910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 75h:34m:43s remains)
INFO - root - 2017-12-05 16:03:48.073271: step 22920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 73h:00m:10s remains)
INFO - root - 2017-12-05 16:03:56.570608: step 22930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 73h:28m:18s remains)
INFO - root - 2017-12-05 16:04:05.118764: step 22940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 74h:56m:40s remains)
INFO - root - 2017-12-05 16:04:13.668968: step 22950, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 71h:54m:04s remains)
INFO - root - 2017-12-05 16:04:22.263455: step 22960, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 73h:40m:24s remains)
INFO - root - 2017-12-05 16:04:30.795026: step 22970, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 72h:25m:49s remains)
INFO - root - 2017-12-05 16:04:39.230896: step 22980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 73h:49m:01s remains)
INFO - root - 2017-12-05 16:04:47.842906: step 22990, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 71h:24m:12s remains)
INFO - root - 2017-12-05 16:04:56.318930: step 23000, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 74h:03m:44s remains)
2017-12-05 16:04:57.063504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1710916 -4.1904511 -4.1735735 -4.1322894 -4.0883284 -4.0597167 -4.065918 -4.089509 -4.1093941 -4.1278372 -4.1444716 -4.1461072 -4.1326733 -4.1432152 -4.1563425][-4.1778932 -4.1944518 -4.1841187 -4.1556106 -4.1182051 -4.0844903 -4.0878296 -4.1164627 -4.1441875 -4.1606293 -4.1755295 -4.1821761 -4.163126 -4.1567369 -4.1528449][-4.17379 -4.1947913 -4.1965404 -4.1843019 -4.1554494 -4.1216731 -4.1137 -4.1344957 -4.1583271 -4.1715455 -4.1889753 -4.2080564 -4.1998506 -4.1864777 -4.1701226][-4.1621771 -4.1857119 -4.197001 -4.1951318 -4.1741924 -4.1412325 -4.1214123 -4.127902 -4.1479936 -4.1654539 -4.1912923 -4.2179608 -4.2212782 -4.2120533 -4.1894965][-4.1471019 -4.1644087 -4.1789713 -4.1841016 -4.16981 -4.1433043 -4.1197457 -4.1132369 -4.1294527 -4.1562238 -4.189424 -4.2151217 -4.2212296 -4.219728 -4.1983933][-4.1285233 -4.136765 -4.1532736 -4.1677308 -4.1586518 -4.1395273 -4.1222849 -4.1084065 -4.1139903 -4.1371651 -4.1684484 -4.19057 -4.2009025 -4.2109766 -4.2000561][-4.1074305 -4.1107445 -4.1292877 -4.145925 -4.1378202 -4.1247964 -4.1160946 -4.1066656 -4.1031938 -4.114059 -4.1342816 -4.1461973 -4.1574888 -4.1777782 -4.1786537][-4.1031427 -4.0988708 -4.1169186 -4.1279941 -4.1160851 -4.102201 -4.1069045 -4.1132116 -4.1133823 -4.1182761 -4.1273465 -4.1291289 -4.1372142 -4.1549315 -4.1556153][-4.1074877 -4.0992808 -4.1147189 -4.1204009 -4.1056161 -4.0863533 -4.0986938 -4.1302609 -4.1526074 -4.1645751 -4.1695061 -4.1666884 -4.1694832 -4.1793041 -4.1757588][-4.1031723 -4.0953155 -4.1160512 -4.1299853 -4.1170259 -4.0931396 -4.1065106 -4.1539965 -4.1951294 -4.2152042 -4.2202368 -4.2205629 -4.2232594 -4.2270141 -4.2221923][-4.0984869 -4.0921731 -4.1261511 -4.1591592 -4.157691 -4.1326127 -4.1364264 -4.1736679 -4.2111335 -4.2321892 -4.242692 -4.2541432 -4.2615514 -4.2664418 -4.2667][-4.0884905 -4.0779786 -4.1159396 -4.1646996 -4.1835942 -4.1684132 -4.1617761 -4.1809063 -4.20675 -4.2229061 -4.237515 -4.2567892 -4.2698727 -4.2797084 -4.2837577][-4.0762649 -4.0588522 -4.087317 -4.1364942 -4.1677575 -4.1670094 -4.1616287 -4.1733103 -4.1920424 -4.2052326 -4.2210217 -4.2420011 -4.2599282 -4.27194 -4.2792339][-4.0480447 -4.0316935 -4.0575252 -4.1127825 -4.1527877 -4.1578174 -4.1511345 -4.1577888 -4.1720295 -4.18539 -4.1990876 -4.2166338 -4.2329073 -4.2415075 -4.2488241][-4.0043974 -3.9929562 -4.0217962 -4.0902886 -4.1424513 -4.1508937 -4.1408429 -4.1400323 -4.1463094 -4.1587849 -4.1710181 -4.1881094 -4.2035723 -4.211112 -4.2175355]]...]
INFO - root - 2017-12-05 16:05:05.617750: step 23010, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 73h:51m:12s remains)
INFO - root - 2017-12-05 16:05:14.170787: step 23020, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 73h:12m:10s remains)
INFO - root - 2017-12-05 16:05:22.868064: step 23030, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 75h:03m:26s remains)
INFO - root - 2017-12-05 16:05:31.399749: step 23040, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 74h:06m:39s remains)
INFO - root - 2017-12-05 16:05:39.961723: step 23050, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 75h:42m:18s remains)
INFO - root - 2017-12-05 16:05:48.482255: step 23060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 72h:55m:31s remains)
INFO - root - 2017-12-05 16:05:57.182654: step 23070, loss = 2.08, batch loss = 2.02 (8.2 examples/sec; 0.980 sec/batch; 84h:14m:59s remains)
INFO - root - 2017-12-05 16:06:05.626376: step 23080, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 73h:07m:25s remains)
INFO - root - 2017-12-05 16:06:13.983206: step 23090, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.809 sec/batch; 69h:33m:16s remains)
INFO - root - 2017-12-05 16:06:22.475470: step 23100, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 73h:15m:17s remains)
2017-12-05 16:06:23.266327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1724877 -4.1770296 -4.1731067 -4.1535935 -4.1275234 -4.1106009 -4.1148119 -4.1342421 -4.1674867 -4.2161775 -4.2555156 -4.2783995 -4.2885413 -4.2893376 -4.2864928][-4.152854 -4.1630449 -4.1613832 -4.1407866 -4.1138997 -4.0894089 -4.0873089 -4.1023917 -4.1400023 -4.2016673 -4.2503929 -4.2751832 -4.2825842 -4.2826505 -4.279603][-4.1400571 -4.1565752 -4.1572757 -4.1397772 -4.1138568 -4.0849857 -4.0749655 -4.0870962 -4.1282158 -4.1929803 -4.2390041 -4.2574587 -4.2624426 -4.2625952 -4.2605076][-4.1146417 -4.134676 -4.1423373 -4.1338606 -4.1127911 -4.0867782 -4.0718884 -4.0804286 -4.1221905 -4.1782856 -4.214129 -4.2219715 -4.2209377 -4.2209048 -4.2206798][-4.0732121 -4.0880308 -4.1051707 -4.1158648 -4.1083169 -4.0861855 -4.05862 -4.0551243 -4.0967579 -4.1503944 -4.17972 -4.1813979 -4.1778188 -4.1807671 -4.1845059][-4.0462456 -4.0584278 -4.0812507 -4.0993652 -4.0901794 -4.0476708 -3.9824865 -3.960001 -4.0148311 -4.0877891 -4.1294155 -4.140399 -4.144299 -4.151123 -4.1562805][-4.0475445 -4.0590315 -4.0795703 -4.0822849 -4.0453529 -3.9601481 -3.8363748 -3.7894447 -3.8803258 -3.9965534 -4.070796 -4.1045189 -4.1192088 -4.1302338 -4.1358447][-4.0595222 -4.0671034 -4.0786519 -4.065629 -4.0035572 -3.8839617 -3.7174659 -3.6569021 -3.7925925 -3.9513917 -4.0529976 -4.1029048 -4.1214442 -4.1311426 -4.1347933][-4.0615716 -4.0633492 -4.0714226 -4.06259 -4.00181 -3.8952088 -3.7566588 -3.709487 -3.8427196 -3.9986517 -4.0923285 -4.1330113 -4.143981 -4.1485748 -4.1472654][-4.0447941 -4.046814 -4.064177 -4.0721412 -4.0373983 -3.969754 -3.8806248 -3.8463652 -3.9375539 -4.0636663 -4.1373186 -4.1658311 -4.1713462 -4.1693673 -4.16059][-4.0262547 -4.0288997 -4.0496154 -4.0710526 -4.0683923 -4.0355959 -3.9782598 -3.9479856 -4.0078793 -4.1121082 -4.1760979 -4.1938734 -4.1949592 -4.185842 -4.1663876][-4.0290656 -4.0243711 -4.0346107 -4.0618296 -4.0813365 -4.0735621 -4.0372295 -4.0094352 -4.0483427 -4.133472 -4.1899638 -4.1988087 -4.1996875 -4.1887274 -4.1650376][-4.0500436 -4.0338378 -4.0314813 -4.0560122 -4.0819736 -4.08721 -4.0676966 -4.04622 -4.0658989 -4.1280127 -4.16985 -4.1770096 -4.1831512 -4.1756797 -4.1517267][-4.0857477 -4.0606651 -4.0417385 -4.0568337 -4.0789413 -4.085381 -4.0785737 -4.061729 -4.064023 -4.1015244 -4.12803 -4.1362762 -4.148582 -4.1496978 -4.131484][-4.1149216 -4.0854011 -4.0624189 -4.0706062 -4.081696 -4.0857372 -4.0865397 -4.0723929 -4.0632429 -4.0804391 -4.0984254 -4.11403 -4.1323576 -4.1388812 -4.1243706]]...]
INFO - root - 2017-12-05 16:06:31.816573: step 23110, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 72h:35m:29s remains)
INFO - root - 2017-12-05 16:06:40.375400: step 23120, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 75h:35m:42s remains)
INFO - root - 2017-12-05 16:06:48.985191: step 23130, loss = 2.06, batch loss = 2.00 (8.0 examples/sec; 0.996 sec/batch; 85h:34m:28s remains)
INFO - root - 2017-12-05 16:06:57.574430: step 23140, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 72h:25m:59s remains)
INFO - root - 2017-12-05 16:07:06.049021: step 23150, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 70h:33m:51s remains)
INFO - root - 2017-12-05 16:07:14.537937: step 23160, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 75h:45m:16s remains)
INFO - root - 2017-12-05 16:07:22.971801: step 23170, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.829 sec/batch; 71h:12m:57s remains)
INFO - root - 2017-12-05 16:07:31.415687: step 23180, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 73h:13m:52s remains)
INFO - root - 2017-12-05 16:07:40.023979: step 23190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 72h:04m:38s remains)
INFO - root - 2017-12-05 16:07:48.530055: step 23200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 73h:40m:54s remains)
2017-12-05 16:07:49.287675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3461466 -4.3409929 -4.3353457 -4.3306651 -4.3294549 -4.3307 -4.3347492 -4.3370457 -4.3373127 -4.3370523 -4.3351092 -4.3331814 -4.3321662 -4.3316331 -4.3321157][-4.3460045 -4.3428097 -4.3389359 -4.3349247 -4.333025 -4.3326325 -4.3343563 -4.3354716 -4.3362317 -4.33693 -4.3358173 -4.3339415 -4.3323445 -4.331871 -4.3331814][-4.3438549 -4.3424611 -4.340148 -4.3369069 -4.3338733 -4.331048 -4.3301358 -4.3305597 -4.3327751 -4.3359518 -4.3375745 -4.3377309 -4.3360929 -4.3344574 -4.3353186][-4.3418813 -4.3419566 -4.340898 -4.33767 -4.3320417 -4.3255172 -4.3213372 -4.3205595 -4.3244333 -4.3307347 -4.33689 -4.3414927 -4.3411627 -4.3382697 -4.3368788][-4.3382392 -4.3393116 -4.3381772 -4.3324575 -4.3214397 -4.3078146 -4.2956176 -4.2889681 -4.2938185 -4.3071971 -4.3226662 -4.3368249 -4.3428349 -4.3405309 -4.3365526][-4.3282046 -4.3295612 -4.3271003 -4.3159108 -4.2941446 -4.2653637 -4.2352543 -4.2166977 -4.2235627 -4.2499037 -4.2836843 -4.3157024 -4.33493 -4.3386049 -4.3344278][-4.3131528 -4.3145366 -4.3100247 -4.2911959 -4.2515821 -4.191926 -4.127635 -4.0891476 -4.1030264 -4.1534367 -4.2194781 -4.2779512 -4.3154178 -4.32983 -4.32913][-4.2991762 -4.3008013 -4.2926106 -4.2632947 -4.2008209 -4.1011686 -3.9906392 -3.9247479 -3.9529247 -4.0368719 -4.1412945 -4.23051 -4.2890277 -4.3175311 -4.32433][-4.2946968 -4.2973371 -4.2865334 -4.2485766 -4.1680665 -4.0394521 -3.8926971 -3.8029275 -3.8484864 -3.9628191 -4.0948243 -4.2018471 -4.2735505 -4.3120103 -4.3249564][-4.3016944 -4.3067508 -4.296319 -4.2588553 -4.1818805 -4.0594645 -3.9221544 -3.8379412 -3.8810802 -3.9870343 -4.1080294 -4.2062736 -4.2741232 -4.3136292 -4.3284063][-4.3106475 -4.3199825 -4.3144426 -4.2872181 -4.2326093 -4.1452951 -4.049778 -3.992131 -4.017405 -4.0876422 -4.1728258 -4.2432885 -4.2932858 -4.3226738 -4.3324518][-4.3157678 -4.3299842 -4.3319259 -4.3193197 -4.2919531 -4.2448716 -4.1926217 -4.1605678 -4.1720676 -4.20885 -4.2550654 -4.2939615 -4.3212657 -4.3357692 -4.3375635][-4.3144736 -4.3320804 -4.3405404 -4.3398161 -4.3309879 -4.3108306 -4.2871938 -4.2719431 -4.2751236 -4.2908053 -4.3118348 -4.3288121 -4.3388162 -4.3425374 -4.3395834][-4.3107309 -4.3286543 -4.3406334 -4.3469357 -4.3461194 -4.3370023 -4.3235931 -4.312705 -4.310226 -4.3155184 -4.3262181 -4.3347807 -4.3396649 -4.3421583 -4.340292][-4.3045826 -4.319407 -4.3328223 -4.3435774 -4.3461404 -4.338213 -4.3239093 -4.3085141 -4.2996573 -4.3008137 -4.3118396 -4.3243465 -4.3338256 -4.3409619 -4.34252]]...]
INFO - root - 2017-12-05 16:07:57.711897: step 23210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 72h:22m:54s remains)
INFO - root - 2017-12-05 16:08:06.241526: step 23220, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 75h:26m:57s remains)
INFO - root - 2017-12-05 16:08:14.827761: step 23230, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 73h:12m:52s remains)
INFO - root - 2017-12-05 16:08:23.299260: step 23240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 73h:16m:40s remains)
INFO - root - 2017-12-05 16:08:31.931460: step 23250, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.842 sec/batch; 72h:22m:16s remains)
INFO - root - 2017-12-05 16:08:40.593334: step 23260, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 73h:55m:03s remains)
INFO - root - 2017-12-05 16:08:49.180306: step 23270, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 74h:56m:59s remains)
INFO - root - 2017-12-05 16:08:57.488747: step 23280, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 72h:43m:42s remains)
INFO - root - 2017-12-05 16:09:06.031825: step 23290, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 72h:36m:08s remains)
INFO - root - 2017-12-05 16:09:14.581577: step 23300, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.884 sec/batch; 75h:54m:43s remains)
2017-12-05 16:09:15.418086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1001797 -4.07254 -4.0653591 -4.0691466 -4.0639796 -4.0528178 -4.0578632 -4.0741339 -4.0853853 -4.0891323 -4.0946965 -4.0970736 -4.0849757 -4.0864911 -4.10207][-4.1190891 -4.0922532 -4.07663 -4.0697041 -4.0586638 -4.0409584 -4.0341849 -4.0518913 -4.0768604 -4.0875831 -4.0918965 -4.0924811 -4.0835171 -4.0889215 -4.1102829][-4.1268959 -4.107553 -4.0876579 -4.072948 -4.0557709 -4.0347981 -4.0209908 -4.0365829 -4.0654535 -4.0783663 -4.0858865 -4.0922723 -4.0892963 -4.1002107 -4.1262169][-4.1191678 -4.104857 -4.080246 -4.0565381 -4.0287561 -4.0035114 -3.9977689 -4.0240412 -4.0553393 -4.0639262 -4.0700722 -4.0762715 -4.0765238 -4.0945997 -4.1253142][-4.1132278 -4.0981832 -4.0671754 -4.0349522 -3.9968221 -3.9647443 -3.9672844 -4.0098829 -4.0501971 -4.0568728 -4.055131 -4.0569825 -4.0534687 -4.0688677 -4.1019697][-4.1284208 -4.1177135 -4.089221 -4.0562053 -4.0155764 -3.9802029 -3.9799659 -4.0218496 -4.0599656 -4.0717239 -4.072825 -4.0698962 -4.0499086 -4.0435042 -4.0628042][-4.1318836 -4.1329775 -4.1173978 -4.0959229 -4.0659771 -4.0399694 -4.0402021 -4.072906 -4.1021624 -4.110209 -4.1127553 -4.1087632 -4.0809464 -4.0493994 -4.0428543][-4.126543 -4.1334906 -4.1252003 -4.1170254 -4.1054373 -4.0924969 -4.0935788 -4.1193118 -4.1454959 -4.1522813 -4.1500335 -4.148685 -4.126884 -4.0974474 -4.0809264][-4.1333442 -4.13633 -4.1231575 -4.1166534 -4.1136127 -4.1096349 -4.110178 -4.1275029 -4.1537576 -4.1692219 -4.1713753 -4.1708236 -4.1535878 -4.1338754 -4.1218224][-4.1529007 -4.1504602 -4.1347694 -4.12807 -4.1298194 -4.1283455 -4.1259031 -4.1276522 -4.138082 -4.1503887 -4.158525 -4.1638393 -4.1550636 -4.1395664 -4.1266727][-4.163856 -4.1613836 -4.150095 -4.1452017 -4.1470237 -4.1462131 -4.1404839 -4.133667 -4.1351871 -4.1422639 -4.1508265 -4.157958 -4.1537795 -4.1406784 -4.1214218][-4.150558 -4.1517267 -4.1512971 -4.1528153 -4.1565676 -4.1602583 -4.1572385 -4.1499166 -4.1515093 -4.1596994 -4.1670003 -4.1728306 -4.1685042 -4.1548882 -4.1300578][-4.1330304 -4.1328092 -4.1369977 -4.1408353 -4.1424923 -4.1454887 -4.147295 -4.1461296 -4.1513715 -4.1633873 -4.1746473 -4.1852031 -4.1861911 -4.1763783 -4.1557655][-4.1471443 -4.1409144 -4.1399932 -4.1376152 -4.1335831 -4.1312132 -4.1290822 -4.1279726 -4.1316319 -4.1405067 -4.1503754 -4.1604309 -4.1643176 -4.1640043 -4.1573944][-4.1775351 -4.1706457 -4.1647439 -4.1567636 -4.1474123 -4.1386786 -4.1309352 -4.1250243 -4.1190062 -4.1151814 -4.1156788 -4.1213183 -4.1237321 -4.1262708 -4.1279483]]...]
INFO - root - 2017-12-05 16:09:24.084905: step 23310, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.900 sec/batch; 77h:15m:50s remains)
INFO - root - 2017-12-05 16:09:32.549813: step 23320, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 74h:20m:06s remains)
INFO - root - 2017-12-05 16:09:41.333314: step 23330, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 75h:10m:38s remains)
INFO - root - 2017-12-05 16:09:49.797453: step 23340, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 74h:30m:56s remains)
INFO - root - 2017-12-05 16:09:58.363369: step 23350, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 72h:23m:38s remains)
INFO - root - 2017-12-05 16:10:06.880759: step 23360, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 74h:18m:55s remains)
INFO - root - 2017-12-05 16:10:15.455218: step 23370, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 73h:56m:27s remains)
INFO - root - 2017-12-05 16:10:24.013386: step 23380, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 73h:13m:34s remains)
INFO - root - 2017-12-05 16:10:32.402476: step 23390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 72h:59m:52s remains)
INFO - root - 2017-12-05 16:10:40.867080: step 23400, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 72h:20m:50s remains)
2017-12-05 16:10:41.653768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2482953 -4.2369065 -4.2223744 -4.1996789 -4.1617346 -4.1175637 -4.0864797 -4.0844474 -4.1043315 -4.1422958 -4.18858 -4.2416434 -4.284337 -4.3090014 -4.3135357][-4.2289534 -4.2216463 -4.2082577 -4.1831546 -4.1392536 -4.0825739 -4.0409 -4.0386715 -4.0610394 -4.1068034 -4.1583681 -4.22181 -4.2789488 -4.3132586 -4.3231964][-4.2088933 -4.2048044 -4.1935897 -4.1669369 -4.1157622 -4.0455503 -3.9936194 -3.9886987 -4.0185537 -4.0721078 -4.1300321 -4.1994195 -4.2659369 -4.3090425 -4.327949][-4.179687 -4.1843686 -4.1787515 -4.1511993 -4.0958753 -4.0155859 -3.9523046 -3.9402537 -3.9739027 -4.0349216 -4.1043339 -4.1783409 -4.248827 -4.2993121 -4.3279762][-4.1487989 -4.161664 -4.1631594 -4.1367164 -4.0787964 -3.9978273 -3.9223981 -3.8991327 -3.9303203 -3.9983501 -4.0793686 -4.1577644 -4.2349052 -4.291316 -4.3260078][-4.1321654 -4.1463847 -4.1476669 -4.1240697 -4.070797 -3.9964159 -3.922394 -3.9008594 -3.9263303 -3.9892292 -4.070168 -4.14565 -4.22157 -4.2794933 -4.31822][-4.1296377 -4.1383014 -4.1384139 -4.1236582 -4.0895686 -4.0365863 -3.9798787 -3.9490108 -3.944088 -3.9854133 -4.0601058 -4.1336012 -4.2082396 -4.2680035 -4.3084936][-4.1483612 -4.1482019 -4.1436872 -4.1331062 -4.11676 -4.0881152 -4.0400691 -3.9892857 -3.9564173 -3.9717302 -4.0345945 -4.1082258 -4.1838379 -4.2494521 -4.297791][-4.1870165 -4.18559 -4.176043 -4.1573124 -4.1442637 -4.1275053 -4.0819817 -4.0255284 -3.9902439 -3.9964576 -4.0461264 -4.1154981 -4.1825147 -4.2436371 -4.2931256][-4.2326741 -4.2333283 -4.2223907 -4.2000332 -4.1855469 -4.1711946 -4.132669 -4.0832963 -4.0544777 -4.0562019 -4.08998 -4.1488161 -4.2062693 -4.258317 -4.2990251][-4.2782273 -4.2754674 -4.2614417 -4.2420764 -4.23032 -4.2165661 -4.1840382 -4.1401114 -4.1164427 -4.1168394 -4.1413083 -4.1912584 -4.2373514 -4.2797184 -4.3107953][-4.3090773 -4.3067684 -4.2945042 -4.28028 -4.2693419 -4.257781 -4.2335782 -4.1995173 -4.1825695 -4.1865535 -4.2079926 -4.2459784 -4.2779856 -4.3051496 -4.3240085][-4.3253379 -4.3244925 -4.3180861 -4.3108015 -4.3023815 -4.2937827 -4.280992 -4.2613506 -4.2508049 -4.2538404 -4.2685394 -4.2935967 -4.3129129 -4.327579 -4.3369431][-4.338048 -4.3359461 -4.3332219 -4.3299894 -4.3251023 -4.3202968 -4.3156033 -4.3072824 -4.3026638 -4.3033466 -4.3099761 -4.3221822 -4.3326383 -4.3398819 -4.342701][-4.3431735 -4.3397164 -4.3379869 -4.3357034 -4.3328242 -4.3309951 -4.3302531 -4.3283663 -4.3271418 -4.3270445 -4.3286147 -4.3326006 -4.3369479 -4.3392477 -4.3392925]]...]
INFO - root - 2017-12-05 16:10:50.194984: step 23410, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 73h:57m:08s remains)
INFO - root - 2017-12-05 16:10:58.707605: step 23420, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 72h:42m:03s remains)
INFO - root - 2017-12-05 16:11:07.350220: step 23430, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 70h:56m:39s remains)
INFO - root - 2017-12-05 16:11:16.010390: step 23440, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 75h:03m:01s remains)
INFO - root - 2017-12-05 16:11:24.567007: step 23450, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 74h:22m:57s remains)
INFO - root - 2017-12-05 16:11:33.182662: step 23460, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 74h:56m:44s remains)
INFO - root - 2017-12-05 16:11:41.658470: step 23470, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 73h:22m:48s remains)
INFO - root - 2017-12-05 16:11:50.138782: step 23480, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 70h:07m:22s remains)
INFO - root - 2017-12-05 16:11:58.659822: step 23490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 72h:23m:57s remains)
INFO - root - 2017-12-05 16:12:07.339119: step 23500, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 71h:46m:25s remains)
2017-12-05 16:12:08.193866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2502551 -4.2475491 -4.2487493 -4.2554655 -4.2659154 -4.279026 -4.2817826 -4.2817965 -4.2799077 -4.2784023 -4.2750154 -4.2706075 -4.2641835 -4.2587419 -4.2598772][-4.206809 -4.2006326 -4.2042694 -4.2153273 -4.2299232 -4.2428074 -4.245513 -4.2455063 -4.2444882 -4.2442074 -4.2401981 -4.2341394 -4.2280812 -4.2262111 -4.2307277][-4.1683702 -4.1633081 -4.1699553 -4.1836305 -4.20011 -4.2105451 -4.2046866 -4.201365 -4.2027612 -4.2048836 -4.1989007 -4.1896415 -4.1886458 -4.1960859 -4.206594][-4.1410832 -4.1426125 -4.1557856 -4.1686182 -4.1745119 -4.1699491 -4.1477637 -4.1402574 -4.1537209 -4.1650872 -4.1617951 -4.1516724 -4.1584167 -4.1759729 -4.1935043][-4.1230145 -4.1315584 -4.15379 -4.1639585 -4.1525388 -4.1163387 -4.064589 -4.0575976 -4.0962086 -4.1231985 -4.1222167 -4.1133747 -4.1309004 -4.1603131 -4.1889148][-4.1157656 -4.1203942 -4.14382 -4.1475973 -4.1147475 -4.0379972 -3.9444754 -3.942142 -4.0232043 -4.0841203 -4.0982394 -4.0979624 -4.1292486 -4.172976 -4.2089405][-4.0996261 -4.092155 -4.1102843 -4.1041646 -4.0423641 -3.9183118 -3.7780588 -3.7885728 -3.9338031 -4.0465708 -4.0912156 -4.1036053 -4.1387429 -4.1876326 -4.2268758][-4.1040845 -4.0811906 -4.0867457 -4.0763016 -4.007349 -3.8702035 -3.717232 -3.7381163 -3.9031539 -4.0352793 -4.0945921 -4.1116724 -4.1426711 -4.1861129 -4.22564][-4.1229291 -4.092195 -4.09256 -4.0946517 -4.0572677 -3.9724584 -3.8789544 -3.8976538 -4.0090375 -4.1085896 -4.1502056 -4.1572795 -4.1766973 -4.2086296 -4.241395][-4.1154413 -4.0833988 -4.0899992 -4.1076369 -4.1021395 -4.0664339 -4.0234394 -4.0401177 -4.1082754 -4.1759448 -4.1981583 -4.1946688 -4.2086458 -4.2320132 -4.2553415][-4.1216483 -4.0971985 -4.114192 -4.1375771 -4.14437 -4.1300631 -4.1110907 -4.1267786 -4.1699882 -4.2144885 -4.2292209 -4.2254725 -4.23839 -4.2574987 -4.27253][-4.1504617 -4.1427231 -4.1714683 -4.1982946 -4.2087297 -4.2013221 -4.1916347 -4.2053514 -4.2335873 -4.2623434 -4.2735505 -4.2729163 -4.2813683 -4.2935514 -4.2996917][-4.1867967 -4.1949167 -4.2269044 -4.2510595 -4.2583385 -4.253552 -4.2493238 -4.2588177 -4.2787871 -4.297739 -4.3040028 -4.3005104 -4.30337 -4.3105512 -4.3137827][-4.2322826 -4.2427673 -4.268086 -4.2865953 -4.2910166 -4.2865 -4.2847271 -4.2910914 -4.3041158 -4.3148146 -4.3160982 -4.3086376 -4.3060489 -4.3093877 -4.3128753][-4.2775178 -4.2803521 -4.2922983 -4.3018627 -4.3055587 -4.3047552 -4.3064265 -4.3117781 -4.3191628 -4.3246169 -4.3228679 -4.3161163 -4.310873 -4.3121195 -4.3166828]]...]
INFO - root - 2017-12-05 16:12:16.700009: step 23510, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 74h:59m:43s remains)
INFO - root - 2017-12-05 16:12:25.208034: step 23520, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 74h:29m:48s remains)
INFO - root - 2017-12-05 16:12:33.679928: step 23530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 74h:16m:38s remains)
INFO - root - 2017-12-05 16:12:42.148194: step 23540, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:42m:57s remains)
INFO - root - 2017-12-05 16:12:50.683135: step 23550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 72h:01m:36s remains)
INFO - root - 2017-12-05 16:12:59.221170: step 23560, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 75h:24m:21s remains)
INFO - root - 2017-12-05 16:13:07.855949: step 23570, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 72h:54m:47s remains)
INFO - root - 2017-12-05 16:13:16.215614: step 23580, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 71h:49m:14s remains)
INFO - root - 2017-12-05 16:13:24.680322: step 23590, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:50m:28s remains)
INFO - root - 2017-12-05 16:13:33.246734: step 23600, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.810 sec/batch; 69h:28m:44s remains)
2017-12-05 16:13:34.032809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3360338 -4.3335042 -4.3311553 -4.3300066 -4.3183422 -4.3054161 -4.299839 -4.3033924 -4.3063731 -4.3121219 -4.3180056 -4.3218589 -4.3261108 -4.3290238 -4.3295569][-4.329433 -4.3213768 -4.313457 -4.3035703 -4.2806416 -4.2591276 -4.2534556 -4.2655549 -4.2752333 -4.290092 -4.3043833 -4.3122211 -4.3192754 -4.3253756 -4.3286476][-4.3213434 -4.3054757 -4.2881021 -4.2660871 -4.2305903 -4.1965337 -4.1872582 -4.2081189 -4.22706 -4.2533512 -4.2802382 -4.2948623 -4.30735 -4.3192916 -4.3265581][-4.314415 -4.2913618 -4.2622232 -4.2243266 -4.1722507 -4.122695 -4.1065979 -4.1322327 -4.1629386 -4.2040071 -4.2491422 -4.2750845 -4.2939048 -4.3107443 -4.320374][-4.3114195 -4.2830496 -4.2410941 -4.184083 -4.1110678 -4.0446486 -4.0226393 -4.0534797 -4.0960612 -4.1537676 -4.2182088 -4.257781 -4.2833295 -4.3031812 -4.3125896][-4.31355 -4.2858119 -4.2390251 -4.1691408 -4.0806584 -4.0031195 -3.9800172 -4.0165243 -4.0680189 -4.1352744 -4.2069545 -4.2528424 -4.280323 -4.2992091 -4.3060803][-4.3171144 -4.29323 -4.251564 -4.185009 -4.0985065 -4.0206771 -4.0015626 -4.0420723 -4.0965967 -4.1590919 -4.2224731 -4.26257 -4.2860065 -4.3008628 -4.3038993][-4.3204856 -4.3010139 -4.2690558 -4.2155914 -4.146328 -4.0808711 -4.0657263 -4.1025534 -4.1536403 -4.2060061 -4.2551436 -4.2833595 -4.2996907 -4.3094382 -4.3086505][-4.324234 -4.3085413 -4.2826443 -4.23912 -4.1837053 -4.130909 -4.1171241 -4.1468248 -4.1914158 -4.2370687 -4.2781444 -4.2989907 -4.3105321 -4.3164144 -4.3141246][-4.3276539 -4.3154564 -4.2932792 -4.2540193 -4.2034421 -4.1545444 -4.136354 -4.1572132 -4.19566 -4.2378697 -4.2788167 -4.300106 -4.3107052 -4.3160272 -4.3154488][-4.3295441 -4.3195877 -4.3000917 -4.26257 -4.2124567 -4.162457 -4.135613 -4.1450367 -4.1775069 -4.2189217 -4.2631736 -4.2882485 -4.3023419 -4.3109517 -4.3140779][-4.3288617 -4.3184242 -4.2991433 -4.2622442 -4.2128906 -4.1623664 -4.1297913 -4.1298714 -4.1598916 -4.2048364 -4.2502627 -4.2773557 -4.2944307 -4.3064065 -4.3124247][-4.3266821 -4.3145843 -4.292428 -4.2563128 -4.2103629 -4.1651855 -4.1357417 -4.1347866 -4.165041 -4.2100692 -4.2511377 -4.2768726 -4.294138 -4.3063793 -4.3128953][-4.3251357 -4.3126149 -4.2893395 -4.257164 -4.2189555 -4.1839442 -4.163166 -4.1655736 -4.1934066 -4.2318506 -4.2640681 -4.285965 -4.3010936 -4.3116112 -4.3168826][-4.325243 -4.31423 -4.2939744 -4.2688231 -4.2411804 -4.2180657 -4.2060905 -4.2106748 -4.2328682 -4.2610264 -4.2829204 -4.2981973 -4.3092713 -4.31708 -4.3201981]]...]
INFO - root - 2017-12-05 16:13:42.554194: step 23610, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 74h:04m:22s remains)
INFO - root - 2017-12-05 16:13:51.064951: step 23620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 75h:18m:37s remains)
INFO - root - 2017-12-05 16:13:59.500659: step 23630, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.789 sec/batch; 67h:39m:39s remains)
INFO - root - 2017-12-05 16:14:08.156164: step 23640, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 75h:12m:15s remains)
INFO - root - 2017-12-05 16:14:16.716732: step 23650, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 74h:43m:51s remains)
INFO - root - 2017-12-05 16:14:25.203037: step 23660, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 72h:59m:47s remains)
INFO - root - 2017-12-05 16:14:33.768539: step 23670, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 71h:53m:28s remains)
INFO - root - 2017-12-05 16:14:42.287842: step 23680, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 72h:43m:06s remains)
INFO - root - 2017-12-05 16:14:50.847455: step 23690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 74h:55m:56s remains)
INFO - root - 2017-12-05 16:14:59.363075: step 23700, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 72h:23m:20s remains)
2017-12-05 16:15:00.120691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3053021 -4.3108153 -4.3122034 -4.3112183 -4.3099279 -4.3092608 -4.3089614 -4.3083582 -4.3072877 -4.307776 -4.3087492 -4.3072262 -4.3020592 -4.296627 -4.2939634][-4.2782016 -4.2864246 -4.2909026 -4.293447 -4.2952824 -4.2958555 -4.2955995 -4.2947664 -4.2942052 -4.2955623 -4.2975435 -4.2960382 -4.2887917 -4.2814822 -4.2776113][-4.2544727 -4.2648077 -4.2706156 -4.2734823 -4.2737603 -4.2714152 -4.2694783 -4.2698226 -4.2717195 -4.2752743 -4.2787962 -4.2786717 -4.2716427 -4.2643385 -4.2611532][-4.2354078 -4.2456708 -4.2505293 -4.25122 -4.2475109 -4.24179 -4.2384229 -4.2395205 -4.2442389 -4.2498088 -4.255362 -4.2580328 -4.2528524 -4.2467785 -4.2438774][-4.2125006 -4.2219825 -4.2257748 -4.2244196 -4.2175913 -4.2089481 -4.2052379 -4.209969 -4.2212424 -4.2305217 -4.2385769 -4.2441788 -4.2397275 -4.2302265 -4.2242475][-4.2019591 -4.2037382 -4.1997318 -4.1917315 -4.1788607 -4.1629014 -4.1521649 -4.1593885 -4.1808429 -4.1998425 -4.2138748 -4.2234182 -4.220284 -4.2095671 -4.203166][-4.2033968 -4.1954541 -4.1802239 -4.1600065 -4.1333556 -4.1005383 -4.0703731 -4.0706029 -4.1031742 -4.1384444 -4.1645093 -4.185904 -4.1933007 -4.1879048 -4.18302][-4.205791 -4.1954269 -4.1782637 -4.1543994 -4.1231179 -4.0813422 -4.0353885 -4.0203953 -4.049746 -4.0923548 -4.1281433 -4.159277 -4.1764312 -4.1751962 -4.1689239][-4.2045064 -4.199451 -4.1909389 -4.1765809 -4.155014 -4.1240964 -4.0875854 -4.0702949 -4.0866094 -4.1183038 -4.1489406 -4.1775746 -4.1945095 -4.194397 -4.1848254][-4.1927862 -4.1924133 -4.1922588 -4.1882586 -4.17939 -4.1642966 -4.1446028 -4.1311111 -4.1371446 -4.1563592 -4.179987 -4.2041492 -4.2222352 -4.2288504 -4.2243319][-4.1765695 -4.1785579 -4.18335 -4.1886892 -4.1922655 -4.1914239 -4.1859603 -4.1777029 -4.1752791 -4.1844687 -4.2011752 -4.2192631 -4.2360244 -4.2487955 -4.2530861][-4.1807919 -4.18289 -4.1875348 -4.1953335 -4.2035041 -4.2102613 -4.2137556 -4.212718 -4.2127962 -4.2201257 -4.2320828 -4.242691 -4.2518611 -4.2608266 -4.2652707][-4.1845589 -4.185112 -4.185987 -4.1907182 -4.196754 -4.204031 -4.2098022 -4.2129116 -4.2175279 -4.2277985 -4.2405014 -4.2491655 -4.2546892 -4.2601075 -4.263104][-4.1759019 -4.1744094 -4.1733212 -4.1755614 -4.1792545 -4.1855626 -4.1913338 -4.1959887 -4.2015681 -4.2098336 -4.2187657 -4.2245984 -4.2284031 -4.2335563 -4.2388868][-4.1772981 -4.1710215 -4.1651616 -4.1619086 -4.1608114 -4.1649532 -4.1703849 -4.1762257 -4.1818824 -4.1874371 -4.1924891 -4.1961169 -4.1996427 -4.2048392 -4.2104683]]...]
INFO - root - 2017-12-05 16:15:08.603311: step 23710, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 72h:41m:33s remains)
INFO - root - 2017-12-05 16:15:17.213034: step 23720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 74h:05m:01s remains)
INFO - root - 2017-12-05 16:15:25.800741: step 23730, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 75h:10m:49s remains)
INFO - root - 2017-12-05 16:15:34.163001: step 23740, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 72h:55m:24s remains)
INFO - root - 2017-12-05 16:15:42.788391: step 23750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 74h:23m:33s remains)
INFO - root - 2017-12-05 16:15:51.485310: step 23760, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 75h:30m:57s remains)
INFO - root - 2017-12-05 16:16:00.080338: step 23770, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 75h:29m:39s remains)
INFO - root - 2017-12-05 16:16:08.499668: step 23780, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 73h:18m:02s remains)
INFO - root - 2017-12-05 16:16:17.081337: step 23790, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 72h:20m:47s remains)
INFO - root - 2017-12-05 16:16:25.700598: step 23800, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 74h:16m:49s remains)
2017-12-05 16:16:26.446805: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043157 -4.1937828 -4.18209 -4.1687932 -4.1557178 -4.1534257 -4.1598234 -4.17481 -4.1937528 -4.2087345 -4.2193575 -4.2179623 -4.2068052 -4.2006092 -4.2011161][-4.2134795 -4.1957235 -4.1713576 -4.1446733 -4.1188869 -4.1111951 -4.1220241 -4.1483965 -4.1853123 -4.2169924 -4.2414374 -4.2498531 -4.2462125 -4.2427258 -4.2409854][-4.2239342 -4.205091 -4.1724038 -4.1361217 -4.1010327 -4.0869741 -4.0973668 -4.1289029 -4.1777472 -4.2211046 -4.2567425 -4.275619 -4.2804909 -4.2807841 -4.2768316][-4.218812 -4.2075148 -4.172473 -4.1287346 -4.0860662 -4.065227 -4.0713191 -4.102777 -4.1566815 -4.2088594 -4.2513528 -4.2785268 -4.2893043 -4.2912769 -4.2836523][-4.1994171 -4.2005587 -4.1681347 -4.1204119 -4.0714722 -4.0407662 -4.03828 -4.0668688 -4.12192 -4.1793742 -4.2246656 -4.2534304 -4.2661014 -4.2687154 -4.2586288][-4.1546073 -4.1742258 -4.1575065 -4.1164317 -4.0660324 -4.0247669 -4.0114317 -4.0319552 -4.0802751 -4.1352582 -4.1789169 -4.2067442 -4.2179365 -4.2218561 -4.2162175][-4.0904617 -4.1308866 -4.1396894 -4.1156774 -4.0721593 -4.0298877 -4.0109143 -4.0225344 -4.060113 -4.1061058 -4.1457014 -4.1725531 -4.1845617 -4.1951294 -4.2030783][-4.0558167 -4.1067009 -4.1321344 -4.1230068 -4.0913591 -4.0592117 -4.04294 -4.0519943 -4.0820165 -4.11712 -4.1507425 -4.1730566 -4.1802359 -4.191968 -4.2105632][-4.0819774 -4.1285386 -4.1550026 -4.1558709 -4.1376171 -4.117979 -4.1066113 -4.1116195 -4.1327791 -4.1550932 -4.1777511 -4.1906509 -4.190237 -4.1979389 -4.2189865][-4.1491423 -4.1845922 -4.2039442 -4.2093167 -4.1996703 -4.18725 -4.1767445 -4.1749806 -4.187737 -4.1963339 -4.2038422 -4.2088852 -4.2043142 -4.2058935 -4.2211971][-4.215589 -4.2371349 -4.2481737 -4.2531323 -4.2486529 -4.2402072 -4.2287493 -4.2234521 -4.2305293 -4.2317414 -4.2285576 -4.226006 -4.2189965 -4.2139606 -4.2174649][-4.2693605 -4.2822814 -4.2868676 -4.2883615 -4.2869244 -4.2831049 -4.2767005 -4.2730761 -4.2771807 -4.2760763 -4.2689004 -4.2607307 -4.2479243 -4.2356548 -4.2264967][-4.2995014 -4.3113761 -4.3158755 -4.3174543 -4.3187971 -4.318388 -4.3163104 -4.3139534 -4.3146677 -4.31177 -4.3034663 -4.2903223 -4.2711029 -4.2519736 -4.233314][-4.2764482 -4.2961721 -4.3109393 -4.320715 -4.3271432 -4.3298645 -4.3296223 -4.3278203 -4.3243127 -4.3185215 -4.3100028 -4.2995667 -4.28288 -4.2636743 -4.2427559][-4.2265639 -4.2570763 -4.2846489 -4.3046184 -4.3163109 -4.3213229 -4.3217883 -4.3203115 -4.3152432 -4.309412 -4.302968 -4.2962379 -4.2828364 -4.26573 -4.2473383]]...]
INFO - root - 2017-12-05 16:16:34.948196: step 23810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 74h:12m:35s remains)
INFO - root - 2017-12-05 16:16:43.397804: step 23820, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 70h:47m:22s remains)
INFO - root - 2017-12-05 16:16:51.905387: step 23830, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 74h:23m:21s remains)
INFO - root - 2017-12-05 16:17:00.417413: step 23840, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 73h:14m:45s remains)
INFO - root - 2017-12-05 16:17:08.832630: step 23850, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 72h:04m:38s remains)
INFO - root - 2017-12-05 16:17:17.547373: step 23860, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 74h:12m:52s remains)
INFO - root - 2017-12-05 16:17:26.119027: step 23870, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 74h:01m:59s remains)
INFO - root - 2017-12-05 16:17:34.540954: step 23880, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 73h:00m:32s remains)
INFO - root - 2017-12-05 16:17:43.178016: step 23890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 73h:54m:20s remains)
INFO - root - 2017-12-05 16:17:51.734807: step 23900, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 72h:27m:55s remains)
2017-12-05 16:17:52.558715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2489796 -4.2527304 -4.2554626 -4.2578058 -4.2585287 -4.2517557 -4.2434297 -4.2430048 -4.2517309 -4.2590909 -4.2622151 -4.2643862 -4.2690978 -4.2786479 -4.2947955][-4.2108779 -4.2125 -4.217968 -4.224771 -4.2316585 -4.2276583 -4.2138638 -4.2093172 -4.224247 -4.241116 -4.2537661 -4.2648859 -4.2757707 -4.28672 -4.3006406][-4.1648645 -4.1574831 -4.1612711 -4.1718473 -4.1850896 -4.1771512 -4.1425104 -4.1267347 -4.1551957 -4.190362 -4.2182245 -4.2414765 -4.2639174 -4.2799888 -4.2953095][-4.1425867 -4.1279516 -4.1290855 -4.1389823 -4.1488652 -4.121841 -4.048656 -4.0140424 -4.0595117 -4.1192751 -4.1675067 -4.2026558 -4.237184 -4.2609997 -4.2821584][-4.1428094 -4.123251 -4.1212134 -4.1303897 -4.137702 -4.0945864 -3.9886291 -3.9384675 -3.999511 -4.0740237 -4.1295872 -4.1672053 -4.2100687 -4.2408714 -4.2670135][-4.1229324 -4.0978289 -4.0915895 -4.103272 -4.1158285 -4.069768 -3.9499121 -3.8900819 -3.9671094 -4.052947 -4.1101809 -4.1476669 -4.19716 -4.2335443 -4.2605557][-4.094903 -4.0681767 -4.0595808 -4.06422 -4.0662436 -4.000823 -3.8509111 -3.775485 -3.8904853 -4.0089297 -4.0811415 -4.1271219 -4.1865282 -4.2278895 -4.2581482][-4.0923743 -4.0658984 -4.0519915 -4.0431209 -4.0172229 -3.9113421 -3.7134261 -3.6283846 -3.796267 -3.9518709 -4.0408411 -4.100904 -4.169528 -4.2160959 -4.2509351][-4.1172643 -4.095015 -4.084795 -4.0686789 -4.0286412 -3.91092 -3.7199078 -3.6496558 -3.8177238 -3.9619541 -4.0447106 -4.1025953 -4.1683064 -4.2119803 -4.2461286][-4.1356421 -4.1208429 -4.1227741 -4.1131897 -4.0813208 -3.991401 -3.8622704 -3.8173409 -3.9368494 -4.0395679 -4.102922 -4.1452928 -4.1962533 -4.2293749 -4.2547255][-4.1415911 -4.1276054 -4.1321707 -4.1286149 -4.1108155 -4.0496006 -3.965344 -3.9292438 -4.0127873 -4.09182 -4.14568 -4.1825304 -4.2228379 -4.2476449 -4.2660871][-4.151814 -4.1347308 -4.1364064 -4.1321931 -4.1193781 -4.0770435 -4.0139809 -3.9753585 -4.0339293 -4.1045246 -4.1592655 -4.1973109 -4.2344995 -4.2580686 -4.2763629][-4.1829343 -4.1705332 -4.1727586 -4.1696544 -4.1591234 -4.1295242 -4.0784006 -4.040288 -4.0753479 -4.1285257 -4.1755719 -4.210073 -4.2428 -4.2675085 -4.2881732][-4.2319689 -4.2264576 -4.2255673 -4.2211142 -4.2134361 -4.1928959 -4.1510615 -4.1165833 -4.1333246 -4.166842 -4.2006922 -4.2284641 -4.254879 -4.2788372 -4.29998][-4.2779512 -4.2735062 -4.2682953 -4.261189 -4.2523937 -4.2340922 -4.1981797 -4.1700997 -4.1793647 -4.2001023 -4.2232914 -4.2458315 -4.2669849 -4.2883186 -4.307744]]...]
INFO - root - 2017-12-05 16:18:01.107349: step 23910, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 72h:44m:11s remains)
INFO - root - 2017-12-05 16:18:09.643592: step 23920, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 74h:12m:32s remains)
INFO - root - 2017-12-05 16:18:18.129970: step 23930, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 73h:51m:49s remains)
INFO - root - 2017-12-05 16:18:26.680037: step 23940, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.824 sec/batch; 70h:35m:43s remains)
INFO - root - 2017-12-05 16:18:35.155456: step 23950, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.772 sec/batch; 66h:10m:18s remains)
INFO - root - 2017-12-05 16:18:43.478739: step 23960, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 71h:13m:42s remains)
INFO - root - 2017-12-05 16:18:51.993320: step 23970, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 70h:33m:04s remains)
INFO - root - 2017-12-05 16:19:00.368131: step 23980, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 70h:55m:03s remains)
INFO - root - 2017-12-05 16:19:08.882019: step 23990, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 70h:13m:13s remains)
INFO - root - 2017-12-05 16:19:17.437714: step 24000, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:31m:52s remains)
2017-12-05 16:19:18.206484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2037826 -4.1927719 -4.1852112 -4.1840034 -4.18277 -4.1840796 -4.1919022 -4.204288 -4.215982 -4.2327032 -4.25338 -4.2693639 -4.2945061 -4.3159542 -4.316699][-4.1866746 -4.1743994 -4.16702 -4.1676731 -4.172328 -4.172421 -4.183816 -4.20487 -4.2212014 -4.2406964 -4.2634416 -4.27828 -4.30551 -4.3271203 -4.3250618][-4.1782889 -4.1667166 -4.1569972 -4.1595197 -4.168138 -4.1694155 -4.1823292 -4.2071371 -4.2261443 -4.2452765 -4.2654696 -4.2811303 -4.3097377 -4.3287711 -4.3247066][-4.1791739 -4.16784 -4.1562886 -4.1539831 -4.1545267 -4.1476021 -4.1565385 -4.185524 -4.2164774 -4.2449808 -4.270247 -4.2888551 -4.3124275 -4.3261104 -4.3194842][-4.1823788 -4.1680098 -4.1518416 -4.1363039 -4.1178741 -4.0940719 -4.09516 -4.1327806 -4.1781535 -4.2222142 -4.2618589 -4.2900567 -4.3110404 -4.3210864 -4.3124089][-4.1785049 -4.1646247 -4.1478443 -4.120049 -4.0800223 -4.0360713 -4.0214663 -4.0549984 -4.1109862 -4.1713147 -4.2268648 -4.2691236 -4.2977982 -4.3112755 -4.3066287][-4.1904778 -4.179635 -4.164022 -4.1350732 -4.0919542 -4.04791 -4.0238519 -4.03745 -4.0773754 -4.1340184 -4.1908188 -4.2353811 -4.2698278 -4.2923274 -4.2973313][-4.2104959 -4.2061343 -4.1943612 -4.1718674 -4.1430874 -4.1161156 -4.0995908 -4.1055865 -4.1248765 -4.1587639 -4.1959319 -4.2302542 -4.2609191 -4.2850251 -4.292974][-4.2260532 -4.2269669 -4.2217932 -4.2089305 -4.1938047 -4.1784387 -4.1675205 -4.1727257 -4.1839476 -4.2058053 -4.2317514 -4.2557058 -4.2768407 -4.2938952 -4.2971172][-4.2384524 -4.2428565 -4.2416096 -4.2365484 -4.2306714 -4.2225628 -4.216032 -4.2195768 -4.2284222 -4.247704 -4.2713842 -4.2902784 -4.3040481 -4.313334 -4.3095551][-4.2464843 -4.2512097 -4.2521205 -4.2516065 -4.2500196 -4.2442322 -4.2383933 -4.2382469 -4.2476854 -4.2702875 -4.2918272 -4.3051887 -4.3154364 -4.3198118 -4.3144617][-4.2515745 -4.2543588 -4.2564421 -4.2582178 -4.2581277 -4.2521052 -4.2410574 -4.2348733 -4.2433095 -4.2661858 -4.2833614 -4.2955475 -4.3078146 -4.31191 -4.3086934][-4.2502403 -4.2504921 -4.2522035 -4.253366 -4.2509112 -4.2409177 -4.2237363 -4.2133408 -4.2194924 -4.239759 -4.2571869 -4.2712893 -4.2865524 -4.2940636 -4.297791][-4.2455306 -4.2451878 -4.245553 -4.245481 -4.2414885 -4.2297587 -4.2096844 -4.1999292 -4.2087564 -4.2314305 -4.2522883 -4.2644539 -4.2767181 -4.2847281 -4.291451][-4.2530808 -4.2553492 -4.2562442 -4.2563052 -4.253747 -4.2461958 -4.2304826 -4.2253695 -4.2377152 -4.2584677 -4.2747836 -4.2792821 -4.2872481 -4.2925735 -4.296237]]...]
INFO - root - 2017-12-05 16:19:26.809224: step 24010, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 74h:07m:55s remains)
INFO - root - 2017-12-05 16:19:35.278832: step 24020, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 72h:54m:05s remains)
INFO - root - 2017-12-05 16:19:43.837345: step 24030, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 73h:35m:01s remains)
INFO - root - 2017-12-05 16:19:52.252458: step 24040, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 71h:43m:25s remains)
INFO - root - 2017-12-05 16:20:00.883881: step 24050, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 73h:45m:52s remains)
INFO - root - 2017-12-05 16:20:09.256218: step 24060, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 70h:04m:17s remains)
INFO - root - 2017-12-05 16:20:17.885727: step 24070, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 73h:29m:39s remains)
INFO - root - 2017-12-05 16:20:26.258592: step 24080, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 70h:41m:12s remains)
INFO - root - 2017-12-05 16:20:34.625689: step 24090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:24m:17s remains)
INFO - root - 2017-12-05 16:20:43.238411: step 24100, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 73h:17m:42s remains)
2017-12-05 16:20:44.037414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2217879 -4.2235332 -4.2179704 -4.2007627 -4.1711597 -4.1346655 -4.1258483 -4.1285181 -4.1196866 -4.1186562 -4.1488667 -4.175447 -4.1721783 -4.1547365 -4.1454587][-4.2145262 -4.2223444 -4.2208285 -4.20241 -4.1685014 -4.1405606 -4.1384263 -4.1403184 -4.1287346 -4.1251802 -4.1555433 -4.1839118 -4.1832156 -4.162674 -4.1474104][-4.1991234 -4.2076321 -4.2088914 -4.1908841 -4.1561818 -4.1422663 -4.1448874 -4.1425967 -4.1299572 -4.1297765 -4.1559238 -4.1772418 -4.1792789 -4.1608682 -4.1464434][-4.1887574 -4.1966672 -4.1995077 -4.1794081 -4.1381087 -4.1276388 -4.1314392 -4.1195025 -4.0993743 -4.1094823 -4.1430922 -4.1663127 -4.1753 -4.1626024 -4.1511345][-4.1782675 -4.1899357 -4.1994996 -4.1790848 -4.1293988 -4.1123109 -4.1061816 -4.0735765 -4.0312638 -4.0532813 -4.1092005 -4.1475806 -4.170156 -4.1679931 -4.1585336][-4.1714706 -4.1894708 -4.2057242 -4.1840506 -4.127481 -4.1067071 -4.0889645 -4.0297484 -3.9513483 -3.9834981 -4.0661168 -4.1207151 -4.1595573 -4.1714215 -4.1612544][-4.17385 -4.193687 -4.2107954 -4.1881914 -4.1325445 -4.1091404 -4.0819449 -4.0003119 -3.8879728 -3.926034 -4.032773 -4.1021714 -4.1511636 -4.1710978 -4.1549454][-4.1889353 -4.2051 -4.2177949 -4.1959391 -4.1454468 -4.1139154 -4.074512 -3.9847653 -3.8679633 -3.9137819 -4.0336156 -4.1110144 -4.159462 -4.1758885 -4.1491652][-4.2161961 -4.2273455 -4.2359185 -4.2183223 -4.1761379 -4.1381392 -4.0914145 -4.0099382 -3.9176595 -3.9635713 -4.0735989 -4.1415286 -4.1834936 -4.1947646 -4.166347][-4.2531905 -4.2608471 -4.2629743 -4.2478604 -4.2117877 -4.1755052 -4.1338696 -4.0726595 -4.0100412 -4.0498571 -4.1397552 -4.1959763 -4.2266622 -4.2321334 -4.2089996][-4.2866249 -4.2893958 -4.2810721 -4.2665763 -4.239634 -4.21122 -4.1782074 -4.1349053 -4.098248 -4.1320481 -4.2004638 -4.241838 -4.2623787 -4.2666349 -4.2524323][-4.3036919 -4.3002415 -4.2849936 -4.2755532 -4.2600789 -4.2426043 -4.2228565 -4.1978984 -4.1824927 -4.2073507 -4.2498589 -4.268754 -4.276763 -4.2812386 -4.2772055][-4.3025451 -4.2956386 -4.2805572 -4.2766871 -4.2714357 -4.264286 -4.2577019 -4.2493539 -4.2468114 -4.263001 -4.2845087 -4.284801 -4.2793207 -4.2793808 -4.2800193][-4.2944636 -4.2869115 -4.2735796 -4.2708836 -4.2713704 -4.2705007 -4.2712297 -4.273046 -4.278986 -4.2904363 -4.300138 -4.2915659 -4.2753239 -4.2684531 -4.2683258][-4.2901568 -4.2833042 -4.2712731 -4.2661014 -4.2667542 -4.2689142 -4.2718878 -4.2772307 -4.2866812 -4.297718 -4.3031187 -4.2930622 -4.275701 -4.2659879 -4.2630806]]...]
INFO - root - 2017-12-05 16:20:52.652845: step 24110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 73h:56m:56s remains)
INFO - root - 2017-12-05 16:21:01.051025: step 24120, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 72h:22m:58s remains)
INFO - root - 2017-12-05 16:21:09.566618: step 24130, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 72h:53m:47s remains)
INFO - root - 2017-12-05 16:21:18.128567: step 24140, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 73h:49m:26s remains)
INFO - root - 2017-12-05 16:21:26.595304: step 24150, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 73h:32m:04s remains)
INFO - root - 2017-12-05 16:21:35.204664: step 24160, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 70h:27m:22s remains)
INFO - root - 2017-12-05 16:21:43.606230: step 24170, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 72h:51m:26s remains)
INFO - root - 2017-12-05 16:21:52.021464: step 24180, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 72h:27m:26s remains)
INFO - root - 2017-12-05 16:22:00.673911: step 24190, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 73h:12m:07s remains)
INFO - root - 2017-12-05 16:22:09.203684: step 24200, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 75h:03m:10s remains)
2017-12-05 16:22:10.003926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.216332 -4.2055492 -4.1961503 -4.1816773 -4.19331 -4.2241125 -4.2497392 -4.2550669 -4.2552104 -4.2579932 -4.262125 -4.2649179 -4.2477679 -4.2303762 -4.2099295][-4.1780653 -4.1572413 -4.1368971 -4.1170573 -4.1343203 -4.1769671 -4.2030396 -4.2124839 -4.2190161 -4.2287083 -4.2362118 -4.2328343 -4.20841 -4.1855106 -4.1556334][-4.1496577 -4.117599 -4.0833187 -4.05692 -4.080483 -4.1309824 -4.1538053 -4.1662135 -4.1837878 -4.2025485 -4.2130957 -4.2027512 -4.1748996 -4.14679 -4.1059942][-4.1396289 -4.097311 -4.05026 -4.0215125 -4.0404949 -4.0813122 -4.0968642 -4.114851 -4.14952 -4.17709 -4.1893525 -4.1764731 -4.1474161 -4.1147857 -4.0694327][-4.1533079 -4.1066575 -4.0539427 -4.0237112 -4.026176 -4.0385904 -4.0419669 -4.067616 -4.1156573 -4.1510243 -4.1661568 -4.1553669 -4.1343803 -4.1064591 -4.0653253][-4.1793938 -4.132165 -4.0778184 -4.043232 -4.0351906 -4.0238719 -4.0123277 -4.0282178 -4.068676 -4.1182261 -4.1578903 -4.1665177 -4.16103 -4.1390505 -4.102109][-4.1905727 -4.144434 -4.0911407 -4.0528417 -4.0372066 -4.0103192 -3.9807079 -3.9522727 -3.9604692 -4.0368948 -4.1231122 -4.1640687 -4.178936 -4.1720977 -4.1477518][-4.1851788 -4.1439419 -4.0967312 -4.0590658 -4.0397024 -4.0053868 -3.9383061 -3.8254654 -3.7798004 -3.902885 -4.0415759 -4.1148772 -4.1487794 -4.1722951 -4.1772161][-4.1502337 -4.1166987 -4.0812473 -4.0493221 -4.0340047 -4.0046573 -3.9092774 -3.7325401 -3.6455593 -3.8014739 -3.9598823 -4.0464206 -4.098506 -4.154182 -4.1866956][-4.0907869 -4.064405 -4.0462327 -4.0331759 -4.0327682 -4.0171113 -3.9316182 -3.7660265 -3.6877515 -3.8079381 -3.9284732 -4.0017376 -4.0599608 -4.1316414 -4.1782188][-4.0601907 -4.0367122 -4.0350547 -4.0471668 -4.06424 -4.0668678 -4.0172763 -3.9021857 -3.8500738 -3.9007392 -3.9587357 -4.0102525 -4.0617762 -4.1290207 -4.1708889][-4.0839729 -4.05929 -4.062984 -4.0875592 -4.1134281 -4.13007 -4.1142044 -4.056149 -4.0220633 -4.0237775 -4.0388532 -4.0717812 -4.1142349 -4.1626167 -4.18633][-4.1518049 -4.1272888 -4.1301517 -4.156785 -4.1814537 -4.2035441 -4.2118082 -4.1930032 -4.1680417 -4.1510897 -4.1488423 -4.1690154 -4.1940832 -4.2220082 -4.2347178][-4.2335777 -4.218739 -4.2221971 -4.2431355 -4.2612395 -4.2794504 -4.2936397 -4.2911367 -4.2770348 -4.2617226 -4.2557106 -4.2608805 -4.2663169 -4.2765455 -4.2811193][-4.2889357 -4.2829337 -4.2854514 -4.2987885 -4.309166 -4.3201222 -4.3313966 -4.3317423 -4.3250189 -4.3174019 -4.313108 -4.3113923 -4.3095326 -4.3102245 -4.312006]]...]
INFO - root - 2017-12-05 16:22:18.516822: step 24210, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 71h:11m:36s remains)
INFO - root - 2017-12-05 16:22:27.121739: step 24220, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 74h:54m:16s remains)
INFO - root - 2017-12-05 16:22:35.759056: step 24230, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:39m:29s remains)
INFO - root - 2017-12-05 16:22:44.193229: step 24240, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.838 sec/batch; 71h:43m:13s remains)
INFO - root - 2017-12-05 16:22:52.813717: step 24250, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.906 sec/batch; 77h:32m:02s remains)
INFO - root - 2017-12-05 16:23:01.326933: step 24260, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 72h:18m:36s remains)
INFO - root - 2017-12-05 16:23:09.825006: step 24270, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 72h:45m:05s remains)
INFO - root - 2017-12-05 16:23:18.237638: step 24280, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.847 sec/batch; 72h:28m:43s remains)
INFO - root - 2017-12-05 16:23:26.703247: step 24290, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 70h:51m:48s remains)
INFO - root - 2017-12-05 16:23:35.245714: step 24300, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 73h:33m:26s remains)
2017-12-05 16:23:36.007030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2611275 -4.239306 -4.2137394 -4.1850958 -4.1438723 -4.100966 -4.0765452 -4.092453 -4.1204395 -4.1506267 -4.1735272 -4.1779957 -4.1654577 -4.1428423 -4.1337037][-4.2652779 -4.24749 -4.2257586 -4.1980672 -4.1571217 -4.1162839 -4.0970097 -4.1204619 -4.1510534 -4.1810117 -4.1991534 -4.199544 -4.1852384 -4.1681585 -4.1602707][-4.265161 -4.2497597 -4.2326283 -4.2121181 -4.1798596 -4.1488466 -4.1369925 -4.1637926 -4.1853824 -4.2001629 -4.2054911 -4.1989837 -4.1861453 -4.1825414 -4.1866269][-4.2566223 -4.2374382 -4.2208853 -4.205483 -4.1802673 -4.1616573 -4.1622257 -4.1900959 -4.2040958 -4.2033415 -4.1925888 -4.1784358 -4.1712136 -4.1848803 -4.201056][-4.2411509 -4.2133336 -4.189332 -4.1723013 -4.1502671 -4.1399765 -4.1507072 -4.1819921 -4.1925054 -4.1832328 -4.1602197 -4.1386352 -4.1368585 -4.1641006 -4.1903424][-4.2295327 -4.1899705 -4.1562352 -4.1325679 -4.1058235 -4.0956526 -4.1077838 -4.1419473 -4.1544056 -4.1423059 -4.1175752 -4.0954 -4.0922079 -4.1133418 -4.13318][-4.224227 -4.178606 -4.1375704 -4.1082411 -4.074132 -4.0516424 -4.0486193 -4.0779057 -4.0971909 -4.0933456 -4.0843177 -4.0738964 -4.0662613 -4.061738 -4.0526962][-4.2263317 -4.1776147 -4.1334462 -4.0983806 -4.0536637 -4.0113921 -3.9827912 -3.999244 -4.0271873 -4.0409794 -4.0553427 -4.0651045 -4.065846 -4.0523176 -4.0269094][-4.231472 -4.1814723 -4.1346617 -4.098825 -4.05448 -4.0020909 -3.9562016 -3.9548604 -3.9781432 -4.0038552 -4.0380549 -4.0712051 -4.0881228 -4.0874782 -4.0714197][-4.238657 -4.1956372 -4.1570859 -4.1316113 -4.0985732 -4.0518656 -4.00633 -4.0011997 -4.0207815 -4.0435128 -4.079124 -4.1132736 -4.1379542 -4.1474738 -4.1423788][-4.2498689 -4.219594 -4.1945496 -4.1811776 -4.1611214 -4.1285357 -4.0964847 -4.0959411 -4.1136618 -4.1318669 -4.1594367 -4.1826825 -4.203176 -4.2155781 -4.217185][-4.255815 -4.2349024 -4.221529 -4.2196732 -4.2121835 -4.1935225 -4.1744227 -4.1761227 -4.1883364 -4.203094 -4.22301 -4.2368026 -4.251091 -4.2618036 -4.2653437][-4.2633686 -4.2472649 -4.23983 -4.2441988 -4.2444153 -4.2365909 -4.2267814 -4.2308888 -4.238924 -4.2487373 -4.2634096 -4.273108 -4.2801738 -4.28535 -4.2890081][-4.2723389 -4.2587986 -4.251543 -4.2558727 -4.2591724 -4.2573504 -4.2550359 -4.2614503 -4.2694526 -4.27707 -4.2895813 -4.2969618 -4.2991581 -4.299922 -4.3005142][-4.2795033 -4.2688613 -4.261662 -4.2626753 -4.2648048 -4.2641988 -4.2634659 -4.26999 -4.2793632 -4.2872987 -4.2993331 -4.3058333 -4.3063488 -4.3045435 -4.3011966]]...]
INFO - root - 2017-12-05 16:23:44.580947: step 24310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 73h:07m:01s remains)
INFO - root - 2017-12-05 16:23:53.112773: step 24320, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 73h:34m:10s remains)
INFO - root - 2017-12-05 16:24:01.689678: step 24330, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 74h:42m:03s remains)
INFO - root - 2017-12-05 16:24:10.200224: step 24340, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 71h:16m:49s remains)
INFO - root - 2017-12-05 16:24:18.825261: step 24350, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 71h:54m:53s remains)
INFO - root - 2017-12-05 16:24:27.375687: step 24360, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 72h:19m:46s remains)
INFO - root - 2017-12-05 16:24:35.830327: step 24370, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:31m:14s remains)
INFO - root - 2017-12-05 16:24:44.154068: step 24380, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 73h:50m:29s remains)
INFO - root - 2017-12-05 16:24:52.688556: step 24390, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.865 sec/batch; 74h:00m:18s remains)
INFO - root - 2017-12-05 16:25:01.213037: step 24400, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 73h:45m:39s remains)
2017-12-05 16:25:01.943662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.284193 -4.2863908 -4.302494 -4.321384 -4.33467 -4.3400211 -4.3377814 -4.3304634 -4.3279924 -4.3245716 -4.3210297 -4.3185797 -4.3126988 -4.3010812 -4.2908144][-4.23196 -4.2334156 -4.2553921 -4.2815189 -4.2979827 -4.3047748 -4.3024044 -4.2926655 -4.2913365 -4.291151 -4.2905374 -4.2896624 -4.2820892 -4.2637777 -4.2448912][-4.1920872 -4.1963744 -4.2266278 -4.2565193 -4.2723246 -4.2786126 -4.2770805 -4.2671218 -4.26709 -4.2694259 -4.2694778 -4.2657037 -4.2553053 -4.2281785 -4.1990681][-4.189229 -4.1935644 -4.227212 -4.2536645 -4.2628927 -4.2631125 -4.2600193 -4.2534633 -4.2556119 -4.2613921 -4.2625947 -4.2571626 -4.2466187 -4.2176075 -4.1837106][-4.2063522 -4.2045803 -4.2317319 -4.248383 -4.2467117 -4.2343831 -4.2233772 -4.2219238 -4.2296329 -4.2424421 -4.2482777 -4.24766 -4.2417159 -4.2191825 -4.1883273][-4.2183671 -4.2080226 -4.2205534 -4.2205296 -4.2012863 -4.1677713 -4.1425357 -4.1399312 -4.1548362 -4.1785088 -4.1966243 -4.2114234 -4.2195354 -4.2094626 -4.187705][-4.2171521 -4.1940246 -4.1842275 -4.1610341 -4.1190228 -4.0566797 -4.0050144 -3.9891987 -4.0149984 -4.0590119 -4.105597 -4.1503768 -4.1833105 -4.1933751 -4.1872354][-4.208416 -4.1758294 -4.1449261 -4.0998783 -4.0367823 -3.9480143 -3.8679452 -3.8262548 -3.8588614 -3.9284883 -4.0047593 -4.0799966 -4.1375737 -4.1690469 -4.1812892][-4.2106347 -4.181653 -4.1519327 -4.1130447 -4.0575066 -3.9796503 -3.9103012 -3.8722944 -3.8941932 -3.9539487 -4.0218682 -4.089819 -4.1429949 -4.1706619 -4.1814427][-4.1873083 -4.1686831 -4.1588922 -4.147646 -4.1197958 -4.0778513 -4.04294 -4.024622 -4.0362434 -4.0759659 -4.1199503 -4.1608853 -4.1867251 -4.1889153 -4.1790223][-4.1486025 -4.1417341 -4.1569304 -4.1786609 -4.1807022 -4.1690111 -4.1568313 -4.148366 -4.1537275 -4.1750031 -4.1957259 -4.2107272 -4.2130256 -4.1961412 -4.1713266][-4.1177912 -4.1193824 -4.1532283 -4.1977267 -4.2232227 -4.2340369 -4.2364225 -4.2342806 -4.2333474 -4.2371278 -4.2375507 -4.2308192 -4.2133188 -4.1830192 -4.1537528][-4.1227288 -4.1319547 -4.1703873 -4.218719 -4.2533917 -4.2755265 -4.2819304 -4.2806535 -4.27537 -4.2679095 -4.2568517 -4.2373934 -4.2088685 -4.1752949 -4.1471581][-4.1910291 -4.2042994 -4.2353177 -4.270175 -4.2943792 -4.3101358 -4.312438 -4.3076496 -4.3015475 -4.2921972 -4.2797279 -4.2611151 -4.2358513 -4.2097468 -4.19133][-4.2838321 -4.29533 -4.31437 -4.3331432 -4.3430977 -4.3464794 -4.3416905 -4.3356428 -4.3307657 -4.324409 -4.3172436 -4.3073506 -4.2921333 -4.2776327 -4.2709389]]...]
INFO - root - 2017-12-05 16:25:10.455679: step 24410, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 74h:17m:26s remains)
INFO - root - 2017-12-05 16:25:18.943239: step 24420, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 72h:26m:11s remains)
INFO - root - 2017-12-05 16:25:27.403234: step 24430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:38m:53s remains)
INFO - root - 2017-12-05 16:25:35.874524: step 24440, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 72h:40m:14s remains)
INFO - root - 2017-12-05 16:25:44.339561: step 24450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 72h:03m:01s remains)
INFO - root - 2017-12-05 16:25:52.849770: step 24460, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 75h:15m:42s remains)
INFO - root - 2017-12-05 16:26:01.363866: step 24470, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 72h:19m:12s remains)
INFO - root - 2017-12-05 16:26:09.819635: step 24480, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 74h:58m:21s remains)
INFO - root - 2017-12-05 16:26:18.211118: step 24490, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 72h:04m:39s remains)
INFO - root - 2017-12-05 16:26:26.759255: step 24500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 73h:20m:21s remains)
2017-12-05 16:26:27.587007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2272487 -4.2274995 -4.2393694 -4.2550182 -4.2618623 -4.2558575 -4.25105 -4.2598581 -4.2743497 -4.2802248 -4.2804809 -4.2858324 -4.2981524 -4.3032432 -4.3099041][-4.20201 -4.2042222 -4.21833 -4.23824 -4.2490492 -4.2435436 -4.237586 -4.244616 -4.2612209 -4.2705355 -4.2709513 -4.2738428 -4.2830606 -4.2869263 -4.2883177][-4.1731348 -4.17878 -4.1937647 -4.2195697 -4.236064 -4.2334619 -4.223506 -4.223845 -4.239203 -4.2524447 -4.2560124 -4.2562451 -4.2642765 -4.2684946 -4.2673497][-4.15442 -4.1625504 -4.1801686 -4.207058 -4.22483 -4.2213354 -4.2067189 -4.198699 -4.2124724 -4.2301078 -4.2405362 -4.244215 -4.2550869 -4.262939 -4.2611804][-4.1564641 -4.1652184 -4.184299 -4.205698 -4.2152896 -4.2018409 -4.1725955 -4.157208 -4.1758866 -4.2027826 -4.2248421 -4.2387609 -4.2524867 -4.2640619 -4.2667284][-4.1591544 -4.1654119 -4.1821151 -4.1966677 -4.1953917 -4.161984 -4.10482 -4.0795264 -4.1118531 -4.1561279 -4.19252 -4.2194295 -4.2377706 -4.25131 -4.2603559][-4.1732125 -4.1802697 -4.1896868 -4.1911869 -4.1703887 -4.1057415 -4.0126634 -3.9733717 -4.0235591 -4.0918078 -4.1439128 -4.1802058 -4.2055607 -4.2269249 -4.2479839][-4.1834922 -4.1948228 -4.1999726 -4.1886759 -4.1550226 -4.078382 -3.975076 -3.9285982 -3.976387 -4.0517683 -4.109406 -4.14984 -4.1796966 -4.2078166 -4.2388725][-4.1812925 -4.1943636 -4.2010384 -4.1898522 -4.157536 -4.0968761 -4.0273385 -3.9995756 -4.0269446 -4.0771842 -4.1232004 -4.1573348 -4.1818109 -4.20542 -4.2361388][-4.184175 -4.1971769 -4.2025738 -4.1944695 -4.1732883 -4.1429019 -4.114502 -4.1105185 -4.127387 -4.1500297 -4.1795907 -4.2000504 -4.2096539 -4.2185268 -4.23805][-4.2038417 -4.2123179 -4.212162 -4.2068748 -4.1941404 -4.1858191 -4.1867132 -4.2020946 -4.2169886 -4.2226787 -4.2350574 -4.2421746 -4.2378578 -4.2339091 -4.2434864][-4.23443 -4.237031 -4.2329359 -4.2280812 -4.2198296 -4.2228804 -4.2380247 -4.2607341 -4.27662 -4.2735982 -4.2677956 -4.2621675 -4.2529511 -4.2445946 -4.2484908][-4.2608337 -4.2602863 -4.2566395 -4.2503953 -4.2410502 -4.2461419 -4.26342 -4.2823787 -4.2936687 -4.2879581 -4.273355 -4.2605953 -4.2535381 -4.2486372 -4.2515073][-4.2739825 -4.2739072 -4.27153 -4.2616735 -4.2489624 -4.2518377 -4.2664733 -4.2781835 -4.2840924 -4.2813444 -4.2694283 -4.2578354 -4.2522621 -4.2498255 -4.2521324][-4.27268 -4.2707748 -4.2717638 -4.2664037 -4.25648 -4.2566342 -4.2665763 -4.2728381 -4.2726431 -4.2706804 -4.262949 -4.2545948 -4.2521815 -4.2528248 -4.2554317]]...]
INFO - root - 2017-12-05 16:26:36.111064: step 24510, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 73h:23m:21s remains)
INFO - root - 2017-12-05 16:26:44.613611: step 24520, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 71h:57m:27s remains)
INFO - root - 2017-12-05 16:26:53.172457: step 24530, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.865 sec/batch; 73h:58m:08s remains)
INFO - root - 2017-12-05 16:27:01.670402: step 24540, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 72h:32m:35s remains)
INFO - root - 2017-12-05 16:27:10.202452: step 24550, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 70h:27m:51s remains)
INFO - root - 2017-12-05 16:27:18.901316: step 24560, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 75h:26m:02s remains)
INFO - root - 2017-12-05 16:27:27.441079: step 24570, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 71h:14m:48s remains)
INFO - root - 2017-12-05 16:27:35.938748: step 24580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 73h:43m:43s remains)
INFO - root - 2017-12-05 16:27:44.387480: step 24590, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 73h:52m:33s remains)
INFO - root - 2017-12-05 16:27:52.854044: step 24600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:34m:51s remains)
2017-12-05 16:27:53.640078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2942381 -4.2757421 -4.260447 -4.2478647 -4.2431216 -4.2481575 -4.2577109 -4.2697182 -4.2846003 -4.2996826 -4.3051705 -4.3019609 -4.2945676 -4.2854466 -4.283823][-4.27376 -4.2458668 -4.2201281 -4.1960526 -4.1830378 -4.1863408 -4.1967983 -4.2110987 -4.2346683 -4.2600369 -4.2750425 -4.2781844 -4.2726989 -4.2595506 -4.2502561][-4.2546968 -4.2171741 -4.1792951 -4.1398544 -4.1167641 -4.1143284 -4.120121 -4.13356 -4.1658068 -4.2049875 -4.2316408 -4.2447295 -4.2435341 -4.2307549 -4.2172][-4.2377863 -4.19193 -4.1405716 -4.0837393 -4.0498376 -4.0402665 -4.0347667 -4.0402269 -4.0815358 -4.135107 -4.172153 -4.1943812 -4.2027655 -4.1978884 -4.1856384][-4.229527 -4.1794233 -4.1189013 -4.051383 -4.0091877 -3.9898179 -3.96475 -3.9492345 -3.9885116 -4.0543528 -4.1060982 -4.1359143 -4.1557779 -4.1645117 -4.1609306][-4.2305064 -4.1828561 -4.1198411 -4.048171 -3.9973152 -3.9587374 -3.9038072 -3.8586724 -3.8950257 -3.9748588 -4.0414925 -4.076149 -4.099391 -4.1172972 -4.1272068][-4.2351227 -4.1921191 -4.1330009 -4.0603876 -4.0005178 -3.9365497 -3.8468056 -3.7712326 -3.8112376 -3.908834 -3.9812956 -4.0094166 -4.0179877 -4.0424366 -4.0755148][-4.240562 -4.2035728 -4.1491747 -4.0790262 -4.0154095 -3.9378743 -3.8334994 -3.756664 -3.8008344 -3.9027183 -3.9625804 -3.9654303 -3.9476063 -3.9668531 -4.0188456][-4.2473722 -4.2171884 -4.1712866 -4.1129518 -4.059804 -3.9926274 -3.9097874 -3.8636904 -3.9005146 -3.9732242 -3.9975748 -3.9649963 -3.9191766 -3.9214864 -3.9680462][-4.2507229 -4.2252655 -4.1900187 -4.1493635 -4.11491 -4.06805 -4.0131478 -3.9895537 -4.0062923 -4.0472441 -4.052217 -4.0048695 -3.9475336 -3.9305637 -3.9578032][-4.2512069 -4.2273126 -4.2019992 -4.1772165 -4.157939 -4.127964 -4.0932555 -4.0764184 -4.0729365 -4.0935278 -4.0989828 -4.0672584 -4.0260463 -4.0058475 -4.0137568][-4.2500038 -4.2233415 -4.2013497 -4.1842484 -4.1772108 -4.1637983 -4.1471281 -4.1361804 -4.1251125 -4.1369715 -4.1474433 -4.1391816 -4.1227865 -4.109005 -4.1034255][-4.2512016 -4.2200513 -4.1957006 -4.180707 -4.1827703 -4.1833858 -4.1807446 -4.177495 -4.1684 -4.1785679 -4.1884508 -4.1921616 -4.192194 -4.1843395 -4.1710443][-4.2592177 -4.2253771 -4.1986032 -4.183939 -4.1881995 -4.1952972 -4.1997037 -4.2003345 -4.1985774 -4.208962 -4.2146053 -4.219286 -4.2260704 -4.2253609 -4.2159266][-4.2723932 -4.2399244 -4.2138023 -4.2009139 -4.2046328 -4.2121081 -4.2181625 -4.22181 -4.2270412 -4.2383766 -4.2412796 -4.2430744 -4.249783 -4.2520871 -4.2490144]]...]
INFO - root - 2017-12-05 16:28:02.284302: step 24610, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 74h:20m:29s remains)
INFO - root - 2017-12-05 16:28:10.811695: step 24620, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 72h:01m:01s remains)
INFO - root - 2017-12-05 16:28:19.347927: step 24630, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 73h:54m:23s remains)
INFO - root - 2017-12-05 16:28:27.829083: step 24640, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 73h:33m:10s remains)
INFO - root - 2017-12-05 16:28:36.342205: step 24650, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 73h:58m:59s remains)
INFO - root - 2017-12-05 16:28:44.977923: step 24660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 73h:41m:06s remains)
INFO - root - 2017-12-05 16:28:53.508354: step 24670, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 73h:13m:11s remains)
INFO - root - 2017-12-05 16:29:01.984293: step 24680, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 72h:46m:11s remains)
INFO - root - 2017-12-05 16:29:10.528616: step 24690, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 74h:19m:48s remains)
INFO - root - 2017-12-05 16:29:18.820065: step 24700, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 73h:42m:03s remains)
2017-12-05 16:29:19.523753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2819281 -4.2888632 -4.2897787 -4.2753911 -4.2546105 -4.2306957 -4.207067 -4.1970086 -4.2076979 -4.2225156 -4.2340474 -4.2380514 -4.2266569 -4.2064409 -4.1834826][-4.2878127 -4.2933969 -4.2951589 -4.282692 -4.2648625 -4.2452931 -4.2244205 -4.2155738 -4.2223425 -4.2329192 -4.242723 -4.2469573 -4.2361283 -4.2169313 -4.1972823][-4.281291 -4.2844882 -4.2859817 -4.2770452 -4.2651854 -4.2507148 -4.2324829 -4.2252183 -4.22879 -4.2358222 -4.2454166 -4.2537518 -4.2488489 -4.2346644 -4.2207117][-4.2685752 -4.2688479 -4.2690334 -4.2628088 -4.2565393 -4.2437429 -4.2254419 -4.2184625 -4.2204967 -4.227139 -4.2401657 -4.2557383 -4.2607546 -4.2559247 -4.2478929][-4.2535114 -4.2509856 -4.248579 -4.2417765 -4.2373857 -4.2235203 -4.2024045 -4.1951823 -4.1992612 -4.2096767 -4.2294521 -4.2542214 -4.2707715 -4.2767529 -4.2727733][-4.2340131 -4.22826 -4.2236209 -4.2141385 -4.2074757 -4.1890821 -4.1609893 -4.1500425 -4.1575656 -4.1741586 -4.2023568 -4.2348638 -4.2579937 -4.2705398 -4.2675428][-4.20202 -4.1879997 -4.1767344 -4.1604104 -4.1466236 -4.11952 -4.0806227 -4.0643005 -4.0818682 -4.1112771 -4.1501751 -4.1889753 -4.2135391 -4.2272711 -4.2224307][-4.1787477 -4.1599383 -4.1429515 -4.1186876 -4.0944257 -4.0566158 -4.006927 -3.9864898 -4.0172644 -4.0599985 -4.1066732 -4.1481981 -4.1706967 -4.1816597 -4.1748033][-4.19301 -4.1808496 -4.167254 -4.146452 -4.1260443 -4.0941625 -4.0555959 -4.0443435 -4.0753589 -4.1126394 -4.1499124 -4.1808543 -4.1944237 -4.1986918 -4.19074][-4.2272925 -4.2242584 -4.2173476 -4.2046566 -4.1915874 -4.1710176 -4.1497335 -4.1500168 -4.1748867 -4.1998734 -4.2228913 -4.2399054 -4.2443528 -4.24306 -4.2351561][-4.2582922 -4.2618375 -4.2609749 -4.2546005 -4.2465534 -4.2339644 -4.2248383 -4.2314525 -4.2485852 -4.2620888 -4.2733517 -4.2798438 -4.2770166 -4.2713118 -4.264051][-4.285008 -4.2903571 -4.2913394 -4.28765 -4.2829375 -4.2759509 -4.2737346 -4.2811408 -4.2913165 -4.2976594 -4.3025017 -4.3039174 -4.299006 -4.2927361 -4.2867122][-4.3045278 -4.3087783 -4.3111548 -4.3101344 -4.3093591 -4.3067641 -4.3069639 -4.3112683 -4.315248 -4.3169928 -4.3183465 -4.3182669 -4.3145576 -4.3104482 -4.3070383][-4.3112774 -4.3136749 -4.3170228 -4.318768 -4.3200917 -4.3197026 -4.3200684 -4.321641 -4.3221841 -4.3214245 -4.3208537 -4.3199196 -4.3178539 -4.315918 -4.3146529][-4.3106279 -4.3102632 -4.3120656 -4.3141942 -4.3159623 -4.3165407 -4.3170595 -4.3175073 -4.3174 -4.3169665 -4.3167949 -4.3164692 -4.3157449 -4.31496 -4.3144946]]...]
INFO - root - 2017-12-05 16:29:27.988226: step 24710, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 73h:11m:54s remains)
INFO - root - 2017-12-05 16:29:36.567794: step 24720, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 73h:59m:06s remains)
INFO - root - 2017-12-05 16:29:45.196216: step 24730, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 74h:03m:16s remains)
INFO - root - 2017-12-05 16:29:53.723082: step 24740, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:16m:22s remains)
INFO - root - 2017-12-05 16:30:02.194263: step 24750, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 71h:05m:00s remains)
INFO - root - 2017-12-05 16:30:10.831054: step 24760, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.872 sec/batch; 74h:30m:42s remains)
INFO - root - 2017-12-05 16:30:19.361555: step 24770, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 72h:42m:53s remains)
INFO - root - 2017-12-05 16:30:27.823935: step 24780, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 73h:26m:31s remains)
INFO - root - 2017-12-05 16:30:36.341701: step 24790, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 73h:05m:50s remains)
INFO - root - 2017-12-05 16:30:44.893548: step 24800, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 75h:43m:15s remains)
2017-12-05 16:30:45.621215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2431841 -4.2332745 -4.2248645 -4.223556 -4.2248015 -4.2237606 -4.2213397 -4.2230539 -4.2244825 -4.2226152 -4.2233067 -4.2278986 -4.2304339 -4.2262268 -4.2214203][-4.2161551 -4.2036672 -4.1913333 -4.1893954 -4.1920896 -4.1914806 -4.1904974 -4.196125 -4.2022171 -4.2022438 -4.2016068 -4.2041512 -4.2063465 -4.2031345 -4.1996112][-4.1952853 -4.1853466 -4.1716824 -4.1677566 -4.1687956 -4.1654425 -4.1645756 -4.1733894 -4.1819029 -4.1842937 -4.1825557 -4.1837492 -4.1844673 -4.1807289 -4.17859][-4.1883473 -4.1835828 -4.1699004 -4.1598921 -4.1526785 -4.1423149 -4.1385641 -4.1465125 -4.1551714 -4.1593533 -4.1608906 -4.1622496 -4.1603875 -4.1544137 -4.1479235][-4.1935 -4.1895108 -4.1749668 -4.1565104 -4.1411066 -4.1274524 -4.1239567 -4.1295695 -4.1355519 -4.1400762 -4.1433144 -4.1421237 -4.1369543 -4.1302452 -4.1225119][-4.1867042 -4.1801405 -4.1673789 -4.1440673 -4.1226392 -4.1065984 -4.1064296 -4.1137314 -4.1191087 -4.1255455 -4.1313386 -4.12501 -4.1139059 -4.1107292 -4.1111465][-4.1764784 -4.172091 -4.1619372 -4.1315584 -4.0978646 -4.0723948 -4.07415 -4.0878587 -4.1002269 -4.1148438 -4.1176476 -4.1037946 -4.084559 -4.0843258 -4.0946856][-4.1797094 -4.1802063 -4.1674981 -4.1308379 -4.0853248 -4.0529933 -4.0538554 -4.0712123 -4.08977 -4.1068707 -4.1047525 -4.0820522 -4.0564828 -4.0600033 -4.0749664][-4.1808872 -4.1874247 -4.1688814 -4.1270051 -4.0778937 -4.0457492 -4.0490813 -4.0666156 -4.0811448 -4.0938144 -4.088058 -4.06632 -4.0476933 -4.0600877 -4.0785174][-4.1761427 -4.1822615 -4.156755 -4.1161137 -4.0740795 -4.0471048 -4.0531435 -4.0678644 -4.0760069 -4.0870781 -4.0888262 -4.0759525 -4.0618834 -4.0773005 -4.094265][-4.1754375 -4.1806297 -4.15288 -4.1165037 -4.0852551 -4.0647764 -4.0715394 -4.076673 -4.07592 -4.0884113 -4.0999746 -4.0906062 -4.0735869 -4.0865445 -4.1040821][-4.1610703 -4.1675196 -4.144856 -4.1123323 -4.0898623 -4.08047 -4.0917082 -4.0892377 -4.0789609 -4.0893445 -4.105814 -4.1017814 -4.0856466 -4.0965433 -4.1170788][-4.1320634 -4.1413455 -4.1266141 -4.1017327 -4.0889592 -4.0940051 -4.1148562 -4.113091 -4.0950179 -4.0929275 -4.1056924 -4.1082854 -4.1027145 -4.1153469 -4.133738][-4.1276789 -4.1361651 -4.1258144 -4.10433 -4.0966763 -4.1102643 -4.1372175 -4.141048 -4.1219177 -4.1064854 -4.1094451 -4.1204534 -4.1308432 -4.1487083 -4.1591969][-4.1561637 -4.1661067 -4.1524949 -4.1242485 -4.1131444 -4.1268873 -4.1566129 -4.1678276 -4.1514988 -4.1273003 -4.1216288 -4.13449 -4.1553645 -4.1788459 -4.1865172]]...]
INFO - root - 2017-12-05 16:30:54.137200: step 24810, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 74h:21m:01s remains)
INFO - root - 2017-12-05 16:31:02.579513: step 24820, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:13m:21s remains)
INFO - root - 2017-12-05 16:31:11.246978: step 24830, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 74h:36m:14s remains)
INFO - root - 2017-12-05 16:31:19.696677: step 24840, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 72h:21m:39s remains)
INFO - root - 2017-12-05 16:31:28.237206: step 24850, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 73h:29m:03s remains)
INFO - root - 2017-12-05 16:31:36.782557: step 24860, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:33m:41s remains)
INFO - root - 2017-12-05 16:31:45.238984: step 24870, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 73h:16m:36s remains)
INFO - root - 2017-12-05 16:31:53.799845: step 24880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 74h:40m:20s remains)
INFO - root - 2017-12-05 16:32:02.298288: step 24890, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 72h:57m:28s remains)
INFO - root - 2017-12-05 16:32:10.780427: step 24900, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 70h:23m:52s remains)
2017-12-05 16:32:11.553901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2708125 -4.2693243 -4.2531848 -4.231338 -4.2155585 -4.2072592 -4.2003183 -4.2004223 -4.1983452 -4.1942387 -4.1960006 -4.20546 -4.2202535 -4.2386713 -4.2543406][-4.2608523 -4.257514 -4.2360229 -4.2109385 -4.1982617 -4.192205 -4.1868644 -4.1876006 -4.1850886 -4.1765933 -4.1732249 -4.186851 -4.2083411 -4.2288394 -4.2430887][-4.2491989 -4.2454748 -4.2212844 -4.1935453 -4.1818142 -4.1766825 -4.1741762 -4.1777792 -4.178484 -4.1718316 -4.1676335 -4.183332 -4.2071276 -4.22187 -4.2290411][-4.2516723 -4.2435913 -4.2150855 -4.1811504 -4.1627049 -4.1549735 -4.1556516 -4.1667438 -4.17741 -4.178493 -4.1797638 -4.1978717 -4.2183018 -4.2258563 -4.2230248][-4.26593 -4.2504597 -4.2172904 -4.1754193 -4.1423016 -4.1238041 -4.1197653 -4.1316862 -4.1543007 -4.1734414 -4.1858306 -4.2047834 -4.2220483 -4.2246952 -4.2195549][-4.2780085 -4.2604365 -4.2283363 -4.186336 -4.1421752 -4.1094222 -4.0954142 -4.10133 -4.1271725 -4.1563106 -4.1781516 -4.197031 -4.2097507 -4.206883 -4.2047558][-4.2827477 -4.2677217 -4.2412553 -4.2051244 -4.1609592 -4.1161962 -4.0858417 -4.07456 -4.0918417 -4.1269875 -4.1600685 -4.1838055 -4.1967158 -4.1918263 -4.1931477][-4.2749896 -4.2624159 -4.241858 -4.2153654 -4.1751008 -4.1211452 -4.0711513 -4.0377383 -4.0430441 -4.0855613 -4.1340504 -4.1733465 -4.1936059 -4.1884589 -4.1887355][-4.26063 -4.2463031 -4.224812 -4.2045035 -4.1676145 -4.110414 -4.0531564 -4.0101571 -4.009727 -4.0528269 -4.1117573 -4.16672 -4.1981182 -4.1972623 -4.1945095][-4.2374563 -4.221045 -4.1994958 -4.1865935 -4.162466 -4.119669 -4.0740461 -4.0362186 -4.0334821 -4.0644212 -4.1168737 -4.1748638 -4.2102976 -4.2113481 -4.2063842][-4.2266145 -4.2132587 -4.1946421 -4.1870551 -4.1767578 -4.1525679 -4.122664 -4.0992866 -4.0981221 -4.118124 -4.1578488 -4.206634 -4.2361274 -4.2333045 -4.2183528][-4.2252178 -4.2188954 -4.2072721 -4.1992397 -4.19138 -4.1768203 -4.1607065 -4.1545029 -4.1606941 -4.1795564 -4.212749 -4.2519879 -4.2690811 -4.2573547 -4.2333474][-4.2316637 -4.2333336 -4.2293143 -4.2209616 -4.2091055 -4.1962438 -4.1897783 -4.1959691 -4.2108464 -4.2347984 -4.2650886 -4.2934365 -4.2996116 -4.2826824 -4.2547603][-4.246563 -4.2502103 -4.2493424 -4.243742 -4.2344327 -4.2261982 -4.2243342 -4.2336884 -4.2514892 -4.2752495 -4.2970057 -4.3120718 -4.3117151 -4.295392 -4.2728276][-4.2724147 -4.2739534 -4.2704449 -4.2651987 -4.2580824 -4.2525854 -4.2523689 -4.2611856 -4.2775111 -4.2943487 -4.3059444 -4.3099079 -4.3046031 -4.2925348 -4.2808018]]...]
INFO - root - 2017-12-05 16:32:19.979481: step 24910, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 72h:09m:35s remains)
INFO - root - 2017-12-05 16:32:28.412612: step 24920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 72h:00m:12s remains)
INFO - root - 2017-12-05 16:32:37.063499: step 24930, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 74h:41m:31s remains)
INFO - root - 2017-12-05 16:32:45.577025: step 24940, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 71h:35m:31s remains)
INFO - root - 2017-12-05 16:32:54.074641: step 24950, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 73h:05m:05s remains)
INFO - root - 2017-12-05 16:33:02.534140: step 24960, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 73h:41m:40s remains)
INFO - root - 2017-12-05 16:33:11.086124: step 24970, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 75h:03m:52s remains)
INFO - root - 2017-12-05 16:33:19.526165: step 24980, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 72h:50m:00s remains)
INFO - root - 2017-12-05 16:33:28.066113: step 24990, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 72h:00m:36s remains)
INFO - root - 2017-12-05 16:33:36.592201: step 25000, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 73h:13m:04s remains)
2017-12-05 16:33:37.321855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2462173 -4.2481818 -4.2496834 -4.2471347 -4.2448277 -4.2487779 -4.2597661 -4.2662239 -4.2687173 -4.2683725 -4.2607188 -4.2386141 -4.1989746 -4.1403809 -4.0799913][-4.2359023 -4.2281737 -4.2223668 -4.2142224 -4.2070045 -4.2109537 -4.2230625 -4.231214 -4.24009 -4.2495317 -4.2446084 -4.2252154 -4.1911988 -4.1329103 -4.0698895][-4.2300725 -4.2133961 -4.1971979 -4.1769629 -4.1587896 -4.153574 -4.1579781 -4.1677938 -4.1913776 -4.2167192 -4.2197452 -4.2043371 -4.17816 -4.128283 -4.0688][-4.2279749 -4.2025342 -4.1716604 -4.1320362 -4.0991449 -4.0763273 -4.0606265 -4.0705538 -4.1179605 -4.1642871 -4.1798978 -4.1728077 -4.1559119 -4.1164002 -4.0629568][-4.2171111 -4.1829748 -4.138463 -4.0841022 -4.0380025 -3.9901848 -3.9401374 -3.9506929 -4.0354857 -4.1100874 -4.1374187 -4.139348 -4.1278839 -4.0922346 -4.0456247][-4.1995473 -4.1587434 -4.1064758 -4.0408611 -3.9757435 -3.8896184 -3.7863648 -3.7906086 -3.9244826 -4.0350523 -4.0795279 -4.0939679 -4.0847735 -4.052999 -4.0187058][-4.1781721 -4.1369123 -4.0850596 -4.0207572 -3.9413929 -3.826786 -3.6794524 -3.6758032 -3.84383 -3.975075 -4.0328665 -4.0586166 -4.0533881 -4.0384231 -4.0244918][-4.1594925 -4.1246834 -4.0864892 -4.0508919 -3.9955301 -3.9065132 -3.7965548 -3.7917013 -3.9220104 -4.0238991 -4.0658617 -4.0831418 -4.0759306 -4.0719609 -4.071569][-4.1338286 -4.1103373 -4.0943937 -4.0934796 -4.0720816 -4.0235453 -3.9695542 -3.9738386 -4.0514164 -4.1140575 -4.1322036 -4.1330628 -4.1190262 -4.1150465 -4.1216059][-4.1027856 -4.0905294 -4.0989904 -4.1274633 -4.1350088 -4.1153593 -4.0969639 -4.1097589 -4.1545267 -4.1906815 -4.1972528 -4.1909833 -4.1767745 -4.1744967 -4.1883054][-4.0735869 -4.0688877 -4.0955515 -4.1449523 -4.1739235 -4.1775227 -4.1848493 -4.2038574 -4.2316937 -4.2529912 -4.2522483 -4.2424517 -4.231708 -4.2292371 -4.2423186][-4.0808439 -4.0777793 -4.1086578 -4.1621718 -4.1983528 -4.2192574 -4.2434821 -4.2681017 -4.2880197 -4.2994165 -4.2943978 -4.2836809 -4.272294 -4.2640438 -4.2652936][-4.1331449 -4.1247511 -4.1423497 -4.1842074 -4.2169671 -4.2422667 -4.26975 -4.2943115 -4.3081403 -4.31225 -4.3105893 -4.3044248 -4.2934136 -4.2817578 -4.2765393][-4.1900368 -4.1806188 -4.1905942 -4.2207146 -4.2467461 -4.2698765 -4.2923174 -4.3082409 -4.3134489 -4.3126812 -4.3111606 -4.3093462 -4.3020205 -4.2948322 -4.2908015][-4.2439432 -4.2388663 -4.2462745 -4.265687 -4.2829518 -4.2992487 -4.3130984 -4.3205523 -4.3188639 -4.3159208 -4.3152847 -4.3144207 -4.309988 -4.3055291 -4.3033676]]...]
INFO - root - 2017-12-05 16:33:45.848028: step 25010, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 71h:27m:41s remains)
INFO - root - 2017-12-05 16:33:54.262400: step 25020, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 74h:48m:06s remains)
INFO - root - 2017-12-05 16:34:02.783980: step 25030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 73h:06m:38s remains)
INFO - root - 2017-12-05 16:34:11.246446: step 25040, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 72h:10m:14s remains)
INFO - root - 2017-12-05 16:34:19.717447: step 25050, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 74h:03m:46s remains)
INFO - root - 2017-12-05 16:34:28.355385: step 25060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:32m:30s remains)
INFO - root - 2017-12-05 16:34:36.864794: step 25070, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 70h:14m:40s remains)
INFO - root - 2017-12-05 16:34:45.209364: step 25080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 72h:43m:26s remains)
INFO - root - 2017-12-05 16:34:53.759571: step 25090, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 71h:45m:11s remains)
INFO - root - 2017-12-05 16:35:02.297807: step 25100, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 71h:32m:04s remains)
2017-12-05 16:35:03.109261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2327042 -4.2420292 -4.2311168 -4.2058029 -4.182766 -4.1586385 -4.1132956 -4.0613356 -4.0608711 -4.1105232 -4.1632824 -4.2023244 -4.2206554 -4.2426844 -4.2689652][-4.233995 -4.2426085 -4.2292261 -4.2016349 -4.1792779 -4.1578746 -4.1126785 -4.0572586 -4.0547547 -4.1073756 -4.1619849 -4.2011609 -4.2197366 -4.2422972 -4.2690425][-4.2336283 -4.2429 -4.2297053 -4.2033982 -4.1854634 -4.1705961 -4.13017 -4.0739651 -4.0668964 -4.1172318 -4.1687226 -4.2045474 -4.2217607 -4.2435365 -4.2696571][-4.2308812 -4.2412548 -4.23064 -4.2071733 -4.1930823 -4.1835079 -4.1461573 -4.0867581 -4.0725393 -4.1193142 -4.1681571 -4.2030725 -4.2214208 -4.2444682 -4.2708893][-4.2237535 -4.2330208 -4.2225146 -4.1991186 -4.1850939 -4.1766806 -4.1381779 -4.0726724 -4.0524797 -4.1007032 -4.154006 -4.1952033 -4.219058 -4.2450023 -4.2718349][-4.2134094 -4.2174687 -4.2026916 -4.1758561 -4.1584673 -4.1474209 -4.1033998 -4.0300112 -4.0087185 -4.0678315 -4.1327934 -4.1845636 -4.2153306 -4.2429852 -4.2697825][-4.2040505 -4.2003508 -4.1780519 -4.1460609 -4.125113 -4.1112833 -4.061408 -3.9810987 -3.9655609 -4.0408382 -4.117342 -4.1774058 -4.2123365 -4.23986 -4.2663045][-4.1967444 -4.1858678 -4.157362 -4.1232896 -4.1030474 -4.0905342 -4.0400152 -3.9612834 -3.9563179 -4.0394239 -4.1187725 -4.1791997 -4.2128406 -4.2366819 -4.2606592][-4.1908903 -4.1763673 -4.146574 -4.1164794 -4.1038122 -4.0980911 -4.0552411 -3.9876363 -3.989275 -4.0659032 -4.1361685 -4.1889229 -4.2168694 -4.2344894 -4.2546797][-4.1798663 -4.1661425 -4.1415348 -4.1215067 -4.12107 -4.1259689 -4.0964422 -4.0424938 -4.0455494 -4.1077886 -4.162641 -4.2040067 -4.2241387 -4.23616 -4.2537746][-4.1684222 -4.1592937 -4.1431117 -4.1344385 -4.1447272 -4.1584616 -4.1387696 -4.0936866 -4.0926886 -4.1402287 -4.1818295 -4.2149553 -4.2315865 -4.2416182 -4.2567444][-4.1591206 -4.1544647 -4.1454463 -4.1433997 -4.158061 -4.1755691 -4.1610847 -4.1188941 -4.110147 -4.1469731 -4.1816554 -4.2126722 -4.2319522 -4.243546 -4.2566376][-4.1501703 -4.149796 -4.1468406 -4.1490593 -4.16471 -4.1821427 -4.1678176 -4.1223569 -4.1047421 -4.13575 -4.17044 -4.2022381 -4.2257881 -4.2402735 -4.2525764][-4.1401663 -4.1447639 -4.1479788 -4.1537251 -4.1696134 -4.1862621 -4.1718564 -4.1242924 -4.1017642 -4.1297879 -4.1631207 -4.1911016 -4.2140727 -4.2302837 -4.2426705][-4.132719 -4.1441913 -4.1544309 -4.1630945 -4.1787949 -4.19445 -4.1800809 -4.1335745 -4.1096888 -4.1324191 -4.1574974 -4.1764922 -4.1971617 -4.2160707 -4.2295027]]...]
INFO - root - 2017-12-05 16:35:11.793262: step 25110, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.877 sec/batch; 74h:51m:17s remains)
INFO - root - 2017-12-05 16:35:20.281789: step 25120, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 73h:20m:18s remains)
INFO - root - 2017-12-05 16:35:28.697825: step 25130, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.820 sec/batch; 70h:01m:03s remains)
INFO - root - 2017-12-05 16:35:37.212933: step 25140, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 74h:20m:32s remains)
INFO - root - 2017-12-05 16:35:45.786942: step 25150, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 70h:41m:03s remains)
INFO - root - 2017-12-05 16:35:54.348363: step 25160, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 70h:53m:51s remains)
INFO - root - 2017-12-05 16:36:02.925024: step 25170, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 73h:12m:06s remains)
INFO - root - 2017-12-05 16:36:11.465060: step 25180, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 72h:24m:35s remains)
INFO - root - 2017-12-05 16:36:20.021715: step 25190, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 72h:43m:08s remains)
INFO - root - 2017-12-05 16:36:28.632006: step 25200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 73h:24m:21s remains)
2017-12-05 16:36:29.496002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.22789 -4.1858673 -4.1469135 -4.1145926 -4.0961194 -4.0784144 -4.0732446 -4.0928535 -4.1302032 -4.1542993 -4.1675868 -4.1858621 -4.2052283 -4.2200279 -4.2314935][-4.2275343 -4.1835895 -4.1416588 -4.1046114 -4.0777693 -4.048038 -4.0357103 -4.0521841 -4.0958338 -4.1294708 -4.147728 -4.1675177 -4.1893549 -4.20962 -4.2247891][-4.2290549 -4.1810207 -4.1349993 -4.09678 -4.0635376 -4.0203266 -3.9915633 -4.0000763 -4.0497661 -4.0914159 -4.1129994 -4.1373878 -4.1677437 -4.19282 -4.2091312][-4.2273335 -4.1739807 -4.1278477 -4.0962505 -4.0727839 -4.0287943 -3.9818659 -3.9741731 -4.0227423 -4.0620441 -4.0807319 -4.1079073 -4.1472754 -4.1714277 -4.1778884][-4.2243142 -4.1688027 -4.1233931 -4.1030617 -4.0954418 -4.0603523 -3.9994676 -3.9683461 -4.0107079 -4.05189 -4.0737329 -4.1027293 -4.1374655 -4.1490393 -4.14125][-4.2220922 -4.1664639 -4.1212492 -4.1071558 -4.1004052 -4.0631032 -3.980335 -3.9268446 -3.9800775 -4.04313 -4.0752726 -4.0972757 -4.117497 -4.1115155 -4.093751][-4.2200789 -4.1622581 -4.1142654 -4.0930238 -4.0719938 -4.01284 -3.8854527 -3.8029039 -3.8968751 -4.0045977 -4.0521274 -4.0655861 -4.0726738 -4.0575957 -4.0369725][-4.222033 -4.1646223 -4.1144381 -4.0802088 -4.0379643 -3.9540863 -3.7877305 -3.6870484 -3.8312488 -3.9845126 -4.0368643 -4.0283585 -4.017808 -3.993536 -3.966866][-4.2330847 -4.1782594 -4.1269112 -4.0843854 -4.0304236 -3.9536934 -3.8250933 -3.7631097 -3.8887527 -4.0110235 -4.0396552 -4.0129747 -3.9843631 -3.9458966 -3.9154239][-4.2471266 -4.1921868 -4.140327 -4.1007476 -4.055603 -4.0118589 -3.9494772 -3.9235215 -3.9942505 -4.0597725 -4.0683632 -4.0389156 -4.0081787 -3.9665313 -3.9317677][-4.2570782 -4.2027426 -4.1552486 -4.1235685 -4.0921965 -4.0717964 -4.0465212 -4.0361786 -4.0709729 -4.1006322 -4.0999622 -4.0804582 -4.0573592 -4.0303454 -4.0028014][-4.2652273 -4.215261 -4.1728568 -4.1474543 -4.126574 -4.1165485 -4.1058726 -4.1021662 -4.1156349 -4.1223555 -4.1176624 -4.1115489 -4.1031756 -4.091804 -4.07606][-4.27291 -4.2302017 -4.1947289 -4.1761875 -4.1630421 -4.1566458 -4.1503935 -4.1526623 -4.1592393 -4.1557217 -4.1480341 -4.1441112 -4.13877 -4.1345372 -4.1241193][-4.2795525 -4.244628 -4.2180648 -4.2065082 -4.2006555 -4.1970987 -4.1907535 -4.1972136 -4.2045979 -4.1987162 -4.1871753 -4.1772747 -4.1640973 -4.1554222 -4.1410322][-4.2845592 -4.2560558 -4.2358866 -4.2308621 -4.23096 -4.2313271 -4.2230825 -4.2275205 -4.237483 -4.2330289 -4.2188077 -4.2013755 -4.1777329 -4.15516 -4.1316371]]...]
INFO - root - 2017-12-05 16:36:38.131072: step 25210, loss = 2.03, batch loss = 1.98 (9.0 examples/sec; 0.885 sec/batch; 75h:33m:42s remains)
INFO - root - 2017-12-05 16:36:46.663089: step 25220, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 73h:22m:13s remains)
INFO - root - 2017-12-05 16:36:55.100952: step 25230, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.877 sec/batch; 74h:53m:14s remains)
INFO - root - 2017-12-05 16:37:03.686571: step 25240, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 74h:50m:48s remains)
INFO - root - 2017-12-05 16:37:12.145507: step 25250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 72h:49m:19s remains)
INFO - root - 2017-12-05 16:37:20.744680: step 25260, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 74h:33m:16s remains)
INFO - root - 2017-12-05 16:37:29.353040: step 25270, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 72h:45m:47s remains)
INFO - root - 2017-12-05 16:37:37.859856: step 25280, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 75h:55m:39s remains)
INFO - root - 2017-12-05 16:37:46.425994: step 25290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 73h:02m:10s remains)
INFO - root - 2017-12-05 16:37:54.887060: step 25300, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 73h:28m:23s remains)
2017-12-05 16:37:55.648520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2312636 -4.2339625 -4.22748 -4.2181668 -4.2142091 -4.2165155 -4.2240024 -4.2221141 -4.2172289 -4.22279 -4.2341747 -4.2486825 -4.2667341 -4.2838907 -4.3036108][-4.2016916 -4.2027111 -4.1941776 -4.1868386 -4.1871552 -4.1951165 -4.2080731 -4.2071123 -4.1999874 -4.2048206 -4.2151766 -4.2318182 -4.2558169 -4.2777996 -4.3016105][-4.1639142 -4.1607175 -4.1487746 -4.143383 -4.1519127 -4.1699486 -4.1893682 -4.1875 -4.1808386 -4.190578 -4.2057905 -4.22552 -4.2520962 -4.275856 -4.3013282][-4.1378736 -4.1291337 -4.1118026 -4.10278 -4.1125669 -4.1316071 -4.1498003 -4.1473913 -4.147965 -4.1677785 -4.1899996 -4.2126875 -4.2415466 -4.268981 -4.29701][-4.1170321 -4.1023197 -4.08148 -4.0675254 -4.0726252 -4.0850844 -4.0950756 -4.0933318 -4.1053119 -4.1368408 -4.1662979 -4.1931157 -4.2238126 -4.255301 -4.2883124][-4.091496 -4.0685496 -4.036756 -4.0096574 -4.0008354 -3.9949255 -3.9890554 -3.9893541 -4.0239921 -4.0771852 -4.1224251 -4.1610284 -4.1986341 -4.2359018 -4.2741685][-4.0638022 -4.0345459 -3.9929202 -3.9550626 -3.9328933 -3.9099698 -3.8857286 -3.8861785 -3.9392552 -4.0074849 -4.0690141 -4.1251087 -4.1751704 -4.2202129 -4.2627912][-4.0435853 -4.0180788 -3.9824574 -3.9481547 -3.9306483 -3.9132569 -3.8943269 -3.903923 -3.9583721 -4.0180116 -4.0738983 -4.1265917 -4.1737227 -4.2166967 -4.2588429][-4.039319 -4.0191727 -3.9915066 -3.9643149 -3.9530406 -3.9445653 -3.9370739 -3.9567158 -4.0082188 -4.0574 -4.1011405 -4.1412783 -4.1790051 -4.2155828 -4.2560263][-4.0640941 -4.0542707 -4.0399575 -4.0248814 -4.0208526 -4.01758 -4.0162158 -4.0364203 -4.0762916 -4.1110773 -4.1416788 -4.1714096 -4.1995325 -4.2272811 -4.2620196][-4.1094742 -4.1080766 -4.1065922 -4.1039729 -4.1073146 -4.1077747 -4.1090388 -4.1243386 -4.1498861 -4.1701751 -4.1886215 -4.2091331 -4.2292519 -4.2497945 -4.2773252][-4.16729 -4.170146 -4.1756921 -4.1804748 -4.1863441 -4.1879478 -4.1893134 -4.1975689 -4.2117329 -4.2230577 -4.233871 -4.2478843 -4.2622495 -4.2775211 -4.2979264][-4.218277 -4.2206593 -4.2273583 -4.2333078 -4.2384944 -4.239831 -4.2400517 -4.2424006 -4.2493062 -4.2568607 -4.2650547 -4.2762504 -4.287879 -4.3005428 -4.31589][-4.2529044 -4.253305 -4.2568927 -4.2603788 -4.2637286 -4.2655535 -4.2663026 -4.2669015 -4.2705688 -4.2760749 -4.2829971 -4.292685 -4.3031025 -4.3140883 -4.3255954][-4.2881384 -4.2879047 -4.289495 -4.2914152 -4.2933073 -4.2946453 -4.2955012 -4.2954721 -4.2963848 -4.2991519 -4.3038259 -4.310544 -4.317831 -4.3254251 -4.3326712]]...]
INFO - root - 2017-12-05 16:38:04.198989: step 25310, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 73h:41m:24s remains)
INFO - root - 2017-12-05 16:38:12.743496: step 25320, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 72h:33m:29s remains)
INFO - root - 2017-12-05 16:38:21.352093: step 25330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 74h:26m:28s remains)
INFO - root - 2017-12-05 16:38:29.815842: step 25340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 72h:51m:33s remains)
INFO - root - 2017-12-05 16:38:38.409646: step 25350, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 72h:39m:23s remains)
INFO - root - 2017-12-05 16:38:46.939973: step 25360, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 72h:31m:10s remains)
INFO - root - 2017-12-05 16:38:55.568364: step 25370, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.901 sec/batch; 76h:53m:45s remains)
INFO - root - 2017-12-05 16:39:04.097655: step 25380, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.902 sec/batch; 76h:57m:22s remains)
INFO - root - 2017-12-05 16:39:12.642779: step 25390, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 73h:42m:30s remains)
INFO - root - 2017-12-05 16:39:21.223790: step 25400, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:23m:21s remains)
2017-12-05 16:39:21.964994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2842183 -4.2674327 -4.2457471 -4.2290273 -4.22293 -4.2230616 -4.2161441 -4.2035074 -4.1974125 -4.2050123 -4.2265892 -4.2569284 -4.2940459 -4.3252726 -4.3494697][-4.2437477 -4.2218409 -4.1941738 -4.1737633 -4.1676722 -4.1658249 -4.1535273 -4.12697 -4.109129 -4.1163263 -4.1484189 -4.1938386 -4.2482204 -4.2944551 -4.332633][-4.208128 -4.1815758 -4.1439085 -4.1199689 -4.1115036 -4.110168 -4.0969591 -4.05298 -4.0177345 -4.0217834 -4.0652876 -4.1280165 -4.201767 -4.2647047 -4.3166728][-4.1712809 -4.1423311 -4.1011925 -4.0776567 -4.067184 -4.0659261 -4.0525265 -3.9920545 -3.9378591 -3.9390295 -3.995466 -4.0742927 -4.1626744 -4.2392011 -4.3031731][-4.1288505 -4.1009736 -4.0603647 -4.0453258 -4.0324545 -4.0238247 -4.004467 -3.934104 -3.8613164 -3.8596034 -3.9366279 -4.0334964 -4.13296 -4.2191319 -4.2921438][-4.0957336 -4.0713534 -4.0342107 -4.0274467 -4.0127912 -3.9909263 -3.9606404 -3.8856015 -3.7951434 -3.7876713 -3.8902597 -4.0112314 -4.1207075 -4.2114887 -4.2872834][-4.0663977 -4.0484214 -4.0181627 -4.011775 -3.9948726 -3.9641514 -3.924643 -3.8502278 -3.7394595 -3.7154188 -3.8477452 -3.9979565 -4.1185141 -4.2109489 -4.2854137][-4.0369434 -4.0328522 -4.0160723 -4.0103488 -3.9982507 -3.9735792 -3.9290791 -3.8495126 -3.7182055 -3.6699884 -3.8238506 -3.9934309 -4.1220622 -4.213099 -4.2840276][-4.0379338 -4.0467439 -4.0450025 -4.0458312 -4.045845 -4.0375667 -3.9972086 -3.91488 -3.7855635 -3.7300141 -3.8631876 -4.0204172 -4.14351 -4.2258081 -4.2877746][-4.0714464 -4.0958309 -4.1090131 -4.1179204 -4.1262193 -4.1310883 -4.09691 -4.0140553 -3.8985395 -3.8501205 -3.9430468 -4.0661139 -4.17419 -4.2435269 -4.2934523][-4.140624 -4.1665154 -4.1797781 -4.1883092 -4.1975236 -4.2006688 -4.166604 -4.086803 -3.987252 -3.945725 -4.0125093 -4.1097484 -4.2036195 -4.2608194 -4.3002591][-4.2152448 -4.2292027 -4.2322407 -4.2358713 -4.2421641 -4.2420449 -4.2102461 -4.1404357 -4.0538354 -4.0175881 -4.0693555 -4.150105 -4.2296472 -4.278286 -4.3088417][-4.2671752 -4.2722774 -4.2702155 -4.2740068 -4.2774706 -4.2735052 -4.2450557 -4.1885633 -4.1175137 -4.0855703 -4.125351 -4.1900764 -4.25559 -4.2964869 -4.3201041][-4.3054566 -4.30556 -4.3019109 -4.3059106 -4.3092685 -4.305429 -4.284821 -4.2414122 -4.1886468 -4.1604357 -4.186594 -4.2322578 -4.2830644 -4.3156562 -4.3340111][-4.3309746 -4.3310504 -4.3278055 -4.3308024 -4.3335085 -4.3312521 -4.3178287 -4.2857471 -4.2486529 -4.2254295 -4.2416253 -4.2731061 -4.3091588 -4.333272 -4.3479185]]...]
INFO - root - 2017-12-05 16:39:30.683205: step 25410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 73h:36m:13s remains)
INFO - root - 2017-12-05 16:39:39.327954: step 25420, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 72h:35m:11s remains)
INFO - root - 2017-12-05 16:39:47.908832: step 25430, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 73h:57m:32s remains)
INFO - root - 2017-12-05 16:39:56.564303: step 25440, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 73h:13m:28s remains)
INFO - root - 2017-12-05 16:40:04.995121: step 25450, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 73h:23m:03s remains)
INFO - root - 2017-12-05 16:40:13.621996: step 25460, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 75h:07m:32s remains)
INFO - root - 2017-12-05 16:40:22.048070: step 25470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 73h:31m:10s remains)
INFO - root - 2017-12-05 16:40:30.470024: step 25480, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:22m:07s remains)
INFO - root - 2017-12-05 16:40:39.093377: step 25490, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.893 sec/batch; 76h:08m:36s remains)
INFO - root - 2017-12-05 16:40:47.663644: step 25500, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 75h:24m:36s remains)
2017-12-05 16:40:48.396651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.288013 -4.270874 -4.2428455 -4.1966596 -4.1411147 -4.0875626 -4.04653 -4.0381384 -4.0598416 -4.1046824 -4.1430874 -4.1790433 -4.2170033 -4.2113447 -4.1710129][-4.2878404 -4.2700944 -4.244494 -4.1986527 -4.1395574 -4.0803013 -4.0323377 -4.0181155 -4.0338306 -4.0816784 -4.1292186 -4.1644335 -4.2003732 -4.2011671 -4.1669745][-4.2864532 -4.2694135 -4.2472343 -4.2037592 -4.1418672 -4.0770073 -4.0257635 -4.0100203 -4.01903 -4.0668654 -4.124671 -4.1599169 -4.1872463 -4.1863174 -4.1572633][-4.28452 -4.2684155 -4.2474308 -4.2050495 -4.1391144 -4.06913 -4.0156026 -4.0018845 -4.0166192 -4.0714426 -4.1367865 -4.1687551 -4.18534 -4.1783781 -4.1532974][-4.2816277 -4.2642126 -4.2433558 -4.2015634 -4.131598 -4.0557513 -3.9987388 -3.9892473 -4.0186496 -4.0859823 -4.1555405 -4.1836386 -4.1863394 -4.170414 -4.1487045][-4.2790451 -4.2589626 -4.2354717 -4.1959686 -4.1279945 -4.0488153 -3.9870522 -3.9783204 -4.0231833 -4.0978832 -4.1631627 -4.1841812 -4.1751127 -4.1545382 -4.1336255][-4.2772007 -4.2566051 -4.2329922 -4.1951404 -4.131813 -4.0520225 -3.9825957 -3.9721572 -4.0216875 -4.0925312 -4.1496716 -4.1664386 -4.1505685 -4.1293554 -4.115572][-4.2741113 -4.2562222 -4.2346106 -4.1966867 -4.1338153 -4.0520916 -3.9749014 -3.9636164 -4.0117116 -4.0761495 -4.128613 -4.1509624 -4.1350145 -4.1133828 -4.1045623][-4.2699413 -4.2537131 -4.232142 -4.1937628 -4.1342063 -4.0591516 -3.98094 -3.966855 -4.011198 -4.069448 -4.1193442 -4.1444826 -4.1358166 -4.1176805 -4.108624][-4.2663407 -4.2505784 -4.2285938 -4.1906762 -4.1397004 -4.0828614 -4.0189595 -4.0073328 -4.0474119 -4.0989985 -4.1388035 -4.1561909 -4.1474957 -4.1316304 -4.1212363][-4.26482 -4.2503057 -4.2285643 -4.1930208 -4.1474137 -4.1050329 -4.0581141 -4.0505395 -4.0841632 -4.1302481 -4.1636343 -4.1729503 -4.16217 -4.1468253 -4.135468][-4.2648931 -4.2520008 -4.2317677 -4.2017775 -4.1636839 -4.1306925 -4.0920348 -4.0817275 -4.1028757 -4.1440592 -4.1770115 -4.1878533 -4.1789184 -4.1666412 -4.1563239][-4.263473 -4.2499709 -4.2281508 -4.2027593 -4.174367 -4.1459126 -4.1088333 -4.092473 -4.0999846 -4.1382985 -4.1788845 -4.1983862 -4.1965756 -4.1908464 -4.1851072][-4.2604375 -4.2431498 -4.2179441 -4.1937761 -4.1684775 -4.1389127 -4.10615 -4.0897851 -4.0885253 -4.1239042 -4.171351 -4.1955142 -4.1998172 -4.1990776 -4.1953888][-4.2598991 -4.2396517 -4.2096772 -4.1846356 -4.1568122 -4.1269326 -4.1025791 -4.0929046 -4.0896173 -4.1229286 -4.1707025 -4.1967916 -4.2057705 -4.2078352 -4.2048078]]...]
INFO - root - 2017-12-05 16:40:56.851242: step 25510, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 72h:00m:39s remains)
INFO - root - 2017-12-05 16:41:05.340433: step 25520, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 73h:54m:10s remains)
INFO - root - 2017-12-05 16:41:13.846333: step 25530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 71h:47m:16s remains)
INFO - root - 2017-12-05 16:41:22.285320: step 25540, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.812 sec/batch; 69h:11m:52s remains)
INFO - root - 2017-12-05 16:41:30.822274: step 25550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 73h:17m:41s remains)
INFO - root - 2017-12-05 16:41:39.389308: step 25560, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 73h:07m:47s remains)
INFO - root - 2017-12-05 16:41:47.920607: step 25570, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 71h:58m:56s remains)
INFO - root - 2017-12-05 16:41:56.359288: step 25580, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 74h:36m:20s remains)
INFO - root - 2017-12-05 16:42:04.878719: step 25590, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 72h:48m:34s remains)
INFO - root - 2017-12-05 16:42:13.402044: step 25600, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 75h:16m:55s remains)
2017-12-05 16:42:14.230993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2270584 -4.2191882 -4.2041874 -4.19134 -4.1843271 -4.1831465 -4.1746659 -4.1544018 -4.126503 -4.1055589 -4.0979872 -4.1015015 -4.1072459 -4.1216426 -4.1392384][-4.2338219 -4.2253952 -4.2095847 -4.1984072 -4.1945848 -4.193049 -4.1809487 -4.1575546 -4.1277866 -4.10752 -4.1044536 -4.114779 -4.12433 -4.1463695 -4.171761][-4.2401466 -4.2358093 -4.22216 -4.2096834 -4.2007856 -4.19392 -4.1781673 -4.1569252 -4.1314945 -4.11255 -4.1056361 -4.1182027 -4.134129 -4.1601148 -4.1860366][-4.2271967 -4.2225361 -4.2075138 -4.189764 -4.1760955 -4.1675835 -4.1568751 -4.1472039 -4.13232 -4.1135387 -4.0994382 -4.1078744 -4.1225586 -4.1493826 -4.1765852][-4.201118 -4.1923809 -4.1738434 -4.1520147 -4.1356859 -4.1237149 -4.1158338 -4.1173725 -4.1139288 -4.0990806 -4.0846457 -4.086978 -4.099534 -4.1250992 -4.1542654][-4.1764011 -4.1657987 -4.1477966 -4.1251411 -4.1057959 -4.0870371 -4.0757313 -4.0791178 -4.0836787 -4.0754442 -4.0642071 -4.0737262 -4.0918217 -4.1195426 -4.1464176][-4.1527572 -4.1377106 -4.1196175 -4.1003909 -4.08511 -4.0652447 -4.0514717 -4.053864 -4.0582395 -4.0536757 -4.0488563 -4.0651207 -4.0922127 -4.1288104 -4.1547418][-4.1404996 -4.1199226 -4.1037531 -4.0912075 -4.0784039 -4.0578938 -4.039597 -4.0389676 -4.0360003 -4.0301752 -4.0255585 -4.0490389 -4.0903096 -4.1358018 -4.1594095][-4.1464581 -4.1243153 -4.1029997 -4.0873756 -4.0758305 -4.0575266 -4.0350947 -4.0282779 -4.0222411 -4.0155563 -4.0106354 -4.0369129 -4.0856261 -4.1354065 -4.154901][-4.1838388 -4.1632953 -4.1372366 -4.1167817 -4.100533 -4.0812263 -4.0538092 -4.0394287 -4.0313549 -4.0240879 -4.0186787 -4.042428 -4.0894995 -4.1294546 -4.139679][-4.2214732 -4.2011795 -4.1741424 -4.1510806 -4.1315861 -4.1128583 -4.0885448 -4.0757155 -4.0668659 -4.0587811 -4.05145 -4.0714903 -4.1148186 -4.1388407 -4.1354074][-4.2405767 -4.2191167 -4.1954646 -4.1761756 -4.1627903 -4.147542 -4.1318941 -4.1275144 -4.1215038 -4.1131043 -4.1053462 -4.1197534 -4.1518431 -4.16444 -4.1517382][-4.2422681 -4.2250943 -4.2070165 -4.1947565 -4.1880846 -4.1768818 -4.16892 -4.1721077 -4.1724524 -4.167006 -4.162919 -4.1744342 -4.1905375 -4.1874318 -4.1684937][-4.2445517 -4.2357717 -4.2179761 -4.2055435 -4.2042608 -4.19884 -4.19382 -4.1998234 -4.2044921 -4.205822 -4.2082376 -4.2165394 -4.2187233 -4.2021441 -4.1828032][-4.2471466 -4.239615 -4.2187133 -4.2052894 -4.2089486 -4.211834 -4.2090435 -4.2141943 -4.2201977 -4.2292371 -4.2367873 -4.2444191 -4.23944 -4.2194386 -4.2046843]]...]
INFO - root - 2017-12-05 16:42:22.802250: step 25610, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:21m:22s remains)
INFO - root - 2017-12-05 16:42:31.400408: step 25620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 72h:32m:59s remains)
INFO - root - 2017-12-05 16:42:40.056521: step 25630, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 74h:30m:47s remains)
INFO - root - 2017-12-05 16:42:48.660933: step 25640, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:24m:06s remains)
INFO - root - 2017-12-05 16:42:57.293124: step 25650, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 74h:09m:59s remains)
INFO - root - 2017-12-05 16:43:05.766187: step 25660, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.884 sec/batch; 75h:22m:38s remains)
INFO - root - 2017-12-05 16:43:14.332675: step 25670, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.845 sec/batch; 72h:02m:44s remains)
INFO - root - 2017-12-05 16:43:22.788768: step 25680, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 74h:05m:02s remains)
INFO - root - 2017-12-05 16:43:31.285192: step 25690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 73h:12m:57s remains)
INFO - root - 2017-12-05 16:43:39.873723: step 25700, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 72h:57m:55s remains)
2017-12-05 16:43:40.590343: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2970643 -4.2926297 -4.2770534 -4.2647462 -4.2733264 -4.2932472 -4.302032 -4.2926636 -4.2724781 -4.2380781 -4.1992054 -4.1699142 -4.1599793 -4.1758637 -4.2069316][-4.2900915 -4.2848859 -4.2711744 -4.2636843 -4.2755213 -4.2942996 -4.3054733 -4.3043528 -4.290463 -4.2577767 -4.2129822 -4.1744075 -4.1577282 -4.1716118 -4.2034044][-4.2624421 -4.2501717 -4.2307019 -4.2209783 -4.23121 -4.247571 -4.2635221 -4.27853 -4.2822938 -4.2670445 -4.23397 -4.1969256 -4.1724434 -4.1739135 -4.1969404][-4.2232876 -4.1999264 -4.1678586 -4.14748 -4.14947 -4.1622396 -4.18705 -4.2238479 -4.2511816 -4.2592521 -4.2456808 -4.2186217 -4.1903296 -4.1772251 -4.18657][-4.1807508 -4.1476631 -4.1037059 -4.073256 -4.0620232 -4.0652747 -4.0971594 -4.157268 -4.2099195 -4.2389636 -4.2454715 -4.2318039 -4.2055292 -4.1820459 -4.1799431][-4.1388259 -4.0975704 -4.0487409 -4.0136571 -3.991056 -3.9800482 -4.01191 -4.0909567 -4.1693883 -4.2166348 -4.2380204 -4.2342596 -4.2132974 -4.1841588 -4.1713948][-4.1191111 -4.0697074 -4.0201015 -3.9893839 -3.9624646 -3.9367175 -3.9533 -4.0335064 -4.1275334 -4.1880293 -4.2209868 -4.2290449 -4.2209725 -4.1956029 -4.1768036][-4.1337094 -4.0845885 -4.039824 -4.0164452 -3.9910555 -3.957026 -3.9523661 -4.0115547 -4.0980988 -4.1602612 -4.2029133 -4.225738 -4.2326169 -4.217639 -4.1985445][-4.1806078 -4.1406779 -4.105092 -4.0871663 -4.0653429 -4.0308595 -4.0130119 -4.0454607 -4.1099563 -4.1636572 -4.2083826 -4.2392144 -4.2530894 -4.2453771 -4.2282462][-4.2324739 -4.203907 -4.1771803 -4.1665926 -4.1535368 -4.1265798 -4.1047812 -4.1182795 -4.1599512 -4.1989288 -4.2347641 -4.2614546 -4.2735023 -4.2693176 -4.25457][-4.2594409 -4.2412162 -4.2257242 -4.2252522 -4.2243333 -4.2096462 -4.1898847 -4.1912665 -4.2165689 -4.242939 -4.2673397 -4.2835779 -4.2902274 -4.2857862 -4.2714977][-4.2524037 -4.2439055 -4.2409744 -4.2522616 -4.2619638 -4.2556915 -4.2401428 -4.237432 -4.2529783 -4.2692666 -4.2825565 -4.2883787 -4.2893705 -4.2818027 -4.2657189][-4.2238145 -4.2216043 -4.2300243 -4.2514892 -4.26692 -4.2626691 -4.247704 -4.2426963 -4.25277 -4.26339 -4.268311 -4.2682695 -4.2659941 -4.2579455 -4.2428346][-4.2022195 -4.203104 -4.21597 -4.2373533 -4.2496929 -4.2407875 -4.2227316 -4.2164173 -4.225523 -4.2356672 -4.2390423 -4.2376542 -4.2370343 -4.2327385 -4.2218819][-4.218215 -4.2202315 -4.2289348 -4.240315 -4.2420244 -4.2248154 -4.2014751 -4.1939831 -4.2027235 -4.2144628 -4.2207165 -4.2224603 -4.2267485 -4.2286062 -4.2234139]]...]
INFO - root - 2017-12-05 16:43:49.192282: step 25710, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 73h:46m:07s remains)
INFO - root - 2017-12-05 16:43:57.711088: step 25720, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:23m:25s remains)
INFO - root - 2017-12-05 16:44:06.336806: step 25730, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 74h:23m:09s remains)
INFO - root - 2017-12-05 16:44:14.868113: step 25740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 73h:00m:39s remains)
INFO - root - 2017-12-05 16:44:23.387491: step 25750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 72h:46m:00s remains)
INFO - root - 2017-12-05 16:44:31.898256: step 25760, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 70h:46m:00s remains)
INFO - root - 2017-12-05 16:44:40.317497: step 25770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 72h:40m:59s remains)
INFO - root - 2017-12-05 16:44:48.839503: step 25780, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:13m:52s remains)
INFO - root - 2017-12-05 16:44:57.264193: step 25790, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.797 sec/batch; 67h:52m:51s remains)
INFO - root - 2017-12-05 16:45:05.704280: step 25800, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 72h:54m:39s remains)
2017-12-05 16:45:06.474398: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1543641 -4.1321282 -4.1186547 -4.1235347 -4.1461267 -4.18198 -4.2117786 -4.2209949 -4.2093778 -4.1939549 -4.183238 -4.1785283 -4.1666746 -4.1464586 -4.1339421][-4.1562686 -4.1256161 -4.1062884 -4.1090631 -4.1335449 -4.1675062 -4.1915116 -4.1949744 -4.182456 -4.173708 -4.1712193 -4.1735973 -4.1604505 -4.132494 -4.116024][-4.1516418 -4.1137757 -4.0922556 -4.0983472 -4.1269093 -4.1584921 -4.1735072 -4.1698337 -4.1605573 -4.1624594 -4.169982 -4.1751518 -4.1543975 -4.1138668 -4.0868139][-4.1213193 -4.0835943 -4.0747352 -4.0955029 -4.1255951 -4.1492267 -4.1525259 -4.1379352 -4.1265907 -4.1385512 -4.161099 -4.1698036 -4.1377773 -4.0821004 -4.0456128][-4.089807 -4.0666428 -4.0794182 -4.1097593 -4.131125 -4.1383533 -4.1206303 -4.0861487 -4.0689354 -4.0928626 -4.132442 -4.1450977 -4.107151 -4.0462756 -4.0143552][-4.0964775 -4.092968 -4.110332 -4.1317868 -4.1366043 -4.119987 -4.0709558 -4.0075855 -3.9902534 -4.0351758 -4.09339 -4.1144938 -4.0841651 -4.0417304 -4.0308971][-4.1130438 -4.1199913 -4.1268034 -4.130527 -4.11833 -4.0718312 -3.9811976 -3.8828077 -3.8909585 -3.9811852 -4.0639777 -4.1018863 -4.0982184 -4.0811605 -4.080574][-4.1100636 -4.1208935 -4.1205945 -4.1134605 -4.0859647 -4.0209951 -3.9093196 -3.8126848 -3.8670344 -3.988806 -4.0824766 -4.1280603 -4.1368847 -4.1276197 -4.1177926][-4.1159935 -4.1321316 -4.1310153 -4.1191845 -4.0848203 -4.0225964 -3.9404714 -3.8956051 -3.9570787 -4.054987 -4.1270947 -4.1588678 -4.1633816 -4.1497536 -4.1273975][-4.1430893 -4.1559286 -4.1483445 -4.1324043 -4.1005836 -4.0487485 -3.9985318 -3.9935482 -4.0522943 -4.1202908 -4.1628022 -4.1750255 -4.1729403 -4.154655 -4.1244683][-4.164825 -4.1600165 -4.1375613 -4.1167994 -4.088963 -4.0521121 -4.0306807 -4.0568352 -4.1160994 -4.16871 -4.1945891 -4.1972857 -4.1884842 -4.1655426 -4.1308913][-4.1723075 -4.15474 -4.1245961 -4.1026988 -4.081511 -4.0624509 -4.0687819 -4.1119285 -4.16632 -4.2101035 -4.2289624 -4.2284045 -4.2164946 -4.1904745 -4.1565332][-4.1906042 -4.1698537 -4.1424217 -4.1226015 -4.1058469 -4.1009068 -4.1238985 -4.1715727 -4.2142587 -4.2463584 -4.2603707 -4.2607365 -4.2503815 -4.2275314 -4.2000837][-4.22747 -4.2076044 -4.1858082 -4.1709843 -4.1614261 -4.1610255 -4.183486 -4.2225413 -4.2524519 -4.27195 -4.2815185 -4.2818918 -4.2723193 -4.2535553 -4.2323909][-4.2713103 -4.2555952 -4.2382941 -4.2289467 -4.2253084 -4.2268419 -4.2419276 -4.2663693 -4.2849097 -4.2939043 -4.2978387 -4.2961311 -4.2863131 -4.2709775 -4.2549744]]...]
INFO - root - 2017-12-05 16:45:15.024983: step 25810, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 70h:23m:16s remains)
INFO - root - 2017-12-05 16:45:23.487125: step 25820, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:50m:19s remains)
INFO - root - 2017-12-05 16:45:32.005891: step 25830, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 72h:07m:39s remains)
INFO - root - 2017-12-05 16:45:40.606372: step 25840, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 72h:15m:49s remains)
INFO - root - 2017-12-05 16:45:49.258012: step 25850, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 73h:46m:01s remains)
INFO - root - 2017-12-05 16:45:57.883105: step 25860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 71h:46m:51s remains)
INFO - root - 2017-12-05 16:46:06.277536: step 25870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:19m:11s remains)
INFO - root - 2017-12-05 16:46:14.643591: step 25880, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 70h:49m:52s remains)
INFO - root - 2017-12-05 16:46:23.180815: step 25890, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:50m:18s remains)
INFO - root - 2017-12-05 16:46:31.704980: step 25900, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 69h:00m:34s remains)
2017-12-05 16:46:32.431996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3448343 -4.3372512 -4.3323646 -4.3301811 -4.3287325 -4.328362 -4.328516 -4.3329563 -4.3341327 -4.3342419 -4.3387084 -4.3409534 -4.3454885 -4.3530536 -4.3623323][-4.314836 -4.2994752 -4.2910819 -4.2862654 -4.281538 -4.2801208 -4.2834272 -4.2912612 -4.2945528 -4.2988181 -4.3073754 -4.3087368 -4.3140593 -4.3253188 -4.339437][-4.2882924 -4.2644224 -4.2500219 -4.2389679 -4.22931 -4.2255082 -4.2296576 -4.2405829 -4.2480345 -4.2570724 -4.2690654 -4.269567 -4.2766066 -4.2925858 -4.313014][-4.2728429 -4.2430315 -4.2225432 -4.201148 -4.1847219 -4.1815777 -4.1901612 -4.2050695 -4.2120295 -4.2168884 -4.2260823 -4.2260404 -4.2364907 -4.2579966 -4.2862458][-4.2711058 -4.2420397 -4.2170253 -4.1874967 -4.1680493 -4.1627922 -4.1696777 -4.1779652 -4.1756496 -4.1689348 -4.1728673 -4.1728549 -4.1876168 -4.2151179 -4.2525773][-4.2833261 -4.2585788 -4.2318606 -4.1989131 -4.1744671 -4.1577797 -4.1476121 -4.1364112 -4.1146722 -4.0961494 -4.1015882 -4.1087089 -4.1333737 -4.1703057 -4.2179227][-4.2860708 -4.2645059 -4.2350678 -4.1966324 -4.1633458 -4.1286159 -4.0956225 -4.0625854 -4.0223165 -3.9970031 -4.01049 -4.0271635 -4.0675597 -4.1210289 -4.18344][-4.2585669 -4.2303271 -4.1902308 -4.1405845 -4.0932121 -4.0397325 -3.9895182 -3.9435928 -3.8959343 -3.87424 -3.9004755 -3.9340065 -3.9967852 -4.0724554 -4.1540384][-4.1930795 -4.1497312 -4.0943594 -4.0306635 -3.9692345 -3.9038119 -3.8535874 -3.8143525 -3.7776871 -3.7761002 -3.8259237 -3.8841224 -3.9711168 -4.0644875 -4.1570015][-4.135983 -4.0783486 -4.0154614 -3.9510655 -3.8924417 -3.8383355 -3.8099854 -3.7953069 -3.784647 -3.8087349 -3.8729639 -3.9442527 -4.0352163 -4.1235342 -4.205565][-4.1594481 -4.1096697 -4.0609226 -4.0163536 -3.9780536 -3.9481616 -3.9430873 -3.9458814 -3.9529016 -3.9840903 -4.038332 -4.0970888 -4.165379 -4.2266545 -4.28058][-4.2460332 -4.2200637 -4.1959219 -4.1750207 -4.157414 -4.1444473 -4.1457591 -4.1511068 -4.1613717 -4.1843638 -4.2166824 -4.2521715 -4.2892413 -4.3192477 -4.3445449][-4.3216205 -4.3134522 -4.3071795 -4.3017325 -4.297174 -4.2931643 -4.2951455 -4.2995677 -4.3058133 -4.3164606 -4.3298516 -4.3440089 -4.3577743 -4.3676229 -4.3755708][-4.3561373 -4.3527527 -4.351881 -4.3516712 -4.3516183 -4.3519807 -4.3536005 -4.3574233 -4.3610554 -4.3656106 -4.3699474 -4.3739328 -4.3776889 -4.3800535 -4.3819361][-4.3681135 -4.3650274 -4.3644943 -4.3651042 -4.3665342 -4.3681517 -4.36955 -4.3722134 -4.3745904 -4.3765693 -4.3779078 -4.3785777 -4.3792849 -4.3798838 -4.3807282]]...]
INFO - root - 2017-12-05 16:46:41.033159: step 25910, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 71h:39m:19s remains)
INFO - root - 2017-12-05 16:46:49.555234: step 25920, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 74h:27m:17s remains)
INFO - root - 2017-12-05 16:46:58.074413: step 25930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 74h:05m:49s remains)
INFO - root - 2017-12-05 16:47:06.575033: step 25940, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 71h:29m:32s remains)
INFO - root - 2017-12-05 16:47:15.147584: step 25950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 73h:40m:58s remains)
INFO - root - 2017-12-05 16:47:23.839643: step 25960, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 70h:02m:20s remains)
INFO - root - 2017-12-05 16:47:32.328685: step 25970, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 71h:28m:02s remains)
INFO - root - 2017-12-05 16:47:40.615611: step 25980, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 73h:27m:16s remains)
INFO - root - 2017-12-05 16:47:49.214973: step 25990, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 71h:56m:27s remains)
INFO - root - 2017-12-05 16:47:57.676266: step 26000, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 69h:33m:29s remains)
2017-12-05 16:47:58.534705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1145263 -4.1301708 -4.1452084 -4.14185 -4.1320114 -4.1236892 -4.1248016 -4.1357951 -4.151175 -4.1622844 -4.1713104 -4.1733217 -4.1549292 -4.1242356 -4.0950837][-4.1563358 -4.1697817 -4.1824541 -4.1787796 -4.1662612 -4.1520343 -4.1442933 -4.1468606 -4.1554875 -4.1618485 -4.1691389 -4.1723013 -4.1546288 -4.1261554 -4.10014][-4.21496 -4.2240052 -4.2329497 -4.231473 -4.2244749 -4.2136717 -4.20269 -4.198173 -4.1984482 -4.1972528 -4.2008481 -4.2039375 -4.1887665 -4.164196 -4.1426][-4.2707696 -4.2754545 -4.2768726 -4.2744689 -4.272234 -4.2667837 -4.2581506 -4.2507844 -4.2454991 -4.2406464 -4.2433414 -4.2489176 -4.239048 -4.219233 -4.2031717][-4.302527 -4.3030329 -4.2956924 -4.2887483 -4.2867126 -4.2832003 -4.2773113 -4.2689915 -4.2609305 -4.2570581 -4.2646742 -4.2772827 -4.2744713 -4.2599654 -4.2484117][-4.2982812 -4.2906909 -4.2734594 -4.2584105 -4.2506328 -4.2457466 -4.2398825 -4.2290263 -4.221333 -4.2254038 -4.2432837 -4.2675996 -4.2746763 -4.2675314 -4.2588][-4.2827473 -4.2646728 -4.2366128 -4.2112088 -4.189714 -4.1700311 -4.1517143 -4.1327391 -4.1266155 -4.1448092 -4.1806483 -4.2212572 -4.2426963 -4.246789 -4.2438021][-4.2696438 -4.2424793 -4.2053843 -4.1676917 -4.1299524 -4.0886364 -4.0476136 -4.0162096 -4.009666 -4.0399966 -4.0936847 -4.148797 -4.1862626 -4.2095141 -4.2191281][-4.2712231 -4.2401571 -4.1993814 -4.1564054 -4.1146984 -4.0628085 -4.0053697 -3.9610572 -3.9468176 -3.9742918 -4.0336494 -4.0954113 -4.1402907 -4.1755037 -4.1968975][-4.289227 -4.2640414 -4.2278109 -4.1900749 -4.1583986 -4.1186514 -4.0736704 -4.0368114 -4.0174718 -4.0227242 -4.0589738 -4.1028152 -4.135129 -4.1627254 -4.1824422][-4.3093143 -4.2950706 -4.270659 -4.2448225 -4.223907 -4.2014723 -4.1789575 -4.1607852 -4.14433 -4.134 -4.1443739 -4.1612158 -4.168087 -4.1724916 -4.1793289][-4.3242941 -4.3195443 -4.3065486 -4.2917848 -4.2771974 -4.2633643 -4.2537203 -4.2461572 -4.2356367 -4.2232008 -4.2240348 -4.2250457 -4.209908 -4.1943007 -4.1885095][-4.3339367 -4.3336859 -4.32755 -4.3220191 -4.3121824 -4.2999635 -4.2930779 -4.2897987 -4.284369 -4.2762294 -4.2754331 -4.2690377 -4.2426963 -4.2179961 -4.2064071][-4.3347411 -4.3363457 -4.3360543 -4.336091 -4.331017 -4.3233018 -4.3175564 -4.314786 -4.3118949 -4.3071933 -4.3054514 -4.2949967 -4.267704 -4.2414441 -4.23035][-4.3188372 -4.3233304 -4.3278661 -4.3326411 -4.331717 -4.331141 -4.32951 -4.3251934 -4.3199944 -4.3158288 -4.312851 -4.3026013 -4.2797089 -4.2581086 -4.2509818]]...]
INFO - root - 2017-12-05 16:48:06.879679: step 26010, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.818 sec/batch; 69h:38m:44s remains)
INFO - root - 2017-12-05 16:48:15.436211: step 26020, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 74h:53m:19s remains)
INFO - root - 2017-12-05 16:48:24.023875: step 26030, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 72h:31m:33s remains)
INFO - root - 2017-12-05 16:48:32.563040: step 26040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:19m:34s remains)
INFO - root - 2017-12-05 16:48:41.160894: step 26050, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 71h:43m:43s remains)
INFO - root - 2017-12-05 16:48:49.716955: step 26060, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 72h:26m:52s remains)
INFO - root - 2017-12-05 16:48:58.186616: step 26070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 72h:53m:12s remains)
INFO - root - 2017-12-05 16:49:06.566941: step 26080, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 74h:24m:49s remains)
INFO - root - 2017-12-05 16:49:14.981496: step 26090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 72h:59m:06s remains)
INFO - root - 2017-12-05 16:49:23.433229: step 26100, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 71h:04m:52s remains)
2017-12-05 16:49:24.188921: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2645931 -4.2597332 -4.2575336 -4.2646327 -4.2722735 -4.2638874 -4.2382021 -4.215919 -4.1993542 -4.2016668 -4.2163224 -4.2272992 -4.2367067 -4.2277393 -4.19876][-4.2713194 -4.2645245 -4.2603269 -4.2658534 -4.2702107 -4.2544203 -4.223495 -4.1989555 -4.1828027 -4.1925578 -4.2172384 -4.2333903 -4.24243 -4.2318292 -4.2015743][-4.2732449 -4.2652707 -4.2606726 -4.2660136 -4.2640476 -4.2407012 -4.2088957 -4.1845894 -4.1749187 -4.1970506 -4.2308278 -4.24948 -4.25492 -4.24017 -4.2130728][-4.2749019 -4.26545 -4.2594452 -4.2628269 -4.2504611 -4.2169876 -4.1841741 -4.1634169 -4.1699009 -4.2076912 -4.2462974 -4.2641888 -4.2696252 -4.2560906 -4.2304144][-4.2660346 -4.2544274 -4.2470269 -4.2490005 -4.2271152 -4.1847 -4.14618 -4.1282253 -4.1553216 -4.20903 -4.2503667 -4.2645893 -4.272964 -4.2663927 -4.2434816][-4.2430367 -4.2292809 -4.2230024 -4.2246838 -4.1957812 -4.1464925 -4.0974536 -4.0757842 -4.1218152 -4.1943712 -4.239995 -4.2524052 -4.2615294 -4.2576342 -4.2358255][-4.217732 -4.2067361 -4.2036271 -4.2008348 -4.1623697 -4.1068068 -4.0412493 -4.0140181 -4.0829186 -4.1733971 -4.2258782 -4.2397938 -4.2496662 -4.2446 -4.2185125][-4.2097988 -4.2017174 -4.1967268 -4.1836977 -4.1351337 -4.0744963 -3.99044 -3.9611869 -4.0481172 -4.15089 -4.212687 -4.2318959 -4.2423744 -4.2305641 -4.19801][-4.2220922 -4.2107048 -4.198103 -4.1721621 -4.120965 -4.0607109 -3.9738717 -3.9563322 -4.04808 -4.1457472 -4.2109766 -4.2342114 -4.2422338 -4.2263832 -4.19049][-4.2400255 -4.2259493 -4.2103291 -4.1833715 -4.139287 -4.08665 -4.0135193 -4.010848 -4.0905495 -4.1693168 -4.2261276 -4.2489877 -4.25368 -4.2342124 -4.1983442][-4.2630148 -4.2488251 -4.2338 -4.2110257 -4.1745682 -4.133718 -4.0768018 -4.083499 -4.1439242 -4.2061396 -4.2523046 -4.2675695 -4.2658677 -4.2417917 -4.2043629][-4.2815437 -4.2680621 -4.2535462 -4.2332716 -4.2009683 -4.1628795 -4.1174531 -4.133512 -4.1886497 -4.2448554 -4.2778769 -4.2831354 -4.2769923 -4.2505217 -4.2133193][-4.288681 -4.2747836 -4.2603374 -4.2424664 -4.212934 -4.1756849 -4.1390772 -4.1627412 -4.2211385 -4.2707486 -4.2919745 -4.2913637 -4.2842593 -4.2592993 -4.2257][-4.2892294 -4.2721477 -4.2554135 -4.2395391 -4.2164278 -4.1849146 -4.159339 -4.1868792 -4.2407403 -4.2801995 -4.2936363 -4.2907181 -4.2838836 -4.259305 -4.2265725][-4.2888513 -4.271883 -4.2544003 -4.23937 -4.2216163 -4.1967645 -4.1820941 -4.2082572 -4.2518196 -4.2796173 -4.2843618 -4.2783141 -4.2715173 -4.2500939 -4.2173295]]...]
INFO - root - 2017-12-05 16:49:32.736016: step 26110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 73h:12m:14s remains)
INFO - root - 2017-12-05 16:49:41.161441: step 26120, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 71h:53m:05s remains)
INFO - root - 2017-12-05 16:49:49.670726: step 26130, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 69h:53m:23s remains)
INFO - root - 2017-12-05 16:49:58.097141: step 26140, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 70h:20m:28s remains)
INFO - root - 2017-12-05 16:50:06.608805: step 26150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 73h:13m:09s remains)
INFO - root - 2017-12-05 16:50:15.111884: step 26160, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 72h:32m:22s remains)
INFO - root - 2017-12-05 16:50:23.625811: step 26170, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 70h:14m:32s remains)
INFO - root - 2017-12-05 16:50:32.131678: step 26180, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 76h:16m:39s remains)
INFO - root - 2017-12-05 16:50:40.673766: step 26190, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 74h:48m:59s remains)
INFO - root - 2017-12-05 16:50:49.218102: step 26200, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:57m:03s remains)
2017-12-05 16:50:49.927852: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3062568 -4.2988434 -4.2927079 -4.2902346 -4.2950997 -4.3036709 -4.313921 -4.3229856 -4.329412 -4.328537 -4.3264108 -4.3270893 -4.3288741 -4.3315244 -4.3329668][-4.2998161 -4.2850666 -4.2702479 -4.2607527 -4.2632518 -4.2737422 -4.2869282 -4.2999821 -4.311594 -4.3132505 -4.314353 -4.319519 -4.3249645 -4.3302279 -4.3327689][-4.2805867 -4.2536049 -4.2276149 -4.2088509 -4.2043262 -4.2122273 -4.227746 -4.2488909 -4.2676644 -4.2706876 -4.2755122 -4.2872128 -4.2999063 -4.3129334 -4.3215685][-4.2557774 -4.2170777 -4.1802812 -4.1478906 -4.1324744 -4.1325607 -4.1492534 -4.1858621 -4.2171693 -4.2217689 -4.2270813 -4.2415228 -4.2574 -4.2753525 -4.292613][-4.230176 -4.1788826 -4.1286778 -4.0767355 -4.0395956 -4.0225539 -4.0363212 -4.0998011 -4.1593604 -4.1736617 -4.1799035 -4.1920352 -4.20401 -4.2218571 -4.24832][-4.2066822 -4.1451416 -4.0828834 -4.011024 -3.9453743 -3.8953295 -3.8922853 -3.9802036 -4.0800257 -4.12291 -4.1439319 -4.1548357 -4.1592369 -4.1742778 -4.2103987][-4.1828303 -4.1111517 -4.039454 -3.947437 -3.842598 -3.7409434 -3.7023544 -3.809001 -3.957366 -4.047328 -4.1005068 -4.1253324 -4.1317687 -4.1456976 -4.1866326][-4.1704926 -4.0901203 -4.0091429 -3.9014695 -3.7673478 -3.6187527 -3.5324726 -3.6393647 -3.83224 -3.9735596 -4.0611234 -4.1027675 -4.1191459 -4.1368251 -4.1792078][-4.1747761 -4.0923653 -4.0133162 -3.9173698 -3.8028481 -3.6698732 -3.5784781 -3.646421 -3.8105741 -3.9522352 -4.0489364 -4.0967708 -4.1206269 -4.141211 -4.1833558][-4.1950049 -4.1209702 -4.0503178 -3.9740267 -3.894233 -3.8060026 -3.739974 -3.766762 -3.8671989 -3.9765067 -4.0614214 -4.1104903 -4.1426687 -4.1668878 -4.2069058][-4.2214909 -4.1612515 -4.1025848 -4.0443621 -3.9909625 -3.9403069 -3.8986549 -3.9010551 -3.9534461 -4.0277596 -4.0933676 -4.1372237 -4.1718779 -4.2020254 -4.2407613][-4.2491703 -4.2054873 -4.1630182 -4.1239848 -4.0925894 -4.0647044 -4.0396791 -4.0316172 -4.054491 -4.1012697 -4.1490593 -4.1851425 -4.2146859 -4.2432337 -4.2759938][-4.277133 -4.2490821 -4.2220592 -4.19882 -4.1837668 -4.1735144 -4.1625466 -4.157269 -4.1678877 -4.1942267 -4.2239671 -4.2480097 -4.266964 -4.2883425 -4.3109641][-4.2985878 -4.2845974 -4.2719269 -4.2622547 -4.2572579 -4.2559943 -4.2518783 -4.2484236 -4.2534976 -4.2673717 -4.2842026 -4.2990541 -4.3112764 -4.3256335 -4.3389111][-4.3108363 -4.3043871 -4.2992325 -4.2951312 -4.29402 -4.2955747 -4.2961664 -4.2973809 -4.30227 -4.3104219 -4.3202763 -4.3294077 -4.3371258 -4.344882 -4.3501644]]...]
INFO - root - 2017-12-05 16:50:58.537067: step 26210, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 72h:55m:52s remains)
INFO - root - 2017-12-05 16:51:07.067398: step 26220, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 73h:43m:18s remains)
INFO - root - 2017-12-05 16:51:15.617780: step 26230, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 75h:10m:34s remains)
INFO - root - 2017-12-05 16:51:24.099861: step 26240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 73h:15m:27s remains)
INFO - root - 2017-12-05 16:51:32.691322: step 26250, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 74h:12m:57s remains)
INFO - root - 2017-12-05 16:51:41.187205: step 26260, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 72h:12m:22s remains)
INFO - root - 2017-12-05 16:51:49.794361: step 26270, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 73h:34m:44s remains)
INFO - root - 2017-12-05 16:51:58.225177: step 26280, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 71h:54m:09s remains)
INFO - root - 2017-12-05 16:52:06.859226: step 26290, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 72h:48m:39s remains)
INFO - root - 2017-12-05 16:52:15.419819: step 26300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:45m:47s remains)
2017-12-05 16:52:16.170641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1385131 -4.1643 -4.177433 -4.1719418 -4.1648817 -4.14959 -4.1243014 -4.1081109 -4.1157794 -4.1373844 -4.1630111 -4.1781569 -4.1800847 -4.1695647 -4.1531796][-4.1323156 -4.1562924 -4.1697807 -4.1670594 -4.164547 -4.1556058 -4.1315432 -4.115027 -4.1233377 -4.1497512 -4.179904 -4.1969738 -4.1968937 -4.186729 -4.1710176][-4.1193376 -4.1437645 -4.1610594 -4.1627779 -4.1637578 -4.1550388 -4.13075 -4.1135545 -4.1243458 -4.1550303 -4.1903596 -4.2136555 -4.2158961 -4.2088647 -4.1972685][-4.13787 -4.1691451 -4.1889167 -4.1859694 -4.1772532 -4.1569457 -4.1281209 -4.1086192 -4.1203165 -4.1554718 -4.199223 -4.2295651 -4.23737 -4.2362947 -4.2331862][-4.1786366 -4.2123041 -4.2270408 -4.2132058 -4.1910691 -4.1587753 -4.1200204 -4.0880551 -4.0935564 -4.1336412 -4.1908569 -4.232029 -4.2474303 -4.2579136 -4.2680416][-4.2287817 -4.2526979 -4.2527566 -4.2229342 -4.1819992 -4.1329656 -4.075882 -4.0223255 -4.0181346 -4.0702543 -4.1447649 -4.1965909 -4.21998 -4.2465715 -4.2705803][-4.2584677 -4.268117 -4.2520657 -4.2094955 -4.1563921 -4.0938015 -4.0157065 -3.935019 -3.9256959 -3.9932632 -4.0779066 -4.1360416 -4.1655383 -4.201261 -4.2337022][-4.2600527 -4.2618165 -4.2380733 -4.1915803 -4.1384478 -4.0715613 -3.9778817 -3.878746 -3.8627067 -3.9326618 -4.0153279 -4.0743208 -4.1087203 -4.1488142 -4.184648][-4.2477136 -4.2498755 -4.227623 -4.18345 -4.1347289 -4.073103 -3.9850392 -3.8867171 -3.8667791 -3.9309483 -4.0052123 -4.0553141 -4.0888329 -4.1222057 -4.1537685][-4.2357283 -4.2362881 -4.2113161 -4.167305 -4.11734 -4.0674329 -3.9961894 -3.9140522 -3.9091196 -3.9806547 -4.0464563 -4.0821748 -4.107832 -4.1273432 -4.1472616][-4.2195449 -4.2177567 -4.1923313 -4.1521616 -4.1083555 -4.0648785 -4.009058 -3.9528339 -3.9628706 -4.0358591 -4.0890222 -4.1084671 -4.1225705 -4.1288962 -4.138957][-4.2010436 -4.2017174 -4.1830959 -4.1559134 -4.1233263 -4.0866232 -4.04685 -4.0159407 -4.030148 -4.0847745 -4.1158323 -4.119967 -4.1253071 -4.1297026 -4.1390696][-4.1953697 -4.20033 -4.1910748 -4.1787057 -4.1531296 -4.1198 -4.0907664 -4.0769053 -4.0868959 -4.1142588 -4.1215 -4.1211438 -4.1291533 -4.1435242 -4.1574388][-4.1982646 -4.2096157 -4.2061038 -4.2017636 -4.1811771 -4.15308 -4.1333961 -4.1273332 -4.1255865 -4.1247416 -4.1201744 -4.1245422 -4.1406169 -4.1628933 -4.1815486][-4.2020564 -4.2145371 -4.21363 -4.2112741 -4.1914 -4.1631584 -4.1476126 -4.1443191 -4.1349826 -4.1172867 -4.1143775 -4.1320624 -4.15703 -4.1820693 -4.2023816]]...]
INFO - root - 2017-12-05 16:52:24.812802: step 26310, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 72h:55m:46s remains)
INFO - root - 2017-12-05 16:52:33.271608: step 26320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 73h:19m:15s remains)
INFO - root - 2017-12-05 16:52:41.868405: step 26330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 74h:12m:40s remains)
INFO - root - 2017-12-05 16:52:50.327057: step 26340, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 72h:53m:44s remains)
INFO - root - 2017-12-05 16:52:59.016291: step 26350, loss = 2.03, batch loss = 1.98 (8.8 examples/sec; 0.905 sec/batch; 76h:57m:06s remains)
INFO - root - 2017-12-05 16:53:07.592450: step 26360, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:41m:55s remains)
INFO - root - 2017-12-05 16:53:16.246470: step 26370, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 73h:56m:24s remains)
INFO - root - 2017-12-05 16:53:24.844199: step 26380, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 74h:05m:51s remains)
INFO - root - 2017-12-05 16:53:33.413501: step 26390, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 71h:05m:13s remains)
INFO - root - 2017-12-05 16:53:42.064137: step 26400, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 71h:43m:26s remains)
2017-12-05 16:53:42.873931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.32068 -4.317996 -4.3131161 -4.3089561 -4.3078914 -4.3147678 -4.3266034 -4.3360848 -4.3410115 -4.3411837 -4.3354959 -4.3262296 -4.3156104 -4.3085647 -4.306972][-4.3185229 -4.3090229 -4.2972631 -4.2892838 -4.28852 -4.2985554 -4.3167706 -4.3336945 -4.3438764 -4.3481412 -4.3454657 -4.3372927 -4.32795 -4.3210297 -4.3187485][-4.3139353 -4.29272 -4.2681022 -4.2504783 -4.2462339 -4.2595572 -4.2858686 -4.3127694 -4.3321614 -4.34464 -4.3479052 -4.3428717 -4.3356023 -4.3295941 -4.3273511][-4.3094091 -4.2747555 -4.2323666 -4.1981697 -4.1855245 -4.2003603 -4.2330847 -4.2692275 -4.3018212 -4.3286028 -4.3438067 -4.3463268 -4.343348 -4.3391156 -4.3362846][-4.3078103 -4.2640033 -4.2049961 -4.15034 -4.1219363 -4.1295834 -4.16024 -4.2000914 -4.2473865 -4.2937031 -4.326158 -4.3424869 -4.34949 -4.3509612 -4.3490286][-4.3080764 -4.2636366 -4.1966352 -4.1253247 -4.0757995 -4.0627918 -4.0747757 -4.1080604 -4.1702538 -4.2402191 -4.2923961 -4.3250022 -4.3453555 -4.3562107 -4.3583884][-4.3085485 -4.2716722 -4.2087774 -4.130971 -4.0612788 -4.0153842 -3.9892764 -4.0010157 -4.0744042 -4.1696453 -4.2426491 -4.2920408 -4.326684 -4.3486714 -4.3581777][-4.3109283 -4.2865477 -4.2366362 -4.1638923 -4.0837059 -4.0055637 -3.9324098 -3.9088321 -3.9812841 -4.0916457 -4.180953 -4.2467842 -4.2967906 -4.3308969 -4.3491435][-4.3143692 -4.3024378 -4.2688923 -4.2116084 -4.1369529 -4.0451312 -3.9438157 -3.8866804 -3.9340568 -4.0324082 -4.1229382 -4.1974244 -4.2590103 -4.3047795 -4.3326516][-4.3165669 -4.3139887 -4.2957239 -4.2588325 -4.20277 -4.1210294 -4.021224 -3.9512463 -3.9609392 -4.0223632 -4.0936623 -4.1621914 -4.2243218 -4.2763333 -4.3124528][-4.3173089 -4.3199124 -4.3120885 -4.2929749 -4.2586808 -4.1999917 -4.1215487 -4.055675 -4.037261 -4.0584679 -4.0995951 -4.1494479 -4.2013822 -4.25212 -4.292089][-4.316885 -4.3221078 -4.3207855 -4.3140755 -4.2978606 -4.2645526 -4.2149506 -4.1643553 -4.1342278 -4.1275382 -4.1405592 -4.1652346 -4.1986561 -4.2402878 -4.2775636][-4.3178511 -4.3244638 -4.3259144 -4.3244061 -4.3190784 -4.3066096 -4.2837496 -4.2538862 -4.2277327 -4.2114019 -4.2063508 -4.2082925 -4.2207932 -4.2476678 -4.27585][-4.319519 -4.3290319 -4.3321419 -4.3305564 -4.3280363 -4.3258505 -4.3196836 -4.3070192 -4.2930484 -4.2808156 -4.2715592 -4.2627759 -4.2613239 -4.2735085 -4.2892303][-4.3155689 -4.3284893 -4.33278 -4.3301787 -4.327105 -4.3267822 -4.3267312 -4.3241415 -4.32094 -4.3171477 -4.3119264 -4.3039804 -4.2991047 -4.3022394 -4.3078265]]...]
INFO - root - 2017-12-05 16:53:51.264569: step 26410, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 74h:47m:14s remains)
INFO - root - 2017-12-05 16:53:59.913892: step 26420, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 74h:06m:36s remains)
INFO - root - 2017-12-05 16:54:08.558469: step 26430, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.797 sec/batch; 67h:45m:55s remains)
INFO - root - 2017-12-05 16:54:17.131069: step 26440, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 71h:42m:00s remains)
INFO - root - 2017-12-05 16:54:25.665116: step 26450, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 74h:35m:49s remains)
INFO - root - 2017-12-05 16:54:34.177496: step 26460, loss = 2.07, batch loss = 2.02 (10.0 examples/sec; 0.799 sec/batch; 67h:56m:56s remains)
INFO - root - 2017-12-05 16:54:42.728357: step 26470, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 73h:41m:59s remains)
INFO - root - 2017-12-05 16:54:51.049105: step 26480, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 70h:11m:21s remains)
INFO - root - 2017-12-05 16:54:59.610897: step 26490, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 74h:14m:04s remains)
INFO - root - 2017-12-05 16:55:08.198140: step 26500, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 72h:23m:40s remains)
2017-12-05 16:55:08.919898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3547454 -4.35483 -4.3534122 -4.3515019 -4.349659 -4.3480377 -4.3460078 -4.3441377 -4.3438344 -4.3456645 -4.3486638 -4.3512645 -4.3533506 -4.3544517 -4.3536372][-4.3483377 -4.3456111 -4.340693 -4.3360019 -4.33308 -4.3303776 -4.3262291 -4.3216205 -4.321486 -4.3274431 -4.336875 -4.346107 -4.3526869 -4.3553057 -4.354249][-4.3407173 -4.3351269 -4.32665 -4.3184791 -4.3130956 -4.3070149 -4.2969818 -4.2859874 -4.2838244 -4.2947803 -4.3148355 -4.335649 -4.35115 -4.3584075 -4.3580608][-4.3333373 -4.3232169 -4.30942 -4.2958918 -4.2862053 -4.2731671 -4.2518792 -4.231607 -4.2276335 -4.2470388 -4.2827067 -4.3197527 -4.3486814 -4.3631458 -4.36421][-4.3248305 -4.3078356 -4.285624 -4.2634559 -4.24537 -4.2207294 -4.1833234 -4.1492715 -4.1444349 -4.1779633 -4.2354965 -4.2942824 -4.3396735 -4.3641205 -4.3685231][-4.3139348 -4.2885423 -4.2552938 -4.2214603 -4.1918163 -4.1506677 -4.0937495 -4.0462251 -4.0465279 -4.1017413 -4.18268 -4.26366 -4.3241277 -4.3592587 -4.3690863][-4.3056631 -4.27423 -4.2324929 -4.1873112 -4.1437459 -4.0837111 -4.0038176 -3.9434631 -3.9537892 -4.0336771 -4.136827 -4.234808 -4.3061495 -4.3495817 -4.3661013][-4.3010077 -4.2675204 -4.22446 -4.1727843 -4.1141019 -4.0351934 -3.9356465 -3.864768 -3.8831 -3.9806838 -4.1007223 -4.2100315 -4.2883444 -4.3374982 -4.3603692][-4.2970386 -4.2653317 -4.2261391 -4.1762052 -4.1097713 -4.0226908 -3.9210131 -3.8504963 -3.8663292 -3.9636869 -4.0840778 -4.1931829 -4.2725768 -4.3264079 -4.3542132][-4.2893982 -4.2594175 -4.2265763 -4.186553 -4.1284442 -4.0525951 -3.9739642 -3.9194369 -3.9295917 -4.0059795 -4.10607 -4.2021041 -4.2756748 -4.3273797 -4.3542905][-4.2691436 -4.2408805 -4.2146273 -4.191298 -4.157104 -4.1092081 -4.0635462 -4.034 -4.0419478 -4.0939617 -4.1664152 -4.2409167 -4.3010139 -4.3411479 -4.3613667][-4.2275743 -4.2051082 -4.192759 -4.1934 -4.1900444 -4.1764035 -4.1630211 -4.1561217 -4.1654067 -4.1982522 -4.2454758 -4.2960205 -4.3363132 -4.3608894 -4.3706579][-4.1759491 -4.1696668 -4.1823745 -4.21072 -4.2350159 -4.2471738 -4.2517133 -4.2528276 -4.257349 -4.27612 -4.3062472 -4.3374977 -4.3629088 -4.3754592 -4.3766937][-4.1326604 -4.153553 -4.1959195 -4.2439489 -4.2806535 -4.3011665 -4.3062139 -4.3032341 -4.2994375 -4.3078437 -4.3295689 -4.3522196 -4.3698792 -4.376852 -4.3754539][-4.119401 -4.1694403 -4.2313123 -4.2849884 -4.319345 -4.3349705 -4.3332763 -4.3190584 -4.3049736 -4.3032327 -4.3165808 -4.3348446 -4.3507681 -4.3596063 -4.3628716]]...]
INFO - root - 2017-12-05 16:55:17.516480: step 26510, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 71h:48m:37s remains)
INFO - root - 2017-12-05 16:55:26.068138: step 26520, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 73h:58m:58s remains)
INFO - root - 2017-12-05 16:55:34.599139: step 26530, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 71h:54m:24s remains)
INFO - root - 2017-12-05 16:55:43.148478: step 26540, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 71h:37m:55s remains)
INFO - root - 2017-12-05 16:55:51.732292: step 26550, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 74h:03m:28s remains)
INFO - root - 2017-12-05 16:56:00.293242: step 26560, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 73h:12m:49s remains)
INFO - root - 2017-12-05 16:56:08.928628: step 26570, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 72h:42m:57s remains)
INFO - root - 2017-12-05 16:56:17.463626: step 26580, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 73h:52m:51s remains)
INFO - root - 2017-12-05 16:56:26.052728: step 26590, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 75h:20m:23s remains)
INFO - root - 2017-12-05 16:56:34.551032: step 26600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:09m:05s remains)
2017-12-05 16:56:35.325543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2671671 -4.2608957 -4.2504458 -4.234633 -4.21576 -4.2011056 -4.2049952 -4.2251215 -4.2391405 -4.2338028 -4.2222528 -4.2171283 -4.2215734 -4.2305293 -4.2329531][-4.2532253 -4.2408147 -4.2200313 -4.1903334 -4.16202 -4.1471324 -4.1572466 -4.1883368 -4.2083745 -4.2057166 -4.1958318 -4.1898532 -4.1881046 -4.19229 -4.1953425][-4.2259684 -4.2045007 -4.1743135 -4.1318469 -4.0976787 -4.0856948 -4.1071534 -4.1485815 -4.1708536 -4.1684031 -4.1595888 -4.1511536 -4.1432304 -4.144412 -4.1542797][-4.1885405 -4.1600084 -4.1255722 -4.0783386 -4.0429564 -4.0337782 -4.0615468 -4.1079459 -4.1295953 -4.125598 -4.11805 -4.1061444 -4.0951066 -4.1008825 -4.1225309][-4.1554508 -4.1232462 -4.0891442 -4.043076 -4.0089045 -3.9994779 -4.02224 -4.060616 -4.0724463 -4.0638733 -4.0554447 -4.0457091 -4.0438766 -4.06712 -4.1042337][-4.144546 -4.1122231 -4.0816875 -4.03552 -3.9938805 -3.9742732 -3.9789381 -3.9966483 -3.990078 -3.9778931 -3.9775238 -3.9820964 -4.0045266 -4.0495028 -4.0922403][-4.1518197 -4.1226749 -4.0936079 -4.0437207 -3.9876864 -3.9502528 -3.9335632 -3.9231141 -3.8960888 -3.8877053 -3.9082408 -3.9371681 -3.9821665 -4.0347638 -4.0719504][-4.1633325 -4.1398635 -4.1117215 -4.0590277 -3.9925685 -3.9407306 -3.896872 -3.8491068 -3.8110521 -3.8298059 -3.8798795 -3.9287813 -3.9798818 -4.0240536 -4.048408][-4.17431 -4.1561127 -4.1255689 -4.072053 -4.0076733 -3.9485238 -3.8820698 -3.8181353 -3.7988322 -3.8432508 -3.9038486 -3.9574606 -4.0027466 -4.03011 -4.0416851][-4.180563 -4.1648436 -4.1335363 -4.0820856 -4.0226278 -3.9616213 -3.8969347 -3.8605356 -3.8741207 -3.9138196 -3.95295 -3.9939501 -4.0327749 -4.0555344 -4.0609155][-4.1846275 -4.1666579 -4.13369 -4.0847654 -4.03076 -3.9781985 -3.939564 -3.9437683 -3.9737816 -3.9954698 -4.0076046 -4.0325775 -4.0696316 -4.096293 -4.1022053][-4.1838484 -4.1583409 -4.1220717 -4.0780916 -4.0349145 -3.9979725 -3.98624 -4.0119753 -4.0412588 -4.0524259 -4.0550961 -4.0746007 -4.1070266 -4.1315069 -4.1390905][-4.1783705 -4.1481218 -4.1100655 -4.0717854 -4.0388861 -4.0168028 -4.020421 -4.0492191 -4.0741453 -4.0807743 -4.0865612 -4.1088605 -4.1361504 -4.1522326 -4.1569276][-4.1838565 -4.1542354 -4.1194296 -4.0863953 -4.0603237 -4.0455713 -4.0528541 -4.075881 -4.0978847 -4.1083512 -4.1189928 -4.1407232 -4.1615748 -4.1687727 -4.1682191][-4.2120686 -4.1886463 -4.161664 -4.1366796 -4.1167922 -4.1077352 -4.1135025 -4.1298995 -4.1474128 -4.1591916 -4.1684537 -4.1823339 -4.1954627 -4.1994538 -4.1973124]]...]
INFO - root - 2017-12-05 16:56:43.802584: step 26610, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 71h:09m:00s remains)
INFO - root - 2017-12-05 16:56:52.300208: step 26620, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 71h:54m:21s remains)
INFO - root - 2017-12-05 16:57:00.895745: step 26630, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 72h:53m:31s remains)
INFO - root - 2017-12-05 16:57:09.539382: step 26640, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.897 sec/batch; 76h:11m:33s remains)
INFO - root - 2017-12-05 16:57:18.120272: step 26650, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:37m:27s remains)
INFO - root - 2017-12-05 16:57:26.659783: step 26660, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:49m:06s remains)
INFO - root - 2017-12-05 16:57:35.134304: step 26670, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 72h:30m:51s remains)
INFO - root - 2017-12-05 16:57:43.713876: step 26680, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 74h:39m:48s remains)
INFO - root - 2017-12-05 16:57:52.226350: step 26690, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 73h:52m:01s remains)
INFO - root - 2017-12-05 16:58:00.902920: step 26700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 73h:42m:17s remains)
2017-12-05 16:58:01.660502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2626371 -4.2657309 -4.2685409 -4.2640533 -4.2582526 -4.264945 -4.2723894 -4.2709622 -4.263618 -4.2578087 -4.2586765 -4.2619328 -4.2618718 -4.2544446 -4.2381587][-4.238503 -4.2473707 -4.2512436 -4.2448511 -4.235858 -4.241322 -4.24841 -4.2409372 -4.2235703 -4.2096567 -4.2113914 -4.2229152 -4.2274947 -4.2182121 -4.2025895][-4.210886 -4.2258158 -4.2311964 -4.2257409 -4.21542 -4.216476 -4.2181134 -4.1993914 -4.1684546 -4.1479564 -4.1563234 -4.1819992 -4.1927128 -4.1846075 -4.1775856][-4.1871486 -4.2044115 -4.2070484 -4.198586 -4.1886387 -4.1838894 -4.17381 -4.1429324 -4.1019483 -4.0821638 -4.1061826 -4.1490822 -4.1676254 -4.1609521 -4.1631594][-4.1726947 -4.1841478 -4.1816511 -4.169404 -4.1586823 -4.143086 -4.1148276 -4.066587 -4.0174065 -4.0153642 -4.0694346 -4.1277676 -4.1487875 -4.1439328 -4.1504288][-4.1589389 -4.1659851 -4.1590433 -4.1412277 -4.1263275 -4.0952005 -4.0414019 -3.9630058 -3.9042776 -3.9427917 -4.04129 -4.1152592 -4.1401916 -4.1375895 -4.1426253][-4.1351576 -4.13913 -4.1325893 -4.113308 -4.0952196 -4.0565224 -3.9788051 -3.8616116 -3.7911904 -3.8847556 -4.0267811 -4.1130047 -4.1454463 -4.1429234 -4.1412592][-4.1151118 -4.1157389 -4.1061754 -4.0907903 -4.0827413 -4.0515761 -3.9699087 -3.8373933 -3.7719331 -3.8968873 -4.0438476 -4.1223741 -4.1518455 -4.1446753 -4.1336627][-4.111557 -4.1131592 -4.1004677 -4.0918365 -4.098979 -4.0811343 -4.01667 -3.9175148 -3.8892505 -3.9945555 -4.1039176 -4.1566896 -4.1719661 -4.1577649 -4.1359749][-4.1368222 -4.1397767 -4.1252089 -4.117023 -4.1278296 -4.1144991 -4.0656772 -4.0039959 -4.0032673 -4.0848894 -4.1605263 -4.1950436 -4.2002573 -4.1821842 -4.1531248][-4.1650853 -4.1646452 -4.1557169 -4.1502028 -4.1632757 -4.15015 -4.1070547 -4.0653887 -4.0766969 -4.1415815 -4.1977649 -4.2244239 -4.2291641 -4.2146964 -4.18327][-4.1787171 -4.1806688 -4.1850338 -4.1883311 -4.1998234 -4.1853542 -4.145071 -4.1156807 -4.1316442 -4.1797075 -4.2194123 -4.2425137 -4.250515 -4.2413812 -4.2104688][-4.1912022 -4.2015648 -4.2171612 -4.226018 -4.2325764 -4.2184772 -4.187604 -4.1706858 -4.1829352 -4.2083297 -4.231307 -4.2498922 -4.2600846 -4.2555904 -4.2302847][-4.2124529 -4.2262988 -4.2451253 -4.2536087 -4.2553959 -4.2405357 -4.2211537 -4.2161827 -4.2206416 -4.226367 -4.2381682 -4.2534809 -4.2643676 -4.2617717 -4.2438936][-4.2443576 -4.2549787 -4.269043 -4.2758346 -4.2755241 -4.2640204 -4.2533736 -4.2522054 -4.2521129 -4.2508044 -4.25817 -4.2712669 -4.2802081 -4.27972 -4.2682996]]...]
INFO - root - 2017-12-05 16:58:10.191406: step 26710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 73h:31m:49s remains)
INFO - root - 2017-12-05 16:58:18.837409: step 26720, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 72h:21m:15s remains)
INFO - root - 2017-12-05 16:58:27.416876: step 26730, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 72h:53m:03s remains)
INFO - root - 2017-12-05 16:58:35.957044: step 26740, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 74h:31m:24s remains)
INFO - root - 2017-12-05 16:58:44.580193: step 26750, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 73h:52m:16s remains)
INFO - root - 2017-12-05 16:58:53.119712: step 26760, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 72h:24m:51s remains)
INFO - root - 2017-12-05 16:59:01.683067: step 26770, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:46m:13s remains)
INFO - root - 2017-12-05 16:59:10.100700: step 26780, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 71h:26m:15s remains)
INFO - root - 2017-12-05 16:59:18.665353: step 26790, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 75h:52m:39s remains)
INFO - root - 2017-12-05 16:59:27.276585: step 26800, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 71h:41m:18s remains)
2017-12-05 16:59:28.059064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2827749 -4.2397432 -4.2008896 -4.1709995 -4.142138 -4.1089082 -4.07049 -4.0879927 -4.1276383 -4.154036 -4.1648107 -4.1561065 -4.1334853 -4.0879664 -4.0394263][-4.2827859 -4.2374477 -4.1939216 -4.16117 -4.1302557 -4.0893612 -4.0468435 -4.063211 -4.0962296 -4.1205897 -4.1334071 -4.1247244 -4.1067595 -4.0690112 -4.0275784][-4.2793703 -4.2341509 -4.1867633 -4.1508312 -4.11441 -4.0589132 -4.00968 -4.0266986 -4.0510979 -4.0739484 -4.0932245 -4.08698 -4.0800238 -4.0562744 -4.026648][-4.2737513 -4.22719 -4.1735582 -4.1282258 -4.0806484 -4.0064597 -3.9504204 -3.972496 -3.9973645 -4.0284982 -4.0609035 -4.0587554 -4.0599661 -4.0503149 -4.0350895][-4.26906 -4.2194195 -4.1595168 -4.1039696 -4.0467687 -3.9526091 -3.8779283 -3.9086854 -3.94842 -3.996912 -4.0466619 -4.0467544 -4.0407515 -4.03718 -4.0361047][-4.262958 -4.2116647 -4.1478915 -4.0817537 -4.0079308 -3.8812182 -3.7647705 -3.8165987 -3.9045377 -3.9914532 -4.0551963 -4.0596542 -4.04465 -4.0380869 -4.0427217][-4.2587323 -4.207273 -4.1421962 -4.0656834 -3.9656267 -3.797657 -3.6368251 -3.7314219 -3.8844652 -4.00422 -4.0739794 -4.085474 -4.0730138 -4.0679793 -4.0756774][-4.2578177 -4.2041831 -4.13638 -4.0529833 -3.9469087 -3.7873204 -3.6563268 -3.7668748 -3.9293306 -4.0428805 -4.1040878 -4.1192961 -4.1156359 -4.1118774 -4.116241][-4.2585187 -4.2012634 -4.1321115 -4.0509424 -3.9615064 -3.8530369 -3.7777917 -3.8648562 -3.9955535 -4.0896564 -4.1448603 -4.1636586 -4.1666617 -4.1586742 -4.1561465][-4.2617874 -4.20645 -4.1436605 -4.0738277 -4.0034103 -3.9308946 -3.887955 -3.9581218 -4.0678773 -4.1478539 -4.1931171 -4.2091365 -4.2148023 -4.2054014 -4.1972661][-4.2638721 -4.2152085 -4.1618485 -4.1023517 -4.0385652 -3.9792385 -3.9570496 -4.02739 -4.1242476 -4.1874566 -4.2240963 -4.2420912 -4.2503943 -4.2407455 -4.226656][-4.2647839 -4.221735 -4.1736326 -4.1206923 -4.0642323 -4.0142665 -4.0033269 -4.0748262 -4.1586404 -4.2075052 -4.2398491 -4.2586379 -4.2630205 -4.2506418 -4.2316289][-4.2652826 -4.2281079 -4.1829715 -4.1353407 -4.0894909 -4.0470409 -4.0380793 -4.1003065 -4.1686654 -4.2048759 -4.2305489 -4.24556 -4.2494097 -4.2380772 -4.216289][-4.2656512 -4.2332053 -4.1907043 -4.1437845 -4.0990272 -4.0524535 -4.0359149 -4.0900512 -4.1487417 -4.1771874 -4.1938076 -4.2043757 -4.2126274 -4.2072735 -4.186553][-4.2677279 -4.2364316 -4.193038 -4.1398869 -4.0864868 -4.0326715 -4.0107942 -4.0631413 -4.1214967 -4.1474595 -4.158987 -4.1650991 -4.1742735 -4.1737065 -4.151948]]...]
INFO - root - 2017-12-05 16:59:36.680918: step 26810, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.833 sec/batch; 70h:43m:09s remains)
INFO - root - 2017-12-05 16:59:45.282114: step 26820, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 73h:29m:18s remains)
INFO - root - 2017-12-05 16:59:53.746049: step 26830, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 73h:23m:49s remains)
INFO - root - 2017-12-05 17:00:02.270692: step 26840, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 73h:29m:40s remains)
INFO - root - 2017-12-05 17:00:10.822307: step 26850, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 72h:10m:22s remains)
INFO - root - 2017-12-05 17:00:19.383587: step 26860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 71h:28m:54s remains)
INFO - root - 2017-12-05 17:00:27.942573: step 26870, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:54m:56s remains)
INFO - root - 2017-12-05 17:00:36.559365: step 26880, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 75h:31m:57s remains)
INFO - root - 2017-12-05 17:00:45.034710: step 26890, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 74h:06m:19s remains)
INFO - root - 2017-12-05 17:00:53.548737: step 26900, loss = 2.07, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 69h:52m:39s remains)
2017-12-05 17:00:54.338457: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2729173 -4.2734952 -4.273108 -4.2728415 -4.2723837 -4.2713418 -4.2698288 -4.2688084 -4.2685423 -4.2685852 -4.2688766 -4.2692604 -4.2688913 -4.2671628 -4.264607][-4.3065643 -4.30805 -4.3090405 -4.3098054 -4.3099813 -4.3092685 -4.3080883 -4.3076639 -4.308516 -4.3101511 -4.3122945 -4.3148136 -4.3159933 -4.3149385 -4.3120384][-4.3156323 -4.3183322 -4.3207135 -4.3218594 -4.3209286 -4.3186626 -4.3161922 -4.3154488 -4.3167152 -4.3204865 -4.3256965 -4.331461 -4.3356771 -4.3372245 -4.3358259][-4.29341 -4.2971931 -4.3007994 -4.3015695 -4.2973886 -4.291204 -4.2854557 -4.2819786 -4.2828984 -4.2886696 -4.2965455 -4.3045 -4.3120389 -4.3170857 -4.3180742][-4.2587643 -4.2657704 -4.2703333 -4.2690458 -4.2596288 -4.2454295 -4.2325273 -4.2257385 -4.2269039 -4.2373762 -4.2515497 -4.2636037 -4.2741494 -4.2822013 -4.2847586][-4.2007461 -4.2127004 -4.2190657 -4.2138629 -4.1959662 -4.1701736 -4.1463451 -4.1340528 -4.1355295 -4.1549931 -4.1824107 -4.205801 -4.2234406 -4.2346234 -4.2370796][-4.1061296 -4.122016 -4.1309786 -4.1202507 -4.0907469 -4.0487461 -4.0081363 -3.9834757 -3.9830542 -4.01616 -4.064867 -4.1067233 -4.138381 -4.1575546 -4.1640186][-4.0187173 -4.0383053 -4.052249 -4.0413241 -4.0046492 -3.9481988 -3.8874078 -3.84153 -3.8308685 -3.8761904 -3.9433355 -3.9984105 -4.0386267 -4.0646229 -4.0776081][-4.0271707 -4.0442624 -4.06001 -4.0541463 -4.0226574 -3.9676108 -3.9046967 -3.8538158 -3.8387723 -3.8806174 -3.9423609 -3.9904475 -4.0225873 -4.0392194 -4.0459366][-4.1071982 -4.1186571 -4.1312361 -4.1322837 -4.1149745 -4.07928 -4.0375738 -4.004797 -3.9974537 -4.0254164 -4.0640268 -4.0903668 -4.1015296 -4.0968471 -4.0869484][-4.1904378 -4.1957955 -4.2005744 -4.2011876 -4.1927371 -4.1746292 -4.1562138 -4.144721 -4.1460848 -4.16401 -4.1827655 -4.1901321 -4.1840692 -4.1643286 -4.1442952][-4.2412391 -4.2439818 -4.2440414 -4.2434907 -4.2392273 -4.2315326 -4.2264738 -4.2283669 -4.2367129 -4.2506986 -4.2596478 -4.2574453 -4.2439322 -4.2205462 -4.2001491][-4.2539983 -4.2560911 -4.2542839 -4.2533603 -4.2510266 -4.2477827 -4.2476416 -4.2552505 -4.266912 -4.2791348 -4.2844105 -4.2808342 -4.2661295 -4.2439041 -4.2253313][-4.2476673 -4.2475233 -4.24495 -4.2424335 -4.2374344 -4.2335935 -4.2351165 -4.2434506 -4.2531528 -4.2631288 -4.2682452 -4.2687922 -4.26138 -4.2479143 -4.2366958][-4.2454429 -4.2374535 -4.22836 -4.2184482 -4.2061467 -4.1987152 -4.2001519 -4.2064619 -4.2120261 -4.2182879 -4.2230825 -4.2302155 -4.2357874 -4.2401705 -4.2444754]]...]
INFO - root - 2017-12-05 17:01:02.869693: step 26910, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 72h:37m:04s remains)
INFO - root - 2017-12-05 17:01:11.395821: step 26920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 72h:03m:54s remains)
INFO - root - 2017-12-05 17:01:19.929451: step 26930, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 73h:36m:01s remains)
INFO - root - 2017-12-05 17:01:28.571978: step 26940, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 74h:32m:06s remains)
INFO - root - 2017-12-05 17:01:36.989850: step 26950, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 70h:47m:33s remains)
INFO - root - 2017-12-05 17:01:45.580747: step 26960, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 72h:37m:34s remains)
INFO - root - 2017-12-05 17:01:54.220361: step 26970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 73h:20m:51s remains)
INFO - root - 2017-12-05 17:02:02.608463: step 26980, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 71h:57m:13s remains)
INFO - root - 2017-12-05 17:02:11.275045: step 26990, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 71h:46m:03s remains)
INFO - root - 2017-12-05 17:02:19.673262: step 27000, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 73h:12m:55s remains)
2017-12-05 17:02:20.470686: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2692437 -4.2490716 -4.241868 -4.2373061 -4.2443051 -4.2511592 -4.2520733 -4.2484493 -4.2487335 -4.2511153 -4.2517695 -4.258934 -4.2648535 -4.2676373 -4.2760954][-4.2524486 -4.2353582 -4.2317977 -4.2223463 -4.2223654 -4.2255917 -4.2212391 -4.2174578 -4.21891 -4.2226658 -4.2265115 -4.2396212 -4.2474909 -4.2479796 -4.255549][-4.2544093 -4.2429805 -4.2385492 -4.2182021 -4.2021017 -4.1929221 -4.184103 -4.1816492 -4.1887655 -4.2018065 -4.2169108 -4.2409797 -4.25001 -4.2471094 -4.2507463][-4.2677259 -4.2566833 -4.2461553 -4.2139869 -4.1838245 -4.1645613 -4.153327 -4.1550441 -4.168191 -4.1938691 -4.22152 -4.2527294 -4.2651114 -4.2617192 -4.2576656][-4.2728462 -4.2594004 -4.2460928 -4.2093968 -4.168088 -4.1328764 -4.1158614 -4.1206174 -4.1481938 -4.1863322 -4.2179413 -4.248374 -4.2683482 -4.2715726 -4.2628536][-4.2775426 -4.2633352 -4.2471471 -4.2088461 -4.1515789 -4.085114 -4.0428467 -4.0423861 -4.0925665 -4.14789 -4.1810308 -4.2110348 -4.2391906 -4.254559 -4.2514539][-4.2788458 -4.260778 -4.2347131 -4.1911635 -4.12067 -4.020565 -3.9271789 -3.9059148 -3.988102 -4.0764523 -4.1206741 -4.1497483 -4.1824875 -4.2111874 -4.2241488][-4.2729197 -4.2463841 -4.206152 -4.1535516 -4.079586 -3.9553046 -3.8031435 -3.743016 -3.8662891 -4.0047421 -4.0741415 -4.1114159 -4.1419511 -4.1774836 -4.2040505][-4.2713618 -4.2395039 -4.1905255 -4.1372781 -4.0784569 -3.9700165 -3.8260229 -3.7576146 -3.8633559 -3.9999597 -4.0806141 -4.1247706 -4.1474056 -4.1753669 -4.20266][-4.2739558 -4.2473655 -4.2075877 -4.167315 -4.1323752 -4.0716672 -3.9912071 -3.9425616 -3.984699 -4.0689263 -4.1326571 -4.1671457 -4.177753 -4.1943827 -4.2131748][-4.2758074 -4.2642922 -4.244616 -4.2164726 -4.1943021 -4.1626983 -4.124836 -4.0888877 -4.0898318 -4.1227484 -4.1584377 -4.1804419 -4.1875443 -4.2018423 -4.2174234][-4.2796292 -4.2816892 -4.2744269 -4.2502866 -4.2288556 -4.2081113 -4.1883726 -4.16308 -4.1484118 -4.1450305 -4.1539893 -4.172576 -4.186295 -4.2069225 -4.2259684][-4.2836547 -4.2922926 -4.2912965 -4.2682104 -4.2401633 -4.2190576 -4.2090621 -4.1986523 -4.188396 -4.1682158 -4.16177 -4.1751142 -4.1935968 -4.221221 -4.2433577][-4.2856789 -4.2928805 -4.2946715 -4.275527 -4.244422 -4.2208371 -4.2140613 -4.2145958 -4.2141004 -4.1981931 -4.1893539 -4.1971574 -4.2160959 -4.242383 -4.2630792][-4.2916527 -4.293716 -4.2940812 -4.2808676 -4.25643 -4.2377539 -4.2334609 -4.2366076 -4.2375522 -4.2301154 -4.2287273 -4.2384925 -4.2557845 -4.2755218 -4.290278]]...]
INFO - root - 2017-12-05 17:02:28.982333: step 27010, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 73h:25m:16s remains)
INFO - root - 2017-12-05 17:02:37.419387: step 27020, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 72h:03m:51s remains)
INFO - root - 2017-12-05 17:02:45.855455: step 27030, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 72h:36m:22s remains)
INFO - root - 2017-12-05 17:02:54.489682: step 27040, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 72h:28m:03s remains)
INFO - root - 2017-12-05 17:03:03.023818: step 27050, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 73h:17m:54s remains)
INFO - root - 2017-12-05 17:03:11.576597: step 27060, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 72h:44m:26s remains)
INFO - root - 2017-12-05 17:03:20.086313: step 27070, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 73h:36m:40s remains)
INFO - root - 2017-12-05 17:03:28.560390: step 27080, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 70h:11m:23s remains)
INFO - root - 2017-12-05 17:03:37.144723: step 27090, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.865 sec/batch; 73h:21m:09s remains)
INFO - root - 2017-12-05 17:03:45.619227: step 27100, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 70h:39m:59s remains)
2017-12-05 17:03:46.344357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.309083 -4.309226 -4.3056936 -4.3012495 -4.2977333 -4.2972403 -4.298789 -4.3010168 -4.3034897 -4.3052373 -4.3058524 -4.30636 -4.3047948 -4.3040404 -4.3048148][-4.2881527 -4.2888255 -4.2833161 -4.2758012 -4.270504 -4.2720013 -4.2750998 -4.2774792 -4.2805953 -4.2834978 -4.285913 -4.289063 -4.2881808 -4.2866473 -4.28613][-4.2664633 -4.2668695 -4.255795 -4.2402368 -4.2325239 -4.2373023 -4.2419047 -4.2432761 -4.2475076 -4.2530069 -4.2595043 -4.2671003 -4.2717657 -4.2741017 -4.2740269][-4.2539468 -4.2526269 -4.2308455 -4.2019582 -4.1899853 -4.1969309 -4.2031841 -4.20267 -4.2075286 -4.2152572 -4.2272387 -4.2407327 -4.2535229 -4.2639766 -4.2695179][-4.2522798 -4.248816 -4.2173023 -4.1744537 -4.1563792 -4.1620984 -4.170083 -4.168035 -4.1711392 -4.1794052 -4.1967964 -4.2168622 -4.2403884 -4.2599082 -4.2709956][-4.2569408 -4.2516623 -4.2135005 -4.1624956 -4.1397805 -4.1417284 -4.1476789 -4.1438913 -4.1440349 -4.153286 -4.173481 -4.1976328 -4.2279682 -4.2543712 -4.2727942][-4.2572289 -4.2513876 -4.2117105 -4.1600637 -4.1330013 -4.1271515 -4.1294942 -4.1278939 -4.1303282 -4.1392889 -4.1546283 -4.1774149 -4.2103004 -4.241087 -4.2642064][-4.2471232 -4.2460017 -4.2131872 -4.1682653 -4.1387706 -4.12496 -4.1240735 -4.1284451 -4.1381855 -4.1455855 -4.1520219 -4.1699362 -4.1999288 -4.2266178 -4.2481766][-4.236495 -4.2434464 -4.2190213 -4.1784368 -4.1504307 -4.137464 -4.1392751 -4.1501956 -4.165628 -4.1743555 -4.1771274 -4.1871963 -4.2082009 -4.2255988 -4.2427473][-4.2236571 -4.2364416 -4.2192574 -4.1839027 -4.1623306 -4.1603742 -4.1703734 -4.1880097 -4.2066283 -4.21556 -4.2185483 -4.2203388 -4.2298169 -4.23682 -4.2478504][-4.2113705 -4.2226954 -4.2062979 -4.1739955 -4.1603646 -4.1721458 -4.1930542 -4.2171326 -4.2357054 -4.2464828 -4.2518091 -4.2484465 -4.2416615 -4.2358727 -4.2390971][-4.2125416 -4.217227 -4.1963663 -4.1626997 -4.1494966 -4.1678934 -4.1973538 -4.2230639 -4.2359481 -4.2469673 -4.2568254 -4.2569585 -4.2425995 -4.2272949 -4.2235389][-4.231751 -4.2294617 -4.2039075 -4.16536 -4.14359 -4.1529226 -4.1759038 -4.1980381 -4.2094707 -4.2225938 -4.2376475 -4.2436013 -4.2341189 -4.2224064 -4.2179012][-4.2409215 -4.2393613 -4.2160034 -4.1802368 -4.1536684 -4.1463256 -4.1507716 -4.1623254 -4.1715517 -4.1870146 -4.2094183 -4.2251058 -4.2268071 -4.2227583 -4.217453][-4.2375488 -4.2428994 -4.23067 -4.2038064 -4.1778045 -4.15751 -4.146347 -4.1447592 -4.1437936 -4.1538076 -4.1787872 -4.2017789 -4.2151175 -4.2181706 -4.2132554]]...]
INFO - root - 2017-12-05 17:03:54.801712: step 27110, loss = 2.02, batch loss = 1.97 (9.9 examples/sec; 0.810 sec/batch; 68h:42m:26s remains)
INFO - root - 2017-12-05 17:04:03.362792: step 27120, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 71h:41m:20s remains)
INFO - root - 2017-12-05 17:04:11.856405: step 27130, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 71h:24m:43s remains)
INFO - root - 2017-12-05 17:04:20.514716: step 27140, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 73h:19m:54s remains)
INFO - root - 2017-12-05 17:04:28.853019: step 27150, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 71h:36m:14s remains)
INFO - root - 2017-12-05 17:04:37.430406: step 27160, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 71h:32m:21s remains)
INFO - root - 2017-12-05 17:04:45.947963: step 27170, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 74h:05m:35s remains)
INFO - root - 2017-12-05 17:04:54.386025: step 27180, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 71h:43m:06s remains)
INFO - root - 2017-12-05 17:05:02.965927: step 27190, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 74h:01m:25s remains)
INFO - root - 2017-12-05 17:05:11.479905: step 27200, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 73h:44m:17s remains)
2017-12-05 17:05:12.228937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2796016 -4.2964373 -4.3044276 -4.3072753 -4.3111563 -4.3160076 -4.32181 -4.3283868 -4.3338757 -4.3375063 -4.3400478 -4.3411913 -4.3377209 -4.333715 -4.329402][-4.2547994 -4.2743635 -4.2798591 -4.2781487 -4.2781787 -4.2804642 -4.2874751 -4.3032794 -4.3212953 -4.3341804 -4.3422 -4.3447137 -4.339992 -4.3328714 -4.32571][-4.2357163 -4.2499156 -4.2453389 -4.2334495 -4.2237597 -4.220118 -4.2280216 -4.2575321 -4.2935753 -4.3192315 -4.3343768 -4.3390651 -4.3325257 -4.3227181 -4.3140335][-4.2268581 -4.2298522 -4.212729 -4.1871672 -4.1607075 -4.1417789 -4.1434231 -4.1856928 -4.2433453 -4.2861209 -4.30981 -4.3159461 -4.3084636 -4.2978606 -4.2914157][-4.2292061 -4.2192049 -4.1885724 -4.1469383 -4.0977106 -4.0482831 -4.0296206 -4.0804162 -4.1635318 -4.2286739 -4.2657776 -4.2768269 -4.2715497 -4.263659 -4.2625628][-4.229949 -4.2072911 -4.1664829 -4.1103263 -4.0319653 -3.9400067 -3.8895171 -3.9517961 -4.0717521 -4.1636152 -4.2151251 -4.2316132 -4.2316937 -4.2277637 -4.2320995][-4.2295756 -4.1953931 -4.1460934 -4.0771356 -3.9690058 -3.8336129 -3.747263 -3.8301573 -3.9952304 -4.1136355 -4.1754079 -4.1956124 -4.1994095 -4.199337 -4.2073522][-4.2334375 -4.1955681 -4.1464887 -4.0766096 -3.9583447 -3.80123 -3.7030883 -3.8042283 -3.9889252 -4.1066504 -4.1559992 -4.1684103 -4.16513 -4.1631942 -4.1765175][-4.24204 -4.2088189 -4.1675906 -4.1130791 -4.0182667 -3.8925207 -3.8219121 -3.9071698 -4.0518169 -4.1323972 -4.1495876 -4.14185 -4.1229253 -4.1165824 -4.1373239][-4.2485948 -4.2262697 -4.1971364 -4.15998 -4.0945835 -4.0172343 -3.98176 -4.0377889 -4.1193228 -4.1459036 -4.123662 -4.0918236 -4.0619383 -4.0563278 -4.0887456][-4.2495108 -4.2380061 -4.2188892 -4.1906962 -4.1449819 -4.1017685 -4.0887666 -4.1176667 -4.1442609 -4.1231031 -4.070385 -4.0189052 -3.9813581 -3.9817734 -4.0238538][-4.2424555 -4.2400522 -4.2272205 -4.2044439 -4.1715364 -4.1456985 -4.1416359 -4.1521363 -4.1425977 -4.092555 -4.0225105 -3.9611502 -3.9162383 -3.9136422 -3.9558222][-4.234684 -4.2396088 -4.2343879 -4.2203088 -4.1949496 -4.1760793 -4.172946 -4.1680174 -4.1321497 -4.0688572 -4.0021119 -3.9427245 -3.8966591 -3.8899379 -3.9270048][-4.2337089 -4.2436371 -4.2464147 -4.2398233 -4.2220345 -4.2067971 -4.2014256 -4.1864319 -4.1390619 -4.0751634 -4.0182 -3.975359 -3.9476357 -3.9469726 -3.9708977][-4.2297764 -4.2436237 -4.2553434 -4.2557583 -4.2461433 -4.235034 -4.2266994 -4.2078385 -4.1644435 -4.1091 -4.0628796 -4.0386286 -4.0322003 -4.0357766 -4.0443139]]...]
INFO - root - 2017-12-05 17:05:20.778039: step 27210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.847 sec/batch; 71h:47m:23s remains)
INFO - root - 2017-12-05 17:05:29.438467: step 27220, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 74h:04m:23s remains)
INFO - root - 2017-12-05 17:05:37.954703: step 27230, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 71h:35m:50s remains)
INFO - root - 2017-12-05 17:05:46.415512: step 27240, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.836 sec/batch; 70h:51m:44s remains)
INFO - root - 2017-12-05 17:05:54.927735: step 27250, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 73h:29m:42s remains)
INFO - root - 2017-12-05 17:06:03.404715: step 27260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 71h:24m:16s remains)
INFO - root - 2017-12-05 17:06:11.951259: step 27270, loss = 2.03, batch loss = 1.97 (10.4 examples/sec; 0.771 sec/batch; 65h:22m:02s remains)
INFO - root - 2017-12-05 17:06:20.383882: step 27280, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 71h:10m:09s remains)
INFO - root - 2017-12-05 17:06:28.959386: step 27290, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 72h:25m:29s remains)
INFO - root - 2017-12-05 17:06:37.575702: step 27300, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 72h:46m:49s remains)
2017-12-05 17:06:38.365917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2824988 -4.2804747 -4.2752595 -4.2673655 -4.258657 -4.2572479 -4.257607 -4.2547636 -4.2560716 -4.2612944 -4.2663269 -4.2732654 -4.2751756 -4.258924 -4.2459269][-4.3054647 -4.3102622 -4.3105741 -4.3085108 -4.3007197 -4.2956963 -4.2944584 -4.2904253 -4.2898741 -4.2938724 -4.2950888 -4.2970557 -4.29796 -4.2847333 -4.2707362][-4.3136659 -4.3222127 -4.3263464 -4.329926 -4.3245764 -4.318666 -4.3176508 -4.3134975 -4.314621 -4.3187814 -4.3182626 -4.3165584 -4.3126473 -4.295197 -4.2744951][-4.2835779 -4.294436 -4.3011942 -4.3087015 -4.307024 -4.3064904 -4.3066349 -4.2999735 -4.2988873 -4.3034835 -4.3027515 -4.2942266 -4.2851534 -4.2664189 -4.240046][-4.2208371 -4.2326579 -4.240725 -4.2480607 -4.2466464 -4.2488971 -4.2453465 -4.2298317 -4.2277303 -4.2428603 -4.25106 -4.2435513 -4.2360759 -4.217917 -4.1854777][-4.1435094 -4.1517425 -4.1531391 -4.149323 -4.137651 -4.1330972 -4.1133862 -4.0734406 -4.0719075 -4.115 -4.1461244 -4.1522164 -4.1550479 -4.1482115 -4.1216722][-4.1009274 -4.0955243 -4.0770674 -4.0479703 -4.0144439 -3.9936914 -3.9447546 -3.8613091 -3.8639922 -3.9518778 -4.0188918 -4.0533228 -4.0765076 -4.0891275 -4.0800185][-4.1425281 -4.1317554 -4.1031795 -4.0631084 -4.0188961 -3.9926097 -3.9461029 -3.8688276 -3.8698602 -3.9495854 -4.0119491 -4.0428653 -4.0631819 -4.0850739 -4.086][-4.1978559 -4.1922922 -4.1756687 -4.1530294 -4.1257048 -4.1119857 -4.0882673 -4.0426135 -4.0331755 -4.0701933 -4.1005645 -4.1122942 -4.1159496 -4.1299977 -4.1298165][-4.22648 -4.2293344 -4.2264962 -4.2176161 -4.2069793 -4.2082934 -4.1967087 -4.1670866 -4.1513605 -4.1610675 -4.1711636 -4.1764374 -4.1789994 -4.1882367 -4.182632][-4.2050252 -4.2143316 -4.2234125 -4.2321391 -4.2387066 -4.2499771 -4.2434616 -4.2200451 -4.202858 -4.1981273 -4.1990671 -4.20204 -4.2075815 -4.2137594 -4.2064185][-4.144722 -4.1616473 -4.1840396 -4.2125115 -4.2370195 -4.25458 -4.2540655 -4.2400579 -4.2292962 -4.2206459 -4.2177382 -4.2182684 -4.2225909 -4.2273755 -4.2212167][-4.0956621 -4.1119394 -4.1438704 -4.1863265 -4.2217956 -4.2481418 -4.2564535 -4.2541037 -4.2481012 -4.2372885 -4.231739 -4.2314644 -4.2330594 -4.2361717 -4.23166][-4.0983863 -4.1062388 -4.1354079 -4.1757097 -4.2111573 -4.2412853 -4.2571626 -4.2637677 -4.2619367 -4.2511582 -4.2464762 -4.245604 -4.2428951 -4.243454 -4.2382817][-4.125886 -4.1241932 -4.1496525 -4.1845813 -4.2138653 -4.2412391 -4.2597437 -4.2687912 -4.2685552 -4.2598553 -4.2564921 -4.2563963 -4.2533946 -4.252347 -4.2464776]]...]
INFO - root - 2017-12-05 17:06:46.879272: step 27310, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 69h:43m:14s remains)
INFO - root - 2017-12-05 17:06:55.323204: step 27320, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:46m:08s remains)
INFO - root - 2017-12-05 17:07:03.752997: step 27330, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 70h:37m:18s remains)
INFO - root - 2017-12-05 17:07:12.218202: step 27340, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 72h:22m:52s remains)
INFO - root - 2017-12-05 17:07:20.761213: step 27350, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:41m:43s remains)
INFO - root - 2017-12-05 17:07:29.321828: step 27360, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 72h:22m:58s remains)
INFO - root - 2017-12-05 17:07:37.637289: step 27370, loss = 2.07, batch loss = 2.02 (10.4 examples/sec; 0.771 sec/batch; 65h:20m:25s remains)
INFO - root - 2017-12-05 17:07:46.188457: step 27380, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 72h:35m:08s remains)
INFO - root - 2017-12-05 17:07:54.732691: step 27390, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 72h:04m:21s remains)
INFO - root - 2017-12-05 17:08:03.289459: step 27400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 73h:27m:54s remains)
2017-12-05 17:08:04.039508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2108021 -4.1776667 -4.1377392 -4.1075788 -4.0880089 -4.1051321 -4.1618366 -4.2078743 -4.2229939 -4.2170939 -4.1943331 -4.168601 -4.1420388 -4.1335931 -4.1433153][-4.2017012 -4.1739817 -4.1373744 -4.1129985 -4.0991726 -4.1162524 -4.1650057 -4.2011824 -4.2111554 -4.2004886 -4.1822586 -4.1717114 -4.1585197 -4.1550837 -4.15906][-4.1863217 -4.1640387 -4.12963 -4.1108074 -4.1035347 -4.1193776 -4.1556745 -4.182374 -4.1873751 -4.1746507 -4.1662049 -4.167871 -4.1663017 -4.1708813 -4.1718607][-4.17638 -4.1579137 -4.1286526 -4.113492 -4.110466 -4.1193037 -4.135437 -4.1493955 -4.154211 -4.1487412 -4.1539516 -4.166245 -4.17571 -4.1865168 -4.1860681][-4.17491 -4.1613994 -4.143486 -4.1356506 -4.1318159 -4.12059 -4.1060042 -4.1059089 -4.1140122 -4.1220655 -4.1428328 -4.1685967 -4.1870217 -4.2021356 -4.2017336][-4.1811047 -4.1738105 -4.169528 -4.1631994 -4.1501222 -4.1130695 -4.0692558 -4.0594225 -4.0782876 -4.1039538 -4.1362228 -4.1705432 -4.1933389 -4.2060676 -4.2072697][-4.1897945 -4.1899958 -4.1908407 -4.1803231 -4.1571827 -4.0975904 -4.0296516 -4.0190787 -4.0592489 -4.1063147 -4.1436214 -4.1751652 -4.1929259 -4.2036633 -4.2079778][-4.1969566 -4.2040272 -4.2010288 -4.1856451 -4.1557884 -4.0886731 -4.0134182 -4.007741 -4.0686355 -4.1315522 -4.1642327 -4.1806297 -4.1889615 -4.1985464 -4.2036757][-4.2050996 -4.2138124 -4.2092118 -4.1960392 -4.1691866 -4.1122761 -4.0449944 -4.040194 -4.0969291 -4.1549826 -4.1756196 -4.1757584 -4.1753173 -4.1847558 -4.1926017][-4.20803 -4.2158546 -4.2139335 -4.2089586 -4.1928668 -4.1518559 -4.0950246 -4.0789657 -4.1101165 -4.1480627 -4.1551595 -4.144362 -4.141922 -4.1591849 -4.1771822][-4.2100039 -4.2191687 -4.2222309 -4.2207613 -4.2096486 -4.180192 -4.1350365 -4.10987 -4.1156745 -4.1283016 -4.1148109 -4.0918684 -4.0893211 -4.1217885 -4.1593056][-4.2170982 -4.2265487 -4.2308731 -4.228128 -4.2136149 -4.1903725 -4.1584082 -4.1349592 -4.1296144 -4.12267 -4.0886693 -4.0537577 -4.0515289 -4.0936322 -4.1403589][-4.2240996 -4.23209 -4.2352986 -4.229393 -4.2125487 -4.1936717 -4.175066 -4.1625705 -4.15897 -4.1445785 -4.1068568 -4.0707097 -4.0651484 -4.0944753 -4.1291537][-4.2215219 -4.2275343 -4.2329521 -4.2283521 -4.2131324 -4.1991987 -4.189889 -4.1847582 -4.1832328 -4.1737108 -4.1520276 -4.1302981 -4.1212568 -4.1262131 -4.1378436][-4.2110548 -4.2134361 -4.2218122 -4.2213907 -4.2110791 -4.1996827 -4.1907625 -4.1867967 -4.1850128 -4.1808004 -4.1734638 -4.1661983 -4.1595521 -4.1560707 -4.1565742]]...]
INFO - root - 2017-12-05 17:08:12.554577: step 27410, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 72h:40m:10s remains)
INFO - root - 2017-12-05 17:08:21.064894: step 27420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 73h:12m:47s remains)
INFO - root - 2017-12-05 17:08:29.744907: step 27430, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:29m:07s remains)
INFO - root - 2017-12-05 17:08:38.226040: step 27440, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 71h:14m:15s remains)
INFO - root - 2017-12-05 17:08:46.799056: step 27450, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.835 sec/batch; 70h:47m:14s remains)
INFO - root - 2017-12-05 17:08:55.331285: step 27460, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 71h:05m:23s remains)
INFO - root - 2017-12-05 17:09:03.589149: step 27470, loss = 2.04, batch loss = 1.99 (10.8 examples/sec; 0.743 sec/batch; 62h:57m:55s remains)
INFO - root - 2017-12-05 17:09:11.986278: step 27480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 71h:42m:18s remains)
INFO - root - 2017-12-05 17:09:20.603647: step 27490, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 72h:23m:18s remains)
INFO - root - 2017-12-05 17:09:29.088645: step 27500, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 74h:42m:02s remains)
2017-12-05 17:09:29.842348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2426381 -4.2364473 -4.2156124 -4.1756511 -4.138824 -4.1390834 -4.1538606 -4.1724467 -4.1981997 -4.2206984 -4.2262449 -4.218051 -4.2043958 -4.1935005 -4.1805387][-4.2357225 -4.2255659 -4.1994834 -4.1544213 -4.1102004 -4.1039963 -4.1132011 -4.1281419 -4.1618857 -4.1956716 -4.2067037 -4.1987948 -4.1884089 -4.18048 -4.1637492][-4.23291 -4.2203298 -4.19109 -4.1458669 -4.0983839 -4.0774422 -4.0696673 -4.07337 -4.11215 -4.15948 -4.1790819 -4.178905 -4.1806679 -4.1803904 -4.1594324][-4.23969 -4.229723 -4.2008162 -4.1541634 -4.10174 -4.0580196 -4.0205455 -4.0051656 -4.0469327 -4.1100378 -4.1475515 -4.1643381 -4.179997 -4.1873269 -4.1663651][-4.24564 -4.2386413 -4.20976 -4.159719 -4.0990248 -4.0294681 -3.9544377 -3.9097366 -3.9590275 -4.0478234 -4.1148696 -4.1506128 -4.1741071 -4.1850595 -4.1694074][-4.249588 -4.2414227 -4.2085891 -4.152647 -4.0846195 -3.993973 -3.8756154 -3.7917473 -3.8555226 -3.987282 -4.0926242 -4.1450171 -4.1699162 -4.1792169 -4.1675534][-4.2538781 -4.2443647 -4.2084866 -4.1477466 -4.076251 -3.9718111 -3.8181655 -3.6932435 -3.7764301 -3.9511161 -4.088625 -4.1507645 -4.1726165 -4.1794844 -4.1684275][-4.2591357 -4.2499561 -4.2165093 -4.1593437 -4.0926204 -3.9908507 -3.8356633 -3.7084928 -3.7913022 -3.9698484 -4.1059251 -4.1647854 -4.1796818 -4.1834569 -4.172605][-4.2628007 -4.25638 -4.2278824 -4.1806092 -4.1265736 -4.0469685 -3.9267023 -3.8371749 -3.8970511 -4.0337839 -4.1415319 -4.184402 -4.1878204 -4.1874385 -4.1845212][-4.2643948 -4.2606316 -4.2391219 -4.2045131 -4.1696997 -4.1203856 -4.0437727 -3.9856277 -4.0131917 -4.1005173 -4.1799593 -4.2090273 -4.206068 -4.2050519 -4.2098331][-4.2653246 -4.2625942 -4.2479763 -4.2252879 -4.2096086 -4.187736 -4.1485982 -4.1105933 -4.1145782 -4.1621208 -4.2171688 -4.2389283 -4.2385674 -4.2422915 -4.2481656][-4.2651381 -4.2649755 -4.2554145 -4.2426262 -4.242928 -4.24381 -4.2288065 -4.2037683 -4.1959543 -4.2189136 -4.2566385 -4.2763577 -4.2826157 -4.2885284 -4.2872434][-4.2692962 -4.2723374 -4.2690439 -4.2638645 -4.2737861 -4.2876868 -4.2860241 -4.2691822 -4.2588997 -4.2700377 -4.2951503 -4.3124661 -4.3224535 -4.3288279 -4.322094][-4.2801418 -4.2847013 -4.285778 -4.2845187 -4.2949438 -4.3110518 -4.3148437 -4.3059907 -4.3000565 -4.3070436 -4.32333 -4.3361745 -4.344449 -4.3492093 -4.3405609][-4.2959518 -4.2994614 -4.3013277 -4.3017368 -4.31036 -4.3247018 -4.3308544 -4.3283048 -4.32511 -4.3288879 -4.3367958 -4.3419762 -4.3455262 -4.3469906 -4.33892]]...]
INFO - root - 2017-12-05 17:09:38.434601: step 27510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 74h:00m:02s remains)
INFO - root - 2017-12-05 17:09:47.123587: step 27520, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 75h:14m:30s remains)
INFO - root - 2017-12-05 17:09:55.725477: step 27530, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 73h:47m:57s remains)
INFO - root - 2017-12-05 17:10:04.215298: step 27540, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 72h:08m:00s remains)
INFO - root - 2017-12-05 17:10:12.675943: step 27550, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 72h:13m:30s remains)
INFO - root - 2017-12-05 17:10:21.108123: step 27560, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.797 sec/batch; 67h:31m:00s remains)
INFO - root - 2017-12-05 17:10:29.662901: step 27570, loss = 2.05, batch loss = 2.00 (10.6 examples/sec; 0.754 sec/batch; 63h:51m:32s remains)
INFO - root - 2017-12-05 17:10:38.171815: step 27580, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 73h:52m:11s remains)
INFO - root - 2017-12-05 17:10:46.783372: step 27590, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 72h:13m:21s remains)
INFO - root - 2017-12-05 17:10:55.381203: step 27600, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 73h:02m:07s remains)
2017-12-05 17:10:56.165493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2161975 -4.2123842 -4.2092428 -4.2100892 -4.2061892 -4.2007308 -4.1985126 -4.2090573 -4.2107344 -4.1927996 -4.16904 -4.1480622 -4.1398697 -4.1489925 -4.1776209][-4.2148919 -4.2126365 -4.2132754 -4.2201943 -4.2144432 -4.2015495 -4.1914077 -4.2003846 -4.2037668 -4.1856861 -4.1613369 -4.138238 -4.1310453 -4.1419621 -4.1699238][-4.2043371 -4.2097659 -4.2206511 -4.235796 -4.2311921 -4.2093663 -4.1916356 -4.195096 -4.1983967 -4.1834865 -4.162498 -4.1423249 -4.1380529 -4.1485729 -4.17112][-4.1872044 -4.2011337 -4.222971 -4.2458143 -4.2412925 -4.2123408 -4.184463 -4.1793146 -4.1847391 -4.1782322 -4.1653857 -4.1497736 -4.1486225 -4.1572914 -4.17434][-4.173315 -4.190691 -4.2160378 -4.2436972 -4.2367907 -4.1967134 -4.1521182 -4.1369271 -4.1538415 -4.167028 -4.1677752 -4.1529822 -4.1484871 -4.1518917 -4.1648636][-4.1664639 -4.181417 -4.2041979 -4.2291875 -4.2191763 -4.1659412 -4.0993209 -4.0687046 -4.0998373 -4.142261 -4.1623893 -4.1500487 -4.1383667 -4.1357613 -4.1493053][-4.150548 -4.1673613 -4.1904049 -4.2107244 -4.1915426 -4.1214046 -4.0210857 -3.9588573 -4.001554 -4.08535 -4.13622 -4.1359234 -4.126771 -4.123816 -4.1414647][-4.1235313 -4.1425719 -4.1694689 -4.1936789 -4.1709933 -4.0847239 -3.9482427 -3.8350236 -3.8805823 -4.0131 -4.1032276 -4.1266036 -4.1259785 -4.1284413 -4.147953][-4.119772 -4.1368103 -4.1638 -4.1873941 -4.1736913 -4.1005564 -3.9705992 -3.8467326 -3.8722708 -4.0057616 -4.1064177 -4.1435294 -4.1500082 -4.1564217 -4.1726637][-4.1521664 -4.1615314 -4.180903 -4.1957026 -4.191092 -4.1503649 -4.0669203 -3.9892325 -3.9948719 -4.0735855 -4.1411095 -4.1694126 -4.1795349 -4.1883044 -4.202538][-4.2067146 -4.2060947 -4.2113 -4.2118769 -4.2118759 -4.1961017 -4.1561418 -4.1236196 -4.1206427 -4.150135 -4.1799345 -4.19486 -4.202383 -4.209938 -4.2215052][-4.2535067 -4.2501583 -4.2435207 -4.2308674 -4.2277665 -4.2210402 -4.2069554 -4.2001724 -4.196919 -4.1985469 -4.2031837 -4.208735 -4.215456 -4.2249327 -4.2364435][-4.2803793 -4.2785053 -4.2716074 -4.2590351 -4.2537513 -4.2486024 -4.2455254 -4.2468476 -4.2394457 -4.2264462 -4.2146964 -4.21001 -4.2171082 -4.2324305 -4.2502036][-4.2800784 -4.2841864 -4.2836266 -4.2791409 -4.27728 -4.27813 -4.2766833 -4.2761135 -4.2628341 -4.2409 -4.2193613 -4.2067137 -4.2151484 -4.2384782 -4.2647185][-4.2625518 -4.2727127 -4.2756681 -4.2715254 -4.2695808 -4.2770753 -4.2808447 -4.2796092 -4.2644172 -4.24082 -4.2189417 -4.2062907 -4.216095 -4.2432261 -4.2740574]]...]
INFO - root - 2017-12-05 17:11:04.770228: step 27610, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 71h:35m:07s remains)
INFO - root - 2017-12-05 17:11:13.358655: step 27620, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 71h:16m:36s remains)
INFO - root - 2017-12-05 17:11:21.883126: step 27630, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 71h:46m:28s remains)
INFO - root - 2017-12-05 17:11:30.501509: step 27640, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 73h:42m:47s remains)
INFO - root - 2017-12-05 17:11:39.067344: step 27650, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.852 sec/batch; 72h:11m:20s remains)
INFO - root - 2017-12-05 17:11:47.442701: step 27660, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 69h:45m:13s remains)
INFO - root - 2017-12-05 17:11:56.013986: step 27670, loss = 2.09, batch loss = 2.03 (10.6 examples/sec; 0.751 sec/batch; 63h:37m:51s remains)
INFO - root - 2017-12-05 17:12:04.469522: step 27680, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:26m:21s remains)
INFO - root - 2017-12-05 17:12:13.002322: step 27690, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.833 sec/batch; 70h:31m:44s remains)
INFO - root - 2017-12-05 17:12:21.528389: step 27700, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 73h:16m:27s remains)
2017-12-05 17:12:22.325674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2178025 -4.2029409 -4.2049322 -4.2105541 -4.1900415 -4.1455579 -4.1239796 -4.12958 -4.1569085 -4.1997361 -4.2243028 -4.2482686 -4.2649231 -4.2660332 -4.2470016][-4.2202277 -4.2030592 -4.2005663 -4.2016115 -4.18267 -4.1485834 -4.1369076 -4.1457891 -4.1702023 -4.2109647 -4.2402959 -4.2680378 -4.2836442 -4.2808127 -4.2539144][-4.2219357 -4.20252 -4.19472 -4.1864729 -4.1669512 -4.1430435 -4.1397567 -4.1472659 -4.1642909 -4.2026181 -4.2387123 -4.2739387 -4.2947006 -4.2952976 -4.2698][-4.223948 -4.2021976 -4.1875286 -4.16798 -4.1422105 -4.1206369 -4.1155624 -4.1172895 -4.1303096 -4.16875 -4.2126055 -4.2574964 -4.2904634 -4.30034 -4.2798734][-4.2211804 -4.1973515 -4.1772432 -4.1499567 -4.1154552 -4.0860367 -4.0652623 -4.0523353 -4.0680041 -4.1139708 -4.1700454 -4.2269478 -4.2716155 -4.2896304 -4.2750711][-4.2134781 -4.1880279 -4.1647124 -4.1317554 -4.0897245 -4.0419884 -3.9864459 -3.9461029 -3.964149 -4.0261521 -4.0999475 -4.173449 -4.2349334 -4.2668562 -4.2596512][-4.2030473 -4.1732888 -4.1427732 -4.1008811 -4.0454431 -3.9701204 -3.8697762 -3.7915354 -3.8186893 -3.9152732 -4.0170422 -4.1065407 -4.1819162 -4.224719 -4.2225914][-4.1911449 -4.1524711 -4.1092572 -4.0541282 -3.9838398 -3.8925402 -3.7739766 -3.6820555 -3.7261302 -3.8578026 -3.9831862 -4.0770802 -4.148221 -4.1871281 -4.1866946][-4.1853328 -4.1361647 -4.0788579 -4.0164084 -3.9495521 -3.878767 -3.8026454 -3.7584736 -3.80955 -3.9230564 -4.0286474 -4.10243 -4.1548185 -4.18027 -4.1782155][-4.1868992 -4.1359038 -4.0760717 -4.0209703 -3.9729557 -3.936564 -3.9112391 -3.9141681 -3.9686451 -4.0504284 -4.1209598 -4.1652908 -4.1960254 -4.2098927 -4.2076464][-4.1994429 -4.1573191 -4.1082106 -4.0688863 -4.0412388 -4.0300713 -4.037765 -4.0659451 -4.1148958 -4.16933 -4.21202 -4.2355795 -4.2523203 -4.2597871 -4.2585998][-4.2175331 -4.1868687 -4.1541796 -4.1338615 -4.12445 -4.1284871 -4.1518664 -4.1867619 -4.2251835 -4.2580404 -4.2815089 -4.29398 -4.3029647 -4.3064475 -4.3035946][-4.2358413 -4.2124887 -4.1902738 -4.1857915 -4.1932783 -4.2078981 -4.2344246 -4.2631993 -4.2882543 -4.3064027 -4.3178058 -4.3216925 -4.3257375 -4.3276882 -4.3246236][-4.2498121 -4.2309127 -4.2138858 -4.2173119 -4.234036 -4.2547188 -4.2816429 -4.3013787 -4.3142734 -4.321878 -4.3257284 -4.3242393 -4.3234258 -4.3239226 -4.3225293][-4.2544165 -4.2402124 -4.2276125 -4.2339087 -4.2519641 -4.2744217 -4.3000474 -4.3120151 -4.3148885 -4.3150034 -4.3135724 -4.3095546 -4.3069558 -4.3059506 -4.304739]]...]
INFO - root - 2017-12-05 17:12:30.810564: step 27710, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 72h:15m:24s remains)
INFO - root - 2017-12-05 17:12:39.306602: step 27720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 72h:42m:47s remains)
INFO - root - 2017-12-05 17:12:47.801582: step 27730, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 72h:43m:49s remains)
INFO - root - 2017-12-05 17:12:56.340478: step 27740, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 71h:18m:37s remains)
INFO - root - 2017-12-05 17:13:04.923231: step 27750, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 72h:59m:58s remains)
INFO - root - 2017-12-05 17:13:13.471773: step 27760, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.849 sec/batch; 71h:53m:37s remains)
INFO - root - 2017-12-05 17:13:22.006413: step 27770, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.768 sec/batch; 65h:00m:36s remains)
INFO - root - 2017-12-05 17:13:30.513677: step 27780, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 74h:23m:13s remains)
INFO - root - 2017-12-05 17:13:38.866110: step 27790, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 72h:32m:49s remains)
INFO - root - 2017-12-05 17:13:47.412066: step 27800, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 70h:50m:17s remains)
2017-12-05 17:13:48.175081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19239 -4.1952477 -4.2053738 -4.2203794 -4.2243781 -4.210319 -4.20033 -4.2000537 -4.198771 -4.206059 -4.2181211 -4.2367826 -4.2498412 -4.261199 -4.2556543][-4.1808681 -4.1897078 -4.2013812 -4.2190757 -4.2251606 -4.2113056 -4.2029395 -4.2057028 -4.203311 -4.2092013 -4.2177558 -4.232482 -4.2448397 -4.260386 -4.26169][-4.1483564 -4.1637583 -4.1789989 -4.1955624 -4.2085533 -4.1999049 -4.1951241 -4.1966195 -4.1904855 -4.1962276 -4.2039022 -4.2157574 -4.2309666 -4.2519312 -4.26001][-4.1164041 -4.137239 -4.1556873 -4.17632 -4.2009373 -4.2000875 -4.1927557 -4.1891017 -4.1828184 -4.1864238 -4.1945815 -4.2041564 -4.2196951 -4.2448945 -4.2617092][-4.0852118 -4.1145086 -4.1387343 -4.1636162 -4.1898723 -4.1858172 -4.1695156 -4.1644335 -4.1655145 -4.1742892 -4.1844621 -4.1964669 -4.2187867 -4.2466674 -4.2691545][-4.0712194 -4.1070328 -4.1385217 -4.1619334 -4.1739259 -4.1491427 -4.1076031 -4.0960183 -4.115675 -4.14507 -4.1681347 -4.1874146 -4.2150903 -4.2473793 -4.2743773][-4.09404 -4.1225734 -4.1522093 -4.1632462 -4.1485562 -4.0910487 -4.0088658 -3.9826832 -4.0330706 -4.0954256 -4.14023 -4.1711335 -4.1984463 -4.232697 -4.2618237][-4.137239 -4.1521807 -4.1643796 -4.1536956 -4.104702 -4.006361 -3.8849905 -3.8496747 -3.9432762 -4.0474572 -4.1147609 -4.15729 -4.1811352 -4.2053256 -4.2303944][-4.175631 -4.17902 -4.1718116 -4.1398721 -4.0721207 -3.9597721 -3.8329995 -3.8024082 -3.9151216 -4.0345855 -4.1068096 -4.148406 -4.1650372 -4.1741943 -4.1872935][-4.1872325 -4.1830769 -4.1691365 -4.1328835 -4.0747232 -3.9959149 -3.9171643 -3.9025569 -3.9806464 -4.0716696 -4.1279368 -4.155757 -4.1601868 -4.1556206 -4.1564527][-4.1795316 -4.1728044 -4.1565795 -4.1227789 -4.0873075 -4.0534539 -4.0258479 -4.0305576 -4.0768113 -4.1286139 -4.1635542 -4.176403 -4.1687055 -4.1516433 -4.1410294][-4.1603456 -4.1538014 -4.1373405 -4.1101804 -4.0942564 -4.0950823 -4.1075511 -4.1294007 -4.1596789 -4.1848984 -4.2016687 -4.204174 -4.1903291 -4.1636415 -4.1436157][-4.1681156 -4.1619458 -4.1442318 -4.1202273 -4.1133165 -4.1325355 -4.1684971 -4.1988306 -4.2209258 -4.2341776 -4.2372942 -4.230094 -4.215342 -4.1889896 -4.1636815][-4.20064 -4.1924028 -4.1729689 -4.15177 -4.1459651 -4.1678166 -4.2063365 -4.2386742 -4.259449 -4.269527 -4.2674212 -4.2550578 -4.2401958 -4.214159 -4.1857448][-4.238389 -4.2313647 -4.2137461 -4.1973476 -4.1905575 -4.2062573 -4.235961 -4.26314 -4.281899 -4.2877412 -4.2838783 -4.2721653 -4.2600465 -4.2369967 -4.2123933]]...]
INFO - root - 2017-12-05 17:13:56.715391: step 27810, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 70h:16m:15s remains)
INFO - root - 2017-12-05 17:14:05.351573: step 27820, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 70h:16m:17s remains)
INFO - root - 2017-12-05 17:14:14.063424: step 27830, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 71h:06m:02s remains)
INFO - root - 2017-12-05 17:14:22.570785: step 27840, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 74h:27m:16s remains)
INFO - root - 2017-12-05 17:14:31.114831: step 27850, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 71h:35m:11s remains)
INFO - root - 2017-12-05 17:14:39.566587: step 27860, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 74h:07m:33s remains)
INFO - root - 2017-12-05 17:14:48.046083: step 27870, loss = 2.09, batch loss = 2.03 (10.0 examples/sec; 0.799 sec/batch; 67h:37m:07s remains)
INFO - root - 2017-12-05 17:14:56.660379: step 27880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 73h:29m:52s remains)
INFO - root - 2017-12-05 17:15:05.091376: step 27890, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 72h:26m:50s remains)
INFO - root - 2017-12-05 17:15:13.450762: step 27900, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 71h:23m:45s remains)
2017-12-05 17:15:14.227818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.27308 -4.2590179 -4.2495012 -4.2497544 -4.2603736 -4.2714038 -4.2738376 -4.2728319 -4.2787185 -4.285449 -4.2907796 -4.2946639 -4.294106 -4.3002691 -4.3163676][-4.2775412 -4.2651081 -4.2606788 -4.267272 -4.2842512 -4.2997761 -4.3055778 -4.3101335 -4.3210845 -4.3264079 -4.3247657 -4.3192868 -4.3109093 -4.3094325 -4.3212094][-4.2604103 -4.2473712 -4.245091 -4.2583132 -4.284502 -4.303123 -4.3089709 -4.3149638 -4.3291774 -4.336174 -4.3327432 -4.3261538 -4.3167868 -4.3135467 -4.3224654][-4.2271943 -4.2104969 -4.2084384 -4.227109 -4.2611637 -4.2838707 -4.2898765 -4.2951212 -4.3109479 -4.3224869 -4.3229861 -4.3205681 -4.315135 -4.315249 -4.3237352][-4.1702542 -4.1507492 -4.1498337 -4.175241 -4.2167873 -4.2428484 -4.2495174 -4.2555833 -4.278069 -4.3000951 -4.3076715 -4.312006 -4.3144407 -4.3195691 -4.3268132][-4.097465 -4.0802636 -4.0791187 -4.1078873 -4.1518579 -4.1803131 -4.1892343 -4.1965117 -4.2278056 -4.2622633 -4.282352 -4.2974916 -4.3095427 -4.3221493 -4.3297343][-4.0349278 -4.0222592 -4.0188203 -4.0417404 -4.0808644 -4.1105075 -4.1189847 -4.1239877 -4.1616211 -4.2095208 -4.2446523 -4.2738247 -4.2987118 -4.3212452 -4.3321652][-3.9935966 -3.9832838 -3.9769752 -3.9882329 -4.0168629 -4.0421305 -4.0463614 -4.0486012 -4.0951486 -4.1562757 -4.206255 -4.2485666 -4.283854 -4.3161249 -4.3311157][-3.9805896 -3.9717069 -3.9621143 -3.9574749 -3.9657803 -3.9788661 -3.9795263 -3.9836531 -4.03707 -4.1082792 -4.1692739 -4.2196074 -4.2609286 -4.3002357 -4.3218327][-4.0094509 -3.9979012 -3.9815514 -3.9629288 -3.950861 -3.9491682 -3.9441385 -3.9499693 -4.0026422 -4.0728688 -4.1371913 -4.1927304 -4.2384362 -4.2831244 -4.312439][-4.0609069 -4.0405512 -4.0172944 -3.9941921 -3.9786706 -3.9715528 -3.9653769 -3.9739602 -4.0198555 -4.0807514 -4.1403089 -4.1950412 -4.2397513 -4.2837806 -4.3148694][-4.1238203 -4.0968847 -4.0714564 -4.0540838 -4.0482421 -4.0479007 -4.0445762 -4.0527778 -4.0861592 -4.1306977 -4.1796904 -4.2266889 -4.2647438 -4.3010683 -4.3274183][-4.1963024 -4.1728616 -4.1540055 -4.1444335 -4.1451612 -4.1488991 -4.1483388 -4.1523666 -4.1705117 -4.1986537 -4.234838 -4.2703767 -4.2976689 -4.3227596 -4.3409553][-4.2624817 -4.2464175 -4.2366424 -4.2332768 -4.2352309 -4.2390246 -4.2406144 -4.2424374 -4.250473 -4.2664142 -4.2882314 -4.3099523 -4.3261456 -4.3402963 -4.3500967][-4.307157 -4.2980375 -4.2942257 -4.2942219 -4.2972012 -4.30064 -4.3028951 -4.3034973 -4.3054461 -4.3116994 -4.3213468 -4.3318162 -4.3403463 -4.3463826 -4.3500371]]...]
INFO - root - 2017-12-05 17:15:22.760331: step 27910, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 72h:36m:48s remains)
INFO - root - 2017-12-05 17:15:31.421329: step 27920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 71h:25m:47s remains)
INFO - root - 2017-12-05 17:15:40.010985: step 27930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 72h:14m:07s remains)
INFO - root - 2017-12-05 17:15:48.538511: step 27940, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 72h:46m:52s remains)
INFO - root - 2017-12-05 17:15:57.015251: step 27950, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 73h:00m:27s remains)
INFO - root - 2017-12-05 17:16:05.439813: step 27960, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 69h:42m:14s remains)
INFO - root - 2017-12-05 17:16:13.873541: step 27970, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.790 sec/batch; 66h:52m:09s remains)
INFO - root - 2017-12-05 17:16:22.418640: step 27980, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 75h:57m:02s remains)
INFO - root - 2017-12-05 17:16:30.944043: step 27990, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 72h:45m:30s remains)
INFO - root - 2017-12-05 17:16:39.543577: step 28000, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 74h:58m:42s remains)
2017-12-05 17:16:40.340138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.328351 -4.3278341 -4.3275743 -4.3304462 -4.3346124 -4.3345222 -4.3268137 -4.3126006 -4.2994366 -4.2962885 -4.3013239 -4.3056879 -4.3097539 -4.3184767 -4.3288112][-4.3268595 -4.3261724 -4.3263969 -4.3263555 -4.325171 -4.3203864 -4.3101115 -4.2941537 -4.2798047 -4.2784724 -4.2873445 -4.2945681 -4.3020658 -4.3126221 -4.3217225][-4.3162041 -4.3167048 -4.3173966 -4.3156247 -4.3105907 -4.3016353 -4.28789 -4.2683129 -4.2534685 -4.2584791 -4.2754135 -4.2858019 -4.2964063 -4.3090196 -4.3167725][-4.2969236 -4.3009324 -4.3036165 -4.3001342 -4.2910218 -4.279623 -4.2654176 -4.2429209 -4.2255435 -4.2351894 -4.2605433 -4.2755108 -4.2886834 -4.3033609 -4.3110743][-4.2709761 -4.2789636 -4.2814655 -4.27386 -4.2593393 -4.2422876 -4.2253709 -4.1988764 -4.1780853 -4.1927595 -4.2296987 -4.2534943 -4.2705288 -4.2885203 -4.3006773][-4.2420235 -4.2496371 -4.2465558 -4.2311163 -4.2082434 -4.183074 -4.1576624 -4.1209526 -4.0930772 -4.1195116 -4.1767607 -4.214066 -4.2348638 -4.2564287 -4.2781153][-4.1990304 -4.2003708 -4.1872253 -4.1626463 -4.1320319 -4.0997891 -4.0609446 -4.002984 -3.9650557 -4.013299 -4.1008945 -4.1594076 -4.1914277 -4.2209716 -4.2544003][-4.1571107 -4.1567731 -4.1437011 -4.119297 -4.0895967 -4.0577664 -4.0130076 -3.9405038 -3.8921409 -3.9524338 -4.056488 -4.1270967 -4.1679049 -4.2041082 -4.244266][-4.1529784 -4.1569829 -4.1520381 -4.1404185 -4.128406 -4.1167674 -4.0914974 -4.0387549 -4.0006776 -4.0394473 -4.116982 -4.1738014 -4.2083158 -4.2365227 -4.2661166][-4.1889944 -4.193028 -4.1902437 -4.1845837 -4.1839995 -4.1888061 -4.1808977 -4.1509056 -4.1313763 -4.1557465 -4.2063456 -4.2467523 -4.2703414 -4.2874694 -4.3012738][-4.2372236 -4.2338414 -4.2261949 -4.2211161 -4.226017 -4.2357216 -4.2300167 -4.209342 -4.2022367 -4.22165 -4.25696 -4.2848668 -4.3031979 -4.3163471 -4.3216414][-4.2797928 -4.2729888 -4.2607431 -4.2517443 -4.2545748 -4.2586327 -4.2451258 -4.2234015 -4.2214861 -4.2405834 -4.2697892 -4.2911992 -4.3089681 -4.3240104 -4.3277359][-4.3074837 -4.3043385 -4.2940593 -4.2855253 -4.2850165 -4.2810006 -4.2586255 -4.234107 -4.232759 -4.25075 -4.2755775 -4.2926574 -4.3089232 -4.3248844 -4.3289161][-4.3196511 -4.3183579 -4.3102579 -4.3025675 -4.2998753 -4.2914171 -4.2679348 -4.246923 -4.2489281 -4.26709 -4.2868214 -4.2984729 -4.3118238 -4.3257637 -4.3282518][-4.3228631 -4.3213267 -4.3134179 -4.3055253 -4.3007827 -4.2898707 -4.26714 -4.2505908 -4.2559705 -4.2738457 -4.2902446 -4.299561 -4.313448 -4.3250155 -4.3244014]]...]
INFO - root - 2017-12-05 17:16:48.877400: step 28010, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 71h:10m:03s remains)
INFO - root - 2017-12-05 17:16:57.384381: step 28020, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 71h:51m:18s remains)
INFO - root - 2017-12-05 17:17:05.909196: step 28030, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 71h:15m:08s remains)
INFO - root - 2017-12-05 17:17:14.395916: step 28040, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 72h:57m:29s remains)
INFO - root - 2017-12-05 17:17:22.791977: step 28050, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 69h:07m:21s remains)
INFO - root - 2017-12-05 17:17:31.452087: step 28060, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 73h:15m:26s remains)
INFO - root - 2017-12-05 17:17:39.980212: step 28070, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:20m:30s remains)
INFO - root - 2017-12-05 17:17:48.512352: step 28080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:39m:14s remains)
INFO - root - 2017-12-05 17:17:57.065089: step 28090, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 72h:25m:08s remains)
INFO - root - 2017-12-05 17:18:05.494605: step 28100, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 73h:45m:16s remains)
2017-12-05 17:18:06.310151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2510118 -4.2449841 -4.24005 -4.2377071 -4.238914 -4.2429352 -4.2483368 -4.2417479 -4.223155 -4.2179337 -4.2306957 -4.2534671 -4.2791076 -4.3003683 -4.32019][-4.2503605 -4.249763 -4.2468905 -4.2442412 -4.2442241 -4.2469649 -4.2534642 -4.2522135 -4.2396884 -4.2385821 -4.2523065 -4.2730126 -4.294363 -4.3127985 -4.3293958][-4.2342529 -4.2355475 -4.2303729 -4.2235188 -4.2209377 -4.2245398 -4.2339215 -4.2403784 -4.2373781 -4.2432303 -4.2590418 -4.278254 -4.2963815 -4.3122988 -4.3283968][-4.2061667 -4.2021217 -4.188344 -4.1729269 -4.1658816 -4.1698556 -4.1820602 -4.1968908 -4.2060752 -4.2236433 -4.2493978 -4.2758703 -4.2967539 -4.3120322 -4.3272643][-4.1697512 -4.1564755 -4.1302586 -4.10266 -4.0881133 -4.0890846 -4.1004486 -4.1164966 -4.1307006 -4.1578441 -4.2021503 -4.2473793 -4.28202 -4.3056517 -4.3247995][-4.1334691 -4.1135197 -4.0803266 -4.0458527 -4.0254426 -4.0194588 -4.0181203 -4.0188341 -4.0235357 -4.0575352 -4.1236548 -4.1949615 -4.2495723 -4.2865186 -4.3162422][-4.1102839 -4.0931253 -4.0668612 -4.0381522 -4.0168476 -4.0017719 -3.9857664 -3.9714739 -3.9630075 -3.9920347 -4.065515 -4.1524615 -4.2186279 -4.263464 -4.302876][-4.1071639 -4.0973315 -4.0810089 -4.0610523 -4.0423322 -4.0237784 -4.0046558 -3.9848638 -3.9661443 -3.9833574 -4.0482597 -4.130621 -4.1966653 -4.2428908 -4.2867479][-4.1230612 -4.1172819 -4.1082025 -4.09641 -4.0831513 -4.0648689 -4.0430851 -4.0153894 -3.9851224 -3.9912047 -4.042223 -4.115087 -4.1788211 -4.2256413 -4.2740283][-4.147408 -4.1481657 -4.1453791 -4.137464 -4.1236629 -4.1019254 -4.0768538 -4.0454493 -4.0107474 -4.0140061 -4.0583849 -4.1222992 -4.1831861 -4.22753 -4.2746115][-4.1756334 -4.1812234 -4.1815405 -4.1738935 -4.1602755 -4.1411362 -4.1186738 -4.0893021 -4.059629 -4.0683808 -4.1108613 -4.1659675 -4.2162528 -4.2495961 -4.2868505][-4.2084827 -4.2144675 -4.2144265 -4.2070637 -4.1946106 -4.1795888 -4.1644812 -4.1440506 -4.1241441 -4.1374774 -4.1721525 -4.2113657 -4.2458439 -4.2697153 -4.2975712][-4.2417297 -4.2438903 -4.240757 -4.2328963 -4.223361 -4.2144384 -4.2080674 -4.1980262 -4.1891212 -4.2010388 -4.2236242 -4.2473655 -4.268086 -4.2852435 -4.306272][-4.268537 -4.2675934 -4.263763 -4.2572169 -4.2496896 -4.2439704 -4.2410054 -4.2369609 -4.2349863 -4.2441349 -4.2567072 -4.2706437 -4.2844129 -4.2961373 -4.3090286][-4.2861896 -4.2837815 -4.2806387 -4.2763104 -4.2717314 -4.269031 -4.26819 -4.2678843 -4.2686439 -4.2732964 -4.279808 -4.2882428 -4.2966542 -4.3032508 -4.3090658]]...]
INFO - root - 2017-12-05 17:18:14.732000: step 28110, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.835 sec/batch; 70h:38m:23s remains)
INFO - root - 2017-12-05 17:18:23.400502: step 28120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 73h:54m:49s remains)
INFO - root - 2017-12-05 17:18:31.909662: step 28130, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 72h:54m:33s remains)
INFO - root - 2017-12-05 17:18:40.432312: step 28140, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 71h:36m:00s remains)
INFO - root - 2017-12-05 17:18:48.897335: step 28150, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 71h:17m:05s remains)
INFO - root - 2017-12-05 17:18:57.476304: step 28160, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 73h:20m:14s remains)
INFO - root - 2017-12-05 17:19:05.894062: step 28170, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 72h:14m:35s remains)
INFO - root - 2017-12-05 17:19:14.364423: step 28180, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 73h:21m:40s remains)
INFO - root - 2017-12-05 17:19:22.946643: step 28190, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 73h:12m:37s remains)
INFO - root - 2017-12-05 17:19:31.377120: step 28200, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 71h:45m:16s remains)
2017-12-05 17:19:32.108880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.26841 -4.2710791 -4.261869 -4.247561 -4.2384543 -4.2337823 -4.2249832 -4.2226005 -4.2288966 -4.2364907 -4.2461243 -4.2529621 -4.2525105 -4.2553458 -4.2600842][-4.2210059 -4.2349687 -4.2344513 -4.2189279 -4.2043872 -4.1926651 -4.179986 -4.1830788 -4.1996245 -4.2114034 -4.2219782 -4.2303596 -4.2322154 -4.2419853 -4.2546005][-4.1698246 -4.1906519 -4.1967125 -4.1851373 -4.1699572 -4.150528 -4.1285067 -4.1260877 -4.1485648 -4.1685328 -4.1849136 -4.1965432 -4.2023029 -4.218936 -4.2384372][-4.1428957 -4.1609826 -4.1593051 -4.1447477 -4.1284947 -4.1072679 -4.08289 -4.072052 -4.0918574 -4.113955 -4.1352487 -4.1523294 -4.1648955 -4.1873188 -4.2077055][-4.1363597 -4.1522212 -4.1418414 -4.1127753 -4.0831757 -4.0575118 -4.0433593 -4.036603 -4.0481791 -4.0647249 -4.0883312 -4.1138983 -4.1376882 -4.1660576 -4.1872845][-4.139401 -4.156424 -4.1490216 -4.1089568 -4.0519404 -4.0010204 -3.9887149 -4.0094895 -4.0416551 -4.0601072 -4.0794878 -4.1050129 -4.132019 -4.1609006 -4.1840577][-4.1511416 -4.1723013 -4.1712432 -4.1314216 -4.05104 -3.9575906 -3.9239795 -3.9772995 -4.0545931 -4.0959406 -4.1145983 -4.1295195 -4.1479907 -4.1725984 -4.1975183][-4.1522264 -4.1772685 -4.1816459 -4.1508117 -4.0729456 -3.9624753 -3.8984511 -3.9568272 -4.0561981 -4.1119461 -4.1324539 -4.1421952 -4.1528974 -4.1698518 -4.199707][-4.1456118 -4.166326 -4.1667705 -4.1426454 -4.0863247 -4.0093665 -3.9681354 -4.0045638 -4.0695467 -4.1085453 -4.1256957 -4.1371188 -4.1467218 -4.1632152 -4.195981][-4.1425552 -4.1580629 -4.1495504 -4.1238647 -4.0876789 -4.052556 -4.0469737 -4.0753059 -4.1088333 -4.1275234 -4.1390524 -4.1521068 -4.161581 -4.1749196 -4.2010098][-4.1421938 -4.1629739 -4.1600857 -4.1384134 -4.1092968 -4.0910974 -4.1028419 -4.1322632 -4.155395 -4.1652193 -4.1742406 -4.1851783 -4.192903 -4.2034292 -4.22155][-4.1529765 -4.1829224 -4.1942921 -4.1847692 -4.1616449 -4.1439061 -4.1542411 -4.1833348 -4.2065072 -4.2168713 -4.2234192 -4.2282233 -4.2305875 -4.2379532 -4.2523165][-4.1938257 -4.2227345 -4.2394505 -4.2378249 -4.2229195 -4.2074423 -4.209938 -4.231853 -4.2533541 -4.2652988 -4.2720833 -4.2743821 -4.2753487 -4.2815161 -4.2935858][-4.2570767 -4.2755136 -4.2873979 -4.2890062 -4.2812247 -4.2720828 -4.2723756 -4.2837996 -4.2977996 -4.3064966 -4.3120251 -4.3152175 -4.3185859 -4.3249164 -4.3335967][-4.3165474 -4.3255782 -4.3317962 -4.33277 -4.3282132 -4.3232064 -4.32363 -4.3283887 -4.3350282 -4.3399067 -4.3430409 -4.3455739 -4.3478 -4.3505225 -4.3544993]]...]
INFO - root - 2017-12-05 17:19:40.587651: step 28210, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 73h:18m:57s remains)
INFO - root - 2017-12-05 17:19:49.026238: step 28220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 72h:18m:55s remains)
INFO - root - 2017-12-05 17:19:57.538029: step 28230, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 70h:21m:06s remains)
INFO - root - 2017-12-05 17:20:06.016210: step 28240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 71h:45m:20s remains)
INFO - root - 2017-12-05 17:20:14.597499: step 28250, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 70h:59m:03s remains)
INFO - root - 2017-12-05 17:20:23.146969: step 28260, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 73h:19m:14s remains)
INFO - root - 2017-12-05 17:20:31.665187: step 28270, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 73h:38m:10s remains)
INFO - root - 2017-12-05 17:20:40.126670: step 28280, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 73h:26m:52s remains)
INFO - root - 2017-12-05 17:20:48.521396: step 28290, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.801 sec/batch; 67h:41m:25s remains)
INFO - root - 2017-12-05 17:20:57.005755: step 28300, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 70h:39m:35s remains)
2017-12-05 17:20:57.843069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2876735 -4.2842011 -4.2774916 -4.2710309 -4.2565417 -4.23256 -4.2058921 -4.18647 -4.1783776 -4.1911974 -4.2105346 -4.2206297 -4.2090898 -4.1878567 -4.1691122][-4.2758641 -4.2690692 -4.2619524 -4.2563109 -4.2396197 -4.2091041 -4.1764369 -4.156446 -4.1523128 -4.1766024 -4.2067752 -4.2217717 -4.213028 -4.1943631 -4.174][-4.2724867 -4.2674322 -4.2600722 -4.2483168 -4.2211518 -4.1809959 -4.14457 -4.1247635 -4.12407 -4.1614261 -4.2032266 -4.2215018 -4.2150545 -4.2005272 -4.181459][-4.2796149 -4.2750335 -4.2616048 -4.2380033 -4.1966872 -4.1459937 -4.0999064 -4.0743866 -4.0850878 -4.142436 -4.1958628 -4.21902 -4.2173905 -4.2076058 -4.1890554][-4.2910604 -4.2849507 -4.26605 -4.2331252 -4.1841264 -4.1226444 -4.0600996 -4.0209723 -4.0440645 -4.1213617 -4.1885257 -4.2274036 -4.2364516 -4.2292624 -4.2100096][-4.2994766 -4.2899837 -4.2677178 -4.2317266 -4.1752825 -4.0946255 -4.0034776 -3.9457121 -3.9920259 -4.0964766 -4.1844406 -4.2471943 -4.2689719 -4.2654696 -4.2428532][-4.2996912 -4.2914038 -4.2691746 -4.228694 -4.1555424 -4.0476837 -3.9152851 -3.8307507 -3.9153283 -4.0603504 -4.1785326 -4.2641611 -4.2992773 -4.2972565 -4.2703681][-4.2960229 -4.2920079 -4.2716851 -4.2238111 -4.1329169 -4.0064144 -3.8500383 -3.7652452 -3.89088 -4.0613346 -4.1873307 -4.2736549 -4.314589 -4.3165536 -4.2938561][-4.2892737 -4.2896481 -4.2703476 -4.2206922 -4.1322765 -4.0151076 -3.8855884 -3.8413582 -3.9606304 -4.10082 -4.2026744 -4.2745419 -4.3130136 -4.3197083 -4.3088884][-4.2832494 -4.2844825 -4.26693 -4.2229409 -4.1478572 -4.0494871 -3.9539165 -3.9403358 -4.0392232 -4.1428418 -4.2200179 -4.2782521 -4.3094425 -4.3154616 -4.3137569][-4.2821093 -4.2813015 -4.2643137 -4.225842 -4.1636543 -4.0828514 -4.0129542 -4.0210624 -4.1034646 -4.1819882 -4.2447939 -4.2915606 -4.3117027 -4.3121996 -4.311718][-4.2867556 -4.2822394 -4.2659354 -4.2334328 -4.1853952 -4.126471 -4.081852 -4.1039133 -4.1707554 -4.2285991 -4.2753854 -4.3062491 -4.3151579 -4.3116374 -4.3077884][-4.2854691 -4.2765307 -4.2605591 -4.2337646 -4.199121 -4.1635709 -4.1455417 -4.1739163 -4.2238846 -4.2643309 -4.2951775 -4.312201 -4.3141103 -4.3103247 -4.3064733][-4.2841196 -4.2726927 -4.2570238 -4.2368345 -4.2166209 -4.2015204 -4.1992984 -4.223794 -4.258594 -4.2855439 -4.30411 -4.3136358 -4.3141065 -4.312593 -4.3105922][-4.2887435 -4.2764874 -4.2639246 -4.2505584 -4.2395363 -4.2327824 -4.2355919 -4.2519326 -4.2745485 -4.2953086 -4.3092418 -4.3162217 -4.316844 -4.3181624 -4.3190837]]...]
INFO - root - 2017-12-05 17:21:06.396708: step 28310, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 72h:58m:11s remains)
INFO - root - 2017-12-05 17:21:14.912131: step 28320, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 71h:14m:11s remains)
INFO - root - 2017-12-05 17:21:23.359772: step 28330, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 72h:03m:56s remains)
INFO - root - 2017-12-05 17:21:31.842865: step 28340, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 73h:13m:58s remains)
INFO - root - 2017-12-05 17:21:40.453893: step 28350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:26m:44s remains)
INFO - root - 2017-12-05 17:21:48.937184: step 28360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:16m:27s remains)
INFO - root - 2017-12-05 17:21:57.481293: step 28370, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 70h:59m:37s remains)
INFO - root - 2017-12-05 17:22:05.995296: step 28380, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 71h:30m:59s remains)
INFO - root - 2017-12-05 17:22:14.514326: step 28390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 72h:29m:33s remains)
INFO - root - 2017-12-05 17:22:23.090938: step 28400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 71h:37m:13s remains)
2017-12-05 17:22:23.845672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30527 -4.2976031 -4.2896314 -4.2850695 -4.282228 -4.2811766 -4.280724 -4.2849417 -4.2893558 -4.2911668 -4.295311 -4.3018422 -4.3107734 -4.3214622 -4.3315754][-4.2949061 -4.2868953 -4.2805843 -4.2784462 -4.27534 -4.2719426 -4.2665377 -4.26624 -4.2663693 -4.2640638 -4.2673674 -4.2759624 -4.2886105 -4.3053465 -4.3214474][-4.2854643 -4.2758656 -4.269763 -4.2677565 -4.2631555 -4.2565093 -4.2475643 -4.2449222 -4.2450919 -4.2428436 -4.2482867 -4.2609549 -4.2764049 -4.2956944 -4.3150959][-4.2728434 -4.259326 -4.2501278 -4.2451315 -4.2359929 -4.2240195 -4.213244 -4.2149825 -4.2217937 -4.2255874 -4.2354789 -4.2518125 -4.2690554 -4.2895436 -4.310914][-4.2591472 -4.2447014 -4.2357559 -4.2296686 -4.2153206 -4.1948953 -4.1770864 -4.18005 -4.1963005 -4.2101831 -4.2282176 -4.2511549 -4.271162 -4.290844 -4.3105569][-4.2389226 -4.2197104 -4.2067051 -4.1942711 -4.1668067 -4.1291118 -4.098999 -4.10421 -4.1378517 -4.168345 -4.2015362 -4.2384019 -4.2666607 -4.2896428 -4.3097658][-4.2269697 -4.1997037 -4.1742272 -4.1418481 -4.086719 -4.0154724 -3.9605935 -3.9731812 -4.0389085 -4.0953326 -4.1488438 -4.2034688 -4.243578 -4.2766371 -4.3038335][-4.2091026 -4.1671166 -4.1231461 -4.0659108 -3.9836059 -3.8774469 -3.7857122 -3.8048177 -3.9101751 -4.00498 -4.0860109 -4.1616616 -4.2146711 -4.2581968 -4.2943783][-4.2126842 -4.1672978 -4.1203947 -4.0626335 -3.9895396 -3.899941 -3.8134823 -3.8225381 -3.9170203 -4.0105782 -4.0893312 -4.16005 -4.2092094 -4.2533994 -4.2922292][-4.2418365 -4.2091541 -4.1747289 -4.1330838 -4.0864806 -4.0348372 -3.9813361 -3.9769807 -4.0291004 -4.0907683 -4.1444197 -4.1943469 -4.2294111 -4.2641439 -4.2943349][-4.2609529 -4.2428269 -4.2234378 -4.1970062 -4.1681395 -4.1424437 -4.1130838 -4.1042018 -4.13139 -4.1689668 -4.2040811 -4.2360587 -4.2560382 -4.2792997 -4.300034][-4.2693996 -4.2602115 -4.2499204 -4.2343922 -4.2168927 -4.2039595 -4.1893258 -4.18318 -4.1989856 -4.220191 -4.2432842 -4.2650251 -4.2763996 -4.2912288 -4.30521][-4.2740097 -4.2652388 -4.2557459 -4.2429132 -4.2284403 -4.218246 -4.2097945 -4.2089162 -4.2236447 -4.2380838 -4.256846 -4.2760839 -4.2867141 -4.29827 -4.3100796][-4.2842188 -4.27525 -4.2651372 -4.2529931 -4.2414904 -4.2332306 -4.2290144 -4.2320375 -4.24321 -4.2527423 -4.2684355 -4.2865076 -4.2983537 -4.3085542 -4.31814][-4.2964334 -4.2885423 -4.279685 -4.2705417 -4.2614188 -4.2550211 -4.2545466 -4.2592645 -4.2675476 -4.2735949 -4.2845225 -4.2991505 -4.31067 -4.31945 -4.3264751]]...]
INFO - root - 2017-12-05 17:22:32.289500: step 28410, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:48m:27s remains)
INFO - root - 2017-12-05 17:22:40.690446: step 28420, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 69h:24m:58s remains)
INFO - root - 2017-12-05 17:22:49.097118: step 28430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 71h:06m:45s remains)
INFO - root - 2017-12-05 17:22:57.647551: step 28440, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 73h:36m:01s remains)
INFO - root - 2017-12-05 17:23:06.098437: step 28450, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 69h:05m:07s remains)
INFO - root - 2017-12-05 17:23:14.672962: step 28460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 71h:35m:18s remains)
INFO - root - 2017-12-05 17:23:23.037464: step 28470, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 71h:30m:55s remains)
INFO - root - 2017-12-05 17:23:31.647374: step 28480, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 73h:18m:39s remains)
INFO - root - 2017-12-05 17:23:40.181923: step 28490, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 74h:08m:51s remains)
INFO - root - 2017-12-05 17:23:48.739326: step 28500, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.865 sec/batch; 73h:01m:37s remains)
2017-12-05 17:23:49.503176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2590919 -4.2509837 -4.2506022 -4.2616243 -4.2727838 -4.275485 -4.2708735 -4.2636037 -4.2509027 -4.2339773 -4.2277303 -4.2339764 -4.2388477 -4.2496147 -4.2726555][-4.2554188 -4.2468204 -4.243072 -4.2513032 -4.2619696 -4.2628236 -4.2554064 -4.2422857 -4.2204876 -4.1965423 -4.1912904 -4.2051649 -4.2192349 -4.2365322 -4.2637429][-4.2427649 -4.2312202 -4.2237868 -4.2302909 -4.2415395 -4.2390971 -4.2269573 -4.2097721 -4.1857619 -4.1646566 -4.1636953 -4.1852865 -4.210237 -4.2338729 -4.26392][-4.2174983 -4.1982646 -4.1872978 -4.1941004 -4.204268 -4.1947217 -4.1745529 -4.1576328 -4.1373515 -4.1234603 -4.1259704 -4.1517119 -4.1854472 -4.2179208 -4.254178][-4.1836133 -4.1581459 -4.1464109 -4.1539788 -4.1585541 -4.130434 -4.0910473 -4.0766354 -4.0675297 -4.0627079 -4.06616 -4.0939927 -4.1399617 -4.1899524 -4.2358642][-4.1459503 -4.1153946 -4.1003456 -4.0997195 -4.0865269 -4.0284576 -3.9666111 -3.9597311 -3.9661903 -3.9740226 -3.993017 -4.0403981 -4.1061411 -4.1739259 -4.2292352][-4.1071525 -4.0700517 -4.0462003 -4.0258341 -3.9787581 -3.8883586 -3.814611 -3.8246322 -3.8609035 -3.9037232 -3.9580777 -4.0341187 -4.1156292 -4.188868 -4.244246][-4.1012235 -4.0542464 -4.018084 -3.9808788 -3.9187503 -3.8269956 -3.7704916 -3.802125 -3.8722644 -3.9467027 -4.0193782 -4.0935225 -4.1647325 -4.2268014 -4.2743587][-4.1274805 -4.0795336 -4.0434875 -4.0108328 -3.9681456 -3.9141884 -3.8894746 -3.9249256 -3.9898834 -4.05725 -4.119379 -4.1759014 -4.229372 -4.275651 -4.3088322][-4.1648269 -4.1244659 -4.0981903 -4.0755486 -4.0508604 -4.0306172 -4.0291581 -4.0586414 -4.1049681 -4.1537919 -4.2027807 -4.2457876 -4.2860708 -4.3168755 -4.3354416][-4.1995325 -4.1734595 -4.1645947 -4.1561971 -4.1417708 -4.1389527 -4.1510553 -4.1760082 -4.2063212 -4.2364616 -4.2674918 -4.2958302 -4.3222246 -4.3407612 -4.3495307][-4.2301373 -4.21367 -4.2133641 -4.2148008 -4.211225 -4.2181082 -4.2345328 -4.2567086 -4.27752 -4.2941551 -4.3117671 -4.3277574 -4.3426566 -4.3525743 -4.3534217][-4.2619767 -4.251214 -4.2542419 -4.26183 -4.2661557 -4.276772 -4.2916603 -4.3056035 -4.3155966 -4.3240514 -4.3323388 -4.3401551 -4.3486328 -4.353673 -4.350769][-4.2902517 -4.2850509 -4.2891607 -4.2985015 -4.3053126 -4.3119884 -4.3190188 -4.324894 -4.3287845 -4.3300772 -4.3320651 -4.3362451 -4.3406653 -4.3423462 -4.3389983][-4.3060493 -4.3017468 -4.3012767 -4.3054996 -4.309906 -4.3135591 -4.3170142 -4.320096 -4.3210826 -4.3196282 -4.3187928 -4.3200603 -4.3228869 -4.3254085 -4.3260741]]...]
INFO - root - 2017-12-05 17:23:58.051677: step 28510, loss = 2.02, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 72h:48m:00s remains)
INFO - root - 2017-12-05 17:24:06.552579: step 28520, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 72h:50m:49s remains)
INFO - root - 2017-12-05 17:24:15.073451: step 28530, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 72h:29m:41s remains)
INFO - root - 2017-12-05 17:24:23.477857: step 28540, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 72h:29m:31s remains)
INFO - root - 2017-12-05 17:24:31.977927: step 28550, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 69h:12m:12s remains)
INFO - root - 2017-12-05 17:24:40.516763: step 28560, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 72h:37m:53s remains)
INFO - root - 2017-12-05 17:24:48.931730: step 28570, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 70h:37m:39s remains)
INFO - root - 2017-12-05 17:24:57.504525: step 28580, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 72h:12m:53s remains)
INFO - root - 2017-12-05 17:25:06.058302: step 28590, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 72h:37m:05s remains)
INFO - root - 2017-12-05 17:25:14.576542: step 28600, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 70h:38m:36s remains)
2017-12-05 17:25:15.409495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1921945 -4.1910663 -4.175055 -4.1439142 -4.0874758 -4.0286984 -4.022346 -4.077672 -4.141211 -4.2039576 -4.2551689 -4.2687364 -4.2688556 -4.274775 -4.2781677][-4.2122679 -4.2090688 -4.1868997 -4.1522665 -4.0968218 -4.0336647 -4.0232587 -4.0720639 -4.1328692 -4.1960778 -4.2461267 -4.2588253 -4.259058 -4.2680459 -4.2748208][-4.2283692 -4.2275929 -4.2005224 -4.1610847 -4.0997152 -4.0272841 -4.0047607 -4.0434818 -4.1051984 -4.1724534 -4.22638 -4.2464366 -4.2530322 -4.2670217 -4.2774277][-4.2379045 -4.2400074 -4.2089319 -4.1620994 -4.0910244 -4.0043049 -3.9650707 -3.9945791 -4.0633044 -4.1423745 -4.2047677 -4.2368603 -4.2493978 -4.2626538 -4.2722683][-4.2432475 -4.2473564 -4.2164369 -4.1609216 -4.0790992 -3.9799068 -3.9237213 -3.9445379 -4.0229683 -4.1130104 -4.1839352 -4.2249117 -4.2421227 -4.25238 -4.2580891][-4.2466354 -4.252244 -4.2244272 -4.1657252 -4.0764179 -3.9616864 -3.8825412 -3.8899202 -3.9781585 -4.0843287 -4.1699638 -4.2214136 -4.2441134 -4.24979 -4.24508][-4.2559848 -4.2612314 -4.2410645 -4.1846271 -4.0863004 -3.9524746 -3.8462281 -3.8444571 -3.9520297 -4.0749111 -4.1679668 -4.2221017 -4.246016 -4.24396 -4.2267137][-4.2673388 -4.2725291 -4.2618961 -4.2127242 -4.11359 -3.9711576 -3.8533349 -3.8505869 -3.9672072 -4.0859337 -4.1704321 -4.2237954 -4.2459564 -4.2386246 -4.2107038][-4.2765646 -4.2787104 -4.2753062 -4.2368779 -4.1442919 -4.0094948 -3.8973136 -3.8872318 -3.9851804 -4.0826821 -4.1551981 -4.2097063 -4.2329783 -4.22435 -4.1935554][-4.2730188 -4.2724557 -4.2738142 -4.2482557 -4.1719103 -4.0577955 -3.9624248 -3.9408951 -4.0067186 -4.0779767 -4.1385207 -4.1912251 -4.2116566 -4.2055674 -4.1833935][-4.2615862 -4.2635574 -4.2705832 -4.2556148 -4.198523 -4.1053905 -4.0200839 -3.9880342 -4.0290179 -4.0827489 -4.1366935 -4.1834393 -4.1989284 -4.1951065 -4.1802821][-4.2545395 -4.26488 -4.27762 -4.2675242 -4.2262678 -4.1505904 -4.0683603 -4.0269675 -4.0487576 -4.0888615 -4.1374044 -4.1805673 -4.1944785 -4.1952348 -4.188128][-4.2497559 -4.2739353 -4.2940588 -4.2879877 -4.2566013 -4.198988 -4.1297154 -4.0839496 -4.0876102 -4.1144886 -4.1571889 -4.1967688 -4.2096443 -4.21404 -4.211503][-4.2477145 -4.2858973 -4.3122849 -4.3103194 -4.2865291 -4.2440505 -4.1901593 -4.1469278 -4.1393647 -4.1559443 -4.1867242 -4.2192879 -4.2313714 -4.2391205 -4.2448559][-4.2331052 -4.285306 -4.3186092 -4.3212333 -4.3031 -4.2716837 -4.2347088 -4.1995125 -4.1864858 -4.1925912 -4.2132773 -4.2420516 -4.2586989 -4.2735624 -4.2848039]]...]
INFO - root - 2017-12-05 17:25:23.929277: step 28610, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 74h:27m:14s remains)
INFO - root - 2017-12-05 17:25:32.398097: step 28620, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 72h:09m:32s remains)
INFO - root - 2017-12-05 17:25:40.903818: step 28630, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:45m:47s remains)
INFO - root - 2017-12-05 17:25:49.398853: step 28640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 71h:51m:26s remains)
INFO - root - 2017-12-05 17:25:57.759800: step 28650, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 71h:36m:33s remains)
INFO - root - 2017-12-05 17:26:06.248683: step 28660, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 74h:06m:03s remains)
INFO - root - 2017-12-05 17:26:14.769378: step 28670, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:09m:59s remains)
INFO - root - 2017-12-05 17:26:23.440649: step 28680, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:11m:04s remains)
INFO - root - 2017-12-05 17:26:32.071037: step 28690, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 70h:13m:55s remains)
INFO - root - 2017-12-05 17:26:40.618136: step 28700, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 71h:10m:06s remains)
2017-12-05 17:26:41.351699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9534414 -4.0376005 -4.1261349 -4.1986737 -4.2498589 -4.2811341 -4.2980623 -4.3167195 -4.3354979 -4.3429461 -4.3387775 -4.3325152 -4.3279781 -4.3273768 -4.3273306][-4.0759916 -4.1174417 -4.1659608 -4.2097683 -4.2408056 -4.2587366 -4.2751269 -4.299562 -4.3263087 -4.3401985 -4.3383384 -4.3320408 -4.327384 -4.3260388 -4.3250761][-4.1841226 -4.1968842 -4.2148943 -4.2289429 -4.2340865 -4.2345333 -4.2450452 -4.271832 -4.3066597 -4.3319545 -4.339107 -4.3351512 -4.3294687 -4.3260922 -4.3234859][-4.2644849 -4.2626777 -4.2524595 -4.2353568 -4.2074652 -4.18879 -4.1951494 -4.226748 -4.2704873 -4.3072004 -4.3269339 -4.3325968 -4.3310671 -4.327949 -4.3242249][-4.2977958 -4.294426 -4.2705445 -4.2289104 -4.169374 -4.1200428 -4.1139565 -4.1539254 -4.2130461 -4.2636466 -4.2992392 -4.3215127 -4.3318725 -4.3321958 -4.3285131][-4.2868237 -4.2911849 -4.2650957 -4.205512 -4.1109743 -4.0179949 -3.9891341 -4.0451446 -4.1357336 -4.21091 -4.2647634 -4.3052883 -4.3301091 -4.335947 -4.333744][-4.261169 -4.275919 -4.2547 -4.18553 -4.0619993 -3.9219708 -3.8459487 -3.9073622 -4.0386019 -4.1493063 -4.2273054 -4.2852612 -4.3216314 -4.3348265 -4.3356347][-4.2273455 -4.263597 -4.2559257 -4.1917872 -4.0623565 -3.8956459 -3.7669878 -3.804776 -3.9570036 -4.0962439 -4.1978364 -4.2684016 -4.3117776 -4.3282008 -4.3328538][-4.1951122 -4.2513876 -4.2592754 -4.2157817 -4.1120338 -3.9728053 -3.8498287 -3.8537953 -3.970988 -4.096652 -4.1993017 -4.2705517 -4.3120742 -4.3262191 -4.332252][-4.1891322 -4.2491031 -4.2628045 -4.2364 -4.1693325 -4.0856476 -4.0050559 -3.9904153 -4.0538769 -4.1383963 -4.2225485 -4.2832289 -4.316505 -4.3275862 -4.3336282][-4.2030892 -4.2529655 -4.2635913 -4.2440457 -4.2040782 -4.1680946 -4.1319003 -4.1212468 -4.1521344 -4.2030621 -4.260005 -4.3053722 -4.328052 -4.3340983 -4.3377752][-4.228652 -4.2619991 -4.2624617 -4.2414784 -4.2161045 -4.2099862 -4.2054839 -4.2094569 -4.2332764 -4.2703614 -4.3085942 -4.3360291 -4.3460913 -4.3455257 -4.3440514][-4.2587948 -4.2724314 -4.2541595 -4.2178903 -4.1894612 -4.1924562 -4.2115645 -4.2385855 -4.2688689 -4.3058372 -4.3390789 -4.3556027 -4.3568916 -4.3527513 -4.3492284][-4.2763376 -4.2737384 -4.2341127 -4.1720519 -4.1151948 -4.1022367 -4.1417842 -4.2030835 -4.2568622 -4.3054676 -4.3449788 -4.36296 -4.3619757 -4.3550935 -4.3516264][-4.2658119 -4.2602487 -4.2085 -4.1189036 -4.0173645 -3.9597321 -4.0074277 -4.1148953 -4.2053432 -4.2754331 -4.3281693 -4.3539829 -4.3561482 -4.3507109 -4.35022]]...]
INFO - root - 2017-12-05 17:26:49.742477: step 28710, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 71h:14m:33s remains)
INFO - root - 2017-12-05 17:26:58.263738: step 28720, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 73h:02m:48s remains)
INFO - root - 2017-12-05 17:27:06.845640: step 28730, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 73h:07m:27s remains)
INFO - root - 2017-12-05 17:27:15.374076: step 28740, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 72h:52m:27s remains)
INFO - root - 2017-12-05 17:27:23.804098: step 28750, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 70h:43m:01s remains)
INFO - root - 2017-12-05 17:27:32.278082: step 28760, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 73h:09m:45s remains)
INFO - root - 2017-12-05 17:27:40.716688: step 28770, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:28m:09s remains)
INFO - root - 2017-12-05 17:27:49.275087: step 28780, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 71h:18m:01s remains)
INFO - root - 2017-12-05 17:27:57.755277: step 28790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 72h:18m:34s remains)
INFO - root - 2017-12-05 17:28:06.194563: step 28800, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.810 sec/batch; 68h:19m:13s remains)
2017-12-05 17:28:06.942148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2269974 -4.2276077 -4.2249608 -4.2205706 -4.2129292 -4.2043996 -4.1961751 -4.2020297 -4.2120075 -4.2251258 -4.24327 -4.2591968 -4.2739606 -4.2897081 -4.3106723][-4.237927 -4.2409563 -4.2385535 -4.2324319 -4.2281322 -4.2228842 -4.2156944 -4.2186131 -4.2193646 -4.2263551 -4.2445092 -4.2618346 -4.2777252 -4.294054 -4.31715][-4.2243013 -4.2391825 -4.2453222 -4.2455044 -4.24895 -4.2428308 -4.2316051 -4.2287092 -4.2240539 -4.2288589 -4.2435203 -4.2606177 -4.2790165 -4.29572 -4.317637][-4.1811976 -4.2062759 -4.2218685 -4.2324228 -4.2454267 -4.2409792 -4.2258687 -4.2206688 -4.2165227 -4.2221627 -4.2365127 -4.2566357 -4.2791839 -4.2988539 -4.3192086][-4.1182256 -4.1476097 -4.1707516 -4.19161 -4.2131972 -4.2169743 -4.2101164 -4.2145867 -4.2179089 -4.227643 -4.2427411 -4.2623734 -4.2824631 -4.2998343 -4.317894][-4.0547323 -4.0786195 -4.1043944 -4.1297684 -4.1575179 -4.1670637 -4.1676888 -4.1803803 -4.1886549 -4.2037325 -4.2282505 -4.2545052 -4.2781324 -4.2958913 -4.3142738][-4.0075779 -4.0156589 -4.028348 -4.0487247 -4.0754972 -4.0881743 -4.096344 -4.1181669 -4.1351252 -4.1610227 -4.1993685 -4.23481 -4.2651243 -4.2876325 -4.31092][-4.0127492 -4.0016766 -3.9940417 -3.9995997 -4.0143571 -4.0217247 -4.0317535 -4.05919 -4.0877223 -4.1302934 -4.182838 -4.2246857 -4.2584186 -4.2823524 -4.3097539][-4.0729132 -4.0557928 -4.0394363 -4.0336857 -4.0340047 -4.0329957 -4.0434985 -4.0702147 -4.1013417 -4.1452961 -4.1916003 -4.2258029 -4.2559218 -4.2792797 -4.3085814][-4.143784 -4.1348281 -4.1241961 -4.1173973 -4.110466 -4.10334 -4.1112056 -4.1296692 -4.152833 -4.1856637 -4.2138848 -4.2335091 -4.2553082 -4.2760091 -4.3056831][-4.2023878 -4.2035985 -4.1991262 -4.1919856 -4.1794677 -4.1627855 -4.1646371 -4.1747017 -4.1893525 -4.2121835 -4.2301316 -4.242105 -4.2579436 -4.2774563 -4.3058605][-4.2417426 -4.2475004 -4.2442107 -4.2339754 -4.2171574 -4.1933956 -4.1890182 -4.19425 -4.20606 -4.2251906 -4.2426867 -4.25506 -4.2678766 -4.2846923 -4.3100715][-4.2541609 -4.2632079 -4.2608881 -4.2492356 -4.229846 -4.2043509 -4.2009535 -4.2052717 -4.2142105 -4.2334309 -4.253684 -4.2696147 -4.2817154 -4.2947493 -4.3164434][-4.2331071 -4.2424731 -4.24174 -4.231185 -4.2133884 -4.1908655 -4.1939416 -4.202806 -4.2133045 -4.2357969 -4.2591329 -4.2800059 -4.2925925 -4.3043156 -4.3229][-4.1922846 -4.2006493 -4.2019153 -4.1928692 -4.1774735 -4.1618423 -4.1723824 -4.1861534 -4.2049 -4.2361922 -4.2648406 -4.2877655 -4.2999368 -4.3126326 -4.3284931]]...]
INFO - root - 2017-12-05 17:28:15.476785: step 28810, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 71h:05m:56s remains)
INFO - root - 2017-12-05 17:28:23.956336: step 28820, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 73h:23m:21s remains)
INFO - root - 2017-12-05 17:28:32.419786: step 28830, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 72h:41m:29s remains)
INFO - root - 2017-12-05 17:28:40.931383: step 28840, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 70h:09m:38s remains)
INFO - root - 2017-12-05 17:28:49.374159: step 28850, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 74h:06m:39s remains)
INFO - root - 2017-12-05 17:28:57.706242: step 28860, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 71h:32m:51s remains)
INFO - root - 2017-12-05 17:29:05.826502: step 28870, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 71h:34m:36s remains)
INFO - root - 2017-12-05 17:29:14.357694: step 28880, loss = 2.05, batch loss = 2.00 (10.0 examples/sec; 0.800 sec/batch; 67h:26m:04s remains)
INFO - root - 2017-12-05 17:29:22.838264: step 28890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:55m:59s remains)
INFO - root - 2017-12-05 17:29:31.309621: step 28900, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 71h:13m:12s remains)
2017-12-05 17:29:32.070415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1655703 -4.1491952 -4.1371188 -4.1259084 -4.112412 -4.095645 -4.0987883 -4.1352034 -4.1763606 -4.1996851 -4.2161202 -4.2384448 -4.2527003 -4.2640548 -4.2798452][-4.1634164 -4.14509 -4.1302056 -4.113905 -4.1074247 -4.1008687 -4.1045132 -4.1408911 -4.1848936 -4.2042322 -4.2140112 -4.2357388 -4.2548666 -4.271718 -4.2863894][-4.1679721 -4.1464744 -4.1299672 -4.1161036 -4.1106844 -4.0972047 -4.0872531 -4.1172433 -4.1694975 -4.195312 -4.2058883 -4.2285733 -4.2528639 -4.2743893 -4.2886748][-4.1696396 -4.143384 -4.128808 -4.1226783 -4.1188064 -4.0891171 -4.0514207 -4.0652695 -4.1237411 -4.1661863 -4.1867604 -4.2160244 -4.2467747 -4.27396 -4.2895684][-4.1718802 -4.1437554 -4.1302867 -4.1278486 -4.1243739 -4.0822811 -4.0163374 -4.0159063 -4.0818877 -4.1399908 -4.1717467 -4.2105889 -4.2466035 -4.2746053 -4.2900519][-4.1664166 -4.1406069 -4.1263151 -4.1168895 -4.1045089 -4.0491705 -3.9563932 -3.9511845 -4.0360141 -4.1115308 -4.155519 -4.2019176 -4.2418084 -4.2703838 -4.2846804][-4.1500449 -4.12398 -4.1077528 -4.08075 -4.0478363 -3.9696579 -3.8352871 -3.8188522 -3.9407873 -4.0482321 -4.1089115 -4.1666064 -4.2153935 -4.2521191 -4.2714176][-4.131031 -4.0970883 -4.0737095 -4.033082 -3.9886429 -3.8968537 -3.7313051 -3.697032 -3.840826 -3.9670568 -4.0380754 -4.1107063 -4.1757412 -4.2247224 -4.25465][-4.1396713 -4.1028967 -4.0757904 -4.0347691 -3.9996021 -3.9288807 -3.7913225 -3.7552927 -3.8716025 -3.9734824 -4.0273943 -4.0960855 -4.165606 -4.2185187 -4.2517381][-4.1640973 -4.127491 -4.0964546 -4.0561924 -4.0262341 -3.9822648 -3.8911448 -3.867866 -3.9482491 -4.0185747 -4.0571613 -4.1151853 -4.18083 -4.2323446 -4.2629395][-4.1924767 -4.1583405 -4.123847 -4.0828247 -4.0540085 -4.0288916 -3.9722831 -3.9606423 -4.0189466 -4.0696421 -4.101337 -4.1492877 -4.2072873 -4.2552986 -4.2805657][-4.2230039 -4.1940718 -4.1604085 -4.1235876 -4.0998411 -4.0863781 -4.0501618 -4.0440478 -4.0911255 -4.129756 -4.156601 -4.1940632 -4.2405105 -4.280448 -4.2999954][-4.2444038 -4.2220168 -4.1959591 -4.1682754 -4.1547618 -4.150723 -4.1309109 -4.1258245 -4.1614466 -4.1899719 -4.2097507 -4.2349029 -4.2669215 -4.2956634 -4.3092532][-4.2606187 -4.2453074 -4.2292662 -4.2129154 -4.2078781 -4.2076526 -4.1985703 -4.1950769 -4.2148108 -4.2310543 -4.2433505 -4.2596688 -4.2799091 -4.2983217 -4.3063073][-4.2727666 -4.2607126 -4.2507319 -4.2421546 -4.240272 -4.2395334 -4.2368703 -4.2361913 -4.2429304 -4.24879 -4.2563982 -4.2675171 -4.280273 -4.2920437 -4.2987747]]...]
INFO - root - 2017-12-05 17:29:40.517055: step 28910, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 71h:34m:23s remains)
INFO - root - 2017-12-05 17:29:49.051487: step 28920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 71h:01m:09s remains)
INFO - root - 2017-12-05 17:29:57.636839: step 28930, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 70h:02m:01s remains)
INFO - root - 2017-12-05 17:30:06.132862: step 28940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 72h:52m:11s remains)
INFO - root - 2017-12-05 17:30:14.644476: step 28950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 73h:01m:31s remains)
INFO - root - 2017-12-05 17:30:23.185903: step 28960, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 73h:01m:40s remains)
INFO - root - 2017-12-05 17:30:31.510258: step 28970, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.829 sec/batch; 69h:55m:22s remains)
INFO - root - 2017-12-05 17:30:39.930828: step 28980, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 71h:01m:34s remains)
INFO - root - 2017-12-05 17:30:48.402519: step 28990, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.852 sec/batch; 71h:52m:01s remains)
INFO - root - 2017-12-05 17:30:56.856496: step 29000, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:55m:01s remains)
2017-12-05 17:30:57.684987: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28468 -4.2932925 -4.2982464 -4.3011184 -4.3022771 -4.3031907 -4.304697 -4.3036871 -4.3031044 -4.3044462 -4.304347 -4.2958388 -4.271873 -4.2294941 -4.1761785][-4.2866278 -4.2951064 -4.298399 -4.2987242 -4.2968912 -4.2949915 -4.2936497 -4.2900214 -4.2891273 -4.2900782 -4.2876425 -4.269424 -4.2279363 -4.1603689 -4.0774665][-4.2917318 -4.2981553 -4.2976828 -4.2947807 -4.2895603 -4.2848229 -4.2804651 -4.27373 -4.2679548 -4.2625861 -4.2528629 -4.2253089 -4.1736646 -4.0975385 -4.0060959][-4.2961106 -4.30072 -4.2975941 -4.2937613 -4.2866611 -4.278646 -4.270824 -4.260663 -4.2467194 -4.2312269 -4.2176805 -4.19499 -4.157721 -4.1059852 -4.0491872][-4.3001161 -4.3044505 -4.3007636 -4.2964859 -4.2858715 -4.2720027 -4.2549539 -4.2349758 -4.2098441 -4.1875033 -4.1804595 -4.1780286 -4.1724772 -4.1559944 -4.1369076][-4.3009381 -4.3036976 -4.2968111 -4.2856703 -4.266017 -4.2402606 -4.2093096 -4.1763172 -4.1340051 -4.1069627 -4.1166105 -4.1428022 -4.1701794 -4.1860752 -4.1969318][-4.287333 -4.2831845 -4.2685604 -4.2468696 -4.2157435 -4.1758056 -4.1300025 -4.0843015 -4.0277457 -4.007627 -4.0461745 -4.1048994 -4.16274 -4.2113118 -4.2457581][-4.2663612 -4.2497864 -4.2228556 -4.1887712 -4.149323 -4.1010232 -4.0484776 -3.9955788 -3.9453306 -3.9537547 -4.0201216 -4.0966043 -4.1703906 -4.2350764 -4.2763591][-4.2451806 -4.2184744 -4.1865969 -4.1516728 -4.1156807 -4.0789504 -4.044261 -4.0109081 -3.9875598 -4.0094547 -4.0670772 -4.1291566 -4.1901488 -4.2419629 -4.2689309][-4.2230678 -4.1930723 -4.167583 -4.1475229 -4.1293206 -4.1128893 -4.1039462 -4.0993614 -4.0993171 -4.1182952 -4.1504498 -4.1849828 -4.2154527 -4.2390318 -4.2412624][-4.1999087 -4.1710529 -4.1547723 -4.15403 -4.1573639 -4.1604557 -4.1699405 -4.1829529 -4.1930485 -4.2075462 -4.2217221 -4.2311754 -4.2332582 -4.234199 -4.2199426][-4.1953254 -4.1719117 -4.1644297 -4.1763172 -4.1928177 -4.2115178 -4.2340517 -4.2548194 -4.2651658 -4.2706671 -4.2636786 -4.2480888 -4.2214251 -4.2017789 -4.1863947][-4.2191372 -4.2078762 -4.2074766 -4.2214465 -4.2383714 -4.2584248 -4.2813458 -4.2955828 -4.2959156 -4.2817283 -4.250679 -4.2058439 -4.1470447 -4.1142039 -4.1170969][-4.2463708 -4.2419243 -4.2410784 -4.247612 -4.2548013 -4.2634192 -4.273304 -4.2708058 -4.2533236 -4.21762 -4.1636181 -4.0913696 -4.00572 -3.98253 -4.0247941][-4.2703381 -4.2655892 -4.2591653 -4.2548389 -4.2480612 -4.2409735 -4.234241 -4.2132745 -4.1794677 -4.1300106 -4.067997 -3.9945631 -3.9207392 -3.9272804 -3.9967334]]...]
INFO - root - 2017-12-05 17:31:06.190863: step 29010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 72h:09m:22s remains)
INFO - root - 2017-12-05 17:31:14.786278: step 29020, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 72h:13m:24s remains)
INFO - root - 2017-12-05 17:31:23.273575: step 29030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 71h:43m:01s remains)
INFO - root - 2017-12-05 17:31:31.930368: step 29040, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 71h:18m:47s remains)
INFO - root - 2017-12-05 17:31:40.695457: step 29050, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.892 sec/batch; 75h:12m:49s remains)
INFO - root - 2017-12-05 17:31:49.286284: step 29060, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:23m:03s remains)
INFO - root - 2017-12-05 17:31:57.577169: step 29070, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 72h:43m:37s remains)
INFO - root - 2017-12-05 17:32:06.165339: step 29080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:24m:56s remains)
INFO - root - 2017-12-05 17:32:14.649248: step 29090, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 71h:06m:15s remains)
INFO - root - 2017-12-05 17:32:23.230676: step 29100, loss = 2.02, batch loss = 1.97 (9.2 examples/sec; 0.870 sec/batch; 73h:17m:32s remains)
2017-12-05 17:32:24.029023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2441025 -4.2396622 -4.2371483 -4.235189 -4.233284 -4.232132 -4.2320743 -4.2349195 -4.2439871 -4.2571545 -4.2695947 -4.2739644 -4.2718024 -4.2693591 -4.2692413][-4.2295594 -4.2208223 -4.214292 -4.2101979 -4.2066765 -4.203095 -4.2020307 -4.2078652 -4.2236042 -4.2456589 -4.2653394 -4.2748141 -4.27695 -4.2767158 -4.275651][-4.2084041 -4.1934004 -4.1831822 -4.1760578 -4.166635 -4.1557312 -4.1522369 -4.1614904 -4.1845155 -4.2187476 -4.2504745 -4.2702584 -4.2813196 -4.28553 -4.2833452][-4.1786509 -4.1579833 -4.1428056 -4.1288424 -4.1095285 -4.0888972 -4.08181 -4.0925865 -4.1206884 -4.1667275 -4.2142057 -4.2518044 -4.2783489 -4.29119 -4.2909126][-4.1507912 -4.1294165 -4.1106052 -4.0862131 -4.0532155 -4.0199733 -4.0043883 -4.007906 -4.0320048 -4.0827522 -4.1480956 -4.2106833 -4.2569828 -4.2824507 -4.2897005][-4.1326146 -4.115716 -4.0947261 -4.0607529 -4.0163116 -3.9695768 -3.9365304 -3.9169981 -3.9255779 -3.9786129 -4.0643263 -4.1529202 -4.2184973 -4.2549291 -4.2709465][-4.1231837 -4.10695 -4.0804319 -4.0397282 -3.9898958 -3.9326491 -3.8723693 -3.8139079 -3.8042626 -3.86874 -3.9761782 -4.086277 -4.1663132 -4.2089987 -4.2295947][-4.1292095 -4.114048 -4.0820394 -4.0410786 -3.9932561 -3.9287705 -3.8370717 -3.7356658 -3.7064939 -3.7814031 -3.9073548 -4.0327621 -4.1213331 -4.1661839 -4.1834188][-4.1408653 -4.1296091 -4.1058741 -4.0776367 -4.0359225 -3.9735122 -3.8772516 -3.769253 -3.7317071 -3.7846973 -3.8926466 -4.010438 -4.0961065 -4.1454368 -4.1603742][-4.1499276 -4.1406722 -4.1281672 -4.1142569 -4.0842447 -4.036231 -3.9665375 -3.8900349 -3.8544586 -3.869684 -3.9309585 -4.0161314 -4.0913525 -4.1433077 -4.1589451][-4.151979 -4.1428127 -4.1373372 -4.1364837 -4.1214075 -4.0902462 -4.0435252 -3.9935868 -3.9591084 -3.948159 -3.974966 -4.03114 -4.0963712 -4.1479826 -4.1654797][-4.1427078 -4.1324139 -4.1307254 -4.13642 -4.1306987 -4.1154971 -4.08882 -4.0548639 -4.020937 -4.001843 -4.0206947 -4.0610294 -4.1188288 -4.1720848 -4.19671][-4.11822 -4.1109362 -4.1118317 -4.1174121 -4.1170549 -4.1171689 -4.1094956 -4.0889907 -4.0651932 -4.0546851 -4.0762916 -4.1105533 -4.16226 -4.216033 -4.2462173][-4.0958309 -4.09094 -4.0899324 -4.0934181 -4.0949392 -4.1027851 -4.1037841 -4.0966187 -4.0878482 -4.0961461 -4.1252403 -4.1589565 -4.2054234 -4.2544117 -4.2864475][-4.0746865 -4.0708785 -4.0653219 -4.0625982 -4.0600257 -4.0622063 -4.0576811 -4.0558195 -4.0618253 -4.088655 -4.1348228 -4.1763268 -4.2228327 -4.2661629 -4.29299]]...]
INFO - root - 2017-12-05 17:32:32.586677: step 29110, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 73h:31m:44s remains)
INFO - root - 2017-12-05 17:32:41.202006: step 29120, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.895 sec/batch; 75h:24m:34s remains)
INFO - root - 2017-12-05 17:32:49.700050: step 29130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 70h:39m:42s remains)
INFO - root - 2017-12-05 17:32:58.284190: step 29140, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 73h:22m:58s remains)
INFO - root - 2017-12-05 17:33:06.833268: step 29150, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 73h:22m:29s remains)
INFO - root - 2017-12-05 17:33:15.382902: step 29160, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 70h:38m:24s remains)
INFO - root - 2017-12-05 17:33:23.812354: step 29170, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 72h:13m:17s remains)
INFO - root - 2017-12-05 17:33:32.365368: step 29180, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 74h:21m:49s remains)
INFO - root - 2017-12-05 17:33:40.878258: step 29190, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 70h:08m:13s remains)
INFO - root - 2017-12-05 17:33:49.512411: step 29200, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 73h:47m:54s remains)
2017-12-05 17:33:50.337283: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2738914 -4.2414789 -4.2069154 -4.1764507 -4.1569905 -4.132288 -4.0998693 -4.0982313 -4.1372256 -4.1378503 -4.1292939 -4.13052 -4.133275 -4.1316452 -4.1377425][-4.2542229 -4.2125473 -4.1631851 -4.1185055 -4.0914507 -4.0628638 -4.0267687 -4.0200405 -4.0708838 -4.0743594 -4.0627041 -4.0675898 -4.0775 -4.0753679 -4.0707064][-4.2371655 -4.18606 -4.127862 -4.0818534 -4.0582438 -4.0329189 -3.9950137 -3.9812422 -4.0353017 -4.0332031 -4.0132961 -4.0214505 -4.0421615 -4.0395412 -4.0223403][-4.2212262 -4.1632218 -4.1058064 -4.0705547 -4.0530295 -4.0289192 -3.9877214 -3.9684916 -4.0234847 -4.0201588 -3.9973562 -4.00431 -4.0252872 -4.020472 -3.9946854][-4.2050848 -4.141736 -4.0856371 -4.0589919 -4.0463476 -4.0189705 -3.9709814 -3.9471498 -4.0056524 -4.0182204 -4.0069571 -4.0136929 -4.0313816 -4.0292888 -4.0115466][-4.1898446 -4.11969 -4.0562687 -4.0241346 -4.0080814 -3.9740508 -3.9142287 -3.875659 -3.9411674 -3.9791493 -3.98498 -3.9946516 -4.0131803 -4.0296206 -4.036375][-4.174 -4.0922818 -4.01123 -3.9592638 -3.9253755 -3.8697257 -3.7683268 -3.7053716 -3.8026447 -3.8833408 -3.9142282 -3.9383924 -3.9766321 -4.0247545 -4.0559978][-4.1543903 -4.0625548 -3.9670534 -3.896477 -3.8289618 -3.7180037 -3.5421014 -3.4656489 -3.6374631 -3.7861934 -3.8484757 -3.8891916 -3.9480352 -4.0199552 -4.0643959][-4.1357369 -4.044013 -3.953886 -3.8817246 -3.7991076 -3.6665845 -3.4762774 -3.4168139 -3.6074615 -3.7601285 -3.8210955 -3.8651738 -3.9325454 -4.0165038 -4.0648422][-4.1339574 -4.0531631 -3.9807878 -3.9285631 -3.868252 -3.7769628 -3.6650853 -3.6369722 -3.7606874 -3.8467879 -3.8740668 -3.9047341 -3.9605455 -4.0323658 -4.0769897][-4.1558027 -4.0824447 -4.0170851 -3.9803579 -3.9438622 -3.894069 -3.8421521 -3.835031 -3.9089692 -3.9555819 -3.9639373 -3.9829936 -4.0214095 -4.0725241 -4.1080947][-4.1910915 -4.123909 -4.063436 -4.0337372 -4.0140176 -3.9894435 -3.9660532 -3.9661314 -4.0147977 -4.0420828 -4.041811 -4.0521331 -4.0727682 -4.1080165 -4.1386447][-4.2349563 -4.1741152 -4.1188 -4.0911126 -4.0801992 -4.0689406 -4.0595708 -4.0661492 -4.102663 -4.1194477 -4.1139174 -4.1156883 -4.1253448 -4.1478729 -4.1698446][-4.2803993 -4.2331233 -4.1887293 -4.1643381 -4.155592 -4.1501284 -4.149178 -4.1541438 -4.1762557 -4.1837378 -4.1798797 -4.1823382 -4.1900687 -4.2031755 -4.2155852][-4.313437 -4.2806873 -4.2500706 -4.2336097 -4.2295451 -4.229393 -4.2319679 -4.2339597 -4.2408562 -4.2415628 -4.2398872 -4.2435222 -4.2497077 -4.2577424 -4.2656603]]...]
INFO - root - 2017-12-05 17:33:58.899835: step 29210, loss = 2.01, batch loss = 1.95 (9.5 examples/sec; 0.841 sec/batch; 70h:48m:40s remains)
INFO - root - 2017-12-05 17:34:07.450627: step 29220, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 68h:18m:19s remains)
INFO - root - 2017-12-05 17:34:16.017181: step 29230, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 72h:45m:37s remains)
INFO - root - 2017-12-05 17:34:24.550506: step 29240, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 72h:23m:04s remains)
INFO - root - 2017-12-05 17:34:33.033446: step 29250, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 71h:04m:24s remains)
INFO - root - 2017-12-05 17:34:41.679811: step 29260, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 76h:15m:40s remains)
INFO - root - 2017-12-05 17:34:50.186360: step 29270, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:12m:19s remains)
INFO - root - 2017-12-05 17:34:58.757972: step 29280, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 72h:38m:00s remains)
INFO - root - 2017-12-05 17:35:07.150575: step 29290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.856 sec/batch; 72h:03m:45s remains)
INFO - root - 2017-12-05 17:35:15.685242: step 29300, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 70h:24m:46s remains)
2017-12-05 17:35:16.444700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3091831 -4.2967219 -4.2845974 -4.2733941 -4.2683654 -4.2688727 -4.2723651 -4.2813663 -4.2903066 -4.2947564 -4.2982211 -4.3029232 -4.3082876 -4.3096452 -4.309329][-4.2934251 -4.2689347 -4.24521 -4.2315712 -4.2331381 -4.242703 -4.2491312 -4.2552233 -4.2581863 -4.2581491 -4.2607713 -4.269259 -4.2848544 -4.2954388 -4.3009148][-4.2711873 -4.2311316 -4.1946373 -4.1804104 -4.189456 -4.2017236 -4.2069411 -4.2127419 -4.2154312 -4.2175889 -4.2241478 -4.2338562 -4.2558875 -4.2757778 -4.28619][-4.2492104 -4.1982918 -4.153317 -4.1374488 -4.145987 -4.1530461 -4.1527743 -4.1574388 -4.1645865 -4.1752152 -4.1922064 -4.2042031 -4.2309532 -4.2553887 -4.2692986][-4.2358646 -4.1824641 -4.1354475 -4.1131096 -4.1081619 -4.1005516 -4.0912642 -4.0904465 -4.1014752 -4.1185584 -4.14799 -4.1717849 -4.2048936 -4.2345304 -4.2527151][-4.2272983 -4.1760969 -4.125474 -4.0856209 -4.0541534 -4.0249157 -3.9943573 -3.9836016 -4.00681 -4.0353956 -4.0814896 -4.1286879 -4.1710973 -4.2054386 -4.2294188][-4.2135949 -4.1582136 -4.0911312 -4.0248566 -3.9603987 -3.8930912 -3.8256445 -3.8102047 -3.8737655 -3.9394479 -4.0119829 -4.0878744 -4.1403484 -4.1765532 -4.2068248][-4.1981387 -4.1272874 -4.0381775 -3.9584198 -3.8685887 -3.7504196 -3.6328235 -3.6244705 -3.747879 -3.8550267 -3.9462967 -4.0417113 -4.1053052 -4.146637 -4.1848][-4.1874332 -4.1023903 -4.0026712 -3.9222255 -3.8211827 -3.682373 -3.5479305 -3.565398 -3.7237546 -3.8441153 -3.9301457 -4.0201049 -4.0828867 -4.1290541 -4.1700807][-4.18727 -4.104661 -4.0141163 -3.9429967 -3.8612902 -3.7582426 -3.6835797 -3.7172778 -3.8389435 -3.9217107 -3.9714129 -4.0382419 -4.0972295 -4.1415629 -4.1794147][-4.2001309 -4.1309586 -4.0597277 -4.0064039 -3.9574893 -3.9088936 -3.8968415 -3.9273257 -3.9933476 -4.0253725 -4.0433989 -4.0870552 -4.1385036 -4.181459 -4.2178464][-4.2300177 -4.179225 -4.1328759 -4.10214 -4.0750771 -4.0594506 -4.0745049 -4.0903144 -4.1130328 -4.1226883 -4.1317616 -4.1645594 -4.205802 -4.2418303 -4.2729707][-4.2622108 -4.2310381 -4.2078433 -4.1900616 -4.1717906 -4.1698484 -4.1887026 -4.1984739 -4.2089052 -4.2161741 -4.2247119 -4.2469082 -4.2728829 -4.2955008 -4.3143339][-4.2885303 -4.2729135 -4.2647247 -4.2541766 -4.2427316 -4.24617 -4.2624083 -4.2719383 -4.2779603 -4.2814374 -4.2894359 -4.3051481 -4.3206644 -4.332551 -4.3388023][-4.3075552 -4.3011303 -4.2992129 -4.2946424 -4.2900677 -4.2944341 -4.3055692 -4.3103061 -4.3114634 -4.3141894 -4.3224735 -4.3343349 -4.3434505 -4.3479509 -4.3473644]]...]
INFO - root - 2017-12-05 17:35:25.020146: step 29310, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 71h:53m:34s remains)
INFO - root - 2017-12-05 17:35:33.667616: step 29320, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 73h:40m:25s remains)
INFO - root - 2017-12-05 17:35:42.394032: step 29330, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 73h:35m:21s remains)
INFO - root - 2017-12-05 17:35:50.934869: step 29340, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 70h:59m:09s remains)
INFO - root - 2017-12-05 17:35:59.543313: step 29350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 73h:28m:01s remains)
INFO - root - 2017-12-05 17:36:08.061050: step 29360, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 74h:04m:48s remains)
INFO - root - 2017-12-05 17:36:16.388016: step 29370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 72h:11m:45s remains)
INFO - root - 2017-12-05 17:36:24.953317: step 29380, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 73h:42m:01s remains)
INFO - root - 2017-12-05 17:36:33.493996: step 29390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 73h:27m:13s remains)
INFO - root - 2017-12-05 17:36:42.080607: step 29400, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 73h:55m:42s remains)
2017-12-05 17:36:42.817194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2526731 -4.2624631 -4.2629976 -4.2581968 -4.2521787 -4.2512665 -4.25489 -4.25422 -4.2539835 -4.2614512 -4.274137 -4.2922072 -4.3080096 -4.3195324 -4.3322635][-4.2233472 -4.241282 -4.2407818 -4.2296753 -4.21536 -4.2075996 -4.2095408 -4.2077508 -4.2085805 -4.2211714 -4.243556 -4.2703133 -4.2898855 -4.3045053 -4.323185][-4.1996574 -4.2190881 -4.2128091 -4.1925979 -4.1700444 -4.1568823 -4.1586452 -4.1576948 -4.1619291 -4.1784892 -4.2117825 -4.250587 -4.2770309 -4.2947745 -4.3183064][-4.1880484 -4.2017097 -4.185802 -4.152843 -4.1212378 -4.105875 -4.1091413 -4.1175127 -4.1333895 -4.1542506 -4.1940179 -4.2401881 -4.2719336 -4.2927747 -4.3191214][-4.1801276 -4.1886415 -4.1619406 -4.1128178 -4.0664735 -4.0512857 -4.0642109 -4.0905118 -4.1234636 -4.1479239 -4.1875262 -4.2358789 -4.2718978 -4.2967563 -4.3243141][-4.16693 -4.1715331 -4.1321764 -4.0627704 -4.003891 -3.9965742 -4.0238705 -4.0674763 -4.1164818 -4.1485434 -4.1874681 -4.2368555 -4.2743487 -4.3009229 -4.3286705][-4.144701 -4.148181 -4.0997815 -4.0140753 -3.9474576 -3.9481826 -3.9861319 -4.0407114 -4.103888 -4.1502452 -4.1923308 -4.2436008 -4.2790966 -4.30488 -4.3314624][-4.116221 -4.1102471 -4.0531569 -3.9586971 -3.8956733 -3.9065683 -3.9564326 -4.0233712 -4.1015272 -4.1621141 -4.2055578 -4.2523246 -4.2841072 -4.3073707 -4.3315187][-4.1040916 -4.0821781 -4.0161562 -3.9243722 -3.8745084 -3.9052608 -3.9650719 -4.0401063 -4.129149 -4.1920762 -4.2248087 -4.2609215 -4.2886815 -4.3098912 -4.3304868][-4.1143003 -4.0800595 -4.0126567 -3.9306998 -3.8934338 -3.9392841 -4.0053205 -4.0807147 -4.1699886 -4.2264886 -4.2492008 -4.2757311 -4.2979527 -4.3154416 -4.3325386][-4.1353741 -4.0923004 -4.0281973 -3.9608529 -3.9353778 -3.9883053 -4.0523195 -4.1248169 -4.2070532 -4.2545958 -4.2730269 -4.2933416 -4.3105412 -4.32294 -4.33628][-4.1684246 -4.1252542 -4.0684805 -4.0184155 -4.0052981 -4.0565214 -4.1119981 -4.1728735 -4.2414231 -4.27917 -4.2935419 -4.3101029 -4.3252172 -4.3328552 -4.3403764][-4.2066693 -4.1689892 -4.1216455 -4.0863504 -4.0842361 -4.1249614 -4.1698427 -4.2209921 -4.2734342 -4.3020763 -4.3117781 -4.3220663 -4.3313127 -4.3356109 -4.3409176][-4.2437582 -4.2165956 -4.1828208 -4.1598964 -4.1613855 -4.1912947 -4.2250233 -4.2625542 -4.2974825 -4.3174844 -4.3219032 -4.3253069 -4.33051 -4.334518 -4.3398719][-4.2810054 -4.2631764 -4.2413831 -4.2274208 -4.2297196 -4.248528 -4.2697287 -4.2929058 -4.3137121 -4.3261213 -4.3278518 -4.3276939 -4.3314362 -4.33525 -4.3395863]]...]
INFO - root - 2017-12-05 17:36:51.340310: step 29410, loss = 2.02, batch loss = 1.97 (9.2 examples/sec; 0.865 sec/batch; 72h:51m:31s remains)
INFO - root - 2017-12-05 17:36:59.795965: step 29420, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.811 sec/batch; 68h:16m:09s remains)
INFO - root - 2017-12-05 17:37:08.491677: step 29430, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 72h:42m:55s remains)
INFO - root - 2017-12-05 17:37:17.115324: step 29440, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.819 sec/batch; 68h:58m:12s remains)
INFO - root - 2017-12-05 17:37:25.647967: step 29450, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 71h:55m:48s remains)
INFO - root - 2017-12-05 17:37:34.302391: step 29460, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 72h:52m:26s remains)
INFO - root - 2017-12-05 17:37:42.840364: step 29470, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 72h:07m:55s remains)
INFO - root - 2017-12-05 17:37:51.354115: step 29480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 71h:06m:28s remains)
INFO - root - 2017-12-05 17:37:59.910972: step 29490, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 71h:28m:10s remains)
INFO - root - 2017-12-05 17:38:08.396072: step 29500, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 72h:55m:40s remains)
2017-12-05 17:38:09.127470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1108112 -4.1694312 -4.2112904 -4.2251029 -4.2229981 -4.2057133 -4.1749349 -4.1502328 -4.1570106 -4.1860576 -4.2131505 -4.224864 -4.2237329 -4.2196755 -4.2156482][-4.1208582 -4.182745 -4.2189384 -4.2203922 -4.2059922 -4.1789269 -4.136241 -4.1011033 -4.1110554 -4.1580048 -4.2025719 -4.2226577 -4.2227478 -4.2154708 -4.2077618][-4.148654 -4.2082639 -4.234705 -4.2247038 -4.1985106 -4.163703 -4.1084862 -4.0600934 -4.0707636 -4.1320257 -4.1934452 -4.2259889 -4.2314582 -4.2223744 -4.21044][-4.1838312 -4.2314124 -4.2458405 -4.2279496 -4.191299 -4.1467314 -4.07527 -4.005609 -4.0158081 -4.0945892 -4.1720524 -4.2220836 -4.2375603 -4.230999 -4.2190413][-4.2081661 -4.2417979 -4.2465024 -4.2239161 -4.1792321 -4.122272 -4.0318 -3.9307008 -3.9331439 -4.0395803 -4.14222 -4.2112961 -4.2377305 -4.2347779 -4.22515][-4.2137733 -4.2432027 -4.2433095 -4.2187905 -4.1687379 -4.098371 -3.9844787 -3.8479023 -3.8349371 -3.9763393 -4.1101336 -4.1921396 -4.2259507 -4.22989 -4.2274728][-4.2171926 -4.2450728 -4.2439275 -4.2194238 -4.170866 -4.0948343 -3.9653497 -3.8098025 -3.789598 -3.952271 -4.0996408 -4.1808691 -4.2133427 -4.2193756 -4.2193332][-4.2352409 -4.2574406 -4.2537255 -4.2308059 -4.1913781 -4.1298013 -4.0182972 -3.8868618 -3.8757319 -4.0077105 -4.1232309 -4.1840191 -4.2051725 -4.2090292 -4.2093024][-4.2588091 -4.2773485 -4.2729659 -4.2515564 -4.2206826 -4.1801887 -4.1025562 -4.0124164 -4.0016418 -4.0815854 -4.1513071 -4.1848726 -4.194664 -4.1959858 -4.1984534][-4.2763968 -4.2919655 -4.2887144 -4.2693038 -4.2453341 -4.218843 -4.1677623 -4.1080179 -4.093926 -4.1353946 -4.1709957 -4.1872668 -4.1916828 -4.1891661 -4.1882372][-4.2890239 -4.301343 -4.2987747 -4.2833223 -4.2676425 -4.2482438 -4.2159171 -4.17358 -4.1599607 -4.1789885 -4.1943135 -4.1989727 -4.1973486 -4.1898952 -4.1831603][-4.2953196 -4.3032508 -4.2990384 -4.2878618 -4.2797222 -4.2672458 -4.2445421 -4.2104259 -4.1990013 -4.208055 -4.2158356 -4.2157536 -4.2097154 -4.1988721 -4.1909881][-4.29041 -4.2922931 -4.2847075 -4.2773151 -4.2783909 -4.2738829 -4.2577066 -4.2300167 -4.219521 -4.2250061 -4.2302737 -4.2287135 -4.2196035 -4.2087512 -4.2050514][-4.2822213 -4.2820458 -4.2717056 -4.26255 -4.2685051 -4.270659 -4.26112 -4.2391829 -4.2261682 -4.2271986 -4.2306647 -4.2311807 -4.226191 -4.219965 -4.2200527][-4.2820754 -4.2823353 -4.2736521 -4.263155 -4.2653656 -4.2657738 -4.2589521 -4.2412114 -4.2254863 -4.2233624 -4.2269707 -4.23409 -4.2381821 -4.2362118 -4.2353053]]...]
INFO - root - 2017-12-05 17:38:17.656491: step 29510, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 72h:12m:22s remains)
INFO - root - 2017-12-05 17:38:26.156819: step 29520, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 73h:02m:29s remains)
INFO - root - 2017-12-05 17:38:34.706287: step 29530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 71h:31m:55s remains)
INFO - root - 2017-12-05 17:38:43.191902: step 29540, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 75h:23m:27s remains)
INFO - root - 2017-12-05 17:38:51.720136: step 29550, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 69h:41m:47s remains)
INFO - root - 2017-12-05 17:39:00.225397: step 29560, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 71h:26m:24s remains)
INFO - root - 2017-12-05 17:39:08.595599: step 29570, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 0.808 sec/batch; 68h:01m:00s remains)
INFO - root - 2017-12-05 17:39:17.167915: step 29580, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 75h:17m:35s remains)
INFO - root - 2017-12-05 17:39:25.636564: step 29590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 71h:51m:01s remains)
INFO - root - 2017-12-05 17:39:34.205267: step 29600, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 72h:40m:08s remains)
2017-12-05 17:39:34.956177: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1736341 -4.19324 -4.2043123 -4.2124581 -4.2246509 -4.2326794 -4.2355952 -4.238317 -4.2432752 -4.2464757 -4.2419024 -4.2304826 -4.2145753 -4.1961751 -4.1819563][-4.199183 -4.2165823 -4.2266889 -4.2331543 -4.241673 -4.2429633 -4.2402492 -4.2408133 -4.2446251 -4.247107 -4.2412081 -4.2302618 -4.215302 -4.1973629 -4.1854429][-4.2297492 -4.2428207 -4.2487597 -4.2546291 -4.2602782 -4.2550821 -4.242641 -4.2353005 -4.2379193 -4.2437205 -4.2459183 -4.2465444 -4.2383409 -4.2254686 -4.2169008][-4.2571292 -4.2634144 -4.2639651 -4.2635064 -4.2561116 -4.2361307 -4.2128263 -4.2033219 -4.2130189 -4.2332482 -4.2570834 -4.2808204 -4.2853937 -4.2758365 -4.2647896][-4.2451129 -4.2476668 -4.2467866 -4.2384868 -4.2125425 -4.1765647 -4.1416974 -4.1303091 -4.1526594 -4.1958156 -4.2489204 -4.2994013 -4.3212519 -4.3189116 -4.3066278][-4.2112145 -4.2093229 -4.2016611 -4.1800551 -4.1308951 -4.0712295 -4.0132866 -3.9867811 -4.0233259 -4.0999904 -4.1857972 -4.2601347 -4.3007236 -4.3145552 -4.3135004][-4.164793 -4.1556616 -4.1332426 -4.0848923 -3.9997602 -3.9006078 -3.7968063 -3.7344322 -3.7936416 -3.926897 -4.059989 -4.1688671 -4.238873 -4.2738795 -4.2865324][-4.1470551 -4.1275835 -4.083827 -4.0083556 -3.8910196 -3.7465196 -3.5844553 -3.466351 -3.5428686 -3.7363958 -3.9147561 -4.0575333 -4.1591978 -4.2166147 -4.2425857][-4.1765471 -4.1546431 -4.10547 -4.0313067 -3.9247832 -3.795521 -3.6460652 -3.5317922 -3.59229 -3.7596025 -3.9103994 -4.0362315 -4.1329417 -4.1873937 -4.2123466][-4.2339687 -4.2159696 -4.1748905 -4.1213722 -4.046279 -3.9636812 -3.8785925 -3.8188286 -3.8577344 -3.9536092 -4.0344296 -4.10563 -4.1675839 -4.1978035 -4.2110395][-4.2739673 -4.2571149 -4.2281675 -4.1962 -4.14948 -4.09717 -4.053957 -4.031867 -4.0586348 -4.1139765 -4.1525064 -4.183022 -4.2141266 -4.2227664 -4.221045][-4.2744565 -4.2569461 -4.2353058 -4.2169533 -4.1904268 -4.15705 -4.13821 -4.1393242 -4.163168 -4.1983871 -4.2158489 -4.2218089 -4.2300944 -4.2260714 -4.2154484][-4.2528391 -4.2364 -4.21841 -4.2048578 -4.1853542 -4.1618381 -4.1613016 -4.17912 -4.2055216 -4.2324529 -4.2419519 -4.2386646 -4.2354641 -4.2265792 -4.215745][-4.2376385 -4.2209625 -4.2076316 -4.2007165 -4.1843915 -4.1651759 -4.1712618 -4.1984305 -4.2290049 -4.2565103 -4.2671213 -4.2658296 -4.2642517 -4.2566085 -4.2437992][-4.2461848 -4.2296791 -4.2190843 -4.2144914 -4.1990018 -4.1781259 -4.1800575 -4.20191 -4.22812 -4.256546 -4.2725739 -4.2764311 -4.2802782 -4.276247 -4.26195]]...]
INFO - root - 2017-12-05 17:39:43.384925: step 29610, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 73h:18m:03s remains)
INFO - root - 2017-12-05 17:39:51.994816: step 29620, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 71h:17m:09s remains)
INFO - root - 2017-12-05 17:40:00.621033: step 29630, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 74h:10m:45s remains)
INFO - root - 2017-12-05 17:40:09.100046: step 29640, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 71h:29m:38s remains)
INFO - root - 2017-12-05 17:40:17.604324: step 29650, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 70h:32m:01s remains)
INFO - root - 2017-12-05 17:40:26.236617: step 29660, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 71h:57m:11s remains)
INFO - root - 2017-12-05 17:40:34.762975: step 29670, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 74h:34m:48s remains)
INFO - root - 2017-12-05 17:40:43.319431: step 29680, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 69h:37m:27s remains)
INFO - root - 2017-12-05 17:40:51.781662: step 29690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 71h:33m:43s remains)
INFO - root - 2017-12-05 17:41:00.305824: step 29700, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 70h:36m:40s remains)
2017-12-05 17:41:01.015996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.33033 -4.3163896 -4.3067026 -4.2901144 -4.251318 -4.18464 -4.1202803 -4.092063 -4.1016655 -4.1282492 -4.1514211 -4.1509914 -4.1517081 -4.169621 -4.20729][-4.3274026 -4.3108459 -4.3026638 -4.2919731 -4.2574458 -4.1856356 -4.1080041 -4.0749784 -4.0956039 -4.137516 -4.1707296 -4.1721725 -4.1672335 -4.1830578 -4.2207608][-4.3244715 -4.3047781 -4.2978868 -4.2926364 -4.26184 -4.1871934 -4.0958481 -4.0543637 -4.0854011 -4.1484785 -4.1944547 -4.199667 -4.1933718 -4.2076354 -4.2417026][-4.3221054 -4.2986317 -4.2911596 -4.2900872 -4.2624049 -4.1855264 -4.0851264 -4.037755 -4.0753694 -4.1544113 -4.2138457 -4.2280965 -4.2239428 -4.2344918 -4.2598829][-4.3210764 -4.2952166 -4.2855206 -4.2858024 -4.2608991 -4.1796827 -4.0706568 -4.0166698 -4.0584688 -4.1527376 -4.2298746 -4.2556739 -4.2540874 -4.2611403 -4.2785311][-4.3217854 -4.2947783 -4.2816086 -4.2801309 -4.2562232 -4.1732688 -4.0545263 -3.9911764 -4.0354233 -4.1465936 -4.241046 -4.2774105 -4.2789197 -4.2821813 -4.2926736][-4.3233533 -4.2962928 -4.27938 -4.2751031 -4.2518678 -4.1697683 -4.0465026 -3.971067 -4.0106192 -4.132071 -4.23872 -4.2854238 -4.2936597 -4.2952914 -4.296689][-4.3249869 -4.2977343 -4.2785029 -4.2715344 -4.2504654 -4.1762357 -4.0583935 -3.9715767 -3.989634 -4.1066046 -4.2207828 -4.277966 -4.2947259 -4.2978969 -4.2942734][-4.3253713 -4.2982173 -4.2779527 -4.2704968 -4.2546225 -4.1930547 -4.0917935 -4.0055776 -3.9954481 -4.0887918 -4.1973128 -4.261538 -4.2861209 -4.2937584 -4.29004][-4.32469 -4.2969909 -4.2769136 -4.2702751 -4.2605085 -4.214047 -4.1397042 -4.0706148 -4.0428276 -4.0992417 -4.1814909 -4.2415795 -4.2735443 -4.2869291 -4.2848835][-4.3230186 -4.2943807 -4.2752376 -4.2704716 -4.2661252 -4.2351041 -4.1880074 -4.14293 -4.1110663 -4.1338787 -4.179781 -4.2210307 -4.2559114 -4.2754326 -4.2783875][-4.3214192 -4.2912736 -4.2725091 -4.2688618 -4.2678518 -4.2487097 -4.2242608 -4.202693 -4.176652 -4.1796608 -4.1960874 -4.2118483 -4.2419548 -4.2629147 -4.2721658][-4.3206873 -4.2877941 -4.2671146 -4.2628603 -4.2625194 -4.2505779 -4.2407947 -4.2369604 -4.2221003 -4.2205338 -4.2207637 -4.2164845 -4.2358332 -4.2550955 -4.2665162][-4.3214107 -4.2856441 -4.2608695 -4.2539897 -4.2505803 -4.2417254 -4.2396259 -4.2457643 -4.24418 -4.249454 -4.2466521 -4.230402 -4.2361965 -4.2501764 -4.2610464][-4.3244481 -4.2876534 -4.2576523 -4.2446918 -4.2356987 -4.2275538 -4.2298479 -4.2407684 -4.2490206 -4.2602525 -4.2628112 -4.2455468 -4.2407746 -4.2473907 -4.2561445]]...]
INFO - root - 2017-12-05 17:41:09.493552: step 29710, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 74h:10m:41s remains)
INFO - root - 2017-12-05 17:41:18.117192: step 29720, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 71h:38m:11s remains)
INFO - root - 2017-12-05 17:41:26.717587: step 29730, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 72h:45m:04s remains)
INFO - root - 2017-12-05 17:41:35.280117: step 29740, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 74h:39m:00s remains)
INFO - root - 2017-12-05 17:41:43.826895: step 29750, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:24m:03s remains)
INFO - root - 2017-12-05 17:41:52.305844: step 29760, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 68h:55m:46s remains)
INFO - root - 2017-12-05 17:42:00.772433: step 29770, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 70h:28m:57s remains)
INFO - root - 2017-12-05 17:42:09.317501: step 29780, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 73h:28m:11s remains)
INFO - root - 2017-12-05 17:42:17.825913: step 29790, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 72h:45m:31s remains)
INFO - root - 2017-12-05 17:42:26.448652: step 29800, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 70h:32m:52s remains)
2017-12-05 17:42:27.278703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3069029 -4.30117 -4.2979789 -4.2981319 -4.3017821 -4.3068285 -4.3119316 -4.3143654 -4.310904 -4.3024 -4.2919092 -4.2806711 -4.271595 -4.2660003 -4.2644448][-4.2776651 -4.2747498 -4.2736583 -4.2747846 -4.2788754 -4.2862053 -4.2951474 -4.300643 -4.2969851 -4.2874813 -4.2768517 -4.2632971 -4.2502728 -4.2402697 -4.2355137][-4.2467933 -4.2451439 -4.2472429 -4.2511725 -4.2547379 -4.259891 -4.2665443 -4.2722888 -4.2712684 -4.2669983 -4.26237 -4.2506652 -4.2363977 -4.2243543 -4.2184253][-4.22632 -4.2198825 -4.2194066 -4.2202516 -4.2192259 -4.2172227 -4.2174869 -4.225019 -4.2368517 -4.2489734 -4.2592468 -4.2570477 -4.2478838 -4.2384238 -4.2317996][-4.2184038 -4.1958494 -4.1772418 -4.163095 -4.1489153 -4.1317286 -4.1222153 -4.1391978 -4.176877 -4.2162795 -4.2475762 -4.2599297 -4.2643747 -4.2644863 -4.2596769][-4.2124381 -4.1683345 -4.123539 -4.0812116 -4.0391283 -3.9916508 -3.9714353 -4.010107 -4.0845194 -4.1569877 -4.2088704 -4.2379179 -4.2599134 -4.2727141 -4.2757659][-4.2086778 -4.1501732 -4.0773 -3.9971595 -3.9166484 -3.8402145 -3.822669 -3.8916881 -3.9941432 -4.0852861 -4.1451254 -4.1863823 -4.2244873 -4.25001 -4.2660484][-4.2166834 -4.1612539 -4.0806417 -3.9791286 -3.8766079 -3.7992697 -3.7980068 -3.8771703 -3.9719713 -4.0494165 -4.1032619 -4.1518831 -4.2010503 -4.2378058 -4.2647328][-4.2379208 -4.2067595 -4.1432781 -4.0521288 -3.9626379 -3.9108233 -3.9194486 -3.9749267 -4.0352855 -4.0845671 -4.1236758 -4.1664186 -4.2104778 -4.2475009 -4.2769279][-4.2468572 -4.243001 -4.2074351 -4.1495795 -4.0952845 -4.0647507 -4.0712557 -4.1016641 -4.1316323 -4.1609874 -4.1912045 -4.2244487 -4.2559319 -4.2821422 -4.3029332][-4.2406955 -4.257071 -4.248435 -4.2237391 -4.200798 -4.1864815 -4.1917539 -4.2085757 -4.2261672 -4.2472138 -4.2723489 -4.2959394 -4.314507 -4.3272738 -4.3362632][-4.2478375 -4.2709126 -4.2796159 -4.2755318 -4.2735343 -4.2718272 -4.2759619 -4.2860837 -4.2988043 -4.3156376 -4.3359451 -4.3497853 -4.3561292 -4.3581624 -4.3591652][-4.2785058 -4.2989416 -4.3136215 -4.3198795 -4.3249187 -4.3271585 -4.3281474 -4.3334818 -4.3428769 -4.3552127 -4.3660665 -4.3698621 -4.3695874 -4.3692579 -4.3675723][-4.3213825 -4.3366308 -4.3472266 -4.354845 -4.3588219 -4.3574576 -4.353766 -4.3548207 -4.359611 -4.3654723 -4.3676977 -4.3653545 -4.3641891 -4.3641829 -4.362442][-4.3558707 -4.3628392 -4.3665762 -4.3700628 -4.3706818 -4.3684769 -4.363708 -4.361268 -4.3613009 -4.3615675 -4.3597097 -4.3552809 -4.3531017 -4.3523259 -4.3507037]]...]
INFO - root - 2017-12-05 17:42:35.880679: step 29810, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 72h:17m:25s remains)
INFO - root - 2017-12-05 17:42:44.426552: step 29820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 71h:55m:08s remains)
INFO - root - 2017-12-05 17:42:53.076115: step 29830, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 71h:53m:02s remains)
INFO - root - 2017-12-05 17:43:01.639814: step 29840, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 73h:28m:46s remains)
INFO - root - 2017-12-05 17:43:10.213375: step 29850, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 72h:37m:15s remains)
INFO - root - 2017-12-05 17:43:18.792268: step 29860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 70h:45m:11s remains)
INFO - root - 2017-12-05 17:43:27.284659: step 29870, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:22m:03s remains)
INFO - root - 2017-12-05 17:43:35.780159: step 29880, loss = 2.04, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 69h:15m:12s remains)
INFO - root - 2017-12-05 17:43:44.245837: step 29890, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 72h:45m:16s remains)
INFO - root - 2017-12-05 17:43:52.853564: step 29900, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 72h:45m:23s remains)
2017-12-05 17:43:53.673362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2170787 -4.218524 -4.2205982 -4.2250876 -4.2171326 -4.1983919 -4.1813922 -4.1632214 -4.1605029 -4.1789503 -4.2035661 -4.2212825 -4.2296157 -4.2431221 -4.2388406][-4.2186427 -4.2218142 -4.2223725 -4.2263269 -4.2233825 -4.2067575 -4.1889734 -4.165946 -4.1590891 -4.1739755 -4.191638 -4.2074842 -4.2155089 -4.2308645 -4.225358][-4.1958356 -4.2015467 -4.2005868 -4.2076287 -4.2192039 -4.2133718 -4.1982813 -4.1818147 -4.1770988 -4.1858573 -4.1973963 -4.2066822 -4.2047381 -4.2115231 -4.2038317][-4.1537461 -4.159461 -4.16406 -4.1765609 -4.1970358 -4.2017632 -4.1910024 -4.1886635 -4.1954765 -4.2055984 -4.2136946 -4.2156382 -4.1993752 -4.1885676 -4.1802325][-4.1196508 -4.1293631 -4.1394386 -4.1552777 -4.176599 -4.1739078 -4.1573439 -4.1728058 -4.2027726 -4.2206044 -4.2281103 -4.2230563 -4.1925864 -4.1648498 -4.1540332][-4.1052551 -4.11188 -4.1258087 -4.1432323 -4.1482868 -4.117702 -4.0805244 -4.1125026 -4.17123 -4.204978 -4.2187796 -4.2129531 -4.1783972 -4.1470203 -4.1349325][-4.0846977 -4.0729675 -4.0821896 -4.0893097 -4.06932 -3.9926524 -3.90764 -3.9500785 -4.0534964 -4.112071 -4.1386757 -4.1481853 -4.1331134 -4.11129 -4.1079063][-4.0651765 -4.0296736 -4.0263438 -4.0144486 -3.9684718 -3.8508744 -3.708374 -3.7609563 -3.9164438 -3.9998837 -4.043983 -4.083488 -4.1021953 -4.0975451 -4.1051064][-4.0963459 -4.0543041 -4.0473814 -4.0315952 -3.9875691 -3.8885293 -3.7775836 -3.8137262 -3.9433687 -4.0109954 -4.0499792 -4.0948567 -4.12722 -4.1329808 -4.1461558][-4.1674948 -4.1347446 -4.1258569 -4.1143537 -4.0855727 -4.0280795 -3.9704003 -3.9889798 -4.0593171 -4.10067 -4.1256847 -4.1592755 -4.1862159 -4.1881728 -4.1978722][-4.2196493 -4.204319 -4.1963506 -4.1888609 -4.1713948 -4.1401625 -4.1130228 -4.119175 -4.156744 -4.1848769 -4.2076859 -4.2326689 -4.2511139 -4.2429867 -4.2404985][-4.2356935 -4.239006 -4.2377043 -4.2362852 -4.228632 -4.2106991 -4.2006388 -4.2083745 -4.2302804 -4.2470312 -4.2641211 -4.2835827 -4.2911491 -4.2763491 -4.2692385][-4.2255597 -4.2412772 -4.248138 -4.25633 -4.256568 -4.2470455 -4.2451897 -4.2541122 -4.26788 -4.2754083 -4.2816567 -4.2927961 -4.2942615 -4.2766919 -4.2689371][-4.20942 -4.2293768 -4.2406359 -4.2533813 -4.2588658 -4.2555065 -4.2555871 -4.26135 -4.2665372 -4.2697306 -4.2715139 -4.27765 -4.2782855 -4.26534 -4.2581658][-4.2094374 -4.2266374 -4.2375011 -4.2484126 -4.2531452 -4.2506065 -4.2503223 -4.2514086 -4.2526908 -4.2533846 -4.254076 -4.2570806 -4.2586203 -4.2533569 -4.2487807]]...]
INFO - root - 2017-12-05 17:44:02.230767: step 29910, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.854 sec/batch; 71h:45m:32s remains)
INFO - root - 2017-12-05 17:44:10.742868: step 29920, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 73h:08m:16s remains)
INFO - root - 2017-12-05 17:44:19.387397: step 29930, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 71h:02m:12s remains)
INFO - root - 2017-12-05 17:44:27.885349: step 29940, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 72h:46m:22s remains)
INFO - root - 2017-12-05 17:44:36.474533: step 29950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:27m:45s remains)
INFO - root - 2017-12-05 17:44:44.946108: step 29960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 72h:14m:57s remains)
INFO - root - 2017-12-05 17:44:53.416089: step 29970, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 69h:12m:29s remains)
INFO - root - 2017-12-05 17:45:02.040719: step 29980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 72h:32m:14s remains)
INFO - root - 2017-12-05 17:45:10.662166: step 29990, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 73h:33m:02s remains)
INFO - root - 2017-12-05 17:45:19.233724: step 30000, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 69h:42m:11s remains)
2017-12-05 17:45:19.972355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1216097 -4.13238 -4.1308928 -4.1101642 -4.0811577 -4.0958381 -4.1257281 -4.1276865 -4.1130486 -4.0924683 -4.094059 -4.1128745 -4.138133 -4.1327543 -4.1086049][-4.131475 -4.1361246 -4.1318984 -4.1078887 -4.0763054 -4.0863752 -4.1139746 -4.1255331 -4.1131368 -4.0946069 -4.0977793 -4.1158304 -4.1392837 -4.1388164 -4.1265321][-4.1365819 -4.1353807 -4.1230655 -4.0923219 -4.0627308 -4.0727997 -4.0959167 -4.1108832 -4.1023083 -4.0939488 -4.1043024 -4.1174645 -4.1331053 -4.1372418 -4.140049][-4.1195889 -4.119967 -4.1063619 -4.0785112 -4.0634151 -4.0771728 -4.0899577 -4.0975103 -4.092886 -4.0933061 -4.1055479 -4.1092277 -4.1165991 -4.1244407 -4.1373739][-4.0940309 -4.1008348 -4.088275 -4.0657854 -4.0635066 -4.0758567 -4.0765228 -4.077198 -4.0790148 -4.0857296 -4.0953727 -4.0928545 -4.0965629 -4.1064577 -4.1225963][-4.0757284 -4.0873575 -4.0801067 -4.0680561 -4.0717325 -4.0731163 -4.0603895 -4.0568242 -4.0613737 -4.0688581 -4.0713859 -4.0731521 -4.0916305 -4.1126037 -4.1262279][-4.0671225 -4.07645 -4.0726924 -4.076139 -4.086688 -4.0818353 -4.0616083 -4.0511017 -4.0531292 -4.0538664 -4.0538225 -4.0677409 -4.1059155 -4.1391249 -4.1484532][-4.0654345 -4.0764432 -4.0819182 -4.0938559 -4.1029239 -4.0894151 -4.0671296 -4.0557017 -4.0534678 -4.0484991 -4.0520968 -4.0791788 -4.1284184 -4.1629324 -4.1697984][-4.0800114 -4.0878863 -4.098321 -4.1177168 -4.12523 -4.1118164 -4.0923929 -4.0775642 -4.0699477 -4.0599017 -4.0662737 -4.1065683 -4.1536345 -4.1792336 -4.1798964][-4.1067495 -4.1124692 -4.1180935 -4.1375794 -4.1478572 -4.1398277 -4.1277308 -4.1158738 -4.11117 -4.1033869 -4.1110411 -4.1490746 -4.1838436 -4.1955867 -4.1894493][-4.1265774 -4.1316233 -4.1328931 -4.1508436 -4.1616158 -4.1550555 -4.1485195 -4.1427317 -4.14698 -4.1464539 -4.1590476 -4.1908779 -4.2089067 -4.2104721 -4.1986594][-4.1385641 -4.14249 -4.1369104 -4.1472521 -4.1526933 -4.1468673 -4.1454606 -4.1427155 -4.1486712 -4.1581197 -4.180068 -4.2087941 -4.2227249 -4.2200589 -4.1991191][-4.1425352 -4.1442847 -4.132349 -4.1308055 -4.1301451 -4.1279082 -4.1349268 -4.1374092 -4.142694 -4.1590114 -4.1873312 -4.2113056 -4.2220984 -4.216362 -4.1865392][-4.139143 -4.1367569 -4.1206794 -4.1100726 -4.1041117 -4.1063137 -4.1226549 -4.1331158 -4.1487679 -4.1726432 -4.1995931 -4.2171564 -4.2211647 -4.2064714 -4.1713409][-4.1454558 -4.1389852 -4.1219893 -4.1100368 -4.1075711 -4.11738 -4.1358476 -4.150773 -4.1711006 -4.1928883 -4.2139521 -4.2279992 -4.2265167 -4.2067366 -4.1753259]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 17:45:29.148719: step 30010, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 72h:22m:18s remains)
INFO - root - 2017-12-05 17:45:37.697658: step 30020, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 71h:14m:11s remains)
INFO - root - 2017-12-05 17:45:46.168832: step 30030, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 72h:12m:51s remains)
INFO - root - 2017-12-05 17:45:54.766791: step 30040, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 71h:07m:01s remains)
INFO - root - 2017-12-05 17:46:03.271885: step 30050, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 69h:42m:42s remains)
INFO - root - 2017-12-05 17:46:11.704082: step 30060, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.825 sec/batch; 69h:16m:32s remains)
INFO - root - 2017-12-05 17:46:20.185488: step 30070, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 69h:14m:37s remains)
INFO - root - 2017-12-05 17:46:28.743194: step 30080, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 72h:49m:56s remains)
INFO - root - 2017-12-05 17:46:37.296014: step 30090, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 73h:09m:33s remains)
INFO - root - 2017-12-05 17:46:45.731537: step 30100, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 70h:05m:40s remains)
2017-12-05 17:46:46.553559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2033496 -4.2013173 -4.2124629 -4.2253876 -4.237638 -4.2550993 -4.2741289 -4.2940879 -4.3098893 -4.3114128 -4.2992921 -4.2785993 -4.2545109 -4.2344942 -4.2280884][-4.1858058 -4.1846204 -4.1980839 -4.216898 -4.2345743 -4.2567883 -4.2781348 -4.2968564 -4.3121519 -4.3145461 -4.3028159 -4.2824268 -4.2575474 -4.2312288 -4.2196431][-4.1724887 -4.1633253 -4.1696773 -4.1826081 -4.1987352 -4.223865 -4.2534523 -4.2805576 -4.2989874 -4.3030028 -4.2936978 -4.2736759 -4.2485886 -4.2201996 -4.2033949][-4.1675076 -4.1380253 -4.1200762 -4.1134167 -4.1250529 -4.155364 -4.1974368 -4.2401214 -4.2673516 -4.2783651 -4.2753673 -4.2624826 -4.2438607 -4.2163863 -4.1936288][-4.1867638 -4.1416879 -4.0963545 -4.0575128 -4.0484757 -4.0744491 -4.1263061 -4.183681 -4.2222419 -4.2429914 -4.2509251 -4.2487903 -4.2398624 -4.2164588 -4.1880255][-4.2191043 -4.1739082 -4.1227779 -4.0684924 -4.0335526 -4.0321431 -4.0656347 -4.1166892 -4.1584792 -4.1906819 -4.2113709 -4.2198124 -4.2177944 -4.2000933 -4.169343][-4.2415442 -4.2073979 -4.170135 -4.1266575 -4.0866232 -4.0624347 -4.0611219 -4.0785041 -4.098465 -4.1283751 -4.1567035 -4.1770344 -4.1831155 -4.1716542 -4.1384449][-4.2503614 -4.2297397 -4.2125406 -4.1896057 -4.1617365 -4.134593 -4.1142573 -4.1037126 -4.0921555 -4.0999641 -4.1199307 -4.147347 -4.1660194 -4.1634073 -4.135118][-4.2449837 -4.2318463 -4.2256427 -4.219861 -4.2096062 -4.1956649 -4.1782947 -4.1609154 -4.1348324 -4.1242423 -4.1331091 -4.1638231 -4.1841383 -4.1843815 -4.1676178][-4.2370391 -4.2243085 -4.219173 -4.2203007 -4.2212434 -4.2218251 -4.2160716 -4.2088323 -4.18644 -4.1688156 -4.1711593 -4.2038383 -4.2260337 -4.2262006 -4.2132125][-4.2294164 -4.2180004 -4.2125831 -4.214921 -4.2183738 -4.2258534 -4.2315726 -4.2359929 -4.2251472 -4.2113295 -4.2109 -4.2374039 -4.2564049 -4.2545915 -4.2443342][-4.2156444 -4.2016826 -4.1947112 -4.1945252 -4.1980076 -4.2111821 -4.2281275 -4.2420387 -4.2405114 -4.23124 -4.2318683 -4.2496829 -4.2601948 -4.2562909 -4.2516985][-4.2111 -4.1907015 -4.1754017 -4.1659145 -4.1641645 -4.1768417 -4.198751 -4.22058 -4.2295418 -4.2284856 -4.23521 -4.2492838 -4.2530189 -4.2465882 -4.2448411][-4.2246423 -4.1992145 -4.1747541 -4.1526051 -4.14042 -4.1457038 -4.1655917 -4.190609 -4.2050867 -4.2162213 -4.2360139 -4.2561927 -4.2593508 -4.2514043 -4.24855][-4.2490969 -4.2238846 -4.1949544 -4.1671457 -4.1495194 -4.148025 -4.1652861 -4.1913214 -4.2085385 -4.2259226 -4.2486391 -4.2698717 -4.2758851 -4.2697182 -4.2617245]]...]
INFO - root - 2017-12-05 17:46:55.074320: step 30110, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 70h:50m:17s remains)
INFO - root - 2017-12-05 17:47:03.608138: step 30120, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 71h:59m:37s remains)
INFO - root - 2017-12-05 17:47:12.058106: step 30130, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 70h:55m:01s remains)
INFO - root - 2017-12-05 17:47:20.548402: step 30140, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 71h:11m:55s remains)
INFO - root - 2017-12-05 17:47:29.125229: step 30150, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 72h:57m:16s remains)
INFO - root - 2017-12-05 17:47:37.548561: step 30160, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 71h:12m:49s remains)
INFO - root - 2017-12-05 17:47:46.072302: step 30170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 72h:04m:45s remains)
INFO - root - 2017-12-05 17:47:54.591729: step 30180, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 72h:50m:47s remains)
INFO - root - 2017-12-05 17:48:03.107774: step 30190, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 74h:30m:02s remains)
INFO - root - 2017-12-05 17:48:11.674407: step 30200, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 71h:22m:27s remains)
2017-12-05 17:48:12.420416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3467116 -4.3408561 -4.3309679 -4.3212895 -4.3148937 -4.3122163 -4.3120561 -4.31643 -4.3221946 -4.3246412 -4.3222928 -4.3122749 -4.2951455 -4.2716112 -4.2451963][-4.3496852 -4.340024 -4.3250422 -4.311995 -4.304491 -4.3015828 -4.2977343 -4.3004928 -4.3097858 -4.3174624 -4.3189468 -4.3089261 -4.28814 -4.2612233 -4.2367082][-4.345551 -4.3316903 -4.3128896 -4.2975035 -4.2896824 -4.2855053 -4.2744651 -4.2739124 -4.2883625 -4.3034425 -4.3086653 -4.2995915 -4.2776933 -4.2537866 -4.2389064][-4.3365111 -4.3196521 -4.2991838 -4.2807283 -4.2678246 -4.2550497 -4.233048 -4.2315183 -4.2572761 -4.2837067 -4.2944956 -4.292697 -4.2780685 -4.2615366 -4.2560244][-4.3277116 -4.3077216 -4.2845368 -4.2583256 -4.2332788 -4.2038264 -4.16842 -4.17072 -4.2123795 -4.2530785 -4.2738481 -4.2835684 -4.2805586 -4.2715359 -4.2724357][-4.317421 -4.2944207 -4.26301 -4.2234216 -4.1798062 -4.1209178 -4.0624661 -4.0702519 -4.1360683 -4.197958 -4.2356186 -4.2614779 -4.2746134 -4.2792659 -4.2841225][-4.3027444 -4.2743087 -4.2326827 -4.1824107 -4.1183252 -4.0249515 -3.9425523 -3.9637275 -4.0603752 -4.1489697 -4.2087708 -4.2512541 -4.2809224 -4.2958188 -4.2923794][-4.2843032 -4.2498951 -4.2022419 -4.1468763 -4.0692058 -3.9579034 -3.8749504 -3.9130106 -4.0307741 -4.1347184 -4.2032018 -4.2523108 -4.2891607 -4.301291 -4.2854757][-4.2682323 -4.2272353 -4.1764169 -4.1219678 -4.0453191 -3.9515805 -3.9039874 -3.9632378 -4.0777063 -4.164988 -4.2151103 -4.24862 -4.2729836 -4.2722883 -4.2458639][-4.2634816 -4.2204294 -4.17235 -4.1255989 -4.0686235 -4.0200534 -4.021018 -4.0911069 -4.1783404 -4.22949 -4.2451077 -4.2452774 -4.2384305 -4.206213 -4.1595154][-4.2732186 -4.2346706 -4.1971207 -4.1674414 -4.1377897 -4.1264052 -4.1500473 -4.2070632 -4.258461 -4.2774076 -4.2623134 -4.2322888 -4.18526 -4.1068449 -4.0363379][-4.29046 -4.2580929 -4.2291503 -4.2105174 -4.1970358 -4.2013736 -4.2253728 -4.26394 -4.291995 -4.2919221 -4.2609587 -4.2140222 -4.136229 -4.0236812 -3.9440985][-4.3075495 -4.28111 -4.2596045 -4.2480931 -4.2402253 -4.24592 -4.2655325 -4.2935033 -4.30753 -4.2955971 -4.2573886 -4.2004595 -4.1126838 -4.0120406 -3.9623041][-4.3146424 -4.2940516 -4.2806554 -4.2729135 -4.2656012 -4.2671027 -4.2838836 -4.3067293 -4.3107352 -4.2916756 -4.2522078 -4.1968613 -4.1277828 -4.0732517 -4.0690808][-4.3124866 -4.2942061 -4.2821951 -4.2720108 -4.2566948 -4.2501788 -4.2677031 -4.2919774 -4.2940526 -4.2760782 -4.2455683 -4.2067208 -4.1680408 -4.1548347 -4.1766167]]...]
INFO - root - 2017-12-05 17:48:20.964322: step 30210, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:58m:23s remains)
INFO - root - 2017-12-05 17:48:29.500148: step 30220, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 70h:46m:36s remains)
INFO - root - 2017-12-05 17:48:38.080484: step 30230, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 72h:07m:10s remains)
INFO - root - 2017-12-05 17:48:46.432460: step 30240, loss = 2.08, batch loss = 2.02 (10.6 examples/sec; 0.754 sec/batch; 63h:20m:06s remains)
INFO - root - 2017-12-05 17:48:55.012613: step 30250, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 72h:40m:14s remains)
INFO - root - 2017-12-05 17:49:03.578991: step 30260, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 72h:08m:52s remains)
INFO - root - 2017-12-05 17:49:12.121373: step 30270, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 72h:28m:51s remains)
INFO - root - 2017-12-05 17:49:20.693943: step 30280, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 69h:08m:52s remains)
INFO - root - 2017-12-05 17:49:29.055706: step 30290, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 71h:19m:18s remains)
INFO - root - 2017-12-05 17:49:37.536338: step 30300, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 72h:21m:11s remains)
2017-12-05 17:49:38.331888: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2119637 -4.1936655 -4.1930146 -4.1993294 -4.1995111 -4.2090282 -4.2102718 -4.2101126 -4.2073436 -4.2028918 -4.1941261 -4.1751466 -4.15008 -4.1249261 -4.1079783][-4.2073832 -4.1860976 -4.1797166 -4.1758246 -4.1734595 -4.1867809 -4.18985 -4.1934214 -4.1939473 -4.1922817 -4.1865206 -4.1701965 -4.1454377 -4.1200037 -4.1052856][-4.1909523 -4.1675005 -4.1588125 -4.1502557 -4.1487861 -4.1650963 -4.171916 -4.1763072 -4.1802459 -4.181241 -4.1815929 -4.1714096 -4.1507668 -4.126564 -4.1118731][-4.1838984 -4.1553459 -4.1403341 -4.127037 -4.1225753 -4.1392426 -4.1482177 -4.154161 -4.1640615 -4.168417 -4.1728482 -4.1693559 -4.1568642 -4.1412849 -4.12603][-4.1902022 -4.1584191 -4.1377249 -4.1174636 -4.1026545 -4.1117592 -4.1194558 -4.1264582 -4.1443148 -4.1534119 -4.1579051 -4.16016 -4.1567073 -4.1507206 -4.1372609][-4.1843915 -4.1541781 -4.1384897 -4.1194367 -4.0964322 -4.0908856 -4.0844965 -4.0852857 -4.1151428 -4.1357207 -4.1454277 -4.1526527 -4.1524029 -4.1500778 -4.1371274][-4.1792593 -4.1530108 -4.1356263 -4.1177192 -4.094418 -4.0747938 -4.0438337 -4.0246491 -4.0634112 -4.1072984 -4.1339574 -4.1502757 -4.1566191 -4.1563091 -4.1438036][-4.1696429 -4.147234 -4.1253633 -4.1042948 -4.0751638 -4.0371943 -3.9727917 -3.9214582 -3.9663939 -4.04116 -4.0958567 -4.1302328 -4.1540112 -4.1635842 -4.1575875][-4.1545968 -4.1393609 -4.1217618 -4.1041126 -4.0745249 -4.0236554 -3.9366708 -3.8601005 -3.8931499 -3.9723432 -4.0416865 -4.0940661 -4.1341085 -4.1524391 -4.1572471][-4.161479 -4.1545243 -4.1467986 -4.1415358 -4.1280646 -4.0958457 -4.0312338 -3.9691443 -3.9749548 -4.0117655 -4.0512223 -4.0886641 -4.1268873 -4.1463094 -4.1515608][-4.1874242 -4.1838822 -4.1794186 -4.17726 -4.1725097 -4.1579261 -4.1208668 -4.0782633 -4.0692968 -4.0779328 -4.0958929 -4.1135721 -4.1356082 -4.1489415 -4.1515789][-4.2072468 -4.205934 -4.2021875 -4.1972642 -4.191062 -4.1837711 -4.1631765 -4.1326652 -4.1185536 -4.1182175 -4.1293569 -4.1356306 -4.1392813 -4.1414948 -4.1414905][-4.2146015 -4.2104917 -4.2053995 -4.2026982 -4.1954861 -4.1904449 -4.17945 -4.1570797 -4.1434 -4.1438556 -4.1536345 -4.1533885 -4.1444907 -4.1390939 -4.1378083][-4.2228312 -4.2148018 -4.2065659 -4.2062459 -4.2035351 -4.1987853 -4.1900744 -4.1694417 -4.153048 -4.1528497 -4.1648426 -4.1688628 -4.1623268 -4.1569891 -4.1547675][-4.2233047 -4.2129459 -4.2034822 -4.2069912 -4.2109165 -4.2096882 -4.2042079 -4.184577 -4.1626234 -4.15366 -4.160037 -4.1629219 -4.1630039 -4.1678209 -4.1727691]]...]
INFO - root - 2017-12-05 17:49:46.782334: step 30310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 71h:19m:40s remains)
INFO - root - 2017-12-05 17:49:55.410979: step 30320, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:58m:06s remains)
INFO - root - 2017-12-05 17:50:03.877767: step 30330, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 70h:39m:39s remains)
INFO - root - 2017-12-05 17:50:12.395356: step 30340, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 72h:08m:17s remains)
INFO - root - 2017-12-05 17:50:20.805412: step 30350, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 69h:29m:27s remains)
INFO - root - 2017-12-05 17:50:29.312341: step 30360, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 72h:22m:55s remains)
INFO - root - 2017-12-05 17:50:37.774382: step 30370, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 72h:44m:43s remains)
INFO - root - 2017-12-05 17:50:46.302744: step 30380, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 69h:22m:06s remains)
INFO - root - 2017-12-05 17:50:54.827773: step 30390, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 71h:26m:54s remains)
INFO - root - 2017-12-05 17:51:03.308198: step 30400, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 73h:17m:42s remains)
2017-12-05 17:51:04.127382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2704477 -4.2475872 -4.2296481 -4.2197652 -4.2255182 -4.2500019 -4.2735376 -4.2879953 -4.2930202 -4.2933474 -4.2908006 -4.2866783 -4.2828832 -4.2807965 -4.2810154][-4.2445812 -4.205431 -4.1720734 -4.1546416 -4.1617451 -4.1983981 -4.2345877 -4.2571855 -4.2695541 -4.27565 -4.2776952 -4.2745829 -4.2677674 -4.2624941 -4.262145][-4.2327542 -4.1817641 -4.1330004 -4.1000085 -4.1011629 -4.1455078 -4.19057 -4.2193112 -4.241075 -4.2597475 -4.2734 -4.2769284 -4.2705412 -4.2614117 -4.2586088][-4.23427 -4.1778612 -4.1209478 -4.0715919 -4.0577688 -4.0988235 -4.148921 -4.1792936 -4.2087502 -4.2419767 -4.2692924 -4.281291 -4.2791605 -4.2720132 -4.268466][-4.2419496 -4.1822624 -4.1212893 -4.0637779 -4.03292 -4.0566788 -4.1011848 -4.130712 -4.1667132 -4.2140727 -4.2517948 -4.2682819 -4.2697515 -4.2686076 -4.2666855][-4.2512865 -4.1919918 -4.1287351 -4.0674257 -4.0242143 -4.0228033 -4.0456066 -4.0746317 -4.1155825 -4.168232 -4.2199011 -4.2412071 -4.2423139 -4.2448177 -4.2488265][-4.2552633 -4.196909 -4.1327133 -4.0664945 -4.0107207 -3.9891975 -3.9849555 -4.0052733 -4.0495839 -4.113451 -4.1766777 -4.2033525 -4.19851 -4.1966014 -4.2044082][-4.2596083 -4.2043104 -4.1437631 -4.0767355 -4.0209656 -3.9958675 -3.9763722 -3.9716797 -4.000423 -4.0682468 -4.1406417 -4.1699395 -4.1632628 -4.1581817 -4.168117][-4.266283 -4.2149372 -4.1590986 -4.0937619 -4.0421615 -4.0187707 -3.9919443 -3.9684236 -3.9698064 -4.0270061 -4.09962 -4.1277719 -4.121943 -4.1148396 -4.1275272][-4.2759895 -4.2289724 -4.1769509 -4.1159534 -4.0641203 -4.0379338 -4.0065832 -3.973042 -3.9610264 -4.0013962 -4.0588326 -4.0799556 -4.0710983 -4.0701737 -4.0921183][-4.2853889 -4.245656 -4.1983438 -4.1463518 -4.0997252 -4.0705328 -4.0390038 -4.0069556 -3.9911146 -4.0129819 -4.0435343 -4.0487938 -4.0336881 -4.0390987 -4.0711732][-4.2907753 -4.2592096 -4.22278 -4.1842766 -4.1484284 -4.1261787 -4.0957351 -4.0626693 -4.0466471 -4.0550971 -4.062057 -4.053967 -4.0300765 -4.034718 -4.0703773][-4.29458 -4.2696123 -4.2456889 -4.2227468 -4.201407 -4.1911545 -4.1645823 -4.1303382 -4.1065559 -4.0988989 -4.0953846 -4.078558 -4.0518541 -4.0518923 -4.0861282][-4.2958946 -4.2744679 -4.2590451 -4.2483187 -4.2385535 -4.2359838 -4.2171044 -4.1797652 -4.1463437 -4.1273446 -4.121635 -4.1057582 -4.0827813 -4.0860457 -4.1207275][-4.293767 -4.2731638 -4.2605743 -4.2549191 -4.2490382 -4.2510762 -4.2402053 -4.20432 -4.1652832 -4.1409907 -4.1369085 -4.1261597 -4.1111012 -4.1167464 -4.1492529]]...]
INFO - root - 2017-12-05 17:51:12.611217: step 30410, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 69h:26m:54s remains)
INFO - root - 2017-12-05 17:51:21.070916: step 30420, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 72h:10m:35s remains)
INFO - root - 2017-12-05 17:51:29.680686: step 30430, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:43m:25s remains)
INFO - root - 2017-12-05 17:51:38.277053: step 30440, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 73h:56m:53s remains)
INFO - root - 2017-12-05 17:51:46.811727: step 30450, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 71h:08m:19s remains)
INFO - root - 2017-12-05 17:51:55.202763: step 30460, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.865 sec/batch; 72h:33m:14s remains)
INFO - root - 2017-12-05 17:52:03.640494: step 30470, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.829 sec/batch; 69h:31m:54s remains)
INFO - root - 2017-12-05 17:52:12.170987: step 30480, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 71h:28m:01s remains)
INFO - root - 2017-12-05 17:52:20.801089: step 30490, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 72h:35m:57s remains)
INFO - root - 2017-12-05 17:52:29.316097: step 30500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:55m:41s remains)
2017-12-05 17:52:30.099531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3023887 -4.3010769 -4.3016639 -4.3022685 -4.3018446 -4.2993679 -4.2962842 -4.2952943 -4.2979341 -4.3034344 -4.3085146 -4.3109031 -4.3096557 -4.306242 -4.3026047][-4.2904711 -4.286324 -4.2844296 -4.283206 -4.2808919 -4.274631 -4.26653 -4.2608323 -4.2630329 -4.2739868 -4.2880926 -4.2990417 -4.3026571 -4.30061 -4.2961555][-4.2886081 -4.2807574 -4.2749515 -4.2700129 -4.264101 -4.2528458 -4.23595 -4.2188883 -4.2134562 -4.2292008 -4.257556 -4.284667 -4.3012109 -4.3058677 -4.3035021][-4.2947741 -4.2829561 -4.2704849 -4.2566786 -4.2418485 -4.2199144 -4.1868382 -4.1479383 -4.12853 -4.1505232 -4.2007952 -4.2555113 -4.295773 -4.3138328 -4.3173208][-4.2991676 -4.2837696 -4.2631354 -4.2371449 -4.2064996 -4.1641378 -4.1016765 -4.0260262 -3.982223 -4.0135345 -4.0970159 -4.1907697 -4.2649617 -4.30491 -4.3215203][-4.2990589 -4.284061 -4.2590208 -4.2244024 -4.1784453 -4.113564 -4.017374 -3.8950157 -3.8128438 -3.8487341 -3.9651921 -4.0977459 -4.2083645 -4.2742248 -4.3088255][-4.2981162 -4.2885728 -4.2666464 -4.2337551 -4.1870432 -4.1179333 -4.0111322 -3.8658383 -3.7524171 -3.7737942 -3.8922117 -4.0364032 -4.1629238 -4.2433114 -4.2900395][-4.3035235 -4.2989364 -4.2826786 -4.2575588 -4.2206645 -4.1627169 -4.0707808 -3.9473779 -3.8485253 -3.8570316 -3.9439178 -4.060864 -4.17054 -4.2426186 -4.2859769][-4.3169217 -4.3155484 -4.3037171 -4.2851543 -4.2598877 -4.2182174 -4.1514378 -4.0655117 -3.999536 -4.0045066 -4.0588708 -4.1394873 -4.21899 -4.2705083 -4.3005152][-4.3284059 -4.3299193 -4.3233523 -4.3124614 -4.2996545 -4.2743325 -4.2319965 -4.1798229 -4.1412706 -4.1445341 -4.1744585 -4.2240877 -4.2757721 -4.3070941 -4.3219409][-4.3347163 -4.3389034 -4.3385272 -4.3359885 -4.3332691 -4.3226328 -4.3015532 -4.2752051 -4.2561035 -4.2584925 -4.2731242 -4.2990932 -4.3254824 -4.3387833 -4.3400235][-4.3369274 -4.3435378 -4.3491192 -4.3534641 -4.3578563 -4.3567805 -4.3491759 -4.33817 -4.3302402 -4.3323593 -4.3384051 -4.3478155 -4.3559508 -4.3564634 -4.3488765][-4.328557 -4.3375678 -4.3481317 -4.3579016 -4.3652892 -4.368381 -4.3669291 -4.362926 -4.3600874 -4.3608751 -4.3624 -4.3630872 -4.3613915 -4.3560758 -4.3462873][-4.309875 -4.317966 -4.3294129 -4.3400235 -4.3477592 -4.35199 -4.3531837 -4.3525476 -4.3520141 -4.3525944 -4.3526478 -4.3512897 -4.3478708 -4.3427863 -4.3362589][-4.2917781 -4.2972188 -4.3072019 -4.317028 -4.324439 -4.3286653 -4.3304844 -4.3309331 -4.3310866 -4.33216 -4.3329868 -4.3332238 -4.3323178 -4.3303189 -4.3274517]]...]
INFO - root - 2017-12-05 17:52:38.426213: step 30510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.856 sec/batch; 71h:46m:04s remains)
INFO - root - 2017-12-05 17:52:46.868959: step 30520, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 70h:18m:53s remains)
INFO - root - 2017-12-05 17:52:55.392912: step 30530, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 69h:47m:00s remains)
INFO - root - 2017-12-05 17:53:03.773646: step 30540, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 71h:24m:14s remains)
INFO - root - 2017-12-05 17:53:12.420271: step 30550, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 71h:42m:10s remains)
INFO - root - 2017-12-05 17:53:20.929654: step 30560, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.800 sec/batch; 67h:05m:52s remains)
INFO - root - 2017-12-05 17:53:29.523997: step 30570, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 70h:49m:23s remains)
INFO - root - 2017-12-05 17:53:38.001672: step 30580, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 71h:57m:02s remains)
INFO - root - 2017-12-05 17:53:46.410164: step 30590, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:51m:51s remains)
INFO - root - 2017-12-05 17:53:54.995353: step 30600, loss = 2.09, batch loss = 2.04 (8.9 examples/sec; 0.895 sec/batch; 75h:05m:19s remains)
2017-12-05 17:53:55.777427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2592349 -4.260016 -4.2594581 -4.2603035 -4.2627411 -4.2660279 -4.2710509 -4.2810721 -4.28991 -4.2923012 -4.2889137 -4.2864141 -4.2865472 -4.28655 -4.2854509][-4.2649207 -4.2618155 -4.2563157 -4.2496905 -4.24596 -4.2458782 -4.2513084 -4.2661357 -4.2818689 -4.2900519 -4.2890949 -4.2858396 -4.2830577 -4.2796097 -4.2757597][-4.2722554 -4.2607665 -4.2462411 -4.2297115 -4.2180185 -4.2120376 -4.2143359 -4.2332635 -4.2579179 -4.27289 -4.2750511 -4.2731743 -4.2685771 -4.2604561 -4.2531195][-4.2708211 -4.2480597 -4.21959 -4.1886277 -4.1656632 -4.1500773 -4.1490545 -4.1764021 -4.215919 -4.2437358 -4.2532773 -4.254427 -4.2475805 -4.2314181 -4.2212605][-4.2445612 -4.2093563 -4.16586 -4.1196079 -4.0829859 -4.0562539 -4.05165 -4.0911121 -4.153615 -4.2001405 -4.2202268 -4.2248545 -4.2151608 -4.1912127 -4.1785073][-4.2169151 -4.1715617 -4.11665 -4.0558791 -4.0039649 -3.9618273 -3.9437299 -3.9888296 -4.07961 -4.1507611 -4.1855516 -4.1948781 -4.185461 -4.1635733 -4.1543603][-4.1982307 -4.1509995 -4.0919757 -4.0255289 -3.9648681 -3.9029827 -3.8534174 -3.8903592 -3.9970083 -4.0906363 -4.1466527 -4.1677685 -4.1683617 -4.1602144 -4.1585755][-4.2156153 -4.1712561 -4.11092 -4.0453563 -3.98318 -3.9072707 -3.8269026 -3.8399246 -3.9386706 -4.0420704 -4.11977 -4.1589308 -4.177784 -4.1887317 -4.195415][-4.2579012 -4.2222176 -4.1699 -4.1145935 -4.0650911 -3.9999521 -3.9245086 -3.9183488 -3.9822366 -4.0685487 -4.1501527 -4.1970787 -4.2242484 -4.2445154 -4.2574606][-4.3023891 -4.279192 -4.2408247 -4.1975751 -4.1605387 -4.1143975 -4.0610256 -4.0464768 -4.0779505 -4.139564 -4.2081017 -4.2502804 -4.2762589 -4.2967768 -4.3096366][-4.3335223 -4.3220415 -4.2948117 -4.2594376 -4.2312465 -4.2013483 -4.166924 -4.1534238 -4.167695 -4.2085881 -4.2578626 -4.2923331 -4.3141389 -4.3305225 -4.3358436][-4.3463106 -4.3454766 -4.33125 -4.3056521 -4.2849941 -4.2640657 -4.24235 -4.2317371 -4.2376075 -4.2599244 -4.2910018 -4.3157506 -4.32879 -4.33629 -4.3336368][-4.3440189 -4.350224 -4.3480272 -4.3343797 -4.3219528 -4.3073635 -4.2932677 -4.2850981 -4.2858405 -4.2950706 -4.3101239 -4.3247676 -4.3297744 -4.3289003 -4.3194885][-4.3372951 -4.3462715 -4.3513947 -4.3466458 -4.3409853 -4.3330932 -4.3234234 -4.3176231 -4.3138051 -4.3129911 -4.3176937 -4.3232532 -4.3216038 -4.315474 -4.30266][-4.3337617 -4.3408446 -4.3475466 -4.3473239 -4.3460054 -4.3429117 -4.3384724 -4.33448 -4.326179 -4.317028 -4.3134637 -4.3117414 -4.309288 -4.3041377 -4.2931728]]...]
INFO - root - 2017-12-05 17:54:04.201197: step 30610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 71h:05m:57s remains)
INFO - root - 2017-12-05 17:54:12.606495: step 30620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 70h:59m:42s remains)
INFO - root - 2017-12-05 17:54:21.168739: step 30630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 73h:07m:49s remains)
INFO - root - 2017-12-05 17:54:29.604705: step 30640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 71h:03m:37s remains)
INFO - root - 2017-12-05 17:54:38.125873: step 30650, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 70h:08m:21s remains)
INFO - root - 2017-12-05 17:54:46.549395: step 30660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 70h:21m:16s remains)
INFO - root - 2017-12-05 17:54:54.917164: step 30670, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 72h:23m:23s remains)
INFO - root - 2017-12-05 17:55:03.442669: step 30680, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.818 sec/batch; 68h:36m:43s remains)
INFO - root - 2017-12-05 17:55:12.101916: step 30690, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.867 sec/batch; 72h:40m:38s remains)
INFO - root - 2017-12-05 17:55:20.561235: step 30700, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 71h:27m:37s remains)
2017-12-05 17:55:21.359715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3028035 -4.2910714 -4.2894616 -4.2961264 -4.2994604 -4.2982197 -4.2987661 -4.2970896 -4.2908087 -4.2821021 -4.2775993 -4.2801418 -4.287118 -4.2966318 -4.3063831][-4.2807431 -4.263895 -4.2615328 -4.2699409 -4.2744861 -4.2715921 -4.2732968 -4.272171 -4.2645254 -4.2558026 -4.2532382 -4.2578135 -4.2662039 -4.27646 -4.2875805][-4.2595978 -4.2396541 -4.2369623 -4.2447538 -4.2488403 -4.2431135 -4.2458067 -4.2468982 -4.242136 -4.2373233 -4.2363281 -4.2407351 -4.2460093 -4.2531223 -4.264811][-4.2416959 -4.2197623 -4.2152882 -4.2210112 -4.2211409 -4.2103534 -4.2129307 -4.2162156 -4.216116 -4.2190843 -4.2235742 -4.2305412 -4.2344327 -4.2393064 -4.2522774][-4.21807 -4.195034 -4.189559 -4.1925964 -4.1913948 -4.1812344 -4.1838384 -4.1865239 -4.1854253 -4.1941447 -4.2060037 -4.2140269 -4.218492 -4.2229776 -4.2379985][-4.1873236 -4.1637435 -4.1606736 -4.1683979 -4.1735525 -4.1695828 -4.1668196 -4.1515956 -4.1405582 -4.1505437 -4.1667562 -4.1752443 -4.1797085 -4.1889124 -4.2120514][-4.1809425 -4.1613913 -4.1594758 -4.168993 -4.1727042 -4.1615949 -4.1333704 -4.0873637 -4.0618863 -4.0825047 -4.1137562 -4.1298018 -4.137846 -4.1578627 -4.1972737][-4.1894493 -4.1681705 -4.15862 -4.1566277 -4.1487107 -4.1230597 -4.0680037 -3.9925468 -3.9633133 -4.0181851 -4.078856 -4.1065607 -4.1166058 -4.1434135 -4.1919179][-4.1933904 -4.1621757 -4.1410694 -4.1289597 -4.1196537 -4.1013017 -4.0566745 -4.0012178 -3.9957747 -4.0625434 -4.1210604 -4.1396794 -4.1341653 -4.1507 -4.1925483][-4.1907206 -4.1525774 -4.1291685 -4.1244287 -4.129065 -4.1304636 -4.1169038 -4.0958028 -4.1007977 -4.1451902 -4.1805968 -4.1811423 -4.162725 -4.1676059 -4.1974888][-4.188529 -4.1546006 -4.1427288 -4.1509819 -4.1628675 -4.1688957 -4.1656065 -4.1567144 -4.1603851 -4.1855578 -4.2046967 -4.1975303 -4.1762304 -4.1775346 -4.2025228][-4.2039948 -4.1798372 -4.1778588 -4.1904316 -4.2028403 -4.2058792 -4.20156 -4.192956 -4.1926446 -4.2076626 -4.2227988 -4.2182584 -4.1997647 -4.1988211 -4.2190013][-4.2268047 -4.2092195 -4.20803 -4.2199244 -4.2322669 -4.2344356 -4.2303424 -4.22376 -4.2250538 -4.2357922 -4.2483597 -4.2495637 -4.2367573 -4.2334237 -4.2458544][-4.2542877 -4.2407422 -4.2388749 -4.2481356 -4.2581677 -4.2610092 -4.2603359 -4.2571087 -4.259449 -4.2662416 -4.273788 -4.2761803 -4.2693419 -4.2661281 -4.2726197][-4.282033 -4.2718067 -4.2712779 -4.2786536 -4.28599 -4.2888103 -4.2891183 -4.2883415 -4.2912383 -4.2966971 -4.3000679 -4.3005075 -4.2959976 -4.29292 -4.2964797]]...]
INFO - root - 2017-12-05 17:55:30.067105: step 30710, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 71h:07m:52s remains)
INFO - root - 2017-12-05 17:55:38.580689: step 30720, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 74h:21m:19s remains)
INFO - root - 2017-12-05 17:55:47.153866: step 30730, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 73h:56m:22s remains)
INFO - root - 2017-12-05 17:55:55.723049: step 30740, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 70h:24m:21s remains)
INFO - root - 2017-12-05 17:56:04.291815: step 30750, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 70h:01m:00s remains)
INFO - root - 2017-12-05 17:56:12.862233: step 30760, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.851 sec/batch; 71h:21m:10s remains)
INFO - root - 2017-12-05 17:56:21.213564: step 30770, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 72h:30m:44s remains)
INFO - root - 2017-12-05 17:56:29.672024: step 30780, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 71h:43m:17s remains)
INFO - root - 2017-12-05 17:56:38.270796: step 30790, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 71h:57m:25s remains)
INFO - root - 2017-12-05 17:56:46.726421: step 30800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 70h:12m:56s remains)
2017-12-05 17:56:47.547226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3028984 -4.2990618 -4.2953405 -4.292624 -4.2911005 -4.2880578 -4.28389 -4.2774329 -4.2742667 -4.275044 -4.2780361 -4.2797775 -4.2828703 -4.2901521 -4.2997675][-4.3012304 -4.2964449 -4.2941408 -4.2945309 -4.2937226 -4.2872782 -4.2780652 -4.2672796 -4.2641988 -4.2654591 -4.2670593 -4.2668133 -4.2685394 -4.2750092 -4.284831][-4.3057804 -4.2988377 -4.2942886 -4.2927275 -4.2880564 -4.2785616 -4.2667212 -4.2528043 -4.2505941 -4.2537012 -4.2551475 -4.2548685 -4.2564049 -4.2633519 -4.2747087][-4.3071718 -4.2973146 -4.2879748 -4.278605 -4.2672725 -4.2582784 -4.2462626 -4.2282724 -4.2244897 -4.2295351 -4.231173 -4.231041 -4.2340851 -4.2453966 -4.2625093][-4.2980809 -4.2845 -4.269877 -4.2522931 -4.2374916 -4.2281933 -4.21472 -4.1934752 -4.1914525 -4.20121 -4.201601 -4.2024932 -4.2091517 -4.226757 -4.25177][-4.276371 -4.2573237 -4.2372408 -4.212791 -4.1960034 -4.1856451 -4.168117 -4.1423187 -4.1492643 -4.1734514 -4.1778874 -4.1798992 -4.1894479 -4.2123981 -4.2442751][-4.2563486 -4.229835 -4.2019515 -4.1709652 -4.1516919 -4.1359396 -4.1063585 -4.0684147 -4.0870566 -4.13548 -4.1523438 -4.159966 -4.1743073 -4.2027941 -4.2398834][-4.2408843 -4.2048497 -4.1678867 -4.1307607 -4.1062956 -4.0785437 -4.0264273 -3.9652109 -4.0002675 -4.0811186 -4.11854 -4.1375189 -4.1599197 -4.1950321 -4.2372031][-4.2355437 -4.1931024 -4.1496625 -4.1079254 -4.079946 -4.040597 -3.9740918 -3.8993704 -3.9478307 -4.0485663 -4.1007872 -4.1306868 -4.1614828 -4.1980114 -4.2387862][-4.2535996 -4.2184882 -4.1807718 -4.1465793 -4.1285763 -4.1011934 -4.0502505 -3.9928749 -4.0263152 -4.1007485 -4.1421618 -4.1673908 -4.19319 -4.2198443 -4.2497392][-4.2780132 -4.250999 -4.2187281 -4.1947017 -4.1874566 -4.1788592 -4.1539211 -4.1187811 -4.1346378 -4.1758542 -4.2004275 -4.2144146 -4.230433 -4.248908 -4.2684708][-4.294651 -4.2741451 -4.2480006 -4.2308407 -4.2298107 -4.231606 -4.2198081 -4.1975832 -4.2022629 -4.224267 -4.2402067 -4.248672 -4.25829 -4.2718415 -4.2862072][-4.299437 -4.2839885 -4.2647204 -4.2530956 -4.2534089 -4.2572761 -4.2499394 -4.2317877 -4.2296119 -4.2412453 -4.2541661 -4.2618184 -4.2695713 -4.2829127 -4.2965651][-4.2944021 -4.2826791 -4.2693124 -4.2610946 -4.2597818 -4.2632394 -4.2597618 -4.2461605 -4.24163 -4.2464991 -4.2565303 -4.2646279 -4.2725797 -4.2864318 -4.3010478][-4.2946367 -4.2839713 -4.27227 -4.265326 -4.2617097 -4.2643166 -4.265862 -4.2614336 -4.2578797 -4.2568769 -4.2626247 -4.2704892 -4.2786775 -4.2914128 -4.3060713]]...]
INFO - root - 2017-12-05 17:56:56.093947: step 30810, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 72h:59m:47s remains)
INFO - root - 2017-12-05 17:57:04.582514: step 30820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 71h:35m:31s remains)
INFO - root - 2017-12-05 17:57:13.183579: step 30830, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.836 sec/batch; 70h:03m:40s remains)
INFO - root - 2017-12-05 17:57:21.685958: step 30840, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 71h:54m:22s remains)
INFO - root - 2017-12-05 17:57:30.287525: step 30850, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 71h:32m:40s remains)
INFO - root - 2017-12-05 17:57:38.863273: step 30860, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:28m:17s remains)
INFO - root - 2017-12-05 17:57:47.291899: step 30870, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 70h:06m:36s remains)
INFO - root - 2017-12-05 17:57:55.689736: step 30880, loss = 2.08, batch loss = 2.02 (10.8 examples/sec; 0.743 sec/batch; 62h:15m:30s remains)
INFO - root - 2017-12-05 17:58:04.174276: step 30890, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 70h:22m:07s remains)
INFO - root - 2017-12-05 17:58:12.723915: step 30900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:27m:33s remains)
2017-12-05 17:58:13.472797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2326031 -4.2099562 -4.2081022 -4.207931 -4.2019458 -4.1946354 -4.1929131 -4.1943088 -4.1938915 -4.2079692 -4.2249589 -4.2474551 -4.262414 -4.2568393 -4.2400451][-4.2354779 -4.2088532 -4.2004266 -4.1958661 -4.1856251 -4.1731443 -4.1673646 -4.1620703 -4.1601424 -4.1845207 -4.2160478 -4.2451563 -4.2578278 -4.2476058 -4.2319098][-4.2215514 -4.1932011 -4.1822824 -4.177238 -4.1675 -4.1530147 -4.1416 -4.1308484 -4.1210546 -4.1493745 -4.1892905 -4.2258291 -4.241569 -4.2322159 -4.2224574][-4.2023444 -4.1762242 -4.1673918 -4.1656666 -4.1576447 -4.138103 -4.1198392 -4.1038995 -4.088737 -4.1167436 -4.1609688 -4.1994686 -4.2180042 -4.2114234 -4.2116537][-4.207397 -4.1862731 -4.1843185 -4.1887374 -4.18165 -4.1530466 -4.1187425 -4.0841761 -4.0552573 -4.0792665 -4.1274304 -4.1679935 -4.1881132 -4.1834273 -4.1903076][-4.2235651 -4.2100978 -4.2131667 -4.2176685 -4.2052779 -4.1671481 -4.1203294 -4.073885 -4.0405393 -4.0597038 -4.1100917 -4.1514044 -4.168612 -4.1594224 -4.1638265][-4.2520609 -4.2414269 -4.2429619 -4.2442851 -4.22883 -4.1874819 -4.1364236 -4.0941772 -4.0695443 -4.085885 -4.1237574 -4.1572113 -4.1701574 -4.1483445 -4.1411257][-4.2797318 -4.267148 -4.2643957 -4.2623692 -4.2459006 -4.2107406 -4.1630788 -4.1282573 -4.1072588 -4.1153865 -4.1390734 -4.1630859 -4.1767497 -4.1508622 -4.1338496][-4.2964678 -4.2826357 -4.2764988 -4.2723179 -4.2562294 -4.2255306 -4.1834283 -4.1520147 -4.1277456 -4.1252127 -4.1409445 -4.1588869 -4.1742697 -4.1561136 -4.1401339][-4.3086176 -4.2940731 -4.2859988 -4.2826881 -4.2698169 -4.2414064 -4.2024517 -4.1694531 -4.1398191 -4.1255431 -4.1363053 -4.1541352 -4.1713638 -4.1640954 -4.152246][-4.3173246 -4.3053522 -4.2980051 -4.2948627 -4.2845783 -4.2598929 -4.2234383 -4.190176 -4.1590204 -4.1356764 -4.141901 -4.1609445 -4.1802449 -4.1821218 -4.1741285][-4.3232374 -4.3142257 -4.3080335 -4.3035469 -4.2940931 -4.2758675 -4.2478719 -4.2200751 -4.1933975 -4.170085 -4.1730313 -4.188345 -4.2025156 -4.2021184 -4.1932583][-4.3259487 -4.3214517 -4.3182206 -4.3133574 -4.3049159 -4.2921295 -4.2742691 -4.2565551 -4.2398539 -4.2243624 -4.225462 -4.234746 -4.2380557 -4.2291756 -4.2160163][-4.3268332 -4.3256454 -4.3238382 -4.3192391 -4.3132973 -4.3062973 -4.2967305 -4.2876973 -4.2813911 -4.2744555 -4.273849 -4.2797489 -4.2753 -4.2624846 -4.2504106][-4.3269987 -4.3281045 -4.3268952 -4.32298 -4.318891 -4.3146124 -4.3095403 -4.3057814 -4.3070583 -4.3072319 -4.3064852 -4.3097954 -4.3041334 -4.2922 -4.2842364]]...]
INFO - root - 2017-12-05 17:58:21.986431: step 30910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 71h:53m:47s remains)
INFO - root - 2017-12-05 17:58:30.533401: step 30920, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.822 sec/batch; 68h:49m:17s remains)
INFO - root - 2017-12-05 17:58:39.049129: step 30930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 72h:06m:15s remains)
INFO - root - 2017-12-05 17:58:47.476257: step 30940, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:42m:38s remains)
INFO - root - 2017-12-05 17:58:55.956533: step 30950, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 71h:27m:49s remains)
INFO - root - 2017-12-05 17:59:04.422299: step 30960, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 69h:38m:27s remains)
INFO - root - 2017-12-05 17:59:12.852200: step 30970, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 70h:03m:22s remains)
INFO - root - 2017-12-05 17:59:21.279683: step 30980, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 70h:42m:46s remains)
INFO - root - 2017-12-05 17:59:29.699804: step 30990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 72h:15m:15s remains)
INFO - root - 2017-12-05 17:59:38.268905: step 31000, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 70h:30m:04s remains)
2017-12-05 17:59:39.017619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3206205 -4.3187513 -4.3157659 -4.3141484 -4.3137121 -4.3143663 -4.3168387 -4.3197174 -4.3211946 -4.3189344 -4.3135076 -4.3089871 -4.3075447 -4.307972 -4.3087015][-4.3184934 -4.3136873 -4.3088412 -4.3053803 -4.3038206 -4.3041921 -4.3065028 -4.3115993 -4.3138032 -4.3097043 -4.3001008 -4.2921963 -4.2913809 -4.2957463 -4.3009562][-4.2875896 -4.2852592 -4.2830019 -4.2787738 -4.2747593 -4.2709341 -4.266727 -4.2665648 -4.2681861 -4.2655234 -4.2563066 -4.2482929 -4.2522244 -4.2667394 -4.2825737][-4.2242732 -4.2328758 -4.2401667 -4.2368779 -4.2257729 -4.208231 -4.1865954 -4.1707134 -4.1712523 -4.178782 -4.1781483 -4.177557 -4.193553 -4.2243786 -4.2555761][-4.1370277 -4.1704092 -4.1940055 -4.1931715 -4.1693506 -4.1245713 -4.06736 -4.0246129 -4.0281773 -4.0590229 -4.081614 -4.1022682 -4.1403441 -4.1896696 -4.2340336][-4.0545707 -4.120141 -4.1656179 -4.1690741 -4.1315989 -4.0536284 -3.9495153 -3.8716459 -3.8854172 -3.9546173 -4.0149693 -4.0701537 -4.1342044 -4.1932306 -4.2372427][-4.0148883 -4.1044064 -4.165184 -4.1724515 -4.1296782 -4.0360556 -3.9118252 -3.8237531 -3.845825 -3.93707 -4.023531 -4.1015587 -4.1756682 -4.2286115 -4.2591176][-4.0468545 -4.1331158 -4.1903133 -4.19739 -4.1577 -4.0731487 -3.9726884 -3.9114439 -3.9336014 -4.0109024 -4.0913415 -4.1662769 -4.2299476 -4.2676172 -4.2836437][-4.1220131 -4.1914535 -4.2352204 -4.2414546 -4.210928 -4.1452065 -4.0793571 -4.0467167 -4.0627117 -4.1127868 -4.1711178 -4.2284489 -4.2726865 -4.2940388 -4.2983012][-4.199295 -4.2480206 -4.2786808 -4.2835579 -4.2621303 -4.2185469 -4.1820703 -4.169035 -4.1784124 -4.2039118 -4.2368312 -4.2708244 -4.2941546 -4.3020549 -4.2994952][-4.2607174 -4.2893243 -4.307004 -4.3083634 -4.2922878 -4.2663345 -4.2493172 -4.247467 -4.251863 -4.2612696 -4.2754035 -4.290976 -4.3000207 -4.3005147 -4.2954869][-4.2931962 -4.3081708 -4.3175335 -4.3155971 -4.3042264 -4.2903657 -4.2837925 -4.2850342 -4.286108 -4.2881818 -4.2930336 -4.2989359 -4.3008852 -4.2978754 -4.2928424][-4.3077383 -4.3137732 -4.3158197 -4.3119221 -4.3048525 -4.2990746 -4.2973709 -4.2988114 -4.2982011 -4.2977786 -4.2988815 -4.2998843 -4.2987247 -4.2948384 -4.2905579][-4.3102317 -4.3109365 -4.3081894 -4.3035216 -4.2993631 -4.2972403 -4.2969279 -4.2978239 -4.2968693 -4.2955689 -4.2948356 -4.294137 -4.2925963 -4.2895923 -4.28655][-4.3047366 -4.3031054 -4.2981539 -4.293005 -4.289794 -4.28841 -4.2884197 -4.2885742 -4.2876797 -4.2869806 -4.2862353 -4.2858963 -4.2852831 -4.2834358 -4.2819037]]...]
INFO - root - 2017-12-05 17:59:47.531153: step 31010, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 72h:21m:26s remains)
INFO - root - 2017-12-05 17:59:55.976632: step 31020, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 69h:30m:31s remains)
INFO - root - 2017-12-05 18:00:04.475507: step 31030, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 70h:21m:45s remains)
INFO - root - 2017-12-05 18:00:13.093607: step 31040, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.873 sec/batch; 73h:05m:11s remains)
INFO - root - 2017-12-05 18:00:21.678919: step 31050, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 74h:33m:03s remains)
INFO - root - 2017-12-05 18:00:30.005830: step 31060, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 71h:50m:18s remains)
INFO - root - 2017-12-05 18:00:38.403052: step 31070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 72h:49m:10s remains)
INFO - root - 2017-12-05 18:00:46.939302: step 31080, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 69h:42m:23s remains)
INFO - root - 2017-12-05 18:00:55.516645: step 31090, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 71h:13m:29s remains)
INFO - root - 2017-12-05 18:01:03.987256: step 31100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:47m:51s remains)
2017-12-05 18:01:04.733819: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1636705 -4.1623659 -4.1724348 -4.1599407 -4.1426678 -4.1426997 -4.18234 -4.2259922 -4.2471056 -4.2513041 -4.2466264 -4.2288232 -4.1996212 -4.1707921 -4.1549773][-4.157136 -4.1536632 -4.1677675 -4.1600571 -4.1492696 -4.1499696 -4.1896491 -4.2321692 -4.2496681 -4.249218 -4.2406163 -4.2204847 -4.1931872 -4.1674266 -4.1587539][-4.1527829 -4.15559 -4.1759334 -4.1709137 -4.1617718 -4.1579266 -4.1848531 -4.2185783 -4.2367764 -4.2413211 -4.2360978 -4.2190351 -4.1963077 -4.1781244 -4.174582][-4.1446643 -4.1554794 -4.1757545 -4.1686916 -4.1569033 -4.1437778 -4.1517649 -4.1736341 -4.1991048 -4.2227859 -4.2353964 -4.2267113 -4.2085514 -4.193079 -4.1923752][-4.148036 -4.1563129 -4.1698627 -4.1572733 -4.1325788 -4.0935292 -4.0746312 -4.0825686 -4.1183691 -4.1739326 -4.2229648 -4.2346559 -4.2249455 -4.2157497 -4.2168617][-4.1530738 -4.1522584 -4.152216 -4.1258841 -4.0828767 -4.0175729 -3.9718459 -3.9647665 -4.0173564 -4.1150455 -4.2037764 -4.243433 -4.2497711 -4.2467961 -4.240087][-4.157485 -4.1504722 -4.147172 -4.1131644 -4.0582628 -3.9750676 -3.9101748 -3.8920822 -3.961719 -4.0885959 -4.2009087 -4.2576728 -4.2770176 -4.2734385 -4.2543426][-4.1468997 -4.1407485 -4.1438732 -4.118381 -4.068574 -3.9865255 -3.9199915 -3.9042249 -3.9774384 -4.1064711 -4.2180934 -4.2776594 -4.3000989 -4.2916822 -4.2620869][-4.1278591 -4.125535 -4.1413937 -4.1362376 -4.1073523 -4.0477662 -4.0010452 -3.9950292 -4.0592146 -4.1651068 -4.2543492 -4.3014374 -4.3154716 -4.2971053 -4.25296][-4.1143579 -4.1134152 -4.1357846 -4.1502347 -4.1462817 -4.1174726 -4.1010489 -4.1148114 -4.1680098 -4.2459364 -4.3023849 -4.32109 -4.31228 -4.2774796 -4.2278147][-4.1064711 -4.1016083 -4.1287694 -4.1622558 -4.1799932 -4.1741056 -4.1808558 -4.2095442 -4.2550263 -4.3048654 -4.3261523 -4.3187556 -4.2941217 -4.2523522 -4.205689][-4.1130772 -4.0968237 -4.1201034 -4.166132 -4.1986227 -4.2131624 -4.2359767 -4.2686644 -4.3022394 -4.3287106 -4.3302245 -4.3128843 -4.285511 -4.24227 -4.2017655][-4.1230669 -4.0991488 -4.1189828 -4.1709085 -4.2137122 -4.2423038 -4.2731786 -4.3038211 -4.3247643 -4.3328071 -4.3230581 -4.3051348 -4.2812138 -4.2421341 -4.2075233][-4.1255827 -4.1068735 -4.1356483 -4.1907339 -4.2337689 -4.2672696 -4.2965183 -4.3187118 -4.3278809 -4.3225169 -4.3073459 -4.2914596 -4.2712507 -4.2374897 -4.2040038][-4.14195 -4.1354613 -4.1719704 -4.2238116 -4.2609568 -4.2911091 -4.3112745 -4.3227429 -4.3259559 -4.3185043 -4.3028774 -4.2849603 -4.2647357 -4.2326126 -4.195405]]...]
INFO - root - 2017-12-05 18:01:13.200670: step 31110, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 70h:13m:38s remains)
INFO - root - 2017-12-05 18:01:21.776996: step 31120, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 69h:42m:02s remains)
INFO - root - 2017-12-05 18:01:30.338292: step 31130, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 75h:01m:47s remains)
INFO - root - 2017-12-05 18:01:38.821332: step 31140, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 71h:52m:00s remains)
INFO - root - 2017-12-05 18:01:47.390092: step 31150, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 71h:10m:27s remains)
INFO - root - 2017-12-05 18:01:55.901353: step 31160, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 73h:40m:52s remains)
INFO - root - 2017-12-05 18:02:04.329284: step 31170, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 70h:28m:32s remains)
INFO - root - 2017-12-05 18:02:12.857316: step 31180, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 71h:39m:49s remains)
INFO - root - 2017-12-05 18:02:21.311075: step 31190, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 70h:27m:30s remains)
INFO - root - 2017-12-05 18:02:29.751020: step 31200, loss = 2.06, batch loss = 2.01 (10.1 examples/sec; 0.789 sec/batch; 66h:00m:51s remains)
2017-12-05 18:02:30.578055: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29268 -4.2961731 -4.2892833 -4.281611 -4.2742872 -4.2690258 -4.2672515 -4.2714152 -4.281867 -4.2847648 -4.2654552 -4.2326651 -4.2058611 -4.1904006 -4.1834989][-4.26926 -4.26977 -4.2575603 -4.24196 -4.2287645 -4.2197852 -4.21733 -4.2269073 -4.2502365 -4.2655463 -4.2522316 -4.2187405 -4.1873927 -4.166625 -4.1566648][-4.2449102 -4.2379007 -4.2185006 -4.194952 -4.1726012 -4.1559663 -4.1536889 -4.1728311 -4.2179685 -4.2516055 -4.2494721 -4.2207737 -4.189918 -4.1650262 -4.1446338][-4.22845 -4.2154384 -4.1902218 -4.1613588 -4.1312013 -4.10663 -4.1000714 -4.125639 -4.1903715 -4.249279 -4.2628164 -4.239347 -4.2067628 -4.1764507 -4.1437798][-4.231173 -4.217154 -4.1882758 -4.1545248 -4.111547 -4.0693169 -4.0453396 -4.0714769 -4.1563282 -4.2452021 -4.2838469 -4.2714291 -4.2376308 -4.2004538 -4.158061][-4.2423925 -4.2323265 -4.2047925 -4.1604905 -4.0919108 -4.0133352 -3.9572997 -3.9871604 -4.1032429 -4.2292266 -4.2941966 -4.2990675 -4.2692423 -4.2291346 -4.1797857][-4.2524424 -4.2478261 -4.2200809 -4.1580629 -4.0512357 -3.9184272 -3.8142385 -3.8571692 -4.0220613 -4.1895838 -4.2807789 -4.3058038 -4.2871723 -4.2485108 -4.1945176][-4.2579856 -4.2578149 -4.2267208 -4.1452355 -3.998771 -3.8050702 -3.6429548 -3.7012305 -3.9255285 -4.1333294 -4.2505889 -4.298686 -4.2959547 -4.2589736 -4.2001204][-4.2534018 -4.2576818 -4.2297096 -4.1438761 -3.9881179 -3.7756453 -3.5815282 -3.6372066 -3.8817623 -4.1037512 -4.2335243 -4.2956114 -4.3013754 -4.2601175 -4.1933765][-4.2489028 -4.2632012 -4.2437139 -4.1713939 -4.0441775 -3.872962 -3.7142253 -3.7433717 -3.938767 -4.1261497 -4.2415586 -4.2984872 -4.301301 -4.2528195 -4.1780453][-4.2334509 -4.2576942 -4.2502594 -4.2001958 -4.1122293 -3.9987254 -3.8950844 -3.9092021 -4.0391116 -4.1706533 -4.2536178 -4.29198 -4.28372 -4.2292447 -4.1562238][-4.2270288 -4.2530465 -4.2548752 -4.2263393 -4.1704321 -4.099257 -4.0391378 -4.051311 -4.1348944 -4.2172527 -4.2712874 -4.2914104 -4.2718949 -4.215744 -4.1513839][-4.2346625 -4.2555475 -4.2567978 -4.242054 -4.2115693 -4.1693115 -4.1351933 -4.1473517 -4.2016325 -4.25285 -4.287261 -4.2983174 -4.2756577 -4.2234907 -4.1754947][-4.2499843 -4.2666345 -4.267314 -4.263082 -4.2519841 -4.2283058 -4.2072568 -4.2168226 -4.2515469 -4.284338 -4.3076472 -4.3147454 -4.2939472 -4.2511325 -4.21841][-4.2720318 -4.2850218 -4.2884398 -4.2902689 -4.2902722 -4.2815437 -4.2698755 -4.2734442 -4.2933917 -4.3132229 -4.3256459 -4.3275757 -4.3118091 -4.2830424 -4.2641234]]...]
INFO - root - 2017-12-05 18:02:39.095806: step 31210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:41m:21s remains)
INFO - root - 2017-12-05 18:02:47.597142: step 31220, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 70h:56m:10s remains)
INFO - root - 2017-12-05 18:02:56.181155: step 31230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 70h:35m:07s remains)
INFO - root - 2017-12-05 18:03:04.768214: step 31240, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 72h:02m:23s remains)
INFO - root - 2017-12-05 18:03:13.226751: step 31250, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 72h:34m:59s remains)
INFO - root - 2017-12-05 18:03:21.816160: step 31260, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 71h:01m:48s remains)
INFO - root - 2017-12-05 18:03:30.268363: step 31270, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 72h:09m:25s remains)
INFO - root - 2017-12-05 18:03:38.586776: step 31280, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.815 sec/batch; 68h:12m:50s remains)
INFO - root - 2017-12-05 18:03:47.227752: step 31290, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:44m:19s remains)
INFO - root - 2017-12-05 18:03:55.698803: step 31300, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 69h:17m:16s remains)
2017-12-05 18:03:56.452525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1353135 -4.1266813 -4.117527 -4.123476 -4.1489868 -4.175899 -4.194005 -4.2029548 -4.2048793 -4.1979251 -4.1807971 -4.1632962 -4.1469116 -4.1353989 -4.1329589][-4.1472073 -4.1367064 -4.1330042 -4.1483927 -4.179574 -4.2080932 -4.2250152 -4.2289028 -4.2248387 -4.2122035 -4.1904411 -4.1664929 -4.1441965 -4.1303921 -4.127377][-4.1663508 -4.1611853 -4.1628585 -4.1816173 -4.2108388 -4.235899 -4.250329 -4.2522268 -4.246109 -4.2328658 -4.2110534 -4.1841474 -4.159339 -4.1456409 -4.1456089][-4.1805358 -4.1843228 -4.1905518 -4.2055874 -4.2257209 -4.2428317 -4.2525816 -4.2545276 -4.2522459 -4.2451978 -4.23001 -4.2069497 -4.1853828 -4.1756763 -4.1784286][-4.180263 -4.1908011 -4.1975579 -4.2056508 -4.21496 -4.2232809 -4.2276535 -4.2317967 -4.2378349 -4.2405658 -4.2363586 -4.2242465 -4.2126775 -4.2107887 -4.2149177][-4.1749272 -4.184454 -4.1883059 -4.1884465 -4.18726 -4.1846676 -4.1839519 -4.1921968 -4.208076 -4.2221789 -4.2306471 -4.2333689 -4.2362413 -4.2445636 -4.2502842][-4.1725111 -4.1707673 -4.1650758 -4.1548057 -4.1425037 -4.1293416 -4.1247873 -4.1367159 -4.1598048 -4.1840477 -4.2046065 -4.2231851 -4.243094 -4.2635384 -4.2713423][-4.1762362 -4.1628366 -4.1463232 -4.1242123 -4.1003861 -4.0782561 -4.071424 -4.0867319 -4.1127796 -4.1410413 -4.1687307 -4.1992435 -4.2328277 -4.2626262 -4.2704444][-4.1936741 -4.1725864 -4.1478968 -4.1186848 -4.0898843 -4.0677996 -4.0635557 -4.0756106 -4.0927315 -4.1140647 -4.1395335 -4.1731234 -4.2119446 -4.24453 -4.252542][-4.2203689 -4.1944013 -4.1650705 -4.1342368 -4.1078558 -4.0925903 -4.0934968 -4.0976419 -4.0996923 -4.1095405 -4.1280632 -4.1569862 -4.1921778 -4.2212148 -4.22894][-4.2465143 -4.2202945 -4.1916695 -4.1640682 -4.1424055 -4.132237 -4.1325431 -4.1267171 -4.1169896 -4.1170759 -4.1270089 -4.1481709 -4.1745338 -4.1960955 -4.2013874][-4.2644715 -4.2422519 -4.2191238 -4.1980033 -4.18275 -4.1755638 -4.1720543 -4.1576233 -4.1409721 -4.1340413 -4.1352091 -4.148138 -4.1653705 -4.1786532 -4.1818676][-4.2743831 -4.2586093 -4.2440047 -4.23236 -4.2260857 -4.2236419 -4.2191343 -4.2023144 -4.1847367 -4.173532 -4.1662931 -4.16832 -4.1721015 -4.1740112 -4.17283][-4.2749767 -4.2671375 -4.2620378 -4.2605314 -4.2628732 -4.2658587 -4.2639027 -4.2500157 -4.2358294 -4.2216191 -4.2056532 -4.1939673 -4.1838403 -4.1755476 -4.168838][-4.2690358 -4.2689748 -4.2727866 -4.2803373 -4.2895579 -4.2968426 -4.2979364 -4.2873745 -4.2745657 -4.254653 -4.2274094 -4.20154 -4.1818094 -4.1698532 -4.1617446]]...]
INFO - root - 2017-12-05 18:04:04.890611: step 31310, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.875 sec/batch; 73h:10m:00s remains)
INFO - root - 2017-12-05 18:04:13.454738: step 31320, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 69h:56m:15s remains)
INFO - root - 2017-12-05 18:04:21.974634: step 31330, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 71h:29m:55s remains)
INFO - root - 2017-12-05 18:04:30.532368: step 31340, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 70h:38m:54s remains)
INFO - root - 2017-12-05 18:04:39.073992: step 31350, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 73h:25m:40s remains)
INFO - root - 2017-12-05 18:04:47.628652: step 31360, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 72h:30m:46s remains)
INFO - root - 2017-12-05 18:04:55.960244: step 31370, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 72h:27m:25s remains)
INFO - root - 2017-12-05 18:05:04.542553: step 31380, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 71h:33m:33s remains)
INFO - root - 2017-12-05 18:05:13.081787: step 31390, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 72h:19m:40s remains)
INFO - root - 2017-12-05 18:05:21.478403: step 31400, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 70h:10m:11s remains)
2017-12-05 18:05:22.210730: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1516848 -4.1455817 -4.120389 -4.0965157 -4.0977983 -4.13552 -4.1898522 -4.2439704 -4.2916069 -4.3245635 -4.338233 -4.3348246 -4.3247604 -4.3145914 -4.3016524][-4.1729836 -4.1816006 -4.1682887 -4.153336 -4.1558867 -4.1844649 -4.2236886 -4.2678618 -4.3088088 -4.3351703 -4.3454056 -4.34273 -4.3319612 -4.3172135 -4.2996907][-4.1962395 -4.2132616 -4.2133923 -4.2120004 -4.2171993 -4.2339063 -4.2529721 -4.283658 -4.317616 -4.3370886 -4.3435574 -4.3436813 -4.3352218 -4.3182306 -4.2975492][-4.2033892 -4.2212343 -4.2294903 -4.2377663 -4.2440534 -4.246767 -4.2457161 -4.2635474 -4.2934737 -4.3141217 -4.3247681 -4.3336573 -4.3332415 -4.3189182 -4.2974968][-4.1915388 -4.2053409 -4.2102242 -4.2173624 -4.2184496 -4.204042 -4.18395 -4.1987286 -4.2398763 -4.2738156 -4.2967634 -4.31843 -4.32784 -4.3170137 -4.2933426][-4.1688476 -4.1733317 -4.1689825 -4.1687341 -4.1617265 -4.1251268 -4.078783 -4.0920167 -4.1572738 -4.2173529 -4.2608848 -4.2967796 -4.313766 -4.3039651 -4.2758079][-4.1276021 -4.123189 -4.1122379 -4.1037483 -4.0863318 -4.0243883 -3.948385 -3.9638128 -4.0608339 -4.1538134 -4.2171931 -4.2629437 -4.283093 -4.2727127 -4.2399383][-4.0862503 -4.0796962 -4.0690212 -4.0565228 -4.03141 -3.9603317 -3.8827035 -3.9112751 -4.0271635 -4.1307116 -4.1947203 -4.2350283 -4.2496271 -4.2338562 -4.1924758][-4.0769982 -4.0719743 -4.0670209 -4.0596313 -4.0399265 -3.9880905 -3.9369509 -3.9689269 -4.0652347 -4.1456671 -4.190537 -4.2176189 -4.2237177 -4.1999121 -4.1443624][-4.114182 -4.1123333 -4.1106915 -4.1067815 -4.0946813 -4.0635786 -4.0352077 -4.0571666 -4.1186624 -4.1691213 -4.1943479 -4.2082109 -4.2049356 -4.172492 -4.1070929][-4.1719589 -4.1697326 -4.1672816 -4.1623888 -4.1519 -4.1329832 -4.1158104 -4.1254253 -4.158648 -4.18701 -4.2009435 -4.2050743 -4.1960158 -4.1640677 -4.1066365][-4.2211161 -4.2186027 -4.2172241 -4.2124891 -4.2033167 -4.19084 -4.1789846 -4.1802926 -4.1968093 -4.2146339 -4.2243652 -4.2250514 -4.2167945 -4.1950564 -4.1595087][-4.2513514 -4.2512603 -4.2523251 -4.2495995 -4.2426262 -4.2333422 -4.2253671 -4.225709 -4.2370977 -4.25161 -4.2606797 -4.26018 -4.2546139 -4.2432961 -4.2261295][-4.2650323 -4.2679453 -4.2717519 -4.2715144 -4.2675157 -4.2611103 -4.256012 -4.257319 -4.266571 -4.277668 -4.2828946 -4.2801538 -4.2759852 -4.2717009 -4.2661853][-4.2793183 -4.2836504 -4.2883453 -4.290267 -4.2898769 -4.2868094 -4.2837315 -4.2844911 -4.2887368 -4.293849 -4.2934136 -4.2863789 -4.2804594 -4.2775154 -4.2764626]]...]
INFO - root - 2017-12-05 18:05:30.793238: step 31410, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 70h:42m:14s remains)
INFO - root - 2017-12-05 18:05:39.310516: step 31420, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 71h:17m:36s remains)
INFO - root - 2017-12-05 18:05:47.836552: step 31430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 71h:07m:58s remains)
INFO - root - 2017-12-05 18:05:56.325348: step 31440, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 69h:24m:53s remains)
INFO - root - 2017-12-05 18:06:04.877667: step 31450, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:44m:39s remains)
INFO - root - 2017-12-05 18:06:13.377501: step 31460, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 70h:23m:15s remains)
INFO - root - 2017-12-05 18:06:21.828057: step 31470, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 71h:01m:35s remains)
INFO - root - 2017-12-05 18:06:30.308739: step 31480, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.814 sec/batch; 68h:03m:56s remains)
INFO - root - 2017-12-05 18:06:38.848879: step 31490, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 72h:59m:04s remains)
INFO - root - 2017-12-05 18:06:47.482417: step 31500, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 74h:40m:35s remains)
2017-12-05 18:06:48.218670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2808766 -4.2777367 -4.273509 -4.2714934 -4.26963 -4.2646408 -4.2548718 -4.2448163 -4.2372341 -4.2343931 -4.2349663 -4.2349281 -4.230051 -4.2290206 -4.2338204][-4.2752562 -4.26906 -4.2609291 -4.2571812 -4.2544918 -4.2491493 -4.2399192 -4.2297459 -4.2223692 -4.2223787 -4.2271976 -4.2306304 -4.2302957 -4.232379 -4.2372775][-4.2806616 -4.2716165 -4.2620215 -4.2583327 -4.2555327 -4.2506261 -4.2418623 -4.2330666 -4.2290025 -4.2335715 -4.2422881 -4.247406 -4.2491384 -4.2519612 -4.2537303][-4.2787848 -4.2696924 -4.2620945 -4.2580152 -4.2543874 -4.2487159 -4.2391968 -4.2301064 -4.2291188 -4.2398138 -4.2540336 -4.2606745 -4.2634354 -4.2645946 -4.2633691][-4.2585282 -4.25037 -4.2427897 -4.2358241 -4.2278414 -4.2183104 -4.2054477 -4.194591 -4.196341 -4.2130184 -4.2314334 -4.2412066 -4.2455306 -4.2474642 -4.24701][-4.22935 -4.2195573 -4.2091713 -4.1967125 -4.1828017 -4.1692357 -4.1516981 -4.1362147 -4.1381149 -4.1576552 -4.1779747 -4.1900077 -4.1969862 -4.2027268 -4.2077479][-4.15211 -4.1394348 -4.1274924 -4.1131454 -4.0957808 -4.0793624 -4.0600181 -4.0421104 -4.0442963 -4.0659528 -4.0862875 -4.0995731 -4.1100049 -4.1217976 -4.1352139][-4.0833774 -4.0633683 -4.0471315 -4.0282 -4.0063257 -3.9867327 -3.9661083 -3.9478965 -3.9523711 -3.9765632 -3.9936256 -4.003757 -4.0135365 -4.0280166 -4.0490861][-4.1447749 -4.1271734 -4.1116571 -4.093997 -4.0751181 -4.0591536 -4.0446863 -4.0366282 -4.0441 -4.0601377 -4.0675297 -4.0703673 -4.0725574 -4.0786986 -4.0926466][-4.2101426 -4.2018228 -4.1925855 -4.18315 -4.1741195 -4.1654248 -4.1572313 -4.1560421 -4.163373 -4.1719551 -4.1741338 -4.1747742 -4.17564 -4.178844 -4.1862969][-4.2449641 -4.2455292 -4.2437487 -4.2423358 -4.2414012 -4.2374473 -4.2305574 -4.2284231 -4.2323408 -4.2358723 -4.2362719 -4.2371416 -4.2398462 -4.2426996 -4.2443671][-4.2518783 -4.2560444 -4.2592125 -4.2629833 -4.2663531 -4.2652245 -4.2599697 -4.2586117 -4.2607965 -4.2625728 -4.2636704 -4.2659059 -4.2696505 -4.2720866 -4.2716565][-4.2173653 -4.2233729 -4.2316532 -4.2394037 -4.2448816 -4.2446032 -4.2410517 -4.2408962 -4.2426591 -4.2450824 -4.2485247 -4.2523351 -4.2569246 -4.261775 -4.2651024][-4.192965 -4.1968932 -4.2025933 -4.2080922 -4.2112589 -4.2090926 -4.2055964 -4.2059031 -4.2080431 -4.2117753 -4.2173548 -4.2227106 -4.2265043 -4.2326341 -4.2414026][-4.2071142 -4.2135153 -4.2182 -4.2210374 -4.220777 -4.2169714 -4.2129178 -4.2112188 -4.2096968 -4.2104816 -4.214458 -4.2176414 -4.2197108 -4.2260828 -4.2373228]]...]
INFO - root - 2017-12-05 18:06:56.764773: step 31510, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 73h:16m:52s remains)
INFO - root - 2017-12-05 18:07:05.168042: step 31520, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 68h:52m:49s remains)
INFO - root - 2017-12-05 18:07:13.811870: step 31530, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 73h:36m:49s remains)
INFO - root - 2017-12-05 18:07:22.273569: step 31540, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 69h:59m:47s remains)
INFO - root - 2017-12-05 18:07:30.748855: step 31550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 71h:55m:33s remains)
INFO - root - 2017-12-05 18:07:39.288009: step 31560, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 72h:10m:34s remains)
INFO - root - 2017-12-05 18:07:47.604524: step 31570, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.854 sec/batch; 71h:24m:28s remains)
INFO - root - 2017-12-05 18:07:56.204451: step 31580, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 68h:41m:27s remains)
INFO - root - 2017-12-05 18:08:04.760644: step 31590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 70h:43m:22s remains)
INFO - root - 2017-12-05 18:08:13.332687: step 31600, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.786 sec/batch; 65h:40m:27s remains)
2017-12-05 18:08:14.182652: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28274 -4.2855368 -4.286943 -4.2868071 -4.2857695 -4.2832642 -4.2900481 -4.2953219 -4.2920451 -4.2838421 -4.270237 -4.2562003 -4.2484941 -4.2455249 -4.2392206][-4.2788229 -4.2823639 -4.2821832 -4.280201 -4.2780647 -4.2774153 -4.285428 -4.2901073 -4.2783623 -4.2533674 -4.2284369 -4.2151103 -4.2127023 -4.2126856 -4.2067752][-4.2578449 -4.2608809 -4.2583241 -4.2537823 -4.2476721 -4.2436047 -4.2497053 -4.2516747 -4.2327428 -4.1938667 -4.1616049 -4.156836 -4.1700716 -4.1812963 -4.179656][-4.2326775 -4.2314911 -4.2217317 -4.2118087 -4.200304 -4.1915793 -4.1936383 -4.1917424 -4.1711016 -4.1295252 -4.0964885 -4.0985041 -4.1264753 -4.1513009 -4.1611423][-4.2226291 -4.2124043 -4.189465 -4.1669168 -4.140666 -4.1200771 -4.1129847 -4.1116047 -4.1065755 -4.0850239 -4.0664663 -4.0782647 -4.1160078 -4.147676 -4.1643567][-4.2194734 -4.1985936 -4.1649141 -4.1305981 -4.0868912 -4.0463376 -4.0173955 -4.017374 -4.0438571 -4.0590725 -4.0701046 -4.0988159 -4.1443262 -4.1764441 -4.1929121][-4.2189112 -4.1921282 -4.1559649 -4.1197686 -4.0687327 -4.0087867 -3.9557626 -3.9542916 -4.0101519 -4.0605493 -4.0990624 -4.1410627 -4.1900992 -4.2221589 -4.2357473][-4.2236562 -4.1955791 -4.1660886 -4.1409154 -4.0981894 -4.0370436 -3.9816797 -3.9789004 -4.0352058 -4.0919194 -4.1374264 -4.1816111 -4.22941 -4.2629352 -4.2729697][-4.2182879 -4.1924033 -4.1746974 -4.167645 -4.1446128 -4.1030006 -4.0680175 -4.0697732 -4.1085124 -4.1493688 -4.1873341 -4.2231755 -4.2626109 -4.29032 -4.2934046][-4.209425 -4.1930137 -4.1909451 -4.1975965 -4.1888356 -4.1664534 -4.1525497 -4.1614804 -4.1874256 -4.2146821 -4.2427869 -4.2686324 -4.2940149 -4.3072171 -4.2961197][-4.2152796 -4.2131491 -4.223619 -4.2346272 -4.231988 -4.2218285 -4.2195129 -4.2309852 -4.2501497 -4.2713623 -4.2934809 -4.3119178 -4.3229585 -4.3175144 -4.2889333][-4.2253881 -4.2339897 -4.24981 -4.2624226 -4.2652168 -4.2643027 -4.2699251 -4.2811842 -4.2951713 -4.3106885 -4.326911 -4.3369675 -4.3332944 -4.3107252 -4.2691379][-4.2294669 -4.2456894 -4.2632923 -4.2769294 -4.2844143 -4.2902923 -4.2987647 -4.3071084 -4.3145165 -4.3213296 -4.3292818 -4.3298969 -4.3127222 -4.277956 -4.2317066][-4.23645 -4.2562928 -4.2733722 -4.2878847 -4.29654 -4.3030353 -4.3099937 -4.3149796 -4.3176618 -4.3163977 -4.312789 -4.3013377 -4.2740211 -4.2333493 -4.1881623][-4.2601876 -4.2770119 -4.2891374 -4.2984047 -4.3022108 -4.3047304 -4.3068223 -4.3091307 -4.3096056 -4.3033066 -4.29093 -4.2714539 -4.2401896 -4.1994214 -4.1594281]]...]
INFO - root - 2017-12-05 18:08:22.545108: step 31610, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 71h:10m:50s remains)
INFO - root - 2017-12-05 18:08:30.956697: step 31620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 70h:53m:59s remains)
INFO - root - 2017-12-05 18:08:39.409885: step 31630, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 70h:30m:03s remains)
INFO - root - 2017-12-05 18:08:47.834547: step 31640, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 69h:41m:54s remains)
INFO - root - 2017-12-05 18:08:56.274920: step 31650, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.817 sec/batch; 68h:17m:16s remains)
INFO - root - 2017-12-05 18:09:04.716823: step 31660, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 69h:45m:22s remains)
INFO - root - 2017-12-05 18:09:13.243713: step 31670, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 69h:07m:32s remains)
INFO - root - 2017-12-05 18:09:21.725578: step 31680, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 69h:49m:40s remains)
INFO - root - 2017-12-05 18:09:30.202098: step 31690, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 69h:56m:23s remains)
INFO - root - 2017-12-05 18:09:38.693253: step 31700, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 70h:21m:52s remains)
2017-12-05 18:09:39.594232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1993074 -4.201529 -4.1849647 -4.16767 -4.1492167 -4.1190276 -4.085032 -4.0828638 -4.1073451 -4.1405773 -4.1660848 -4.1927314 -4.220273 -4.2288589 -4.2188144][-4.2147956 -4.2150187 -4.1947479 -4.17444 -4.1465373 -4.0923014 -4.0335436 -4.0297551 -4.0769563 -4.1386285 -4.1775661 -4.209178 -4.233593 -4.2344112 -4.2228956][-4.2187734 -4.2213621 -4.2014809 -4.1796341 -4.1441584 -4.0743771 -3.9966912 -3.9819622 -4.0418344 -4.1252284 -4.1809874 -4.2165089 -4.2345996 -4.2278223 -4.217454][-4.2131925 -4.2227645 -4.207068 -4.1867986 -4.154017 -4.0860577 -4.0071716 -3.9826052 -4.0374327 -4.1227722 -4.1846185 -4.2205253 -4.2336373 -4.2214918 -4.2127333][-4.1970205 -4.2159266 -4.2064786 -4.1918149 -4.1695666 -4.1148305 -4.0491772 -4.0272241 -4.0694342 -4.1393023 -4.1946921 -4.2259221 -4.2348738 -4.21961 -4.2127748][-4.178956 -4.2025638 -4.1995549 -4.1896663 -4.1726112 -4.1298289 -4.0805922 -4.0671067 -4.1055865 -4.16179 -4.2065125 -4.2304373 -4.2342262 -4.2221603 -4.2191858][-4.169138 -4.1935496 -4.1906433 -4.18479 -4.1723375 -4.1356597 -4.0931678 -4.0876846 -4.1283231 -4.1767745 -4.2116542 -4.2295027 -4.2294931 -4.2217679 -4.224576][-4.1601911 -4.1845918 -4.1812816 -4.1796131 -4.1711655 -4.1385794 -4.0975847 -4.0979776 -4.145298 -4.1902437 -4.21867 -4.2311039 -4.2267866 -4.2201595 -4.2252955][-4.1547413 -4.1812057 -4.175281 -4.1727962 -4.1642804 -4.13263 -4.0916977 -4.0971723 -4.1535764 -4.2001657 -4.2275209 -4.2379909 -4.2308173 -4.2211595 -4.2229824][-4.1552534 -4.1875982 -4.1852722 -4.1796393 -4.1674442 -4.1319604 -4.0890985 -4.0962853 -4.1583233 -4.2076907 -4.2347631 -4.2438478 -4.2336645 -4.2223949 -4.2200317][-4.1639891 -4.205627 -4.2117958 -4.2038527 -4.184854 -4.1469436 -4.1026468 -4.1052761 -4.1656065 -4.2170382 -4.2442122 -4.2498555 -4.2394886 -4.2283444 -4.2231684][-4.1843562 -4.2293286 -4.2405367 -4.2284045 -4.2050962 -4.1725717 -4.1352906 -4.1322603 -4.17976 -4.2264767 -4.2504005 -4.2528982 -4.2443442 -4.2321291 -4.2258868][-4.2030673 -4.2427874 -4.2520676 -4.2356119 -4.2128119 -4.190228 -4.168819 -4.1670036 -4.2020164 -4.2377353 -4.2523413 -4.2502441 -4.2422671 -4.2296624 -4.2237649][-4.2189622 -4.2468767 -4.2513113 -4.2332721 -4.2143016 -4.2009182 -4.1926961 -4.1961975 -4.2237544 -4.2476492 -4.2522206 -4.246244 -4.2370071 -4.2264433 -4.2208343][-4.2417779 -4.2561903 -4.2550659 -4.23889 -4.2257719 -4.2199178 -4.2176585 -4.2243557 -4.2462378 -4.263124 -4.2632751 -4.2540693 -4.2442694 -4.23587 -4.2320223]]...]
INFO - root - 2017-12-05 18:09:47.993709: step 31710, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 70h:31m:33s remains)
INFO - root - 2017-12-05 18:09:56.518247: step 31720, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 69h:54m:31s remains)
INFO - root - 2017-12-05 18:10:04.943804: step 31730, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.886 sec/batch; 73h:59m:45s remains)
INFO - root - 2017-12-05 18:10:13.317373: step 31740, loss = 2.08, batch loss = 2.02 (10.0 examples/sec; 0.802 sec/batch; 67h:01m:00s remains)
INFO - root - 2017-12-05 18:10:21.840937: step 31750, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.812 sec/batch; 67h:48m:55s remains)
INFO - root - 2017-12-05 18:10:30.268491: step 31760, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 68h:12m:58s remains)
INFO - root - 2017-12-05 18:10:38.663477: step 31770, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 72h:07m:10s remains)
INFO - root - 2017-12-05 18:10:47.148872: step 31780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 70h:29m:01s remains)
INFO - root - 2017-12-05 18:10:55.752336: step 31790, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.893 sec/batch; 74h:36m:56s remains)
INFO - root - 2017-12-05 18:11:04.228882: step 31800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 70h:19m:46s remains)
2017-12-05 18:11:05.074107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3508654 -4.3476033 -4.3448133 -4.3392406 -4.3321309 -4.3256221 -4.3139243 -4.28632 -4.2324729 -4.162508 -4.1052628 -4.0984011 -4.1404533 -4.2110672 -4.2786188][-4.3483458 -4.3439751 -4.3381381 -4.3282256 -4.3164086 -4.30618 -4.2906027 -4.2639747 -4.2147188 -4.1458712 -4.0856771 -4.0761676 -4.1193933 -4.193996 -4.2666044][-4.3454432 -4.3400421 -4.3307681 -4.3176413 -4.3007679 -4.2842841 -4.2670894 -4.248939 -4.2228837 -4.1837196 -4.1500468 -4.1479669 -4.1841197 -4.2416978 -4.29506][-4.3409824 -4.3337731 -4.3201451 -4.3019471 -4.2772217 -4.2514057 -4.2302203 -4.2167735 -4.2107334 -4.2038469 -4.2065754 -4.2246776 -4.2601314 -4.3025508 -4.3347206][-4.333714 -4.3235211 -4.3056016 -4.2810054 -4.2485952 -4.2109351 -4.1765885 -4.1568084 -4.1592817 -4.18003 -4.2172437 -4.2636013 -4.306644 -4.3417711 -4.3614259][-4.3186908 -4.3065138 -4.2841883 -4.2524695 -4.2111921 -4.156373 -4.0944052 -4.0509295 -4.0561924 -4.1061177 -4.1795287 -4.256825 -4.3160748 -4.3538866 -4.3681078][-4.29052 -4.2782373 -4.2528105 -4.2138972 -4.1625352 -4.0873971 -3.9890656 -3.9143045 -3.9225988 -4.002264 -4.1083832 -4.2144251 -4.2903247 -4.3368225 -4.3557749][-4.2570271 -4.2492194 -4.226172 -4.1851597 -4.1243 -4.0360613 -3.9195571 -3.8273914 -3.8385911 -3.933594 -4.0556231 -4.1787639 -4.2646871 -4.317977 -4.3438272][-4.2424707 -4.24085 -4.2234631 -4.1858354 -4.1238751 -4.0421343 -3.9450262 -3.8736479 -3.8857813 -3.9687846 -4.0787 -4.1944122 -4.2739234 -4.3213105 -4.3453712][-4.264699 -4.2642908 -4.2488222 -4.2138953 -4.1581759 -4.0909181 -4.0206265 -3.9776249 -3.9907532 -4.0540667 -4.1415024 -4.2372131 -4.30488 -4.3399887 -4.3557243][-4.3041539 -4.301733 -4.28657 -4.2548485 -4.2059507 -4.1486583 -4.09514 -4.0693784 -4.0851789 -4.1387095 -4.2100425 -4.2849145 -4.3379917 -4.3601956 -4.3659649][-4.3363824 -4.333817 -4.322269 -4.2976646 -4.260901 -4.2172322 -4.1780958 -4.1608949 -4.1743293 -4.2177444 -4.2735124 -4.3268032 -4.3613896 -4.3705826 -4.3691058][-4.347959 -4.3469625 -4.3422589 -4.3289671 -4.308217 -4.2842107 -4.2601137 -4.2497411 -4.2598214 -4.2887635 -4.3254075 -4.3568568 -4.372952 -4.3714089 -4.3662462][-4.3445463 -4.3434649 -4.3444519 -4.3425279 -4.3363156 -4.3268638 -4.31485 -4.3109531 -4.318182 -4.334239 -4.3534231 -4.3679924 -4.3714266 -4.3648214 -4.3591938][-4.3328629 -4.3312683 -4.3367395 -4.3446321 -4.3502874 -4.3504715 -4.3466177 -4.3454251 -4.3499517 -4.3566084 -4.3634043 -4.3667755 -4.3639455 -4.3569813 -4.3517394]]...]
INFO - root - 2017-12-05 18:11:13.734713: step 31810, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.830 sec/batch; 69h:19m:47s remains)
INFO - root - 2017-12-05 18:11:22.239678: step 31820, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 69h:00m:02s remains)
INFO - root - 2017-12-05 18:11:30.746885: step 31830, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 69h:50m:49s remains)
INFO - root - 2017-12-05 18:11:39.156956: step 31840, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 72h:43m:03s remains)
INFO - root - 2017-12-05 18:11:47.609092: step 31850, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 69h:47m:54s remains)
INFO - root - 2017-12-05 18:11:56.174581: step 31860, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.829 sec/batch; 69h:14m:55s remains)
INFO - root - 2017-12-05 18:12:04.594579: step 31870, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 70h:37m:43s remains)
INFO - root - 2017-12-05 18:12:13.215900: step 31880, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 68h:23m:03s remains)
INFO - root - 2017-12-05 18:12:21.770154: step 31890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 71h:54m:16s remains)
INFO - root - 2017-12-05 18:12:30.359354: step 31900, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 72h:09m:52s remains)
2017-12-05 18:12:31.106463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2580853 -4.2689185 -4.2802057 -4.2825866 -4.2768064 -4.2704372 -4.2672424 -4.2629147 -4.24907 -4.2289047 -4.2112317 -4.2073073 -4.2230744 -4.2540021 -4.2846613][-4.2553835 -4.2687516 -4.2809744 -4.2805862 -4.2685843 -4.2559114 -4.250175 -4.247252 -4.2371383 -4.2206588 -4.2050915 -4.2023144 -4.2193089 -4.2521763 -4.2839847][-4.2650313 -4.2786665 -4.2894244 -4.2854037 -4.2669344 -4.2477703 -4.239068 -4.2372 -4.2323866 -4.2238588 -4.212718 -4.2102771 -4.2269754 -4.2595305 -4.2893224][-4.2781053 -4.2867875 -4.2937245 -4.2863603 -4.2617779 -4.2345862 -4.2215714 -4.2219944 -4.2257032 -4.2268457 -4.2211666 -4.2199259 -4.2357249 -4.2668791 -4.2923574][-4.2811255 -4.2831 -4.2853994 -4.2757087 -4.2460709 -4.2100015 -4.1899643 -4.1918511 -4.2038679 -4.216712 -4.219697 -4.22259 -4.2375298 -4.2655578 -4.2882814][-4.2665477 -4.2677441 -4.2699165 -4.2566876 -4.221415 -4.1775856 -4.1521444 -4.1529756 -4.1664147 -4.1857595 -4.1973958 -4.2069583 -4.2234759 -4.2518106 -4.2765837][-4.2419004 -4.2473564 -4.2549329 -4.2427006 -4.2048817 -4.1560235 -4.1253328 -4.1207819 -4.1277938 -4.1486216 -4.1659942 -4.1816778 -4.2015619 -4.2348251 -4.2648458][-4.2055769 -4.2207675 -4.2422051 -4.24028 -4.2071385 -4.1586604 -4.1239018 -4.1136818 -4.1140194 -4.1319547 -4.1501317 -4.1687031 -4.1910558 -4.2255135 -4.2586384][-4.1622796 -4.1903577 -4.2307277 -4.247745 -4.2294884 -4.18799 -4.1505375 -4.1357303 -4.1326728 -4.1461582 -4.1609116 -4.177299 -4.1972852 -4.2278566 -4.2603631][-4.1313977 -4.1727867 -4.2303095 -4.2662425 -4.2634764 -4.2289858 -4.1905422 -4.1728859 -4.1694221 -4.1787052 -4.1902533 -4.2032971 -4.2166705 -4.2398663 -4.2691092][-4.1328 -4.1782174 -4.2402525 -4.2820091 -4.2843742 -4.2556529 -4.22031 -4.2028003 -4.1999717 -4.2062955 -4.2161722 -4.224884 -4.2325377 -4.2509103 -4.2790442][-4.1555767 -4.1927433 -4.2468185 -4.2835035 -4.2874022 -4.2651467 -4.2375727 -4.2214689 -4.2179508 -4.2208614 -4.2281785 -4.2329321 -4.2385755 -4.2567539 -4.2845316][-4.1826043 -4.2061906 -4.2468271 -4.2771721 -4.2850456 -4.2724929 -4.2517285 -4.2343397 -4.2280121 -4.2268896 -4.2317953 -4.2354112 -4.2426386 -4.2615933 -4.2880187][-4.2022123 -4.21425 -4.2434244 -4.2708454 -4.2851348 -4.2813091 -4.2649689 -4.2456207 -4.2341709 -4.2291741 -4.2319517 -4.236351 -4.2463641 -4.2670045 -4.29238][-4.214107 -4.2203407 -4.239459 -4.2598252 -4.2779202 -4.2844315 -4.2757831 -4.2575636 -4.241128 -4.2313981 -4.2319727 -4.2381496 -4.2509885 -4.273128 -4.2968073]]...]
INFO - root - 2017-12-05 18:12:39.588511: step 31910, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 69h:15m:22s remains)
INFO - root - 2017-12-05 18:12:48.076129: step 31920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 70h:31m:55s remains)
INFO - root - 2017-12-05 18:12:56.529355: step 31930, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 71h:28m:40s remains)
INFO - root - 2017-12-05 18:13:05.069237: step 31940, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 69h:50m:10s remains)
INFO - root - 2017-12-05 18:13:13.447271: step 31950, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 72h:15m:31s remains)
INFO - root - 2017-12-05 18:13:21.977612: step 31960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 69h:57m:55s remains)
INFO - root - 2017-12-05 18:13:30.458676: step 31970, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 73h:08m:25s remains)
INFO - root - 2017-12-05 18:13:39.150150: step 31980, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 71h:54m:00s remains)
INFO - root - 2017-12-05 18:13:47.699700: step 31990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 71h:22m:18s remains)
INFO - root - 2017-12-05 18:13:56.233994: step 32000, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 70h:02m:20s remains)
2017-12-05 18:13:56.991880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3178763 -4.3029895 -4.2924767 -4.2831993 -4.2711248 -4.2564511 -4.2433004 -4.2336049 -4.2275906 -4.2273359 -4.2295923 -4.2328296 -4.2419367 -4.2546477 -4.2655268][-4.3148856 -4.3003445 -4.2919869 -4.2810764 -4.2632427 -4.2450523 -4.2335391 -4.2226491 -4.2096 -4.2065978 -4.2082977 -4.2129397 -4.2240319 -4.2373748 -4.248198][-4.3135014 -4.2998762 -4.2901511 -4.2731838 -4.2486572 -4.2281671 -4.2187519 -4.2109628 -4.1972957 -4.1945004 -4.1944742 -4.1977539 -4.2121859 -4.2279296 -4.2394767][-4.3082151 -4.2943716 -4.2828355 -4.2605019 -4.2296147 -4.2062464 -4.1981559 -4.1944814 -4.1792707 -4.1743627 -4.1762633 -4.1828074 -4.1985097 -4.2168317 -4.2299557][-4.3007336 -4.2821727 -4.2661462 -4.238658 -4.20459 -4.17985 -4.1747184 -4.1734109 -4.1549087 -4.1459327 -4.1519179 -4.1648312 -4.1808538 -4.199646 -4.2178016][-4.2913547 -4.2626028 -4.2383413 -4.2064872 -4.1725655 -4.1494226 -4.1452165 -4.1468267 -4.1242962 -4.1110215 -4.1214962 -4.1440787 -4.16831 -4.1936607 -4.2188725][-4.2886357 -4.2521462 -4.2183275 -4.1803703 -4.1485529 -4.1276665 -4.1216388 -4.1196361 -4.0966492 -4.08547 -4.1031742 -4.1371942 -4.1731944 -4.2074184 -4.232389][-4.2951517 -4.2563019 -4.2177486 -4.1754122 -4.1407151 -4.1209126 -4.1118536 -4.1035528 -4.0835066 -4.0742388 -4.0953803 -4.1353936 -4.1815138 -4.2193875 -4.2377005][-4.3017197 -4.2665277 -4.2320375 -4.1977406 -4.1686511 -4.147306 -4.125598 -4.1062508 -4.08983 -4.0785642 -4.09598 -4.1353054 -4.1837978 -4.2188058 -4.231637][-4.3032584 -4.2719193 -4.2459426 -4.2259431 -4.2051325 -4.1818886 -4.1468573 -4.1146836 -4.096118 -4.0846291 -4.0988092 -4.1326947 -4.1764193 -4.2089128 -4.2191052][-4.2966976 -4.26482 -4.2427883 -4.2304506 -4.2116132 -4.1816978 -4.1370854 -4.1014457 -4.090899 -4.0897646 -4.1050358 -4.1351247 -4.1684928 -4.1969662 -4.2112007][-4.2868938 -4.2494955 -4.2228837 -4.2088509 -4.1892066 -4.1564717 -4.113915 -4.0896544 -4.0984888 -4.108057 -4.1233821 -4.1412959 -4.1581035 -4.179863 -4.19527][-4.2814536 -4.2375307 -4.2032089 -4.1845679 -4.1650634 -4.139245 -4.1095142 -4.1027346 -4.1246018 -4.1370349 -4.1456113 -4.1435695 -4.1406364 -4.155581 -4.1719356][-4.2859921 -4.2382174 -4.1953888 -4.1726394 -4.1619139 -4.1495848 -4.1387291 -4.1454678 -4.1671939 -4.1759048 -4.17447 -4.1603045 -4.1445293 -4.1537519 -4.1710396][-4.2927346 -4.2421293 -4.1909075 -4.1641212 -4.1618204 -4.1577883 -4.156271 -4.1725745 -4.1959114 -4.2035265 -4.199122 -4.1824889 -4.1645379 -4.1693592 -4.1855569]]...]
INFO - root - 2017-12-05 18:14:05.498914: step 32010, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 69h:15m:01s remains)
INFO - root - 2017-12-05 18:14:13.968055: step 32020, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 73h:14m:46s remains)
INFO - root - 2017-12-05 18:14:22.502100: step 32030, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 71h:23m:24s remains)
INFO - root - 2017-12-05 18:14:30.973499: step 32040, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 70h:26m:53s remains)
INFO - root - 2017-12-05 18:14:39.483588: step 32050, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 71h:33m:46s remains)
INFO - root - 2017-12-05 18:14:47.944209: step 32060, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 71h:25m:56s remains)
INFO - root - 2017-12-05 18:14:56.337519: step 32070, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 71h:52m:31s remains)
INFO - root - 2017-12-05 18:15:04.865759: step 32080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 71h:01m:53s remains)
INFO - root - 2017-12-05 18:15:13.407493: step 32090, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 71h:17m:13s remains)
INFO - root - 2017-12-05 18:15:22.005818: step 32100, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 70h:16m:40s remains)
2017-12-05 18:15:22.757246: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1125951 -4.1003013 -4.1082368 -4.1235385 -4.1358418 -4.1332269 -4.122829 -4.1228995 -4.1387177 -4.1671295 -4.1947818 -4.21841 -4.2348146 -4.2520609 -4.2669482][-4.0886054 -4.0764675 -4.0890985 -4.1030831 -4.111444 -4.1055603 -4.0906582 -4.0880251 -4.1070528 -4.1414523 -4.1751566 -4.206706 -4.2302122 -4.2516236 -4.2635913][-4.0919347 -4.0836525 -4.0975571 -4.1078 -4.1088233 -4.1001186 -4.0856266 -4.0813136 -4.09847 -4.13075 -4.1648979 -4.1997805 -4.2257118 -4.2464571 -4.2576213][-4.1169882 -4.1140795 -4.1287842 -4.13568 -4.131608 -4.1227608 -4.1095157 -4.1048088 -4.1188831 -4.1446781 -4.1707621 -4.1987362 -4.21973 -4.2366762 -4.2468081][-4.1384807 -4.13691 -4.1469584 -4.1471167 -4.1375866 -4.1304345 -4.1258755 -4.1247234 -4.1378617 -4.1578274 -4.1766834 -4.1960435 -4.2098475 -4.222167 -4.2283111][-4.1324177 -4.1258392 -4.1294007 -4.1212506 -4.1101708 -4.10882 -4.1163116 -4.1221781 -4.1329823 -4.1483736 -4.1651382 -4.1798091 -4.1898422 -4.1988912 -4.2029848][-4.1050315 -4.09069 -4.0862179 -4.068758 -4.0527782 -4.0581975 -4.0777988 -4.0921397 -4.1054459 -4.1207671 -4.1390834 -4.1520767 -4.1598716 -4.1680479 -4.1756763][-4.0783653 -4.0567336 -4.0469451 -4.0273347 -4.009769 -4.018188 -4.0494623 -4.0700603 -4.0825281 -4.0947661 -4.1111617 -4.1261163 -4.1355963 -4.1465082 -4.1600771][-4.0757785 -4.0535288 -4.0482755 -4.0435762 -4.0344939 -4.0410156 -4.0687675 -4.0822558 -4.0823045 -4.0850534 -4.0965962 -4.1089349 -4.1181087 -4.1295538 -4.1459179][-4.0968637 -4.0767789 -4.0751085 -4.083602 -4.085948 -4.0895696 -4.1052947 -4.110198 -4.1042051 -4.0993462 -4.1015277 -4.1063461 -4.1097488 -4.1159368 -4.132421][-4.134161 -4.1151094 -4.116003 -4.1274414 -4.13495 -4.1368241 -4.1408777 -4.1393471 -4.13279 -4.1257095 -4.1209531 -4.1204877 -4.1220531 -4.1249089 -4.1375694][-4.1766167 -4.1589026 -4.1639662 -4.1803474 -4.1920085 -4.1941867 -4.1916261 -4.1870294 -4.1798339 -4.1719508 -4.1652389 -4.1616349 -4.161634 -4.1636715 -4.1717167][-4.2145309 -4.1997161 -4.2052312 -4.2218509 -4.235507 -4.240087 -4.2379222 -4.2323093 -4.2249765 -4.216526 -4.2080612 -4.2043629 -4.2054691 -4.2091007 -4.21561][-4.2466226 -4.2350068 -4.2370629 -4.2484279 -4.2581277 -4.261899 -4.2613573 -4.2587295 -4.254355 -4.2479215 -4.2414646 -4.2394695 -4.2412095 -4.2450285 -4.2504964][-4.2734351 -4.265285 -4.2645745 -4.2691689 -4.2742152 -4.2767906 -4.2770696 -4.2758393 -4.274117 -4.2715831 -4.2694073 -4.268981 -4.2701893 -4.2721848 -4.2743111]]...]
INFO - root - 2017-12-05 18:15:31.197377: step 32110, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:37m:59s remains)
INFO - root - 2017-12-05 18:15:39.646926: step 32120, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.824 sec/batch; 68h:44m:20s remains)
INFO - root - 2017-12-05 18:15:48.136886: step 32130, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 70h:53m:39s remains)
INFO - root - 2017-12-05 18:15:56.692265: step 32140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 69h:54m:41s remains)
INFO - root - 2017-12-05 18:16:05.132694: step 32150, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 69h:57m:41s remains)
INFO - root - 2017-12-05 18:16:13.723858: step 32160, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:36m:05s remains)
INFO - root - 2017-12-05 18:16:22.018128: step 32170, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 68h:05m:23s remains)
INFO - root - 2017-12-05 18:16:30.584203: step 32180, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.840 sec/batch; 70h:05m:01s remains)
INFO - root - 2017-12-05 18:16:39.009297: step 32190, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 73h:27m:07s remains)
INFO - root - 2017-12-05 18:16:47.446074: step 32200, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 70h:19m:08s remains)
2017-12-05 18:16:48.198546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1151352 -4.1559544 -4.1810651 -4.19309 -4.1809621 -4.1521554 -4.1287589 -4.1307158 -4.1452055 -4.1625009 -4.1695018 -4.1808963 -4.2046375 -4.2138567 -4.2215033][-4.1073971 -4.1429558 -4.1667757 -4.1708422 -4.151876 -4.1283054 -4.1148925 -4.11712 -4.13014 -4.1476974 -4.1531849 -4.158164 -4.1763325 -4.1909766 -4.2055631][-4.0884094 -4.1109924 -4.1233974 -4.1179905 -4.1017151 -4.0922565 -4.0920782 -4.0982733 -4.1079526 -4.1209507 -4.1233749 -4.1198778 -4.1298704 -4.1503625 -4.1794081][-4.0513282 -4.0655012 -4.0736108 -4.0699263 -4.0694823 -4.0779362 -4.0868669 -4.08993 -4.0959749 -4.1094594 -4.1093273 -4.100039 -4.1025376 -4.1252513 -4.162684][-4.0175123 -4.02814 -4.0377398 -4.0415645 -4.0527563 -4.0711231 -4.0811987 -4.0796156 -4.0844984 -4.1051617 -4.1126232 -4.1073527 -4.1096907 -4.1312685 -4.1647463][-4.019278 -4.0328426 -4.0394297 -4.0409007 -4.0482578 -4.06102 -4.0647507 -4.0600328 -4.0631151 -4.0867772 -4.102879 -4.108223 -4.1183195 -4.1429281 -4.1730623][-4.0253758 -4.0379252 -4.0381088 -4.0324192 -4.0282984 -4.0292549 -4.0259728 -4.0211725 -4.0227017 -4.0389538 -4.0506163 -4.0630679 -4.0859556 -4.1228943 -4.1608987][-4.0399981 -4.0465584 -4.0371532 -4.0227389 -4.0118995 -4.0066104 -4.0015855 -3.9945617 -3.9891927 -3.985172 -3.9828196 -3.9962585 -4.0313454 -4.0840316 -4.1366005][-4.0536304 -4.0597281 -4.0446253 -4.0239305 -4.0151529 -4.0164781 -4.0188446 -4.0108409 -3.9967062 -3.9716516 -3.954921 -3.9625769 -3.9974194 -4.055222 -4.111372][-4.0407209 -4.0502572 -4.0350938 -4.01384 -4.0084028 -4.0181832 -4.0350051 -4.033093 -4.0138688 -3.9836919 -3.9621434 -3.9661059 -3.9964566 -4.0474715 -4.0979476][-4.0337157 -4.04538 -4.0355415 -4.0197496 -4.01996 -4.0334435 -4.0531311 -4.0542545 -4.0373197 -4.0089426 -3.9870377 -3.986418 -4.0117569 -4.0543709 -4.0971913][-4.04882 -4.0701475 -4.0720515 -4.0655122 -4.0662804 -4.0729814 -4.0810957 -4.0771012 -4.0668406 -4.0484943 -4.028604 -4.0238152 -4.0418029 -4.0749626 -4.1095185][-4.0733323 -4.1029215 -4.1151142 -4.1187277 -4.1195455 -4.1185622 -4.111331 -4.098556 -4.0905609 -4.0769777 -4.0620208 -4.0591431 -4.0751572 -4.1032929 -4.1317129][-4.0969505 -4.1270838 -4.14155 -4.1540828 -4.1601887 -4.1584973 -4.1452346 -4.1263824 -4.1104 -4.0939589 -4.0791516 -4.0760169 -4.0899487 -4.11569 -4.1410918][-4.1171622 -4.1420374 -4.1514187 -4.158761 -4.1665864 -4.1678729 -4.1595669 -4.1452045 -4.130549 -4.1156015 -4.1038604 -4.1029239 -4.1164312 -4.1385193 -4.1611013]]...]
INFO - root - 2017-12-05 18:16:56.778252: step 32210, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 69h:31m:31s remains)
INFO - root - 2017-12-05 18:17:05.288699: step 32220, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 70h:44m:17s remains)
INFO - root - 2017-12-05 18:17:13.731556: step 32230, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 70h:09m:32s remains)
INFO - root - 2017-12-05 18:17:22.494009: step 32240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 71h:45m:46s remains)
INFO - root - 2017-12-05 18:17:31.037461: step 32250, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 71h:19m:48s remains)
INFO - root - 2017-12-05 18:17:39.489277: step 32260, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.841 sec/batch; 70h:07m:22s remains)
INFO - root - 2017-12-05 18:17:47.839828: step 32270, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 70h:26m:07s remains)
INFO - root - 2017-12-05 18:17:56.492568: step 32280, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 72h:02m:09s remains)
INFO - root - 2017-12-05 18:18:05.162561: step 32290, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 72h:07m:19s remains)
INFO - root - 2017-12-05 18:18:13.633907: step 32300, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 70h:48m:36s remains)
2017-12-05 18:18:14.388120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2305641 -4.2035928 -4.1763525 -4.1566663 -4.1520343 -4.1617284 -4.1671381 -4.173893 -4.2002163 -4.2273636 -4.2377791 -4.214047 -4.1742687 -4.152421 -4.1557035][-4.2313175 -4.2020493 -4.1744409 -4.1594081 -4.1631222 -4.17411 -4.1717472 -4.1708384 -4.1900277 -4.21895 -4.236434 -4.2194819 -4.1865973 -4.163723 -4.1607642][-4.2261252 -4.1996937 -4.1794167 -4.1744027 -4.1799631 -4.1812239 -4.1673675 -4.1608391 -4.1753726 -4.2060776 -4.2325773 -4.22866 -4.202167 -4.172442 -4.1567469][-4.2176208 -4.2031455 -4.1937923 -4.195816 -4.1964488 -4.1856446 -4.1670418 -4.1575718 -4.1653485 -4.1935191 -4.2275033 -4.2372279 -4.2162237 -4.1791458 -4.1476278][-4.2096181 -4.2085309 -4.2085977 -4.214294 -4.2119751 -4.1930318 -4.1666317 -4.1484637 -4.1510787 -4.181447 -4.2221537 -4.2419453 -4.2286568 -4.1911883 -4.1508508][-4.2028565 -4.2060566 -4.2108769 -4.219739 -4.2196584 -4.1939631 -4.1512136 -4.1223955 -4.1287904 -4.1672888 -4.2104506 -4.2356505 -4.2312546 -4.2020817 -4.1675148][-4.1986952 -4.1962757 -4.1965384 -4.2075 -4.2080245 -4.1697612 -4.10279 -4.0666294 -4.0946889 -4.1481733 -4.1929 -4.2225943 -4.22822 -4.2131305 -4.193398][-4.1941943 -4.18094 -4.1748843 -4.1846948 -4.1818051 -4.1216145 -4.0194488 -3.9784703 -4.0470695 -4.1285653 -4.1811171 -4.2127657 -4.2237997 -4.2211905 -4.2130003][-4.189467 -4.1740904 -4.1662889 -4.174407 -4.1630125 -4.0865088 -3.9491246 -3.8883235 -3.9954748 -4.1106529 -4.1766496 -4.2108107 -4.22283 -4.2263174 -4.2239332][-4.1793275 -4.1731019 -4.1693568 -4.179841 -4.1700268 -4.0997815 -3.9707775 -3.8989279 -3.9922996 -4.1084785 -4.1771207 -4.2083883 -4.2161989 -4.2209473 -4.2229295][-4.1705875 -4.1713977 -4.1699204 -4.1788597 -4.1787643 -4.1353116 -4.0496483 -3.9950738 -4.0446782 -4.1271396 -4.1835847 -4.2092714 -4.2079725 -4.2079558 -4.2105842][-4.168437 -4.1721821 -4.1679287 -4.1690984 -4.1762152 -4.1608748 -4.1146708 -4.0840693 -4.1044483 -4.1498652 -4.1930962 -4.2119365 -4.2024508 -4.1934071 -4.1916709][-4.1749649 -4.1776924 -4.1679859 -4.1594038 -4.165801 -4.1669822 -4.1451488 -4.1309748 -4.1370726 -4.158062 -4.1916232 -4.21071 -4.2015681 -4.1862535 -4.1748443][-4.1927996 -4.1881757 -4.17235 -4.1552253 -4.1583562 -4.1661596 -4.1563683 -4.1481485 -4.1508093 -4.1608076 -4.1861768 -4.206429 -4.2036505 -4.1898274 -4.1700034][-4.2159543 -4.2065849 -4.1889324 -4.1669149 -4.161376 -4.1673379 -4.1603174 -4.14829 -4.1521788 -4.1617146 -4.1829424 -4.2033305 -4.2061315 -4.1995707 -4.1814342]]...]
INFO - root - 2017-12-05 18:18:22.996981: step 32310, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 73h:20m:43s remains)
INFO - root - 2017-12-05 18:18:31.706359: step 32320, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 72h:58m:47s remains)
INFO - root - 2017-12-05 18:18:40.185643: step 32330, loss = 2.09, batch loss = 2.03 (10.1 examples/sec; 0.796 sec/batch; 66h:21m:07s remains)
INFO - root - 2017-12-05 18:18:48.718646: step 32340, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 71h:21m:37s remains)
INFO - root - 2017-12-05 18:18:57.340104: step 32350, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 71h:06m:24s remains)
INFO - root - 2017-12-05 18:19:05.827306: step 32360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 71h:47m:11s remains)
INFO - root - 2017-12-05 18:19:14.239750: step 32370, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 72h:14m:16s remains)
INFO - root - 2017-12-05 18:19:22.738890: step 32380, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 72h:39m:35s remains)
INFO - root - 2017-12-05 18:19:31.310384: step 32390, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 74h:10m:52s remains)
INFO - root - 2017-12-05 18:19:40.003344: step 32400, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 72h:59m:45s remains)
2017-12-05 18:19:40.792777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1850181 -4.1902452 -4.1882157 -4.1938367 -4.2102289 -4.2214952 -4.2269077 -4.2406273 -4.2596197 -4.2775016 -4.2889056 -4.2985044 -4.3039117 -4.3065596 -4.3111634][-4.1284957 -4.1257844 -4.11312 -4.1136065 -4.132771 -4.1458635 -4.1542187 -4.1760206 -4.2051349 -4.2328682 -4.253511 -4.2716293 -4.2831326 -4.2889848 -4.2969055][-4.1081944 -4.0986719 -4.079443 -4.0728226 -4.08259 -4.0887728 -4.09729 -4.1231546 -4.1513438 -4.1795087 -4.2087836 -4.2352676 -4.2527618 -4.2636805 -4.2762194][-4.1260681 -4.1139727 -4.0957785 -4.0828266 -4.073133 -4.0658531 -4.0702181 -4.0887713 -4.1066456 -4.127717 -4.1624746 -4.19665 -4.2209487 -4.2329264 -4.2494073][-4.150054 -4.1377048 -4.125792 -4.1105485 -4.0813966 -4.0549264 -4.0388656 -4.0419559 -4.0484734 -4.0652175 -4.10693 -4.1515355 -4.1835761 -4.1978 -4.2186995][-4.1650453 -4.1544623 -4.1483617 -4.132308 -4.0957427 -4.05167 -4.0052452 -3.9752274 -3.9567537 -3.9831381 -4.0504522 -4.1104794 -4.1481581 -4.1668849 -4.1922927][-4.1699495 -4.1603093 -4.156827 -4.1447029 -4.1131444 -4.0625114 -3.9870126 -3.9038169 -3.8456988 -3.90522 -4.0126047 -4.0846767 -4.1278939 -4.1486731 -4.1721959][-4.1598444 -4.15638 -4.1605778 -4.1564932 -4.1310172 -4.0807238 -3.9916396 -3.8780103 -3.8142586 -3.8993156 -4.0125017 -4.0827112 -4.1225057 -4.1403561 -4.16156][-4.159761 -4.16102 -4.1748524 -4.1822028 -4.1637959 -4.1183095 -4.0442896 -3.9643834 -3.9406528 -3.9951892 -4.057569 -4.1038504 -4.1323266 -4.1453934 -4.1649566][-4.177393 -4.1841044 -4.2034583 -4.218431 -4.2028379 -4.1660075 -4.1183758 -4.082634 -4.0761766 -4.0937681 -4.1141672 -4.1319809 -4.1486506 -4.1583147 -4.1787729][-4.2049489 -4.2156377 -4.2376394 -4.2566338 -4.2449212 -4.2144008 -4.1822419 -4.1661487 -4.1618834 -4.161191 -4.16374 -4.1636896 -4.1670685 -4.1710248 -4.1894889][-4.2466612 -4.2536325 -4.2705631 -4.289341 -4.2846565 -4.2603784 -4.2339773 -4.2207456 -4.2142305 -4.2071052 -4.2069511 -4.1973848 -4.1858635 -4.1823068 -4.201767][-4.2901721 -4.2888861 -4.29686 -4.3122382 -4.3139019 -4.2984467 -4.2807865 -4.2694979 -4.2618318 -4.2564144 -4.2566619 -4.2444096 -4.2216506 -4.2098341 -4.2297673][-4.3115988 -4.3064032 -4.3093014 -4.3194394 -4.3220725 -4.3156252 -4.3070879 -4.3033419 -4.3023491 -4.2998219 -4.2953815 -4.2813406 -4.2562237 -4.2455812 -4.2625322][-4.3181 -4.3142524 -4.3180108 -4.3248177 -4.3239331 -4.3227439 -4.3206434 -4.3220296 -4.3224168 -4.3179836 -4.3087635 -4.2923746 -4.2736683 -4.2703409 -4.282577]]...]
INFO - root - 2017-12-05 18:19:49.431467: step 32410, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 72h:53m:57s remains)
INFO - root - 2017-12-05 18:19:57.980468: step 32420, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 72h:49m:29s remains)
INFO - root - 2017-12-05 18:20:06.690157: step 32430, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 70h:35m:28s remains)
INFO - root - 2017-12-05 18:20:15.151286: step 32440, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 71h:49m:51s remains)
INFO - root - 2017-12-05 18:20:23.778201: step 32450, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 72h:52m:50s remains)
INFO - root - 2017-12-05 18:20:32.330569: step 32460, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 69h:55m:21s remains)
INFO - root - 2017-12-05 18:20:40.746138: step 32470, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 70h:25m:26s remains)
INFO - root - 2017-12-05 18:20:49.168742: step 32480, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.774 sec/batch; 64h:31m:38s remains)
INFO - root - 2017-12-05 18:20:57.870828: step 32490, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 72h:38m:11s remains)
INFO - root - 2017-12-05 18:21:06.449273: step 32500, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 70h:59m:41s remains)
2017-12-05 18:21:07.200250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2904773 -4.2399416 -4.1567283 -4.0715141 -4.0036516 -3.9790049 -4.0059056 -4.0307817 -4.0394 -4.036181 -4.0282106 -4.0053892 -4.0020204 -4.0261707 -4.0750246][-4.2993455 -4.2566209 -4.1836772 -4.1100197 -4.0503349 -4.0241308 -4.041533 -4.055233 -4.0540638 -4.0444407 -4.0345373 -4.0151176 -4.012012 -4.03063 -4.0752115][-4.3069115 -4.2713356 -4.2101545 -4.1520243 -4.1088715 -4.0891104 -4.0974822 -4.1065555 -4.0986867 -4.0748177 -4.0511842 -4.0335269 -4.0312643 -4.0425539 -4.0750546][-4.312079 -4.2802849 -4.2260671 -4.1776066 -4.1422625 -4.1229086 -4.1213851 -4.1303463 -4.1263261 -4.1024513 -4.0796533 -4.0720949 -4.0688872 -4.0691156 -4.0833611][-4.314796 -4.2819028 -4.2275004 -4.1785278 -4.1382623 -4.1111145 -4.103622 -4.1100678 -4.1116719 -4.0969892 -4.0835657 -4.0848794 -4.0806022 -4.0725656 -4.0758228][-4.3140521 -4.276432 -4.2149868 -4.1537323 -4.0998755 -4.0645795 -4.05131 -4.0539265 -4.0630484 -4.0696564 -4.0784235 -4.0925059 -4.0869141 -4.0766644 -4.0838728][-4.3084488 -4.2637496 -4.1889558 -4.1030879 -4.0212274 -3.9735334 -3.9590771 -3.9671793 -3.9905858 -4.0186596 -4.0508666 -4.0753431 -4.0701094 -4.0625992 -4.073493][-4.3000917 -4.2461896 -4.15049 -4.0302238 -3.9121761 -3.8550262 -3.8471067 -3.8670237 -3.9027967 -3.9360337 -3.9740486 -4.0029883 -4.0076065 -4.0097613 -4.0260568][-4.2948885 -4.2353811 -4.1312518 -3.9988625 -3.875124 -3.8263915 -3.83236 -3.8653917 -3.9114575 -3.9467156 -3.975137 -3.9954851 -3.9972157 -3.9979072 -4.0094104][-4.2990746 -4.2459884 -4.1538782 -4.0503058 -3.9666 -3.9362898 -3.9437852 -3.9735417 -4.0170794 -4.0475669 -4.0627432 -4.062469 -4.0518913 -4.0485854 -4.051332][-4.303616 -4.2578583 -4.1787906 -4.0964622 -4.0406804 -4.0235095 -4.031888 -4.0623231 -4.1009092 -4.1218157 -4.1253123 -4.1191444 -4.1070571 -4.0970526 -4.0895457][-4.3047171 -4.2666965 -4.197681 -4.1225805 -4.0748792 -4.0533094 -4.0555825 -4.0852237 -4.1152959 -4.1231966 -4.120079 -4.11874 -4.1100316 -4.0943813 -4.0837007][-4.3022408 -4.2669835 -4.1968608 -4.1198916 -4.0679479 -4.0343237 -4.0241694 -4.0492582 -4.0732079 -4.0770669 -4.0750971 -4.0777655 -4.075285 -4.0638061 -4.0591][-4.2928934 -4.2508593 -4.1708307 -4.0896664 -4.0341711 -3.99597 -3.9808102 -4.0026588 -4.0236974 -4.0322394 -4.0376859 -4.0442953 -4.0484562 -4.0469522 -4.052423][-4.2876363 -4.2391777 -4.1535544 -4.0718546 -4.0118704 -3.9671345 -3.9469836 -3.9651065 -3.99 -4.0048828 -4.0105071 -4.0178418 -4.0219831 -4.0264821 -4.0398974]]...]
INFO - root - 2017-12-05 18:21:15.727665: step 32510, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 70h:55m:01s remains)
INFO - root - 2017-12-05 18:21:24.319449: step 32520, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:29m:37s remains)
INFO - root - 2017-12-05 18:21:32.994212: step 32530, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 73h:45m:19s remains)
INFO - root - 2017-12-05 18:21:41.592987: step 32540, loss = 2.03, batch loss = 1.98 (9.9 examples/sec; 0.811 sec/batch; 67h:34m:18s remains)
INFO - root - 2017-12-05 18:21:50.210960: step 32550, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 73h:13m:17s remains)
INFO - root - 2017-12-05 18:21:58.776673: step 32560, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 72h:04m:59s remains)
INFO - root - 2017-12-05 18:22:07.290010: step 32570, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 70h:38m:39s remains)
INFO - root - 2017-12-05 18:22:15.816060: step 32580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 71h:55m:21s remains)
INFO - root - 2017-12-05 18:22:24.173458: step 32590, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 72h:25m:39s remains)
INFO - root - 2017-12-05 18:22:32.738926: step 32600, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 72h:43m:26s remains)
2017-12-05 18:22:33.526318: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1369014 -4.1457968 -4.1538906 -4.1738963 -4.1945982 -4.2073822 -4.1948519 -4.1730008 -4.1830115 -4.2026272 -4.2178144 -4.228065 -4.2335973 -4.2463365 -4.2664394][-4.1443515 -4.1532316 -4.1590657 -4.1773782 -4.1930652 -4.1984015 -4.1780672 -4.1525283 -4.16277 -4.1838384 -4.2003021 -4.2144055 -4.2225842 -4.2324605 -4.2587066][-4.1363192 -4.1450009 -4.1552048 -4.1790695 -4.1950884 -4.1976228 -4.1758204 -4.1484971 -4.1508675 -4.1626225 -4.1753654 -4.1910286 -4.2018523 -4.2135262 -4.240643][-4.1219912 -4.124404 -4.135736 -4.1634026 -4.1844182 -4.191371 -4.1779509 -4.1580367 -4.1459961 -4.1438751 -4.1536155 -4.1716843 -4.1849194 -4.2024446 -4.2306509][-4.1056108 -4.1004262 -4.1112242 -4.1421075 -4.1695304 -4.1793089 -4.172328 -4.1562977 -4.1308279 -4.1122108 -4.1229353 -4.1522269 -4.1735158 -4.2049971 -4.2365451][-4.0909324 -4.0790143 -4.0885277 -4.1188488 -4.1510582 -4.1609583 -4.1536379 -4.1381626 -4.1101689 -4.0884385 -4.1050177 -4.1456666 -4.1797976 -4.2164574 -4.243886][-4.0923595 -4.0720606 -4.0740747 -4.0929108 -4.1189442 -4.1256752 -4.1163278 -4.1060171 -4.0883732 -4.0790849 -4.1022015 -4.14108 -4.1742015 -4.2088485 -4.2343359][-4.1144352 -4.0848989 -4.0727563 -4.0732803 -4.0811176 -4.0719824 -4.0562439 -4.0529456 -4.0537715 -4.0643358 -4.092154 -4.1254654 -4.1548543 -4.1898556 -4.215683][-4.1315575 -4.0989151 -4.079196 -4.0661497 -4.053648 -4.0285268 -4.0087008 -4.0147791 -4.0335846 -4.0579352 -4.0911851 -4.1236258 -4.1494102 -4.1793385 -4.2016845][-4.1243215 -4.0964928 -4.0771437 -4.05935 -4.0352983 -4.0055962 -3.9886568 -4.0091491 -4.0463166 -4.0791092 -4.1165128 -4.143887 -4.1595278 -4.1752677 -4.189218][-4.1294603 -4.1151323 -4.1032734 -4.084734 -4.0576506 -4.0325375 -4.0217838 -4.044764 -4.0864143 -4.1199961 -4.1506071 -4.1667361 -4.1691604 -4.170043 -4.17604][-4.1651664 -4.1611233 -4.156724 -4.1401119 -4.117167 -4.1019778 -4.0939469 -4.1051517 -4.1352081 -4.1625156 -4.1838708 -4.1895247 -4.1828723 -4.1719437 -4.1664095][-4.1978951 -4.1956449 -4.1921654 -4.1784549 -4.1657495 -4.1627927 -4.1572185 -4.1565967 -4.169652 -4.1842189 -4.194119 -4.1952481 -4.1902509 -4.1764755 -4.1649556][-4.2077966 -4.2059722 -4.20299 -4.1940613 -4.1905937 -4.1978717 -4.1971889 -4.1894193 -4.1858077 -4.1846719 -4.1839452 -4.1850085 -4.1847382 -4.1781254 -4.1704531][-4.2022524 -4.2023396 -4.2044277 -4.2034569 -4.2040906 -4.2134824 -4.215457 -4.2099252 -4.2001505 -4.1865592 -4.1818371 -4.18153 -4.1803012 -4.1789207 -4.176796]]...]
INFO - root - 2017-12-05 18:22:42.148581: step 32610, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 71h:32m:21s remains)
INFO - root - 2017-12-05 18:22:50.701835: step 32620, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 72h:59m:30s remains)
INFO - root - 2017-12-05 18:22:59.287762: step 32630, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 68h:23m:11s remains)
INFO - root - 2017-12-05 18:23:07.720600: step 32640, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 70h:18m:39s remains)
INFO - root - 2017-12-05 18:23:16.199898: step 32650, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 70h:08m:47s remains)
INFO - root - 2017-12-05 18:23:24.810870: step 32660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 71h:45m:12s remains)
INFO - root - 2017-12-05 18:23:33.289642: step 32670, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 72h:01m:38s remains)
INFO - root - 2017-12-05 18:23:41.929877: step 32680, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 70h:54m:27s remains)
INFO - root - 2017-12-05 18:23:50.520232: step 32690, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 71h:48m:19s remains)
INFO - root - 2017-12-05 18:23:58.768112: step 32700, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 70h:43m:50s remains)
2017-12-05 18:23:59.615582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2162752 -4.217351 -4.2213798 -4.2083116 -4.1845884 -4.1685729 -4.1708717 -4.1812305 -4.183691 -4.1860595 -4.1906366 -4.1893935 -4.1793122 -4.1763868 -4.1841993][-4.1987305 -4.199213 -4.2035513 -4.1947269 -4.1736832 -4.1530981 -4.1456738 -4.1466489 -4.1415105 -4.1402421 -4.1468587 -4.1498957 -4.1431928 -4.1389279 -4.1502991][-4.2061133 -4.2026081 -4.202456 -4.19304 -4.1719117 -4.14457 -4.1207604 -4.1100922 -4.0985374 -4.0972328 -4.1111422 -4.1187396 -4.1147161 -4.1097732 -4.1195097][-4.2324638 -4.2212114 -4.2101445 -4.1886668 -4.1559305 -4.1180239 -4.0850139 -4.0687723 -4.0623927 -4.0688105 -4.0878906 -4.0989985 -4.1011715 -4.1006317 -4.1137066][-4.2577243 -4.241003 -4.2159133 -4.1714811 -4.1106625 -4.0480108 -4.0106263 -4.0098267 -4.029397 -4.0522938 -4.0729504 -4.0886021 -4.1049342 -4.1128626 -4.1328197][-4.2741036 -4.2545662 -4.2159181 -4.1443963 -4.0481777 -3.9468067 -3.9014287 -3.9352961 -3.9946134 -4.0399475 -4.071671 -4.1019297 -4.1331606 -4.1509337 -4.1726966][-4.2764406 -4.2566457 -4.2109737 -4.1223478 -4.0000782 -3.8596289 -3.7942 -3.8660352 -3.96887 -4.0399203 -4.0936995 -4.1408677 -4.181138 -4.1989536 -4.21275][-4.2704473 -4.2505217 -4.2040458 -4.1136045 -3.9904451 -3.8452747 -3.7768111 -3.8688998 -3.9868348 -4.0621943 -4.1258903 -4.1798058 -4.21825 -4.2281237 -4.2330732][-4.26502 -4.2412734 -4.1961708 -4.1163449 -4.0105963 -3.8919466 -3.8461077 -3.9342878 -4.0410118 -4.1098208 -4.1741304 -4.2261138 -4.2545886 -4.2549276 -4.2489944][-4.2630763 -4.2357793 -4.1924763 -4.1217813 -4.0382962 -3.9582376 -3.9375422 -4.0129566 -4.0999742 -4.1594663 -4.2172971 -4.2574053 -4.2749658 -4.2712078 -4.2611713][-4.2631869 -4.2350011 -4.19293 -4.1270614 -4.0597534 -4.0078087 -4.0035958 -4.0720453 -4.1463146 -4.2016726 -4.2485371 -4.2740259 -4.2817483 -4.2762642 -4.2681093][-4.2637057 -4.2368479 -4.19652 -4.1336861 -4.0787144 -4.0449257 -4.0528536 -4.1150851 -4.1790857 -4.2279 -4.263588 -4.279355 -4.2843046 -4.2798076 -4.2727695][-4.2641797 -4.2401438 -4.2032137 -4.1451325 -4.0987425 -4.0783162 -4.093791 -4.150147 -4.2010655 -4.2390866 -4.2643251 -4.275207 -4.2831731 -4.2830505 -4.2770782][-4.2662168 -4.2449064 -4.2135363 -4.1618094 -4.1238546 -4.1157713 -4.1339111 -4.1817575 -4.2165837 -4.2376041 -4.249805 -4.2585483 -4.2733684 -4.2799511 -4.2765617][-4.2689567 -4.2499218 -4.2236204 -4.1773319 -4.1475644 -4.1482038 -4.16544 -4.2002335 -4.2159214 -4.2167077 -4.2165833 -4.2275705 -4.2531967 -4.2699938 -4.2730093]]...]
INFO - root - 2017-12-05 18:24:08.098563: step 32710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 74h:27m:24s remains)
INFO - root - 2017-12-05 18:24:16.687439: step 32720, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 68h:56m:54s remains)
INFO - root - 2017-12-05 18:24:25.350463: step 32730, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 73h:52m:58s remains)
INFO - root - 2017-12-05 18:24:33.879081: step 32740, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 69h:47m:26s remains)
INFO - root - 2017-12-05 18:24:42.524098: step 32750, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 70h:17m:28s remains)
INFO - root - 2017-12-05 18:24:51.069960: step 32760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 71h:56m:25s remains)
INFO - root - 2017-12-05 18:24:59.520482: step 32770, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 69h:14m:19s remains)
INFO - root - 2017-12-05 18:25:08.080534: step 32780, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 68h:23m:53s remains)
INFO - root - 2017-12-05 18:25:16.851603: step 32790, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 73h:05m:31s remains)
INFO - root - 2017-12-05 18:25:25.489468: step 32800, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 69h:20m:45s remains)
2017-12-05 18:25:26.273137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2408085 -4.2503757 -4.2458549 -4.2317505 -4.2164879 -4.210773 -4.2192225 -4.235971 -4.2478595 -4.2571306 -4.2674975 -4.2751875 -4.2829452 -4.2881813 -4.282835][-4.2172256 -4.2228465 -4.2123742 -4.1823769 -4.1506 -4.1418328 -4.1573486 -4.1887207 -4.2209845 -4.2485385 -4.268785 -4.2824225 -4.2935762 -4.3040957 -4.3042741][-4.1777124 -4.1788316 -4.1648655 -4.123939 -4.0747209 -4.0613308 -4.084425 -4.1340752 -4.1898389 -4.2312903 -4.2518916 -4.2628317 -4.2761703 -4.293859 -4.3005853][-4.1327767 -4.13109 -4.1187353 -4.0742412 -4.0127535 -3.9937587 -4.0216646 -4.0843644 -4.155333 -4.200758 -4.2162642 -4.2268376 -4.2467928 -4.2711372 -4.2894411][-4.0979466 -4.0944171 -4.08957 -4.0604649 -4.0038857 -3.9835677 -4.0129123 -4.0745649 -4.138885 -4.16934 -4.171463 -4.1782751 -4.2072849 -4.2392774 -4.26799][-4.0967536 -4.0935287 -4.0988913 -4.0891523 -4.0445008 -4.0181122 -4.0252991 -4.0612707 -4.0987649 -4.1123033 -4.1061993 -4.112761 -4.149138 -4.19262 -4.2314491][-4.0994344 -4.0961752 -4.1050091 -4.103282 -4.0596843 -4.0150037 -3.9860809 -3.9814444 -3.9908404 -4.0010753 -4.0041618 -4.0224328 -4.0692129 -4.129447 -4.1855235][-4.0921574 -4.086257 -4.0892715 -4.0781569 -4.0225558 -3.9471557 -3.8792777 -3.8371384 -3.8318262 -3.8680825 -3.9076772 -3.9483185 -4.0006895 -4.0696926 -4.1402035][-4.1027317 -4.093461 -4.0821166 -4.0616708 -4.0012059 -3.9098155 -3.8246183 -3.7718797 -3.7711267 -3.8305774 -3.9003644 -3.9492226 -3.9927504 -4.0531216 -4.1254339][-4.139647 -4.1313553 -4.1154571 -4.0968981 -4.0513387 -3.9750907 -3.9004476 -3.8624308 -3.8746769 -3.9281855 -3.9871554 -4.0245876 -4.0528464 -4.0951633 -4.152802][-4.194448 -4.1890612 -4.1785979 -4.1650105 -4.1359134 -4.0851407 -4.0296435 -4.0036688 -4.0162854 -4.0509686 -4.0868988 -4.1099143 -4.1302252 -4.1580176 -4.1958508][-4.2426848 -4.2402954 -4.237781 -4.2332292 -4.2210455 -4.191071 -4.1548643 -4.1392355 -4.1464229 -4.162436 -4.178813 -4.1919804 -4.2053528 -4.2200136 -4.2397556][-4.2659893 -4.2713122 -4.2781534 -4.2800703 -4.2762232 -4.2615404 -4.2416368 -4.2378078 -4.2450995 -4.2500319 -4.2543635 -4.2588677 -4.264852 -4.2681327 -4.2736306][-4.2754607 -4.289187 -4.3027158 -4.3080626 -4.3052645 -4.2972546 -4.2886057 -4.2909889 -4.3020043 -4.3058667 -4.301836 -4.2994537 -4.3000326 -4.298768 -4.2962055][-4.2791982 -4.2926106 -4.3066783 -4.3131967 -4.3128257 -4.3107257 -4.3084121 -4.3106594 -4.3187079 -4.3204379 -4.3137546 -4.3091006 -4.30814 -4.3059883 -4.3019419]]...]
INFO - root - 2017-12-05 18:25:34.877755: step 32810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 71h:39m:48s remains)
INFO - root - 2017-12-05 18:25:43.455682: step 32820, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 71h:07m:44s remains)
INFO - root - 2017-12-05 18:25:52.036190: step 32830, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 72h:06m:05s remains)
INFO - root - 2017-12-05 18:26:00.579496: step 32840, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.821 sec/batch; 68h:21m:17s remains)
INFO - root - 2017-12-05 18:26:09.154259: step 32850, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:19m:47s remains)
INFO - root - 2017-12-05 18:26:17.777277: step 32860, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 71h:22m:32s remains)
INFO - root - 2017-12-05 18:26:26.249171: step 32870, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 69h:46m:32s remains)
INFO - root - 2017-12-05 18:26:34.773159: step 32880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 71h:25m:45s remains)
INFO - root - 2017-12-05 18:26:43.484446: step 32890, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 73h:32m:39s remains)
INFO - root - 2017-12-05 18:26:52.063663: step 32900, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 70h:16m:32s remains)
2017-12-05 18:26:52.822161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2685919 -4.2676792 -4.2671452 -4.2652607 -4.2646894 -4.2633657 -4.2575917 -4.2447152 -4.230876 -4.2217293 -4.2181935 -4.22766 -4.2464757 -4.2597122 -4.2589703][-4.2713957 -4.2709036 -4.2719173 -4.2729573 -4.2748876 -4.274116 -4.267076 -4.2523866 -4.237762 -4.2279506 -4.2225747 -4.2289329 -4.242362 -4.2501793 -4.2434835][-4.2680306 -4.2675467 -4.2688985 -4.2710905 -4.2720685 -4.2661996 -4.2527351 -4.2328548 -4.2159438 -4.2050648 -4.1993337 -4.2057605 -4.2199616 -4.2270069 -4.2138071][-4.2506356 -4.2489133 -4.2514434 -4.2538171 -4.2499633 -4.233366 -4.2064228 -4.1764488 -4.1571083 -4.1476212 -4.1460066 -4.1592016 -4.1828222 -4.1950378 -4.1781149][-4.2161779 -4.2127838 -4.2178588 -4.2217126 -4.2108884 -4.1772528 -4.1281347 -4.0840607 -4.0656176 -4.0659604 -4.07634 -4.1003532 -4.1339831 -4.1527376 -4.1388516][-4.1665659 -4.1550741 -4.1588764 -4.1641769 -4.1465592 -4.0922012 -4.0156975 -3.9558585 -3.9483409 -3.9755244 -4.0126262 -4.0517249 -4.0908251 -4.113863 -4.1101418][-4.1001391 -4.0800924 -4.081923 -4.0878506 -4.0620217 -3.988507 -3.8882012 -3.821146 -3.8396697 -3.9071312 -3.9797111 -4.0363746 -4.0789776 -4.1061034 -4.1131048][-4.0552154 -4.0333295 -4.03584 -4.0394487 -4.009347 -3.9349365 -3.8443234 -3.8017313 -3.8481913 -3.9353178 -4.0174217 -4.0732737 -4.1099253 -4.1333976 -4.1418347][-4.0649052 -4.0566058 -4.0596547 -4.05643 -4.0280266 -3.9729242 -3.9172339 -3.9053831 -3.9524539 -4.0202947 -4.0824265 -4.123405 -4.1517792 -4.169239 -4.1752663][-4.0963335 -4.10511 -4.1081243 -4.0987444 -4.0728841 -4.0389338 -4.01106 -4.0081692 -4.0339618 -4.0737915 -4.1154294 -4.1494241 -4.1777663 -4.1956296 -4.201303][-4.1163931 -4.133307 -4.1307192 -4.1154385 -4.0938668 -4.072721 -4.0582633 -4.0538073 -4.0575948 -4.0739794 -4.101161 -4.1331782 -4.1696615 -4.195457 -4.2083778][-4.1268544 -4.1401463 -4.128438 -4.109695 -4.0930085 -4.0785446 -4.0695033 -4.0663438 -4.0586166 -4.0595322 -4.0762048 -4.1074586 -4.1526847 -4.1866517 -4.2077923][-4.1447616 -4.1525297 -4.1366811 -4.118856 -4.10804 -4.1005855 -4.0912342 -4.0833907 -4.0682659 -4.0617418 -4.0691924 -4.0949697 -4.1407223 -4.17934 -4.2069345][-4.1651039 -4.1711636 -4.1604495 -4.1499877 -4.1461272 -4.1457753 -4.1343412 -4.118454 -4.0962219 -4.0838628 -4.0814695 -4.0953965 -4.1314745 -4.1680737 -4.2002196][-4.1808109 -4.1888485 -4.1844897 -4.1800036 -4.1810436 -4.1835232 -4.1706595 -4.1470356 -4.1185217 -4.1003156 -4.0919886 -4.0971217 -4.1208634 -4.1540823 -4.1902847]]...]
INFO - root - 2017-12-05 18:27:01.313788: step 32910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:22m:08s remains)
INFO - root - 2017-12-05 18:27:09.888518: step 32920, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 72h:13m:24s remains)
INFO - root - 2017-12-05 18:27:18.413843: step 32930, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 71h:52m:18s remains)
INFO - root - 2017-12-05 18:27:26.946270: step 32940, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 72h:05m:39s remains)
INFO - root - 2017-12-05 18:27:35.596926: step 32950, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:17m:49s remains)
INFO - root - 2017-12-05 18:27:44.282394: step 32960, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 70h:50m:56s remains)
INFO - root - 2017-12-05 18:27:52.725454: step 32970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 71h:46m:02s remains)
INFO - root - 2017-12-05 18:28:01.295981: step 32980, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 70h:33m:29s remains)
INFO - root - 2017-12-05 18:28:09.841482: step 32990, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 71h:11m:34s remains)
INFO - root - 2017-12-05 18:28:18.353149: step 33000, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 73h:31m:09s remains)
2017-12-05 18:28:19.123605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043991 -4.2032008 -4.1898122 -4.1719589 -4.1589556 -4.1517711 -4.1477947 -4.1444416 -4.1415992 -4.1432343 -4.1625323 -4.1893773 -4.1943903 -4.1778278 -4.1673369][-4.1778588 -4.1803184 -4.171103 -4.1560745 -4.1473789 -4.1509123 -4.161633 -4.1707458 -4.1774116 -4.1841707 -4.2036924 -4.2261491 -4.2220912 -4.1994257 -4.1837654][-4.14891 -4.1486659 -4.1423817 -4.1334324 -4.1340504 -4.1508832 -4.177042 -4.1972766 -4.2090979 -4.2140403 -4.226687 -4.2422905 -4.2330942 -4.2076187 -4.1889658][-4.1273422 -4.1220217 -4.1178627 -4.1146741 -4.1197734 -4.1396575 -4.1680326 -4.1921577 -4.2025795 -4.2014012 -4.205195 -4.2148461 -4.209301 -4.188035 -4.1739111][-4.11294 -4.1028461 -4.0971746 -4.0967789 -4.1024442 -4.1172981 -4.1360755 -4.1506639 -4.1513281 -4.141685 -4.14078 -4.1534753 -4.1589127 -4.1477122 -4.1450634][-4.094223 -4.0798187 -4.0716276 -4.0736728 -4.0785379 -4.082314 -4.0877557 -4.0969 -4.0980916 -4.087183 -4.0874624 -4.1065874 -4.1201124 -4.1180625 -4.1261053][-4.0842638 -4.063961 -4.0536485 -4.05815 -4.0622478 -4.05589 -4.0510292 -4.0590644 -4.0732865 -4.0738974 -4.0830235 -4.1057534 -4.1190348 -4.1148915 -4.12657][-4.0966115 -4.0641594 -4.0459414 -4.0485988 -4.053793 -4.04684 -4.0415196 -4.0555992 -4.0833259 -4.1023755 -4.1219616 -4.1411614 -4.1488013 -4.1369605 -4.1384978][-4.12595 -4.0825114 -4.0546474 -4.0542064 -4.062448 -4.0618367 -4.0610108 -4.0791717 -4.1119924 -4.1377287 -4.1571488 -4.1714654 -4.1755538 -4.1608586 -4.1521859][-4.1309862 -4.0919676 -4.0652747 -4.068656 -4.0855765 -4.0969849 -4.098979 -4.1118164 -4.1390629 -4.1619997 -4.1739073 -4.1804638 -4.1820655 -4.171299 -4.1595249][-4.1188965 -4.0918388 -4.07682 -4.0851741 -4.1047482 -4.1214528 -4.1258779 -4.1302361 -4.1460843 -4.16117 -4.1675224 -4.164607 -4.1616383 -4.1581211 -4.1544023][-4.1180539 -4.105834 -4.1032743 -4.1112423 -4.1245041 -4.1350164 -4.1379595 -4.1405969 -4.1493783 -4.155323 -4.1548681 -4.1462736 -4.1402206 -4.1434412 -4.1505175][-4.1290421 -4.1347623 -4.1387272 -4.1443629 -4.1511288 -4.1531162 -4.15246 -4.1556525 -4.1591859 -4.1587257 -4.1549239 -4.1439118 -4.1343775 -4.1389828 -4.1501474][-4.14255 -4.157804 -4.16496 -4.1708112 -4.1740685 -4.1724982 -4.1737442 -4.176899 -4.174922 -4.1670561 -4.1570721 -4.1405435 -4.1267753 -4.1294141 -4.140038][-4.1497374 -4.1690869 -4.171474 -4.1700182 -4.169364 -4.1712117 -4.1785955 -4.182991 -4.1766462 -4.1623287 -4.1463394 -4.1281648 -4.1180515 -4.1208878 -4.1289086]]...]
INFO - root - 2017-12-05 18:28:27.535718: step 33010, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 71h:58m:19s remains)
INFO - root - 2017-12-05 18:28:36.084522: step 33020, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 68h:28m:38s remains)
INFO - root - 2017-12-05 18:28:44.529008: step 33030, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 69h:16m:11s remains)
INFO - root - 2017-12-05 18:28:53.078917: step 33040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 71h:40m:48s remains)
INFO - root - 2017-12-05 18:29:01.621590: step 33050, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 69h:34m:30s remains)
INFO - root - 2017-12-05 18:29:10.107779: step 33060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 71h:29m:04s remains)
INFO - root - 2017-12-05 18:29:18.542067: step 33070, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 69h:01m:26s remains)
INFO - root - 2017-12-05 18:29:27.192112: step 33080, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 70h:29m:50s remains)
INFO - root - 2017-12-05 18:29:35.676158: step 33090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 71h:21m:48s remains)
INFO - root - 2017-12-05 18:29:44.260338: step 33100, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 71h:38m:43s remains)
2017-12-05 18:29:45.001751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2673936 -4.2433887 -4.2213912 -4.2169075 -4.2201896 -4.22256 -4.2214594 -4.212431 -4.1943994 -4.1788669 -4.1714478 -4.1814771 -4.212245 -4.2389326 -4.2420049][-4.2443409 -4.2151155 -4.1856942 -4.1799603 -4.1878715 -4.1963797 -4.197124 -4.1843715 -4.1633735 -4.1480403 -4.1453829 -4.1605115 -4.193481 -4.2164655 -4.211431][-4.215446 -4.1801047 -4.1466188 -4.1419048 -4.1589975 -4.1707883 -4.165668 -4.14607 -4.1211929 -4.111156 -4.12591 -4.1547589 -4.1836214 -4.1959743 -4.179533][-4.1952181 -4.1607647 -4.1298566 -4.1314807 -4.1568046 -4.1675577 -4.154439 -4.1217113 -4.0897641 -4.0868287 -4.1215959 -4.1643209 -4.1900835 -4.1936326 -4.1743641][-4.1930194 -4.160646 -4.1331062 -4.1364026 -4.1644368 -4.1697197 -4.1485519 -4.1028666 -4.0672269 -4.0753651 -4.1237674 -4.1666865 -4.1847363 -4.18265 -4.1679316][-4.208209 -4.1709971 -4.1404767 -4.1365695 -4.1622519 -4.165163 -4.1410913 -4.0946221 -4.0636325 -4.0796628 -4.1300082 -4.1652942 -4.1733484 -4.1677752 -4.1577578][-4.2194166 -4.177907 -4.1477547 -4.1431313 -4.1636381 -4.1649475 -4.1400061 -4.0969357 -4.0688205 -4.0858779 -4.1349378 -4.16586 -4.1693368 -4.166213 -4.1594615][-4.2182188 -4.1772504 -4.1515551 -4.1478958 -4.1671705 -4.1698985 -4.1426015 -4.09902 -4.068243 -4.0746331 -4.1159711 -4.147861 -4.15535 -4.1609564 -4.1628485][-4.2093558 -4.1727509 -4.151175 -4.1513205 -4.1717687 -4.1757317 -4.1497917 -4.1034079 -4.0663905 -4.0650625 -4.098618 -4.1276984 -4.144793 -4.1610274 -4.1706853][-4.1925731 -4.1544914 -4.1329222 -4.1419816 -4.1699624 -4.1787224 -4.15845 -4.1123857 -4.07545 -4.07196 -4.0955186 -4.1205411 -4.1426849 -4.163239 -4.1739564][-4.1793032 -4.1336918 -4.1049442 -4.1103611 -4.1413422 -4.1626 -4.149797 -4.1087766 -4.0812688 -4.0875282 -4.1106009 -4.131422 -4.1494794 -4.1680965 -4.1746454][-4.1815562 -4.122086 -4.07853 -4.0717645 -4.0909171 -4.1119046 -4.108129 -4.0842533 -4.0828953 -4.113977 -4.1498022 -4.1667037 -4.17026 -4.1731453 -4.1660905][-4.1986408 -4.1356692 -4.0866475 -4.0683446 -4.0630856 -4.0608149 -4.0587521 -4.062942 -4.0882711 -4.136899 -4.176898 -4.1878104 -4.1841707 -4.1842771 -4.1735673][-4.2214384 -4.17415 -4.1396084 -4.1206145 -4.0923886 -4.0626454 -4.0524426 -4.0734825 -4.114614 -4.1639271 -4.1954203 -4.2009692 -4.198288 -4.204349 -4.1967039][-4.2418432 -4.2105675 -4.18992 -4.1751113 -4.1416721 -4.1071372 -4.09992 -4.1243711 -4.1641712 -4.2008362 -4.2211161 -4.2210383 -4.213623 -4.2160373 -4.2073469]]...]
INFO - root - 2017-12-05 18:29:53.461550: step 33110, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 69h:50m:08s remains)
INFO - root - 2017-12-05 18:30:01.862029: step 33120, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.740 sec/batch; 61h:32m:55s remains)
INFO - root - 2017-12-05 18:30:10.317794: step 33130, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 72h:09m:42s remains)
INFO - root - 2017-12-05 18:30:18.662546: step 33140, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:55m:50s remains)
INFO - root - 2017-12-05 18:30:27.213655: step 33150, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 72h:35m:32s remains)
INFO - root - 2017-12-05 18:30:35.770790: step 33160, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 71h:08m:05s remains)
INFO - root - 2017-12-05 18:30:44.315599: step 33170, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 71h:29m:55s remains)
INFO - root - 2017-12-05 18:30:52.754610: step 33180, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 68h:26m:33s remains)
INFO - root - 2017-12-05 18:31:01.217900: step 33190, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:16m:50s remains)
INFO - root - 2017-12-05 18:31:09.798713: step 33200, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 70h:56m:37s remains)
2017-12-05 18:31:10.529001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2682376 -4.2656093 -4.256628 -4.238061 -4.2155976 -4.1913271 -4.1659122 -4.1424007 -4.1266642 -4.1339049 -4.16061 -4.192008 -4.2120156 -4.2075515 -4.2154145][-4.2671247 -4.2658119 -4.254765 -4.2292194 -4.1981416 -4.1594334 -4.1198034 -4.0887861 -4.076479 -4.0892234 -4.1224685 -4.1616735 -4.190887 -4.1918745 -4.2009058][-4.2621803 -4.2618251 -4.2471509 -4.2167692 -4.1801004 -4.1300244 -4.0769348 -4.0417218 -4.0383377 -4.0596733 -4.0978007 -4.1440053 -4.1821561 -4.1904635 -4.200613][-4.2516174 -4.2519069 -4.2322865 -4.1996636 -4.1569262 -4.0967817 -4.0380645 -4.0094366 -4.0206923 -4.0507355 -4.0874233 -4.1325321 -4.1777225 -4.1954951 -4.2077909][-4.2240362 -4.2287989 -4.2068586 -4.1770072 -4.135303 -4.0679607 -4.0053716 -3.9944048 -4.023489 -4.0569167 -4.0865417 -4.1261559 -4.1726561 -4.1960711 -4.2115688][-4.1861911 -4.1988397 -4.1816087 -4.155725 -4.1152053 -4.0360556 -3.9640653 -3.9667919 -4.0163779 -4.0573936 -4.0844822 -4.1167383 -4.1583486 -4.1843405 -4.2091684][-4.15114 -4.1655307 -4.1550107 -4.1314049 -4.085578 -3.9852474 -3.8819871 -3.883554 -3.9627361 -4.0314703 -4.073391 -4.1070642 -4.1441107 -4.1749158 -4.2117648][-4.1018925 -4.118237 -4.1141348 -4.090549 -4.0342116 -3.9057224 -3.7503872 -3.7437348 -3.8660405 -3.976794 -4.046443 -4.0923257 -4.132184 -4.1687613 -4.2184749][-4.0247774 -4.047627 -4.0567837 -4.0474367 -3.9974146 -3.8646023 -3.6927454 -3.6868389 -3.8345959 -3.9641218 -4.0467367 -4.0992746 -4.1366816 -4.1718287 -4.2238369][-4.0119948 -4.0397754 -4.0583773 -4.0601664 -4.0259862 -3.9333735 -3.8138618 -3.8129537 -3.9198902 -4.0115576 -4.0681157 -4.1064095 -4.1355195 -4.1666102 -4.2151647][-4.0738726 -4.0944633 -4.1018157 -4.0977025 -4.07724 -4.0293965 -3.9634786 -3.9621134 -4.0203967 -4.0713162 -4.1040797 -4.129705 -4.1526165 -4.17764 -4.2134962][-4.138413 -4.1489153 -4.1442957 -4.1358519 -4.1226668 -4.102262 -4.0728068 -4.0725579 -4.10479 -4.1310487 -4.1512632 -4.1667452 -4.1834083 -4.2038646 -4.226923][-4.1888895 -4.1928639 -4.1835351 -4.173594 -4.160892 -4.1500206 -4.1383348 -4.1421757 -4.1634374 -4.1813936 -4.19213 -4.196239 -4.2090454 -4.2326531 -4.2485466][-4.2191515 -4.2193136 -4.2128353 -4.207859 -4.1989555 -4.1887851 -4.182302 -4.1853037 -4.1967092 -4.2112246 -4.2185683 -4.2164893 -4.2284422 -4.25367 -4.2701521][-4.2446518 -4.2387915 -4.234725 -4.2340918 -4.2321968 -4.2262235 -4.2233758 -4.2252846 -4.2302294 -4.2376661 -4.2403822 -4.2380228 -4.2495809 -4.27363 -4.2907472]]...]
INFO - root - 2017-12-05 18:31:19.124542: step 33210, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.841 sec/batch; 69h:56m:15s remains)
INFO - root - 2017-12-05 18:31:27.611196: step 33220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:48m:16s remains)
INFO - root - 2017-12-05 18:31:36.012476: step 33230, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 71h:22m:49s remains)
INFO - root - 2017-12-05 18:31:44.633170: step 33240, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 72h:37m:07s remains)
INFO - root - 2017-12-05 18:31:53.063026: step 33250, loss = 2.06, batch loss = 2.00 (10.0 examples/sec; 0.801 sec/batch; 66h:35m:11s remains)
INFO - root - 2017-12-05 18:32:01.716078: step 33260, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 71h:15m:51s remains)
INFO - root - 2017-12-05 18:32:10.132675: step 33270, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 71h:34m:43s remains)
INFO - root - 2017-12-05 18:32:18.635107: step 33280, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 71h:27m:49s remains)
INFO - root - 2017-12-05 18:32:27.302772: step 33290, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 71h:33m:35s remains)
INFO - root - 2017-12-05 18:32:35.815939: step 33300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:18m:02s remains)
2017-12-05 18:32:36.594787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.259872 -4.2640333 -4.2642407 -4.249671 -4.2180867 -4.1660938 -4.1092443 -4.0720944 -4.0864019 -4.1470594 -4.2090268 -4.269959 -4.3132286 -4.3358316 -4.3452735][-4.2619004 -4.2674513 -4.2686338 -4.2540402 -4.2168097 -4.1514096 -4.078126 -4.032465 -4.0560942 -4.1336117 -4.2064681 -4.2738771 -4.3196907 -4.3401327 -4.3483329][-4.254 -4.263104 -4.2671275 -4.251646 -4.2068882 -4.1248817 -4.0371523 -3.9904644 -4.0288949 -4.1196852 -4.201561 -4.2764611 -4.3245349 -4.3437433 -4.3506536][-4.2468972 -4.2586455 -4.2621083 -4.2391748 -4.1812363 -4.0823088 -3.9857843 -3.9478881 -4.0055671 -4.1121831 -4.2050052 -4.28518 -4.3320088 -4.347908 -4.352572][-4.2459393 -4.2552972 -4.2513165 -4.2175188 -4.1450524 -4.03024 -3.9361098 -3.9182396 -3.9961162 -4.1169386 -4.2197289 -4.3011184 -4.3423615 -4.353313 -4.3543525][-4.2406187 -4.2435513 -4.2298646 -4.184401 -4.0956573 -3.9675479 -3.8906488 -3.9049821 -4.003293 -4.1305828 -4.2344909 -4.312417 -4.3491654 -4.355844 -4.3534594][-4.2333145 -4.2313824 -4.2094936 -4.153923 -4.0502667 -3.9131351 -3.8639174 -3.9121752 -4.0235 -4.1463838 -4.2445717 -4.3153872 -4.3491158 -4.3551946 -4.3518453][-4.2419176 -4.2382593 -4.2113461 -4.1548328 -4.0540433 -3.9264073 -3.8963857 -3.9514747 -4.0510483 -4.1573539 -4.2460775 -4.3094945 -4.3417339 -4.3511243 -4.3502717][-4.2530894 -4.2517123 -4.2257767 -4.1732392 -4.0843306 -3.9773619 -3.9597919 -4.0038519 -4.0815182 -4.1691017 -4.2463026 -4.30361 -4.33593 -4.3477678 -4.3491421][-4.2587953 -4.2568383 -4.2322569 -4.1872082 -4.11412 -4.0303354 -4.0223794 -4.0543027 -4.1142917 -4.1865406 -4.2529149 -4.3038096 -4.3349519 -4.3475919 -4.3491731][-4.2660508 -4.2652779 -4.2427273 -4.2045884 -4.1459193 -4.0836468 -4.0814052 -4.1037664 -4.1510363 -4.2121792 -4.2687 -4.3121963 -4.3402195 -4.3508697 -4.3504057][-4.2774491 -4.2766161 -4.2569423 -4.22498 -4.1771073 -4.1301346 -4.1306262 -4.1448207 -4.1829443 -4.2370152 -4.2866554 -4.3242579 -4.3470111 -4.3546829 -4.3523092][-4.2801404 -4.2768044 -4.2592793 -4.2322497 -4.1917138 -4.1539593 -4.1558127 -4.1662 -4.1976976 -4.2475071 -4.2945232 -4.329299 -4.348793 -4.3560872 -4.3530602][-4.2689891 -4.265408 -4.2491293 -4.2244458 -4.1896386 -4.1603093 -4.1621466 -4.17035 -4.1994991 -4.2489634 -4.2946754 -4.3282528 -4.3472967 -4.3553996 -4.3523431][-4.2477779 -4.2481947 -4.2382331 -4.2174931 -4.1873269 -4.160995 -4.1568408 -4.1614585 -4.1906934 -4.2416425 -4.2885308 -4.3222694 -4.3438783 -4.3536506 -4.3511505]]...]
INFO - root - 2017-12-05 18:32:45.111559: step 33310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:12m:50s remains)
INFO - root - 2017-12-05 18:32:53.759078: step 33320, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 71h:09m:03s remains)
INFO - root - 2017-12-05 18:33:02.427650: step 33330, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:16m:48s remains)
INFO - root - 2017-12-05 18:33:10.837588: step 33340, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 72h:22m:25s remains)
INFO - root - 2017-12-05 18:33:19.376053: step 33350, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 72h:16m:17s remains)
INFO - root - 2017-12-05 18:33:27.923733: step 33360, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 72h:50m:18s remains)
INFO - root - 2017-12-05 18:33:36.382516: step 33370, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 70h:41m:04s remains)
INFO - root - 2017-12-05 18:33:44.990497: step 33380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 70h:40m:59s remains)
INFO - root - 2017-12-05 18:33:53.529272: step 33390, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 69h:03m:17s remains)
INFO - root - 2017-12-05 18:34:02.131225: step 33400, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:49m:13s remains)
2017-12-05 18:34:02.853981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1783657 -4.1835952 -4.178246 -4.1727285 -4.1663713 -4.1662145 -4.1803904 -4.1862411 -4.194396 -4.2142243 -4.2339592 -4.2379713 -4.2323742 -4.22851 -4.2267737][-4.1288748 -4.1311736 -4.1313972 -4.1355457 -4.1391158 -4.1473651 -4.1724968 -4.1855783 -4.1939945 -4.2123332 -4.2330809 -4.2347097 -4.2232704 -4.21725 -4.2126064][-4.0990534 -4.104455 -4.1153193 -4.1352034 -4.1519461 -4.1672444 -4.1966257 -4.2093554 -4.2117834 -4.2206683 -4.239049 -4.241425 -4.2293506 -4.22451 -4.2171211][-4.116889 -4.1257572 -4.1428909 -4.1686835 -4.1890864 -4.2022905 -4.2192988 -4.2221518 -4.2154264 -4.2162352 -4.2355804 -4.2421455 -4.2354722 -4.2333384 -4.2263603][-4.1584439 -4.1670227 -4.1788983 -4.194665 -4.2033119 -4.2017183 -4.1973372 -4.1868434 -4.1799827 -4.1902661 -4.2165914 -4.2265668 -4.221981 -4.2232003 -4.2233305][-4.2024064 -4.20258 -4.2014985 -4.1991944 -4.1856327 -4.1561451 -4.1203079 -4.0932889 -4.0962276 -4.127286 -4.168891 -4.1964369 -4.2029614 -4.2114391 -4.2247591][-4.2297812 -4.21657 -4.1978655 -4.1746764 -4.1336012 -4.0688744 -3.9860244 -3.936307 -3.9674642 -4.0305662 -4.090251 -4.1465383 -4.1801362 -4.2079329 -4.2396135][-4.2387643 -4.2157831 -4.1799355 -4.1352353 -4.0744815 -3.982619 -3.8527822 -3.7836785 -3.8578339 -3.9644754 -4.0424838 -4.1170235 -4.1717591 -4.2184305 -4.2624125][-4.2377577 -4.2088804 -4.1671348 -4.1123304 -4.042748 -3.9530563 -3.8424711 -3.8007939 -3.8855855 -3.9958544 -4.07821 -4.1556463 -4.2124057 -4.2564955 -4.2927213][-4.235518 -4.2050614 -4.1675134 -4.1195579 -4.0619578 -4.002203 -3.9468205 -3.945158 -4.0141068 -4.096252 -4.1672773 -4.2389731 -4.2865653 -4.3112307 -4.3225718][-4.2347307 -4.1996574 -4.16318 -4.12568 -4.0887389 -4.0612659 -4.0535812 -4.080864 -4.1354012 -4.1934896 -4.2472763 -4.2988467 -4.3279414 -4.3343573 -4.327486][-4.2163787 -4.1653662 -4.1242356 -4.0967679 -4.0820808 -4.0874095 -4.1137142 -4.1628962 -4.2138014 -4.2528729 -4.2839751 -4.3126087 -4.3229132 -4.3178668 -4.3085394][-4.1736722 -4.1094356 -4.0610151 -4.0426064 -4.0556035 -4.0906005 -4.1401353 -4.1999459 -4.24554 -4.2668347 -4.2776055 -4.2871013 -4.2832308 -4.2721748 -4.2648129][-4.13085 -4.0628557 -4.0177679 -4.0150676 -4.0520916 -4.1047697 -4.163692 -4.2206221 -4.2516069 -4.255733 -4.2495832 -4.242979 -4.2326212 -4.2229285 -4.2189207][-4.1376748 -4.0798788 -4.0431633 -4.0487537 -4.0902534 -4.1417208 -4.1902413 -4.2278118 -4.2397065 -4.2274261 -4.2079487 -4.1893535 -4.1785297 -4.177825 -4.1799674]]...]
INFO - root - 2017-12-05 18:34:11.588950: step 33410, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 70h:51m:44s remains)
INFO - root - 2017-12-05 18:34:20.096221: step 33420, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:45m:26s remains)
INFO - root - 2017-12-05 18:34:28.735521: step 33430, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.856 sec/batch; 71h:04m:22s remains)
INFO - root - 2017-12-05 18:34:37.243022: step 33440, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.780 sec/batch; 64h:46m:17s remains)
INFO - root - 2017-12-05 18:34:45.819813: step 33450, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 72h:56m:10s remains)
INFO - root - 2017-12-05 18:34:54.450041: step 33460, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 71h:59m:03s remains)
INFO - root - 2017-12-05 18:35:02.833162: step 33470, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 72h:31m:08s remains)
INFO - root - 2017-12-05 18:35:11.458781: step 33480, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 73h:10m:30s remains)
INFO - root - 2017-12-05 18:35:19.933862: step 33490, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 69h:04m:19s remains)
INFO - root - 2017-12-05 18:35:28.494436: step 33500, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 70h:05m:47s remains)
2017-12-05 18:35:29.255599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3158426 -4.2983994 -4.2769141 -4.2503109 -4.21931 -4.187571 -4.157464 -4.139019 -4.1491728 -4.18318 -4.21632 -4.2363667 -4.2447476 -4.246644 -4.2403574][-4.3171434 -4.3010359 -4.2777977 -4.2457695 -4.2052221 -4.16046 -4.1156321 -4.0815415 -4.0838017 -4.1231422 -4.1699114 -4.2045264 -4.2219839 -4.23086 -4.2315331][-4.3147831 -4.3013821 -4.2801528 -4.2473044 -4.2020712 -4.1466393 -4.086853 -4.0379019 -4.0373197 -4.0852618 -4.1429491 -4.1839657 -4.2005377 -4.2090254 -4.2131815][-4.3107247 -4.2989192 -4.2788687 -4.2450643 -4.196063 -4.1306586 -4.0622096 -4.0070391 -4.0106044 -4.0689931 -4.1361494 -4.1799994 -4.1920776 -4.197299 -4.2009788][-4.3077006 -4.295063 -4.2725997 -4.2331319 -4.1773868 -4.1047559 -4.0337648 -3.9775765 -3.9841909 -4.0496011 -4.1263337 -4.178586 -4.1940379 -4.1990542 -4.2020121][-4.30562 -4.2915735 -4.2644787 -4.2164636 -4.151453 -4.0756655 -4.0090413 -3.9618595 -3.9767361 -4.0476265 -4.131865 -4.192461 -4.2118397 -4.217196 -4.2201376][-4.304215 -4.290031 -4.2620273 -4.2083721 -4.1377959 -4.066504 -4.0146542 -3.9867411 -4.016233 -4.0872083 -4.1684036 -4.228292 -4.2462873 -4.2502532 -4.2514606][-4.3022265 -4.2888103 -4.2620864 -4.21005 -4.143579 -4.0857348 -4.0521612 -4.044054 -4.0855865 -4.1527519 -4.220542 -4.26835 -4.283165 -4.2863603 -4.2831545][-4.298831 -4.2826447 -4.2560158 -4.2111154 -4.1573019 -4.117218 -4.0995889 -4.1072121 -4.1554685 -4.2158251 -4.269124 -4.3021083 -4.3131185 -4.3144827 -4.3071117][-4.2950964 -4.2749023 -4.2479382 -4.2113657 -4.1722097 -4.1511421 -4.14624 -4.1617432 -4.2112446 -4.2659168 -4.3063726 -4.3242612 -4.3285742 -4.3265796 -4.3171906][-4.2938213 -4.2716494 -4.2468729 -4.2188983 -4.1918006 -4.1839147 -4.1899023 -4.2100444 -4.2566018 -4.303751 -4.33135 -4.3355331 -4.3313265 -4.3239231 -4.3117194][-4.2953362 -4.2737079 -4.255609 -4.2390051 -4.22317 -4.2223983 -4.2325683 -4.2510624 -4.2888861 -4.32502 -4.3403335 -4.3356748 -4.3250093 -4.3117709 -4.2950649][-4.2975359 -4.2771468 -4.2634783 -4.2562213 -4.2517171 -4.2576442 -4.267056 -4.278357 -4.305491 -4.3286138 -4.3330832 -4.3224473 -4.3087411 -4.2904882 -4.2685828][-4.3016167 -4.2821164 -4.2690539 -4.2646408 -4.2658916 -4.2758408 -4.2847795 -4.2903252 -4.3071108 -4.3184624 -4.3149385 -4.3026476 -4.2888489 -4.2677374 -4.2416868][-4.3076296 -4.2907996 -4.2788157 -4.2754641 -4.2772622 -4.285913 -4.2927265 -4.2932744 -4.3022981 -4.3061485 -4.2989688 -4.2882771 -4.2742734 -4.2518578 -4.2240782]]...]
INFO - root - 2017-12-05 18:35:37.831655: step 33510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 70h:34m:18s remains)
INFO - root - 2017-12-05 18:35:46.430500: step 33520, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.840 sec/batch; 69h:43m:41s remains)
INFO - root - 2017-12-05 18:35:55.021037: step 33530, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 69h:53m:45s remains)
INFO - root - 2017-12-05 18:36:03.517745: step 33540, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 71h:01m:22s remains)
INFO - root - 2017-12-05 18:36:12.011490: step 33550, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 72h:11m:41s remains)
INFO - root - 2017-12-05 18:36:20.617367: step 33560, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 73h:52m:51s remains)
INFO - root - 2017-12-05 18:36:29.136310: step 33570, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 68h:40m:10s remains)
INFO - root - 2017-12-05 18:36:37.662244: step 33580, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 69h:54m:00s remains)
INFO - root - 2017-12-05 18:36:46.267841: step 33590, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 73h:14m:30s remains)
INFO - root - 2017-12-05 18:36:54.969746: step 33600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 71h:20m:02s remains)
2017-12-05 18:36:55.701059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3100433 -4.2942457 -4.2787786 -4.2681594 -4.2643709 -4.2685132 -4.2760983 -4.27865 -4.28019 -4.2817626 -4.2815633 -4.2848916 -4.2879949 -4.2907972 -4.2912145][-4.2967596 -4.2764587 -4.2570758 -4.245358 -4.2440276 -4.2542214 -4.2687016 -4.2772532 -4.2836981 -4.2886729 -4.2880726 -4.28912 -4.2897692 -4.291461 -4.2925925][-4.2795353 -4.2509756 -4.2246761 -4.2085795 -4.2060614 -4.2196031 -4.2409682 -4.2582831 -4.2757483 -4.2877321 -4.2902966 -4.2921124 -4.2922139 -4.293119 -4.2946873][-4.2647624 -4.2273 -4.1927142 -4.1679368 -4.15729 -4.1668277 -4.1911969 -4.216712 -4.25056 -4.2754931 -4.2855597 -4.2922053 -4.2952571 -4.296454 -4.2994013][-4.2604022 -4.2162871 -4.1731291 -4.1355815 -4.1069608 -4.0994849 -4.1145844 -4.1421428 -4.1943674 -4.2374935 -4.2603655 -4.2750154 -4.2846613 -4.2911372 -4.2983785][-4.2698936 -4.2260437 -4.1784997 -4.1279616 -4.0756426 -4.0352392 -4.018321 -4.0295591 -4.0932617 -4.1579022 -4.1988339 -4.2279367 -4.2497373 -4.2649584 -4.2782211][-4.282187 -4.2434082 -4.1969128 -4.1427183 -4.0769868 -4.0061283 -3.9410529 -3.9047062 -3.955323 -4.0346942 -4.09772 -4.1464248 -4.1835623 -4.2106133 -4.2332859][-4.2937989 -4.2621603 -4.2249389 -4.17856 -4.1152415 -4.0381517 -3.9521019 -3.8728862 -3.8777604 -3.9408336 -4.0109878 -4.0726867 -4.1204853 -4.1561151 -4.1874452][-4.3053222 -4.2795219 -4.2513533 -4.2182832 -4.1694417 -4.1068206 -4.0306931 -3.9479749 -3.9182069 -3.9401879 -3.9858677 -4.0390658 -4.0859427 -4.1232028 -4.1585112][-4.31573 -4.2936797 -4.2701287 -4.2482762 -4.2180948 -4.1773534 -4.1241803 -4.0573683 -4.0194306 -4.0133185 -4.0260677 -4.056118 -4.0899978 -4.1217794 -4.1552892][-4.3300076 -4.31507 -4.2973018 -4.2846279 -4.268723 -4.2460103 -4.2132654 -4.1649241 -4.1293478 -4.1132946 -4.107378 -4.1154313 -4.1324921 -4.1538076 -4.1796727][-4.3437753 -4.3373089 -4.3253703 -4.3171754 -4.30941 -4.2978826 -4.2787046 -4.246181 -4.2192578 -4.2048969 -4.1952519 -4.1948395 -4.2021551 -4.2129035 -4.228291][-4.352788 -4.3527284 -4.3469834 -4.3418565 -4.3370309 -4.3310008 -4.3210177 -4.3044286 -4.2894626 -4.28079 -4.2735138 -4.2716236 -4.2748275 -4.2793913 -4.286613][-4.3534722 -4.3560538 -4.3538327 -4.3507867 -4.3476782 -4.3440385 -4.3389726 -4.3314471 -4.3248816 -4.3217554 -4.3183885 -4.3179603 -4.3208914 -4.3234248 -4.3260407][-4.3476677 -4.3495355 -4.3479257 -4.3456984 -4.3439336 -4.3423457 -4.3399916 -4.33654 -4.3334618 -4.3322115 -4.3310027 -4.3319535 -4.3344731 -4.3365831 -4.3373694]]...]
INFO - root - 2017-12-05 18:37:04.370981: step 33610, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 72h:32m:16s remains)
INFO - root - 2017-12-05 18:37:12.989299: step 33620, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 69h:47m:47s remains)
INFO - root - 2017-12-05 18:37:21.513846: step 33630, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 69h:41m:55s remains)
INFO - root - 2017-12-05 18:37:30.035548: step 33640, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 72h:52m:39s remains)
INFO - root - 2017-12-05 18:37:38.522806: step 33650, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 69h:38m:41s remains)
INFO - root - 2017-12-05 18:37:46.973906: step 33660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:07m:09s remains)
INFO - root - 2017-12-05 18:37:55.416731: step 33670, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 71h:51m:13s remains)
INFO - root - 2017-12-05 18:38:04.009798: step 33680, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 70h:09m:44s remains)
INFO - root - 2017-12-05 18:38:12.456473: step 33690, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 70h:13m:40s remains)
INFO - root - 2017-12-05 18:38:20.884933: step 33700, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 70h:01m:51s remains)
2017-12-05 18:38:21.755913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2318172 -4.2548866 -4.2811708 -4.2871046 -4.27458 -4.26455 -4.2671452 -4.2602315 -4.2419162 -4.2373357 -4.2525158 -4.2763658 -4.3016081 -4.3238692 -4.3386426][-4.1690741 -4.2018666 -4.2394667 -4.2489061 -4.2348719 -4.2243338 -4.2269907 -4.21917 -4.1980515 -4.197113 -4.2184067 -4.2468629 -4.2792273 -4.3104534 -4.3323021][-4.1103024 -4.1558547 -4.2002048 -4.207695 -4.1932969 -4.1812515 -4.1803865 -4.1713948 -4.1505141 -4.1531043 -4.1802545 -4.2159467 -4.2583332 -4.2990947 -4.32642][-4.0848894 -4.1347566 -4.1787996 -4.1785169 -4.1542897 -4.1335468 -4.1241097 -4.1150694 -4.1023412 -4.1144705 -4.151536 -4.196609 -4.2468543 -4.2922764 -4.3212214][-4.0968876 -4.1359644 -4.1608152 -4.1365671 -4.0906858 -4.0525274 -4.037643 -4.037631 -4.0387559 -4.0644379 -4.1167717 -4.1725516 -4.2331634 -4.2817221 -4.3098269][-4.1261353 -4.1484504 -4.1477513 -4.0975471 -4.0247793 -3.9662127 -3.94849 -3.9607997 -3.9744086 -4.0069079 -4.069128 -4.1337748 -4.2085981 -4.2664776 -4.2946863][-4.1681027 -4.1710649 -4.1449184 -4.0754352 -3.9807284 -3.9036779 -3.8762276 -3.8827698 -3.897233 -3.9432855 -4.0187654 -4.0868154 -4.1680536 -4.2387462 -4.2742295][-4.2122893 -4.2006173 -4.1628847 -4.0906 -3.9951155 -3.9188476 -3.8798506 -3.8618639 -3.8581164 -3.90571 -3.9894264 -4.053812 -4.1303139 -4.2074213 -4.2534013][-4.2467089 -4.2302914 -4.1992683 -4.1428785 -4.069963 -4.0146446 -3.9816968 -3.9557421 -3.9326653 -3.9574928 -4.0195966 -4.0674095 -4.1261778 -4.1986332 -4.2473893][-4.2707639 -4.2584367 -4.240171 -4.2041211 -4.1559191 -4.1200395 -4.0946817 -4.07118 -4.0412893 -4.042942 -4.0809975 -4.1099796 -4.1564569 -4.21881 -4.2632928][-4.2923131 -4.2889013 -4.2833939 -4.264317 -4.23626 -4.2145567 -4.1955976 -4.1742892 -4.14431 -4.134284 -4.1488514 -4.1641884 -4.2031631 -4.2556896 -4.2931075][-4.3087845 -4.312325 -4.3142471 -4.3041534 -4.2904429 -4.2813706 -4.2713761 -4.2561131 -4.2281342 -4.2105575 -4.2123466 -4.2220335 -4.2537065 -4.2944937 -4.3224669][-4.3164244 -4.3220334 -4.3280768 -4.3245044 -4.318233 -4.3177071 -4.316205 -4.3083591 -4.2875862 -4.2683754 -4.2622137 -4.2683415 -4.29213 -4.3216233 -4.3409996][-4.3200564 -4.3231335 -4.3285618 -4.3301654 -4.330543 -4.3355904 -4.3389349 -4.3353739 -4.3227797 -4.3062491 -4.2982359 -4.30229 -4.3179088 -4.3376865 -4.349638][-4.3233733 -4.3232422 -4.3253093 -4.3268456 -4.3288865 -4.3346429 -4.3396978 -4.3394771 -4.3324633 -4.3222036 -4.3175788 -4.3216448 -4.3326797 -4.3449783 -4.3505678]]...]
INFO - root - 2017-12-05 18:38:30.194057: step 33710, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.811 sec/batch; 67h:17m:08s remains)
INFO - root - 2017-12-05 18:38:38.647006: step 33720, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 71h:18m:36s remains)
INFO - root - 2017-12-05 18:38:47.206662: step 33730, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 70h:34m:04s remains)
INFO - root - 2017-12-05 18:38:55.885005: step 33740, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 69h:57m:59s remains)
INFO - root - 2017-12-05 18:39:04.350509: step 33750, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 69h:17m:41s remains)
INFO - root - 2017-12-05 18:39:12.837380: step 33760, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 67h:54m:27s remains)
INFO - root - 2017-12-05 18:39:21.174103: step 33770, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 72h:19m:52s remains)
INFO - root - 2017-12-05 18:39:29.648446: step 33780, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 72h:17m:22s remains)
INFO - root - 2017-12-05 18:39:38.173511: step 33790, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:40m:09s remains)
INFO - root - 2017-12-05 18:39:46.636123: step 33800, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 69h:31m:08s remains)
2017-12-05 18:39:47.359887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25521 -4.236692 -4.215137 -4.20358 -4.1836166 -4.1449394 -4.1154013 -4.1260834 -4.1582026 -4.1852813 -4.216126 -4.2511778 -4.2748795 -4.2866759 -4.2891946][-4.2626061 -4.2481 -4.2250857 -4.2109566 -4.1900144 -4.1537533 -4.1202283 -4.1173687 -4.1419363 -4.1692777 -4.2024522 -4.236794 -4.2596607 -4.2758603 -4.284657][-4.2714705 -4.2559819 -4.2299132 -4.2148843 -4.1963806 -4.1640949 -4.1262465 -4.1045713 -4.119401 -4.150547 -4.1895514 -4.225162 -4.25094 -4.2705894 -4.2818136][-4.270575 -4.2505803 -4.2190204 -4.2010555 -4.1845269 -4.1595798 -4.1242175 -4.0958033 -4.10491 -4.1407914 -4.1884732 -4.2244072 -4.24688 -4.2648454 -4.2754068][-4.2628961 -4.2424874 -4.2090268 -4.1887922 -4.1719012 -4.1487284 -4.1175289 -4.0840607 -4.0753269 -4.1081347 -4.1674891 -4.21605 -4.2408066 -4.2540669 -4.2620296][-4.2547503 -4.2425537 -4.21088 -4.182374 -4.1596465 -4.1290679 -4.0852709 -4.0222392 -3.9770336 -4.0158677 -4.108614 -4.1845922 -4.2204008 -4.2361894 -4.2464967][-4.2464304 -4.2456684 -4.2178011 -4.1793365 -4.1465855 -4.1028738 -4.0323896 -3.9236002 -3.8409841 -3.9054563 -4.0436721 -4.1449671 -4.193336 -4.2198348 -4.2365789][-4.2452273 -4.2543478 -4.2315354 -4.1920271 -4.1554623 -4.1064625 -4.0198069 -3.8914626 -3.8095288 -3.8923545 -4.03699 -4.1366644 -4.1840625 -4.2121496 -4.229774][-4.2508516 -4.2629304 -4.2422767 -4.2048111 -4.1691546 -4.1286445 -4.0530434 -3.94307 -3.898684 -3.9823084 -4.0909362 -4.1641808 -4.2000251 -4.2204738 -4.2328176][-4.2611408 -4.2730131 -4.2524505 -4.2170172 -4.1811466 -4.1484041 -4.0887218 -4.0022874 -3.9837341 -4.0585747 -4.1379366 -4.1923966 -4.2221856 -4.2412028 -4.2493024][-4.2690153 -4.2795515 -4.26267 -4.23494 -4.2031116 -4.1704812 -4.1182485 -4.04483 -4.0324984 -4.0927114 -4.1575294 -4.2094626 -4.2421613 -4.2652712 -4.2717314][-4.2763743 -4.2809272 -4.2670774 -4.245718 -4.2207513 -4.1927085 -4.1500516 -4.0906425 -4.0777459 -4.1206613 -4.1724639 -4.2228465 -4.2604618 -4.2862315 -4.2917905][-4.2849312 -4.2824936 -4.2681832 -4.2511606 -4.2325583 -4.2114449 -4.1806178 -4.1371841 -4.1243377 -4.1542673 -4.1947455 -4.2370977 -4.274991 -4.2991462 -4.3034906][-4.292799 -4.2863951 -4.2734184 -4.25988 -4.2454553 -4.2287164 -4.2078781 -4.1790075 -4.17066 -4.1917782 -4.2234831 -4.2582631 -4.2899318 -4.308423 -4.3112378][-4.298399 -4.2924838 -4.2839251 -4.274478 -4.2634182 -4.2512188 -4.237515 -4.2214475 -4.21665 -4.2288265 -4.252316 -4.2793822 -4.3022504 -4.3144255 -4.31608]]...]
INFO - root - 2017-12-05 18:39:55.821548: step 33810, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 70h:15m:32s remains)
INFO - root - 2017-12-05 18:40:04.289505: step 33820, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 69h:04m:09s remains)
INFO - root - 2017-12-05 18:40:12.769525: step 33830, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 70h:26m:46s remains)
INFO - root - 2017-12-05 18:40:21.234406: step 33840, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 72h:02m:20s remains)
INFO - root - 2017-12-05 18:40:29.849403: step 33850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 70h:53m:53s remains)
INFO - root - 2017-12-05 18:40:38.351592: step 33860, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 72h:25m:36s remains)
INFO - root - 2017-12-05 18:40:46.622932: step 33870, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 69h:10m:49s remains)
INFO - root - 2017-12-05 18:40:55.151626: step 33880, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 70h:28m:51s remains)
INFO - root - 2017-12-05 18:41:03.637231: step 33890, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.821 sec/batch; 68h:05m:43s remains)
INFO - root - 2017-12-05 18:41:12.237769: step 33900, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.826 sec/batch; 68h:32m:52s remains)
2017-12-05 18:41:13.004358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1513791 -4.1816635 -4.2057347 -4.2077308 -4.19134 -4.17559 -4.1630664 -4.1436749 -4.1336589 -4.1299605 -4.1380849 -4.1503243 -4.1506796 -4.1370206 -4.1197257][-4.1130247 -4.1481318 -4.1757298 -4.1830068 -4.1641173 -4.1411233 -4.1203318 -4.0971456 -4.0922546 -4.0971189 -4.1106849 -4.1289306 -4.1354942 -4.1261296 -4.1103625][-4.0944867 -4.13297 -4.1670146 -4.1818638 -4.1622872 -4.1305008 -4.099926 -4.0717545 -4.0697718 -4.0833149 -4.1014557 -4.1235414 -4.13312 -4.1292014 -4.1182513][-4.1059957 -4.1436453 -4.1794848 -4.19518 -4.1718802 -4.1325603 -4.0930877 -4.0595994 -4.0621672 -4.0856881 -4.1108937 -4.1378784 -4.1486158 -4.1465054 -4.1359186][-4.1182032 -4.14789 -4.1767817 -4.1887765 -4.160357 -4.1138024 -4.0655422 -4.028183 -4.0413356 -4.0802908 -4.1119542 -4.1385164 -4.1470213 -4.1443429 -4.1349282][-4.134686 -4.1492381 -4.1636472 -4.1705012 -4.1407433 -4.0886378 -4.0298648 -3.9897311 -4.0192089 -4.0757546 -4.111752 -4.1354418 -4.1356292 -4.1282792 -4.1194143][-4.1530557 -4.1547031 -4.1503477 -4.1447363 -4.1103115 -4.0489006 -3.9788337 -3.9341564 -3.983897 -4.0642319 -4.1115494 -4.1368885 -4.1327176 -4.1253576 -4.1195469][-4.1611695 -4.1553459 -4.13572 -4.1138825 -4.0690885 -3.9967279 -3.9157403 -3.8662553 -3.9339414 -4.0370069 -4.1005855 -4.1333275 -4.1372452 -4.1395116 -4.13916][-4.175703 -4.1695228 -4.1470566 -4.119976 -4.0752268 -4.0081596 -3.9341884 -3.8895631 -3.9466484 -4.038033 -4.0996065 -4.1345758 -4.1478071 -4.1610088 -4.1641941][-4.1844292 -4.1804123 -4.1673093 -4.1524487 -4.123333 -4.0751114 -4.0205545 -3.9878812 -4.0199738 -4.0789013 -4.1206069 -4.1465378 -4.161665 -4.1798396 -4.1819878][-4.185514 -4.184442 -4.1825905 -4.1837516 -4.1731839 -4.1433072 -4.1065383 -4.0849128 -4.0981069 -4.1295123 -4.1515117 -4.1674967 -4.1826177 -4.2021918 -4.2005072][-4.1985235 -4.197803 -4.1991405 -4.2051086 -4.2072191 -4.1925316 -4.1698151 -4.1535268 -4.15243 -4.1644473 -4.172966 -4.1829667 -4.1979065 -4.2150569 -4.2100945][-4.2075276 -4.205163 -4.2067285 -4.2128434 -4.220933 -4.2147155 -4.2010927 -4.1884336 -4.1808982 -4.1823092 -4.1847639 -4.1900096 -4.2029624 -4.214232 -4.2092948][-4.2245531 -4.2238135 -4.2244716 -4.22918 -4.2378221 -4.23627 -4.2267575 -4.2164011 -4.2080264 -4.2075357 -4.2098179 -4.2113419 -4.2165594 -4.2157917 -4.207139][-4.243463 -4.2470045 -4.2483573 -4.2515917 -4.2551894 -4.25186 -4.2417345 -4.230145 -4.2213063 -4.2207561 -4.2232246 -4.2218723 -4.2197824 -4.2060819 -4.1902823]]...]
INFO - root - 2017-12-05 18:41:21.410827: step 33910, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 69h:14m:30s remains)
INFO - root - 2017-12-05 18:41:29.945062: step 33920, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 70h:17m:20s remains)
INFO - root - 2017-12-05 18:41:38.466587: step 33930, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 70h:02m:07s remains)
INFO - root - 2017-12-05 18:41:46.941981: step 33940, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 71h:29m:26s remains)
INFO - root - 2017-12-05 18:41:55.503897: step 33950, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 70h:31m:55s remains)
INFO - root - 2017-12-05 18:42:04.134420: step 33960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 71h:03m:30s remains)
INFO - root - 2017-12-05 18:42:12.576493: step 33970, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 70h:38m:06s remains)
INFO - root - 2017-12-05 18:42:21.044042: step 33980, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 71h:59m:05s remains)
INFO - root - 2017-12-05 18:42:29.602970: step 33990, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 72h:20m:09s remains)
INFO - root - 2017-12-05 18:42:38.146991: step 34000, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 71h:18m:09s remains)
2017-12-05 18:42:38.944294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2248688 -4.2164874 -4.2196269 -4.2250023 -4.2304955 -4.2322183 -4.228138 -4.2194452 -4.211813 -4.2083707 -4.2067423 -4.2036166 -4.1930013 -4.1825495 -4.1856408][-4.1953645 -4.1816106 -4.1828418 -4.1874733 -4.1948538 -4.1984649 -4.1938672 -4.1847558 -4.1819248 -4.1843143 -4.1843495 -4.1786284 -4.1622643 -4.1475749 -4.1511135][-4.173943 -4.1541195 -4.149374 -4.14886 -4.1549139 -4.1603661 -4.1567 -4.1491842 -4.1565709 -4.1712747 -4.17613 -4.1691575 -4.1505828 -4.1372066 -4.1420565][-4.162641 -4.1381435 -4.1271749 -4.1186213 -4.118854 -4.1198745 -4.1113334 -4.103642 -4.1222792 -4.1530604 -4.1657906 -4.1627612 -4.1483641 -4.1388392 -4.1442633][-4.1622992 -4.1317711 -4.1115942 -4.0905209 -4.079843 -4.0693641 -4.048275 -4.0403881 -4.076282 -4.1241941 -4.1469951 -4.1532655 -4.147162 -4.1403 -4.1433606][-4.1654544 -4.1257787 -4.0912113 -4.0539007 -4.0258989 -3.9961569 -3.95789 -3.9532375 -4.0178003 -4.0881462 -4.12265 -4.1387196 -4.1427073 -4.1411238 -4.1451826][-4.1613269 -4.1166763 -4.0744529 -4.0264149 -3.9789193 -3.9222841 -3.8571677 -3.8524885 -3.9482808 -4.0411611 -4.0872264 -4.1115475 -4.1231003 -4.1283064 -4.1398597][-4.1529789 -4.110455 -4.0719433 -4.0272584 -3.9757957 -3.9055588 -3.8243356 -3.8164821 -3.9224577 -4.0201726 -4.0668292 -4.0918293 -4.1053553 -4.11228 -4.1294518][-4.1544681 -4.1176858 -4.0871596 -4.0537643 -4.0154982 -3.9641771 -3.909575 -3.9122028 -3.9919574 -4.0604057 -4.0875688 -4.1008263 -4.1101694 -4.1184382 -4.1373239][-4.1748524 -4.145534 -4.1229219 -4.1006227 -4.0770636 -4.0508723 -4.0302372 -4.043726 -4.0947504 -4.1324463 -4.1376386 -4.1337018 -4.1329861 -4.1377487 -4.15402][-4.2053046 -4.1817837 -4.1665854 -4.154079 -4.1434932 -4.1362247 -4.1374636 -4.1543055 -4.1834378 -4.2001867 -4.193049 -4.1786375 -4.1697063 -4.1714764 -4.1850667][-4.2395349 -4.2206507 -4.2099066 -4.2044821 -4.2038035 -4.2078156 -4.2182183 -4.2318769 -4.2470765 -4.2523475 -4.2424607 -4.2249713 -4.212863 -4.2130651 -4.2226963][-4.2737117 -4.2605076 -4.2544689 -4.2530422 -4.2563033 -4.2648444 -4.2759824 -4.28523 -4.293314 -4.2947016 -4.2876616 -4.27387 -4.2630234 -4.2609792 -4.2636924][-4.307128 -4.2991104 -4.295609 -4.2947407 -4.2976842 -4.3053226 -4.3142986 -4.3199434 -4.3242054 -4.32523 -4.3229222 -4.3172927 -4.31277 -4.3114295 -4.30917][-4.3253374 -4.322093 -4.3204479 -4.3194518 -4.3204875 -4.3249073 -4.3301606 -4.3323984 -4.3329406 -4.3325877 -4.3328052 -4.3334231 -4.3358159 -4.3378396 -4.3350234]]...]
INFO - root - 2017-12-05 18:42:47.554370: step 34010, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 72h:08m:22s remains)
INFO - root - 2017-12-05 18:42:55.949256: step 34020, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 70h:29m:54s remains)
INFO - root - 2017-12-05 18:43:04.533131: step 34030, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 70h:23m:52s remains)
INFO - root - 2017-12-05 18:43:13.221238: step 34040, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 69h:36m:46s remains)
INFO - root - 2017-12-05 18:43:21.691421: step 34050, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 71h:33m:28s remains)
INFO - root - 2017-12-05 18:43:30.318690: step 34060, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 72h:07m:01s remains)
INFO - root - 2017-12-05 18:43:38.696043: step 34070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 71h:05m:39s remains)
INFO - root - 2017-12-05 18:43:47.168112: step 34080, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 69h:28m:39s remains)
INFO - root - 2017-12-05 18:43:55.572518: step 34090, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:57m:55s remains)
INFO - root - 2017-12-05 18:44:04.167906: step 34100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 72h:05m:45s remains)
2017-12-05 18:44:04.875116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3338366 -4.3344822 -4.3354864 -4.3338413 -4.3274708 -4.3172083 -4.308363 -4.3057766 -4.3095846 -4.3168526 -4.3231149 -4.3259826 -4.3248119 -4.3216009 -4.3193793][-4.3392105 -4.3394327 -4.341784 -4.3433733 -4.3423042 -4.3387556 -4.33556 -4.3349977 -4.337749 -4.3423028 -4.3447294 -4.3435187 -4.3382945 -4.3320713 -4.3285389][-4.3392067 -4.3395166 -4.3427291 -4.3458962 -4.34769 -4.3482275 -4.3477416 -4.3468976 -4.3480029 -4.3511281 -4.3526154 -4.3505287 -4.3429837 -4.3340955 -4.3274937][-4.3335295 -4.3336744 -4.3353996 -4.3364377 -4.3380685 -4.3387995 -4.3362622 -4.3313484 -4.3299189 -4.334074 -4.3400574 -4.342957 -4.3373909 -4.32666 -4.3155727][-4.3218484 -4.3199186 -4.31665 -4.3095202 -4.3035154 -4.298553 -4.2882504 -4.2746129 -4.2713408 -4.2811317 -4.2985497 -4.3139548 -4.3178592 -4.3109121 -4.297215][-4.3055582 -4.2990742 -4.2854147 -4.2623072 -4.2376475 -4.2156396 -4.1888113 -4.1638789 -4.1642494 -4.189805 -4.229301 -4.2657294 -4.2858987 -4.2877088 -4.2732277][-4.2896953 -4.2756581 -4.2465591 -4.1994677 -4.1462193 -4.0942817 -4.040134 -4.0012741 -4.0129128 -4.0656977 -4.1346164 -4.1935115 -4.23242 -4.2485194 -4.239706][-4.2776175 -4.2566543 -4.2144814 -4.1472497 -4.065259 -3.9797306 -3.8925345 -3.8357546 -3.8546159 -3.9324653 -4.0302815 -4.113184 -4.1733551 -4.2081809 -4.2132549][-4.2641931 -4.24338 -4.2021346 -4.1366067 -4.0528965 -3.9623311 -3.8725002 -3.8203175 -3.8420563 -3.9213238 -4.0191793 -4.1029477 -4.1656008 -4.2049031 -4.2162375][-4.2516932 -4.2393236 -4.2137966 -4.1712394 -4.1132507 -4.0511661 -3.9944923 -3.9701383 -3.9925556 -4.0507283 -4.1209059 -4.1793504 -4.2207794 -4.2439485 -4.2444782][-4.2524366 -4.2536173 -4.2470922 -4.2286716 -4.19963 -4.1670709 -4.140224 -4.1352406 -4.1556964 -4.1947079 -4.2383833 -4.2708063 -4.2880816 -4.2904758 -4.2745023][-4.2695308 -4.277143 -4.2809544 -4.2775736 -4.2692204 -4.2588716 -4.25105 -4.2548637 -4.2700872 -4.2924004 -4.3144255 -4.3294272 -4.3323426 -4.3244133 -4.3021345][-4.2952127 -4.3027472 -4.3083582 -4.3105307 -4.3120785 -4.3124089 -4.3129897 -4.3173962 -4.32553 -4.3353577 -4.3441505 -4.3499961 -4.3490233 -4.3418112 -4.3251128][-4.3185263 -4.324883 -4.32895 -4.3314347 -4.3356657 -4.3378658 -4.3378396 -4.3382869 -4.3403764 -4.3430023 -4.3449793 -4.3461161 -4.3450623 -4.3417997 -4.3337512][-4.331172 -4.3355432 -4.3383913 -4.3401852 -4.3436556 -4.344305 -4.3423476 -4.3400984 -4.3379245 -4.3358674 -4.3346086 -4.3336644 -4.3336563 -4.3338022 -4.3324571]]...]
INFO - root - 2017-12-05 18:44:13.358991: step 34110, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 68h:53m:14s remains)
INFO - root - 2017-12-05 18:44:21.857944: step 34120, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 68h:46m:28s remains)
INFO - root - 2017-12-05 18:44:30.404919: step 34130, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 71h:32m:50s remains)
INFO - root - 2017-12-05 18:44:38.938550: step 34140, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 71h:51m:25s remains)
INFO - root - 2017-12-05 18:44:47.527367: step 34150, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.905 sec/batch; 75h:01m:09s remains)
INFO - root - 2017-12-05 18:44:56.090063: step 34160, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 69h:34m:59s remains)
INFO - root - 2017-12-05 18:45:04.528632: step 34170, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.824 sec/batch; 68h:18m:29s remains)
INFO - root - 2017-12-05 18:45:13.036277: step 34180, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 71h:49m:34s remains)
INFO - root - 2017-12-05 18:45:21.494649: step 34190, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.869 sec/batch; 72h:00m:04s remains)
INFO - root - 2017-12-05 18:45:30.053549: step 34200, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 71h:58m:37s remains)
2017-12-05 18:45:30.835358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1786709 -4.2074881 -4.2370176 -4.2490206 -4.2537665 -4.2492123 -4.2361608 -4.2295761 -4.2269073 -4.218617 -4.2062368 -4.2025313 -4.2043519 -4.2015967 -4.199718][-4.1686072 -4.1988912 -4.223958 -4.2317281 -4.236845 -4.23633 -4.2272844 -4.2238245 -4.2246046 -4.2219486 -4.2119331 -4.2092543 -4.2085361 -4.1989408 -4.1959424][-4.1543241 -4.1817904 -4.1998553 -4.2012329 -4.2039347 -4.2065969 -4.2017407 -4.1995296 -4.2010069 -4.2010441 -4.1983714 -4.2005529 -4.2018032 -4.188529 -4.1846528][-4.1471925 -4.1711254 -4.1816111 -4.1775918 -4.1738353 -4.1761465 -4.1727829 -4.1716228 -4.1684709 -4.1649771 -4.1674352 -4.1776581 -4.1868482 -4.1797237 -4.179029][-4.1344967 -4.1589594 -4.1655655 -4.1534295 -4.1383567 -4.1282024 -4.119473 -4.1199641 -4.1191459 -4.1209726 -4.1319332 -4.1513896 -4.1704664 -4.1736369 -4.17415][-4.1019158 -4.1233368 -4.1296134 -4.1119256 -4.0823016 -4.0503259 -4.0291152 -4.0342603 -4.0490112 -4.0725327 -4.0986037 -4.1295943 -4.1558957 -4.1624246 -4.1622181][-4.0654655 -4.0808477 -4.085268 -4.0591693 -4.0182729 -3.9714234 -3.9387114 -3.9480293 -3.9821403 -4.03188 -4.07634 -4.1181941 -4.1479993 -4.1545749 -4.153192][-4.0622258 -4.0671363 -4.0637546 -4.0312848 -3.9857085 -3.93007 -3.8816724 -3.8856277 -3.9324429 -4.0057936 -4.0668416 -4.1140432 -4.1435919 -4.1512346 -4.1513057][-4.0988283 -4.091115 -4.0809789 -4.0488768 -4.0102339 -3.9591479 -3.9082589 -3.9050906 -3.9445109 -4.0154552 -4.0721045 -4.1112318 -4.1341543 -4.1407475 -4.1453247][-4.1424532 -4.131566 -4.125844 -4.1045151 -4.0798993 -4.0434089 -4.0047331 -3.9969826 -4.0182605 -4.0667987 -4.1051831 -4.1287374 -4.1362486 -4.135088 -4.1400428][-4.163311 -4.1603241 -4.1677976 -4.1618366 -4.150732 -4.1303964 -4.1067176 -4.0977774 -4.105226 -4.1336308 -4.1560431 -4.1646791 -4.1545124 -4.143889 -4.1465797][-4.1578312 -4.1615644 -4.1788821 -4.18534 -4.1836138 -4.1759462 -4.1633043 -4.1574106 -4.1614323 -4.18025 -4.1969466 -4.1963687 -4.1769176 -4.1619039 -4.1625023][-4.1483955 -4.1541028 -4.1700549 -4.1786695 -4.1831026 -4.1882744 -4.1879063 -4.1860285 -4.1923051 -4.2068667 -4.2176666 -4.209971 -4.188724 -4.1741405 -4.1756468][-4.1553164 -4.1582513 -4.1653948 -4.1703916 -4.1761727 -4.1887345 -4.199265 -4.2033367 -4.2099419 -4.2151747 -4.2140994 -4.1980577 -4.1730547 -4.1610513 -4.1678209][-4.175652 -4.1716318 -4.1718469 -4.1774406 -4.1859865 -4.2011824 -4.2166848 -4.2248416 -4.2274966 -4.2199979 -4.2049642 -4.1799369 -4.1521983 -4.140676 -4.1465592]]...]
INFO - root - 2017-12-05 18:45:39.448203: step 34210, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 72h:51m:26s remains)
INFO - root - 2017-12-05 18:45:47.991546: step 34220, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 72h:01m:03s remains)
INFO - root - 2017-12-05 18:45:56.599019: step 34230, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 71h:26m:48s remains)
INFO - root - 2017-12-05 18:46:05.135749: step 34240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 70h:47m:41s remains)
INFO - root - 2017-12-05 18:46:13.693737: step 34250, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.824 sec/batch; 68h:16m:51s remains)
INFO - root - 2017-12-05 18:46:22.332643: step 34260, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 71h:21m:11s remains)
INFO - root - 2017-12-05 18:46:30.768023: step 34270, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 71h:09m:46s remains)
INFO - root - 2017-12-05 18:46:39.381354: step 34280, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 70h:04m:06s remains)
INFO - root - 2017-12-05 18:46:48.054180: step 34290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 70h:34m:05s remains)
INFO - root - 2017-12-05 18:46:56.470411: step 34300, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 67h:32m:42s remains)
2017-12-05 18:46:57.194869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1455007 -4.17624 -4.2103758 -4.2364507 -4.2526951 -4.2642817 -4.2704015 -4.2610679 -4.2494965 -4.244935 -4.2439747 -4.2379031 -4.2322588 -4.2247705 -4.2144332][-4.1121573 -4.1479821 -4.1877761 -4.2172856 -4.2375665 -4.2551746 -4.2637482 -4.251842 -4.2371035 -4.2326469 -4.2326059 -4.2286334 -4.2248545 -4.2145767 -4.1996589][-4.1075611 -4.1447215 -4.1818709 -4.2074924 -4.2255831 -4.2422705 -4.2493052 -4.2355504 -4.2222114 -4.2228904 -4.2282915 -4.2277203 -4.2234488 -4.2092991 -4.1903243][-4.1196041 -4.1528044 -4.1838684 -4.2009749 -4.2105989 -4.2208118 -4.2180858 -4.1986604 -4.1889114 -4.2023816 -4.2205381 -4.2271876 -4.2236972 -4.2092781 -4.1884809][-4.1345053 -4.155364 -4.1719513 -4.1734529 -4.1690216 -4.1678057 -4.1524944 -4.1249018 -4.1219053 -4.1550078 -4.19106 -4.2120819 -4.21573 -4.2061849 -4.1885276][-4.1368361 -4.1467361 -4.1445971 -4.1289978 -4.1097007 -4.0923123 -4.0553489 -4.0100884 -4.0152555 -4.07332 -4.1341715 -4.1770177 -4.1977391 -4.2001081 -4.1916451][-4.1292706 -4.1310234 -4.1119843 -4.0817637 -4.0496535 -4.0179367 -3.9643803 -3.9046988 -3.9188926 -4.0022955 -4.0846252 -4.1424828 -4.1742878 -4.1883063 -4.1911616][-4.1322126 -4.1240177 -4.0947342 -4.0577536 -4.0274048 -4.0052528 -3.9639974 -3.9109206 -3.9273221 -4.0112715 -4.0862441 -4.1356721 -4.1647859 -4.1840568 -4.1950397][-4.1380706 -4.1224627 -4.0930305 -4.0668707 -4.0542164 -4.0550909 -4.040431 -4.0045056 -4.0107431 -4.0704942 -4.1204195 -4.1500611 -4.169775 -4.1874528 -4.1969466][-4.14545 -4.129375 -4.1087613 -4.0969324 -4.1004968 -4.1127286 -4.1124916 -4.0876846 -4.0803709 -4.1093221 -4.1363878 -4.1483717 -4.15821 -4.1702328 -4.1741357][-4.163691 -4.1508527 -4.1339207 -4.1258245 -4.1314893 -4.1418562 -4.14429 -4.1250005 -4.1067133 -4.1104918 -4.1208978 -4.1239295 -4.124342 -4.126647 -4.1226354][-4.19391 -4.1830173 -4.16667 -4.1545634 -4.1530147 -4.1552534 -4.1536589 -4.1368504 -4.1141534 -4.10686 -4.1097121 -4.1067457 -4.0976582 -4.0922618 -4.0864706][-4.2278457 -4.2195249 -4.2047176 -4.1919947 -4.1860418 -4.1808553 -4.1742406 -4.1612177 -4.1428843 -4.1328259 -4.1288137 -4.1142783 -4.0926361 -4.0817471 -4.0772867][-4.2431784 -4.2398791 -4.2297592 -4.2196789 -4.2145495 -4.208128 -4.2000189 -4.1901836 -4.1770687 -4.1658506 -4.1554337 -4.1331954 -4.1035323 -4.0905852 -4.0903106][-4.2390475 -4.2394075 -4.233274 -4.2272248 -4.2239504 -4.2204123 -4.2139244 -4.2051973 -4.1952348 -4.1857481 -4.1757045 -4.1529822 -4.1223931 -4.1081018 -4.108758]]...]
INFO - root - 2017-12-05 18:47:05.761668: step 34310, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 71h:05m:46s remains)
INFO - root - 2017-12-05 18:47:14.390068: step 34320, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 70h:26m:26s remains)
INFO - root - 2017-12-05 18:47:23.092537: step 34330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 69h:59m:20s remains)
INFO - root - 2017-12-05 18:47:31.693553: step 34340, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 70h:58m:10s remains)
INFO - root - 2017-12-05 18:47:40.113646: step 34350, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 70h:55m:47s remains)
INFO - root - 2017-12-05 18:47:48.583418: step 34360, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 70h:29m:09s remains)
INFO - root - 2017-12-05 18:47:57.066632: step 34370, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:37m:30s remains)
INFO - root - 2017-12-05 18:48:05.666631: step 34380, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.868 sec/batch; 71h:55m:02s remains)
INFO - root - 2017-12-05 18:48:14.308765: step 34390, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 70h:12m:15s remains)
INFO - root - 2017-12-05 18:48:22.756794: step 34400, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:53m:05s remains)
2017-12-05 18:48:23.506975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2054882 -4.2036281 -4.1908193 -4.1820273 -4.1832709 -4.1864071 -4.1902232 -4.19397 -4.1970577 -4.2003107 -4.2051182 -4.2098131 -4.2118068 -4.2095003 -4.202992][-4.2256732 -4.2228541 -4.2075095 -4.1972475 -4.1985831 -4.2021208 -4.2060084 -4.2074924 -4.2086525 -4.2108622 -4.2147355 -4.2194619 -4.2220893 -4.2226696 -4.2208295][-4.2576246 -4.2584772 -4.2443671 -4.2340875 -4.2341371 -4.2354321 -4.2351885 -4.232039 -4.2288356 -4.2276931 -4.2294354 -4.2340941 -4.2381063 -4.2405181 -4.2426829][-4.2848496 -4.2854433 -4.2698274 -4.2571664 -4.2542138 -4.2524152 -4.2456989 -4.2347603 -4.2254 -4.2188821 -4.2165875 -4.2213893 -4.2280297 -4.2318611 -4.23865][-4.2990789 -4.2931566 -4.2707663 -4.2507787 -4.2424445 -4.2363091 -4.2212796 -4.1977158 -4.179091 -4.1683979 -4.167325 -4.1770868 -4.1876197 -4.195859 -4.2118831][-4.2935896 -4.2763953 -4.2380123 -4.2030296 -4.18093 -4.1604562 -4.1303716 -4.0959482 -4.0822124 -4.08394 -4.0972676 -4.1178885 -4.1346984 -4.1509604 -4.1716385][-4.2675691 -4.2288051 -4.1628432 -4.1030664 -4.0612063 -4.0228214 -3.9755902 -3.939415 -3.9522109 -3.9826436 -4.01983 -4.0607076 -4.0894771 -4.1126642 -4.1364055][-4.2287874 -4.165123 -4.0725784 -3.9912796 -3.9374228 -3.8937826 -3.8502688 -3.8375025 -3.8867791 -3.9413586 -3.9964263 -4.0547023 -4.0948119 -4.1222219 -4.1475725][-4.2081952 -4.1401424 -4.0490074 -3.9723299 -3.9262416 -3.9004583 -3.8847203 -3.8972437 -3.9522676 -4.0007095 -4.0464897 -4.1018505 -4.1421146 -4.1703711 -4.193665][-4.2325168 -4.1819549 -4.1126094 -4.05535 -4.0245471 -4.0156283 -4.01804 -4.0380869 -4.082685 -4.1163645 -4.1489716 -4.1891365 -4.2174187 -4.2376895 -4.2518311][-4.2786412 -4.2510238 -4.2066 -4.1705313 -4.156045 -4.1571555 -4.1657624 -4.1849718 -4.2155395 -4.2373314 -4.257092 -4.2820864 -4.2980504 -4.3055754 -4.3059225][-4.3166041 -4.3071742 -4.2825675 -4.2643919 -4.2603397 -4.2634287 -4.2721858 -4.2883291 -4.3085122 -4.323195 -4.3372579 -4.3507543 -4.3541765 -4.3468928 -4.332324][-4.3318772 -4.3328276 -4.3192248 -4.3105946 -4.31208 -4.3165522 -4.3257933 -4.3384418 -4.3514886 -4.3616858 -4.3693876 -4.3731422 -4.3680115 -4.3511825 -4.3285141][-4.3193278 -4.3264761 -4.3189745 -4.3155293 -4.3195148 -4.3248711 -4.3337216 -4.3425875 -4.3498611 -4.35493 -4.3570409 -4.3555622 -4.3471327 -4.3268442 -4.3022842][-4.2890835 -4.2958851 -4.2880025 -4.2833381 -4.2855129 -4.2896647 -4.2975678 -4.3042054 -4.3088174 -4.3120503 -4.313128 -4.311307 -4.3041224 -4.2872186 -4.2664618]]...]
INFO - root - 2017-12-05 18:48:32.095519: step 34410, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 71h:25m:17s remains)
INFO - root - 2017-12-05 18:48:40.692223: step 34420, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 69h:39m:35s remains)
INFO - root - 2017-12-05 18:48:49.276507: step 34430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 69h:56m:34s remains)
INFO - root - 2017-12-05 18:48:57.825394: step 34440, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 72h:52m:19s remains)
INFO - root - 2017-12-05 18:49:06.484476: step 34450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 69h:38m:37s remains)
INFO - root - 2017-12-05 18:49:15.103038: step 34460, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 74h:19m:55s remains)
INFO - root - 2017-12-05 18:49:23.741620: step 34470, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 73h:05m:18s remains)
INFO - root - 2017-12-05 18:49:32.342701: step 34480, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 70h:51m:12s remains)
INFO - root - 2017-12-05 18:49:40.845929: step 34490, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 71h:21m:03s remains)
INFO - root - 2017-12-05 18:49:49.448773: step 34500, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:50m:38s remains)
2017-12-05 18:49:50.209429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2163243 -4.2318125 -4.2337341 -4.2288656 -4.2212124 -4.2113981 -4.2078228 -4.1976943 -4.17517 -4.1554241 -4.1692243 -4.1973114 -4.2163272 -4.2212915 -4.2170472][-4.19199 -4.2094178 -4.2171893 -4.2164936 -4.2124453 -4.2055597 -4.2008767 -4.1902523 -4.1706295 -4.1540718 -4.1730657 -4.2069111 -4.2295294 -4.2382274 -4.2378821][-4.1642122 -4.185833 -4.20135 -4.2054038 -4.203577 -4.19397 -4.182333 -4.1703305 -4.1542821 -4.1438284 -4.1657834 -4.2001119 -4.2208533 -4.2283835 -4.2284904][-4.1349792 -4.1589136 -4.1794462 -4.1865654 -4.1854239 -4.1717186 -4.1562357 -4.145205 -4.1310115 -4.1291904 -4.1570039 -4.1883707 -4.2032037 -4.2099729 -4.209517][-4.1190453 -4.143712 -4.1644573 -4.1687641 -4.1645913 -4.1474156 -4.1280241 -4.1145988 -4.1004367 -4.1068735 -4.1375771 -4.163619 -4.1753607 -4.179678 -4.1728044][-4.1214547 -4.145997 -4.164248 -4.1638093 -4.1541433 -4.1319795 -4.1073513 -4.0876527 -4.0713973 -4.0831242 -4.1140265 -4.1340551 -4.1429381 -4.1445141 -4.136445][-4.1276717 -4.1565971 -4.1754 -4.1737113 -4.1593084 -4.1320505 -4.1046352 -4.0776014 -4.0572562 -4.0678172 -4.0918221 -4.1034603 -4.1028733 -4.0975981 -4.0919008][-4.1291962 -4.1628904 -4.1848121 -4.1869431 -4.1718445 -4.1435328 -4.1152544 -4.0841646 -4.0545068 -4.0534244 -4.0645523 -4.0649443 -4.0547447 -4.0483904 -4.047781][-4.1182423 -4.158627 -4.185102 -4.1887589 -4.1746092 -4.1480994 -4.1262393 -4.0989552 -4.0647087 -4.0502939 -4.0491633 -4.0395103 -4.0249186 -4.0184093 -4.0191383][-4.1123075 -4.1545506 -4.1804423 -4.183033 -4.1690373 -4.1433139 -4.1232204 -4.1024051 -4.073051 -4.0551357 -4.0523658 -4.0436554 -4.0333767 -4.0271482 -4.0261731][-4.117866 -4.1572642 -4.1753149 -4.1754522 -4.1619878 -4.1384773 -4.118928 -4.1037769 -4.0830259 -4.0702052 -4.0754695 -4.0749006 -4.0701189 -4.0645366 -4.0645251][-4.1254916 -4.1579385 -4.1699591 -4.1667266 -4.1565609 -4.1396666 -4.1245661 -4.1131124 -4.0930452 -4.0776873 -4.0784535 -4.0798078 -4.0839176 -4.0899305 -4.0955458][-4.1237926 -4.1467123 -4.1560307 -4.1535478 -4.1462607 -4.1340265 -4.1230154 -4.121233 -4.1063128 -4.0840011 -4.0732203 -4.0737944 -4.085885 -4.1012564 -4.1105051][-4.1142912 -4.1252613 -4.1293173 -4.1273961 -4.1187725 -4.1026039 -4.0951123 -4.1118479 -4.1159329 -4.0997772 -4.0846272 -4.0803175 -4.0905848 -4.1081548 -4.1144686][-4.1015477 -4.0980644 -4.0968552 -4.0973816 -4.0847354 -4.0550175 -4.0487251 -4.0857258 -4.114924 -4.1163483 -4.1095085 -4.1034389 -4.1143575 -4.1346297 -4.142386]]...]
INFO - root - 2017-12-05 18:49:58.620175: step 34510, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 69h:05m:44s remains)
INFO - root - 2017-12-05 18:50:07.140165: step 34520, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 71h:59m:36s remains)
INFO - root - 2017-12-05 18:50:15.749718: step 34530, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 69h:42m:59s remains)
INFO - root - 2017-12-05 18:50:24.357535: step 34540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:49m:24s remains)
INFO - root - 2017-12-05 18:50:32.916803: step 34550, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 70h:27m:56s remains)
INFO - root - 2017-12-05 18:50:41.453880: step 34560, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 70h:15m:11s remains)
INFO - root - 2017-12-05 18:50:49.828388: step 34570, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 72h:24m:42s remains)
INFO - root - 2017-12-05 18:50:58.455194: step 34580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 71h:28m:05s remains)
INFO - root - 2017-12-05 18:51:07.067720: step 34590, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 71h:18m:47s remains)
INFO - root - 2017-12-05 18:51:15.547860: step 34600, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 68h:24m:25s remains)
2017-12-05 18:51:16.411617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.297451 -4.3036571 -4.3048372 -4.3052373 -4.3077974 -4.3112454 -4.3142328 -4.3131528 -4.3094306 -4.3050246 -4.3018236 -4.3020353 -4.3082638 -4.31823 -4.3263741][-4.262486 -4.2680345 -4.2694821 -4.2728977 -4.2797132 -4.2864795 -4.2918954 -4.2916527 -4.2869048 -4.2808714 -4.2773223 -4.2766466 -4.2830267 -4.2983465 -4.31441][-4.2237449 -4.2270913 -4.2257652 -4.2291794 -4.2397261 -4.2503581 -4.256732 -4.2533336 -4.2460103 -4.2414756 -4.239253 -4.2390852 -4.244895 -4.266418 -4.2922597][-4.1961393 -4.1941 -4.1856155 -4.1869512 -4.2012196 -4.2152772 -4.2182107 -4.203826 -4.1892209 -4.1893368 -4.1932678 -4.19724 -4.2036109 -4.2316637 -4.2647028][-4.1677437 -4.1580539 -4.1424108 -4.1431856 -4.1613383 -4.1776924 -4.1703248 -4.1386943 -4.1155677 -4.1222796 -4.1375771 -4.1467934 -4.1549735 -4.1909437 -4.2307405][-4.1516523 -4.1340413 -4.1103029 -4.1077528 -4.1220222 -4.12846 -4.0967956 -4.0393953 -4.0077906 -4.0301609 -4.0687356 -4.0905628 -4.1057315 -4.1489539 -4.1927519][-4.1591291 -4.1362581 -4.1059084 -4.0979495 -4.099226 -4.0804586 -4.0129519 -3.920465 -3.8779979 -3.928803 -4.0038524 -4.0509582 -4.0788751 -4.124404 -4.1657853][-4.1734848 -4.1541348 -4.1246095 -4.1105294 -4.0918994 -4.0421619 -3.9408176 -3.8170075 -3.76644 -3.8500748 -3.9635448 -4.0395637 -4.0800567 -4.120553 -4.1522417][-4.1954403 -4.1865182 -4.1628752 -4.1449986 -4.1133585 -4.0508842 -3.9478464 -3.8301864 -3.7884727 -3.878047 -3.9934037 -4.0696859 -4.1063328 -4.13275 -4.1497316][-4.218092 -4.2212448 -4.2051592 -4.1871448 -4.1539025 -4.0999579 -4.0247512 -3.950978 -3.9367177 -4.0025053 -4.0843558 -4.1355186 -4.1544356 -4.1651626 -4.1681418][-4.2323184 -4.2442737 -4.2361484 -4.2219682 -4.1966195 -4.1620388 -4.1224313 -4.0928583 -4.0978293 -4.1366305 -4.1787724 -4.1983685 -4.19828 -4.1946387 -4.1864357][-4.2374234 -4.2545037 -4.2520061 -4.2444077 -4.2323174 -4.215744 -4.1999569 -4.1953554 -4.2082353 -4.2287827 -4.2453184 -4.248373 -4.2390089 -4.2271547 -4.2144938][-4.2312837 -4.2510185 -4.2550836 -4.2562485 -4.2553549 -4.2488046 -4.2423587 -4.247818 -4.2643123 -4.2773976 -4.2837915 -4.282402 -4.2714911 -4.257802 -4.24474][-4.2201166 -4.2391138 -4.2482576 -4.2544031 -4.2540965 -4.2465816 -4.2410078 -4.2495241 -4.2675543 -4.2789469 -4.2822862 -4.2792697 -4.2703657 -4.2602043 -4.2512918][-4.2192841 -4.2342834 -4.2438035 -4.2480707 -4.2424951 -4.2277207 -4.2159662 -4.2209988 -4.2379589 -4.2501917 -4.2549734 -4.2541866 -4.2506971 -4.2470751 -4.2438903]]...]
INFO - root - 2017-12-05 18:51:24.968895: step 34610, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 71h:44m:25s remains)
INFO - root - 2017-12-05 18:51:33.314827: step 34620, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 67h:24m:56s remains)
INFO - root - 2017-12-05 18:51:41.833313: step 34630, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 67h:43m:11s remains)
INFO - root - 2017-12-05 18:51:50.392176: step 34640, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 70h:14m:16s remains)
INFO - root - 2017-12-05 18:51:58.885409: step 34650, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 70h:16m:49s remains)
INFO - root - 2017-12-05 18:52:07.452661: step 34660, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.795 sec/batch; 65h:48m:06s remains)
INFO - root - 2017-12-05 18:52:15.897455: step 34670, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 69h:17m:53s remains)
INFO - root - 2017-12-05 18:52:24.417141: step 34680, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:47m:24s remains)
INFO - root - 2017-12-05 18:52:33.078455: step 34690, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.889 sec/batch; 73h:31m:43s remains)
INFO - root - 2017-12-05 18:52:41.594015: step 34700, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 69h:24m:34s remains)
2017-12-05 18:52:42.335553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.29178 -4.28763 -4.287621 -4.2911682 -4.2872553 -4.2749615 -4.2663379 -4.2656331 -4.2758474 -4.2926564 -4.3004975 -4.2973852 -4.2950983 -4.2967658 -4.3026137][-4.2780938 -4.2727942 -4.2747946 -4.2792344 -4.2685142 -4.2448478 -4.2236924 -4.2178802 -4.2333555 -4.2607131 -4.2741675 -4.27094 -4.2676573 -4.2690353 -4.2780023][-4.2552919 -4.2495832 -4.25406 -4.2607808 -4.2464094 -4.2114816 -4.1737423 -4.1577592 -4.1784434 -4.2200093 -4.2453184 -4.2447438 -4.2404323 -4.24072 -4.2494469][-4.2296314 -4.2214947 -4.2280617 -4.2385674 -4.2250938 -4.1812644 -4.1205544 -4.0894508 -4.1170421 -4.1815867 -4.2232094 -4.2279897 -4.2229319 -4.2207117 -4.2264624][-4.1995397 -4.1839137 -4.1904521 -4.2021265 -4.185492 -4.1250205 -4.0334578 -3.9854898 -4.0320415 -4.1328807 -4.2004375 -4.2173691 -4.2149258 -4.2130723 -4.2195253][-4.1637716 -4.138238 -4.1401 -4.1463904 -4.1116819 -4.0132809 -3.8793721 -3.8120084 -3.8942087 -4.0479517 -4.1528592 -4.1925859 -4.2078819 -4.2173052 -4.2281075][-4.1010146 -4.0636935 -4.0633063 -4.0674262 -4.0138125 -3.8785949 -3.7047513 -3.6212051 -3.7423985 -3.9460249 -4.0830836 -4.1513605 -4.1911759 -4.2168512 -4.2334471][-4.0223656 -3.9692528 -3.9689562 -3.9803329 -3.9355059 -3.805521 -3.6320014 -3.5508854 -3.6809702 -3.8906353 -4.0332632 -4.1157613 -4.1724281 -4.2120256 -4.2306104][-3.959594 -3.8913195 -3.8887026 -3.9077294 -3.8878427 -3.8015246 -3.6805785 -3.6230979 -3.7306435 -3.905699 -4.025866 -4.1022048 -4.1641688 -4.2100611 -4.227581][-3.9502733 -3.8732054 -3.8637521 -3.8837323 -3.8896136 -3.8540664 -3.7904139 -3.7625577 -3.8488369 -3.9824188 -4.0697279 -4.1246214 -4.1769228 -4.220994 -4.2352085][-4.0099192 -3.9332912 -3.9168377 -3.9313302 -3.9470565 -3.9451492 -3.9221115 -3.9112537 -3.9763105 -4.07591 -4.1382508 -4.1728311 -4.2106543 -4.2500572 -4.2638874][-4.1075435 -4.0459518 -4.0316558 -4.0433607 -4.0607886 -4.0691814 -4.062603 -4.0600777 -4.1030688 -4.1690035 -4.2106256 -4.2347045 -4.258831 -4.2856956 -4.2943635][-4.2053065 -4.1670055 -4.1587358 -4.1676631 -4.1824737 -4.19329 -4.1937737 -4.1966434 -4.2179036 -4.2530475 -4.2775865 -4.2932248 -4.3046813 -4.3171072 -4.3191152][-4.2736325 -4.2526078 -4.2503943 -4.2595663 -4.2713509 -4.2798924 -4.2822495 -4.284204 -4.2926946 -4.3081183 -4.3225746 -4.3335991 -4.3361893 -4.3372755 -4.3335881][-4.3114915 -4.3020196 -4.3031979 -4.3105354 -4.3178291 -4.3222561 -4.3230472 -4.3228521 -4.3225837 -4.3264909 -4.3333235 -4.340436 -4.3391724 -4.3371582 -4.3337264]]...]
INFO - root - 2017-12-05 18:52:50.904500: step 34710, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 71h:15m:44s remains)
INFO - root - 2017-12-05 18:52:59.555453: step 34720, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 71h:11m:37s remains)
INFO - root - 2017-12-05 18:53:07.861482: step 34730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 70h:10m:42s remains)
INFO - root - 2017-12-05 18:53:16.282057: step 34740, loss = 2.02, batch loss = 1.96 (9.7 examples/sec; 0.826 sec/batch; 68h:19m:44s remains)
INFO - root - 2017-12-05 18:53:24.919911: step 34750, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 68h:54m:12s remains)
INFO - root - 2017-12-05 18:53:33.545441: step 34760, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 71h:02m:49s remains)
INFO - root - 2017-12-05 18:53:42.029756: step 34770, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 71h:04m:27s remains)
INFO - root - 2017-12-05 18:53:50.711308: step 34780, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 72h:57m:22s remains)
INFO - root - 2017-12-05 18:53:59.144139: step 34790, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 69h:59m:57s remains)
INFO - root - 2017-12-05 18:54:07.842901: step 34800, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 71h:53m:10s remains)
2017-12-05 18:54:08.679363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1214161 -4.1245608 -4.1085982 -4.0924935 -4.0890903 -4.0931067 -4.1062727 -4.1083217 -4.0982485 -4.10629 -4.1267085 -4.1543217 -4.1734724 -4.1789546 -4.1681628][-4.0812654 -4.09152 -4.0835185 -4.0736527 -4.0745568 -4.0810914 -4.0954003 -4.1027832 -4.0988493 -4.109807 -4.1252427 -4.1462874 -4.1595764 -4.1607656 -4.1410441][-4.0972886 -4.1035848 -4.0964217 -4.0893846 -4.0875716 -4.0830588 -4.089149 -4.0985503 -4.10203 -4.121223 -4.1387753 -4.1565919 -4.1651888 -4.1623521 -4.144608][-4.13346 -4.1358814 -4.1293216 -4.1233468 -4.113234 -4.0918131 -4.0818768 -4.084702 -4.0917783 -4.1218333 -4.1516914 -4.1709967 -4.179141 -4.1757464 -4.1614833][-4.1596107 -4.16226 -4.157012 -4.1471977 -4.1259732 -4.0902491 -4.0640349 -4.0595512 -4.0759888 -4.1162972 -4.1560211 -4.17623 -4.185679 -4.1845188 -4.1715512][-4.1584511 -4.160635 -4.1538978 -4.1387062 -4.1059837 -4.0540533 -4.0034223 -3.9940379 -4.035234 -4.0930252 -4.1389122 -4.1609941 -4.1766376 -4.1814466 -4.1686258][-4.1423764 -4.1334105 -4.1180387 -4.0948219 -4.0471091 -3.96604 -3.8823631 -3.8833675 -3.9721379 -4.0571752 -4.113986 -4.1486678 -4.17614 -4.1908703 -4.180891][-4.1307354 -4.1069088 -4.0828805 -4.0576181 -4.000267 -3.89157 -3.7891331 -3.8155434 -3.9369884 -4.0324292 -4.0938673 -4.1410689 -4.1769867 -4.2007217 -4.1950536][-4.1353345 -4.1011128 -4.0784545 -4.0638633 -4.0187674 -3.9307952 -3.8623424 -3.8894813 -3.9783258 -4.0511351 -4.1055408 -4.1527433 -4.1885762 -4.212852 -4.2100506][-4.1353064 -4.098403 -4.0842862 -4.0836706 -4.0580959 -4.0074825 -3.9757147 -3.9877379 -4.0324306 -4.0811663 -4.12936 -4.172914 -4.2059226 -4.2232289 -4.216989][-4.1312613 -4.0950594 -4.0878272 -4.0962319 -4.0854006 -4.0600982 -4.0474391 -4.0462732 -4.0632887 -4.098866 -4.144958 -4.1848631 -4.211257 -4.2218318 -4.213655][-4.1476631 -4.1128807 -4.1026683 -4.1074324 -4.1048269 -4.0924125 -4.0875897 -4.08314 -4.0917892 -4.1208277 -4.1619954 -4.1961684 -4.2148213 -4.21869 -4.2141771][-4.1758628 -4.141716 -4.1230965 -4.1187983 -4.1185317 -4.1166663 -4.1201634 -4.1214261 -4.1282439 -4.1501822 -4.1814795 -4.2066379 -4.2169528 -4.2176838 -4.220161][-4.2074389 -4.1805496 -4.1590548 -4.14503 -4.1419826 -4.1469827 -4.1566591 -4.1625066 -4.1689186 -4.1844444 -4.2042317 -4.2188535 -4.22299 -4.2240481 -4.2297339][-4.2312922 -4.2129083 -4.1967368 -4.1843953 -4.1815653 -4.1871419 -4.1954565 -4.1990557 -4.2024951 -4.2111177 -4.2215576 -4.2286096 -4.2317896 -4.2346439 -4.2395887]]...]
INFO - root - 2017-12-05 18:54:17.252896: step 34810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:45m:03s remains)
INFO - root - 2017-12-05 18:54:25.804819: step 34820, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 69h:54m:52s remains)
INFO - root - 2017-12-05 18:54:34.277826: step 34830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 71h:29m:05s remains)
INFO - root - 2017-12-05 18:54:42.874293: step 34840, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.875 sec/batch; 72h:19m:32s remains)
INFO - root - 2017-12-05 18:54:51.481029: step 34850, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 71h:49m:22s remains)
INFO - root - 2017-12-05 18:55:00.097433: step 34860, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:46m:29s remains)
INFO - root - 2017-12-05 18:55:08.404745: step 34870, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 69h:16m:36s remains)
INFO - root - 2017-12-05 18:55:17.059133: step 34880, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 71h:19m:28s remains)
INFO - root - 2017-12-05 18:55:25.567238: step 34890, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 72h:01m:30s remains)
INFO - root - 2017-12-05 18:55:34.072030: step 34900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 71h:19m:33s remains)
2017-12-05 18:55:34.874557: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1560121 -4.1343141 -4.1106882 -4.0987582 -4.1233687 -4.1692052 -4.193881 -4.1851034 -4.1494994 -4.0953059 -4.0742569 -4.0984869 -4.1350584 -4.1747365 -4.2125068][-4.1806874 -4.1586289 -4.1379352 -4.1284471 -4.1519394 -4.1891875 -4.206099 -4.1917744 -4.1470156 -4.0785813 -4.0462484 -4.06822 -4.1099534 -4.1610289 -4.2094636][-4.2089591 -4.1847005 -4.1631269 -4.1547742 -4.1753564 -4.2043262 -4.2175889 -4.20493 -4.163878 -4.0996461 -4.0639038 -4.0809035 -4.1234202 -4.1777639 -4.22659][-4.2207842 -4.1917434 -4.1686153 -4.1613684 -4.17531 -4.195796 -4.2085676 -4.2067542 -4.1815047 -4.1348128 -4.1003551 -4.1088328 -4.1500297 -4.2037277 -4.2468824][-4.2250795 -4.1892619 -4.1596541 -4.1457 -4.1483393 -4.1602674 -4.1783671 -4.194087 -4.1900234 -4.1639485 -4.1343994 -4.1349788 -4.1689129 -4.2165842 -4.2540665][-4.2299695 -4.1884084 -4.1447635 -4.1158619 -4.1049552 -4.1121588 -4.1404028 -4.1739192 -4.1913915 -4.1835608 -4.1609983 -4.1510587 -4.1679144 -4.2039413 -4.2396417][-4.2336349 -4.1922789 -4.1385121 -4.0930882 -4.0672851 -4.073472 -4.1086674 -4.1535363 -4.1846275 -4.1899838 -4.1721816 -4.1526909 -4.1492429 -4.1675682 -4.2019076][-4.2337894 -4.199471 -4.1466641 -4.0942612 -4.0561686 -4.05588 -4.0909381 -4.1386242 -4.1742849 -4.1859365 -4.1706619 -4.1457953 -4.12651 -4.1290779 -4.1645613][-4.2275805 -4.2012258 -4.1581888 -4.112061 -4.076786 -4.0689921 -4.0902505 -4.1316175 -4.1660004 -4.1778221 -4.1626463 -4.1368566 -4.109931 -4.1049886 -4.1435575][-4.2142506 -4.1947308 -4.166718 -4.1371932 -4.1163588 -4.1073742 -4.108593 -4.1326737 -4.1624131 -4.1714635 -4.1588364 -4.1373253 -4.111165 -4.1063013 -4.1444826][-4.1928215 -4.1800313 -4.1664233 -4.15699 -4.1564245 -4.1557713 -4.1444554 -4.1511703 -4.1716685 -4.1780572 -4.1713991 -4.1597362 -4.1400704 -4.1355195 -4.1661448][-4.1606569 -4.1588211 -4.1571302 -4.1639118 -4.1824069 -4.1899338 -4.177042 -4.1775436 -4.1950507 -4.204535 -4.2100587 -4.2094564 -4.1931505 -4.1864371 -4.2008653][-4.1376872 -4.1447411 -4.1533203 -4.1707044 -4.1996174 -4.2090864 -4.1952291 -4.1951303 -4.2121544 -4.2293272 -4.2464485 -4.2524691 -4.2384424 -4.2315159 -4.2327642][-4.1406488 -4.1504273 -4.1649971 -4.1852422 -4.2129049 -4.2177305 -4.2009716 -4.2001266 -4.217207 -4.2416964 -4.2656341 -4.2729597 -4.2607217 -4.253139 -4.2470326][-4.1699934 -4.1759162 -4.1864119 -4.2019496 -4.2209558 -4.2180033 -4.2008805 -4.1991162 -4.2160735 -4.24336 -4.2689033 -4.2747436 -4.26245 -4.2525754 -4.2413487]]...]
INFO - root - 2017-12-05 18:55:43.477280: step 34910, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.896 sec/batch; 74h:05m:20s remains)
INFO - root - 2017-12-05 18:55:51.985608: step 34920, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:44m:45s remains)
INFO - root - 2017-12-05 18:56:00.627289: step 34930, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 69h:33m:36s remains)
INFO - root - 2017-12-05 18:56:09.086693: step 34940, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 70h:36m:29s remains)
INFO - root - 2017-12-05 18:56:17.633921: step 34950, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 71h:03m:31s remains)
INFO - root - 2017-12-05 18:56:26.325930: step 34960, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 72h:32m:33s remains)
INFO - root - 2017-12-05 18:56:34.697795: step 34970, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 68h:13m:20s remains)
INFO - root - 2017-12-05 18:56:43.402410: step 34980, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:54m:39s remains)
INFO - root - 2017-12-05 18:56:51.973364: step 34990, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 71h:12m:36s remains)
INFO - root - 2017-12-05 18:57:00.643764: step 35000, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 70h:13m:57s remains)
2017-12-05 18:57:01.399567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3543625 -4.356493 -4.3566489 -4.3549333 -4.3528643 -4.3517075 -4.3509979 -4.3507862 -4.3500276 -4.34851 -4.3470383 -4.3459821 -4.3413925 -4.3248825 -4.3029203][-4.3516431 -4.3536787 -4.3532906 -4.3504825 -4.3483257 -4.3473635 -4.3442473 -4.3383379 -4.3306303 -4.322907 -4.3206382 -4.3271146 -4.32905 -4.3111496 -4.284132][-4.3505435 -4.3503551 -4.3475838 -4.3437667 -4.3419485 -4.3401785 -4.3310404 -4.3160591 -4.29948 -4.28848 -4.291028 -4.3072939 -4.3159637 -4.2976952 -4.2617936][-4.3492441 -4.3450975 -4.3383536 -4.3332405 -4.3295841 -4.3225861 -4.303843 -4.2762442 -4.2528543 -4.2438393 -4.2528076 -4.2750554 -4.2903514 -4.2737317 -4.2333593][-4.3485765 -4.3393469 -4.3274512 -4.3175974 -4.3076482 -4.2904663 -4.2576466 -4.2153888 -4.191349 -4.1921148 -4.2105236 -4.2371612 -4.2590294 -4.2502074 -4.2143784][-4.3410749 -4.3275113 -4.3105078 -4.2942257 -4.273766 -4.2391858 -4.1845703 -4.1287055 -4.1119695 -4.1323829 -4.1659346 -4.2029676 -4.2331018 -4.2367353 -4.2127995][-4.3297534 -4.3119383 -4.288847 -4.2598634 -4.2212834 -4.164845 -4.08663 -4.024404 -4.028091 -4.0762048 -4.1336384 -4.1869931 -4.2253737 -4.2418127 -4.2323027][-4.318696 -4.2972889 -4.2645812 -4.2187572 -4.1589026 -4.0801525 -3.9763906 -3.9109497 -3.9430816 -4.0263057 -4.1089044 -4.1788192 -4.2254648 -4.249311 -4.248971][-4.3019414 -4.2787352 -4.2357869 -4.1747785 -4.0931377 -3.9925635 -3.8674769 -3.8069263 -3.8787866 -3.996613 -4.0991635 -4.1771774 -4.2189012 -4.2370338 -4.2401133][-4.2746854 -4.2550869 -4.2094111 -4.140646 -4.0499239 -3.9451897 -3.8281636 -3.7913253 -3.8919909 -4.0240307 -4.1284113 -4.1939545 -4.2119536 -4.2152505 -4.2207427][-4.24519 -4.2365451 -4.1988549 -4.13352 -4.0518403 -3.9694788 -3.8938925 -3.8870969 -3.9851589 -4.100245 -4.1801119 -4.216228 -4.2113838 -4.2041717 -4.2156329][-4.230794 -4.2362909 -4.2139859 -4.1642919 -4.1044183 -4.056427 -4.0258918 -4.0351777 -4.1054 -4.1801209 -4.2222538 -4.2317357 -4.2200441 -4.2175083 -4.2369709][-4.2336073 -4.2536345 -4.2443776 -4.2112837 -4.1725659 -4.1510034 -4.1487927 -4.1621304 -4.1999784 -4.2315512 -4.2400975 -4.2337446 -4.2302423 -4.2414894 -4.2698359][-4.2368789 -4.261158 -4.2578487 -4.2385674 -4.2184448 -4.2162466 -4.2296543 -4.2395806 -4.2470455 -4.2440791 -4.2316189 -4.2246966 -4.2364154 -4.2635221 -4.2998781][-4.2330742 -4.2492766 -4.2494659 -4.2424765 -4.2389665 -4.2512355 -4.2685218 -4.2677121 -4.252984 -4.2306657 -4.214458 -4.2197042 -4.2483516 -4.2866797 -4.3232188]]...]
INFO - root - 2017-12-05 18:57:10.102382: step 35010, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 71h:20m:47s remains)
INFO - root - 2017-12-05 18:57:18.684216: step 35020, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.909 sec/batch; 75h:08m:28s remains)
INFO - root - 2017-12-05 18:57:27.250465: step 35030, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 71h:30m:32s remains)
INFO - root - 2017-12-05 18:57:35.762292: step 35040, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 71h:21m:55s remains)
INFO - root - 2017-12-05 18:57:44.256008: step 35050, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 71h:43m:52s remains)
INFO - root - 2017-12-05 18:57:52.868029: step 35060, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 70h:27m:19s remains)
INFO - root - 2017-12-05 18:58:01.373148: step 35070, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 70h:06m:07s remains)
INFO - root - 2017-12-05 18:58:09.895873: step 35080, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 72h:39m:16s remains)
INFO - root - 2017-12-05 18:58:18.477188: step 35090, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 72h:17m:10s remains)
INFO - root - 2017-12-05 18:58:27.009895: step 35100, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 72h:51m:33s remains)
2017-12-05 18:58:27.791038: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2537136 -4.25251 -4.2547922 -4.2588453 -4.2641835 -4.2663326 -4.2625327 -4.2611609 -4.2643065 -4.2718086 -4.2834377 -4.2894716 -4.2910833 -4.2838922 -4.272089][-4.2270503 -4.232316 -4.2459478 -4.2593126 -4.267293 -4.2695265 -4.2643614 -4.2602272 -4.26056 -4.2680869 -4.2815747 -4.288353 -4.288558 -4.2799644 -4.2699375][-4.2011285 -4.2133813 -4.2385697 -4.2612591 -4.2700367 -4.2707043 -4.2648711 -4.25718 -4.2551007 -4.2641454 -4.2814827 -4.2873578 -4.2837844 -4.2723732 -4.2649074][-4.1725736 -4.1869159 -4.218504 -4.247581 -4.262084 -4.2639213 -4.2583404 -4.2475915 -4.2425971 -4.2541132 -4.2740245 -4.2775259 -4.270041 -4.259439 -4.2566252][-4.1387258 -4.1553187 -4.1916242 -4.2223849 -4.23851 -4.2390761 -4.226192 -4.2090235 -4.1985354 -4.2165089 -4.248487 -4.2578516 -4.2558546 -4.253293 -4.2552657][-4.1039953 -4.1169438 -4.1582527 -4.193985 -4.2122326 -4.2058463 -4.170331 -4.1284389 -4.10927 -4.1412439 -4.1973977 -4.2250729 -4.2377977 -4.2481389 -4.253108][-4.0824466 -4.078362 -4.1144819 -4.152597 -4.1746817 -4.1616278 -4.0966659 -4.0167418 -3.9907613 -4.0484233 -4.1337662 -4.182663 -4.2104311 -4.2291765 -4.2360973][-4.0937452 -4.0638661 -4.0830364 -4.1170173 -4.1429329 -4.1350532 -4.0552192 -3.9414034 -3.9076352 -3.9799201 -4.08207 -4.1422405 -4.1783791 -4.2024226 -4.2135916][-4.1252069 -4.0809064 -4.0814896 -4.1099839 -4.1395273 -4.1430173 -4.0754333 -3.9648519 -3.9222283 -3.9802694 -4.07341 -4.1324916 -4.1679082 -4.19101 -4.2086573][-4.1493745 -4.1075506 -4.1020617 -4.1284289 -4.159863 -4.1751738 -4.1368213 -4.0605936 -4.0192351 -4.0475826 -4.1141896 -4.1606808 -4.1832619 -4.1975813 -4.2156196][-4.1471987 -4.1186748 -4.1207824 -4.1488237 -4.1785765 -4.2016306 -4.1912723 -4.147305 -4.1113162 -4.113934 -4.155458 -4.1911077 -4.2049937 -4.2127085 -4.230041][-4.1380754 -4.1204014 -4.1307898 -4.1604543 -4.1892118 -4.2127171 -4.2194791 -4.1958051 -4.1645184 -4.1508527 -4.1733432 -4.2017651 -4.2137566 -4.2224884 -4.2401161][-4.1434669 -4.1320291 -4.1455021 -4.1729822 -4.1996875 -4.2191372 -4.227828 -4.2137456 -4.1879392 -4.1675825 -4.1762924 -4.1976886 -4.214076 -4.2308111 -4.2470431][-4.1680765 -4.1555648 -4.1634316 -4.181459 -4.2029362 -4.2179132 -4.2230906 -4.2131152 -4.19449 -4.1778073 -4.1800828 -4.1959791 -4.2151604 -4.2364879 -4.2500405][-4.2051287 -4.1878843 -4.1862879 -4.1933932 -4.2064624 -4.2163949 -4.21965 -4.2138038 -4.2044468 -4.1978774 -4.1997681 -4.209178 -4.2221632 -4.2371874 -4.2458634]]...]
INFO - root - 2017-12-05 18:58:36.283493: step 35110, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 70h:06m:01s remains)
INFO - root - 2017-12-05 18:58:44.681802: step 35120, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 71h:20m:26s remains)
INFO - root - 2017-12-05 18:58:53.280388: step 35130, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 71h:44m:08s remains)
INFO - root - 2017-12-05 18:59:01.934536: step 35140, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 71h:19m:45s remains)
INFO - root - 2017-12-05 18:59:10.494902: step 35150, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 70h:19m:08s remains)
INFO - root - 2017-12-05 18:59:19.051319: step 35160, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 69h:35m:06s remains)
INFO - root - 2017-12-05 18:59:27.476740: step 35170, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 72h:17m:41s remains)
INFO - root - 2017-12-05 18:59:36.056453: step 35180, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 69h:45m:45s remains)
INFO - root - 2017-12-05 18:59:44.661398: step 35190, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 72h:53m:47s remains)
INFO - root - 2017-12-05 18:59:53.239445: step 35200, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 71h:19m:41s remains)
2017-12-05 18:59:53.986533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2944932 -4.2928519 -4.2945495 -4.297586 -4.3031044 -4.3061628 -4.3041558 -4.3010807 -4.3027506 -4.3068442 -4.3111215 -4.3146892 -4.3174839 -4.3209758 -4.3267369][-4.2666793 -4.2610903 -4.2596211 -4.2624359 -4.2680535 -4.2707953 -4.268167 -4.2664924 -4.273818 -4.2828975 -4.2912521 -4.2947249 -4.2972145 -4.3035378 -4.3145523][-4.2127481 -4.2037249 -4.2004633 -4.203373 -4.2072463 -4.2085142 -4.2047143 -4.2099576 -4.2282805 -4.2467194 -4.2618847 -4.266223 -4.2692785 -4.2803969 -4.2989087][-4.1541648 -4.1458826 -4.1427884 -4.1436672 -4.142777 -4.1351705 -4.125567 -4.1389384 -4.1729617 -4.2019968 -4.2205868 -4.2258844 -4.2313938 -4.2480016 -4.2766657][-4.1328754 -4.127944 -4.1202812 -4.1099281 -4.0930448 -4.0663762 -4.0419579 -4.0627956 -4.1146874 -4.1539664 -4.1724391 -4.176228 -4.1838188 -4.207531 -4.2512259][-4.1356263 -4.1260891 -4.1061358 -4.0759788 -4.0355272 -3.9789872 -3.9304097 -3.96233 -4.0400085 -4.0970721 -4.1187963 -4.1211424 -4.1308622 -4.1650825 -4.224349][-4.14555 -4.1226492 -4.0820127 -4.0243697 -3.9524882 -3.8582017 -3.7910728 -3.8585956 -3.9774528 -4.0552793 -4.085587 -4.089458 -4.1002674 -4.1423759 -4.2098808][-4.165637 -4.1299238 -4.0732207 -3.9921756 -3.8941607 -3.7727923 -3.7176502 -3.8414955 -3.993072 -4.0761938 -4.1049361 -4.1032767 -4.1071758 -4.146142 -4.211617][-4.1679897 -4.1331186 -4.0834036 -4.0155063 -3.9401753 -3.8606231 -3.8475738 -3.9652376 -4.0887356 -4.1442761 -4.1531591 -4.1356306 -4.123383 -4.1545558 -4.217977][-4.1668234 -4.1487718 -4.1234751 -4.08934 -4.0577836 -4.0323529 -4.0391674 -4.1053739 -4.1734681 -4.1949377 -4.1790538 -4.1437488 -4.1203809 -4.1478977 -4.2153578][-4.1713796 -4.1717668 -4.1659479 -4.1583495 -4.1523609 -4.1543169 -4.1615939 -4.1828313 -4.2070503 -4.2017512 -4.1664119 -4.1227174 -4.0984187 -4.1294489 -4.2043858][-4.1779504 -4.1895242 -4.1954885 -4.1996555 -4.2032413 -4.2095618 -4.2086244 -4.2071018 -4.2053356 -4.1830254 -4.1393337 -4.1004586 -4.088274 -4.1265683 -4.2049279][-4.1985579 -4.211937 -4.2162104 -4.217732 -4.2183814 -4.2175827 -4.2059121 -4.1941009 -4.1835489 -4.1622648 -4.1304808 -4.1086531 -4.1099939 -4.150806 -4.2239337][-4.2251506 -4.2287426 -4.2214284 -4.213872 -4.2041931 -4.1945729 -4.1799808 -4.1699996 -4.1679173 -4.1602964 -4.1478329 -4.140553 -4.149312 -4.1887503 -4.251997][-4.2279353 -4.219945 -4.2045407 -4.1885428 -4.1740732 -4.1652794 -4.1562228 -4.1554446 -4.1656294 -4.1730161 -4.1750922 -4.1774712 -4.1933775 -4.22924 -4.2818356]]...]
INFO - root - 2017-12-05 19:00:02.645275: step 35210, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.903 sec/batch; 74h:32m:18s remains)
INFO - root - 2017-12-05 19:00:11.335395: step 35220, loss = 2.05, batch loss = 2.00 (8.8 examples/sec; 0.912 sec/batch; 75h:17m:47s remains)
INFO - root - 2017-12-05 19:00:19.751274: step 35230, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:58m:33s remains)
INFO - root - 2017-12-05 19:00:28.338509: step 35240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 71h:09m:37s remains)
INFO - root - 2017-12-05 19:00:36.894760: step 35250, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 68h:43m:38s remains)
INFO - root - 2017-12-05 19:00:45.342686: step 35260, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 68h:47m:23s remains)
INFO - root - 2017-12-05 19:00:53.765096: step 35270, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.830 sec/batch; 68h:32m:54s remains)
INFO - root - 2017-12-05 19:01:02.311793: step 35280, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 71h:32m:42s remains)
INFO - root - 2017-12-05 19:01:10.839573: step 35290, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 72h:47m:20s remains)
INFO - root - 2017-12-05 19:01:19.400566: step 35300, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.828 sec/batch; 68h:22m:03s remains)
2017-12-05 19:01:20.263183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2408037 -4.2499251 -4.2669058 -4.2846727 -4.2853503 -4.2717261 -4.2567163 -4.243834 -4.2235041 -4.19965 -4.1830139 -4.1595707 -4.1316795 -4.0905304 -4.0409617][-4.23877 -4.2475057 -4.2580576 -4.26975 -4.26917 -4.2564306 -4.2444935 -4.2410431 -4.2356381 -4.2216072 -4.2060814 -4.179956 -4.1491275 -4.11188 -4.0705857][-4.2303333 -4.2340269 -4.2319155 -4.2322779 -4.2280197 -4.217032 -4.2096982 -4.2152066 -4.2281332 -4.2337742 -4.2297068 -4.2145181 -4.1965704 -4.1782508 -4.1582932][-4.2160921 -4.2094979 -4.1884627 -4.164875 -4.1438046 -4.121335 -4.1058836 -4.1126609 -4.1501107 -4.1894226 -4.2138839 -4.2265997 -4.2342811 -4.2395625 -4.2372217][-4.218544 -4.1958046 -4.147841 -4.0898886 -4.0374441 -3.9866631 -3.9426558 -3.94305 -4.0095854 -4.090425 -4.1451883 -4.1837687 -4.2207432 -4.2502332 -4.2599907][-4.2230649 -4.1833191 -4.1087246 -4.0145807 -3.9163923 -3.8206048 -3.736824 -3.7297843 -3.8288367 -3.9496126 -4.0328603 -4.0944219 -4.1599774 -4.2117324 -4.2314916][-4.2256565 -4.1712527 -4.0821009 -3.9677422 -3.84521 -3.7262998 -3.622366 -3.6130507 -3.7220812 -3.857276 -3.9539139 -4.0309296 -4.1122532 -4.179543 -4.2098546][-4.2164345 -4.1687431 -4.09271 -3.9983897 -3.8993735 -3.8107047 -3.7422132 -3.7385054 -3.8125687 -3.9139023 -3.9933977 -4.0636473 -4.1351585 -4.1969495 -4.2284379][-4.2136693 -4.1878004 -4.1395988 -4.0819726 -4.0252151 -3.9800923 -3.9469395 -3.9419582 -3.9824359 -4.0440521 -4.0978241 -4.1503243 -4.2024508 -4.2476263 -4.2741909][-4.2176352 -4.208734 -4.1858168 -4.1581035 -4.1367154 -4.1247892 -4.11786 -4.1139579 -4.1324091 -4.1632013 -4.1942005 -4.2296996 -4.2615337 -4.2879667 -4.3048391][-4.2079091 -4.2030249 -4.1934729 -4.18723 -4.1931844 -4.203794 -4.2140036 -4.2145977 -4.2207804 -4.2319384 -4.2471285 -4.2688355 -4.2808576 -4.2865796 -4.2910104][-4.1700234 -4.1624961 -4.1586881 -4.1661568 -4.1915107 -4.2188711 -4.23864 -4.2429452 -4.2407961 -4.2406487 -4.2436476 -4.249856 -4.2463918 -4.2353921 -4.225924][-4.1187372 -4.1096497 -4.1128383 -4.1304107 -4.1658778 -4.2043948 -4.2336874 -4.2384086 -4.228013 -4.2196074 -4.216145 -4.2145658 -4.202291 -4.1743236 -4.1497827][-4.0983915 -4.0846596 -4.0912032 -4.1142683 -4.156395 -4.20251 -4.2333035 -4.2371836 -4.2280827 -4.2187262 -4.2132616 -4.20929 -4.1946321 -4.1612911 -4.1313968][-4.1407914 -4.1213174 -4.1252842 -4.1469936 -4.1867261 -4.2293262 -4.2570658 -4.2618752 -4.2577353 -4.250493 -4.2441034 -4.2413449 -4.2328739 -4.2081919 -4.1824293]]...]
INFO - root - 2017-12-05 19:01:28.905080: step 35310, loss = 2.08, batch loss = 2.03 (8.0 examples/sec; 0.999 sec/batch; 82h:26m:55s remains)
INFO - root - 2017-12-05 19:01:37.422625: step 35320, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.838 sec/batch; 69h:08m:23s remains)
INFO - root - 2017-12-05 19:01:46.055184: step 35330, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 71h:32m:37s remains)
INFO - root - 2017-12-05 19:01:54.529163: step 35340, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 70h:38m:06s remains)
INFO - root - 2017-12-05 19:02:03.106783: step 35350, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:38m:40s remains)
INFO - root - 2017-12-05 19:02:11.634687: step 35360, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 71h:27m:01s remains)
INFO - root - 2017-12-05 19:02:20.085756: step 35370, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 70h:59m:51s remains)
INFO - root - 2017-12-05 19:02:28.688235: step 35380, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 70h:31m:05s remains)
INFO - root - 2017-12-05 19:02:37.188719: step 35390, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 70h:03m:39s remains)
INFO - root - 2017-12-05 19:02:45.823813: step 35400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 70h:03m:45s remains)
2017-12-05 19:02:46.550787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1859522 -4.1801004 -4.1732049 -4.1962037 -4.2326331 -4.2447085 -4.2438068 -4.2479329 -4.25493 -4.2604561 -4.2626514 -4.2596951 -4.2564096 -4.2527323 -4.2511334][-4.2124319 -4.2054005 -4.1955371 -4.2126145 -4.2419906 -4.2503214 -4.2468705 -4.2518215 -4.26317 -4.2763047 -4.2843213 -4.2810707 -4.2765875 -4.2729611 -4.2729106][-4.2446585 -4.2356639 -4.2211676 -4.2265067 -4.2424574 -4.2423539 -4.2341566 -4.241293 -4.2609534 -4.2879844 -4.3034782 -4.299129 -4.2906213 -4.2853327 -4.2844305][-4.2600718 -4.2533116 -4.2386456 -4.2346921 -4.2401466 -4.2311668 -4.2135797 -4.2163529 -4.238656 -4.2809873 -4.3058472 -4.3014441 -4.2889266 -4.2827129 -4.2835784][-4.2514167 -4.2471809 -4.2351289 -4.2252917 -4.220952 -4.201364 -4.1718254 -4.159842 -4.1765323 -4.236815 -4.279448 -4.2790213 -4.26812 -4.2673097 -4.2742605][-4.2374787 -4.2337465 -4.2264085 -4.2178578 -4.2026482 -4.165298 -4.1185117 -4.0864449 -4.0957303 -4.1787624 -4.243515 -4.2510695 -4.2421255 -4.2464352 -4.2606754][-4.2067108 -4.202 -4.1993151 -4.1978765 -4.1786981 -4.1268525 -4.063684 -4.003139 -3.9997067 -4.1088734 -4.1966805 -4.2163591 -4.2156472 -4.2261391 -4.2461877][-4.1738839 -4.1659203 -4.1629553 -4.1667557 -4.1520848 -4.0941 -4.0177059 -3.9265311 -3.9059489 -4.0380917 -4.1468706 -4.1801248 -4.1909285 -4.2041273 -4.2258453][-4.1852703 -4.1781936 -4.1743975 -4.1833692 -4.1793342 -4.1345954 -4.0736604 -3.998462 -3.9700444 -4.0656681 -4.1566772 -4.18712 -4.2008677 -4.2152719 -4.2318916][-4.205739 -4.20334 -4.2005916 -4.2151866 -4.2253127 -4.1995454 -4.1656213 -4.1262779 -4.1014981 -4.1472936 -4.2020698 -4.2221928 -4.2354307 -4.2531137 -4.2710633][-4.2130351 -4.2090611 -4.2065196 -4.2280273 -4.2463946 -4.2310638 -4.2148008 -4.1981449 -4.176228 -4.191895 -4.2213573 -4.2371831 -4.2550468 -4.2776513 -4.3005114][-4.2144909 -4.2044263 -4.2020917 -4.2306194 -4.2555861 -4.2479463 -4.2411203 -4.2359886 -4.2176657 -4.2153215 -4.227675 -4.2355294 -4.2504015 -4.2721334 -4.296505][-4.2114711 -4.1938143 -4.1884046 -4.2205367 -4.247263 -4.2422643 -4.238656 -4.2438259 -4.2377577 -4.2332182 -4.2352853 -4.23517 -4.2438025 -4.2602625 -4.2809124][-4.2116137 -4.1910286 -4.1815228 -4.2107062 -4.2359238 -4.23081 -4.227448 -4.2387943 -4.2454796 -4.2453737 -4.2440934 -4.2433677 -4.2470489 -4.2604818 -4.2743511][-4.2207546 -4.2047606 -4.1954551 -4.21991 -4.2385082 -4.2288694 -4.2210121 -4.2317905 -4.2421522 -4.2405434 -4.2390881 -4.2424092 -4.2481341 -4.2590017 -4.2683382]]...]
INFO - root - 2017-12-05 19:02:55.189587: step 35410, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 68h:44m:02s remains)
INFO - root - 2017-12-05 19:03:03.814046: step 35420, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 71h:35m:06s remains)
INFO - root - 2017-12-05 19:03:12.436054: step 35430, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.838 sec/batch; 69h:06m:45s remains)
INFO - root - 2017-12-05 19:03:20.998560: step 35440, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 69h:58m:00s remains)
INFO - root - 2017-12-05 19:03:29.592617: step 35450, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 71h:45m:23s remains)
INFO - root - 2017-12-05 19:03:38.222294: step 35460, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 71h:35m:05s remains)
INFO - root - 2017-12-05 19:03:46.628476: step 35470, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:49m:57s remains)
INFO - root - 2017-12-05 19:03:55.142426: step 35480, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 70h:20m:56s remains)
INFO - root - 2017-12-05 19:04:03.743584: step 35490, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 70h:31m:20s remains)
INFO - root - 2017-12-05 19:04:12.298616: step 35500, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.883 sec/batch; 72h:52m:56s remains)
2017-12-05 19:04:13.058311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3438807 -4.3412061 -4.3399329 -4.3423982 -4.3453317 -4.3455815 -4.3428411 -4.3375797 -4.3367767 -4.3435588 -4.3524895 -4.357223 -4.3568716 -4.3536077 -4.3494844][-4.3214936 -4.3225503 -4.3242478 -4.3274221 -4.3286743 -4.3250256 -4.317862 -4.3095274 -4.3103142 -4.3237929 -4.3408089 -4.3522606 -4.3551769 -4.35282 -4.3489976][-4.2677708 -4.2814817 -4.2924271 -4.3001904 -4.2991328 -4.284658 -4.2626185 -4.2436676 -4.2450085 -4.2717357 -4.3057756 -4.3325138 -4.3443513 -4.3452249 -4.3407235][-4.1826115 -4.2192583 -4.2469606 -4.263298 -4.257863 -4.2237372 -4.1728492 -4.1299715 -4.1277776 -4.1745076 -4.2378926 -4.29096 -4.3194427 -4.3262782 -4.3194518][-4.0870609 -4.1481977 -4.1953759 -4.2214608 -4.2091675 -4.1473989 -4.05878 -3.9845343 -3.9783154 -4.0523725 -4.1538348 -4.2392635 -4.2849736 -4.296545 -4.2824745][-4.0205708 -4.0973639 -4.1572461 -4.1865463 -4.1640534 -4.0782619 -3.9605312 -3.8645868 -3.8600748 -3.9591646 -4.091783 -4.2005072 -4.2567682 -4.267211 -4.240479][-4.0140491 -4.0909138 -4.1499114 -4.1753531 -4.1468468 -4.0564871 -3.9389808 -3.8501611 -3.8559244 -3.9567928 -4.0875869 -4.1951652 -4.2516308 -4.2579513 -4.2188582][-4.0808558 -4.1393018 -4.1809864 -4.1952066 -4.1675754 -4.094902 -4.0089059 -3.951721 -3.9649782 -4.0438771 -4.144031 -4.2294903 -4.2767181 -4.2795792 -4.2374239][-4.1858935 -4.2189608 -4.2364426 -4.2383285 -4.217186 -4.1713042 -4.1241565 -4.0983171 -4.1134491 -4.1632228 -4.226048 -4.2829037 -4.3160906 -4.318635 -4.2870522][-4.2800031 -4.2917233 -4.2904096 -4.2861738 -4.2738795 -4.2514648 -4.2327051 -4.225605 -4.2369146 -4.2610354 -4.2932744 -4.3268952 -4.3488407 -4.3521681 -4.3360572][-4.3371587 -4.33654 -4.3284774 -4.3234739 -4.3186216 -4.310533 -4.3060718 -4.305346 -4.3093953 -4.3152733 -4.3263626 -4.3443346 -4.3596268 -4.3663754 -4.3634157][-4.361496 -4.3570724 -4.3512568 -4.3499331 -4.3507953 -4.3505363 -4.3499818 -4.34722 -4.3428245 -4.3371048 -4.3347807 -4.3423929 -4.3540397 -4.36334 -4.36836][-4.3625245 -4.35573 -4.3534837 -4.3577056 -4.3637619 -4.3681288 -4.3691092 -4.3633237 -4.3525658 -4.34 -4.3302484 -4.3308287 -4.3395481 -4.3503203 -4.3593268][-4.3497634 -4.3393145 -4.33695 -4.3438191 -4.3549232 -4.3654785 -4.3687606 -4.363421 -4.3516436 -4.3379979 -4.32644 -4.3226094 -4.3275657 -4.3368106 -4.3464341][-4.3180785 -4.3039374 -4.3011127 -4.3114309 -4.3290634 -4.3454795 -4.3527331 -4.3499737 -4.341486 -4.3311625 -4.3220673 -4.3169956 -4.318635 -4.3260851 -4.3358808]]...]
INFO - root - 2017-12-05 19:04:21.635392: step 35510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:24m:03s remains)
INFO - root - 2017-12-05 19:04:30.156867: step 35520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 71h:51m:49s remains)
INFO - root - 2017-12-05 19:04:38.823859: step 35530, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 71h:29m:41s remains)
INFO - root - 2017-12-05 19:04:47.362871: step 35540, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 71h:23m:05s remains)
INFO - root - 2017-12-05 19:04:55.955082: step 35550, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:45m:07s remains)
INFO - root - 2017-12-05 19:05:04.564546: step 35560, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 71h:00m:31s remains)
INFO - root - 2017-12-05 19:05:13.167070: step 35570, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 72h:25m:41s remains)
INFO - root - 2017-12-05 19:05:21.563997: step 35580, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 67h:41m:26s remains)
INFO - root - 2017-12-05 19:05:30.031597: step 35590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 69h:36m:35s remains)
INFO - root - 2017-12-05 19:05:38.615277: step 35600, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 70h:55m:55s remains)
2017-12-05 19:05:39.390732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2996507 -4.2836962 -4.2557688 -4.2217875 -4.1859717 -4.1452556 -4.11347 -4.1191959 -4.1547823 -4.194253 -4.2394338 -4.2833204 -4.3204718 -4.3376918 -4.3365636][-4.29639 -4.2767911 -4.2469382 -4.2121387 -4.1741457 -4.125493 -4.0812669 -4.0804806 -4.1196637 -4.1673021 -4.221724 -4.2752247 -4.3202119 -4.3390918 -4.3379297][-4.2951708 -4.2732391 -4.2414966 -4.2035108 -4.158186 -4.1003957 -4.0444531 -4.0384722 -4.0877686 -4.14832 -4.2124968 -4.2732468 -4.3221655 -4.3402762 -4.3378096][-4.29393 -4.2713695 -4.237175 -4.1940126 -4.1381817 -4.0681491 -4.0024028 -3.9927936 -4.0560818 -4.1362262 -4.2113576 -4.2778397 -4.32397 -4.3401146 -4.3366604][-4.2900763 -4.268734 -4.2338572 -4.1886778 -4.1257687 -4.0461287 -3.9730172 -3.9540854 -4.02624 -4.1255631 -4.2107797 -4.2800059 -4.3226423 -4.3375 -4.3337345][-4.2848606 -4.2661796 -4.2334037 -4.188498 -4.1219344 -4.0351963 -3.9528668 -3.9155972 -3.9919136 -4.106854 -4.203959 -4.2767062 -4.317996 -4.33149 -4.3270845][-4.276989 -4.2622018 -4.2322984 -4.1880293 -4.1146688 -4.0182753 -3.9191635 -3.8605149 -3.9427669 -4.0744286 -4.186379 -4.2649879 -4.3072062 -4.3199282 -4.315187][-4.2704544 -4.2596965 -4.2342782 -4.1901531 -4.1098022 -4.0009336 -3.8829465 -3.8051219 -3.8874722 -4.0317497 -4.1590767 -4.2454429 -4.29141 -4.3058877 -4.3014555][-4.2678542 -4.2586422 -4.2393165 -4.199419 -4.1200371 -4.0092411 -3.8861144 -3.797725 -3.8610408 -4.0002751 -4.1358085 -4.2274041 -4.2772107 -4.2940674 -4.2894521][-4.2685447 -4.257803 -4.2431688 -4.2116084 -4.1470151 -4.0532656 -3.9496009 -3.864737 -3.8919578 -4.0039082 -4.1301675 -4.2180495 -4.2670517 -4.2854533 -4.2795553][-4.2720222 -4.2581987 -4.24482 -4.2198129 -4.1712389 -4.10048 -4.024869 -3.9576397 -3.9616723 -4.0412445 -4.1452465 -4.2209029 -4.2619195 -4.2795329 -4.273437][-4.2755041 -4.2602148 -4.2466168 -4.226563 -4.1902218 -4.1394849 -4.0879354 -4.0446095 -4.0450487 -4.10068 -4.1796079 -4.238842 -4.2678704 -4.2793384 -4.2714968][-4.277658 -4.2638097 -4.2532964 -4.239614 -4.2152886 -4.1801996 -4.1479931 -4.1248751 -4.1299477 -4.1692886 -4.2260613 -4.2672453 -4.281888 -4.2839012 -4.2720041][-4.2791095 -4.2682633 -4.2612195 -4.2550979 -4.2418108 -4.2203345 -4.2017441 -4.1917953 -4.1990657 -4.2261472 -4.2646513 -4.2896609 -4.2943039 -4.2900043 -4.2770619][-4.2823114 -4.27431 -4.2696462 -4.2685452 -4.2632747 -4.2516727 -4.241744 -4.2388892 -4.2462816 -4.2647505 -4.2875657 -4.3005924 -4.3008475 -4.2948895 -4.2839837]]...]
INFO - root - 2017-12-05 19:05:47.841186: step 35610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:49m:32s remains)
INFO - root - 2017-12-05 19:05:56.406869: step 35620, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 70h:19m:08s remains)
INFO - root - 2017-12-05 19:06:04.963956: step 35630, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 71h:59m:56s remains)
INFO - root - 2017-12-05 19:06:13.503684: step 35640, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 68h:27m:15s remains)
INFO - root - 2017-12-05 19:06:21.988367: step 35650, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 70h:12m:32s remains)
INFO - root - 2017-12-05 19:06:30.545340: step 35660, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.847 sec/batch; 69h:48m:03s remains)
INFO - root - 2017-12-05 19:06:39.109714: step 35670, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 69h:43m:39s remains)
INFO - root - 2017-12-05 19:06:47.727422: step 35680, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 72h:44m:46s remains)
INFO - root - 2017-12-05 19:06:56.342621: step 35690, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 73h:25m:01s remains)
INFO - root - 2017-12-05 19:07:04.861595: step 35700, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 71h:56m:01s remains)
2017-12-05 19:07:05.599515: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.209815 -4.1997848 -4.1763663 -4.149878 -4.1502571 -4.1809311 -4.2034521 -4.2009168 -4.1783209 -4.1579118 -4.1720343 -4.2152929 -4.2597852 -4.2822776 -4.28047][-4.2344842 -4.2167754 -4.181653 -4.1431742 -4.1363435 -4.1602383 -4.1773334 -4.171566 -4.15354 -4.1398993 -4.1605768 -4.2091055 -4.2562475 -4.276989 -4.273603][-4.2524014 -4.2389894 -4.202878 -4.1629448 -4.1484127 -4.1577239 -4.1592779 -4.1434908 -4.1245985 -4.1120348 -4.1336131 -4.1890192 -4.2425408 -4.2637539 -4.2604437][-4.2470493 -4.2419066 -4.2165775 -4.1877642 -4.17455 -4.1728754 -4.1587024 -4.130301 -4.0980563 -4.0717807 -4.0925779 -4.1589155 -4.2222996 -4.245903 -4.2475595][-4.2183795 -4.2181177 -4.2095685 -4.200388 -4.1997147 -4.1955185 -4.1696625 -4.1267896 -4.0741854 -4.0282593 -4.05091 -4.1305485 -4.205658 -4.2333713 -4.2374053][-4.1807218 -4.1801009 -4.1855235 -4.1964016 -4.2109027 -4.2117605 -4.1832552 -4.1304569 -4.0606003 -4.0050664 -4.0334167 -4.117435 -4.1966071 -4.2279639 -4.2340908][-4.1482224 -4.1489959 -4.1650782 -4.1904092 -4.2143288 -4.2173114 -4.1867647 -4.1278386 -4.0508108 -4.0015063 -4.0410986 -4.1270862 -4.2048526 -4.2371244 -4.2436962][-4.1388659 -4.1437974 -4.16638 -4.194644 -4.2159352 -4.2125387 -4.1744175 -4.1090951 -4.0366197 -4.0059934 -4.0580692 -4.1491642 -4.2246709 -4.2545772 -4.2606096][-4.1510644 -4.1656885 -4.190753 -4.2119479 -4.2203803 -4.2025213 -4.1527219 -4.0830293 -4.0230112 -4.0144897 -4.0784731 -4.17001 -4.2420239 -4.2698264 -4.2747293][-4.1704183 -4.1946378 -4.2195516 -4.2290807 -4.2212548 -4.1920204 -4.1383491 -4.0758762 -4.0349445 -4.0450854 -4.1082358 -4.1876612 -4.249855 -4.2765269 -4.2833581][-4.180872 -4.213098 -4.2336688 -4.2323804 -4.2121358 -4.177691 -4.1345272 -4.0937858 -4.0800276 -4.102838 -4.1524134 -4.2078457 -4.2493415 -4.2693868 -4.2772579][-4.1901631 -4.2239509 -4.2388058 -4.2298265 -4.2030578 -4.166853 -4.1344934 -4.1159706 -4.1285381 -4.1627507 -4.198638 -4.2265272 -4.2441492 -4.2529535 -4.2576008][-4.212244 -4.2431469 -4.2500691 -4.2316146 -4.1983819 -4.1590981 -4.1306653 -4.1299748 -4.16199 -4.2041926 -4.2334146 -4.243926 -4.2448711 -4.2423539 -4.242888][-4.2428837 -4.2699318 -4.2680569 -4.2398977 -4.1958804 -4.1476378 -4.1219358 -4.135211 -4.178802 -4.2253532 -4.2560129 -4.2651052 -4.2617126 -4.2534614 -4.2502222][-4.27542 -4.2923026 -4.2796612 -4.2433009 -4.1904149 -4.138236 -4.1159983 -4.1384306 -4.1853824 -4.2322879 -4.2648025 -4.2807121 -4.2818518 -4.2753544 -4.2694016]]...]
INFO - root - 2017-12-05 19:07:14.182980: step 35710, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 71h:37m:24s remains)
INFO - root - 2017-12-05 19:07:22.835988: step 35720, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 71h:55m:59s remains)
INFO - root - 2017-12-05 19:07:31.359916: step 35730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:44m:51s remains)
INFO - root - 2017-12-05 19:07:39.926434: step 35740, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 71h:28m:03s remains)
INFO - root - 2017-12-05 19:07:48.440352: step 35750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 70h:23m:07s remains)
INFO - root - 2017-12-05 19:07:56.981546: step 35760, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 70h:37m:01s remains)
INFO - root - 2017-12-05 19:08:05.299378: step 35770, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.815 sec/batch; 67h:12m:03s remains)
INFO - root - 2017-12-05 19:08:13.821373: step 35780, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 72h:09m:08s remains)
INFO - root - 2017-12-05 19:08:22.292501: step 35790, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.762 sec/batch; 62h:49m:12s remains)
INFO - root - 2017-12-05 19:08:30.954036: step 35800, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 71h:31m:16s remains)
2017-12-05 19:08:31.737791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9807279 -4.0460196 -4.1122975 -4.1594076 -4.1898847 -4.2087779 -4.2133 -4.1754713 -4.0928755 -3.9931278 -3.9479508 -3.9869468 -4.0699258 -4.1499186 -4.2162857][-4.0617394 -4.1298342 -4.1784296 -4.2026024 -4.2159081 -4.2301717 -4.2392745 -4.22091 -4.1811352 -4.1373615 -4.1241713 -4.1444545 -4.1825347 -4.2241745 -4.2628746][-4.1211295 -4.1868148 -4.22655 -4.2361956 -4.2356257 -4.2363434 -4.2315955 -4.218771 -4.2098732 -4.2064457 -4.2187271 -4.24289 -4.2669969 -4.2904038 -4.3112965][-4.1615982 -4.2138095 -4.2455797 -4.24878 -4.2355227 -4.2135839 -4.17955 -4.1543374 -4.1717782 -4.2112541 -4.2573056 -4.2984171 -4.3248315 -4.33809 -4.3418174][-4.2018867 -4.2262793 -4.238893 -4.228147 -4.195662 -4.1358581 -4.052464 -3.9949994 -4.0321016 -4.126833 -4.22429 -4.2999454 -4.3449826 -4.3595996 -4.3536773][-4.2479978 -4.2415509 -4.225008 -4.1821585 -4.1077437 -3.9872382 -3.8287573 -3.7154951 -3.7798679 -3.9578748 -4.12158 -4.2389584 -4.3099742 -4.3421526 -4.3454089][-4.283052 -4.2586107 -4.2209787 -4.1494918 -4.0288491 -3.8405995 -3.5883319 -3.3877854 -3.4651983 -3.7355902 -3.9771564 -4.1419764 -4.2411551 -4.2956953 -4.3162956][-4.2948346 -4.2677889 -4.2253885 -4.1524963 -4.0357428 -3.8475695 -3.5701308 -3.311568 -3.339051 -3.6106262 -3.873178 -4.055306 -4.171731 -4.2425051 -4.2758894][-4.2817111 -4.2531652 -4.2171817 -4.1643519 -4.0898304 -3.9666176 -3.7637634 -3.5639033 -3.5533352 -3.7253439 -3.9138029 -4.0505753 -4.1466541 -4.2108412 -4.24668][-4.2615547 -4.240912 -4.2138591 -4.1792436 -4.1392674 -4.0740261 -3.9520304 -3.8270183 -3.8110285 -3.8978975 -4.0093923 -4.0957747 -4.1630187 -4.2136335 -4.2413559][-4.2477322 -4.2397466 -4.2280498 -4.2070804 -4.1843252 -4.1498818 -4.0801625 -4.0057182 -3.9795094 -4.0076132 -4.0709929 -4.1337409 -4.1867828 -4.2315297 -4.252552][-4.2370086 -4.2399173 -4.2464895 -4.2405882 -4.2293658 -4.2114053 -4.1701379 -4.1180286 -4.077158 -4.0643387 -4.0839534 -4.1283317 -4.1769166 -4.2277336 -4.258667][-4.2319674 -4.2429118 -4.2601991 -4.2639856 -4.2624421 -4.2587051 -4.2359147 -4.1958203 -4.1398325 -4.0861163 -4.0588207 -4.0766058 -4.1215615 -4.1837292 -4.2355533][-4.2383161 -4.2498126 -4.26598 -4.270936 -4.2712078 -4.2722225 -4.2611928 -4.2337646 -4.1836119 -4.1109867 -4.0473652 -4.0305252 -4.064837 -4.1352053 -4.2047253][-4.2455478 -4.2534261 -4.2623949 -4.2622914 -4.2585225 -4.2569704 -4.2507968 -4.2322054 -4.1941023 -4.1297812 -4.0586 -4.0225577 -4.0440316 -4.11295 -4.1924086]]...]
INFO - root - 2017-12-05 19:08:40.256667: step 35810, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 68h:58m:51s remains)
INFO - root - 2017-12-05 19:08:48.770656: step 35820, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.816 sec/batch; 67h:14m:29s remains)
INFO - root - 2017-12-05 19:08:57.152258: step 35830, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.812 sec/batch; 66h:52m:41s remains)
INFO - root - 2017-12-05 19:09:05.783789: step 35840, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.877 sec/batch; 72h:13m:46s remains)
INFO - root - 2017-12-05 19:09:14.316367: step 35850, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 70h:10m:09s remains)
INFO - root - 2017-12-05 19:09:23.066690: step 35860, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 69h:12m:36s remains)
INFO - root - 2017-12-05 19:09:31.497554: step 35870, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 71h:51m:39s remains)
INFO - root - 2017-12-05 19:09:40.059006: step 35880, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 73h:49m:09s remains)
INFO - root - 2017-12-05 19:09:48.510139: step 35890, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 70h:25m:11s remains)
INFO - root - 2017-12-05 19:09:57.054761: step 35900, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 70h:33m:24s remains)
2017-12-05 19:09:57.813321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1763186 -4.1851544 -4.18789 -4.1917028 -4.1961875 -4.1886697 -4.1715703 -4.1558514 -4.1581783 -4.1836066 -4.216404 -4.2346973 -4.2409873 -4.2268934 -4.1882858][-4.1759181 -4.1796155 -4.1766081 -4.1734238 -4.1699471 -4.1636038 -4.1527128 -4.1417813 -4.1487107 -4.1803379 -4.2271681 -4.2610273 -4.2718458 -4.2479639 -4.1972952][-4.1826673 -4.177186 -4.1693573 -4.1545277 -4.135746 -4.1250529 -4.1297188 -4.1346045 -4.1507025 -4.1852431 -4.2419381 -4.2896523 -4.30705 -4.2809782 -4.2236924][-4.2024813 -4.1827135 -4.164185 -4.1367326 -4.1027055 -4.0823507 -4.1010704 -4.12248 -4.1445694 -4.1792846 -4.240046 -4.2957072 -4.3191338 -4.3038268 -4.25787][-4.2182074 -4.1840591 -4.1521 -4.1189618 -4.0788288 -4.0519552 -4.0660162 -4.0872412 -4.1149392 -4.158864 -4.2228971 -4.27898 -4.3083239 -4.3096333 -4.2833047][-4.2274151 -4.188643 -4.1494966 -4.1131997 -4.0704217 -4.0277562 -4.0107579 -4.0119581 -4.0510468 -4.1236377 -4.2018337 -4.2605658 -4.2957311 -4.3085194 -4.29725][-4.2391214 -4.2075458 -4.1681185 -4.1230679 -4.0664315 -3.9992447 -3.9369919 -3.9107771 -3.9709923 -4.0817266 -4.1758885 -4.2378054 -4.274714 -4.2916074 -4.2918949][-4.2472954 -4.223721 -4.1852269 -4.1328297 -4.0640512 -3.9860654 -3.9005222 -3.8611414 -3.9327118 -4.0562387 -4.148531 -4.2104425 -4.2474074 -4.2608366 -4.270997][-4.2350283 -4.2212954 -4.1903491 -4.1395178 -4.0735855 -4.0078616 -3.9440343 -3.913353 -3.9595079 -4.0496545 -4.1241193 -4.1858835 -4.2230477 -4.2351165 -4.2522187][-4.2008653 -4.1993427 -4.1844325 -4.1485281 -4.1029229 -4.0606227 -4.0198894 -3.9930234 -3.9986427 -4.0442505 -4.10018 -4.15591 -4.1876483 -4.203196 -4.2273955][-4.1662884 -4.17063 -4.1661158 -4.1513381 -4.1344047 -4.1132374 -4.0858092 -4.0525446 -4.0256882 -4.0385175 -4.0849032 -4.1353707 -4.1618214 -4.1764793 -4.1971827][-4.1443295 -4.1454911 -4.1427879 -4.1374884 -4.1355977 -4.1313157 -4.1195712 -4.0885067 -4.0501237 -4.0542021 -4.0983887 -4.1356459 -4.1450562 -4.1441455 -4.153501][-4.1318407 -4.1242371 -4.1181211 -4.1137228 -4.1189771 -4.1278143 -4.1307778 -4.1144915 -4.0879183 -4.0923514 -4.1190448 -4.1275015 -4.1128221 -4.1014504 -4.1086092][-4.1379814 -4.1255274 -4.1142406 -4.1038704 -4.1136594 -4.1342311 -4.1500015 -4.1529722 -4.14265 -4.1400433 -4.1376328 -4.1169782 -4.09 -4.0799046 -4.0875039][-4.1567564 -4.1453018 -4.1287122 -4.1128941 -4.1272712 -4.1562886 -4.1777811 -4.1852732 -4.1783185 -4.166913 -4.14639 -4.1137338 -4.0880947 -4.0831122 -4.0901504]]...]
INFO - root - 2017-12-05 19:10:06.458971: step 35910, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.878 sec/batch; 72h:20m:56s remains)
INFO - root - 2017-12-05 19:10:15.004038: step 35920, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 70h:18m:51s remains)
INFO - root - 2017-12-05 19:10:23.536667: step 35930, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 69h:59m:40s remains)
INFO - root - 2017-12-05 19:10:32.109029: step 35940, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 71h:20m:46s remains)
INFO - root - 2017-12-05 19:10:40.718368: step 35950, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 68h:03m:58s remains)
INFO - root - 2017-12-05 19:10:49.295770: step 35960, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:56m:07s remains)
INFO - root - 2017-12-05 19:10:57.786079: step 35970, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.832 sec/batch; 68h:33m:37s remains)
INFO - root - 2017-12-05 19:11:06.424858: step 35980, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 72h:12m:14s remains)
INFO - root - 2017-12-05 19:11:14.898703: step 35990, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 71h:31m:53s remains)
INFO - root - 2017-12-05 19:11:23.354429: step 36000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 71h:35m:52s remains)
2017-12-05 19:11:24.141590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.24133 -4.22019 -4.2011619 -4.1944394 -4.1934085 -4.1889343 -4.1827393 -4.1863666 -4.1910157 -4.1968455 -4.2233486 -4.2583551 -4.2852859 -4.285409 -4.2685728][-4.2313738 -4.2029457 -4.1723804 -4.1516657 -4.136652 -4.1255074 -4.1301641 -4.1569581 -4.1871409 -4.2089052 -4.2399111 -4.2708521 -4.29038 -4.2847834 -4.2645383][-4.2272153 -4.2033067 -4.1644373 -4.1233497 -4.0841789 -4.0572667 -4.0619321 -4.1069136 -4.1639977 -4.2035847 -4.241437 -4.2711349 -4.2862573 -4.2800393 -4.2595472][-4.2236004 -4.2107291 -4.1679049 -4.1089759 -4.0450072 -3.9936566 -3.9812002 -4.0363641 -4.1219583 -4.1861377 -4.2355494 -4.2670918 -4.2794919 -4.2718267 -4.2519093][-4.2189302 -4.2174277 -4.1719289 -4.0986843 -4.0077572 -3.9211533 -3.8809597 -3.9423444 -4.05854 -4.1516733 -4.2155867 -4.2538104 -4.2685385 -4.2614994 -4.242754][-4.2020674 -4.2114172 -4.1686234 -4.0890722 -3.9727054 -3.8344784 -3.751338 -3.8284061 -3.9864602 -4.1072578 -4.1856694 -4.23367 -4.2551522 -4.2527723 -4.2356129][-4.1771722 -4.2008862 -4.17163 -4.0991373 -3.9710615 -3.786535 -3.6472044 -3.7406187 -3.9282508 -4.0629148 -4.1497984 -4.2104173 -4.244472 -4.2492127 -4.2348375][-4.1729026 -4.204565 -4.1881013 -4.1343627 -4.0304275 -3.8654015 -3.726845 -3.7845433 -3.9249773 -4.0323133 -4.11546 -4.1878929 -4.2365084 -4.2469721 -4.2359495][-4.1873059 -4.2149572 -4.2051563 -4.1720543 -4.1106138 -4.0098672 -3.9135621 -3.9154499 -3.975702 -4.0335441 -4.095551 -4.1654148 -4.2226205 -4.2427435 -4.2384043][-4.187624 -4.2076926 -4.2005649 -4.184639 -4.1581039 -4.1106005 -4.0464563 -4.0133762 -4.0142422 -4.03417 -4.0745926 -4.1338329 -4.1948342 -4.2298417 -4.2360454][-4.1772404 -4.1885505 -4.1809177 -4.1757736 -4.17556 -4.1566267 -4.1075239 -4.053894 -4.0176826 -4.0108042 -4.0352993 -4.0888243 -4.1541777 -4.2031279 -4.2222753][-4.173687 -4.180378 -4.1725297 -4.1723967 -4.1868191 -4.1813512 -4.14017 -4.0821643 -4.0288143 -4.0015764 -4.0145149 -4.0625224 -4.1247854 -4.1777506 -4.2076263][-4.1772566 -4.1850719 -4.1819906 -4.189579 -4.21242 -4.2135048 -4.1821871 -4.1315284 -4.075469 -4.04012 -4.046874 -4.0835619 -4.1302528 -4.1769075 -4.2115364][-4.18659 -4.2000155 -4.20523 -4.2208395 -4.2480659 -4.2561231 -4.2323413 -4.1924105 -4.145134 -4.1126161 -4.1139288 -4.1360812 -4.1658549 -4.2005749 -4.2326097][-4.1946516 -4.2167377 -4.22936 -4.2453694 -4.2704206 -4.2805343 -4.2623682 -4.2305169 -4.1940317 -4.1709661 -4.172883 -4.1878433 -4.2100458 -4.2388277 -4.2643304]]...]
INFO - root - 2017-12-05 19:11:32.587903: step 36010, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 70h:31m:22s remains)
INFO - root - 2017-12-05 19:11:41.107539: step 36020, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:41m:33s remains)
INFO - root - 2017-12-05 19:11:49.614805: step 36030, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:47m:10s remains)
INFO - root - 2017-12-05 19:11:58.255067: step 36040, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 68h:49m:23s remains)
INFO - root - 2017-12-05 19:12:06.820723: step 36050, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 68h:21m:53s remains)
INFO - root - 2017-12-05 19:12:15.391334: step 36060, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 68h:43m:17s remains)
INFO - root - 2017-12-05 19:12:23.835803: step 36070, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.815 sec/batch; 67h:08m:06s remains)
INFO - root - 2017-12-05 19:12:32.353336: step 36080, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.887 sec/batch; 73h:04m:31s remains)
INFO - root - 2017-12-05 19:12:40.880857: step 36090, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 71h:19m:20s remains)
INFO - root - 2017-12-05 19:12:49.496334: step 36100, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 69h:48m:31s remains)
2017-12-05 19:12:50.259837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2588634 -4.2687578 -4.2738991 -4.2769823 -4.2764988 -4.2685647 -4.2637086 -4.2706642 -4.2778878 -4.2800517 -4.2883468 -4.2991819 -4.2999654 -4.292634 -4.2836514][-4.1928883 -4.2087321 -4.2217112 -4.2320342 -4.2328954 -4.220304 -4.213727 -4.2268443 -4.2388468 -4.241663 -4.2532592 -4.269465 -4.2717514 -4.2639217 -4.2536249][-4.1403265 -4.1588149 -4.1752295 -4.190136 -4.1916656 -4.1758819 -4.1674018 -4.183516 -4.2009258 -4.2087526 -4.2255979 -4.2458029 -4.2493768 -4.2417 -4.2299986][-4.1260924 -4.1392779 -4.1537437 -4.1669297 -4.1644945 -4.1456943 -4.133986 -4.1477957 -4.1710086 -4.188755 -4.2096677 -4.2316055 -4.2364206 -4.2292619 -4.2158904][-4.1263504 -4.1321945 -4.1435409 -4.1530662 -4.1478643 -4.129189 -4.1140084 -4.1179843 -4.142169 -4.1708236 -4.1993079 -4.2216372 -4.2284007 -4.2255845 -4.2106991][-4.1444912 -4.1477623 -4.1534939 -4.1551747 -4.1479697 -4.1288409 -4.1082482 -4.103066 -4.1212993 -4.1576076 -4.192524 -4.2154922 -4.2255387 -4.2267518 -4.21091][-4.1880703 -4.1860118 -4.1829925 -4.1751966 -4.1631312 -4.1414165 -4.110971 -4.0971141 -4.1096287 -4.1509714 -4.1935043 -4.2186346 -4.2301774 -4.2311959 -4.2162881][-4.2334051 -4.2232695 -4.2106943 -4.1952243 -4.1777582 -4.1526794 -4.1109324 -4.0826306 -4.0937643 -4.1408272 -4.1923823 -4.2224803 -4.2351174 -4.234798 -4.2199359][-4.2657132 -4.2464757 -4.2277212 -4.2109857 -4.193892 -4.1660032 -4.1173739 -4.0747585 -4.0812159 -4.1295319 -4.188478 -4.2236714 -4.2370491 -4.2366004 -4.2218637][-4.2761593 -4.251905 -4.2308064 -4.2187009 -4.2060709 -4.1780877 -4.1291456 -4.0839629 -4.0867934 -4.1327758 -4.1904006 -4.2239432 -4.2344146 -4.233057 -4.2196875][-4.2697349 -4.244833 -4.2209773 -4.2118363 -4.2075634 -4.1869349 -4.1475706 -4.11237 -4.1135511 -4.1497259 -4.1936908 -4.2198739 -4.2290707 -4.2258039 -4.2115192][-4.2531929 -4.2312384 -4.2076578 -4.2006865 -4.2026186 -4.1937532 -4.1706719 -4.1450086 -4.1407256 -4.1639013 -4.1920619 -4.2107863 -4.2204828 -4.2179041 -4.2030077][-4.2310114 -4.2116818 -4.192286 -4.1893134 -4.1928134 -4.1919694 -4.1794991 -4.1546588 -4.1405954 -4.1527796 -4.1759129 -4.1963329 -4.2104506 -4.2100329 -4.1970797][-4.2125955 -4.1980109 -4.18743 -4.1859884 -4.1852889 -4.1806784 -4.1698995 -4.144556 -4.1211152 -4.1297174 -4.1572518 -4.1843944 -4.204875 -4.2069836 -4.1987557][-4.2049236 -4.1968794 -4.1978292 -4.1996932 -4.1904116 -4.176559 -4.1608558 -4.1290946 -4.1005878 -4.1117268 -4.1492481 -4.1841497 -4.2080803 -4.2116532 -4.20707]]...]
INFO - root - 2017-12-05 19:12:58.757481: step 36110, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 72h:03m:10s remains)
INFO - root - 2017-12-05 19:13:07.372249: step 36120, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 71h:51m:50s remains)
INFO - root - 2017-12-05 19:13:16.040220: step 36130, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 69h:51m:09s remains)
INFO - root - 2017-12-05 19:13:24.630345: step 36140, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 71h:33m:42s remains)
INFO - root - 2017-12-05 19:13:33.265812: step 36150, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 72h:40m:23s remains)
INFO - root - 2017-12-05 19:13:41.904626: step 36160, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 71h:55m:50s remains)
INFO - root - 2017-12-05 19:13:50.422954: step 36170, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 73h:55m:07s remains)
INFO - root - 2017-12-05 19:13:59.051290: step 36180, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 72h:43m:17s remains)
INFO - root - 2017-12-05 19:14:07.605668: step 36190, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 70h:41m:50s remains)
INFO - root - 2017-12-05 19:14:16.133347: step 36200, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:58m:12s remains)
2017-12-05 19:14:16.922607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2182641 -4.2245479 -4.2305856 -4.2279429 -4.2150559 -4.2073603 -4.2092595 -4.2231207 -4.2225018 -4.2196445 -4.2206416 -4.2166944 -4.1992946 -4.1725197 -4.1634645][-4.1804972 -4.1928973 -4.2047215 -4.2005305 -4.1822848 -4.1679397 -4.1669822 -4.1847196 -4.1981592 -4.2064505 -4.2213373 -4.2186747 -4.1926022 -4.1544824 -4.1390281][-4.142931 -4.1635337 -4.1781888 -4.1692138 -4.1454606 -4.1209722 -4.1108737 -4.1239967 -4.1515889 -4.1831927 -4.2124891 -4.2136693 -4.1807847 -4.1367092 -4.1138463][-4.1280966 -4.1463385 -4.1573267 -4.1418223 -4.1108842 -4.0695772 -4.0355287 -4.0280948 -4.0708351 -4.1338725 -4.1796861 -4.1865497 -4.149828 -4.105732 -4.0818348][-4.1246791 -4.1440368 -4.1485887 -4.1274934 -4.0883112 -4.0241952 -3.9491055 -3.8985736 -3.9592476 -4.0694189 -4.1478348 -4.170011 -4.1361938 -4.0866847 -4.061636][-4.1193843 -4.1454306 -4.1499352 -4.128633 -4.0820751 -3.9970691 -3.8734713 -3.75528 -3.8254721 -3.9932859 -4.1110168 -4.1551285 -4.1351247 -4.0843177 -4.0549612][-4.1240239 -4.15752 -4.1618533 -4.1393566 -4.0920472 -4.00249 -3.8562474 -3.6744378 -3.7272248 -3.9332209 -4.0819683 -4.1414695 -4.1312771 -4.085413 -4.0533895][-4.1401176 -4.1812778 -4.1902328 -4.1653519 -4.1224751 -4.0484252 -3.9375181 -3.792743 -3.8055782 -3.9671807 -4.0993476 -4.1469188 -4.1260109 -4.0789366 -4.0496016][-4.1502395 -4.1911726 -4.2092509 -4.1973763 -4.1682196 -4.1164961 -4.0470581 -3.9660311 -3.9604239 -4.0510254 -4.137269 -4.1626234 -4.1267009 -4.0784254 -4.0582266][-4.1446295 -4.1834583 -4.2122312 -4.2171907 -4.19848 -4.1617446 -4.12061 -4.0785975 -4.0699406 -4.1122551 -4.1611505 -4.1701279 -4.1262631 -4.0793915 -4.0707273][-4.1419764 -4.1756268 -4.2067509 -4.2198329 -4.2145648 -4.1954589 -4.1687703 -4.1462655 -4.1413374 -4.1597767 -4.1880903 -4.1858606 -4.1332331 -4.0850587 -4.0851569][-4.1425128 -4.1724687 -4.2044353 -4.2236481 -4.2287936 -4.2209725 -4.2010221 -4.1895666 -4.1890221 -4.2007613 -4.2148781 -4.2049036 -4.1499777 -4.1034184 -4.105969][-4.1498036 -4.1768613 -4.206234 -4.2299805 -4.2397933 -4.2327595 -4.2177353 -4.2099743 -4.2142844 -4.2245965 -4.2327633 -4.2231627 -4.1778622 -4.1360016 -4.1387663][-4.1721568 -4.2005696 -4.2224312 -4.2412968 -4.24692 -4.2409563 -4.2269535 -4.2194791 -4.2267647 -4.2381716 -4.2479734 -4.2452421 -4.2167583 -4.1854143 -4.180779][-4.2091246 -4.2335148 -4.2451453 -4.25319 -4.2518678 -4.2453575 -4.2334843 -4.2300529 -4.2362618 -4.2452435 -4.2569289 -4.2633057 -4.2556653 -4.2399006 -4.2275381]]...]
INFO - root - 2017-12-05 19:14:25.478461: step 36210, loss = 2.10, batch loss = 2.04 (10.5 examples/sec; 0.762 sec/batch; 62h:40m:47s remains)
INFO - root - 2017-12-05 19:14:34.078806: step 36220, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 72h:48m:59s remains)
INFO - root - 2017-12-05 19:14:42.754792: step 36230, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 70h:19m:17s remains)
INFO - root - 2017-12-05 19:14:51.378149: step 36240, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.852 sec/batch; 70h:06m:45s remains)
INFO - root - 2017-12-05 19:14:59.925758: step 36250, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.881 sec/batch; 72h:31m:41s remains)
INFO - root - 2017-12-05 19:15:08.478182: step 36260, loss = 2.06, batch loss = 2.00 (10.9 examples/sec; 0.735 sec/batch; 60h:27m:04s remains)
INFO - root - 2017-12-05 19:15:17.133333: step 36270, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 70h:46m:22s remains)
INFO - root - 2017-12-05 19:15:25.649713: step 36280, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:42m:38s remains)
INFO - root - 2017-12-05 19:15:34.291105: step 36290, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 71h:23m:56s remains)
INFO - root - 2017-12-05 19:15:42.882790: step 36300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 71h:42m:31s remains)
2017-12-05 19:15:43.634753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2377367 -4.2516851 -4.2570004 -4.2615952 -4.2608666 -4.2578492 -4.2535324 -4.2359877 -4.2253075 -4.2309551 -4.2398138 -4.2479773 -4.2570109 -4.2708197 -4.2870011][-4.2386193 -4.2515292 -4.2559547 -4.2584348 -4.2556157 -4.2554832 -4.2564769 -4.2438087 -4.2400627 -4.251678 -4.2618704 -4.2683344 -4.2751231 -4.2862916 -4.29864][-4.2263842 -4.2368274 -4.2384686 -4.2364788 -4.2309647 -4.2316422 -4.236495 -4.2306585 -4.2327452 -4.2485232 -4.260797 -4.269269 -4.2792292 -4.29244 -4.3049912][-4.2016315 -4.20943 -4.2099257 -4.2080121 -4.2028365 -4.2036805 -4.2109642 -4.2104368 -4.2152524 -4.2307444 -4.2430487 -4.2542143 -4.26921 -4.2862687 -4.3014655][-4.1674356 -4.1636605 -4.1551285 -4.1500721 -4.1452956 -4.1483293 -4.1640759 -4.1768837 -4.1876802 -4.2019939 -4.2108593 -4.220952 -4.2378564 -4.2598481 -4.2817459][-4.1232247 -4.1036787 -4.0776491 -4.055747 -4.0363579 -4.0346012 -4.06296 -4.0964637 -4.1192994 -4.13814 -4.1487141 -4.1591878 -4.1791444 -4.2090583 -4.2439876][-4.079349 -4.0511208 -4.0090494 -3.9609525 -3.9111524 -3.8872018 -3.9184196 -3.970217 -4.0076566 -4.0370526 -4.055778 -4.0730658 -4.1012464 -4.1437087 -4.196249][-4.0730219 -4.048491 -4.0047355 -3.9451683 -3.8759117 -3.8277478 -3.8404188 -3.8897569 -3.9308951 -3.9645278 -3.9880261 -4.0116076 -4.0487838 -4.104877 -4.1735897][-4.1037083 -4.0942826 -4.0666432 -4.0237865 -3.9706347 -3.9287257 -3.9248836 -3.9438665 -3.964699 -3.9868395 -4.0066051 -4.0292625 -4.0682812 -4.1283579 -4.198247][-4.1710181 -4.178133 -4.1697931 -4.1480522 -4.117691 -4.0916667 -4.0827117 -4.08224 -4.085947 -4.0953269 -4.1066532 -4.1227956 -4.15275 -4.19988 -4.2545142][-4.2504368 -4.2621193 -4.2646375 -4.2570128 -4.242588 -4.2281919 -4.2194557 -4.2147446 -4.214323 -4.218564 -4.2244072 -4.2337341 -4.2528763 -4.2807007 -4.312829][-4.3167567 -4.3258891 -4.3295422 -4.3285904 -4.3226056 -4.3140726 -4.3062878 -4.3021827 -4.3023062 -4.305182 -4.3099866 -4.3164835 -4.326467 -4.3381729 -4.3513379][-4.3482409 -4.3523769 -4.3540368 -4.3538103 -4.351408 -4.3465943 -4.3414521 -4.3396025 -4.3409715 -4.3440437 -4.3484249 -4.3528066 -4.3579855 -4.3624487 -4.3667293][-4.3579965 -4.35778 -4.35851 -4.3588276 -4.3581333 -4.3563285 -4.3540258 -4.3537021 -4.3549547 -4.3570142 -4.3603425 -4.3636365 -4.3668871 -4.3689404 -4.3702612][-4.3627415 -4.361156 -4.3611865 -4.3609953 -4.3609281 -4.3604536 -4.3595304 -4.3597374 -4.3608127 -4.3622441 -4.3644648 -4.3667669 -4.368793 -4.3700657 -4.3704572]]...]
INFO - root - 2017-12-05 19:15:52.207412: step 36310, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 68h:48m:01s remains)
INFO - root - 2017-12-05 19:16:00.715166: step 36320, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 72h:20m:40s remains)
INFO - root - 2017-12-05 19:16:09.082323: step 36330, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 70h:16m:28s remains)
INFO - root - 2017-12-05 19:16:17.725755: step 36340, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 71h:42m:35s remains)
INFO - root - 2017-12-05 19:16:26.360299: step 36350, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:42m:28s remains)
INFO - root - 2017-12-05 19:16:34.968462: step 36360, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.754 sec/batch; 62h:00m:53s remains)
INFO - root - 2017-12-05 19:16:43.557800: step 36370, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 70h:38m:26s remains)
INFO - root - 2017-12-05 19:16:52.144888: step 36380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:51m:12s remains)
INFO - root - 2017-12-05 19:17:00.705280: step 36390, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 70h:23m:17s remains)
INFO - root - 2017-12-05 19:17:09.269527: step 36400, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 67h:47m:26s remains)
2017-12-05 19:17:10.110930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2847357 -4.2830763 -4.2769594 -4.268764 -4.2604442 -4.255908 -4.2534466 -4.2579966 -4.2718854 -4.2875891 -4.2970009 -4.3013239 -4.30702 -4.3109956 -4.3088641][-4.2736635 -4.2708416 -4.264163 -4.2532382 -4.2415853 -4.2336469 -4.2286072 -4.2341151 -4.2519565 -4.27172 -4.2841034 -4.2904577 -4.2952766 -4.2958341 -4.2915459][-4.2506704 -4.2476344 -4.2390308 -4.22526 -4.2126617 -4.2042532 -4.1981587 -4.2057557 -4.2279696 -4.2524848 -4.2651925 -4.2742867 -4.2818213 -4.2790232 -4.2683516][-4.2142658 -4.2084084 -4.1914034 -4.1670713 -4.1523418 -4.1413994 -4.1297874 -4.1386232 -4.1648269 -4.1959381 -4.2141175 -4.2334771 -4.2544956 -4.2555103 -4.2383285][-4.16998 -4.1608863 -4.13316 -4.09427 -4.0743017 -4.058321 -4.03092 -4.0296183 -4.0554905 -4.0905509 -4.1138434 -4.1473904 -4.1915298 -4.2032642 -4.1803293][-4.1417003 -4.1289067 -4.0915794 -4.0353918 -4.0057793 -3.9765024 -3.9209902 -3.8961067 -3.9112022 -3.9429221 -3.9658248 -4.0170064 -4.0957336 -4.1214128 -4.0943518][-4.1164341 -4.1007428 -4.0579033 -3.9900241 -3.9524343 -3.9109416 -3.8311071 -3.7868505 -3.7926049 -3.8147814 -3.8299284 -3.8975863 -4.0084362 -4.0441275 -4.0146322][-4.108077 -4.1018634 -4.0707331 -4.012197 -3.9832428 -3.9511724 -3.8876858 -3.8452051 -3.8371041 -3.8385696 -3.8381395 -3.8919334 -3.9895673 -4.0134225 -3.9747863][-4.0996809 -4.1181583 -4.1168928 -4.0873523 -4.0740514 -4.0641704 -4.0366464 -4.0044107 -3.98322 -3.9711378 -3.9668591 -3.998033 -4.053791 -4.06164 -4.0280342][-4.0754228 -4.1201844 -4.1489859 -4.1409807 -4.1352754 -4.1364808 -4.1320882 -4.1148481 -4.0965557 -4.083293 -4.0881796 -4.1105089 -4.1401429 -4.1459446 -4.1318278][-4.0526114 -4.1058021 -4.1509938 -4.1575923 -4.1575108 -4.1622176 -4.1696754 -4.1704259 -4.1628942 -4.1514339 -4.1603975 -4.1774063 -4.1943369 -4.2007136 -4.1984911][-4.0381746 -4.0869126 -4.1357212 -4.1525993 -4.1587806 -4.164536 -4.1753416 -4.1840658 -4.1822915 -4.171463 -4.1820369 -4.1947265 -4.2030745 -4.2078919 -4.2065644][-4.0725255 -4.1073337 -4.1451664 -4.1586652 -4.1622529 -4.1661644 -4.175662 -4.1845093 -4.1813769 -4.1735783 -4.182713 -4.1909976 -4.1936283 -4.1952949 -4.187036][-4.1518431 -4.1735191 -4.1955876 -4.2014174 -4.1980438 -4.1972294 -4.2025495 -4.206049 -4.1988811 -4.1922603 -4.19474 -4.1930847 -4.1903677 -4.1886663 -4.1763554][-4.2196736 -4.2320356 -4.2441177 -4.2455697 -4.2406077 -4.2383561 -4.24034 -4.2386775 -4.23042 -4.2258816 -4.224246 -4.2167435 -4.2113709 -4.2066412 -4.1921749]]...]
INFO - root - 2017-12-05 19:17:18.548682: step 36410, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 69h:34m:28s remains)
INFO - root - 2017-12-05 19:17:27.176578: step 36420, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:40m:44s remains)
INFO - root - 2017-12-05 19:17:35.530791: step 36430, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:37m:48s remains)
INFO - root - 2017-12-05 19:17:44.021667: step 36440, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 69h:32m:56s remains)
INFO - root - 2017-12-05 19:17:52.635972: step 36450, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 72h:08m:35s remains)
INFO - root - 2017-12-05 19:18:01.151040: step 36460, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.764 sec/batch; 62h:48m:01s remains)
INFO - root - 2017-12-05 19:18:09.593404: step 36470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 70h:23m:40s remains)
INFO - root - 2017-12-05 19:18:18.103458: step 36480, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 68h:35m:48s remains)
INFO - root - 2017-12-05 19:18:26.588096: step 36490, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 70h:04m:02s remains)
INFO - root - 2017-12-05 19:18:35.119949: step 36500, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 66h:48m:58s remains)
2017-12-05 19:18:35.930763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2786255 -4.297894 -4.3140178 -4.3261328 -4.3317513 -4.3294439 -4.3131714 -4.2758527 -4.239552 -4.2360039 -4.2564049 -4.2847295 -4.3113375 -4.3307133 -4.3404202][-4.2608194 -4.28179 -4.3047657 -4.3217731 -4.3315969 -4.3356752 -4.3241506 -4.2894487 -4.254879 -4.2507668 -4.268239 -4.2920251 -4.3155189 -4.3341951 -4.3433003][-4.2332463 -4.2527943 -4.2796721 -4.3012962 -4.3181138 -4.3302646 -4.322433 -4.2871971 -4.25316 -4.2516742 -4.27009 -4.2930098 -4.3158803 -4.3358846 -4.3460355][-4.2010317 -4.2138424 -4.2369156 -4.2577357 -4.2790704 -4.2952552 -4.28668 -4.2456989 -4.2122812 -4.2202826 -4.2484655 -4.2757711 -4.3024831 -4.3293428 -4.344245][-4.1629806 -4.164753 -4.1795063 -4.1990824 -4.2269182 -4.2460628 -4.2294335 -4.1758337 -4.1408181 -4.1642342 -4.2093906 -4.2468867 -4.281517 -4.3172321 -4.3373218][-4.1303968 -4.1193686 -4.1230741 -4.1377387 -4.1699405 -4.1886468 -4.1595578 -4.0912819 -4.0562015 -4.0977731 -4.1641912 -4.2170248 -4.2608457 -4.3039351 -4.3287086][-4.1044655 -4.0787592 -4.068696 -4.0741844 -4.1016655 -4.1122522 -4.070611 -3.992233 -3.9679642 -4.0331612 -4.1226034 -4.1930456 -4.2484937 -4.2979388 -4.3244276][-4.1059794 -4.072669 -4.0483127 -4.0377789 -4.0490751 -4.0467544 -4.0041881 -3.9367454 -3.9310155 -4.0102553 -4.110261 -4.1927338 -4.2554064 -4.3064971 -4.3306289][-4.1486654 -4.1246114 -4.1014361 -4.0850549 -4.082839 -4.0716109 -4.0396738 -3.9952319 -3.9977391 -4.0589662 -4.1415296 -4.2178607 -4.2777395 -4.324801 -4.3438554][-4.2030063 -4.1905341 -4.1742082 -4.1580973 -4.1478148 -4.1342206 -4.1165524 -4.0907879 -4.0879331 -4.1192665 -4.17472 -4.2392716 -4.2958851 -4.3383746 -4.353466][-4.2292395 -4.2208366 -4.2083826 -4.1943607 -4.181057 -4.1678367 -4.1599803 -4.1447783 -4.13419 -4.14624 -4.1863565 -4.2487178 -4.3049917 -4.3423519 -4.35443][-4.213367 -4.207128 -4.196981 -4.1854553 -4.1704326 -4.1567068 -4.1527238 -4.1423864 -4.1284785 -4.1359849 -4.1751027 -4.2428007 -4.3009562 -4.3346009 -4.3449249][-4.1708217 -4.1612754 -4.1522083 -4.1437387 -4.1274881 -4.1115003 -4.1078844 -4.0982552 -4.0831223 -4.0955396 -4.1429653 -4.2166409 -4.2778511 -4.3136206 -4.3269835][-4.1234546 -4.1140652 -4.1080108 -4.1046042 -4.0917206 -4.076612 -4.0723562 -4.0621181 -4.0484824 -4.0671973 -4.1221666 -4.1974649 -4.2585144 -4.2966809 -4.3142948][-4.1187334 -4.1144409 -4.1125073 -4.1144767 -4.11082 -4.1025891 -4.0995259 -4.0887723 -4.0772858 -4.0990868 -4.1542177 -4.2217212 -4.2735991 -4.3046017 -4.3183904]]...]
INFO - root - 2017-12-05 19:18:44.481843: step 36510, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 70h:13m:21s remains)
INFO - root - 2017-12-05 19:18:52.935487: step 36520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:23m:17s remains)
INFO - root - 2017-12-05 19:19:01.514878: step 36530, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 70h:32m:26s remains)
INFO - root - 2017-12-05 19:19:09.775416: step 36540, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 69h:07m:54s remains)
INFO - root - 2017-12-05 19:19:18.322333: step 36550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 70h:48m:43s remains)
INFO - root - 2017-12-05 19:19:26.909273: step 36560, loss = 2.05, batch loss = 1.99 (10.2 examples/sec; 0.788 sec/batch; 64h:45m:22s remains)
INFO - root - 2017-12-05 19:19:35.425465: step 36570, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:21m:30s remains)
INFO - root - 2017-12-05 19:19:43.948427: step 36580, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.820 sec/batch; 67h:24m:44s remains)
INFO - root - 2017-12-05 19:19:52.557287: step 36590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 71h:21m:05s remains)
INFO - root - 2017-12-05 19:20:01.016876: step 36600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 70h:00m:21s remains)
2017-12-05 19:20:01.881837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2856121 -4.2705069 -4.2642255 -4.2680683 -4.278316 -4.2875209 -4.2917356 -4.2941737 -4.2974963 -4.3017259 -4.3050475 -4.3070145 -4.3072329 -4.3062654 -4.3062878][-4.2527132 -4.2294226 -4.220984 -4.2279563 -4.2441621 -4.2585397 -4.2649803 -4.2692385 -4.2769418 -4.2872214 -4.2954392 -4.2998524 -4.2998829 -4.2979484 -4.2967148][-4.2131877 -4.1809955 -4.1702209 -4.1799459 -4.2040248 -4.2265253 -4.2323394 -4.2332196 -4.2422075 -4.2605605 -4.2772331 -4.2882395 -4.2923207 -4.2924795 -4.2907958][-4.1822076 -4.1425056 -4.1269 -4.1350513 -4.1627445 -4.1878357 -4.1858811 -4.178287 -4.1872063 -4.2141852 -4.2425227 -4.2645597 -4.27756 -4.2843165 -4.2849579][-4.1716533 -4.1281385 -4.1065097 -4.1052022 -4.1260576 -4.1437778 -4.1296105 -4.1092148 -4.1158876 -4.1517224 -4.1930923 -4.2294493 -4.2557425 -4.2711077 -4.2747054][-4.186183 -4.1432672 -4.1157341 -4.1005898 -4.1032925 -4.1008778 -4.0630307 -4.0160165 -4.0129275 -4.0619941 -4.1219473 -4.174798 -4.2160597 -4.2437248 -4.2544637][-4.2072678 -4.167994 -4.1415191 -4.1197343 -4.1056294 -4.0783324 -4.0067024 -3.9175832 -3.894115 -3.9573843 -4.0385146 -4.1072345 -4.162169 -4.2033634 -4.2258158][-4.2209487 -4.1861024 -4.1666417 -4.1511636 -4.1324377 -4.0909414 -4.0016837 -3.8841429 -3.8440342 -3.910243 -3.9912031 -4.0597925 -4.1193929 -4.1684561 -4.1990733][-4.2258563 -4.1970458 -4.1887927 -4.1855597 -4.1726589 -4.1344962 -4.0595078 -3.9652109 -3.9356601 -3.9794278 -4.02625 -4.0638928 -4.1085038 -4.155427 -4.1869621][-4.2341752 -4.2123308 -4.2120509 -4.2148757 -4.2052355 -4.1776519 -4.1289859 -4.0742035 -4.0625548 -4.0902295 -4.1096835 -4.1188354 -4.1364064 -4.1682715 -4.1937695][-4.255825 -4.2412872 -4.2448769 -4.2473397 -4.2374058 -4.2152052 -4.1835594 -4.15452 -4.1565766 -4.1766043 -4.1833773 -4.1778755 -4.177135 -4.1943426 -4.2125998][-4.2833977 -4.2738795 -4.2773809 -4.2775159 -4.2664876 -4.2448158 -4.221241 -4.2064867 -4.214735 -4.2298212 -4.2313561 -4.2244673 -4.2195096 -4.227468 -4.2390609][-4.3082929 -4.3031988 -4.3055129 -4.3039708 -4.2926712 -4.2751546 -4.2586517 -4.2497864 -4.2539392 -4.258431 -4.2566829 -4.2542267 -4.2535682 -4.2580595 -4.2646461][-4.32503 -4.3228564 -4.3245316 -4.3244543 -4.3169556 -4.3057618 -4.2951212 -4.2882228 -4.2858939 -4.2830796 -4.2796445 -4.278543 -4.2805743 -4.2833338 -4.288054][-4.3340383 -4.3336687 -4.3354616 -4.3368063 -4.3337121 -4.3282065 -4.3224139 -4.3176036 -4.3127651 -4.3079386 -4.3046417 -4.3034019 -4.3038721 -4.3049088 -4.3082709]]...]
INFO - root - 2017-12-05 19:20:10.374124: step 36610, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 67h:17m:30s remains)
INFO - root - 2017-12-05 19:20:18.884053: step 36620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 70h:04m:13s remains)
INFO - root - 2017-12-05 19:20:27.455586: step 36630, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:38m:09s remains)
INFO - root - 2017-12-05 19:20:35.960018: step 36640, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.823 sec/batch; 67h:38m:09s remains)
INFO - root - 2017-12-05 19:20:44.386908: step 36650, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 71h:26m:38s remains)
INFO - root - 2017-12-05 19:20:52.861705: step 36660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:20m:43s remains)
INFO - root - 2017-12-05 19:21:01.482280: step 36670, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 71h:38m:50s remains)
INFO - root - 2017-12-05 19:21:10.056499: step 36680, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 72h:16m:31s remains)
INFO - root - 2017-12-05 19:21:18.565421: step 36690, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 72h:30m:18s remains)
INFO - root - 2017-12-05 19:21:27.112987: step 36700, loss = 2.02, batch loss = 1.97 (9.1 examples/sec; 0.879 sec/batch; 72h:15m:28s remains)
2017-12-05 19:21:27.849178: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2482638 -4.2490854 -4.2581563 -4.2629232 -4.2644267 -4.2686472 -4.275692 -4.2795343 -4.2830143 -4.2851768 -4.2871761 -4.2865343 -4.2834377 -4.2763453 -4.27104][-4.2054324 -4.2021384 -4.2099571 -4.2163229 -4.220468 -4.2270594 -4.239481 -4.2463326 -4.2527056 -4.259903 -4.2670431 -4.2687235 -4.2665477 -4.2595038 -4.2543645][-4.1584692 -4.1558089 -4.1652794 -4.1770368 -4.1871996 -4.1901917 -4.197206 -4.2032161 -4.2130523 -4.2257304 -4.2383819 -4.2436666 -4.2419996 -4.2367415 -4.2353268][-4.1171393 -4.1164808 -4.1294255 -4.1458216 -4.1552672 -4.148387 -4.14411 -4.1484756 -4.1621308 -4.1817975 -4.2036476 -4.2183414 -4.2209005 -4.2132092 -4.212688][-4.0929828 -4.1015697 -4.1182194 -4.129765 -4.1212125 -4.0967731 -4.0781155 -4.0806122 -4.0991945 -4.1297379 -4.1613207 -4.183495 -4.19171 -4.1839662 -4.1826253][-4.0905313 -4.1073332 -4.1210661 -4.1151228 -4.0833168 -4.0431809 -4.0166507 -4.0169196 -4.0408549 -4.0832348 -4.1202846 -4.1419091 -4.1546216 -4.1493821 -4.1516805][-4.1120224 -4.1204767 -4.1146631 -4.0806174 -4.0241561 -3.9722037 -3.9476554 -3.9521403 -3.9837775 -4.0361638 -4.0807447 -4.1053777 -4.1222115 -4.1209908 -4.1272087][-4.1403441 -4.1284041 -4.0887995 -4.020803 -3.9424865 -3.8864362 -3.8754249 -3.8928366 -3.931474 -3.9894488 -4.04582 -4.0810776 -4.1008105 -4.1025105 -4.1124535][-4.1785097 -4.1480474 -4.0853782 -3.9987788 -3.9153378 -3.8662767 -3.873925 -3.9125941 -3.9636 -4.0182805 -4.067647 -4.1015162 -4.1197429 -4.1270666 -4.14138][-4.2155666 -4.1803026 -4.1154761 -4.0334382 -3.9587197 -3.9259136 -3.9530816 -4.0134454 -4.0712681 -4.1155734 -4.1444674 -4.1655717 -4.1804557 -4.1906776 -4.2040544][-4.2472935 -4.2148643 -4.1600823 -4.092021 -4.0333037 -4.0143437 -4.0444093 -4.1020784 -4.1536059 -4.19005 -4.2080259 -4.2211723 -4.2339945 -4.2434521 -4.2530646][-4.2750583 -4.2466049 -4.2043695 -4.1543088 -4.110466 -4.0987506 -4.1229753 -4.1668119 -4.2047539 -4.2327404 -4.2431703 -4.2483268 -4.2552667 -4.2621975 -4.269587][-4.285995 -4.2660427 -4.2381439 -4.2055593 -4.175488 -4.1692009 -4.1897826 -4.2191486 -4.2391963 -4.2533846 -4.2566228 -4.2560158 -4.25453 -4.257535 -4.2628827][-4.2784414 -4.2642779 -4.2499242 -4.233274 -4.2183433 -4.2168541 -4.2327495 -4.25075 -4.25855 -4.2641664 -4.2636642 -4.2600284 -4.2523923 -4.2496591 -4.2528172][-4.2682161 -4.2604256 -4.2570152 -4.2513885 -4.2462597 -4.2464209 -4.2567625 -4.2672896 -4.2694411 -4.2718353 -4.2720075 -4.2684579 -4.2583456 -4.2500534 -4.2493782]]...]
INFO - root - 2017-12-05 19:21:36.408243: step 36710, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 69h:17m:28s remains)
INFO - root - 2017-12-05 19:21:44.918232: step 36720, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.820 sec/batch; 67h:22m:18s remains)
INFO - root - 2017-12-05 19:21:53.487060: step 36730, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 69h:58m:51s remains)
INFO - root - 2017-12-05 19:22:02.079559: step 36740, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 67h:47m:12s remains)
INFO - root - 2017-12-05 19:22:10.549022: step 36750, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 72h:34m:31s remains)
INFO - root - 2017-12-05 19:22:18.839873: step 36760, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 69h:28m:40s remains)
INFO - root - 2017-12-05 19:22:27.402502: step 36770, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 70h:14m:27s remains)
INFO - root - 2017-12-05 19:22:35.939062: step 36780, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 71h:26m:57s remains)
INFO - root - 2017-12-05 19:22:44.401329: step 36790, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 69h:15m:26s remains)
INFO - root - 2017-12-05 19:22:52.920691: step 36800, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 70h:25m:50s remains)
2017-12-05 19:22:53.727523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2324643 -4.2056289 -4.1897783 -4.1961031 -4.2161851 -4.2367358 -4.2494073 -4.2498341 -4.2435627 -4.2304235 -4.2129507 -4.1893668 -4.1621714 -4.1467147 -4.1576385][-4.2148976 -4.1752563 -4.154417 -4.163928 -4.1909437 -4.2180476 -4.2326379 -4.2319193 -4.2263236 -4.2155066 -4.1968594 -4.1673307 -4.1273994 -4.1019344 -4.1172748][-4.1871285 -4.1414618 -4.1221423 -4.1401887 -4.1714973 -4.1944814 -4.2010341 -4.1960039 -4.1963692 -4.1968131 -4.1850567 -4.1497316 -4.0951605 -4.0599289 -4.0838718][-4.1666183 -4.1224513 -4.1094065 -4.132874 -4.15916 -4.1668916 -4.1541791 -4.1416206 -4.1496997 -4.1685948 -4.17215 -4.1382117 -4.0799642 -4.0427356 -4.0729556][-4.1662474 -4.1338015 -4.1253371 -4.1414766 -4.1517315 -4.1308756 -4.0846066 -4.0582542 -4.07887 -4.1269636 -4.1612382 -4.1483946 -4.1095729 -4.0846195 -4.1122212][-4.1807318 -4.1597829 -4.1459732 -4.1408587 -4.1222339 -4.0592279 -3.959662 -3.9054778 -3.9546344 -4.053647 -4.1329937 -4.1560159 -4.1494327 -4.1439819 -4.1657872][-4.1861258 -4.1739516 -4.1606274 -4.1378646 -4.0931168 -3.9860721 -3.8213811 -3.718502 -3.804085 -3.9718573 -4.0973496 -4.15345 -4.1713481 -4.1792912 -4.1966][-4.1860995 -4.1786671 -4.1725516 -4.1541691 -4.107873 -3.9907837 -3.8035564 -3.6802859 -3.7799299 -3.9680796 -4.1044335 -4.1702275 -4.1927977 -4.2018 -4.2148643][-4.1818523 -4.1834745 -4.1896038 -4.1847744 -4.1557274 -4.0694704 -3.9331598 -3.8494558 -3.9219162 -4.0601077 -4.1567655 -4.2028375 -4.2137885 -4.2137957 -4.2220726][-4.1749244 -4.1829405 -4.1983662 -4.2052612 -4.1911931 -4.1356573 -4.0554 -4.0164461 -4.0696344 -4.1551847 -4.2097564 -4.2313957 -4.2231879 -4.2058649 -4.20687][-4.1766553 -4.1903257 -4.2083864 -4.219698 -4.2163658 -4.1840806 -4.1393609 -4.131278 -4.1749468 -4.2244949 -4.2479963 -4.2451262 -4.2134905 -4.17891 -4.1745725][-4.18762 -4.20292 -4.2167325 -4.228158 -4.2352066 -4.2271209 -4.2067342 -4.21043 -4.2404761 -4.2590179 -4.2554579 -4.2315044 -4.1861644 -4.1471024 -4.1479058][-4.2032285 -4.216156 -4.2251973 -4.2341518 -4.2442522 -4.2524867 -4.2505856 -4.2584748 -4.2732191 -4.2709293 -4.25121 -4.2169871 -4.172122 -4.1414785 -4.1508446][-4.2381344 -4.245132 -4.2478914 -4.2536058 -4.263906 -4.2768359 -4.2817407 -4.2886496 -4.290688 -4.28097 -4.2585411 -4.2263222 -4.1914263 -4.1737819 -4.186799][-4.2718921 -4.2752156 -4.2763653 -4.2811065 -4.289854 -4.2990823 -4.3029871 -4.3063827 -4.3040967 -4.2948823 -4.2803974 -4.2609558 -4.2407002 -4.2293115 -4.2362714]]...]
INFO - root - 2017-12-05 19:23:02.304947: step 36810, loss = 2.10, batch loss = 2.04 (9.9 examples/sec; 0.809 sec/batch; 66h:28m:41s remains)
INFO - root - 2017-12-05 19:23:10.879756: step 36820, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 67h:41m:56s remains)
INFO - root - 2017-12-05 19:23:19.369717: step 36830, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:06m:07s remains)
INFO - root - 2017-12-05 19:23:27.887825: step 36840, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 72h:05m:50s remains)
INFO - root - 2017-12-05 19:23:36.428241: step 36850, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 67h:53m:54s remains)
INFO - root - 2017-12-05 19:23:44.858442: step 36860, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 73h:05m:48s remains)
INFO - root - 2017-12-05 19:23:53.380880: step 36870, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 71h:12m:30s remains)
INFO - root - 2017-12-05 19:24:01.984018: step 36880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 71h:13m:46s remains)
INFO - root - 2017-12-05 19:24:10.517249: step 36890, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 69h:16m:31s remains)
INFO - root - 2017-12-05 19:24:18.944145: step 36900, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 69h:46m:38s remains)
2017-12-05 19:24:19.700498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3040228 -4.2986412 -4.2779827 -4.2566881 -4.2454853 -4.2422123 -4.2507987 -4.2622871 -4.2578464 -4.23428 -4.1904616 -4.1508331 -4.1362658 -4.1559215 -4.1859393][-4.3091259 -4.312952 -4.3053107 -4.2900338 -4.2760434 -4.2657218 -4.2658291 -4.2682371 -4.26386 -4.2481451 -4.2159181 -4.1878376 -4.1781707 -4.1957307 -4.2153659][-4.2926664 -4.3007731 -4.3008876 -4.2926545 -4.2804871 -4.2693863 -4.2653303 -4.2640839 -4.2650466 -4.2584095 -4.2403722 -4.2266006 -4.2255363 -4.2413416 -4.2502937][-4.2648449 -4.2743282 -4.2760396 -4.2702403 -4.2619905 -4.2545929 -4.2514963 -4.2505326 -4.2562084 -4.2590985 -4.251514 -4.249289 -4.25529 -4.2691431 -4.2704878][-4.2259717 -4.237721 -4.2387996 -4.232379 -4.2269697 -4.2241273 -4.2239676 -4.2239008 -4.2330332 -4.2448149 -4.2425613 -4.247191 -4.2553191 -4.2623072 -4.2614136][-4.172102 -4.1806479 -4.1799092 -4.1735811 -4.168982 -4.1674342 -4.1673779 -4.1671057 -4.17913 -4.1983881 -4.2009616 -4.2130318 -4.22511 -4.22529 -4.223526][-4.1312351 -4.1224623 -4.1105146 -4.0996213 -4.0931358 -4.0925078 -4.0926709 -4.0951204 -4.1144195 -4.1397886 -4.1492848 -4.1696076 -4.1878214 -4.1870136 -4.1852736][-4.142221 -4.1033959 -4.0673246 -4.0390115 -4.0216551 -4.0191813 -4.0229664 -4.0286717 -4.05802 -4.0952687 -4.1231079 -4.1537733 -4.1772537 -4.1767192 -4.1729159][-4.1893525 -4.1332159 -4.0745845 -4.025383 -3.9985161 -3.9970045 -4.0053039 -4.01584 -4.05413 -4.1047568 -4.1482434 -4.1806388 -4.1996627 -4.1966496 -4.1901665][-4.233366 -4.1818829 -4.1199236 -4.0615935 -4.0320535 -4.0342474 -4.0509691 -4.0715728 -4.1131229 -4.1647596 -4.2051163 -4.2210236 -4.2245851 -4.218708 -4.2129946][-4.2568421 -4.2174969 -4.1618681 -4.1003852 -4.0609183 -4.0554314 -4.0714755 -4.097477 -4.138629 -4.1841264 -4.210568 -4.2105589 -4.2023087 -4.1977162 -4.2016978][-4.27355 -4.24422 -4.1913443 -4.1245894 -4.0727081 -4.0496063 -4.0538974 -4.0763817 -4.1143022 -4.1550522 -4.1731262 -4.1621 -4.1455665 -4.1434646 -4.1572633][-4.2887955 -4.2729273 -4.2256274 -4.158659 -4.1015825 -4.0639539 -4.0514026 -4.0614362 -4.0878773 -4.1203809 -4.1314516 -4.1140046 -4.0929875 -4.0909925 -4.1099005][-4.2934337 -4.2919726 -4.25731 -4.201911 -4.1530528 -4.1147656 -4.0961332 -4.0990257 -4.111794 -4.1316104 -4.1378217 -4.1219196 -4.1015816 -4.0968451 -4.1109133][-4.2810626 -4.2938404 -4.2800155 -4.249083 -4.21846 -4.1899219 -4.1740789 -4.1735067 -4.1768503 -4.186327 -4.1916413 -4.1859326 -4.1757932 -4.1706328 -4.173954]]...]
INFO - root - 2017-12-05 19:24:28.290173: step 36910, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 72h:57m:06s remains)
INFO - root - 2017-12-05 19:24:36.828442: step 36920, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 71h:07m:01s remains)
INFO - root - 2017-12-05 19:24:45.427167: step 36930, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 71h:12m:51s remains)
INFO - root - 2017-12-05 19:24:53.914367: step 36940, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 69h:46m:26s remains)
INFO - root - 2017-12-05 19:25:02.461720: step 36950, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 68h:35m:06s remains)
INFO - root - 2017-12-05 19:25:10.882896: step 36960, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 71h:55m:04s remains)
INFO - root - 2017-12-05 19:25:19.319690: step 36970, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:50m:42s remains)
INFO - root - 2017-12-05 19:25:27.678130: step 36980, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 68h:34m:31s remains)
INFO - root - 2017-12-05 19:25:36.118954: step 36990, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 67h:29m:43s remains)
INFO - root - 2017-12-05 19:25:44.747686: step 37000, loss = 2.01, batch loss = 1.95 (9.4 examples/sec; 0.854 sec/batch; 70h:04m:42s remains)
2017-12-05 19:25:45.535511: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1290855 -4.0814075 -4.0537643 -4.0509815 -4.0715833 -4.1054716 -4.1356564 -4.1549983 -4.165194 -4.1695518 -4.185029 -4.20917 -4.2197618 -4.2072344 -4.17747][-4.108633 -4.0784373 -4.0728478 -4.0847321 -4.1039414 -4.1268463 -4.1481175 -4.1537786 -4.1480718 -4.1396041 -4.1513276 -4.1798663 -4.1948581 -4.1893449 -4.173686][-4.0959578 -4.0799828 -4.091619 -4.1069751 -4.122138 -4.1407862 -4.1560678 -4.1466675 -4.1229954 -4.1012945 -4.1055169 -4.1362906 -4.1588531 -4.1640806 -4.1673131][-4.0952396 -4.0846791 -4.1062107 -4.1273279 -4.1411667 -4.1518626 -4.1579223 -4.1383018 -4.103301 -4.0707135 -4.060236 -4.084559 -4.1176662 -4.13941 -4.1602836][-4.1038566 -4.0891161 -4.1050086 -4.1234903 -4.1335855 -4.1374483 -4.1346211 -4.1112719 -4.0693607 -4.0232511 -3.9961767 -4.0245161 -4.0792875 -4.1209888 -4.1566248][-4.13423 -4.0986204 -4.0901852 -4.0915723 -4.084938 -4.0749784 -4.0691748 -4.0585313 -4.0264506 -3.9770317 -3.9435995 -3.978627 -4.0520272 -4.1111383 -4.1561036][-4.1647019 -4.1043444 -4.0650177 -4.039947 -4.0113368 -3.9899371 -3.9959633 -4.01012 -4.0008993 -3.9645107 -3.9295385 -3.959955 -4.0396543 -4.1099515 -4.1574559][-4.1737561 -4.0997348 -4.0420523 -4.003705 -3.9710491 -3.9604442 -3.9834032 -4.0148168 -4.0225706 -4.0016747 -3.9711857 -3.9876909 -4.0500765 -4.1123552 -4.1581416][-4.164927 -4.096559 -4.0481639 -4.0236778 -4.0083284 -4.0145044 -4.0496893 -4.0863862 -4.1045818 -4.0916886 -4.0664787 -4.0687637 -4.1050448 -4.1469922 -4.1823854][-4.161602 -4.1125121 -4.0847607 -4.0751309 -4.0729313 -4.086659 -4.1205683 -4.1539521 -4.1722283 -4.1642323 -4.1442213 -4.1376925 -4.1541696 -4.1786571 -4.2020159][-4.1655989 -4.1370168 -4.1267428 -4.1253648 -4.1283083 -4.1400056 -4.1628952 -4.1875138 -4.2043986 -4.2033172 -4.1880269 -4.1764607 -4.1810632 -4.196 -4.2138848][-4.1745143 -4.1623087 -4.1676574 -4.1789513 -4.1858983 -4.1930737 -4.2061138 -4.2214303 -4.236568 -4.2417488 -4.2340107 -4.2255869 -4.2272677 -4.23555 -4.2485423][-4.2025309 -4.2021317 -4.2161226 -4.2337766 -4.2439775 -4.2501736 -4.2562342 -4.2646041 -4.2770386 -4.2851129 -4.2831373 -4.278863 -4.2787538 -4.282074 -4.289773][-4.2400579 -4.2476473 -4.2647433 -4.2820516 -4.2911873 -4.2940536 -4.2954693 -4.2984142 -4.3063021 -4.3139777 -4.3154345 -4.3141646 -4.3131948 -4.3135996 -4.3166313][-4.2839818 -4.2940478 -4.3090339 -4.3219581 -4.3272753 -4.3275666 -4.327002 -4.3265576 -4.32929 -4.3334093 -4.3349738 -4.3346448 -4.3328657 -4.3305645 -4.3294969]]...]
INFO - root - 2017-12-05 19:25:54.029607: step 37010, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 71h:43m:08s remains)
INFO - root - 2017-12-05 19:26:02.457894: step 37020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 69h:26m:43s remains)
INFO - root - 2017-12-05 19:26:10.997465: step 37030, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 70h:38m:40s remains)
INFO - root - 2017-12-05 19:26:19.438522: step 37040, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 69h:43m:17s remains)
INFO - root - 2017-12-05 19:26:27.953094: step 37050, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 71h:05m:30s remains)
INFO - root - 2017-12-05 19:26:36.377391: step 37060, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 70h:13m:24s remains)
INFO - root - 2017-12-05 19:26:44.827553: step 37070, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 71h:04m:11s remains)
INFO - root - 2017-12-05 19:26:53.344577: step 37080, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:15m:24s remains)
INFO - root - 2017-12-05 19:27:01.874184: step 37090, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 69h:17m:00s remains)
INFO - root - 2017-12-05 19:27:10.485021: step 37100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:14m:38s remains)
2017-12-05 19:27:11.230173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2488217 -4.25272 -4.2591128 -4.2725344 -4.2780089 -4.2753267 -4.2764039 -4.2759032 -4.2727323 -4.2702961 -4.2687092 -4.266077 -4.2615967 -4.25839 -4.25598][-4.2422762 -4.2355094 -4.2359576 -4.2474723 -4.253849 -4.2553296 -4.2605815 -4.2618146 -4.2558875 -4.2496367 -4.2460651 -4.2422829 -4.2367945 -4.23354 -4.234345][-4.24237 -4.2344503 -4.2334776 -4.2437334 -4.2488575 -4.2517343 -4.2570944 -4.2568026 -4.2475514 -4.2375526 -4.2321224 -4.2284937 -4.2257347 -4.2252626 -4.2302241][-4.2478309 -4.2412467 -4.2402124 -4.2468 -4.2488427 -4.2498832 -4.2498779 -4.2409115 -4.2232542 -4.2099156 -4.2063565 -4.2088218 -4.2144909 -4.2197604 -4.2284245][-4.2576842 -4.2501984 -4.2477617 -4.2495284 -4.2490253 -4.2455068 -4.2339311 -4.2095118 -4.1813183 -4.1653757 -4.1671014 -4.1808109 -4.197906 -4.2110023 -4.2251983][-4.2794318 -4.2649956 -4.2566614 -4.2539148 -4.2508588 -4.239996 -4.2117229 -4.1665654 -4.1287704 -4.1192846 -4.1351805 -4.1650372 -4.1948152 -4.2129874 -4.2273641][-4.2865667 -4.25891 -4.2361069 -4.2251391 -4.21737 -4.1982727 -4.1550303 -4.0925856 -4.0506511 -4.0562134 -4.0928226 -4.1388063 -4.1789904 -4.20157 -4.2144313][-4.2531981 -4.2074966 -4.1676168 -4.1487336 -4.1382537 -4.1179132 -4.0715189 -4.0000629 -3.9586143 -3.98046 -4.0354147 -4.0952306 -4.1461406 -4.1754241 -4.1915865][-4.1970243 -4.1377792 -4.0879197 -4.0649314 -4.0567617 -4.0453796 -4.0142069 -3.955919 -3.928803 -3.9669192 -4.0320921 -4.0987029 -4.1535277 -4.1845064 -4.2019095][-4.1701832 -4.1160765 -4.0686135 -4.0450511 -4.0391121 -4.0386391 -4.0298557 -3.9979708 -3.9885783 -4.0301962 -4.0914884 -4.1548491 -4.2053952 -4.2321386 -4.2433281][-4.1841578 -4.145328 -4.1084304 -4.0861745 -4.0786886 -4.0813813 -4.0833793 -4.0708246 -4.0723844 -4.11133 -4.1668062 -4.2235122 -4.2665758 -4.2871037 -4.2883291][-4.2138996 -4.1907091 -4.16418 -4.1447487 -4.1350636 -4.1350574 -4.1395593 -4.1430292 -4.1577597 -4.19572 -4.2421303 -4.2867813 -4.3182554 -4.3284879 -4.3194485][-4.2593713 -4.2480774 -4.2297711 -4.2140141 -4.2050714 -4.2053823 -4.2122207 -4.2246809 -4.2462497 -4.2783327 -4.3094034 -4.3363733 -4.352314 -4.3509626 -4.3363495][-4.3046341 -4.3026013 -4.293447 -4.2846112 -4.2795463 -4.2813082 -4.2892761 -4.3033371 -4.32251 -4.3417611 -4.3542895 -4.3614731 -4.3619165 -4.3518224 -4.334425][-4.3191752 -4.3241487 -4.3239603 -4.3241386 -4.3257523 -4.3306952 -4.3376522 -4.3463812 -4.3540812 -4.3578215 -4.3554211 -4.3492274 -4.3410878 -4.3281264 -4.3124237]]...]
INFO - root - 2017-12-05 19:27:19.650806: step 37110, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:48m:36s remains)
INFO - root - 2017-12-05 19:27:28.099158: step 37120, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 68h:05m:36s remains)
INFO - root - 2017-12-05 19:27:36.593204: step 37130, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:30m:40s remains)
INFO - root - 2017-12-05 19:27:45.122801: step 37140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 71h:57m:06s remains)
INFO - root - 2017-12-05 19:27:53.558855: step 37150, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:13m:33s remains)
INFO - root - 2017-12-05 19:28:02.048861: step 37160, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 69h:53m:39s remains)
INFO - root - 2017-12-05 19:28:10.534134: step 37170, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:31m:37s remains)
INFO - root - 2017-12-05 19:28:18.819603: step 37180, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 71h:13m:58s remains)
INFO - root - 2017-12-05 19:28:27.340893: step 37190, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 68h:48m:47s remains)
INFO - root - 2017-12-05 19:28:35.855604: step 37200, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 69h:06m:19s remains)
2017-12-05 19:28:36.641251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2445769 -4.2462211 -4.2460556 -4.2472925 -4.25875 -4.276052 -4.28598 -4.2881103 -4.2845392 -4.2788205 -4.2742009 -4.2712708 -4.2690973 -4.2666264 -4.2685337][-4.1928797 -4.1884069 -4.184392 -4.1849651 -4.2007132 -4.2282591 -4.2436786 -4.2478766 -4.2445021 -4.2388382 -4.2351203 -4.2330823 -4.2306614 -4.2266455 -4.2296615][-4.158607 -4.1453466 -4.1367373 -4.13919 -4.1577072 -4.1899724 -4.2063956 -4.2111297 -4.2100735 -4.2079949 -4.2091422 -4.2119393 -4.2101645 -4.2031441 -4.2036228][-4.1612597 -4.1416683 -4.12873 -4.1306319 -4.1460147 -4.1715508 -4.1822453 -4.18443 -4.1849875 -4.1878052 -4.1978889 -4.2082877 -4.2107949 -4.2047234 -4.2030773][-4.1878648 -4.1703796 -4.1592889 -4.158052 -4.1647491 -4.1752186 -4.174613 -4.1707883 -4.1711121 -4.179194 -4.196157 -4.2135611 -4.2198548 -4.2178421 -4.2178173][-4.2112479 -4.1981959 -4.1928196 -4.1913352 -4.1904926 -4.1886744 -4.178916 -4.1702313 -4.1687741 -4.1781292 -4.1978655 -4.2171359 -4.2260723 -4.2283263 -4.2318311][-4.2134976 -4.2031975 -4.2027407 -4.20505 -4.2030458 -4.1970429 -4.182735 -4.170012 -4.1663694 -4.1748405 -4.192193 -4.2098703 -4.2206631 -4.2270317 -4.2347584][-4.1977143 -4.1897664 -4.1923847 -4.198761 -4.2018313 -4.2011046 -4.1856875 -4.1698508 -4.1643267 -4.1721339 -4.1856823 -4.1999488 -4.2115707 -4.2196312 -4.2287769][-4.1865392 -4.1826992 -4.1875772 -4.1980591 -4.2094584 -4.2164736 -4.2027493 -4.1823297 -4.1728239 -4.1784773 -4.1868038 -4.1953449 -4.2061424 -4.2155361 -4.2247329][-4.1952496 -4.1964087 -4.20364 -4.2173982 -4.2353759 -4.2474227 -4.2380486 -4.2174816 -4.202548 -4.2005486 -4.2023749 -4.2049017 -4.2138596 -4.2225142 -4.2304912][-4.2260714 -4.2305675 -4.2398772 -4.2538261 -4.2723131 -4.2866569 -4.2837553 -4.2675891 -4.24953 -4.2398143 -4.2343721 -4.2329044 -4.239398 -4.2436848 -4.2494187][-4.2684059 -4.275043 -4.2844224 -4.296382 -4.3106236 -4.3233314 -4.3246021 -4.31442 -4.2971807 -4.2848825 -4.2761168 -4.2716222 -4.2754345 -4.2769041 -4.2798619][-4.3092113 -4.3162842 -4.3234334 -4.3311758 -4.3395791 -4.3483715 -4.3518362 -4.3486009 -4.3370104 -4.3266897 -4.3187952 -4.3150148 -4.3167548 -4.315762 -4.3158259][-4.3383193 -4.3437214 -4.3470812 -4.350564 -4.35415 -4.3584232 -4.3627224 -4.3645158 -4.359529 -4.3526382 -4.3476734 -4.3455567 -4.3455052 -4.3426061 -4.34029][-4.3494592 -4.3528233 -4.3533721 -4.3536496 -4.3546062 -4.357038 -4.3605738 -4.3641086 -4.3640924 -4.3609157 -4.3581433 -4.3563638 -4.3551426 -4.3522325 -4.3498158]]...]
INFO - root - 2017-12-05 19:28:45.117259: step 37210, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 70h:41m:54s remains)
INFO - root - 2017-12-05 19:28:53.676161: step 37220, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.925 sec/batch; 75h:53m:45s remains)
INFO - root - 2017-12-05 19:29:02.183358: step 37230, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 70h:15m:11s remains)
INFO - root - 2017-12-05 19:29:10.727483: step 37240, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 68h:32m:14s remains)
INFO - root - 2017-12-05 19:29:19.213058: step 37250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 68h:52m:34s remains)
INFO - root - 2017-12-05 19:29:27.657183: step 37260, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.860 sec/batch; 70h:30m:39s remains)
INFO - root - 2017-12-05 19:29:36.208282: step 37270, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:32m:09s remains)
INFO - root - 2017-12-05 19:29:44.772212: step 37280, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 69h:34m:33s remains)
INFO - root - 2017-12-05 19:29:53.109903: step 37290, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 66h:49m:50s remains)
INFO - root - 2017-12-05 19:30:01.799707: step 37300, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 69h:49m:40s remains)
2017-12-05 19:30:02.589311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3085971 -4.3099089 -4.3036203 -4.2864594 -4.26457 -4.2450743 -4.235528 -4.2368355 -4.24316 -4.2577477 -4.2788639 -4.2919931 -4.2958279 -4.2991877 -4.30635][-4.2907634 -4.2941542 -4.2886081 -4.2683854 -4.2402525 -4.2139354 -4.1971846 -4.1879869 -4.1831546 -4.1945796 -4.2212448 -4.2426548 -4.2531261 -4.2587905 -4.2675018][-4.2734451 -4.2730403 -4.2659435 -4.2470164 -4.2207131 -4.1942029 -4.17135 -4.1501861 -4.1312432 -4.1384444 -4.1675639 -4.1924224 -4.2103033 -4.2208619 -4.2282176][-4.2608023 -4.2494068 -4.2365117 -4.218894 -4.1975617 -4.174334 -4.150847 -4.1262345 -4.0992813 -4.0979948 -4.1192956 -4.1396871 -4.165133 -4.1846981 -4.1933227][-4.2452388 -4.2227521 -4.2027941 -4.1846619 -4.1695576 -4.1573277 -4.1429 -4.122076 -4.0924344 -4.074399 -4.0771036 -4.0867772 -4.1162491 -4.145442 -4.1610508][-4.2277536 -4.2023163 -4.1731949 -4.1488004 -4.1387444 -4.1459446 -4.1505127 -4.1395884 -4.1095438 -4.0744057 -4.05708 -4.0541296 -4.0774 -4.1108022 -4.1325831][-4.2113585 -4.1892738 -4.1551862 -4.121901 -4.1125655 -4.1319761 -4.1565337 -4.1614213 -4.1402335 -4.0996051 -4.0668187 -4.0485258 -4.0593376 -4.0920854 -4.1211805][-4.1912432 -4.181685 -4.153553 -4.1165781 -4.1005187 -4.1169162 -4.14642 -4.1641374 -4.1599154 -4.1288738 -4.0942249 -4.0684476 -4.065865 -4.0928764 -4.1291184][-4.1678929 -4.1812134 -4.171926 -4.14756 -4.1314564 -4.1362081 -4.1563158 -4.1732903 -4.1781011 -4.1608796 -4.1325531 -4.1034975 -4.0873933 -4.1004825 -4.1349645][-4.1310577 -4.1636481 -4.1762376 -4.1738472 -4.1673241 -4.1674013 -4.17586 -4.1853228 -4.1925507 -4.187005 -4.1660619 -4.1376829 -4.1117163 -4.1067805 -4.1307912][-4.1057835 -4.1475616 -4.1743913 -4.1873097 -4.1900539 -4.1906767 -4.1957703 -4.202405 -4.2080007 -4.2087989 -4.1944151 -4.1691713 -4.1432815 -4.1300139 -4.1394038][-4.11067 -4.140532 -4.1649075 -4.1846061 -4.1965008 -4.2036176 -4.2100077 -4.2177505 -4.2227511 -4.2277842 -4.2201529 -4.1983356 -4.1755896 -4.1634321 -4.1671476][-4.1217208 -4.1341276 -4.1476712 -4.1615443 -4.174068 -4.1876712 -4.2001882 -4.2133956 -4.2225242 -4.2332268 -4.2356243 -4.225214 -4.210259 -4.1990204 -4.1964579][-4.1161652 -4.1197777 -4.1284885 -4.1348805 -4.144094 -4.157732 -4.1744127 -4.1942787 -4.2081685 -4.2222433 -4.2350974 -4.2383189 -4.2324581 -4.2236071 -4.2158365][-4.1037107 -4.0976057 -4.0960774 -4.0952911 -4.1042571 -4.1196833 -4.1394582 -4.1637979 -4.1813259 -4.1950412 -4.2098923 -4.2209759 -4.2241139 -4.2216935 -4.2155819]]...]
INFO - root - 2017-12-05 19:30:10.972317: step 37310, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 67h:42m:07s remains)
INFO - root - 2017-12-05 19:30:19.391486: step 37320, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 70h:33m:56s remains)
INFO - root - 2017-12-05 19:30:27.998823: step 37330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 71h:04m:54s remains)
INFO - root - 2017-12-05 19:30:36.523654: step 37340, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 70h:57m:06s remains)
INFO - root - 2017-12-05 19:30:45.060228: step 37350, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 70h:52m:40s remains)
INFO - root - 2017-12-05 19:30:53.449860: step 37360, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 69h:04m:35s remains)
INFO - root - 2017-12-05 19:31:02.035355: step 37370, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:09m:53s remains)
INFO - root - 2017-12-05 19:31:10.625747: step 37380, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 68h:56m:08s remains)
INFO - root - 2017-12-05 19:31:19.069428: step 37390, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 73h:23m:27s remains)
INFO - root - 2017-12-05 19:31:27.661967: step 37400, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.819 sec/batch; 67h:10m:00s remains)
2017-12-05 19:31:28.476420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2802458 -4.2762904 -4.2783413 -4.2918916 -4.3001924 -4.3017006 -4.3031297 -4.3032084 -4.2971039 -4.2785311 -4.2593403 -4.2354841 -4.1990542 -4.1668825 -4.1514621][-4.3039312 -4.3018847 -4.3042006 -4.3141079 -4.3172069 -4.3145638 -4.3145275 -4.3127117 -4.3028746 -4.277874 -4.250638 -4.2219586 -4.1858406 -4.154788 -4.1403027][-4.3109608 -4.3097258 -4.3105192 -4.3148823 -4.3130832 -4.3054929 -4.300066 -4.2954378 -4.2860184 -4.2625871 -4.2327843 -4.203393 -4.1711035 -4.1404018 -4.1265731][-4.29964 -4.2939582 -4.2928481 -4.2933726 -4.2864451 -4.2706604 -4.2570963 -4.2522459 -4.2507992 -4.2386465 -4.2146564 -4.1885419 -4.1596141 -4.1329479 -4.119483][-4.26402 -4.2472591 -4.2430243 -4.2409897 -4.2275343 -4.1989908 -4.1692519 -4.1683049 -4.1879139 -4.1950808 -4.1833968 -4.1640062 -4.1452136 -4.1315994 -4.12373][-4.2081246 -4.16946 -4.1592746 -4.1571407 -4.1342816 -4.0792093 -4.011848 -4.0158272 -4.0781403 -4.116848 -4.1217122 -4.10394 -4.0927892 -4.1040387 -4.1186461][-4.1427269 -4.0745859 -4.0635252 -4.070251 -4.0397882 -3.9497588 -3.8237441 -3.8339512 -3.9574242 -4.0353832 -4.05017 -4.0220838 -4.0136585 -4.0533175 -4.0972981][-4.0894418 -4.0074525 -4.0083213 -4.0258403 -3.9928889 -3.8885028 -3.7344177 -3.7513013 -3.9066896 -4.0042667 -4.0271454 -3.9916031 -3.9774675 -4.027431 -4.0835948][-4.0695271 -4.0098572 -4.0249281 -4.038784 -4.0117207 -3.94091 -3.8406155 -3.8490422 -3.9601526 -4.0441866 -4.074719 -4.0514765 -4.030973 -4.0549483 -4.0957537][-4.0851264 -4.0595636 -4.0770316 -4.0859442 -4.0743704 -4.0475588 -4.0019979 -3.9997373 -4.0614023 -4.122653 -4.155273 -4.1420512 -4.1168976 -4.1157093 -4.1346464][-4.1339641 -4.1330986 -4.1472783 -4.1558876 -4.1554437 -4.1500669 -4.129353 -4.1211529 -4.1503191 -4.1910415 -4.2197905 -4.2167325 -4.1941261 -4.18386 -4.191256][-4.2055135 -4.2127213 -4.2195072 -4.2278223 -4.2370844 -4.2376552 -4.2261124 -4.218996 -4.2288566 -4.2527361 -4.2725444 -4.2746892 -4.2604146 -4.2524638 -4.2568727][-4.269722 -4.276547 -4.2800231 -4.2873878 -4.2975082 -4.3005533 -4.2958746 -4.2912269 -4.2921929 -4.3032141 -4.312397 -4.3138423 -4.3057609 -4.3032341 -4.3071084][-4.3111162 -4.3172603 -4.3207445 -4.3275204 -4.3348866 -4.337564 -4.3355346 -4.3323069 -4.3292589 -4.3301487 -4.3313346 -4.3319464 -4.3285151 -4.3272552 -4.3292141][-4.32933 -4.3355613 -4.3406835 -4.3468471 -4.3505492 -4.3487673 -4.3452969 -4.3426127 -4.3367915 -4.331439 -4.3281388 -4.3269858 -4.3258328 -4.3263278 -4.3285389]]...]
INFO - root - 2017-12-05 19:31:37.091412: step 37410, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 69h:13m:04s remains)
INFO - root - 2017-12-05 19:31:45.502678: step 37420, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 69h:28m:17s remains)
INFO - root - 2017-12-05 19:31:54.103361: step 37430, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 72h:29m:25s remains)
INFO - root - 2017-12-05 19:32:02.763438: step 37440, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 71h:37m:31s remains)
INFO - root - 2017-12-05 19:32:11.408857: step 37450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:39m:29s remains)
INFO - root - 2017-12-05 19:32:19.941610: step 37460, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 69h:42m:32s remains)
INFO - root - 2017-12-05 19:32:28.598022: step 37470, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 71h:25m:41s remains)
INFO - root - 2017-12-05 19:32:37.179096: step 37480, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.857 sec/batch; 70h:11m:59s remains)
INFO - root - 2017-12-05 19:32:45.836041: step 37490, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 71h:08m:51s remains)
INFO - root - 2017-12-05 19:32:54.289942: step 37500, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:40m:28s remains)
2017-12-05 19:32:55.051099: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1331944 -4.1257787 -4.1374946 -4.1655507 -4.1778359 -4.1770425 -4.173099 -4.176888 -4.1832347 -4.1911087 -4.2039175 -4.2108245 -4.2017097 -4.1856203 -4.172492][-4.1391611 -4.133204 -4.145524 -4.173996 -4.1914554 -4.19107 -4.1801462 -4.1745644 -4.1790218 -4.1869268 -4.2044716 -4.2161431 -4.2089787 -4.1929073 -4.178431][-4.160059 -4.1522274 -4.1550846 -4.1763763 -4.1948562 -4.1935716 -4.178566 -4.1672864 -4.16748 -4.174993 -4.1974983 -4.2135015 -4.2139912 -4.205554 -4.194344][-4.1581659 -4.1508551 -4.1561413 -4.1819057 -4.2041869 -4.1997004 -4.1773605 -4.1603971 -4.1564851 -4.1643367 -4.1916237 -4.2166958 -4.2292471 -4.2309217 -4.2222729][-4.1441493 -4.1464853 -4.1645818 -4.1940389 -4.2108927 -4.1949935 -4.1639533 -4.1398125 -4.1319041 -4.1461544 -4.189374 -4.2332015 -4.25812 -4.2642469 -4.2524371][-4.1397672 -4.15503 -4.178586 -4.1963196 -4.1920528 -4.1571317 -4.1134772 -4.0811825 -4.0795159 -4.1139369 -4.1813703 -4.2436666 -4.2775269 -4.2828975 -4.2666707][-4.1523094 -4.1700568 -4.1851363 -4.1821432 -4.1520085 -4.0920215 -4.0302639 -3.9914849 -4.0083103 -4.0797586 -4.1688762 -4.2333727 -4.266593 -4.2693739 -4.2536545][-4.1616988 -4.1771317 -4.1849804 -4.1695209 -4.1244154 -4.0506692 -3.9803171 -3.944869 -3.9831128 -4.0733147 -4.1612182 -4.2162395 -4.2436872 -4.2476139 -4.2394023][-4.1702275 -4.1869936 -4.1912112 -4.17322 -4.1301327 -4.0667458 -4.0104842 -3.9890881 -4.0289125 -4.1038718 -4.1723642 -4.2133746 -4.232079 -4.2357807 -4.2331777][-4.1958661 -4.2134666 -4.2158675 -4.2005119 -4.1675711 -4.1234317 -4.0833697 -4.0655031 -4.0888076 -4.1412416 -4.1927662 -4.2229462 -4.2329335 -4.2325511 -4.2301726][-4.2268982 -4.2397947 -4.23784 -4.2214913 -4.1969142 -4.168087 -4.1423907 -4.1288967 -4.1432862 -4.1828222 -4.2183208 -4.2364936 -4.2371984 -4.231545 -4.2259879][-4.2361274 -4.241776 -4.2369981 -4.2227321 -4.2052264 -4.1876688 -4.1754613 -4.1732016 -4.1884356 -4.2166815 -4.2354059 -4.2375512 -4.2291107 -4.2216234 -4.21687][-4.2268348 -4.2275105 -4.22497 -4.2190213 -4.2121968 -4.2088995 -4.2088637 -4.2126217 -4.220818 -4.2325721 -4.2352676 -4.2268262 -4.2173452 -4.2166877 -4.2189636][-4.2147565 -4.2141962 -4.2202172 -4.2242537 -4.2281432 -4.2367578 -4.2417393 -4.2414131 -4.23668 -4.2327685 -4.2271495 -4.2201715 -4.2207203 -4.2286143 -4.2347007][-4.1871324 -4.1858721 -4.2022605 -4.2242246 -4.2450542 -4.2635722 -4.2693996 -4.2610188 -4.244513 -4.2315297 -4.2256269 -4.2251482 -4.2348423 -4.2460265 -4.2497654]]...]
INFO - root - 2017-12-05 19:33:03.545039: step 37510, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 67h:37m:17s remains)
INFO - root - 2017-12-05 19:33:12.155202: step 37520, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 70h:38m:43s remains)
INFO - root - 2017-12-05 19:33:20.572158: step 37530, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 70h:48m:48s remains)
INFO - root - 2017-12-05 19:33:29.191728: step 37540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:33m:47s remains)
INFO - root - 2017-12-05 19:33:37.763742: step 37550, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 71h:18m:01s remains)
INFO - root - 2017-12-05 19:33:46.209162: step 37560, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 71h:06m:53s remains)
INFO - root - 2017-12-05 19:33:54.811300: step 37570, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 71h:00m:56s remains)
INFO - root - 2017-12-05 19:34:03.258871: step 37580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 68h:52m:46s remains)
INFO - root - 2017-12-05 19:34:11.768349: step 37590, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:53m:20s remains)
INFO - root - 2017-12-05 19:34:20.241102: step 37600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 70h:20m:18s remains)
2017-12-05 19:34:21.007469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1650414 -4.1410656 -4.1240911 -4.1118288 -4.1076632 -4.1119022 -4.1174474 -4.1155243 -4.1162467 -4.129756 -4.1522145 -4.1608834 -4.1573877 -4.1487808 -4.1429319][-4.1532116 -4.1282849 -4.1150789 -4.1088781 -4.1058192 -4.1113348 -4.1201811 -4.1203051 -4.1252089 -4.1439352 -4.1618247 -4.16181 -4.1522875 -4.1372614 -4.1276116][-4.1593556 -4.1441059 -4.1321521 -4.123065 -4.1168547 -4.1218877 -4.1290722 -4.1316428 -4.1404037 -4.15827 -4.1705823 -4.1622534 -4.1473875 -4.1279516 -4.1141963][-4.1764441 -4.1697726 -4.1565981 -4.1396556 -4.1290388 -4.1291614 -4.1293712 -4.133965 -4.1511545 -4.1754408 -4.1856709 -4.1737103 -4.1553359 -4.1331306 -4.1178293][-4.1924386 -4.1889215 -4.1759882 -4.1553907 -4.136755 -4.1242728 -4.1151628 -4.1211438 -4.14632 -4.1773071 -4.1893058 -4.1799321 -4.164206 -4.1470718 -4.1383119][-4.1916938 -4.1908469 -4.1773787 -4.1494875 -4.1174512 -4.0899739 -4.0761786 -4.0900965 -4.1246204 -4.158587 -4.1754479 -4.1757445 -4.1725 -4.1660438 -4.1674552][-4.1777492 -4.1757812 -4.1569471 -4.1172776 -4.07391 -4.0431757 -4.0355053 -4.0583 -4.0962086 -4.1326122 -4.1564903 -4.168468 -4.1748185 -4.179605 -4.1870866][-4.1613226 -4.1598096 -4.1374469 -4.0919027 -4.0442619 -4.0132332 -4.0111995 -4.0368476 -4.0743046 -4.1127224 -4.1408839 -4.1570764 -4.1628737 -4.1671152 -4.1750484][-4.1558509 -4.156765 -4.1377091 -4.0996003 -4.0617595 -4.0386658 -4.0371046 -4.0534706 -4.0851145 -4.119513 -4.1386423 -4.1453915 -4.1384296 -4.1287327 -4.1323471][-4.1528988 -4.158257 -4.1485815 -4.1286569 -4.108603 -4.096673 -4.0934005 -4.1032295 -4.1284428 -4.1498561 -4.1546807 -4.1470656 -4.123909 -4.1007433 -4.099719][-4.1553655 -4.1629181 -4.1644897 -4.1596017 -4.1493731 -4.14042 -4.1378078 -4.1479983 -4.1663179 -4.1784129 -4.1749277 -4.1578584 -4.124876 -4.0974507 -4.0961223][-4.1659565 -4.1663876 -4.1680565 -4.1666279 -4.1615038 -4.1585631 -4.1592531 -4.1718321 -4.1844897 -4.1933641 -4.187952 -4.1681895 -4.1347618 -4.1115966 -4.1130919][-4.180346 -4.1681652 -4.1620178 -4.1609621 -4.1604075 -4.1655779 -4.173058 -4.18607 -4.192349 -4.1996384 -4.1945953 -4.1776218 -4.1514554 -4.1321545 -4.1289][-4.1765938 -4.1596403 -4.1518021 -4.1527343 -4.15681 -4.1673274 -4.176064 -4.1855054 -4.1887922 -4.1960182 -4.1917348 -4.1790447 -4.1611171 -4.1429405 -4.131628][-4.1480918 -4.1317358 -4.1338167 -4.1442223 -4.1542735 -4.1659384 -4.1723204 -4.174026 -4.1735048 -4.1801186 -4.1761274 -4.1646395 -4.1520424 -4.1338181 -4.1189604]]...]
INFO - root - 2017-12-05 19:34:29.464262: step 37610, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 70h:26m:11s remains)
INFO - root - 2017-12-05 19:34:38.086530: step 37620, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 72h:11m:08s remains)
INFO - root - 2017-12-05 19:34:46.800053: step 37630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 69h:57m:41s remains)
INFO - root - 2017-12-05 19:34:55.252131: step 37640, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 70h:17m:50s remains)
INFO - root - 2017-12-05 19:35:03.830973: step 37650, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 68h:31m:57s remains)
INFO - root - 2017-12-05 19:35:12.212743: step 37660, loss = 2.07, batch loss = 2.01 (8.8 examples/sec; 0.908 sec/batch; 74h:23m:37s remains)
INFO - root - 2017-12-05 19:35:20.697775: step 37670, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 71h:17m:53s remains)
INFO - root - 2017-12-05 19:35:29.324322: step 37680, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 69h:43m:36s remains)
INFO - root - 2017-12-05 19:35:37.764044: step 37690, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 70h:11m:59s remains)
INFO - root - 2017-12-05 19:35:46.396451: step 37700, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.882 sec/batch; 72h:15m:28s remains)
2017-12-05 19:35:47.209088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1678491 -4.2005353 -4.2280354 -4.2370934 -4.2449431 -4.2460179 -4.2436891 -4.2390132 -4.2316303 -4.2145681 -4.1773348 -4.1201296 -4.0719347 -4.0710559 -4.1128039][-4.1414838 -4.1793985 -4.2133064 -4.2255359 -4.2336807 -4.2323303 -4.2279606 -4.222321 -4.2130089 -4.1941495 -4.1549091 -4.098403 -4.0604744 -4.0759344 -4.1266179][-4.1289363 -4.1634421 -4.1997 -4.2144532 -4.2225132 -4.2185078 -4.211019 -4.20296 -4.1915631 -4.1766195 -4.1497765 -4.1095347 -4.089045 -4.1131988 -4.1595144][-4.1160092 -4.1398616 -4.17047 -4.1869521 -4.1939349 -4.1858034 -4.169209 -4.1540933 -4.142334 -4.1399713 -4.1393218 -4.128768 -4.1296854 -4.1545038 -4.1893978][-4.106926 -4.1217785 -4.1437764 -4.1577282 -4.1584096 -4.138649 -4.1071892 -4.0852456 -4.080442 -4.1010032 -4.1300855 -4.1482182 -4.164753 -4.1848984 -4.2087207][-4.1029382 -4.1185813 -4.140842 -4.15147 -4.1429873 -4.1066828 -4.0528426 -4.013864 -4.0104423 -4.0530295 -4.1059179 -4.1447735 -4.1704726 -4.1850452 -4.202147][-4.0861845 -4.114728 -4.1387482 -4.1466918 -4.1304092 -4.0747957 -3.9917467 -3.9207506 -3.9069793 -3.9709976 -4.0495534 -4.1091852 -4.1456881 -4.1622391 -4.178194][-4.0462217 -4.0918288 -4.1165442 -4.1227078 -4.102879 -4.0328741 -3.9175982 -3.8083904 -3.783164 -3.8717473 -3.9815328 -4.0660152 -4.1159544 -4.1387849 -4.152751][-4.0553989 -4.1095142 -4.1334257 -4.1412783 -4.1277971 -4.0624833 -3.9499671 -3.8422685 -3.8235154 -3.9104574 -4.0158739 -4.0967927 -4.1416068 -4.1574883 -4.159358][-4.0976491 -4.149024 -4.1752605 -4.1848888 -4.1781607 -4.130044 -4.0496907 -3.9809813 -3.9807277 -4.0409789 -4.1114154 -4.1628723 -4.185853 -4.1860595 -4.1758308][-4.1430688 -4.1878433 -4.213654 -4.2200437 -4.2122335 -4.1777081 -4.1291142 -4.0960751 -4.1071877 -4.1449666 -4.1816239 -4.2019811 -4.2017484 -4.1865077 -4.1731477][-4.1702423 -4.2131596 -4.239325 -4.2410455 -4.2336631 -4.2120638 -4.1877904 -4.1774378 -4.1893034 -4.2073445 -4.2184715 -4.2139854 -4.193368 -4.1680121 -4.1609387][-4.1824155 -4.2265882 -4.2519259 -4.249382 -4.2433529 -4.2309456 -4.2227883 -4.2250171 -4.234724 -4.2385855 -4.2302346 -4.2055774 -4.1693687 -4.1443043 -4.1510992][-4.1779256 -4.225204 -4.2485118 -4.2431369 -4.2370982 -4.2288513 -4.2252932 -4.2291188 -4.236321 -4.2316837 -4.2092085 -4.1704149 -4.130497 -4.1196804 -4.1401362][-4.1846647 -4.2331748 -4.2562456 -4.2496319 -4.2423339 -4.2351217 -4.23173 -4.234786 -4.2379923 -4.22605 -4.1952581 -4.1508145 -4.117178 -4.1215458 -4.1491523]]...]
INFO - root - 2017-12-05 19:35:55.634657: step 37710, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 68h:26m:21s remains)
INFO - root - 2017-12-05 19:36:04.216612: step 37720, loss = 2.09, batch loss = 2.04 (9.0 examples/sec; 0.886 sec/batch; 72h:35m:00s remains)
INFO - root - 2017-12-05 19:36:12.691171: step 37730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 70h:04m:39s remains)
INFO - root - 2017-12-05 19:36:21.250543: step 37740, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.847 sec/batch; 69h:20m:16s remains)
INFO - root - 2017-12-05 19:36:29.732872: step 37750, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 71h:49m:33s remains)
INFO - root - 2017-12-05 19:36:38.290780: step 37760, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.876 sec/batch; 71h:43m:57s remains)
INFO - root - 2017-12-05 19:36:46.886375: step 37770, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 69h:50m:23s remains)
INFO - root - 2017-12-05 19:36:55.437191: step 37780, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 72h:35m:35s remains)
INFO - root - 2017-12-05 19:37:04.035399: step 37790, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 69h:44m:30s remains)
INFO - root - 2017-12-05 19:37:12.596540: step 37800, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 72h:16m:01s remains)
2017-12-05 19:37:13.457459: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.256814 -4.2629437 -4.243814 -4.2017279 -4.1481133 -4.1102095 -4.1094708 -4.1433682 -4.1877966 -4.2134557 -4.2150097 -4.2107954 -4.2067294 -4.2057152 -4.2080593][-4.253726 -4.2620053 -4.2465987 -4.2117152 -4.1648722 -4.1300993 -4.1290526 -4.1604214 -4.2035103 -4.2256889 -4.2223015 -4.2156353 -4.2046337 -4.1982508 -4.2000804][-4.2373595 -4.2504258 -4.245645 -4.2247982 -4.1942735 -4.1679869 -4.1628089 -4.1833949 -4.2167492 -4.2326531 -4.2234154 -4.2143664 -4.19934 -4.1884365 -4.1869245][-4.2156935 -4.2372284 -4.2477045 -4.2444577 -4.2281857 -4.2044878 -4.1885653 -4.1935496 -4.2127776 -4.2191825 -4.2062721 -4.1970062 -4.1845174 -4.1756134 -4.1755137][-4.2056365 -4.232872 -4.2523103 -4.2569647 -4.2407737 -4.2108808 -4.1839638 -4.1783018 -4.1925092 -4.1933479 -4.1793623 -4.1711483 -4.1616688 -4.1555367 -4.1593661][-4.2221041 -4.2435603 -4.2573576 -4.2558546 -4.2269344 -4.1814437 -4.139967 -4.1294513 -4.1510329 -4.1588149 -4.1501594 -4.1433682 -4.1354246 -4.1306429 -4.1378355][-4.2500973 -4.2592683 -4.25792 -4.2440381 -4.2032242 -4.1390066 -4.0721378 -4.0530429 -4.0927863 -4.1233826 -4.1282344 -4.1258688 -4.1214938 -4.1186848 -4.1254859][-4.2555285 -4.2498708 -4.2357693 -4.2136097 -4.167964 -4.0908566 -3.9993863 -3.9715362 -4.0376487 -4.1017785 -4.12922 -4.1328564 -4.1298637 -4.125114 -4.1266413][-4.2391219 -4.216918 -4.19086 -4.1665468 -4.1311336 -4.06778 -3.9843657 -3.9640229 -4.0404572 -4.1164412 -4.1519132 -4.157433 -4.1541677 -4.1483474 -4.1420913][-4.2127085 -4.1785164 -4.1474404 -4.128963 -4.1159363 -4.08859 -4.0440483 -4.04104 -4.1000848 -4.1560297 -4.1814017 -4.1824117 -4.1763005 -4.1648788 -4.1471758][-4.1904616 -4.161725 -4.136982 -4.1265497 -4.1297431 -4.1263247 -4.1090503 -4.1110468 -4.1463118 -4.1811213 -4.1994338 -4.1997037 -4.1907911 -4.1735754 -4.1464858][-4.184844 -4.1704311 -4.1571112 -4.1514678 -4.1575241 -4.16008 -4.1492515 -4.1461625 -4.1630077 -4.1858144 -4.2054372 -4.2096863 -4.2029119 -4.1843953 -4.1496248][-4.1926928 -4.1931057 -4.192358 -4.1904364 -4.1924043 -4.192493 -4.1786194 -4.1627579 -4.163116 -4.1784487 -4.1990585 -4.2067342 -4.2051873 -4.1893587 -4.1535454][-4.2083325 -4.2184563 -4.2262316 -4.227519 -4.2263122 -4.2234931 -4.2063847 -4.180397 -4.1673312 -4.1735492 -4.1885381 -4.1977015 -4.2007346 -4.1901956 -4.1623745][-4.2277746 -4.2432022 -4.2567983 -4.2606955 -4.2584181 -4.2528811 -4.2345467 -4.2041435 -4.180058 -4.173305 -4.1766195 -4.1839232 -4.1888947 -4.1861348 -4.1731939]]...]
INFO - root - 2017-12-05 19:37:22.046676: step 37810, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 67h:09m:28s remains)
INFO - root - 2017-12-05 19:37:30.393962: step 37820, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 68h:07m:47s remains)
INFO - root - 2017-12-05 19:37:38.893198: step 37830, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 69h:59m:56s remains)
INFO - root - 2017-12-05 19:37:47.665228: step 37840, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 70h:13m:19s remains)
INFO - root - 2017-12-05 19:37:56.188790: step 37850, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 70h:25m:47s remains)
INFO - root - 2017-12-05 19:38:04.590538: step 37860, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 69h:34m:55s remains)
INFO - root - 2017-12-05 19:38:13.100703: step 37870, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:21m:29s remains)
INFO - root - 2017-12-05 19:38:21.680478: step 37880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 70h:15m:56s remains)
INFO - root - 2017-12-05 19:38:30.220201: step 37890, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 67h:58m:26s remains)
INFO - root - 2017-12-05 19:38:38.854607: step 37900, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 71h:15m:50s remains)
2017-12-05 19:38:39.609031: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2516313 -4.2462306 -4.2208452 -4.1984797 -4.186079 -4.1872826 -4.20094 -4.2151728 -4.2269177 -4.2236066 -4.2160335 -4.2131138 -4.2160769 -4.2172308 -4.2189436][-4.2307134 -4.2373886 -4.22561 -4.2112579 -4.2051587 -4.2077136 -4.2156262 -4.2246652 -4.2352586 -4.2323046 -4.2211342 -4.2123175 -4.211235 -4.21185 -4.2148132][-4.220017 -4.2303977 -4.2286916 -4.224165 -4.2265668 -4.2340555 -4.2414393 -4.2491536 -4.2575426 -4.2524447 -4.2358236 -4.2169528 -4.2075205 -4.2055774 -4.2077646][-4.2366056 -4.2457285 -4.2482553 -4.2480369 -4.2493472 -4.2525125 -4.257453 -4.2634058 -4.26916 -4.2627025 -4.2441797 -4.2188282 -4.203021 -4.2013674 -4.2050171][-4.2703218 -4.2773495 -4.2757139 -4.2676883 -4.2566361 -4.245966 -4.240159 -4.2407517 -4.2462478 -4.2426081 -4.2256846 -4.2028117 -4.1924791 -4.1986418 -4.2105465][-4.2938852 -4.2994418 -4.2920914 -4.2751317 -4.2496262 -4.22094 -4.1956334 -4.1825891 -4.1858268 -4.186893 -4.1767335 -4.1663647 -4.1709356 -4.1897168 -4.2107053][-4.3092289 -4.3099613 -4.2964444 -4.2733502 -4.2359576 -4.1867476 -4.1338739 -4.0963798 -4.0941906 -4.1064372 -4.1126075 -4.1219664 -4.1440263 -4.1742659 -4.2013178][-4.3147836 -4.3102574 -4.2908607 -4.260952 -4.2154126 -4.1506615 -4.074297 -4.0137591 -4.0103378 -4.0394063 -4.0663805 -4.0932417 -4.1283331 -4.1641483 -4.1915274][-4.3038878 -4.293272 -4.2702913 -4.2371817 -4.1898837 -4.1256294 -4.0505581 -3.9929483 -3.994369 -4.0319781 -4.0708437 -4.1050043 -4.137958 -4.1650786 -4.1836886][-4.2810512 -4.2638974 -4.2391033 -4.2093725 -4.1717148 -4.1259236 -4.0767589 -4.0433254 -4.0513639 -4.088131 -4.124372 -4.1516118 -4.1728539 -4.1832762 -4.1889863][-4.2507591 -4.2320704 -4.2095242 -4.18768 -4.1679382 -4.1489191 -4.1296954 -4.1197309 -4.1333742 -4.1643906 -4.1910067 -4.2084589 -4.2198181 -4.2175813 -4.2135582][-4.2219586 -4.2088394 -4.1924553 -4.1811929 -4.1794262 -4.1827855 -4.1845074 -4.1874809 -4.2022839 -4.2268858 -4.2459435 -4.2563329 -4.2612376 -4.2542892 -4.2483435][-4.2039771 -4.1975422 -4.1879468 -4.186779 -4.1975965 -4.2115917 -4.220798 -4.2262831 -4.2385974 -4.2567911 -4.2727866 -4.2822189 -4.2827044 -4.2731462 -4.2665482][-4.2021065 -4.20012 -4.1956348 -4.200325 -4.2143536 -4.2261162 -4.2298484 -4.2281632 -4.234097 -4.2481666 -4.2673874 -4.2817507 -4.2824106 -4.2733612 -4.2675529][-4.2056956 -4.2081871 -4.2074895 -4.2136645 -4.2241158 -4.2275829 -4.2213755 -4.2097645 -4.2076745 -4.2201362 -4.2421703 -4.2583914 -4.257772 -4.2494779 -4.2435665]]...]
INFO - root - 2017-12-05 19:38:48.023747: step 37910, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.809 sec/batch; 66h:11m:21s remains)
INFO - root - 2017-12-05 19:38:56.464239: step 37920, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.786 sec/batch; 64h:19m:24s remains)
INFO - root - 2017-12-05 19:39:04.953718: step 37930, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 70h:46m:44s remains)
INFO - root - 2017-12-05 19:39:13.491993: step 37940, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 69h:41m:13s remains)
INFO - root - 2017-12-05 19:39:22.263692: step 37950, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.832 sec/batch; 68h:03m:39s remains)
INFO - root - 2017-12-05 19:39:30.720776: step 37960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 70h:37m:05s remains)
INFO - root - 2017-12-05 19:39:39.146868: step 37970, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 69h:53m:22s remains)
INFO - root - 2017-12-05 19:39:47.721958: step 37980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 68h:09m:57s remains)
INFO - root - 2017-12-05 19:39:56.269547: step 37990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 68h:46m:58s remains)
INFO - root - 2017-12-05 19:40:04.650178: step 38000, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.809 sec/batch; 66h:10m:35s remains)
2017-12-05 19:40:05.386439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2640781 -4.2389197 -4.207828 -4.184052 -4.1727152 -4.1761527 -4.1807618 -4.186801 -4.1960487 -4.2084994 -4.2166243 -4.2130833 -4.2084808 -4.208312 -4.2049632][-4.2467284 -4.2218227 -4.1917853 -4.1688824 -4.1540203 -4.1582365 -4.16691 -4.1722755 -4.1815362 -4.1949096 -4.2048464 -4.203229 -4.2040143 -4.2095017 -4.2083697][-4.2244797 -4.2045465 -4.1842146 -4.1688576 -4.1569376 -4.1647267 -4.1749744 -4.175199 -4.1787839 -4.1881838 -4.1982927 -4.2023282 -4.2103324 -4.2224813 -4.2277293][-4.1953454 -4.1817913 -4.1765108 -4.1777458 -4.178874 -4.1916313 -4.2019248 -4.1951046 -4.1885767 -4.1906729 -4.2005486 -4.2128544 -4.2288065 -4.2478418 -4.2583027][-4.1517611 -4.1426697 -4.1511068 -4.169723 -4.18615 -4.2046838 -4.2149959 -4.2041049 -4.1926675 -4.1920757 -4.2032986 -4.2222304 -4.2413054 -4.2597666 -4.2709332][-4.0997381 -4.0887923 -4.1061454 -4.139112 -4.1666584 -4.1902204 -4.2070212 -4.2029977 -4.1981306 -4.1999688 -4.2110605 -4.2297335 -4.2456112 -4.2573023 -4.2636318][-4.0665703 -4.0474625 -4.0647044 -4.0998507 -4.1262245 -4.1495285 -4.1716919 -4.1808515 -4.1897006 -4.1975665 -4.2079797 -4.2245884 -4.2380958 -4.2444968 -4.2480111][-4.0596423 -4.0318718 -4.0399017 -4.0608444 -4.0753074 -4.096364 -4.1213827 -4.1410809 -4.1602221 -4.1756253 -4.187283 -4.2015176 -4.2131886 -4.22036 -4.2245593][-4.0752687 -4.04666 -4.0423083 -4.0416574 -4.0450029 -4.0628271 -4.08615 -4.1116824 -4.1365304 -4.1578994 -4.1720266 -4.1835217 -4.1935644 -4.2053857 -4.21187][-4.0950632 -4.0766759 -4.0674806 -4.0549889 -4.0527344 -4.0639849 -4.0833883 -4.1047149 -4.1276197 -4.1495109 -4.1645012 -4.1706047 -4.1779213 -4.1950531 -4.2045288][-4.1103473 -4.1032181 -4.0957437 -4.0821829 -4.0768471 -4.0827384 -4.0951829 -4.1105928 -4.1282659 -4.1480484 -4.1621065 -4.1625519 -4.1616044 -4.1767659 -4.1871371][-4.1109304 -4.111784 -4.107151 -4.094871 -4.0893197 -4.0923405 -4.1028175 -4.1157818 -4.1292 -4.1465669 -4.1581368 -4.1558709 -4.1504493 -4.1594391 -4.170557][-4.1014462 -4.1058922 -4.1022983 -4.0937486 -4.0922427 -4.0950575 -4.1029568 -4.1122637 -4.1223068 -4.1350489 -4.1440086 -4.1467509 -4.1460276 -4.1521983 -4.1644049][-4.0953908 -4.1022077 -4.0979056 -4.0910282 -4.092648 -4.0982509 -4.1037278 -4.1056538 -4.1067495 -4.112709 -4.11993 -4.1306276 -4.1386075 -4.1458144 -4.1568704][-4.1089382 -4.1145339 -4.110333 -4.1050396 -4.1104531 -4.1199446 -4.1244311 -4.1229458 -4.1202469 -4.1201544 -4.1253242 -4.1365495 -4.1446033 -4.1489158 -4.1548629]]...]
INFO - root - 2017-12-05 19:40:13.923556: step 38010, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 69h:32m:02s remains)
INFO - root - 2017-12-05 19:40:22.567024: step 38020, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 69h:33m:10s remains)
INFO - root - 2017-12-05 19:40:31.027983: step 38030, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 69h:01m:16s remains)
INFO - root - 2017-12-05 19:40:39.531259: step 38040, loss = 2.11, batch loss = 2.05 (9.6 examples/sec; 0.830 sec/batch; 67h:51m:25s remains)
INFO - root - 2017-12-05 19:40:48.154272: step 38050, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 70h:51m:30s remains)
INFO - root - 2017-12-05 19:40:56.548132: step 38060, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.815 sec/batch; 66h:38m:57s remains)
INFO - root - 2017-12-05 19:41:05.104026: step 38070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:41m:08s remains)
INFO - root - 2017-12-05 19:41:13.610302: step 38080, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:33m:10s remains)
INFO - root - 2017-12-05 19:41:22.245220: step 38090, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:54m:40s remains)
INFO - root - 2017-12-05 19:41:30.736246: step 38100, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 67h:10m:19s remains)
2017-12-05 19:41:31.536084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0757957 -4.0613556 -4.0828977 -4.1207132 -4.1614523 -4.1773682 -4.1773086 -4.1673203 -4.14104 -4.0940685 -4.0502234 -4.0416775 -4.068409 -4.1101155 -4.1562853][-4.0661173 -4.0566635 -4.078908 -4.1226411 -4.1616883 -4.1731625 -4.1616273 -4.1385875 -4.1029129 -4.0485282 -3.99745 -3.9905012 -4.033628 -4.0949039 -4.1497164][-4.0858707 -4.0841808 -4.1006136 -4.139195 -4.1660247 -4.1642113 -4.13988 -4.1094465 -4.0751 -4.02475 -3.9715981 -3.9668212 -4.0224319 -4.0993991 -4.164638][-4.1288295 -4.12952 -4.1370873 -4.1565576 -4.1595268 -4.1365943 -4.0994744 -4.06985 -4.0564575 -4.0325603 -3.9938536 -3.9929528 -4.0489678 -4.1249804 -4.1949754][-4.1923623 -4.1908288 -4.183239 -4.1730318 -4.1393232 -4.0873165 -4.03216 -4.0078759 -4.0307417 -4.0545144 -4.0537772 -4.0677719 -4.1196756 -4.182559 -4.2418108][-4.2384944 -4.2268095 -4.2023487 -4.1629591 -4.0938148 -4.0051951 -3.92316 -3.9031219 -3.972193 -4.0513444 -4.0901651 -4.1243711 -4.176856 -4.2315512 -4.2781777][-4.2596812 -4.2358642 -4.1944432 -4.1305966 -4.0298023 -3.9032779 -3.7852955 -3.7651262 -3.8811717 -4.0151739 -4.0892196 -4.1438131 -4.2071061 -4.26315 -4.3014][-4.257123 -4.2279592 -4.1836715 -4.1193895 -4.0168881 -3.8834877 -3.7482309 -3.71733 -3.8401175 -3.9859443 -4.0756574 -4.14603 -4.2203979 -4.27981 -4.3132896][-4.2504163 -4.224154 -4.1903687 -4.1421046 -4.0637584 -3.9619548 -3.8622711 -3.8402052 -3.9216142 -4.0272293 -4.1004419 -4.1654992 -4.2322769 -4.2847638 -4.3132458][-4.2568903 -4.2340822 -4.2100353 -4.1773162 -4.1201081 -4.0507717 -3.9965086 -3.9916172 -4.0417147 -4.1112046 -4.1639977 -4.2120528 -4.2574582 -4.2919087 -4.3115749][-4.2785444 -4.2599063 -4.2432652 -4.2197876 -4.1801605 -4.13625 -4.1116586 -4.1178875 -4.1514654 -4.1982117 -4.2347021 -4.26596 -4.2893782 -4.3055954 -4.3164239][-4.3082047 -4.2979236 -4.2857666 -4.2671585 -4.2420769 -4.2148933 -4.2016644 -4.2094855 -4.2345033 -4.2665052 -4.2905674 -4.3096967 -4.3201647 -4.3240247 -4.3277411][-4.3313808 -4.3296647 -4.3226395 -4.309586 -4.294805 -4.2767682 -4.2650089 -4.2672977 -4.2828145 -4.3033667 -4.3203158 -4.3328209 -4.33686 -4.3361521 -4.3371119][-4.343348 -4.345974 -4.3437529 -4.3359632 -4.3277917 -4.3169894 -4.3069992 -4.3033895 -4.3089871 -4.3197932 -4.3315015 -4.3404851 -4.3435011 -4.3421288 -4.341825][-4.345284 -4.3478022 -4.3478408 -4.3447738 -4.3414617 -4.3370447 -4.3315778 -4.328455 -4.3289409 -4.3331075 -4.3387742 -4.3432908 -4.3452935 -4.3445387 -4.3438458]]...]
INFO - root - 2017-12-05 19:41:39.916047: step 38110, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 68h:23m:20s remains)
INFO - root - 2017-12-05 19:41:48.456147: step 38120, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 68h:18m:10s remains)
INFO - root - 2017-12-05 19:41:57.034850: step 38130, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 71h:29m:19s remains)
INFO - root - 2017-12-05 19:42:05.490346: step 38140, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 68h:20m:45s remains)
INFO - root - 2017-12-05 19:42:14.099179: step 38150, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.853 sec/batch; 69h:44m:37s remains)
INFO - root - 2017-12-05 19:42:22.474392: step 38160, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 68h:37m:38s remains)
INFO - root - 2017-12-05 19:42:31.096643: step 38170, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 68h:55m:16s remains)
INFO - root - 2017-12-05 19:42:39.618219: step 38180, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:38m:49s remains)
INFO - root - 2017-12-05 19:42:48.092218: step 38190, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 67h:01m:49s remains)
INFO - root - 2017-12-05 19:42:56.572294: step 38200, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 71h:09m:00s remains)
2017-12-05 19:42:57.338688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3069835 -4.29949 -4.2963481 -4.2967639 -4.2986703 -4.29967 -4.3001952 -4.3006635 -4.3006878 -4.2987676 -4.2959628 -4.2949486 -4.3030438 -4.3186307 -4.3319111][-4.2726431 -4.2623119 -4.2578373 -4.2544641 -4.2543368 -4.2548146 -4.2562633 -4.2599325 -4.2624979 -4.2615647 -4.2592406 -4.2590852 -4.2720389 -4.29393 -4.3134022][-4.2386065 -4.2290006 -4.2225156 -4.2131319 -4.206202 -4.202198 -4.2047596 -4.2155652 -4.2241249 -4.2270761 -4.2272849 -4.2293072 -4.245759 -4.2724624 -4.2988825][-4.2034812 -4.1961536 -4.1882973 -4.1734633 -4.1572437 -4.1446776 -4.1448011 -4.1619034 -4.1799836 -4.1888666 -4.1941671 -4.1985221 -4.2166586 -4.246738 -4.2791128][-4.1725426 -4.16568 -4.15668 -4.1342678 -4.1037955 -4.073926 -4.0603809 -4.0815887 -4.1146374 -4.1358252 -4.1492496 -4.1571164 -4.1766896 -4.2098207 -4.2454748][-4.1343718 -4.1218438 -4.1086488 -4.0774908 -4.0308032 -3.9738486 -3.9348731 -3.9576883 -4.0230956 -4.07405 -4.101294 -4.1124539 -4.13703 -4.1746225 -4.2125082][-4.0939779 -4.0688834 -4.046175 -4.0045037 -3.9410338 -3.853919 -3.7734063 -3.7811513 -3.8799109 -3.9674063 -4.0174656 -4.04 -4.0752044 -4.1258783 -4.1768203][-4.0816216 -4.0491896 -4.0203314 -3.97546 -3.9095984 -3.8117392 -3.7008915 -3.6705701 -3.7622375 -3.8600295 -3.9238913 -3.9582119 -4.006393 -4.0710931 -4.1422873][-4.1174126 -4.0928211 -4.0716305 -4.0424914 -3.9976485 -3.9249611 -3.8336751 -3.7828927 -3.8164504 -3.8733706 -3.9213176 -3.954093 -4.0014167 -4.0611892 -4.1342626][-4.1700759 -4.1581545 -4.1483536 -4.1348143 -4.1114597 -4.0720091 -4.0180616 -3.9725757 -3.9685111 -3.986774 -4.0120549 -4.0379858 -4.0778646 -4.121788 -4.1753092][-4.2231269 -4.2196565 -4.2176547 -4.215188 -4.206687 -4.1912203 -4.1702957 -4.1463418 -4.1332846 -4.1308575 -4.1348634 -4.148973 -4.1743808 -4.2009821 -4.2332807][-4.2762494 -4.2772593 -4.2787018 -4.2804666 -4.2784615 -4.2724786 -4.2665329 -4.2567754 -4.2478414 -4.24109 -4.2358689 -4.2381172 -4.2507262 -4.2676458 -4.2869773][-4.3146148 -4.317709 -4.3203092 -4.3232546 -4.3246484 -4.3233509 -4.3214631 -4.3173218 -4.314075 -4.3107882 -4.3057413 -4.3040686 -4.310657 -4.3228779 -4.3326068][-4.3359165 -4.338233 -4.3402333 -4.3428326 -4.3454838 -4.3470073 -4.3478537 -4.347034 -4.3462377 -4.3455658 -4.3443666 -4.3436012 -4.3458247 -4.3513627 -4.3543711][-4.3436937 -4.3433414 -4.3435965 -4.3447275 -4.3463545 -4.348084 -4.34999 -4.3517642 -4.3534555 -4.3548632 -4.3561673 -4.356391 -4.3567162 -4.3580103 -4.3588352]]...]
INFO - root - 2017-12-05 19:43:06.069884: step 38210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:51m:34s remains)
INFO - root - 2017-12-05 19:43:14.557809: step 38220, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.889 sec/batch; 72h:40m:52s remains)
INFO - root - 2017-12-05 19:43:23.063670: step 38230, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 69h:48m:03s remains)
INFO - root - 2017-12-05 19:43:31.542263: step 38240, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 69h:46m:06s remains)
INFO - root - 2017-12-05 19:43:39.931984: step 38250, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 70h:58m:57s remains)
INFO - root - 2017-12-05 19:43:48.379331: step 38260, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:15m:11s remains)
INFO - root - 2017-12-05 19:43:56.807196: step 38270, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 68h:45m:58s remains)
INFO - root - 2017-12-05 19:44:05.305892: step 38280, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 71h:22m:12s remains)
INFO - root - 2017-12-05 19:44:13.803816: step 38290, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:46m:27s remains)
INFO - root - 2017-12-05 19:44:22.346966: step 38300, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 71h:44m:58s remains)
2017-12-05 19:44:23.180226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3026724 -4.2965546 -4.2755346 -4.251091 -4.2316089 -4.2174315 -4.2053308 -4.1966715 -4.1742349 -4.128644 -4.0836062 -4.0596581 -4.0581093 -4.0726376 -4.0831375][-4.2996416 -4.2908969 -4.2688022 -4.2452755 -4.2295122 -4.2218904 -4.2179122 -4.2162476 -4.1995053 -4.1549706 -4.1096745 -4.0894504 -4.0885415 -4.0971785 -4.0982742][-4.281456 -4.2694378 -4.248435 -4.227066 -4.2142143 -4.2103786 -4.2105546 -4.2126093 -4.2018065 -4.1642208 -4.1207647 -4.102397 -4.1065683 -4.1171484 -4.1196103][-4.2547812 -4.2413187 -4.22153 -4.2045569 -4.19465 -4.1884866 -4.1847448 -4.1805773 -4.171267 -4.1425824 -4.1074729 -4.0957355 -4.1070948 -4.124393 -4.1381907][-4.2258248 -4.2134113 -4.1931653 -4.1731081 -4.1540065 -4.1351128 -4.1167626 -4.0998411 -4.0914888 -4.0797839 -4.070106 -4.0805826 -4.1054697 -4.1312733 -4.1544871][-4.18063 -4.1659169 -4.1398945 -4.1086206 -4.0718026 -4.0359306 -3.9981267 -3.9745405 -3.9845996 -4.0115395 -4.04257 -4.0826864 -4.1212292 -4.153172 -4.1790962][-4.1073041 -4.0841169 -4.0552559 -4.0207734 -3.9729514 -3.9214053 -3.8675685 -3.8483026 -3.9012258 -3.9780729 -4.047513 -4.1088333 -4.1503563 -4.1801448 -4.1998863][-4.0286303 -4.0123925 -3.9980979 -3.9801016 -3.9467065 -3.9109213 -3.8732951 -3.8723688 -3.9381487 -4.0162382 -4.08033 -4.1320863 -4.1647987 -4.1909952 -4.2072477][-4.0055246 -4.0115905 -4.0187597 -4.0212865 -4.0095572 -3.9990194 -3.9882412 -3.9952729 -4.0369945 -4.0804749 -4.1132097 -4.1402764 -4.1603656 -4.1812539 -4.1972713][-4.0294681 -4.0508628 -4.0708613 -4.0859551 -4.0868378 -4.0889583 -4.0899858 -4.0954347 -4.1133661 -4.12261 -4.1229644 -4.1274652 -4.137866 -4.1564612 -4.1738911][-4.0673366 -4.0885143 -4.1109443 -4.1303654 -4.1393719 -4.144825 -4.1463337 -4.1457977 -4.1433105 -4.128242 -4.1073437 -4.0983963 -4.1041527 -4.1249685 -4.1473308][-4.0816312 -4.0959387 -4.1184244 -4.1392961 -4.15118 -4.1541305 -4.1496086 -4.1435423 -4.1334677 -4.1119576 -4.0884757 -4.0833063 -4.097672 -4.1255388 -4.1508574][-4.086978 -4.0897417 -4.1078529 -4.1274271 -4.1389036 -4.1409645 -4.1376238 -4.1341305 -4.127532 -4.1148429 -4.1017094 -4.103961 -4.1216216 -4.1442056 -4.1594796][-4.0930333 -4.0876317 -4.0995374 -4.1199894 -4.1343374 -4.1402316 -4.1409044 -4.1411166 -4.1394286 -4.1355333 -4.1276846 -4.1258755 -4.1320386 -4.1377792 -4.1408305][-4.1082044 -4.1001048 -4.1051645 -4.1224303 -4.1372848 -4.143765 -4.1456327 -4.1446657 -4.1422586 -4.1422153 -4.1354856 -4.1251903 -4.1169872 -4.1106458 -4.1063786]]...]
INFO - root - 2017-12-05 19:44:31.714809: step 38310, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 70h:12m:54s remains)
INFO - root - 2017-12-05 19:44:40.365266: step 38320, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 71h:04m:39s remains)
INFO - root - 2017-12-05 19:44:48.934288: step 38330, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:50m:42s remains)
INFO - root - 2017-12-05 19:44:57.617763: step 38340, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 72h:28m:07s remains)
INFO - root - 2017-12-05 19:45:06.035030: step 38350, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 70h:21m:46s remains)
INFO - root - 2017-12-05 19:45:14.456827: step 38360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 69h:54m:32s remains)
INFO - root - 2017-12-05 19:45:23.037734: step 38370, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 67h:35m:52s remains)
INFO - root - 2017-12-05 19:45:31.609160: step 38380, loss = 2.10, batch loss = 2.04 (8.9 examples/sec; 0.897 sec/batch; 73h:16m:03s remains)
INFO - root - 2017-12-05 19:45:40.127558: step 38390, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:34m:59s remains)
INFO - root - 2017-12-05 19:45:48.517763: step 38400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 69h:33m:36s remains)
2017-12-05 19:45:49.279394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1735926 -4.1758265 -4.1794615 -4.1826711 -4.1869903 -4.1971011 -4.2082891 -4.2165527 -4.2147117 -4.1916304 -4.1503234 -4.1062474 -4.0657091 -4.0250077 -4.0041947][-4.1581607 -4.1578 -4.1597366 -4.1641731 -4.1751204 -4.1952515 -4.2184405 -4.2346125 -4.2367735 -4.2171974 -4.1812334 -4.1414118 -4.103394 -4.0682411 -4.0471187][-4.1584706 -4.1509581 -4.1471791 -4.1479459 -4.1585493 -4.1823063 -4.2145305 -4.2400103 -4.2498827 -4.240221 -4.216795 -4.1898737 -4.1642537 -4.1415377 -4.1284285][-4.1845245 -4.1732349 -4.1668272 -4.1662879 -4.1729836 -4.1908154 -4.2209096 -4.2472892 -4.2582393 -4.2541623 -4.2397084 -4.2230639 -4.2051978 -4.186636 -4.1728067][-4.2115335 -4.1973386 -4.1892452 -4.1874065 -4.1903305 -4.20203 -4.2270222 -4.2526426 -4.2658887 -4.2680173 -4.2611952 -4.2512569 -4.2375817 -4.2175708 -4.191453][-4.2248974 -4.2036834 -4.1865296 -4.175457 -4.1675634 -4.1709127 -4.19532 -4.2287359 -4.2549863 -4.26972 -4.27273 -4.2691436 -4.2607656 -4.2442923 -4.2146282][-4.220233 -4.1911688 -4.1610985 -4.1330581 -4.1054978 -4.0934649 -4.1188073 -4.1675205 -4.2159557 -4.2517328 -4.27264 -4.2856054 -4.2871943 -4.278091 -4.2514548][-4.210537 -4.1722908 -4.1290731 -4.0832343 -4.0307417 -3.9978194 -4.0234423 -4.0930028 -4.1678238 -4.2269421 -4.2676768 -4.2963181 -4.3069396 -4.3034544 -4.2829504][-4.2192917 -4.1757069 -4.1293588 -4.0760341 -4.0091667 -3.9596615 -3.9726043 -4.0425181 -4.12709 -4.1987133 -4.2544188 -4.2947984 -4.3126159 -4.312993 -4.2999063][-4.2414947 -4.2013774 -4.1612535 -4.1167669 -4.0611296 -4.0168815 -4.02005 -4.0708532 -4.13725 -4.1981521 -4.2495975 -4.2886782 -4.3072157 -4.3105979 -4.3027854][-4.264492 -4.2317772 -4.2005668 -4.1709681 -4.1368022 -4.1121159 -4.11731 -4.1486993 -4.188766 -4.2276726 -4.2632957 -4.2914968 -4.3053603 -4.3066554 -4.3028221][-4.2771187 -4.2541566 -4.2336512 -4.2166882 -4.2004704 -4.189786 -4.197094 -4.2181239 -4.2405362 -4.262907 -4.2839971 -4.3014865 -4.3106356 -4.3113303 -4.310061][-4.2851353 -4.2684469 -4.2547526 -4.2454171 -4.2379336 -4.2345982 -4.2391438 -4.2526932 -4.2692 -4.2859626 -4.3002319 -4.3115368 -4.3188095 -4.3219366 -4.3217406][-4.2987833 -4.2838211 -4.2708259 -4.2608304 -4.2535887 -4.2509961 -4.2520857 -4.2583508 -4.2695789 -4.2839384 -4.2980022 -4.309269 -4.3182092 -4.3240356 -4.326077][-4.313241 -4.3007917 -4.2905478 -4.2818217 -4.2742295 -4.2695212 -4.2678552 -4.26892 -4.2740641 -4.282999 -4.2935824 -4.3031931 -4.3127356 -4.3199587 -4.3237777]]...]
INFO - root - 2017-12-05 19:45:57.876204: step 38410, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:51m:46s remains)
INFO - root - 2017-12-05 19:46:06.435823: step 38420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 70h:27m:31s remains)
INFO - root - 2017-12-05 19:46:14.934045: step 38430, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:12m:01s remains)
INFO - root - 2017-12-05 19:46:23.428740: step 38440, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 68h:23m:37s remains)
INFO - root - 2017-12-05 19:46:32.012903: step 38450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 71h:02m:08s remains)
INFO - root - 2017-12-05 19:46:40.349335: step 38460, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 69h:18m:06s remains)
INFO - root - 2017-12-05 19:46:48.782605: step 38470, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 69h:05m:37s remains)
INFO - root - 2017-12-05 19:46:57.379113: step 38480, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 71h:41m:31s remains)
INFO - root - 2017-12-05 19:47:06.004458: step 38490, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 70h:50m:23s remains)
INFO - root - 2017-12-05 19:47:14.576412: step 38500, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 69h:30m:31s remains)
2017-12-05 19:47:15.407757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1772118 -4.1613379 -4.1491132 -4.1390967 -4.1335883 -4.1342545 -4.1350193 -4.1386957 -4.1432128 -4.1486173 -4.1617775 -4.1812124 -4.1963124 -4.2028694 -4.2074909][-4.1793485 -4.1702223 -4.1609049 -4.1532092 -4.151567 -4.1576309 -4.1608458 -4.1643395 -4.1642962 -4.1608615 -4.1575861 -4.1595831 -4.1624522 -4.1620436 -4.1671429][-4.1688528 -4.1628647 -4.1538343 -4.1459665 -4.1451464 -4.1541443 -4.1617303 -4.1675425 -4.1710019 -4.1665912 -4.1518922 -4.1375046 -4.129981 -4.1305375 -4.1373882][-4.1488252 -4.1492686 -4.144846 -4.1379981 -4.1282415 -4.1304884 -4.1387119 -4.1466503 -4.1529336 -4.1547651 -4.1439285 -4.1259785 -4.1152234 -4.1147857 -4.1189609][-4.1321011 -4.1384916 -4.1324806 -4.1188617 -4.0967021 -4.0876088 -4.0923643 -4.1024246 -4.1148934 -4.1285253 -4.1376724 -4.1329403 -4.1272206 -4.12358 -4.1185813][-4.1076326 -4.1123714 -4.10099 -4.0722265 -4.0339346 -4.0090446 -4.0034819 -4.01744 -4.0483327 -4.0870676 -4.1254182 -4.14396 -4.1442595 -4.1376204 -4.1245961][-4.0885458 -4.0825434 -4.0612931 -4.02575 -3.9830561 -3.9478796 -3.9314198 -3.9479289 -3.9934647 -4.0514393 -4.104342 -4.1358547 -4.1439762 -4.1405 -4.1341796][-4.099721 -4.0761662 -4.04198 -4.0080914 -3.9772115 -3.9530749 -3.9345455 -3.9435425 -3.9850638 -4.0379267 -4.0867343 -4.1156154 -4.1262622 -4.1279712 -4.1328897][-4.1231337 -4.090858 -4.0525441 -4.0245781 -4.0046935 -3.9905024 -3.9732692 -3.9704547 -3.9962072 -4.0300708 -4.0603824 -4.0782647 -4.089963 -4.0971231 -4.1126051][-4.15371 -4.123817 -4.0909057 -4.0694056 -4.0577312 -4.0447464 -4.0254951 -4.0155358 -4.0263796 -4.0414524 -4.0540805 -4.0662985 -4.081233 -4.0919485 -4.1067114][-4.1883698 -4.1697464 -4.1516128 -4.1389017 -4.1334858 -4.12484 -4.1074934 -4.0963979 -4.1008463 -4.1006885 -4.0983462 -4.1042976 -4.1192613 -4.131536 -4.1390986][-4.2236934 -4.2170062 -4.2117867 -4.209105 -4.2106466 -4.2083464 -4.1969786 -4.1882172 -4.1903644 -4.1859717 -4.1778536 -4.1799412 -4.1899409 -4.1998787 -4.1969628][-4.2441211 -4.2430606 -4.2428904 -4.2452326 -4.2499228 -4.2524362 -4.2499528 -4.2478814 -4.250351 -4.2504973 -4.2462955 -4.2458491 -4.2512951 -4.253933 -4.2428341][-4.2405081 -4.2420077 -4.2425733 -4.2440491 -4.24717 -4.2499638 -4.2506127 -4.2516885 -4.2563844 -4.2608442 -4.2612686 -4.262599 -4.2681136 -4.26869 -4.2557697][-4.2273455 -4.232235 -4.2333708 -4.2336936 -4.23451 -4.2355828 -4.2369814 -4.239244 -4.2436543 -4.2484784 -4.2512164 -4.2538872 -4.2590542 -4.25899 -4.2461338]]...]
INFO - root - 2017-12-05 19:47:23.991581: step 38510, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 70h:17m:06s remains)
INFO - root - 2017-12-05 19:47:32.554646: step 38520, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 69h:11m:27s remains)
INFO - root - 2017-12-05 19:47:41.015818: step 38530, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.838 sec/batch; 68h:27m:20s remains)
INFO - root - 2017-12-05 19:47:49.399943: step 38540, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 70h:33m:27s remains)
INFO - root - 2017-12-05 19:47:57.801876: step 38550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 68h:27m:01s remains)
INFO - root - 2017-12-05 19:48:06.171873: step 38560, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 70h:01m:56s remains)
INFO - root - 2017-12-05 19:48:14.749889: step 38570, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 67h:30m:30s remains)
INFO - root - 2017-12-05 19:48:23.233015: step 38580, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.842 sec/batch; 68h:46m:12s remains)
INFO - root - 2017-12-05 19:48:31.685911: step 38590, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 70h:53m:28s remains)
INFO - root - 2017-12-05 19:48:40.221400: step 38600, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 69h:30m:45s remains)
2017-12-05 19:48:40.996801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2703948 -4.2497716 -4.2206149 -4.1829062 -4.1353512 -4.0793157 -4.0328016 -4.0291409 -4.0887218 -4.1437902 -4.1797013 -4.197309 -4.200294 -4.199564 -4.2019582][-4.274436 -4.2612047 -4.2382135 -4.2082868 -4.1740603 -4.1390724 -4.109251 -4.1121497 -4.153563 -4.1891184 -4.2170777 -4.2315907 -4.2291346 -4.2231426 -4.2173443][-4.2764673 -4.2703547 -4.252543 -4.2251015 -4.2012262 -4.1882114 -4.1802611 -4.1849642 -4.20635 -4.2238441 -4.243494 -4.2586474 -4.2612467 -4.25819 -4.2439623][-4.2752314 -4.2750497 -4.2586007 -4.2278342 -4.2034755 -4.2003984 -4.2088289 -4.2195663 -4.2301021 -4.2368989 -4.2431211 -4.2528839 -4.2621818 -4.2687035 -4.255939][-4.2711644 -4.2733846 -4.2523937 -4.2161894 -4.1886129 -4.1811385 -4.1895852 -4.202404 -4.2089033 -4.2123184 -4.2083807 -4.2096 -4.2189436 -4.231101 -4.2257624][-4.2661018 -4.2701006 -4.2418823 -4.1965094 -4.156157 -4.1294827 -4.1220345 -4.1261225 -4.1430979 -4.1639128 -4.1661434 -4.1629815 -4.1646986 -4.1730404 -4.1671495][-4.260674 -4.2673411 -4.237411 -4.1829505 -4.1233058 -4.0652771 -4.0201368 -4.0020094 -4.0437055 -4.1074195 -4.1381536 -4.141839 -4.1376653 -4.1355128 -4.124258][-4.2495384 -4.2604675 -4.2339129 -4.1814532 -4.1106119 -4.0251856 -3.9382155 -3.8936751 -3.960753 -4.0610909 -4.1146359 -4.1340327 -4.133956 -4.1282663 -4.1162686][-4.2382693 -4.2541475 -4.2350144 -4.1920896 -4.1271753 -4.04647 -3.9557257 -3.9044552 -3.958564 -4.0476046 -4.1001205 -4.1324615 -4.1457992 -4.1480656 -4.1394782][-4.2463379 -4.2609067 -4.2455721 -4.2083158 -4.1551352 -4.0940866 -4.0214972 -3.9775238 -4.0060263 -4.066545 -4.1065545 -4.1420112 -4.1651645 -4.1786909 -4.1768627][-4.2643771 -4.273253 -4.2600937 -4.2269311 -4.1809344 -4.1325192 -4.0802741 -4.0519924 -4.0708108 -4.1165404 -4.1481729 -4.1777744 -4.2005267 -4.2183385 -4.2216578][-4.2763281 -4.2829 -4.2742381 -4.2446108 -4.2027779 -4.1616278 -4.12812 -4.1156783 -4.1365018 -4.1780782 -4.2071209 -4.2284093 -4.2476034 -4.2626138 -4.2698612][-4.2835436 -4.2913642 -4.2881713 -4.2655425 -4.2302704 -4.196733 -4.1771617 -4.177351 -4.1991696 -4.2373357 -4.2652483 -4.2815433 -4.2949743 -4.3032722 -4.3092074][-4.2910724 -4.2984223 -4.2989206 -4.2845244 -4.2608595 -4.2383785 -4.2272005 -4.2317085 -4.2519727 -4.2841959 -4.3084664 -4.32193 -4.3274317 -4.3236547 -4.3187923][-4.2997828 -4.3038177 -4.3045888 -4.2962813 -4.2829432 -4.2707324 -4.2649751 -4.2698054 -4.2847962 -4.3075576 -4.3266988 -4.3358283 -4.33368 -4.3200378 -4.3051782]]...]
INFO - root - 2017-12-05 19:48:49.544225: step 38610, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 68h:20m:27s remains)
INFO - root - 2017-12-05 19:48:57.994539: step 38620, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 69h:54m:05s remains)
INFO - root - 2017-12-05 19:49:06.276278: step 38630, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 66h:52m:52s remains)
INFO - root - 2017-12-05 19:49:14.865199: step 38640, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 70h:43m:24s remains)
INFO - root - 2017-12-05 19:49:23.446265: step 38650, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 69h:05m:39s remains)
INFO - root - 2017-12-05 19:49:31.893239: step 38660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 68h:58m:36s remains)
INFO - root - 2017-12-05 19:49:40.351298: step 38670, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 73h:21m:52s remains)
INFO - root - 2017-12-05 19:49:48.863295: step 38680, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 68h:52m:13s remains)
INFO - root - 2017-12-05 19:49:57.468356: step 38690, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 70h:41m:09s remains)
INFO - root - 2017-12-05 19:50:05.910289: step 38700, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:40m:52s remains)
2017-12-05 19:50:06.644895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1693234 -4.1670594 -4.1570606 -4.1497364 -4.1619711 -4.1940527 -4.2175074 -4.2144585 -4.1970735 -4.1716104 -4.144989 -4.1400881 -4.1620131 -4.1980877 -4.2319841][-4.1707745 -4.171627 -4.1590042 -4.1555109 -4.1728287 -4.2014451 -4.2172461 -4.2054968 -4.1794944 -4.155057 -4.1344385 -4.1341667 -4.1607423 -4.2019029 -4.2381392][-4.1796694 -4.18013 -4.1675425 -4.1715183 -4.1905713 -4.2093282 -4.2139997 -4.192174 -4.1597137 -4.1401296 -4.1324973 -4.1403818 -4.1742382 -4.2184019 -4.2523694][-4.1904435 -4.1890812 -4.18069 -4.1888275 -4.2025433 -4.2114577 -4.2085686 -4.1812234 -4.1443968 -4.1222429 -4.1214848 -4.1408896 -4.1868143 -4.237062 -4.2724104][-4.2111354 -4.2111483 -4.2058034 -4.2122712 -4.2148366 -4.2099152 -4.1931505 -4.159133 -4.1219049 -4.0989938 -4.1069665 -4.1431088 -4.2029033 -4.2595387 -4.297503][-4.2277918 -4.2307396 -4.2251277 -4.2243605 -4.2173929 -4.1982121 -4.1634259 -4.1203656 -4.0864744 -4.0722613 -4.0919824 -4.1459169 -4.2163787 -4.2780552 -4.3188686][-4.2329326 -4.234962 -4.2260561 -4.2201591 -4.2077112 -4.1762791 -4.123024 -4.067101 -4.03573 -4.0380263 -4.0792418 -4.1502929 -4.2275124 -4.2924728 -4.3356524][-4.2257295 -4.218554 -4.2038674 -4.1930761 -4.1791363 -4.1470175 -4.0894833 -4.0289416 -4.0007744 -4.0212688 -4.0856676 -4.1670928 -4.2449431 -4.3084946 -4.349061][-4.212522 -4.1984773 -4.1792421 -4.1628551 -4.1441755 -4.1130829 -4.0655031 -4.0138793 -3.9954433 -4.0281 -4.1037235 -4.188127 -4.2626572 -4.3212438 -4.3568392][-4.2071419 -4.1916075 -4.1734347 -4.1535516 -4.1287837 -4.09641 -4.0528326 -4.0102959 -4.0027061 -4.0411377 -4.1181784 -4.2004914 -4.2721658 -4.32774 -4.3584237][-4.216826 -4.2013412 -4.1842194 -4.163166 -4.136888 -4.104948 -4.0670996 -4.0336642 -4.03006 -4.0671391 -4.13632 -4.2109861 -4.2801948 -4.3315535 -4.3572226][-4.2390041 -4.2251134 -4.2074356 -4.1869869 -4.1617937 -4.1324863 -4.1003561 -4.074604 -4.0734568 -4.1042304 -4.1615844 -4.2268314 -4.2904687 -4.3351297 -4.3543396][-4.2574873 -4.2498765 -4.2361283 -4.2204542 -4.1985431 -4.1726627 -4.1468034 -4.1264782 -4.1278396 -4.1536889 -4.1990228 -4.2524014 -4.3041883 -4.3381548 -4.3514233][-4.2758946 -4.2744021 -4.2676711 -4.2587695 -4.2422705 -4.2209668 -4.1990809 -4.1823053 -4.1836538 -4.2049227 -4.2396712 -4.2801681 -4.3167644 -4.3405247 -4.3495021][-4.2974482 -4.2986164 -4.2964606 -4.2923045 -4.28301 -4.2675009 -4.2498641 -4.237165 -4.2379 -4.253397 -4.2782159 -4.3057766 -4.3284173 -4.3429489 -4.3486943]]...]
INFO - root - 2017-12-05 19:50:15.098948: step 38710, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 71h:12m:16s remains)
INFO - root - 2017-12-05 19:50:23.627617: step 38720, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 70h:47m:27s remains)
INFO - root - 2017-12-05 19:50:32.087216: step 38730, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 69h:24m:10s remains)
INFO - root - 2017-12-05 19:50:40.571845: step 38740, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 70h:27m:03s remains)
INFO - root - 2017-12-05 19:50:49.177734: step 38750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 71h:19m:06s remains)
INFO - root - 2017-12-05 19:50:57.528437: step 38760, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 68h:34m:03s remains)
INFO - root - 2017-12-05 19:51:06.095416: step 38770, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.856 sec/batch; 69h:48m:38s remains)
INFO - root - 2017-12-05 19:51:14.581721: step 38780, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 70h:39m:15s remains)
INFO - root - 2017-12-05 19:51:23.204275: step 38790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 70h:21m:35s remains)
INFO - root - 2017-12-05 19:51:31.848542: step 38800, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.805 sec/batch; 65h:40m:26s remains)
2017-12-05 19:51:32.699116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2295175 -4.2370205 -4.2458162 -4.2478781 -4.2419982 -4.236382 -4.2346129 -4.2365642 -4.2448883 -4.2516708 -4.2539144 -4.2547216 -4.2536016 -4.2489119 -4.2375436][-4.234602 -4.2422671 -4.2497983 -4.2499084 -4.2423205 -4.2372932 -4.2382412 -4.2429867 -4.2521496 -4.2608309 -4.2656803 -4.267478 -4.2662768 -4.2605968 -4.2487631][-4.2382827 -4.2462034 -4.2513261 -4.2481828 -4.238925 -4.2332568 -4.2354465 -4.2409286 -4.250123 -4.2612667 -4.271009 -4.2767224 -4.2778959 -4.2737823 -4.2655749][-4.2330441 -4.242054 -4.245141 -4.2408714 -4.2331843 -4.2284331 -4.2298393 -4.2332263 -4.2418394 -4.2554708 -4.2710481 -4.2828932 -4.2882013 -4.2873487 -4.2830691][-4.2173109 -4.2243471 -4.2245393 -4.2193232 -4.2125955 -4.2085109 -4.2062244 -4.2039118 -4.2103267 -4.2265983 -4.24906 -4.2682729 -4.2789536 -4.283174 -4.2830124][-4.195364 -4.1994529 -4.1964383 -4.1888919 -4.1788387 -4.1668105 -4.151041 -4.1352434 -4.1365933 -4.155231 -4.1839528 -4.2096896 -4.2265115 -4.2375569 -4.2439208][-4.1779623 -4.1812062 -4.177722 -4.16808 -4.1517119 -4.1241226 -4.0862737 -4.0525723 -4.0495725 -4.0707607 -4.1035557 -4.1334038 -4.154047 -4.1695862 -4.1821513][-4.1747313 -4.1798763 -4.1801558 -4.1723728 -4.1533003 -4.1135783 -4.0591736 -4.0144277 -4.0110426 -4.0337691 -4.065455 -4.0931764 -4.1129513 -4.1298752 -4.1456656][-4.1875997 -4.195756 -4.20088 -4.1990476 -4.1853576 -4.1491857 -4.0993748 -4.0609465 -4.0611019 -4.0831041 -4.1103029 -4.1322846 -4.1481152 -4.1620588 -4.1740966][-4.1983938 -4.2065506 -4.2139592 -4.2169394 -4.2093034 -4.1826186 -4.1452456 -4.1183643 -4.1225553 -4.1429749 -4.1650119 -4.180233 -4.1898913 -4.196384 -4.19847][-4.2044082 -4.2102461 -4.2171569 -4.2210646 -4.2161789 -4.1972752 -4.1716876 -4.1548386 -4.1594715 -4.1743069 -4.1883879 -4.1963353 -4.2005477 -4.2000656 -4.1929493][-4.2166286 -4.2205634 -4.2258973 -4.2288084 -4.2244043 -4.2115111 -4.1972313 -4.1879821 -4.1887403 -4.1946635 -4.1998739 -4.2020059 -4.2025456 -4.1984272 -4.1863341][-4.2327175 -4.2358613 -4.2396541 -4.2401934 -4.2356334 -4.2273278 -4.2206917 -4.2166162 -4.2159519 -4.2175422 -4.2180524 -4.2175145 -4.2169995 -4.2129407 -4.201458][-4.2462931 -4.2497706 -4.2527833 -4.2519393 -4.248405 -4.2431149 -4.2400455 -4.2383513 -4.2387872 -4.239584 -4.2390575 -4.2386608 -4.2385736 -4.2364936 -4.23005][-4.2544336 -4.25659 -4.2579069 -4.256671 -4.254981 -4.252696 -4.2518244 -4.2510114 -4.2521558 -4.2530832 -4.253109 -4.2535462 -4.2541909 -4.2538853 -4.2516809]]...]
INFO - root - 2017-12-05 19:51:41.238925: step 38810, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 70h:06m:31s remains)
INFO - root - 2017-12-05 19:51:49.820616: step 38820, loss = 2.05, batch loss = 2.00 (9.9 examples/sec; 0.807 sec/batch; 65h:51m:03s remains)
INFO - root - 2017-12-05 19:51:58.480137: step 38830, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 69h:14m:05s remains)
INFO - root - 2017-12-05 19:52:06.999763: step 38840, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 68h:52m:04s remains)
INFO - root - 2017-12-05 19:52:15.462765: step 38850, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 70h:17m:33s remains)
INFO - root - 2017-12-05 19:52:23.898000: step 38860, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 69h:10m:06s remains)
INFO - root - 2017-12-05 19:52:32.423539: step 38870, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:41m:07s remains)
INFO - root - 2017-12-05 19:52:40.982219: step 38880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 70h:53m:55s remains)
INFO - root - 2017-12-05 19:52:49.367713: step 38890, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 71h:22m:27s remains)
INFO - root - 2017-12-05 19:52:57.830731: step 38900, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 66h:59m:50s remains)
2017-12-05 19:52:58.579904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1372294 -4.1257968 -4.1041036 -4.0880384 -4.0883303 -4.0977249 -4.098258 -4.0917478 -4.0998578 -4.1346688 -4.1664228 -4.1618795 -4.1225934 -4.076654 -4.0673828][-4.1230235 -4.1115332 -4.0918212 -4.0733786 -4.0683413 -4.0776143 -4.084383 -4.0867953 -4.1010056 -4.1397152 -4.1712408 -4.1681933 -4.1376495 -4.1037855 -4.10157][-4.1405511 -4.1263275 -4.1043224 -4.0827713 -4.075913 -4.0895119 -4.1050816 -4.1129436 -4.127852 -4.1638517 -4.1924424 -4.198534 -4.1851349 -4.1677084 -4.1688738][-4.1671009 -4.1482549 -4.1191883 -4.0937948 -4.0912819 -4.111763 -4.1310644 -4.1373973 -4.1496968 -4.1842761 -4.2159686 -4.2368021 -4.2436604 -4.2408295 -4.2402625][-4.1796784 -4.1543231 -4.1241927 -4.1025114 -4.1081014 -4.1289349 -4.1393824 -4.1334167 -4.1380086 -4.175683 -4.2200809 -4.2580361 -4.2778811 -4.279685 -4.2725172][-4.1779308 -4.150557 -4.1298647 -4.119719 -4.1276593 -4.133616 -4.1169858 -4.0847087 -4.0791831 -4.1260266 -4.1889992 -4.2420306 -4.266366 -4.2645407 -4.2526188][-4.1663136 -4.1392393 -4.1270361 -4.1268277 -4.1279497 -4.1100616 -4.0541682 -3.9847834 -3.9723215 -4.0441241 -4.1361222 -4.201611 -4.2238436 -4.2138772 -4.1981888][-4.1562147 -4.1291618 -4.118217 -4.120379 -4.1126695 -4.0724297 -3.98461 -3.8862867 -3.8784282 -3.9833915 -4.096736 -4.1623464 -4.1734715 -4.1511555 -4.1327543][-4.1544781 -4.1275783 -4.1151509 -4.1186137 -4.1098733 -4.06987 -3.9891036 -3.9046462 -3.9067006 -4.0023332 -4.0960417 -4.1377525 -4.1274414 -4.0953846 -4.08638][-4.1547341 -4.1313734 -4.1197448 -4.1285057 -4.128612 -4.1078262 -4.0564547 -4.0033751 -4.0042748 -4.0597682 -4.1084557 -4.1125546 -4.0812044 -4.0525732 -4.0602031][-4.1472125 -4.1352057 -4.1318421 -4.147469 -4.1575136 -4.1513638 -4.1210356 -4.0911388 -4.0879517 -4.1120057 -4.1279745 -4.1071372 -4.0662208 -4.0463409 -4.0656247][-4.1367083 -4.1418796 -4.1483364 -4.1680427 -4.181335 -4.1822767 -4.1677876 -4.1556163 -4.1557937 -4.1671267 -4.164639 -4.1371331 -4.098207 -4.0833592 -4.1024609][-4.1291327 -4.1510372 -4.1692433 -4.193428 -4.2086267 -4.2153306 -4.213594 -4.213222 -4.2171755 -4.2211189 -4.2126751 -4.1905046 -4.161397 -4.1481895 -4.1589131][-4.141315 -4.1744094 -4.2019663 -4.2280207 -4.241785 -4.2497969 -4.2512088 -4.2544389 -4.2599974 -4.2615175 -4.2566476 -4.2447529 -4.2247934 -4.2102652 -4.2128625][-4.1817746 -4.2127614 -4.2371893 -4.2560968 -4.2658005 -4.2708 -4.2709928 -4.2743893 -4.2805972 -4.2835307 -4.283936 -4.2788444 -4.2654409 -4.2503772 -4.2485056]]...]
INFO - root - 2017-12-05 19:53:07.137502: step 38910, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:43m:40s remains)
INFO - root - 2017-12-05 19:53:15.703027: step 38920, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 70h:24m:35s remains)
INFO - root - 2017-12-05 19:53:24.209403: step 38930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 69h:04m:01s remains)
INFO - root - 2017-12-05 19:53:32.739682: step 38940, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 69h:35m:12s remains)
INFO - root - 2017-12-05 19:53:41.306419: step 38950, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 70h:09m:10s remains)
INFO - root - 2017-12-05 19:53:49.683766: step 38960, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 67h:08m:53s remains)
INFO - root - 2017-12-05 19:53:58.195544: step 38970, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 70h:56m:52s remains)
INFO - root - 2017-12-05 19:54:06.706336: step 38980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 70h:46m:15s remains)
INFO - root - 2017-12-05 19:54:15.185534: step 38990, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:24m:39s remains)
INFO - root - 2017-12-05 19:54:23.753067: step 39000, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:28m:26s remains)
2017-12-05 19:54:24.592207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3321471 -4.3287234 -4.3222671 -4.3131123 -4.3045311 -4.2998357 -4.3006048 -4.3072743 -4.3166628 -4.3262768 -4.3340764 -4.3381109 -4.3401923 -4.341619 -4.3413606][-4.3229337 -4.3228559 -4.3171597 -4.3071027 -4.2965426 -4.2875957 -4.2829561 -4.2866812 -4.2961435 -4.3069444 -4.3182878 -4.3256917 -4.3311572 -4.335969 -4.33879][-4.3095903 -4.3124909 -4.3068905 -4.2952819 -4.281117 -4.2655997 -4.2544847 -4.2555704 -4.2667789 -4.2797132 -4.2937083 -4.3063145 -4.3178177 -4.3272643 -4.3335938][-4.2888565 -4.2947626 -4.2857857 -4.2694674 -4.2474623 -4.221108 -4.2007184 -4.2004433 -4.2196121 -4.2401719 -4.2575097 -4.2753944 -4.2944546 -4.312233 -4.3258376][-4.2590718 -4.272543 -4.2633295 -4.2439423 -4.2151375 -4.177094 -4.1416111 -4.1317883 -4.1493306 -4.1734715 -4.1970167 -4.2240009 -4.25551 -4.2872849 -4.3131104][-4.2155814 -4.2443833 -4.2451744 -4.2323484 -4.2050128 -4.1633224 -4.1155815 -4.081861 -4.0702448 -4.0754638 -4.0986266 -4.142971 -4.1969004 -4.2501521 -4.2941957][-4.1511483 -4.1929283 -4.2114453 -4.2154503 -4.2051415 -4.1774683 -4.1310296 -4.0733852 -4.0164142 -3.9758666 -3.9836304 -4.0458031 -4.1295776 -4.207736 -4.2719011][-4.0886841 -4.1206427 -4.1512623 -4.172452 -4.1825986 -4.177177 -4.1447225 -4.0791321 -3.9986222 -3.9269056 -3.9162071 -3.9774883 -4.0741639 -4.168191 -4.24829][-4.088625 -4.0937524 -4.1103826 -4.1321383 -4.1504703 -4.1514521 -4.1249652 -4.0665216 -4.0015612 -3.9489281 -3.9415069 -3.9896846 -4.0699487 -4.1556339 -4.2357531][-4.1302857 -4.1181712 -4.1163187 -4.1264696 -4.133472 -4.12602 -4.09658 -4.0522289 -4.0199642 -4.0060854 -4.0178976 -4.0575123 -4.1151581 -4.178772 -4.2424326][-4.1661782 -4.1457944 -4.133482 -4.1321931 -4.1279736 -4.1162472 -4.0938654 -4.0705724 -4.0667543 -4.077107 -4.0987186 -4.1342468 -4.1772323 -4.2183614 -4.2594571][-4.1929693 -4.1606407 -4.1314721 -4.1121612 -4.1015015 -4.103476 -4.1108971 -4.1186576 -4.133769 -4.1542172 -4.174634 -4.2026868 -4.2315917 -4.252707 -4.2744918][-4.2144432 -4.1786461 -4.1328635 -4.089644 -4.0675082 -4.0850377 -4.1208515 -4.153842 -4.1809354 -4.2050786 -4.2271147 -4.2523818 -4.2710838 -4.2786789 -4.2865987][-4.2300916 -4.2017913 -4.1588926 -4.1084957 -4.0746861 -4.08429 -4.1237917 -4.164979 -4.1949792 -4.21562 -4.2386861 -4.2631712 -4.28042 -4.2852449 -4.2890892][-4.2318611 -4.2221003 -4.2007904 -4.1671281 -4.1342459 -4.1223345 -4.1329417 -4.1535411 -4.1732368 -4.1883926 -4.2117753 -4.241693 -4.2663317 -4.2770696 -4.284071]]...]
INFO - root - 2017-12-05 19:54:33.018019: step 39010, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 66h:37m:19s remains)
INFO - root - 2017-12-05 19:54:41.606281: step 39020, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 71h:48m:12s remains)
INFO - root - 2017-12-05 19:54:50.218432: step 39030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 70h:28m:24s remains)
INFO - root - 2017-12-05 19:54:58.833800: step 39040, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 71h:04m:47s remains)
INFO - root - 2017-12-05 19:55:07.331815: step 39050, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 69h:35m:34s remains)
INFO - root - 2017-12-05 19:55:15.754348: step 39060, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 66h:27m:55s remains)
INFO - root - 2017-12-05 19:55:24.222920: step 39070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:49m:21s remains)
INFO - root - 2017-12-05 19:55:32.740355: step 39080, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 67h:47m:00s remains)
INFO - root - 2017-12-05 19:55:41.274713: step 39090, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 67h:27m:33s remains)
INFO - root - 2017-12-05 19:55:49.838421: step 39100, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 71h:08m:26s remains)
2017-12-05 19:55:50.611628: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2368469 -4.223732 -4.2132797 -4.20084 -4.191771 -4.1731834 -4.1391506 -4.1729546 -4.2324266 -4.2229424 -4.2130237 -4.2089648 -4.1991358 -4.1938944 -4.2030907][-4.2305207 -4.2120342 -4.1979275 -4.1882434 -4.183394 -4.1711569 -4.1431279 -4.1711264 -4.2259746 -4.2180538 -4.2134414 -4.2126722 -4.2089648 -4.2094679 -4.2205997][-4.2362924 -4.2185464 -4.2085762 -4.2054763 -4.2019448 -4.1892009 -4.1573014 -4.1737804 -4.2207522 -4.2220855 -4.22905 -4.2332873 -4.228899 -4.228992 -4.2344007][-4.2562985 -4.2411728 -4.2351933 -4.2314749 -4.2254925 -4.2092609 -4.1712046 -4.1781158 -4.2235308 -4.2360778 -4.2505751 -4.2555642 -4.2473512 -4.2426057 -4.2410011][-4.2656283 -4.2462616 -4.235455 -4.2212005 -4.2024155 -4.1739082 -4.1304684 -4.1360989 -4.1865458 -4.2124548 -4.2359886 -4.245935 -4.2352066 -4.2236419 -4.2154779][-4.2527957 -4.2265887 -4.2064972 -4.1802969 -4.1490107 -4.1078887 -4.052866 -4.0532274 -4.1001835 -4.1305208 -4.1669035 -4.1932187 -4.1893115 -4.1770072 -4.171504][-4.2271428 -4.2015443 -4.1786165 -4.1457362 -4.1030436 -4.0438533 -3.9665289 -3.9448135 -3.9642339 -3.9839172 -4.0441055 -4.1028795 -4.1207166 -4.1227221 -4.1289935][-4.2270322 -4.2104516 -4.1934261 -4.160007 -4.1099663 -4.0383396 -3.9488978 -3.9134369 -3.9046638 -3.9042311 -3.9779942 -4.061985 -4.1037884 -4.120007 -4.1299477][-4.2527566 -4.2473192 -4.2402973 -4.2170124 -4.1730509 -4.1102548 -4.0396705 -4.0211792 -4.0197296 -4.0120406 -4.0593395 -4.1236854 -4.1626844 -4.1821318 -4.189642][-4.275125 -4.2745686 -4.2770391 -4.2659411 -4.2345257 -4.1866622 -4.1399727 -4.1443272 -4.1526723 -4.1427112 -4.1669607 -4.207633 -4.23472 -4.2470732 -4.2480693][-4.2843266 -4.2827749 -4.2862716 -4.2797265 -4.2567825 -4.2179246 -4.185782 -4.204885 -4.2202082 -4.210381 -4.2211885 -4.2457113 -4.26246 -4.2704325 -4.2701893][-4.2809553 -4.2782135 -4.2782736 -4.2748141 -4.2595072 -4.2301197 -4.209547 -4.235486 -4.2501683 -4.2424369 -4.2438321 -4.252655 -4.2563596 -4.2559223 -4.2511783][-4.2725215 -4.2657266 -4.2620282 -4.2600689 -4.2538481 -4.2356753 -4.2256122 -4.25127 -4.2604876 -4.2494907 -4.24244 -4.2399158 -4.2337832 -4.2267423 -4.219172][-4.2618713 -4.2469172 -4.2366204 -4.2336912 -4.2319074 -4.2202053 -4.214437 -4.2348342 -4.2397194 -4.2301254 -4.2245836 -4.2204227 -4.2131181 -4.2061296 -4.2020297][-4.2564783 -4.2365751 -4.2201481 -4.2133365 -4.2078524 -4.1958504 -4.188817 -4.2045374 -4.2113261 -4.2077823 -4.2075524 -4.2079363 -4.2055678 -4.2009764 -4.19854]]...]
INFO - root - 2017-12-05 19:55:59.141285: step 39110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 70h:21m:39s remains)
INFO - root - 2017-12-05 19:56:07.722878: step 39120, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 70h:52m:06s remains)
INFO - root - 2017-12-05 19:56:16.398974: step 39130, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 70h:20m:06s remains)
INFO - root - 2017-12-05 19:56:24.886260: step 39140, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 71h:58m:24s remains)
INFO - root - 2017-12-05 19:56:33.408712: step 39150, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 71h:15m:51s remains)
INFO - root - 2017-12-05 19:56:41.830165: step 39160, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 70h:43m:51s remains)
INFO - root - 2017-12-05 19:56:50.347376: step 39170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 68h:22m:15s remains)
INFO - root - 2017-12-05 19:56:58.782880: step 39180, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 67h:56m:16s remains)
INFO - root - 2017-12-05 19:57:07.324148: step 39190, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 70h:49m:48s remains)
INFO - root - 2017-12-05 19:57:15.951941: step 39200, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 67h:40m:28s remains)
2017-12-05 19:57:16.673266: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3105946 -4.3168087 -4.3214746 -4.3223648 -4.3247771 -4.3243222 -4.32064 -4.3178587 -4.3181515 -4.3173165 -4.3127775 -4.3062882 -4.3033204 -4.3050418 -4.3105149][-4.2731361 -4.2804465 -4.28561 -4.2841558 -4.2899141 -4.2957611 -4.2936516 -4.2887154 -4.285183 -4.2832217 -4.276103 -4.2656565 -4.2601871 -4.2638407 -4.2759552][-4.2138438 -4.2213669 -4.224761 -4.2213016 -4.230207 -4.2431974 -4.2475214 -4.2458715 -4.2421651 -4.2384596 -4.2250924 -4.2090168 -4.19922 -4.2012634 -4.2193127][-4.1419711 -4.14797 -4.1484919 -4.1431518 -4.150043 -4.1647835 -4.1702089 -4.1770487 -4.1871529 -4.185112 -4.1659536 -4.1452479 -4.1347518 -4.1354971 -4.1554608][-4.0877218 -4.0878105 -4.0796847 -4.0666795 -4.0677972 -4.0772915 -4.0721922 -4.0817046 -4.116787 -4.1318851 -4.11739 -4.1014218 -4.0938873 -4.0908785 -4.1051893][-4.0610447 -4.0436645 -4.0069795 -3.9743838 -3.9660046 -3.9569125 -3.9184217 -3.9091823 -3.9830179 -4.0511413 -4.0719657 -4.0755987 -4.0759082 -4.0697231 -4.0746651][-4.0701351 -4.0326376 -3.9653687 -3.9105263 -3.881618 -3.8312941 -3.7279229 -3.6635404 -3.7845507 -3.9251447 -4.0021281 -4.0447545 -4.0612345 -4.0593777 -4.066545][-4.126986 -4.0881729 -4.0202847 -3.9661644 -3.9241576 -3.8430424 -3.6844273 -3.5480859 -3.6640649 -3.8302147 -3.9365325 -3.9969852 -4.020432 -4.0314083 -4.0583706][-4.1811695 -4.1565914 -4.1132684 -4.0764036 -4.0437026 -3.983988 -3.8669498 -3.7601001 -3.8076565 -3.8983297 -3.9559011 -3.9783852 -3.980907 -3.9959557 -4.0386949][-4.2179594 -4.2097239 -4.1880541 -4.1657667 -4.1471109 -4.1138158 -4.0497 -3.9930944 -4.002049 -4.0297861 -4.0325356 -4.0096865 -3.9829011 -3.9853425 -4.02583][-4.2323208 -4.2336617 -4.2274809 -4.215528 -4.2040915 -4.1921444 -4.16477 -4.1346741 -4.1268864 -4.1238527 -4.1022573 -4.0617495 -4.0206981 -4.0095148 -4.037961][-4.2546458 -4.256866 -4.2541718 -4.2483797 -4.2423668 -4.2396083 -4.2276978 -4.212337 -4.2016873 -4.1885915 -4.1685553 -4.1390171 -4.1071458 -4.0916147 -4.1065059][-4.2795572 -4.2777195 -4.273881 -4.2701683 -4.2652335 -4.2649937 -4.2594256 -4.24916 -4.2415037 -4.234921 -4.2270145 -4.2149706 -4.1998916 -4.1876569 -4.19121][-4.2913795 -4.2868495 -4.2818508 -4.2809005 -4.2769866 -4.2759018 -4.2709618 -4.2622848 -4.254458 -4.25377 -4.2552605 -4.2567444 -4.257369 -4.2508225 -4.2485213][-4.3004169 -4.2953787 -4.2901387 -4.292347 -4.2940173 -4.2924051 -4.2866106 -4.2783008 -4.2713394 -4.2735529 -4.2798853 -4.2862973 -4.29092 -4.2873569 -4.2824636]]...]
INFO - root - 2017-12-05 19:57:25.263660: step 39210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 68h:32m:41s remains)
INFO - root - 2017-12-05 19:57:33.861832: step 39220, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 68h:02m:18s remains)
INFO - root - 2017-12-05 19:57:42.322744: step 39230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 68h:43m:39s remains)
INFO - root - 2017-12-05 19:57:50.904898: step 39240, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:31m:00s remains)
INFO - root - 2017-12-05 19:57:59.398471: step 39250, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 69h:11m:39s remains)
INFO - root - 2017-12-05 19:58:07.825920: step 39260, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.874 sec/batch; 71h:09m:54s remains)
INFO - root - 2017-12-05 19:58:16.432864: step 39270, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 67h:44m:06s remains)
INFO - root - 2017-12-05 19:58:25.006074: step 39280, loss = 2.03, batch loss = 1.97 (9.0 examples/sec; 0.885 sec/batch; 72h:04m:00s remains)
INFO - root - 2017-12-05 19:58:33.529030: step 39290, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 69h:09m:52s remains)
INFO - root - 2017-12-05 19:58:42.157192: step 39300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 69h:34m:00s remains)
2017-12-05 19:58:42.981804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.213398 -4.208519 -4.2098703 -4.2112713 -4.2145433 -4.2187715 -4.2237659 -4.2290111 -4.2358274 -4.2421584 -4.2429876 -4.2423353 -4.2361603 -4.2282414 -4.2368321][-4.2216549 -4.2209125 -4.2182851 -4.209486 -4.205574 -4.2086124 -4.2163167 -4.225194 -4.2311363 -4.2358556 -4.2341104 -4.2278738 -4.2193947 -4.2130327 -4.2242284][-4.2198462 -4.2212048 -4.2139306 -4.1991272 -4.1944604 -4.2013178 -4.2125883 -4.2212567 -4.2233176 -4.2254581 -4.2226734 -4.2104435 -4.1969495 -4.189899 -4.1987371][-4.2075028 -4.207377 -4.1978192 -4.181952 -4.1780877 -4.1847334 -4.1909895 -4.1940222 -4.1973987 -4.2028365 -4.1989636 -4.1831942 -4.1699772 -4.1676359 -4.1787152][-4.1980162 -4.1929107 -4.1818285 -4.1679435 -4.1609035 -4.1614738 -4.1609483 -4.1586418 -4.1635671 -4.1697845 -4.1677842 -4.1550465 -4.1440969 -4.1459584 -4.1639376][-4.1838031 -4.1718259 -4.1621995 -4.1471472 -4.1328082 -4.1222396 -4.1122804 -4.1090188 -4.1205859 -4.1322918 -4.1329355 -4.1231937 -4.1181517 -4.1296806 -4.1611][-4.1622643 -4.1437368 -4.1341639 -4.1141233 -4.0859065 -4.053205 -4.0246677 -4.0294876 -4.0635419 -4.0931 -4.1098752 -4.1075368 -4.1094556 -4.1334124 -4.1746321][-4.1350894 -4.1072316 -4.0890265 -4.0592713 -4.0170751 -3.9701419 -3.9355793 -3.9630704 -4.0230236 -4.0689044 -4.1008563 -4.1127582 -4.1265235 -4.1582994 -4.1990342][-4.1195364 -4.0856147 -4.0563917 -4.0182614 -3.9833512 -3.9620016 -3.95144 -3.98501 -4.0410433 -4.0816221 -4.1140852 -4.1367993 -4.16164 -4.1908908 -4.2190981][-4.1346903 -4.1022625 -4.0717134 -4.0410585 -4.0291557 -4.0363717 -4.0477629 -4.0715303 -4.0994778 -4.1245661 -4.1483316 -4.1702824 -4.1927571 -4.2129478 -4.2294121][-4.1623597 -4.1380448 -4.1186967 -4.1038485 -4.1060772 -4.1203794 -4.1325021 -4.1445856 -4.1560993 -4.1695428 -4.1844893 -4.2012458 -4.2142305 -4.2220345 -4.2282033][-4.1734009 -4.161355 -4.1549883 -4.1522126 -4.1569891 -4.1661434 -4.1727986 -4.1793475 -4.1866007 -4.1952028 -4.2054081 -4.2133141 -4.2153192 -4.2153621 -4.2209444][-4.1810422 -4.1785789 -4.1789079 -4.1801329 -4.1806574 -4.1830926 -4.187449 -4.1926293 -4.1981335 -4.2011757 -4.2063661 -4.20796 -4.2090387 -4.212111 -4.2222586][-4.2082686 -4.2041178 -4.2041335 -4.2073903 -4.2077651 -4.2078109 -4.2080779 -4.2096992 -4.2099066 -4.209013 -4.2089238 -4.2052846 -4.2071061 -4.2118106 -4.2218256][-4.248569 -4.2397618 -4.2391534 -4.2453709 -4.2486668 -4.2476563 -4.2442074 -4.2412553 -4.2389312 -4.239346 -4.2375197 -4.2303596 -4.2295036 -4.2319493 -4.2367539]]...]
INFO - root - 2017-12-05 19:58:51.326497: step 39310, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 70h:09m:57s remains)
INFO - root - 2017-12-05 19:58:59.785890: step 39320, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 69h:23m:18s remains)
INFO - root - 2017-12-05 19:59:08.362324: step 39330, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 67h:24m:24s remains)
INFO - root - 2017-12-05 19:59:16.872816: step 39340, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 70h:43m:57s remains)
INFO - root - 2017-12-05 19:59:25.357266: step 39350, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 69h:29m:00s remains)
INFO - root - 2017-12-05 19:59:33.672821: step 39360, loss = 2.06, batch loss = 2.01 (8.2 examples/sec; 0.976 sec/batch; 79h:26m:48s remains)
INFO - root - 2017-12-05 19:59:42.114215: step 39370, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 67h:28m:04s remains)
INFO - root - 2017-12-05 19:59:50.646336: step 39380, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 69h:03m:29s remains)
INFO - root - 2017-12-05 19:59:59.155147: step 39390, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 69h:01m:13s remains)
INFO - root - 2017-12-05 20:00:07.478376: step 39400, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.805 sec/batch; 65h:30m:32s remains)
2017-12-05 20:00:08.281219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3129616 -4.3120322 -4.3101196 -4.308042 -4.304944 -4.299674 -4.2943597 -4.291173 -4.2955942 -4.3029523 -4.30705 -4.3102608 -4.3094492 -4.3045917 -4.3016567][-4.2875867 -4.2833419 -4.2800651 -4.278048 -4.2745776 -4.2678809 -4.2604427 -4.2586732 -4.2692008 -4.2829571 -4.2862277 -4.2873521 -4.2856283 -4.2800283 -4.2786708][-4.265604 -4.2579241 -4.2547374 -4.2532959 -4.249866 -4.2398171 -4.2270508 -4.2243891 -4.2422304 -4.261148 -4.2627854 -4.2613754 -4.2595711 -4.252193 -4.2516427][-4.2525916 -4.2394104 -4.2325292 -4.2272687 -4.2192049 -4.2000284 -4.1780667 -4.1727023 -4.1993551 -4.2278166 -4.2337804 -4.2340579 -4.23326 -4.2266035 -4.2306986][-4.2385588 -4.2172441 -4.2020249 -4.1860232 -4.1640787 -4.1276255 -4.0835519 -4.0660067 -4.10555 -4.1545892 -4.1748 -4.1857982 -4.1954651 -4.1997986 -4.2138596][-4.2264013 -4.1950397 -4.1680145 -4.1391921 -4.1031003 -4.0461941 -3.967587 -3.93045 -3.9924963 -4.0701404 -4.1087666 -4.131608 -4.1527796 -4.17115 -4.1951728][-4.2203603 -4.1773648 -4.1340227 -4.0913515 -4.043191 -3.9670243 -3.8572764 -3.8039145 -3.8950303 -4.0049119 -4.0692019 -4.1037874 -4.1302629 -4.1529036 -4.1802526][-4.2190294 -4.1675653 -4.1144686 -4.066853 -4.0205312 -3.9504936 -3.8459997 -3.7951202 -3.8814657 -3.9941478 -4.0702877 -4.1107264 -4.1374435 -4.1589384 -4.1846061][-4.224647 -4.176053 -4.1287622 -4.0906997 -4.0595832 -4.0172653 -3.9478364 -3.910882 -3.9656858 -4.0489979 -4.1129317 -4.1473722 -4.1694832 -4.1858091 -4.2088747][-4.2445774 -4.2047257 -4.1691818 -4.1455297 -4.1304679 -4.1084042 -4.0718012 -4.050756 -4.0832977 -4.138103 -4.1836882 -4.2071362 -4.2237616 -4.2338581 -4.2509108][-4.26855 -4.237781 -4.2117305 -4.1976671 -4.1929531 -4.185657 -4.1754236 -4.1702003 -4.1904883 -4.2243218 -4.2521372 -4.2648911 -4.276154 -4.2816777 -4.2913957][-4.2919416 -4.2705312 -4.2528548 -4.2440867 -4.24535 -4.246923 -4.2517018 -4.2568035 -4.2701659 -4.28602 -4.2976665 -4.3020005 -4.309423 -4.3144665 -4.319695][-4.3114729 -4.2982926 -4.2866898 -4.2812142 -4.2854776 -4.2924652 -4.3021955 -4.3097835 -4.3182669 -4.3222132 -4.3228188 -4.3222208 -4.3276081 -4.3314419 -4.3332963][-4.3250217 -4.3185062 -4.3118711 -4.3083086 -4.3117113 -4.318903 -4.3278551 -4.3328853 -4.336422 -4.3357234 -4.3328276 -4.3317413 -4.334517 -4.3363214 -4.3366466][-4.3331819 -4.329968 -4.3256621 -4.3227096 -4.3238373 -4.3285785 -4.334115 -4.3365445 -4.33657 -4.3349714 -4.3325558 -4.3308415 -4.3310843 -4.3319378 -4.3325043]]...]
INFO - root - 2017-12-05 20:00:16.736544: step 39410, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 68h:22m:50s remains)
INFO - root - 2017-12-05 20:00:25.211552: step 39420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:59m:59s remains)
INFO - root - 2017-12-05 20:00:33.721388: step 39430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 68h:51m:25s remains)
INFO - root - 2017-12-05 20:00:42.164068: step 39440, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 66h:29m:09s remains)
INFO - root - 2017-12-05 20:00:50.692260: step 39450, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 71h:43m:31s remains)
INFO - root - 2017-12-05 20:00:59.084217: step 39460, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.842 sec/batch; 68h:33m:46s remains)
INFO - root - 2017-12-05 20:01:07.540721: step 39470, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.854 sec/batch; 69h:31m:38s remains)
INFO - root - 2017-12-05 20:01:16.055663: step 39480, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 68h:30m:23s remains)
INFO - root - 2017-12-05 20:01:24.656231: step 39490, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.894 sec/batch; 72h:45m:58s remains)
INFO - root - 2017-12-05 20:01:33.220550: step 39500, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 70h:22m:13s remains)
2017-12-05 20:01:33.963832: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1503339 -4.1671124 -4.172823 -4.175837 -4.1849351 -4.188345 -4.1882839 -4.1908555 -4.1999931 -4.2068739 -4.2008114 -4.1905985 -4.1665225 -4.1462073 -4.138195][-4.1024151 -4.1366825 -4.1596723 -4.1761374 -4.1977377 -4.2033095 -4.1981382 -4.1961489 -4.2046447 -4.211153 -4.1977038 -4.1683073 -4.1241856 -4.0927806 -4.0833607][-4.0312576 -4.0938668 -4.1463342 -4.1797142 -4.20057 -4.1995764 -4.1873732 -4.17862 -4.1866131 -4.1932654 -4.1779518 -4.1396308 -4.0901184 -4.0585938 -4.0555081][-4.0066576 -4.0935726 -4.16381 -4.1952648 -4.198132 -4.176084 -4.1404085 -4.1169538 -4.127552 -4.1461887 -4.1444178 -4.1205711 -4.0912194 -4.079298 -4.0867462][-4.07285 -4.1536465 -4.2074633 -4.211319 -4.1818285 -4.1283035 -4.0611877 -4.0289044 -4.0536456 -4.0968223 -4.1226273 -4.1312919 -4.132359 -4.1415272 -4.1568832][-4.1465287 -4.2014418 -4.2256546 -4.1934648 -4.1290894 -4.0452147 -3.9524612 -3.9300528 -3.9939034 -4.0731716 -4.1293483 -4.1626959 -4.1792369 -4.192524 -4.2027974][-4.1859303 -4.2115655 -4.2025452 -4.130548 -4.0285897 -3.9206223 -3.8152742 -3.8258753 -3.9438517 -4.0549803 -4.1313844 -4.1778555 -4.1960897 -4.1978469 -4.1939826][-4.2082868 -4.2062745 -4.1702356 -4.0769858 -3.9645255 -3.8689084 -3.791801 -3.8313792 -3.9644456 -4.0751019 -4.1443458 -4.1897969 -4.2052841 -4.1950788 -4.1779][-4.2225924 -4.2001233 -4.1509757 -4.0633521 -3.9744041 -3.9180818 -3.8905773 -3.9350684 -4.0402946 -4.1265087 -4.1754537 -4.2103682 -4.2207975 -4.2072434 -4.1887565][-4.2114143 -4.1782365 -4.126235 -4.05802 -4.0003681 -3.9779489 -3.9882038 -4.0327516 -4.1077332 -4.1698756 -4.2005124 -4.2211375 -4.2255712 -4.2155027 -4.20212][-4.19999 -4.1628013 -4.1199317 -4.0750146 -4.0415592 -4.0382113 -4.0679517 -4.1123753 -4.1634364 -4.2010574 -4.2159705 -4.2227592 -4.2211571 -4.2154098 -4.20703][-4.2112265 -4.1790142 -4.1551347 -4.1370692 -4.1237855 -4.1299281 -4.1591296 -4.1927233 -4.2210135 -4.2397943 -4.245018 -4.2419419 -4.236927 -4.2332268 -4.2273545][-4.2514529 -4.2292848 -4.2199855 -4.2174258 -4.2156439 -4.2250032 -4.24729 -4.2669787 -4.2783523 -4.2864709 -4.2891722 -4.2850294 -4.279388 -4.2765851 -4.2704453][-4.2940617 -4.2806063 -4.2787414 -4.2829108 -4.286119 -4.2955742 -4.311388 -4.3213706 -4.3223276 -4.3238306 -4.324389 -4.3196478 -4.3150182 -4.3131933 -4.3082304][-4.31907 -4.3114233 -4.3126583 -4.3187461 -4.3227711 -4.3271818 -4.335011 -4.3382316 -4.3362131 -4.3353615 -4.3356538 -4.3327775 -4.3301344 -4.3296285 -4.3266506]]...]
INFO - root - 2017-12-05 20:01:42.361035: step 39510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 69h:15m:13s remains)
INFO - root - 2017-12-05 20:01:50.864872: step 39520, loss = 2.06, batch loss = 2.01 (10.2 examples/sec; 0.786 sec/batch; 63h:58m:34s remains)
INFO - root - 2017-12-05 20:01:59.314874: step 39530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 70h:32m:17s remains)
INFO - root - 2017-12-05 20:02:07.926808: step 39540, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 72h:04m:28s remains)
INFO - root - 2017-12-05 20:02:16.441262: step 39550, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 70h:22m:40s remains)
INFO - root - 2017-12-05 20:02:24.927102: step 39560, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 71h:26m:46s remains)
INFO - root - 2017-12-05 20:02:33.426014: step 39570, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 67h:45m:26s remains)
INFO - root - 2017-12-05 20:02:41.876781: step 39580, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 66h:44m:03s remains)
INFO - root - 2017-12-05 20:02:50.351008: step 39590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:57m:43s remains)
INFO - root - 2017-12-05 20:02:58.983681: step 39600, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 70h:17m:22s remains)
2017-12-05 20:02:59.738452: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1641374 -4.1999903 -4.2322283 -4.2355657 -4.2188892 -4.2010636 -4.1924934 -4.1999016 -4.2179294 -4.23956 -4.2527905 -4.2494612 -4.2373161 -4.2317772 -4.2328358][-4.1566367 -4.1914797 -4.2208614 -4.2221689 -4.2058411 -4.1867762 -4.1767659 -4.1819038 -4.1977081 -4.218617 -4.23322 -4.2319283 -4.2223816 -4.2168932 -4.2124929][-4.1530042 -4.1842637 -4.2090459 -4.2057281 -4.185142 -4.1622305 -4.1513867 -4.1575851 -4.1737785 -4.1950264 -4.2080755 -4.2054348 -4.1945639 -4.1860018 -4.1764412][-4.1648374 -4.1918478 -4.2112007 -4.2004733 -4.165205 -4.1264405 -4.1096058 -4.1212335 -4.1445103 -4.1689911 -4.180088 -4.1746573 -4.1586657 -4.1417556 -4.1278777][-4.1839972 -4.2075987 -4.2217932 -4.2033434 -4.1476946 -4.0836859 -4.0514436 -4.0718784 -4.1143126 -4.1536016 -4.1681809 -4.1589518 -4.1361346 -4.11266 -4.0976934][-4.1920571 -4.2119484 -4.2223864 -4.1996093 -4.130465 -4.0364528 -3.9787045 -4.0080266 -4.0792222 -4.1422195 -4.1664286 -4.1616788 -4.1411986 -4.1184278 -4.1030459][-4.190414 -4.2057714 -4.2150211 -4.1945052 -4.1201715 -4.0019979 -3.917594 -3.9523795 -4.0505738 -4.1331449 -4.1666818 -4.1709585 -4.1573291 -4.1386662 -4.1270351][-4.1794257 -4.193399 -4.2061839 -4.1943398 -4.1275811 -4.0105219 -3.9192252 -3.9482393 -4.0472164 -4.126184 -4.1617589 -4.1726933 -4.1666074 -4.1532826 -4.1482482][-4.159965 -4.1751227 -4.1926923 -4.1919088 -4.1471376 -4.0609722 -3.9922168 -4.0044689 -4.0729814 -4.1283736 -4.1570692 -4.171473 -4.1717854 -4.1652012 -4.1675873][-4.1364412 -4.154264 -4.1787033 -4.189558 -4.1673303 -4.114439 -4.0664172 -4.0641384 -4.1017756 -4.13427 -4.1523161 -4.1611218 -4.1602211 -4.1590114 -4.1698542][-4.1137152 -4.1355848 -4.1687307 -4.1905012 -4.1834016 -4.1494088 -4.1125665 -4.1038923 -4.1270676 -4.1469369 -4.15741 -4.1593585 -4.1544189 -4.1532969 -4.1674623][-4.1071959 -4.1293077 -4.1653624 -4.19129 -4.1897569 -4.165535 -4.1378164 -4.1294365 -4.1460648 -4.1620488 -4.1712418 -4.1718965 -4.1659837 -4.16247 -4.1744442][-4.1248975 -4.1446247 -4.176271 -4.1999707 -4.1992378 -4.1814208 -4.1605463 -4.1522 -4.1636906 -4.1759491 -4.1827912 -4.1817021 -4.176981 -4.1734157 -4.1815629][-4.1555071 -4.1738148 -4.1988773 -4.2151203 -4.21139 -4.1986856 -4.1840167 -4.1736364 -4.1791029 -4.1875606 -4.1923971 -4.1910744 -4.1898537 -4.19022 -4.1976151][-4.1864028 -4.2038121 -4.2249012 -4.2353544 -4.229166 -4.219316 -4.2071228 -4.1948605 -4.1942153 -4.1989322 -4.2016807 -4.2014728 -4.2063365 -4.2129841 -4.2216487]]...]
INFO - root - 2017-12-05 20:03:08.290234: step 39610, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 69h:23m:23s remains)
INFO - root - 2017-12-05 20:03:16.721768: step 39620, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 69h:24m:49s remains)
INFO - root - 2017-12-05 20:03:25.167208: step 39630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 69h:06m:45s remains)
INFO - root - 2017-12-05 20:03:33.614009: step 39640, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:15m:42s remains)
INFO - root - 2017-12-05 20:03:42.141346: step 39650, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 68h:39m:36s remains)
INFO - root - 2017-12-05 20:03:50.616032: step 39660, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 69h:13m:07s remains)
INFO - root - 2017-12-05 20:03:59.087001: step 39670, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 71h:54m:40s remains)
INFO - root - 2017-12-05 20:04:07.685496: step 39680, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 69h:25m:07s remains)
INFO - root - 2017-12-05 20:04:16.210417: step 39690, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 69h:21m:09s remains)
INFO - root - 2017-12-05 20:04:24.674552: step 39700, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 67h:08m:09s remains)
2017-12-05 20:04:25.412173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219553 -4.2522469 -4.2601652 -4.252358 -4.2408161 -4.23954 -4.2529249 -4.2774553 -4.3038635 -4.32262 -4.3328061 -4.3358731 -4.3362455 -4.3351421 -4.3306389][-4.2609582 -4.2866139 -4.2876768 -4.2707872 -4.2481222 -4.2351379 -4.2392592 -4.2590742 -4.2860413 -4.3088522 -4.3248005 -4.333951 -4.338768 -4.338623 -4.3311806][-4.2937303 -4.3159027 -4.3147287 -4.2958755 -4.2690306 -4.2477131 -4.2411575 -4.2502108 -4.2697673 -4.2911596 -4.3106179 -4.3261638 -4.3361931 -4.3375554 -4.3275948][-4.3125777 -4.3327193 -4.3342848 -4.3208919 -4.2980962 -4.2744703 -4.2576842 -4.2513494 -4.2570405 -4.2716675 -4.2933426 -4.3162026 -4.332417 -4.3368182 -4.3271227][-4.3181043 -4.3361759 -4.3422227 -4.3382535 -4.3237867 -4.3014994 -4.2755952 -4.2522163 -4.2407928 -4.2474914 -4.2713175 -4.3019028 -4.3266115 -4.3380027 -4.3330779][-4.30878 -4.324327 -4.3329906 -4.335556 -4.3274589 -4.3057418 -4.2710681 -4.2320342 -4.2072306 -4.2102671 -4.2387209 -4.2767296 -4.3098564 -4.3298512 -4.3325052][-4.2903738 -4.302803 -4.3100815 -4.3123426 -4.3024073 -4.2740121 -4.2270665 -4.1734056 -4.1399884 -4.1455517 -4.1823621 -4.2302332 -4.2732363 -4.303266 -4.3159389][-4.2742929 -4.2830739 -4.2848 -4.280302 -4.2620082 -4.2222676 -4.1601739 -4.0905557 -4.0477462 -4.0575509 -4.1075888 -4.1693859 -4.2233706 -4.263227 -4.286653][-4.2638445 -4.2706032 -4.2670159 -4.2538438 -4.22394 -4.1704736 -4.0919833 -4.0065389 -3.9551909 -3.97006 -4.03457 -4.1106896 -4.1757646 -4.2243638 -4.255219][-4.2552843 -4.2624512 -4.2566743 -4.2391114 -4.202888 -4.143363 -4.0616074 -3.9755664 -3.9283283 -3.9464123 -4.0133119 -4.0920725 -4.1592569 -4.2088151 -4.2396307][-4.245162 -4.254406 -4.251049 -4.2357349 -4.2046523 -4.1556187 -4.0910606 -4.0249329 -3.9922464 -4.0099454 -4.0661492 -4.1322274 -4.1882753 -4.229104 -4.2526989][-4.232132 -4.2432656 -4.245688 -4.2402048 -4.2261238 -4.2020054 -4.1683803 -4.1323972 -4.1148014 -4.1283927 -4.1666451 -4.2108335 -4.2478647 -4.2736778 -4.2872105][-4.2260165 -4.2374244 -4.2446489 -4.249752 -4.2534781 -4.2540832 -4.2483759 -4.2381358 -4.2317991 -4.2400546 -4.2600584 -4.2819 -4.2985897 -4.309186 -4.3142309][-4.2323875 -4.2404618 -4.2474217 -4.2558136 -4.2670689 -4.2789288 -4.2872925 -4.2908077 -4.2910714 -4.2953868 -4.3031645 -4.3099174 -4.313128 -4.3141308 -4.315969][-4.2464957 -4.2474766 -4.248435 -4.2535996 -4.2646437 -4.2792783 -4.2914286 -4.2983227 -4.3003154 -4.3016438 -4.3035374 -4.3031006 -4.3001194 -4.2980404 -4.300766]]...]
INFO - root - 2017-12-05 20:04:33.963454: step 39710, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 69h:03m:35s remains)
INFO - root - 2017-12-05 20:04:42.470968: step 39720, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 70h:02m:06s remains)
INFO - root - 2017-12-05 20:04:50.885049: step 39730, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 68h:46m:40s remains)
INFO - root - 2017-12-05 20:04:59.341102: step 39740, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 70h:15m:51s remains)
INFO - root - 2017-12-05 20:05:07.894739: step 39750, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 69h:49m:04s remains)
INFO - root - 2017-12-05 20:05:16.445332: step 39760, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 70h:27m:07s remains)
INFO - root - 2017-12-05 20:05:24.957635: step 39770, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.870 sec/batch; 70h:45m:52s remains)
INFO - root - 2017-12-05 20:05:33.498998: step 39780, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 68h:31m:16s remains)
INFO - root - 2017-12-05 20:05:42.013241: step 39790, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 67h:46m:33s remains)
INFO - root - 2017-12-05 20:05:50.548686: step 39800, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 69h:52m:46s remains)
2017-12-05 20:05:51.324258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3009825 -4.2812824 -4.2576203 -4.238729 -4.2326846 -4.2311988 -4.2283583 -4.2184052 -4.221705 -4.2334456 -4.2223845 -4.2146106 -4.2209229 -4.2290249 -4.2383161][-4.302124 -4.2839322 -4.2597508 -4.2372785 -4.2298737 -4.22791 -4.2223635 -4.2084045 -4.2087121 -4.2177038 -4.2049503 -4.1904235 -4.1910658 -4.198215 -4.2072635][-4.3018 -4.2853203 -4.2634254 -4.2440896 -4.2397351 -4.2371488 -4.22613 -4.2087622 -4.2049255 -4.2086663 -4.1920137 -4.1733603 -4.1673841 -4.1708145 -4.1798105][-4.3000059 -4.2831683 -4.263803 -4.246839 -4.2405872 -4.2340198 -4.2191191 -4.2011385 -4.1949182 -4.1920271 -4.171854 -4.1508837 -4.1386266 -4.1428361 -4.1550174][-4.2972088 -4.2764835 -4.2538567 -4.2328806 -4.2171993 -4.203927 -4.18887 -4.1741042 -4.1685114 -4.1620626 -4.1384592 -4.1216125 -4.1109133 -4.1206722 -4.1355424][-4.29419 -4.2693176 -4.2404194 -4.2109909 -4.1826339 -4.1575165 -4.14015 -4.1257977 -4.1178432 -4.1128397 -4.0901885 -4.082653 -4.0824003 -4.1021142 -4.1209769][-4.2922287 -4.2668114 -4.2321177 -4.1915784 -4.1460443 -4.1045456 -4.0820217 -4.0646439 -4.0537791 -4.0500817 -4.02821 -4.0292954 -4.0481615 -4.0897756 -4.1172285][-4.2906723 -4.2665105 -4.2292085 -4.1848726 -4.1341772 -4.0859046 -4.0621266 -4.0452805 -4.0446205 -4.0572462 -4.0492291 -4.0581751 -4.0776625 -4.1224732 -4.1466827][-4.2886252 -4.2629375 -4.2245083 -4.1849127 -4.1452551 -4.1048794 -4.0867519 -4.0731468 -4.0883884 -4.1216965 -4.1278505 -4.135632 -4.1437073 -4.1747622 -4.1893768][-4.2871666 -4.2621269 -4.2278309 -4.1980171 -4.1767287 -4.1519318 -4.1431437 -4.1313372 -4.1519012 -4.1943212 -4.2036877 -4.2040558 -4.2029939 -4.2210813 -4.2298193][-4.2858787 -4.2642269 -4.2379723 -4.2171865 -4.2090006 -4.1977057 -4.1944652 -4.1825275 -4.1978626 -4.2330656 -4.2402325 -4.2393374 -4.235786 -4.245564 -4.2503867][-4.2879505 -4.2702975 -4.2524152 -4.2381196 -4.2365484 -4.2355065 -4.2376733 -4.2291632 -4.2363315 -4.2565789 -4.2598987 -4.2565708 -4.2513332 -4.2535815 -4.2524757][-4.2916174 -4.2766585 -4.2642741 -4.25296 -4.2527266 -4.25827 -4.2652826 -4.2614565 -4.2661939 -4.276309 -4.2736526 -4.2659159 -4.2560544 -4.24852 -4.2408981][-4.2963386 -4.2840085 -4.2750564 -4.2629256 -4.2584076 -4.2626238 -4.2683911 -4.2672219 -4.2757463 -4.2844462 -4.2811623 -4.2707992 -4.2599382 -4.2470174 -4.2289329][-4.2997165 -4.2893839 -4.2817531 -4.2684226 -4.2592235 -4.2578125 -4.2581506 -4.2586722 -4.273807 -4.2879672 -4.2897305 -4.279242 -4.2664175 -4.2467747 -4.2205663]]...]
INFO - root - 2017-12-05 20:05:59.921324: step 39810, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 67h:18m:41s remains)
INFO - root - 2017-12-05 20:06:08.484728: step 39820, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 72h:07m:36s remains)
INFO - root - 2017-12-05 20:06:16.988836: step 39830, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 71h:34m:34s remains)
INFO - root - 2017-12-05 20:06:25.147272: step 39840, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 67h:38m:04s remains)
INFO - root - 2017-12-05 20:06:33.637660: step 39850, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:37m:21s remains)
INFO - root - 2017-12-05 20:06:41.967351: step 39860, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 69h:40m:41s remains)
INFO - root - 2017-12-05 20:06:50.453300: step 39870, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 68h:06m:47s remains)
INFO - root - 2017-12-05 20:06:58.855831: step 39880, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 65h:55m:22s remains)
INFO - root - 2017-12-05 20:07:07.487348: step 39890, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 67h:50m:17s remains)
INFO - root - 2017-12-05 20:07:16.134708: step 39900, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 70h:32m:25s remains)
2017-12-05 20:07:16.883587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2202821 -4.2424388 -4.2360363 -4.2199697 -4.2059169 -4.2017145 -4.2005587 -4.1848264 -4.1493063 -4.126749 -4.13725 -4.1639709 -4.1934419 -4.2373085 -4.2756824][-4.2056041 -4.2197537 -4.2063742 -4.1834865 -4.1665273 -4.1605983 -4.1608634 -4.1605239 -4.1429582 -4.1295667 -4.1413431 -4.164824 -4.1945872 -4.2396412 -4.2776761][-4.1792717 -4.1854668 -4.169301 -4.1463757 -4.1297393 -4.1191607 -4.1154513 -4.1307015 -4.1401653 -4.1442189 -4.1554976 -4.174068 -4.2042947 -4.2479653 -4.2811713][-4.1479445 -4.1506333 -4.1365795 -4.1203551 -4.0986118 -4.0721107 -4.0548382 -4.0826459 -4.1281939 -4.1557374 -4.1715517 -4.1906657 -4.2227149 -4.2640319 -4.2894526][-4.1467814 -4.147666 -4.1309032 -4.1133547 -4.0732841 -4.0127144 -3.9611115 -3.9992785 -4.0917807 -4.15726 -4.1896954 -4.2132907 -4.2473993 -4.2834196 -4.3004265][-4.1791973 -4.1696563 -4.1396255 -4.1090217 -4.0421915 -3.9213233 -3.8056753 -3.850605 -4.0063424 -4.1257882 -4.1884866 -4.2268033 -4.26412 -4.29413 -4.3045154][-4.2204742 -4.1979547 -4.1550765 -4.106555 -4.0136127 -3.8363051 -3.6540761 -3.6994634 -3.9076672 -4.0716367 -4.1614132 -4.2201042 -4.2644906 -4.2912164 -4.2979326][-4.2579622 -4.2300134 -4.1813364 -4.1243691 -4.0234189 -3.8488159 -3.6669972 -3.7019396 -3.8993151 -4.0599613 -4.1496549 -4.2119389 -4.2583151 -4.2853823 -4.2894354][-4.2790093 -4.2599187 -4.2165561 -4.1644096 -4.0786586 -3.9533365 -3.8284638 -3.8499806 -3.9831352 -4.0951176 -4.158524 -4.205699 -4.2468953 -4.2746763 -4.2803879][-4.2825174 -4.2826524 -4.2543154 -4.2142754 -4.1533809 -4.0780911 -4.003932 -4.01196 -4.0783663 -4.1398344 -4.1735635 -4.2011547 -4.2314029 -4.2589941 -4.2713809][-4.2730713 -4.2894759 -4.2752647 -4.2477255 -4.2091484 -4.1706533 -4.1283565 -4.1228576 -4.1444883 -4.1707907 -4.18337 -4.197093 -4.2191315 -4.2472634 -4.2663212][-4.2723527 -4.295754 -4.2903218 -4.2712226 -4.2477732 -4.2304268 -4.2096272 -4.1942883 -4.1906509 -4.1972108 -4.1979938 -4.2037225 -4.2173066 -4.2445612 -4.2675538][-4.268497 -4.2926226 -4.2923627 -4.2829385 -4.2708869 -4.2654657 -4.2573519 -4.2373085 -4.2190819 -4.2120433 -4.2097416 -4.2151346 -4.2253752 -4.2493014 -4.2715816][-4.2725124 -4.2922621 -4.2922578 -4.2868972 -4.2808952 -4.2813783 -4.2784462 -4.2577825 -4.2350726 -4.2225184 -4.2219682 -4.2307782 -4.2399111 -4.2581649 -4.2758183][-4.2828641 -4.2972078 -4.2967134 -4.2925472 -4.2870688 -4.2874756 -4.2857347 -4.2698979 -4.2520118 -4.2408466 -4.2396994 -4.2464633 -4.2538514 -4.2668977 -4.2805204]]...]
INFO - root - 2017-12-05 20:07:25.423839: step 39910, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:37m:53s remains)
INFO - root - 2017-12-05 20:07:33.854094: step 39920, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 68h:43m:03s remains)
INFO - root - 2017-12-05 20:07:42.431407: step 39930, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 68h:51m:33s remains)
INFO - root - 2017-12-05 20:07:50.978628: step 39940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 70h:30m:06s remains)
INFO - root - 2017-12-05 20:07:59.297608: step 39950, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 68h:49m:00s remains)
INFO - root - 2017-12-05 20:08:07.698873: step 39960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 70h:13m:35s remains)
INFO - root - 2017-12-05 20:08:16.175966: step 39970, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 68h:55m:13s remains)
INFO - root - 2017-12-05 20:08:24.706512: step 39980, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 70h:58m:16s remains)
INFO - root - 2017-12-05 20:08:33.335014: step 39990, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 71h:33m:48s remains)
INFO - root - 2017-12-05 20:08:41.862202: step 40000, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 71h:13m:29s remains)
2017-12-05 20:08:42.594591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.21985 -4.1967673 -4.1341023 -4.0666838 -4.0648403 -4.1213984 -4.1807871 -4.2111855 -4.2188268 -4.2145643 -4.2113786 -4.2096791 -4.2050672 -4.1912618 -4.1767726][-4.2063518 -4.2000256 -4.1513977 -4.0754423 -4.0403628 -4.0825357 -4.1496267 -4.1966929 -4.212903 -4.2088094 -4.1973753 -4.1921597 -4.1891532 -4.1824012 -4.1775641][-4.1898236 -4.2027965 -4.1782064 -4.1140594 -4.0511165 -4.0601535 -4.1222191 -4.1783929 -4.2018991 -4.20266 -4.1873446 -4.179626 -4.1809669 -4.1832037 -4.186583][-4.1828341 -4.2086544 -4.20782 -4.1600137 -4.0831323 -4.0461268 -4.0893693 -4.1527872 -4.1899304 -4.1991143 -4.1853971 -4.1727858 -4.1723485 -4.1814208 -4.1919661][-4.1882558 -4.2155848 -4.2278152 -4.1930065 -4.11186 -4.0319338 -4.0384769 -4.1094818 -4.1708341 -4.1971755 -4.1946344 -4.1803074 -4.1744938 -4.1820116 -4.1927285][-4.1999865 -4.2192073 -4.2309361 -4.2038107 -4.1262279 -4.0203495 -3.9728851 -4.0427074 -4.1363893 -4.1930757 -4.2099061 -4.2023621 -4.1897292 -4.1886635 -4.192811][-4.2098818 -4.2214494 -4.2256951 -4.2016239 -4.1327562 -4.0179424 -3.9191184 -3.9637995 -4.0835586 -4.1779752 -4.2214069 -4.2261691 -4.2151113 -4.2072635 -4.2022705][-4.2087364 -4.2141232 -4.2169142 -4.1990452 -4.1485977 -4.0484548 -3.9301908 -3.9192305 -4.0316882 -4.1499095 -4.2196198 -4.2430477 -4.2408123 -4.2326083 -4.2216215][-4.2037029 -4.2034445 -4.20701 -4.2009249 -4.178618 -4.1157589 -4.017889 -3.9570019 -4.0125346 -4.1196175 -4.2024441 -4.2437024 -4.2563896 -4.252634 -4.2391109][-4.2040143 -4.1978889 -4.1968465 -4.2005196 -4.2028146 -4.1802282 -4.118773 -4.04298 -4.0253038 -4.0912061 -4.1700916 -4.2244425 -4.25344 -4.2575388 -4.2445831][-4.2126918 -4.2038641 -4.192987 -4.1955886 -4.2099247 -4.2176538 -4.1905894 -4.1255302 -4.0644078 -4.075201 -4.1362176 -4.1942439 -4.2343011 -4.2491784 -4.2388773][-4.225565 -4.2182841 -4.2008929 -4.1945515 -4.2102885 -4.2330532 -4.2297807 -4.1843109 -4.1142006 -4.0750356 -4.1062 -4.1598487 -4.205081 -4.2279892 -4.2239432][-4.2355552 -4.2312288 -4.2155986 -4.2039251 -4.21355 -4.2384138 -4.2453 -4.2155142 -4.1524973 -4.0896106 -4.0825405 -4.1205831 -4.1686325 -4.1969824 -4.2015944][-4.2345605 -4.2335296 -4.22416 -4.2133536 -4.21474 -4.2330341 -4.240994 -4.2198224 -4.1694121 -4.1058311 -4.0700088 -4.0898681 -4.1372886 -4.1704578 -4.1815472][-4.222446 -4.22673 -4.2218342 -4.2149119 -4.2101116 -4.2162008 -4.2197208 -4.2040091 -4.1684842 -4.1208239 -4.081038 -4.083508 -4.12583 -4.1608286 -4.1753035]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 20:08:51.978272: step 40010, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 71h:49m:56s remains)
INFO - root - 2017-12-05 20:09:00.601193: step 40020, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 67h:26m:18s remains)
INFO - root - 2017-12-05 20:09:09.211472: step 40030, loss = 2.10, batch loss = 2.04 (8.8 examples/sec; 0.913 sec/batch; 74h:09m:35s remains)
INFO - root - 2017-12-05 20:09:17.673144: step 40040, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 69h:34m:35s remains)
INFO - root - 2017-12-05 20:09:26.222501: step 40050, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 70h:35m:45s remains)
INFO - root - 2017-12-05 20:09:34.258255: step 40060, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 69h:18m:24s remains)
INFO - root - 2017-12-05 20:09:42.786911: step 40070, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 70h:41m:51s remains)
INFO - root - 2017-12-05 20:09:51.406725: step 40080, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:53m:26s remains)
INFO - root - 2017-12-05 20:09:59.892934: step 40090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 70h:11m:23s remains)
INFO - root - 2017-12-05 20:10:08.476295: step 40100, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 68h:10m:39s remains)
2017-12-05 20:10:09.335378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1120033 -4.1207433 -4.1149869 -4.1104832 -4.1225266 -4.1496172 -4.1821566 -4.2107553 -4.2250576 -4.2337842 -4.2505083 -4.2687893 -4.2721496 -4.2627983 -4.2569633][-4.1505761 -4.1356659 -4.1074953 -4.0908341 -4.0995312 -4.1212411 -4.1471896 -4.1692886 -4.1716413 -4.1681542 -4.1900048 -4.2316265 -4.2487283 -4.238791 -4.2291751][-4.1577072 -4.1228604 -4.0709624 -4.0425234 -4.0548687 -4.0790687 -4.0978594 -4.107605 -4.0980487 -4.0864525 -4.1215944 -4.1865606 -4.2157912 -4.2065969 -4.1966581][-4.1360841 -4.0897312 -4.0307155 -3.9924641 -4.0012741 -4.0263 -4.0375466 -4.0279994 -3.9993386 -3.9936042 -4.0550594 -4.13803 -4.1681027 -4.1530142 -4.1393738][-4.1068144 -4.0658832 -4.0194511 -3.9825892 -3.9812989 -3.9889853 -3.9814093 -3.9475307 -3.9030735 -3.9155638 -4.0053072 -4.0960693 -4.1177611 -4.0920415 -4.0717144][-4.0777493 -4.0454783 -4.0122547 -3.9855285 -3.9756808 -3.9570413 -3.9176712 -3.8558869 -3.814573 -3.8610368 -3.9714077 -4.0555162 -4.0721145 -4.0410333 -4.0169859][-4.0672493 -4.0371914 -4.0046811 -3.9805722 -3.9620366 -3.9223924 -3.8405128 -3.7395804 -3.7179396 -3.8212106 -3.9442577 -4.011692 -4.023098 -4.003335 -3.9946756][-4.0674686 -4.0353003 -4.0001736 -3.9656363 -3.9372344 -3.8852258 -3.783159 -3.6785295 -3.6963887 -3.8235176 -3.9165282 -3.9590828 -3.9758384 -3.9759445 -3.9953387][-4.0799685 -4.0521812 -4.0170178 -3.9692683 -3.9243951 -3.8740327 -3.8026562 -3.7498786 -3.7910616 -3.8724015 -3.898308 -3.9111962 -3.9335446 -3.9594097 -4.00014][-4.101861 -4.0810924 -4.046339 -3.9883897 -3.9391458 -3.9011164 -3.8749266 -3.8726861 -3.9087491 -3.9357023 -3.9080582 -3.8974712 -3.9184213 -3.9555583 -3.9939594][-4.1288781 -4.111825 -4.0759735 -4.021421 -3.9787707 -3.9611032 -3.9688215 -3.9851522 -4.0045156 -4.0033364 -3.9588742 -3.9350119 -3.9445884 -3.971128 -3.9975371][-4.1563411 -4.1474638 -4.1220946 -4.0874681 -4.0669203 -4.0635033 -4.0740576 -4.0835013 -4.0849304 -4.0697017 -4.0258989 -4.0026774 -4.0059638 -4.0140886 -4.0199351][-4.1843061 -4.1828556 -4.1748309 -4.1593194 -4.1538839 -4.1538334 -4.1500878 -4.1392121 -4.1245332 -4.1100235 -4.0847764 -4.0752506 -4.0846782 -4.0797586 -4.0577941][-4.2173209 -4.2182121 -4.2172852 -4.2111082 -4.20901 -4.203804 -4.1840134 -4.1573982 -4.1347961 -4.1203132 -4.1069641 -4.1142974 -4.1306596 -4.1236939 -4.0922475][-4.2458982 -4.2442875 -4.2411809 -4.2360177 -4.2323761 -4.2211652 -4.1952953 -4.1632376 -4.13042 -4.1082892 -4.0951285 -4.105958 -4.1284947 -4.1309543 -4.1164083]]...]
INFO - root - 2017-12-05 20:10:17.783747: step 40110, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 70h:26m:32s remains)
INFO - root - 2017-12-05 20:10:26.366175: step 40120, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 68h:59m:51s remains)
INFO - root - 2017-12-05 20:10:34.930669: step 40130, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 66h:51m:04s remains)
INFO - root - 2017-12-05 20:10:43.394647: step 40140, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:35m:45s remains)
INFO - root - 2017-12-05 20:10:51.868079: step 40150, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 69h:33m:32s remains)
INFO - root - 2017-12-05 20:11:00.164796: step 40160, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 69h:32m:48s remains)
INFO - root - 2017-12-05 20:11:08.791980: step 40170, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 70h:03m:03s remains)
INFO - root - 2017-12-05 20:11:17.325975: step 40180, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 69h:05m:58s remains)
INFO - root - 2017-12-05 20:11:25.886282: step 40190, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 69h:19m:42s remains)
INFO - root - 2017-12-05 20:11:34.424395: step 40200, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 69h:19m:19s remains)
2017-12-05 20:11:35.200035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3032756 -4.2878242 -4.2665696 -4.2455482 -4.2298379 -4.2281475 -4.2310667 -4.2344379 -4.2446012 -4.2613873 -4.2746415 -4.2739754 -4.273519 -4.2750211 -4.2783632][-4.3014393 -4.2872238 -4.2678 -4.2456393 -4.2241554 -4.2156005 -4.2156572 -4.2226663 -4.2389841 -4.26089 -4.2749529 -4.2687869 -4.2609878 -4.2573404 -4.2571807][-4.2995481 -4.2878523 -4.2702856 -4.2481251 -4.223062 -4.2075124 -4.2022285 -4.2095876 -4.2310882 -4.2582006 -4.2746973 -4.2649379 -4.2507744 -4.2431474 -4.2414455][-4.2982831 -4.289927 -4.2755876 -4.254818 -4.2278118 -4.2051077 -4.1884136 -4.1878462 -4.2119117 -4.2470522 -4.2722955 -4.2672071 -4.251946 -4.2427583 -4.240644][-4.2983146 -4.294405 -4.2843385 -4.2635412 -4.2331848 -4.1995387 -4.1645255 -4.1475816 -4.1702719 -4.2181621 -4.2577295 -4.264276 -4.2542787 -4.2457976 -4.240314][-4.2996559 -4.2990665 -4.2914343 -4.2672811 -4.2292705 -4.1842065 -4.1323838 -4.0974178 -4.1177168 -4.1790266 -4.2326889 -4.2496591 -4.2471023 -4.2423921 -4.2327266][-4.3017921 -4.3017716 -4.2931027 -4.264257 -4.220221 -4.1681361 -4.1041741 -4.0543041 -4.0715013 -4.1428204 -4.205225 -4.2289133 -4.2320919 -4.2325954 -4.2231069][-4.3052707 -4.3044314 -4.2921438 -4.2595296 -4.2110538 -4.1540961 -4.0810986 -4.0208087 -4.0344086 -4.1115479 -4.1802335 -4.2094226 -4.216855 -4.2212553 -4.2141433][-4.3091078 -4.3064985 -4.2912869 -4.2574906 -4.2099967 -4.1538467 -4.0810556 -4.0143366 -4.0203638 -4.0944037 -4.1640105 -4.199986 -4.2141685 -4.2170162 -4.2060971][-4.3123665 -4.3080544 -4.2920113 -4.2600946 -4.2187004 -4.1714988 -4.1083717 -4.0450187 -4.0393314 -4.0964541 -4.1577082 -4.195507 -4.2129765 -4.2131648 -4.1960249][-4.313777 -4.3082442 -4.2918968 -4.2641373 -4.2299447 -4.1921659 -4.1432943 -4.0923018 -4.0780416 -4.1126738 -4.1576886 -4.1880126 -4.205308 -4.2077703 -4.1904][-4.3142805 -4.3079085 -4.2924976 -4.2696819 -4.2409 -4.2085791 -4.1729207 -4.1347108 -4.1180024 -4.1336412 -4.1632161 -4.185267 -4.1978865 -4.2001357 -4.1857014][-4.3142533 -4.3079467 -4.2958851 -4.2775669 -4.2514529 -4.2208533 -4.194788 -4.1685872 -4.1540813 -4.1589603 -4.1798735 -4.195859 -4.1982121 -4.1946678 -4.1817026][-4.3136163 -4.3090339 -4.302022 -4.2888269 -4.2663503 -4.2385831 -4.217422 -4.1971273 -4.1799374 -4.1788397 -4.195015 -4.2053123 -4.1981964 -4.1865821 -4.1745644][-4.312839 -4.3102345 -4.3074479 -4.2993078 -4.2794609 -4.2532411 -4.2352228 -4.2183948 -4.1976385 -4.19113 -4.2005668 -4.2025294 -4.1854382 -4.1651716 -4.1514845]]...]
INFO - root - 2017-12-05 20:11:43.655895: step 40210, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 68h:11m:53s remains)
INFO - root - 2017-12-05 20:11:52.176656: step 40220, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 69h:53m:37s remains)
INFO - root - 2017-12-05 20:12:00.688847: step 40230, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.853 sec/batch; 69h:16m:31s remains)
INFO - root - 2017-12-05 20:12:09.242354: step 40240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 69h:40m:18s remains)
INFO - root - 2017-12-05 20:12:17.708425: step 40250, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 70h:34m:39s remains)
INFO - root - 2017-12-05 20:12:26.122873: step 40260, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 68h:23m:46s remains)
INFO - root - 2017-12-05 20:12:34.620648: step 40270, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 68h:39m:11s remains)
INFO - root - 2017-12-05 20:12:43.080637: step 40280, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 70h:02m:48s remains)
INFO - root - 2017-12-05 20:12:51.568992: step 40290, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.845 sec/batch; 68h:33m:57s remains)
INFO - root - 2017-12-05 20:13:00.030730: step 40300, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 68h:59m:30s remains)
2017-12-05 20:13:00.822308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2624426 -4.2550211 -4.2337847 -4.2086153 -4.1987591 -4.2077656 -4.2210717 -4.2370477 -4.2576833 -4.2743969 -4.2800612 -4.2804918 -4.2824454 -4.2822013 -4.2752104][-4.2591491 -4.2431812 -4.2059097 -4.1640897 -4.1516771 -4.1744275 -4.2027416 -4.231359 -4.2632604 -4.2896414 -4.3007 -4.2982473 -4.2940235 -4.2868676 -4.2707314][-4.2674809 -4.2446027 -4.1932769 -4.1314197 -4.1115522 -4.1490488 -4.1932077 -4.2314844 -4.2706304 -4.3056307 -4.3228097 -4.3207111 -4.3135023 -4.3026938 -4.2800283][-4.2785516 -4.2524281 -4.1905103 -4.105957 -4.0702238 -4.1190448 -4.1779966 -4.2202048 -4.2627463 -4.3046422 -4.328794 -4.3334103 -4.3301 -4.321579 -4.2998319][-4.2825718 -4.2565126 -4.1901231 -4.0900946 -4.0337543 -4.0809855 -4.1464195 -4.1900439 -4.233974 -4.2804308 -4.3134637 -4.3304892 -4.3362679 -4.334094 -4.3196096][-4.2791934 -4.2555757 -4.1925006 -4.0887475 -4.0113487 -4.0347371 -4.0945983 -4.141305 -4.1916633 -4.2457857 -4.2883711 -4.3171782 -4.3346381 -4.3405433 -4.3347664][-4.277873 -4.2576785 -4.2006764 -4.0985827 -3.9917734 -3.9668274 -4.0110955 -4.0700517 -4.1375456 -4.2046771 -4.2593517 -4.2998271 -4.3272943 -4.339427 -4.3387828][-4.2856197 -4.2679267 -4.21573 -4.1168547 -3.9844854 -3.9048276 -3.9238961 -3.9964733 -4.0868492 -4.1683292 -4.2330904 -4.2825384 -4.317986 -4.3347278 -4.336688][-4.2979994 -4.2852511 -4.2413673 -4.1567135 -4.0257163 -3.9163322 -3.9005756 -3.9662127 -4.0637813 -4.1519818 -4.2182379 -4.2681551 -4.306716 -4.3271289 -4.3310404][-4.310637 -4.3052692 -4.2761588 -4.2168489 -4.1134806 -4.0011983 -3.945961 -3.9783802 -4.0615344 -4.1437755 -4.2054992 -4.2527232 -4.2930694 -4.3181405 -4.3259821][-4.3229451 -4.3240771 -4.3085227 -4.2732387 -4.2019033 -4.1016374 -4.0205536 -4.0092106 -4.0650873 -4.1355171 -4.1918082 -4.2380714 -4.2809849 -4.3124642 -4.324275][-4.332098 -4.3337607 -4.3253765 -4.3062844 -4.2615929 -4.1826196 -4.0949121 -4.0452685 -4.0614715 -4.1151619 -4.1669288 -4.2157068 -4.2653604 -4.3033051 -4.3206086][-4.33234 -4.3307595 -4.3241391 -4.3147178 -4.287395 -4.2297993 -4.1479239 -4.0715737 -4.0484157 -4.0854716 -4.1333561 -4.1852455 -4.2437439 -4.289011 -4.3116775][-4.3212695 -4.3168988 -4.3093019 -4.3047986 -4.2908988 -4.2532234 -4.187448 -4.1052618 -4.0547633 -4.071455 -4.1161094 -4.17068 -4.2320628 -4.2780395 -4.3001337][-4.3041859 -4.2995677 -4.2908731 -4.2883987 -4.284586 -4.2659063 -4.2245665 -4.1610589 -4.1067905 -4.1040163 -4.1397266 -4.1878505 -4.2388716 -4.2722692 -4.28463]]...]
INFO - root - 2017-12-05 20:13:09.310629: step 40310, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:24m:04s remains)
INFO - root - 2017-12-05 20:13:17.776232: step 40320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 68h:32m:37s remains)
INFO - root - 2017-12-05 20:13:26.260381: step 40330, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 68h:17m:09s remains)
INFO - root - 2017-12-05 20:13:34.832467: step 40340, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 68h:21m:48s remains)
INFO - root - 2017-12-05 20:13:43.415922: step 40350, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 67h:07m:08s remains)
INFO - root - 2017-12-05 20:13:51.852395: step 40360, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 69h:53m:35s remains)
INFO - root - 2017-12-05 20:14:00.321355: step 40370, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 69h:22m:49s remains)
INFO - root - 2017-12-05 20:14:08.832510: step 40380, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.866 sec/batch; 70h:14m:57s remains)
INFO - root - 2017-12-05 20:14:17.306605: step 40390, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 72h:07m:18s remains)
INFO - root - 2017-12-05 20:14:25.804789: step 40400, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 69h:54m:49s remains)
2017-12-05 20:14:26.708479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2673955 -4.2721395 -4.2812142 -4.286952 -4.2964082 -4.3022122 -4.3048968 -4.305203 -4.3025823 -4.3014565 -4.303587 -4.3065152 -4.3081789 -4.3087878 -4.3073812][-4.2286491 -4.2220597 -4.2285562 -4.2409153 -4.261209 -4.2765708 -4.2869534 -4.2913294 -4.2884941 -4.285502 -4.28669 -4.2882519 -4.2896309 -4.29099 -4.2901545][-4.191854 -4.1696477 -4.1654725 -4.1794395 -4.2080331 -4.2311068 -4.2470946 -4.2579947 -4.2625451 -4.2670169 -4.2715034 -4.2730055 -4.2728758 -4.2726622 -4.2706337][-4.1733465 -4.1411271 -4.1256671 -4.131391 -4.1531024 -4.1732259 -4.188632 -4.200665 -4.2183366 -4.238945 -4.2538247 -4.2600842 -4.2618856 -4.2609267 -4.2578044][-4.1686935 -4.1356497 -4.1126647 -4.1054788 -4.105597 -4.1056948 -4.1036658 -4.1061745 -4.1381693 -4.1825609 -4.216464 -4.2339425 -4.2447181 -4.2501788 -4.2506285][-4.1689272 -4.1433768 -4.1199837 -4.0995588 -4.0686736 -4.0263662 -3.9759603 -3.9403076 -3.9870749 -4.06896 -4.1341019 -4.1725736 -4.1978731 -4.2137551 -4.2212539][-4.1610622 -4.156076 -4.1437039 -4.1150742 -4.0603747 -3.9776258 -3.8667047 -3.7721117 -3.8243725 -3.937968 -4.0306058 -4.08347 -4.1175919 -4.1402078 -4.1548214][-4.1563616 -4.1759119 -4.18085 -4.1603251 -4.1097217 -4.0278687 -3.9127431 -3.8155801 -3.8439431 -3.9265199 -3.9993916 -4.0353646 -4.0555429 -4.0717144 -4.0855627][-4.1436663 -4.177156 -4.2006683 -4.1984043 -4.1674719 -4.1156216 -4.0413322 -3.9778612 -3.9771967 -4.0068583 -4.0385509 -4.0464253 -4.0416903 -4.0415549 -4.044816][-4.1109962 -4.1442304 -4.1833229 -4.205349 -4.2005706 -4.1779509 -4.1397109 -4.1008439 -4.0867596 -4.0872941 -4.0934429 -4.0866222 -4.06625 -4.0513015 -4.0406876][-4.0857954 -4.1036134 -4.1439929 -4.1820769 -4.1967697 -4.1910005 -4.1758127 -4.15243 -4.1354094 -4.1230726 -4.12165 -4.1196671 -4.1036334 -4.0856476 -4.0639911][-4.0880542 -4.0846329 -4.1114211 -4.1505775 -4.1720715 -4.1757307 -4.177289 -4.168294 -4.15194 -4.1316118 -4.1266522 -4.1307807 -4.1269336 -4.120748 -4.1078591][-4.1199107 -4.1069651 -4.1187592 -4.1426086 -4.1535015 -4.1606841 -4.1813021 -4.1900454 -4.1773458 -4.1534286 -4.1430326 -4.142632 -4.1423745 -4.1508107 -4.1595778][-4.1768656 -4.171906 -4.176209 -4.1824679 -4.1790795 -4.1809359 -4.2023873 -4.2171264 -4.2069945 -4.1833076 -4.17021 -4.1696377 -4.1738124 -4.1885939 -4.204772][-4.2387834 -4.2413559 -4.243762 -4.2402968 -4.2370157 -4.2392192 -4.2510743 -4.2579641 -4.2466836 -4.2263203 -4.2161908 -4.2179694 -4.225472 -4.23956 -4.2526588]]...]
INFO - root - 2017-12-05 20:14:35.114162: step 40410, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 68h:50m:02s remains)
INFO - root - 2017-12-05 20:14:43.687086: step 40420, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 70h:47m:49s remains)
INFO - root - 2017-12-05 20:14:52.213765: step 40430, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 68h:32m:11s remains)
INFO - root - 2017-12-05 20:15:00.588856: step 40440, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 68h:53m:45s remains)
INFO - root - 2017-12-05 20:15:09.066447: step 40450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 68h:23m:27s remains)
INFO - root - 2017-12-05 20:15:17.552346: step 40460, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.834 sec/batch; 67h:38m:55s remains)
INFO - root - 2017-12-05 20:15:26.128541: step 40470, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 71h:28m:48s remains)
INFO - root - 2017-12-05 20:15:34.684385: step 40480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 69h:06m:17s remains)
INFO - root - 2017-12-05 20:15:43.232610: step 40490, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 70h:01m:47s remains)
INFO - root - 2017-12-05 20:15:51.717691: step 40500, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:39m:09s remains)
2017-12-05 20:15:52.508721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9598405 -4.0018759 -4.0527911 -4.1130581 -4.191411 -4.2484932 -4.2889819 -4.3061142 -4.313168 -4.3160233 -4.3040705 -4.2863512 -4.2810245 -4.2776661 -4.2758837][-4.0532765 -4.0838165 -4.1246848 -4.1752648 -4.2388926 -4.2851439 -4.3117409 -4.3162894 -4.3145537 -4.316853 -4.3148413 -4.3130622 -4.3190031 -4.3204775 -4.3191519][-4.1430359 -4.1594625 -4.1903844 -4.2295856 -4.2722011 -4.2986121 -4.3054094 -4.3011532 -4.2955103 -4.297893 -4.3043604 -4.3175268 -4.3303366 -4.3335981 -4.332448][-4.2232895 -4.2316022 -4.2520761 -4.2738557 -4.285675 -4.28356 -4.2667127 -4.2514362 -4.2452621 -4.2520905 -4.2669077 -4.2915025 -4.3116736 -4.3183494 -4.3196611][-4.27635 -4.2798848 -4.2889137 -4.2845192 -4.2615089 -4.2259555 -4.1825886 -4.1556811 -4.1582184 -4.1768579 -4.2052317 -4.2411213 -4.2727051 -4.2887206 -4.2959661][-4.29006 -4.2849822 -4.2759953 -4.2427354 -4.1841049 -4.1147614 -4.0504417 -4.0234504 -4.04929 -4.0912461 -4.1397729 -4.1877789 -4.2297554 -4.2534285 -4.2668343][-4.2699695 -4.2481756 -4.2148 -4.1566124 -4.0693884 -3.9702413 -3.8867495 -3.8750391 -3.9324968 -3.9944506 -4.0589929 -4.1192079 -4.1698318 -4.2017412 -4.221622][-4.2339826 -4.1906314 -4.1326942 -4.0582771 -3.9586754 -3.8495159 -3.7746329 -3.8029547 -3.8837714 -3.9514589 -4.0178709 -4.0823326 -4.1370044 -4.173418 -4.1944938][-4.2138662 -4.1567197 -4.0882764 -4.0154867 -3.9308887 -3.8477955 -3.81529 -3.8681755 -3.9390903 -3.9928648 -4.0512753 -4.1089678 -4.1576095 -4.1898832 -4.2082939][-4.2183156 -4.1662908 -4.1130567 -4.0673313 -4.0231943 -3.9858129 -3.982271 -4.020947 -4.058641 -4.0885177 -4.1290612 -4.1679873 -4.2000194 -4.2231903 -4.2390227][-4.2399673 -4.2035985 -4.1739345 -4.1561718 -4.1446934 -4.1373568 -4.1429825 -4.1615639 -4.1738887 -4.1900105 -4.217782 -4.2411375 -4.2590003 -4.2713776 -4.2808232][-4.2776241 -4.2559853 -4.2403145 -4.23468 -4.2361574 -4.2407346 -4.2483735 -4.254951 -4.2586627 -4.2693868 -4.287838 -4.3007779 -4.3117533 -4.319 -4.3240671][-4.306108 -4.2956386 -4.2890272 -4.2890172 -4.2940774 -4.3009233 -4.3066177 -4.3087516 -4.3100333 -4.3166585 -4.3272491 -4.3349447 -4.3406715 -4.3438191 -4.3446903][-4.3159761 -4.3110952 -4.3088641 -4.309876 -4.3145008 -4.3206992 -4.3266249 -4.3285313 -4.328639 -4.3301363 -4.3342209 -4.3370833 -4.3381863 -4.3369379 -4.3343878][-4.3160691 -4.3129234 -4.311007 -4.3105879 -4.3122263 -4.3151755 -4.3189621 -4.3209791 -4.3212657 -4.3207335 -4.3211083 -4.3214879 -4.3216958 -4.3198762 -4.3171587]]...]
INFO - root - 2017-12-05 20:16:00.995186: step 40510, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 69h:21m:21s remains)
INFO - root - 2017-12-05 20:16:09.510840: step 40520, loss = 2.07, batch loss = 2.02 (9.0 examples/sec; 0.893 sec/batch; 72h:24m:16s remains)
INFO - root - 2017-12-05 20:16:18.086608: step 40530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 70h:26m:00s remains)
INFO - root - 2017-12-05 20:16:26.665568: step 40540, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 68h:55m:29s remains)
INFO - root - 2017-12-05 20:16:35.143563: step 40550, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 68h:24m:49s remains)
INFO - root - 2017-12-05 20:16:43.554580: step 40560, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 67h:26m:39s remains)
INFO - root - 2017-12-05 20:16:52.118997: step 40570, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 68h:22m:49s remains)
INFO - root - 2017-12-05 20:17:00.511790: step 40580, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 66h:47m:08s remains)
INFO - root - 2017-12-05 20:17:09.021387: step 40590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 70h:06m:14s remains)
INFO - root - 2017-12-05 20:17:17.547814: step 40600, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 71h:00m:44s remains)
2017-12-05 20:17:18.338666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.319808 -4.3172112 -4.3098507 -4.3019886 -4.2997761 -4.3010278 -4.3054652 -4.3122873 -4.3130593 -4.3028908 -4.28427 -4.2664418 -4.24702 -4.2287536 -4.2222834][-4.3118243 -4.3044782 -4.2931352 -4.283761 -4.284059 -4.2882676 -4.2961211 -4.3064504 -4.3103304 -4.3021445 -4.2817721 -4.2592559 -4.2359357 -4.2199759 -4.222363][-4.3023639 -4.2895961 -4.2724562 -4.2581849 -4.2561674 -4.2588272 -4.2656541 -4.2761607 -4.2847433 -4.2862916 -4.2767849 -4.2633448 -4.2475886 -4.2390289 -4.2459979][-4.291091 -4.2737823 -4.2515564 -4.2313995 -4.2198544 -4.2111993 -4.2089839 -4.2167969 -4.2329936 -4.2525082 -4.2660294 -4.2743969 -4.2746797 -4.2735271 -4.2785916][-4.2792873 -4.2602234 -4.2349062 -4.2071786 -4.1782613 -4.1469889 -4.1260109 -4.125072 -4.1483617 -4.1901636 -4.233633 -4.2705364 -4.289392 -4.2949963 -4.2956915][-4.269352 -4.2524924 -4.2261133 -4.1919723 -4.1452765 -4.0864592 -4.0359559 -4.0149837 -4.0428295 -4.1086674 -4.1821189 -4.2459774 -4.2833886 -4.2953682 -4.2921247][-4.257885 -4.2427683 -4.2160645 -4.1804776 -4.1241126 -4.0437813 -3.9618368 -3.9165888 -3.9466155 -4.0313225 -4.12686 -4.2105832 -4.2621875 -4.2778087 -4.2704554][-4.2514143 -4.2361555 -4.210588 -4.1815467 -4.127223 -4.0378313 -3.9346993 -3.8699119 -3.8981383 -3.9869683 -4.087893 -4.1795044 -4.2371941 -4.2543411 -4.2445478][-4.2545915 -4.2388368 -4.2173266 -4.1992116 -4.1563787 -4.0748825 -3.9733799 -3.907599 -3.9253933 -3.9976239 -4.0848055 -4.1695471 -4.2255497 -4.2421303 -4.2317185][-4.2679191 -4.2599053 -4.2490511 -4.243042 -4.2155228 -4.1526675 -4.0692158 -4.0102582 -4.011138 -4.0585141 -4.1223683 -4.187489 -4.2356033 -4.2527537 -4.24753][-4.2833667 -4.2897058 -4.2906165 -4.2917504 -4.2741289 -4.2236624 -4.1539221 -4.0990624 -4.0857339 -4.1129112 -4.1576228 -4.207623 -4.2522397 -4.2752995 -4.2807617][-4.2821608 -4.2966948 -4.3042579 -4.3106441 -4.299891 -4.2567663 -4.1925249 -4.1364369 -4.1148787 -4.1337957 -4.172091 -4.2165456 -4.2612104 -4.2881112 -4.3017879][-4.2656388 -4.2801237 -4.2885227 -4.2978406 -4.2929235 -4.2557459 -4.1980629 -4.1478729 -4.1293192 -4.1492753 -4.1843376 -4.2235546 -4.2620482 -4.2847824 -4.2979336][-4.2547488 -4.2621779 -4.2643828 -4.2733755 -4.2742343 -4.2496209 -4.2113953 -4.1816874 -4.1752543 -4.1947832 -4.2191954 -4.2427316 -4.2617559 -4.2685566 -4.2728148][-4.2598557 -4.2550578 -4.2498369 -4.2591305 -4.2684774 -4.2638745 -4.2547035 -4.2508869 -4.2545853 -4.2639346 -4.268259 -4.2674494 -4.2615061 -4.249815 -4.2443748]]...]
INFO - root - 2017-12-05 20:17:26.894041: step 40610, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:42m:28s remains)
INFO - root - 2017-12-05 20:17:35.405250: step 40620, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 67h:21m:50s remains)
INFO - root - 2017-12-05 20:17:43.930504: step 40630, loss = 2.11, batch loss = 2.05 (9.6 examples/sec; 0.837 sec/batch; 67h:50m:35s remains)
INFO - root - 2017-12-05 20:17:52.442787: step 40640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 70h:17m:47s remains)
INFO - root - 2017-12-05 20:18:00.905449: step 40650, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 67h:50m:45s remains)
INFO - root - 2017-12-05 20:18:09.246594: step 40660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 68h:30m:46s remains)
INFO - root - 2017-12-05 20:18:17.743130: step 40670, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:42m:52s remains)
INFO - root - 2017-12-05 20:18:26.256724: step 40680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:43m:20s remains)
INFO - root - 2017-12-05 20:18:34.733552: step 40690, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 67h:31m:27s remains)
INFO - root - 2017-12-05 20:18:43.374650: step 40700, loss = 2.06, batch loss = 2.00 (7.6 examples/sec; 1.052 sec/batch; 85h:18m:38s remains)
2017-12-05 20:18:44.145093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1797466 -4.18782 -4.203712 -4.2166147 -4.2204833 -4.2036996 -4.1744776 -4.1493573 -4.147079 -4.170887 -4.2056084 -4.2461734 -4.2816892 -4.3033071 -4.3175173][-4.1609583 -4.1693735 -4.1829166 -4.1956768 -4.2025266 -4.1890855 -4.1593823 -4.1325731 -4.1318541 -4.1628275 -4.2035103 -4.2491417 -4.2923007 -4.3172083 -4.3274732][-4.1622663 -4.1718464 -4.1803651 -4.188818 -4.1933932 -4.1798139 -4.1505833 -4.1254592 -4.1304455 -4.1645675 -4.2076077 -4.2555566 -4.3002987 -4.3256383 -4.3338223][-4.1772494 -4.1851516 -4.1886463 -4.1910844 -4.1879072 -4.1674094 -4.138299 -4.1195664 -4.13197 -4.1737046 -4.2212586 -4.2663679 -4.3047309 -4.3271236 -4.3362775][-4.2021632 -4.2047944 -4.1993179 -4.1903496 -4.1740732 -4.1437531 -4.1109242 -4.0966525 -4.1208644 -4.1753016 -4.2274828 -4.2679605 -4.3002963 -4.3227787 -4.3358316][-4.2330813 -4.2280555 -4.2095294 -4.1840992 -4.1502819 -4.1098251 -4.0708108 -4.0554757 -4.0916739 -4.1623368 -4.2228165 -4.263166 -4.2956047 -4.3206034 -4.3366294][-4.25102 -4.2389517 -4.2087574 -4.1710739 -4.1267543 -4.0803318 -4.0320687 -4.0114384 -4.0576415 -4.1460881 -4.2186489 -4.2622447 -4.2972007 -4.3232665 -4.3387775][-4.2545447 -4.2389565 -4.2062674 -4.1662588 -4.1187081 -4.0648494 -4.0036063 -3.975553 -4.0314779 -4.1350236 -4.2178526 -4.2669559 -4.3038125 -4.328382 -4.34127][-4.2443786 -4.2294712 -4.2000155 -4.165597 -4.1223826 -4.0684333 -4.00158 -3.971015 -4.0329709 -4.139576 -4.2256522 -4.2783408 -4.315124 -4.3351216 -4.3443513][-4.2317371 -4.2179732 -4.1930165 -4.1683164 -4.1376753 -4.094059 -4.0368128 -4.0098152 -4.0621471 -4.1547484 -4.2352896 -4.2872591 -4.3210444 -4.337522 -4.3461514][-4.2155375 -4.2029986 -4.1822548 -4.1660285 -4.1484823 -4.1188316 -4.0768929 -4.056447 -4.0959783 -4.1700258 -4.2408862 -4.2898784 -4.3211155 -4.3371615 -4.34758][-4.1984429 -4.1865382 -4.1697369 -4.1589966 -4.1500726 -4.1329422 -4.1042981 -4.0892315 -4.1183205 -4.1777158 -4.2390141 -4.2859783 -4.3177743 -4.3356318 -4.3483][-4.1825976 -4.1732516 -4.1601505 -4.1529636 -4.149436 -4.1382084 -4.1146173 -4.0999017 -4.1239858 -4.1756949 -4.2306914 -4.2767277 -4.3109822 -4.3319693 -4.3468184][-4.1659303 -4.1638651 -4.1579976 -4.1529226 -4.149466 -4.1384134 -4.1152377 -4.0997405 -4.1215448 -4.170815 -4.2220411 -4.2672143 -4.3030591 -4.3275456 -4.344852][-4.1596489 -4.1644931 -4.1664414 -4.1645789 -4.1581125 -4.1440063 -4.1203737 -4.10506 -4.1248159 -4.1719222 -4.2219062 -4.2658906 -4.3009262 -4.3262587 -4.3444066]]...]
INFO - root - 2017-12-05 20:18:52.546204: step 40710, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 60h:27m:40s remains)
INFO - root - 2017-12-05 20:19:01.202362: step 40720, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:27m:12s remains)
INFO - root - 2017-12-05 20:19:09.700064: step 40730, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 69h:35m:36s remains)
INFO - root - 2017-12-05 20:19:18.149748: step 40740, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 67h:49m:25s remains)
INFO - root - 2017-12-05 20:19:26.691675: step 40750, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 68h:40m:00s remains)
INFO - root - 2017-12-05 20:19:35.080377: step 40760, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.826 sec/batch; 66h:54m:12s remains)
INFO - root - 2017-12-05 20:19:43.570142: step 40770, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 67h:23m:14s remains)
INFO - root - 2017-12-05 20:19:52.099311: step 40780, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 70h:51m:12s remains)
INFO - root - 2017-12-05 20:20:00.629623: step 40790, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 70h:27m:28s remains)
INFO - root - 2017-12-05 20:20:09.039675: step 40800, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.758 sec/batch; 61h:23m:22s remains)
2017-12-05 20:20:09.851801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.291491 -4.29016 -4.2887535 -4.2915239 -4.2951627 -4.2907615 -4.278584 -4.27036 -4.2715588 -4.2769942 -4.2907772 -4.3056407 -4.3165512 -4.3266745 -4.3350978][-4.266027 -4.2581306 -4.2512536 -4.2544217 -4.2627726 -4.2553997 -4.23292 -4.2189093 -4.2233047 -4.2352529 -4.258184 -4.2820807 -4.3000908 -4.3161082 -4.3288007][-4.2413459 -4.2261224 -4.2132635 -4.2205696 -4.2357926 -4.2185111 -4.1753373 -4.1516004 -4.1626325 -4.1862507 -4.2188063 -4.2527609 -4.2793574 -4.3014388 -4.3195891][-4.2262526 -4.2029519 -4.1796303 -4.1895704 -4.2057095 -4.1678991 -4.0967484 -4.0642319 -4.0902634 -4.1334577 -4.178021 -4.2254391 -4.2614965 -4.2888069 -4.3104196][-4.1875682 -4.1546278 -4.1211333 -4.1298633 -4.1400895 -4.0791526 -3.9773886 -3.9412162 -3.9932144 -4.063323 -4.1246042 -4.1910048 -4.2419038 -4.2767148 -4.3027744][-4.1253014 -4.0857515 -4.0437608 -4.0470595 -4.0458207 -3.9671073 -3.8421869 -3.8090861 -3.8942208 -3.9926374 -4.0698452 -4.1514478 -4.2186103 -4.2632217 -4.296207][-4.0826459 -4.035213 -3.9874427 -3.9850149 -3.9718037 -3.875149 -3.7342544 -3.7095432 -3.8221653 -3.9375093 -4.024456 -4.1173515 -4.1981654 -4.2522779 -4.2900543][-4.080862 -4.0347748 -3.9986649 -3.9982247 -3.9805043 -3.8901346 -3.7658081 -3.7398319 -3.8391395 -3.9466109 -4.0283527 -4.1153831 -4.1944728 -4.2489324 -4.2861629][-4.1047878 -4.0737834 -4.0549741 -4.0637865 -4.0573635 -3.9965935 -3.9081366 -3.876832 -3.9371858 -4.0160508 -4.0792513 -4.1456332 -4.2096786 -4.2549086 -4.2872143][-4.1589646 -4.1410851 -4.1338353 -4.145184 -4.1478391 -4.1107345 -4.0498257 -4.0165234 -4.0462093 -4.0974522 -4.1425829 -4.1923685 -4.2408905 -4.2723904 -4.2952595][-4.2003217 -4.1900277 -4.1899066 -4.2010303 -4.2073455 -4.187561 -4.1493425 -4.1241174 -4.1401134 -4.1751428 -4.2083607 -4.2432027 -4.2754822 -4.2942252 -4.3074875][-4.2384157 -4.23507 -4.2387877 -4.2485642 -4.2565732 -4.247056 -4.2253542 -4.2110558 -4.2206864 -4.2441344 -4.2659082 -4.28696 -4.3042769 -4.3125677 -4.3185282][-4.282805 -4.2855167 -4.2924051 -4.2973709 -4.3018408 -4.2972183 -4.285634 -4.281024 -4.2896223 -4.3057652 -4.3192205 -4.3280411 -4.331327 -4.3296075 -4.3289752][-4.3104281 -4.3156123 -4.3236275 -4.3261032 -4.3254323 -4.3214388 -4.3141475 -4.3128681 -4.3208652 -4.3342872 -4.3450122 -4.3483047 -4.3450165 -4.3390903 -4.3351493][-4.3237181 -4.3268814 -4.3335123 -4.3352218 -4.3347321 -4.3317637 -4.3255515 -4.3225064 -4.327939 -4.3387518 -4.3464093 -4.3474321 -4.3440604 -4.33971 -4.3364005]]...]
INFO - root - 2017-12-05 20:20:18.352699: step 40810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 68h:49m:13s remains)
INFO - root - 2017-12-05 20:20:26.780892: step 40820, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.761 sec/batch; 61h:40m:55s remains)
INFO - root - 2017-12-05 20:20:35.412792: step 40830, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 70h:18m:44s remains)
INFO - root - 2017-12-05 20:20:43.922664: step 40840, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 70h:30m:07s remains)
INFO - root - 2017-12-05 20:20:52.469274: step 40850, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 69h:21m:03s remains)
INFO - root - 2017-12-05 20:21:01.007482: step 40860, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 69h:17m:38s remains)
INFO - root - 2017-12-05 20:21:09.576256: step 40870, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 70h:56m:04s remains)
INFO - root - 2017-12-05 20:21:18.129596: step 40880, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 69h:23m:12s remains)
INFO - root - 2017-12-05 20:21:26.649169: step 40890, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:32m:47s remains)
INFO - root - 2017-12-05 20:21:35.114414: step 40900, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 68h:25m:16s remains)
2017-12-05 20:21:35.878795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3082104 -4.3070221 -4.3045154 -4.2983136 -4.2904429 -4.2857842 -4.2811236 -4.2733622 -4.25594 -4.2391229 -4.226954 -4.2204 -4.2306795 -4.257226 -4.2908106][-4.3037949 -4.2992043 -4.2942653 -4.2856193 -4.2763214 -4.2704425 -4.2678905 -4.2554317 -4.2340093 -4.2136283 -4.1973929 -4.1932683 -4.2112 -4.2459674 -4.2881927][-4.2987437 -4.2890086 -4.280611 -4.2725258 -4.267324 -4.2619534 -4.2545638 -4.2346292 -4.2094646 -4.1848416 -4.1639433 -4.1646442 -4.1905704 -4.2363915 -4.284729][-4.2941103 -4.2809591 -4.2704515 -4.2661057 -4.2670593 -4.2582111 -4.2377324 -4.210834 -4.1809154 -4.1539021 -4.1322646 -4.1384082 -4.1734605 -4.2274542 -4.2799115][-4.2937579 -4.2802534 -4.2668505 -4.2609959 -4.2609038 -4.2412281 -4.2002125 -4.1679583 -4.1403627 -4.1192279 -4.1076407 -4.1257253 -4.1672392 -4.2243576 -4.2807837][-4.28944 -4.2733474 -4.2548227 -4.2425933 -4.2324653 -4.1927738 -4.1254411 -4.0860434 -4.0763497 -4.0761647 -4.0858917 -4.1218209 -4.1676831 -4.2184739 -4.2692871][-4.2748218 -4.2533669 -4.2296014 -4.2079554 -4.1781578 -4.106781 -4.0017276 -3.95159 -3.9905236 -4.0409336 -4.0798774 -4.1283445 -4.1684947 -4.2055902 -4.2413626][-4.2409644 -4.2121344 -4.1848741 -4.1558194 -4.1102772 -4.0102386 -3.8555939 -3.7757609 -3.8661306 -3.9817536 -4.0556064 -4.1139088 -4.1455765 -4.1693115 -4.1958556][-4.2121358 -4.1802568 -4.1505141 -4.1212678 -4.0784287 -3.9832268 -3.8259046 -3.7401693 -3.8495681 -3.9842167 -4.0617738 -4.112638 -4.1327181 -4.1383014 -4.1551857][-4.2083831 -4.1791644 -4.1549692 -4.1359763 -4.1129556 -4.0492277 -3.9370372 -3.8735538 -3.9523108 -4.0534587 -4.1088042 -4.1404157 -4.1457496 -4.1347175 -4.1452131][-4.2077632 -4.1833115 -4.1681428 -4.1605062 -4.1542845 -4.1230521 -4.0572262 -4.0169859 -4.0631618 -4.1253057 -4.1567636 -4.1715279 -4.1682258 -4.151896 -4.1573935][-4.2047038 -4.1830754 -4.1746807 -4.1756477 -4.18136 -4.1737866 -4.1390667 -4.114068 -4.1416063 -4.1837215 -4.2037344 -4.2089105 -4.1985245 -4.1823382 -4.17966][-4.2118244 -4.1872668 -4.1734676 -4.1723719 -4.1818786 -4.1868644 -4.172471 -4.1611729 -4.1820874 -4.2123089 -4.2281876 -4.2286229 -4.2196922 -4.2081227 -4.1998277][-4.2448077 -4.2172942 -4.1969643 -4.1915278 -4.200582 -4.2100134 -4.2094784 -4.2069821 -4.220819 -4.2397232 -4.2487593 -4.2496548 -4.2486081 -4.243175 -4.2339783][-4.2886543 -4.2644448 -4.2443042 -4.2392087 -4.2459989 -4.2552452 -4.2626786 -4.2655163 -4.2727833 -4.2807603 -4.2851915 -4.2874408 -4.2903085 -4.2911668 -4.2852168]]...]
INFO - root - 2017-12-05 20:21:44.266117: step 40910, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 69h:15m:23s remains)
INFO - root - 2017-12-05 20:21:52.814923: step 40920, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 69h:45m:23s remains)
INFO - root - 2017-12-05 20:22:01.323789: step 40930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 69h:05m:30s remains)
INFO - root - 2017-12-05 20:22:09.832164: step 40940, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 68h:43m:23s remains)
INFO - root - 2017-12-05 20:22:18.339000: step 40950, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:25m:01s remains)
INFO - root - 2017-12-05 20:22:26.720861: step 40960, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 67h:00m:31s remains)
INFO - root - 2017-12-05 20:22:35.253340: step 40970, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:39m:59s remains)
INFO - root - 2017-12-05 20:22:43.869968: step 40980, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 70h:59m:28s remains)
INFO - root - 2017-12-05 20:22:52.444137: step 40990, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 72h:06m:44s remains)
INFO - root - 2017-12-05 20:23:00.915699: step 41000, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 69h:00m:04s remains)
2017-12-05 20:23:01.766632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1729589 -4.1563125 -4.1610336 -4.1813407 -4.1880736 -4.1731339 -4.1415424 -4.09286 -4.0762715 -4.1220007 -4.1893945 -4.24601 -4.2877121 -4.3200521 -4.3421841][-4.1817303 -4.1558905 -4.1485085 -4.1642885 -4.1675334 -4.147459 -4.1127896 -4.0660768 -4.0614643 -4.1215081 -4.1915088 -4.2466836 -4.2878981 -4.3220711 -4.3430281][-4.1818542 -4.1576867 -4.1481667 -4.1559377 -4.1511679 -4.1232185 -4.0809412 -4.0335722 -4.0391984 -4.1118293 -4.1885338 -4.2472444 -4.2916842 -4.3268113 -4.3440657][-4.2006803 -4.1805205 -4.1718049 -4.167119 -4.1472239 -4.105022 -4.0466819 -3.9893503 -4.0049934 -4.0959654 -4.1826997 -4.2485228 -4.2980132 -4.3313808 -4.3451114][-4.2353492 -4.220366 -4.2099929 -4.1883736 -4.1506643 -4.0918603 -4.0138373 -3.94974 -3.975239 -4.0863829 -4.179914 -4.2509 -4.3038626 -4.334166 -4.3451867][-4.2546544 -4.24334 -4.23034 -4.1988759 -4.1515837 -4.0835786 -3.9875171 -3.9157193 -3.95142 -4.0801616 -4.1791382 -4.2534595 -4.3092279 -4.3380685 -4.3444285][-4.2571673 -4.2475672 -4.2321925 -4.194478 -4.1436386 -4.0728469 -3.9705174 -3.8944454 -3.9333639 -4.07065 -4.1754074 -4.2574277 -4.3157544 -4.3412766 -4.3429041][-4.26215 -4.2467341 -4.22657 -4.1855145 -4.1309023 -4.0600877 -3.9674976 -3.9090433 -3.9556198 -4.082684 -4.1824651 -4.2650609 -4.321054 -4.3407383 -4.34095][-4.2676735 -4.24735 -4.224411 -4.18602 -4.1302652 -4.0593896 -3.9812574 -3.9479485 -4.0005822 -4.1053991 -4.1908917 -4.2642913 -4.3171492 -4.3357725 -4.3385425][-4.2696996 -4.2510686 -4.230557 -4.2000341 -4.1496625 -4.080184 -4.0103383 -3.9889736 -4.0386491 -4.1250072 -4.199657 -4.2631369 -4.3109865 -4.3300304 -4.336412][-4.2654696 -4.251164 -4.2342963 -4.2127967 -4.1722984 -4.1093431 -4.045651 -4.0232878 -4.0625644 -4.1387386 -4.2098565 -4.26744 -4.3074217 -4.3259096 -4.3360429][-4.263555 -4.256084 -4.2429819 -4.2246323 -4.1884413 -4.1305547 -4.0703821 -4.0381894 -4.0640111 -4.1364822 -4.2126389 -4.2721057 -4.3061919 -4.3224912 -4.3365588][-4.2659888 -4.2719989 -4.2648029 -4.2462363 -4.2083097 -4.1508594 -4.0893607 -4.042944 -4.05603 -4.1277394 -4.209887 -4.2720251 -4.3039603 -4.3199015 -4.3365][-4.2673917 -4.2882686 -4.2920108 -4.2802873 -4.2475839 -4.1930652 -4.1310644 -4.0758924 -4.0748844 -4.1359 -4.2130013 -4.2738461 -4.3055 -4.3221178 -4.3389444][-4.2566781 -4.286664 -4.3005452 -4.2971334 -4.2731352 -4.2262058 -4.1703515 -4.1146502 -4.1068826 -4.1554084 -4.2220311 -4.2791257 -4.3103929 -4.3278766 -4.3430705]]...]
INFO - root - 2017-12-05 20:23:10.370625: step 41010, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 72h:04m:10s remains)
INFO - root - 2017-12-05 20:23:18.777346: step 41020, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 70h:11m:00s remains)
INFO - root - 2017-12-05 20:23:27.428600: step 41030, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 70h:07m:44s remains)
INFO - root - 2017-12-05 20:23:35.950648: step 41040, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 68h:34m:48s remains)
INFO - root - 2017-12-05 20:23:44.454187: step 41050, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 67h:28m:41s remains)
INFO - root - 2017-12-05 20:23:52.885682: step 41060, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 68h:40m:13s remains)
INFO - root - 2017-12-05 20:24:01.336207: step 41070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:37m:10s remains)
INFO - root - 2017-12-05 20:24:09.914208: step 41080, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 70h:41m:41s remains)
INFO - root - 2017-12-05 20:24:18.389815: step 41090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:37m:57s remains)
INFO - root - 2017-12-05 20:24:26.916074: step 41100, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 68h:29m:51s remains)
2017-12-05 20:24:27.736360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1907191 -4.1751933 -4.1926436 -4.219305 -4.2341733 -4.2372875 -4.2322474 -4.2221894 -4.212224 -4.2056289 -4.200902 -4.2013488 -4.2186575 -4.2493124 -4.2749529][-4.1747475 -4.1611729 -4.1779675 -4.2014012 -4.2113008 -4.2125173 -4.2090492 -4.2035761 -4.1999393 -4.1976295 -4.1929193 -4.1917396 -4.2098541 -4.2407823 -4.2682624][-4.1814036 -4.1686144 -4.1779809 -4.1890774 -4.1900444 -4.1828771 -4.1737204 -4.1686783 -4.1730094 -4.1829615 -4.1915255 -4.1999993 -4.2199416 -4.2479849 -4.2731895][-4.2122469 -4.2000394 -4.1948066 -4.1858377 -4.1680665 -4.1420312 -4.1162567 -4.1086068 -4.1228914 -4.15503 -4.1907606 -4.2190061 -4.2430649 -4.2681389 -4.2902393][-4.2561626 -4.2425861 -4.22088 -4.18843 -4.1387749 -4.0752916 -4.0180888 -4.0124397 -4.051188 -4.1144075 -4.1810975 -4.2321634 -4.2634754 -4.28849 -4.3072467][-4.2877164 -4.2735028 -4.2395334 -4.182817 -4.0898166 -3.9731147 -3.8796563 -3.8949754 -3.982085 -4.0839715 -4.1716471 -4.2362466 -4.2766557 -4.304966 -4.321311][-4.2949843 -4.2837224 -4.2436829 -4.1671634 -4.0387821 -3.8789616 -3.7669933 -3.8217041 -3.9620039 -4.0936313 -4.185214 -4.2509184 -4.2946844 -4.3211017 -4.3292651][-4.2920628 -4.2847075 -4.239408 -4.1524405 -4.0140767 -3.8599677 -3.7837231 -3.8712459 -4.01702 -4.141396 -4.2215471 -4.2774186 -4.312645 -4.3261437 -4.3179936][-4.2827234 -4.2835412 -4.2418022 -4.158792 -4.0376663 -3.9297266 -3.9130232 -4.0020385 -4.1132531 -4.2055392 -4.2643986 -4.3002391 -4.3161197 -4.3107796 -4.28499][-4.2704182 -4.2853885 -4.2593422 -4.1975384 -4.1137519 -4.0552568 -4.0702467 -4.1359005 -4.2045741 -4.2630939 -4.2965751 -4.3081374 -4.3019714 -4.2827 -4.2525668][-4.268383 -4.2955956 -4.286468 -4.2500558 -4.2049041 -4.1795988 -4.197257 -4.2360935 -4.2717862 -4.299212 -4.306139 -4.2980294 -4.2814341 -4.2643504 -4.2431812][-4.2819057 -4.3088288 -4.3066406 -4.2871332 -4.2668653 -4.2582049 -4.2714663 -4.2937889 -4.3078976 -4.3089809 -4.2961383 -4.2816758 -4.2691283 -4.259613 -4.2447882][-4.30069 -4.3150454 -4.3099937 -4.2954164 -4.2847018 -4.2820282 -4.2919865 -4.3046718 -4.306983 -4.2947569 -4.2771325 -4.2678142 -4.2652907 -4.2610803 -4.2474766][-4.3126383 -4.3123903 -4.2979622 -4.2815194 -4.27377 -4.2718515 -4.274961 -4.2752566 -4.2715311 -4.2591491 -4.2488718 -4.2498469 -4.2544522 -4.2517142 -4.2401781][-4.3159847 -4.3073168 -4.28846 -4.2700038 -4.2609529 -4.2523289 -4.242569 -4.2305856 -4.22272 -4.2137189 -4.2123733 -4.2215953 -4.2320695 -4.2319803 -4.2226439]]...]
INFO - root - 2017-12-05 20:24:36.315963: step 41110, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.847 sec/batch; 68h:32m:51s remains)
INFO - root - 2017-12-05 20:24:44.944266: step 41120, loss = 2.06, batch loss = 2.01 (8.4 examples/sec; 0.954 sec/batch; 77h:13m:41s remains)
INFO - root - 2017-12-05 20:24:53.444353: step 41130, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 67h:36m:30s remains)
INFO - root - 2017-12-05 20:25:01.906959: step 41140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 68h:55m:26s remains)
INFO - root - 2017-12-05 20:25:10.281038: step 41150, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 67h:01m:55s remains)
INFO - root - 2017-12-05 20:25:18.709065: step 41160, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 69h:49m:35s remains)
INFO - root - 2017-12-05 20:25:27.237485: step 41170, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 69h:09m:33s remains)
INFO - root - 2017-12-05 20:25:35.858174: step 41180, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 70h:51m:28s remains)
INFO - root - 2017-12-05 20:25:44.341173: step 41190, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.856 sec/batch; 69h:14m:06s remains)
INFO - root - 2017-12-05 20:25:52.884357: step 41200, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 65h:56m:53s remains)
2017-12-05 20:25:53.676450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2496328 -4.2406034 -4.2385316 -4.2423768 -4.2510524 -4.2622728 -4.2756596 -4.2902617 -4.3033686 -4.314548 -4.3247852 -4.3324542 -4.3364778 -4.3380032 -4.3382611][-4.2579479 -4.244554 -4.2391014 -4.2406263 -4.2479963 -4.2588353 -4.2722025 -4.2873225 -4.3009648 -4.3125634 -4.3233318 -4.331687 -4.3363843 -4.3383207 -4.338728][-4.2943974 -4.2822595 -4.2761831 -4.27548 -4.2789974 -4.2844353 -4.2920465 -4.3018708 -4.3107443 -4.3186235 -4.3267312 -4.3335452 -4.3374391 -4.3390136 -4.33922][-4.3335619 -4.3249736 -4.3187852 -4.3146214 -4.3118777 -4.3090076 -4.3079576 -4.3105559 -4.3143239 -4.3194938 -4.3262086 -4.3327446 -4.3371849 -4.339345 -4.3400097][-4.3482814 -4.3414054 -4.3339181 -4.3258448 -4.3158913 -4.3042626 -4.2958584 -4.2946253 -4.2973533 -4.3039641 -4.3134856 -4.3232412 -4.3312554 -4.3363023 -4.3391466][-4.331356 -4.3232965 -4.3135653 -4.3002725 -4.2815728 -4.2614112 -4.2475648 -4.2449102 -4.2499733 -4.2631574 -4.2815795 -4.299994 -4.3149719 -4.3258038 -4.3329368][-4.2862315 -4.2760453 -4.2618818 -4.2414069 -4.2124748 -4.1830087 -4.1636043 -4.1606569 -4.1696777 -4.1913404 -4.221879 -4.2529912 -4.2792926 -4.2996507 -4.3142805][-4.2267008 -4.2189341 -4.2030644 -4.1758857 -4.1361456 -4.0941691 -4.0649815 -4.0583787 -4.0698438 -4.0989032 -4.1410747 -4.1863723 -4.2271948 -4.259831 -4.2836752][-4.1610241 -4.1640964 -4.1578445 -4.1351428 -4.0951591 -4.0493088 -4.0132079 -4.0005922 -4.0089774 -4.0373874 -4.0819 -4.1330538 -4.1815019 -4.2211332 -4.2504077][-4.1181393 -4.1343431 -4.1445146 -4.1379046 -4.1144962 -4.0838432 -4.055882 -4.0423765 -4.0435209 -4.0585647 -4.08669 -4.1246276 -4.1645107 -4.1991634 -4.2261257][-4.1327696 -4.15482 -4.1755185 -4.1841474 -4.1811242 -4.1718564 -4.1596961 -4.1510735 -4.1464658 -4.1463137 -4.151135 -4.1640687 -4.1824841 -4.2011733 -4.2182255][-4.1911149 -4.2116022 -4.2338085 -4.2507005 -4.2619352 -4.2686362 -4.2690744 -4.2655282 -4.2582922 -4.2476306 -4.2356625 -4.2278771 -4.2262211 -4.2284632 -4.2336135][-4.2528796 -4.2679853 -4.2867279 -4.3044577 -4.3197212 -4.3315983 -4.3377986 -4.3377466 -4.3318248 -4.3202205 -4.3050523 -4.2910595 -4.2803235 -4.2732849 -4.2700195][-4.292377 -4.3001485 -4.3127737 -4.3262143 -4.3390064 -4.3499022 -4.3569903 -4.3594384 -4.3577967 -4.3521533 -4.3430419 -4.3333182 -4.3241339 -4.3161268 -4.3101993][-4.3141646 -4.3158197 -4.3225784 -4.3311424 -4.340239 -4.3484941 -4.3548961 -4.3587589 -4.36027 -4.3589711 -4.3553677 -4.3509636 -4.3460593 -4.3410435 -4.3367338]]...]
INFO - root - 2017-12-05 20:26:02.288278: step 41210, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 71h:58m:53s remains)
INFO - root - 2017-12-05 20:26:10.790445: step 41220, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 71h:38m:12s remains)
INFO - root - 2017-12-05 20:26:19.348568: step 41230, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 70h:20m:43s remains)
INFO - root - 2017-12-05 20:26:27.907898: step 41240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 68h:56m:25s remains)
INFO - root - 2017-12-05 20:26:36.441849: step 41250, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 70h:25m:32s remains)
INFO - root - 2017-12-05 20:26:44.896533: step 41260, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 70h:41m:04s remains)
INFO - root - 2017-12-05 20:26:53.391336: step 41270, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 66h:23m:54s remains)
INFO - root - 2017-12-05 20:27:02.040662: step 41280, loss = 2.05, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 72h:57m:42s remains)
INFO - root - 2017-12-05 20:27:10.700680: step 41290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 69h:28m:09s remains)
INFO - root - 2017-12-05 20:27:19.324579: step 41300, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:29m:28s remains)
2017-12-05 20:27:20.093837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2181158 -4.2251983 -4.2281494 -4.2226758 -4.217031 -4.2276926 -4.2363253 -4.2326412 -4.2232423 -4.208889 -4.1926327 -4.1781383 -4.1538811 -4.1156893 -4.1044726][-4.2197375 -4.2312331 -4.234417 -4.2278776 -4.2230773 -4.2297049 -4.229465 -4.2244649 -4.2208395 -4.2166066 -4.2027659 -4.1840453 -4.1499271 -4.1060123 -4.1004725][-4.2088485 -4.2152786 -4.2125072 -4.2032804 -4.2003717 -4.2032604 -4.1972218 -4.1968927 -4.2056837 -4.2144465 -4.209702 -4.1963735 -4.1698933 -4.136332 -4.1408176][-4.1979532 -4.2010036 -4.1948171 -4.1819086 -4.1767044 -4.1762862 -4.1656876 -4.16968 -4.1908665 -4.2066364 -4.2092881 -4.2027082 -4.1884284 -4.1693215 -4.1812091][-4.1911273 -4.1958504 -4.1924334 -4.1824079 -4.172719 -4.1664262 -4.1473246 -4.1474495 -4.1721716 -4.1917448 -4.2005367 -4.1993008 -4.1919675 -4.180469 -4.1927147][-4.1916881 -4.20276 -4.2035065 -4.1965747 -4.1850038 -4.1692753 -4.1378961 -4.1280046 -4.1487079 -4.17138 -4.1835403 -4.1868086 -4.1797047 -4.1675029 -4.1759014][-4.1765404 -4.1927943 -4.1949244 -4.1917558 -4.1814041 -4.1590009 -4.1204071 -4.1055865 -4.1267138 -4.1571813 -4.1722918 -4.1714115 -4.149353 -4.1233373 -4.12411][-4.1540084 -4.1564689 -4.1426082 -4.1316257 -4.1244974 -4.1098185 -4.0810642 -4.0712833 -4.097311 -4.137085 -4.1564841 -4.151515 -4.1176553 -4.0741267 -4.0614924][-4.1521597 -4.1292243 -4.089757 -4.0635066 -4.056582 -4.0595183 -4.0533166 -4.0552421 -4.0858235 -4.1324191 -4.1521878 -4.1445708 -4.1117315 -4.0648818 -4.04856][-4.166122 -4.1274676 -4.0779915 -4.0456815 -4.0458989 -4.0661325 -4.0787973 -4.0850749 -4.1129913 -4.1532321 -4.1654611 -4.1528306 -4.1298966 -4.0973067 -4.0910072][-4.1849036 -4.1477046 -4.1029215 -4.0740056 -4.0820045 -4.1100111 -4.1285224 -4.1407108 -4.1643124 -4.1872644 -4.188179 -4.1729245 -4.1563191 -4.139606 -4.1444359][-4.2032824 -4.1791663 -4.1502233 -4.1288304 -4.1380143 -4.1611333 -4.1750827 -4.1895461 -4.2044959 -4.2130976 -4.2110615 -4.2008948 -4.1894259 -4.1824732 -4.1903477][-4.2123175 -4.2000952 -4.1895556 -4.1809354 -4.1897483 -4.2064848 -4.2191253 -4.2332711 -4.2410588 -4.242413 -4.2411137 -4.2346206 -4.2243395 -4.2183905 -4.22231][-4.2116232 -4.2134919 -4.2157564 -4.21382 -4.2183466 -4.2271352 -4.2363262 -4.2476153 -4.2514138 -4.253746 -4.2589679 -4.2561526 -4.2469897 -4.2391558 -4.2364359][-4.201817 -4.2150989 -4.2228484 -4.2228575 -4.2249236 -4.228435 -4.2322855 -4.2397985 -4.2426481 -4.2468276 -4.2543311 -4.2549782 -4.2465043 -4.238194 -4.2323508]]...]
INFO - root - 2017-12-05 20:27:28.787628: step 41310, loss = 2.10, batch loss = 2.05 (9.1 examples/sec; 0.877 sec/batch; 70h:56m:38s remains)
INFO - root - 2017-12-05 20:27:37.408532: step 41320, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 68h:12m:39s remains)
INFO - root - 2017-12-05 20:27:46.086900: step 41330, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:46m:12s remains)
INFO - root - 2017-12-05 20:27:54.527130: step 41340, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 69h:49m:58s remains)
INFO - root - 2017-12-05 20:28:03.101683: step 41350, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 69h:58m:43s remains)
INFO - root - 2017-12-05 20:28:11.700617: step 41360, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 70h:46m:54s remains)
INFO - root - 2017-12-05 20:28:20.102841: step 41370, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 66h:54m:24s remains)
INFO - root - 2017-12-05 20:28:28.783453: step 41380, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.891 sec/batch; 72h:02m:33s remains)
INFO - root - 2017-12-05 20:28:37.319790: step 41390, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 67h:57m:47s remains)
INFO - root - 2017-12-05 20:28:45.940861: step 41400, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 69h:54m:17s remains)
2017-12-05 20:28:46.727462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2507033 -4.2411771 -4.2359533 -4.2269311 -4.2162681 -4.2153311 -4.2213778 -4.2362309 -4.2513475 -4.2644172 -4.2668004 -4.2542429 -4.2249784 -4.1928372 -4.1826792][-4.2370014 -4.2303472 -4.2236609 -4.2079487 -4.1891842 -4.1829 -4.1875305 -4.2093596 -4.233254 -4.2553473 -4.2650409 -4.2533617 -4.2212014 -4.1875 -4.1797738][-4.2223215 -4.2159147 -4.2060184 -4.1826715 -4.1541066 -4.1363239 -4.133893 -4.1644397 -4.2036667 -4.2387 -4.257184 -4.2517333 -4.2265997 -4.2013655 -4.2001562][-4.20467 -4.1989918 -4.1867189 -4.1591096 -4.1219845 -4.0889592 -4.0769463 -4.1151218 -4.1719375 -4.2217622 -4.2504478 -4.2538476 -4.2416482 -4.2316656 -4.23655][-4.1934261 -4.18692 -4.1666436 -4.1313682 -4.0823774 -4.030508 -4.0074039 -4.05503 -4.1305466 -4.194809 -4.2329516 -4.2438188 -4.2432346 -4.2458124 -4.2574492][-4.1897492 -4.1826453 -4.1537666 -4.1082969 -4.0433364 -3.9672456 -3.9271064 -3.9807732 -4.0706806 -4.148984 -4.1984687 -4.2158322 -4.2221704 -4.2302527 -4.2450128][-4.1911616 -4.1826162 -4.1439571 -4.0862269 -4.0078578 -3.9137225 -3.8637364 -3.9220393 -4.0141163 -4.1006331 -4.1613874 -4.182189 -4.1878304 -4.1933937 -4.2105312][-4.2108169 -4.2070761 -4.166409 -4.1058216 -4.0240569 -3.9244952 -3.8703809 -3.9182458 -3.9980502 -4.0822034 -4.1468458 -4.1688719 -4.170928 -4.1702089 -4.1860118][-4.2387915 -4.24466 -4.2122931 -4.1608629 -4.0917354 -4.0049577 -3.9547534 -3.9871159 -4.0476632 -4.1161728 -4.1701188 -4.1880507 -4.1871948 -4.1841254 -4.1981139][-4.2559314 -4.2676392 -4.2473254 -4.2098651 -4.1584377 -4.0911283 -4.04926 -4.068471 -4.1128292 -4.1627169 -4.2003736 -4.2149343 -4.216023 -4.2163792 -4.2303443][-4.268364 -4.27577 -4.262176 -4.2359023 -4.1974406 -4.148603 -4.1210823 -4.1350956 -4.1689935 -4.2062769 -4.2339716 -4.2484412 -4.2554216 -4.261745 -4.2748547][-4.268621 -4.2669234 -4.2562628 -4.2374921 -4.2081518 -4.1783223 -4.1683583 -4.1829724 -4.2094841 -4.236516 -4.2558055 -4.2684107 -4.2790885 -4.2915778 -4.3044462][-4.2701378 -4.2601919 -4.2485309 -4.2325888 -4.2106152 -4.1942134 -4.1958675 -4.2138376 -4.2355771 -4.2539859 -4.2672772 -4.2788019 -4.2899652 -4.3022833 -4.3109255][-4.274632 -4.2598786 -4.2464681 -4.232193 -4.2167478 -4.2084522 -4.2120438 -4.2265859 -4.2421474 -4.2529445 -4.25858 -4.2638731 -4.2717671 -4.2839704 -4.2928643][-4.2836137 -4.2674556 -4.252459 -4.2382078 -4.2256527 -4.2185225 -4.2185364 -4.2265277 -4.23535 -4.2382679 -4.2353415 -4.2336421 -4.2374778 -4.2464981 -4.2528124]]...]
INFO - root - 2017-12-05 20:28:55.237902: step 41410, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 66h:02m:53s remains)
INFO - root - 2017-12-05 20:29:03.823831: step 41420, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 70h:22m:52s remains)
INFO - root - 2017-12-05 20:29:12.370927: step 41430, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 69h:55m:56s remains)
INFO - root - 2017-12-05 20:29:20.950017: step 41440, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 69h:04m:02s remains)
INFO - root - 2017-12-05 20:29:29.509030: step 41450, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 68h:42m:46s remains)
INFO - root - 2017-12-05 20:29:37.943764: step 41460, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 66h:34m:04s remains)
INFO - root - 2017-12-05 20:29:46.534622: step 41470, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 69h:25m:29s remains)
INFO - root - 2017-12-05 20:29:55.090924: step 41480, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.884 sec/batch; 71h:28m:01s remains)
INFO - root - 2017-12-05 20:30:03.647516: step 41490, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 69h:53m:46s remains)
INFO - root - 2017-12-05 20:30:12.216054: step 41500, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.859 sec/batch; 69h:25m:56s remains)
2017-12-05 20:30:12.982149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1299763 -4.1428938 -4.1637869 -4.1944022 -4.2319484 -4.2615404 -4.2592483 -4.2317858 -4.2048397 -4.1888909 -4.1916842 -4.205564 -4.2143149 -4.2105274 -4.1903982][-4.1283851 -4.1473694 -4.1737938 -4.2076755 -4.248477 -4.2805161 -4.2785325 -4.2481279 -4.2147694 -4.1976848 -4.2004437 -4.213675 -4.2215552 -4.2199917 -4.2034049][-4.1188016 -4.1485896 -4.1883492 -4.2296128 -4.2706866 -4.2961864 -4.2887836 -4.2550163 -4.2146583 -4.1937294 -4.194139 -4.2071691 -4.2191982 -4.2243552 -4.2151451][-4.1189723 -4.1662269 -4.22085 -4.2620521 -4.2907472 -4.2997866 -4.2799377 -4.239265 -4.1927719 -4.1698675 -4.1759372 -4.1961985 -4.2172461 -4.2310486 -4.2358074][-4.137567 -4.1960411 -4.2531157 -4.2821646 -4.2900572 -4.2768788 -4.2394643 -4.18582 -4.1341081 -4.1233687 -4.1522384 -4.1919184 -4.224349 -4.2473493 -4.2622991][-4.1633797 -4.218379 -4.2638807 -4.2772141 -4.266448 -4.238564 -4.1817303 -4.1066084 -4.0559583 -4.074913 -4.1352868 -4.1929727 -4.2314491 -4.2574306 -4.2770329][-4.1887136 -4.2291527 -4.2537403 -4.252985 -4.230617 -4.1870713 -4.0983624 -3.9885752 -3.9553761 -4.02621 -4.1204343 -4.1930079 -4.2316828 -4.252717 -4.2692323][-4.197969 -4.2143841 -4.2216558 -4.2146153 -4.187252 -4.1288853 -4.0158939 -3.88748 -3.8920336 -4.0103912 -4.1247563 -4.1999149 -4.233849 -4.2478724 -4.2577543][-4.1801767 -4.1742277 -4.1718106 -4.1676168 -4.1470652 -4.0976944 -4.0047741 -3.919858 -3.9562018 -4.0649576 -4.161531 -4.2205343 -4.2442045 -4.2508287 -4.2542114][-4.1500239 -4.1278486 -4.1233048 -4.1255813 -4.1191826 -4.0937304 -4.047617 -4.0196056 -4.0654612 -4.1452813 -4.2122855 -4.25101 -4.264482 -4.2669954 -4.2667327][-4.1338534 -4.1094108 -4.1065207 -4.1117826 -4.1193132 -4.1188107 -4.1091146 -4.1098375 -4.1534972 -4.2103066 -4.2552552 -4.2823954 -4.2919335 -4.2945328 -4.2951212][-4.1227789 -4.1060066 -4.1092653 -4.1200137 -4.138463 -4.1566391 -4.166687 -4.1788211 -4.2141051 -4.255929 -4.289577 -4.3088174 -4.31558 -4.3183122 -4.3215237][-4.1301231 -4.1272597 -4.1382284 -4.1518264 -4.1728697 -4.1949053 -4.2093797 -4.2231817 -4.2506795 -4.2829828 -4.3088307 -4.321085 -4.32282 -4.3234472 -4.3292866][-4.1758437 -4.1840391 -4.1945224 -4.2010059 -4.2128344 -4.2244053 -4.2340555 -4.2430124 -4.2644134 -4.2885442 -4.3044972 -4.3092041 -4.3064651 -4.305603 -4.3144903][-4.2329807 -4.2461777 -4.2540555 -4.2533107 -4.2507696 -4.2460513 -4.2460179 -4.2488256 -4.2633653 -4.2794051 -4.2858667 -4.282618 -4.2757869 -4.2739606 -4.2855773]]...]
INFO - root - 2017-12-05 20:30:21.599311: step 41510, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 69h:08m:24s remains)
INFO - root - 2017-12-05 20:30:30.208413: step 41520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 70h:24m:00s remains)
INFO - root - 2017-12-05 20:30:38.773212: step 41530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 68h:51m:17s remains)
INFO - root - 2017-12-05 20:30:47.286034: step 41540, loss = 2.04, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 65h:27m:10s remains)
INFO - root - 2017-12-05 20:30:55.750867: step 41550, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.810 sec/batch; 65h:27m:41s remains)
INFO - root - 2017-12-05 20:31:04.201971: step 41560, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 68h:04m:17s remains)
INFO - root - 2017-12-05 20:31:12.785178: step 41570, loss = 2.07, batch loss = 2.01 (8.4 examples/sec; 0.950 sec/batch; 76h:48m:18s remains)
INFO - root - 2017-12-05 20:31:21.354413: step 41580, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 68h:21m:25s remains)
INFO - root - 2017-12-05 20:31:29.850661: step 41590, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 67h:29m:48s remains)
INFO - root - 2017-12-05 20:31:38.310891: step 41600, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.826 sec/batch; 66h:46m:32s remains)
2017-12-05 20:31:39.112301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2749305 -4.2693391 -4.2685881 -4.2692041 -4.27692 -4.2840333 -4.2849832 -4.276803 -4.2669425 -4.2580333 -4.2554193 -4.2622452 -4.2750854 -4.2856927 -4.2905746][-4.2583456 -4.263936 -4.27212 -4.2680578 -4.2655096 -4.2645183 -4.2667065 -4.2621894 -4.2492437 -4.2392855 -4.2434559 -4.259757 -4.2780886 -4.2902374 -4.2936921][-4.2593617 -4.2730446 -4.2834039 -4.2674556 -4.2471156 -4.235074 -4.2406425 -4.2397747 -4.2299223 -4.2226062 -4.2361159 -4.2629805 -4.2854819 -4.2975907 -4.2997546][-4.263968 -4.2761631 -4.2812443 -4.25494 -4.2217097 -4.2038984 -4.212924 -4.2149558 -4.2079945 -4.20942 -4.232307 -4.2643509 -4.2904544 -4.3022847 -4.3016067][-4.2583461 -4.2684946 -4.2683949 -4.2383471 -4.202713 -4.1780167 -4.1735859 -4.1702027 -4.1692615 -4.1848984 -4.2126336 -4.2412629 -4.26801 -4.2849722 -4.28794][-4.2519531 -4.2625375 -4.2596626 -4.2243714 -4.1805449 -4.1416645 -4.1110258 -4.094501 -4.1110134 -4.1447096 -4.1725054 -4.1943727 -4.2225089 -4.25101 -4.2634897][-4.2515845 -4.2575741 -4.2454987 -4.2014432 -4.1483035 -4.0916395 -4.0260482 -3.9919107 -4.0346923 -4.089232 -4.1207938 -4.1433663 -4.1767879 -4.2165356 -4.2394786][-4.2490263 -4.2428226 -4.2125225 -4.1630878 -4.1061873 -4.0328436 -3.932637 -3.8812468 -3.9534435 -4.0329967 -4.0821581 -4.11901 -4.1589918 -4.2026744 -4.2294354][-4.2484975 -4.2290039 -4.1851182 -4.1348038 -4.0799232 -4.0141191 -3.918947 -3.8790751 -3.9505591 -4.0247531 -4.07832 -4.1249523 -4.1674581 -4.2053123 -4.228169][-4.2512774 -4.2252183 -4.1766796 -4.1355391 -4.1062202 -4.0734234 -4.0233 -4.0075769 -4.0381689 -4.0736771 -4.1141386 -4.1533384 -4.1901069 -4.2166862 -4.2291994][-4.2512975 -4.2310381 -4.1952505 -4.1664429 -4.1570854 -4.1484952 -4.1264715 -4.1213961 -4.1215177 -4.1223779 -4.1427889 -4.1688166 -4.1974926 -4.2177153 -4.2269349][-4.2521219 -4.2432656 -4.2227297 -4.2028222 -4.2038646 -4.2073412 -4.1964293 -4.1915073 -4.1793556 -4.1577067 -4.1541905 -4.1642184 -4.18577 -4.2100043 -4.2236724][-4.2560468 -4.2563519 -4.245872 -4.2307162 -4.2303047 -4.2294021 -4.2174067 -4.2110066 -4.1951318 -4.1639171 -4.1476173 -4.145926 -4.1660314 -4.1999822 -4.2222428][-4.257792 -4.263608 -4.2602553 -4.248004 -4.238328 -4.2247648 -4.2077065 -4.2037807 -4.1937246 -4.1660528 -4.149065 -4.1443324 -4.1643095 -4.2036104 -4.232018][-4.2661519 -4.2697315 -4.2675304 -4.257761 -4.2452955 -4.2291794 -4.2147346 -4.2137561 -4.2089748 -4.1924214 -4.1818619 -4.1823525 -4.2033563 -4.235467 -4.2589922]]...]
INFO - root - 2017-12-05 20:31:47.782280: step 41610, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.901 sec/batch; 72h:48m:32s remains)
INFO - root - 2017-12-05 20:31:56.350061: step 41620, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 71h:30m:20s remains)
INFO - root - 2017-12-05 20:32:04.817101: step 41630, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 67h:30m:32s remains)
INFO - root - 2017-12-05 20:32:13.450421: step 41640, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 69h:37m:49s remains)
INFO - root - 2017-12-05 20:32:21.924912: step 41650, loss = 2.08, batch loss = 2.02 (10.2 examples/sec; 0.788 sec/batch; 63h:39m:01s remains)
INFO - root - 2017-12-05 20:32:30.379991: step 41660, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 68h:12m:54s remains)
INFO - root - 2017-12-05 20:32:38.963783: step 41670, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.894 sec/batch; 72h:11m:37s remains)
INFO - root - 2017-12-05 20:32:47.482446: step 41680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 69h:24m:49s remains)
INFO - root - 2017-12-05 20:32:55.966299: step 41690, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:28m:04s remains)
INFO - root - 2017-12-05 20:33:04.402100: step 41700, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 69h:37m:37s remains)
2017-12-05 20:33:05.241959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1546931 -4.1378322 -4.125093 -4.1184392 -4.1238608 -4.14601 -4.1863346 -4.2178931 -4.2228012 -4.2272763 -4.231739 -4.2294354 -4.2182765 -4.2054458 -4.18695][-4.1583028 -4.151547 -4.1430864 -4.1368413 -4.1422086 -4.1624875 -4.1965919 -4.2217135 -4.2249789 -4.2365952 -4.2479539 -4.2496452 -4.2419353 -4.2329063 -4.2154784][-4.1583815 -4.1617203 -4.1548643 -4.148129 -4.1506586 -4.1675463 -4.1959276 -4.2117085 -4.2112722 -4.2259226 -4.2404404 -4.2434311 -4.2426157 -4.2409286 -4.2310781][-4.1500535 -4.1629028 -4.1555033 -4.1450968 -4.1422648 -4.1562443 -4.1770439 -4.1815729 -4.1778393 -4.1937413 -4.2147031 -4.2272735 -4.2375507 -4.2439513 -4.2422142][-4.1353779 -4.1557884 -4.1466727 -4.1327481 -4.1268854 -4.1346216 -4.140872 -4.1335459 -4.1277008 -4.1533136 -4.1894627 -4.2161674 -4.2374549 -4.2537642 -4.2605419][-4.13152 -4.155817 -4.1430607 -4.1270051 -4.1161609 -4.1116462 -4.103683 -4.0894008 -4.0890265 -4.1285195 -4.1794043 -4.2158222 -4.241437 -4.2639356 -4.2746372][-4.1463189 -4.1698751 -4.1575661 -4.1407218 -4.1238155 -4.1065364 -4.0844803 -4.0670929 -4.0740356 -4.1230788 -4.182188 -4.2184334 -4.24138 -4.2632332 -4.2705812][-4.1757979 -4.1902461 -4.1771879 -4.1604419 -4.1403813 -4.1154251 -4.0844407 -4.069901 -4.08342 -4.1319742 -4.1899157 -4.2233238 -4.246582 -4.2674918 -4.2697139][-4.2199464 -4.2232876 -4.2045255 -4.1863656 -4.164669 -4.1359577 -4.1050324 -4.0956492 -4.11191 -4.1507931 -4.1988688 -4.2314897 -4.2576208 -4.2770796 -4.2751975][-4.2549539 -4.2538753 -4.2342587 -4.2153239 -4.1929622 -4.1619906 -4.1343961 -4.1289306 -4.1466169 -4.1780019 -4.2158303 -4.246911 -4.2712808 -4.284193 -4.2798958][-4.2648582 -4.2612123 -4.24912 -4.2396851 -4.225987 -4.1991606 -4.1764646 -4.1738925 -4.18767 -4.2083845 -4.2349072 -4.2564516 -4.2730093 -4.2806263 -4.2790184][-4.2514219 -4.2451406 -4.2438068 -4.2507458 -4.251317 -4.2328191 -4.214293 -4.2101583 -4.216074 -4.2249737 -4.2415338 -4.2589359 -4.2707014 -4.2723036 -4.2718][-4.2154236 -4.2070923 -4.2157168 -4.2389379 -4.2529497 -4.2440653 -4.230432 -4.220964 -4.2130771 -4.2104712 -4.226048 -4.249629 -4.2629538 -4.2619257 -4.2590222][-4.1582141 -4.148303 -4.1682448 -4.2074642 -4.2349863 -4.2376604 -4.2289023 -4.2136908 -4.1932564 -4.1812944 -4.1974 -4.229238 -4.2463384 -4.2480292 -4.2456217][-4.1176548 -4.1075621 -4.132865 -4.1765165 -4.2086277 -4.2207065 -4.223784 -4.2114625 -4.1877623 -4.1686831 -4.1802325 -4.2116442 -4.2297678 -4.2351332 -4.2357168]]...]
INFO - root - 2017-12-05 20:33:13.777148: step 41710, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 66h:54m:18s remains)
INFO - root - 2017-12-05 20:33:22.407125: step 41720, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 66h:27m:35s remains)
INFO - root - 2017-12-05 20:33:30.913714: step 41730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 69h:09m:43s remains)
INFO - root - 2017-12-05 20:33:39.383997: step 41740, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 66h:35m:54s remains)
INFO - root - 2017-12-05 20:33:47.795520: step 41750, loss = 2.04, batch loss = 1.99 (11.0 examples/sec; 0.725 sec/batch; 58h:30m:57s remains)
INFO - root - 2017-12-05 20:33:56.207591: step 41760, loss = 2.08, batch loss = 2.03 (10.4 examples/sec; 0.771 sec/batch; 62h:15m:03s remains)
INFO - root - 2017-12-05 20:34:04.789639: step 41770, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.862 sec/batch; 69h:37m:20s remains)
INFO - root - 2017-12-05 20:34:13.378107: step 41780, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.893 sec/batch; 72h:04m:37s remains)
INFO - root - 2017-12-05 20:34:21.925109: step 41790, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:31m:01s remains)
INFO - root - 2017-12-05 20:34:30.451483: step 41800, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 67h:21m:03s remains)
2017-12-05 20:34:31.229236: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1906528 -4.2134805 -4.2283492 -4.2457523 -4.2523069 -4.2395926 -4.2259908 -4.2154465 -4.2125592 -4.2224145 -4.23939 -4.2457061 -4.2383003 -4.2161584 -4.2041168][-4.1611137 -4.18377 -4.2004132 -4.2204852 -4.2227464 -4.1987486 -4.1748753 -4.1587687 -4.1556997 -4.1708074 -4.2030907 -4.2228103 -4.2253656 -4.2103453 -4.2029533][-4.1333704 -4.1549616 -4.1696405 -4.1845746 -4.1771512 -4.1391358 -4.1000972 -4.0726933 -4.0669017 -4.0927567 -4.1505709 -4.1879478 -4.2037873 -4.2024264 -4.2066379][-4.1296854 -4.1457353 -4.1520472 -4.1526065 -4.1242394 -4.0688138 -4.0111704 -3.9745424 -3.9743855 -4.02133 -4.1062846 -4.1608014 -4.1882873 -4.1980858 -4.2121787][-4.1401219 -4.14181 -4.1360993 -4.1178136 -4.0638056 -3.9878764 -3.9144487 -3.8716915 -3.8839059 -3.9588568 -4.0682793 -4.1364355 -4.1734362 -4.1923046 -4.2151866][-4.1420417 -4.1303306 -4.1180019 -4.0909228 -4.0308423 -3.9568665 -3.8815846 -3.8306122 -3.8450246 -3.9300439 -4.0440984 -4.1177039 -4.1613221 -4.1866779 -4.2101245][-4.1461 -4.1283708 -4.1163726 -4.0923305 -4.0483913 -3.9942091 -3.9263685 -3.868546 -3.8729565 -3.9419007 -4.0386496 -4.1083064 -4.1513724 -4.1753511 -4.1940665][-4.1541276 -4.1391921 -4.1356382 -4.1303678 -4.1145592 -4.081871 -4.0230246 -3.9676249 -3.96462 -4.0086913 -4.0731826 -4.1235595 -4.1560488 -4.1730938 -4.181272][-4.1597 -4.1527343 -4.1631866 -4.1755967 -4.1781597 -4.1551805 -4.1060414 -4.0637245 -4.0633125 -4.0905204 -4.1301708 -4.1625781 -4.1793528 -4.1810079 -4.1753216][-4.1497407 -4.1546493 -4.1788492 -4.2002425 -4.2077641 -4.1824837 -4.1422944 -4.1150441 -4.1207037 -4.1447177 -4.1757207 -4.1999044 -4.2037606 -4.1902122 -4.1771483][-4.1393456 -4.1574974 -4.1903992 -4.2156 -4.221015 -4.1962514 -4.1672926 -4.1527586 -4.1638374 -4.1857758 -4.2108264 -4.22826 -4.2228932 -4.2022023 -4.1874628][-4.14735 -4.1733718 -4.2074332 -4.2319613 -4.2375059 -4.2198424 -4.2028337 -4.1984186 -4.2096705 -4.2243886 -4.2400665 -4.2465711 -4.2340031 -4.2126718 -4.1998606][-4.1624861 -4.1899247 -4.2190714 -4.2419453 -4.2515473 -4.2439289 -4.2374611 -4.2371883 -4.24286 -4.2502513 -4.256927 -4.2543259 -4.2387671 -4.2185626 -4.208427][-4.1816015 -4.2085052 -4.2325134 -4.2541037 -4.2662444 -4.2642703 -4.2639503 -4.2645817 -4.2667127 -4.2684474 -4.2690253 -4.2607851 -4.2450733 -4.2277913 -4.2216244][-4.2027431 -4.2280784 -4.2463861 -4.2637329 -4.2749066 -4.2748909 -4.2756596 -4.2761011 -4.2761664 -4.27505 -4.2713518 -4.2614994 -4.2497053 -4.2373013 -4.2374148]]...]
INFO - root - 2017-12-05 20:34:39.743484: step 41810, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 68h:10m:01s remains)
INFO - root - 2017-12-05 20:34:48.273148: step 41820, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.847 sec/batch; 68h:21m:08s remains)
INFO - root - 2017-12-05 20:34:56.868713: step 41830, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 67h:40m:47s remains)
INFO - root - 2017-12-05 20:35:05.398595: step 41840, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 70h:51m:47s remains)
INFO - root - 2017-12-05 20:35:13.843576: step 41850, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 69h:23m:27s remains)
INFO - root - 2017-12-05 20:35:22.340785: step 41860, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 70h:10m:17s remains)
INFO - root - 2017-12-05 20:35:30.815797: step 41870, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 66h:12m:51s remains)
INFO - root - 2017-12-05 20:35:39.389382: step 41880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 69h:31m:27s remains)
INFO - root - 2017-12-05 20:35:47.930975: step 41890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 68h:29m:48s remains)
INFO - root - 2017-12-05 20:35:56.362279: step 41900, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 66h:10m:54s remains)
2017-12-05 20:35:57.070409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2389259 -4.2563195 -4.2635393 -4.2472706 -4.2136903 -4.1795483 -4.1643586 -4.1542006 -4.1314411 -4.1345935 -4.1776628 -4.223042 -4.2554984 -4.2590308 -4.2502842][-4.23923 -4.2573671 -4.2556863 -4.232573 -4.1884484 -4.1425886 -4.1207514 -4.1116581 -4.0875821 -4.0962391 -4.1413679 -4.1911678 -4.2365685 -4.2576342 -4.2575841][-4.23428 -4.2501531 -4.2422681 -4.2121687 -4.1615586 -4.1008978 -4.0725613 -4.0703335 -4.06644 -4.0832095 -4.1228881 -4.1674943 -4.2179842 -4.2506108 -4.2517133][-4.2287731 -4.2431459 -4.22822 -4.1899114 -4.1364737 -4.0756464 -4.0478673 -4.0548315 -4.0731239 -4.0978889 -4.12714 -4.1590719 -4.2070346 -4.2393589 -4.2349648][-4.2245541 -4.229404 -4.1985035 -4.1458015 -4.0909514 -4.0396466 -4.0210919 -4.0406456 -4.0773873 -4.1107574 -4.134254 -4.1571283 -4.1980357 -4.2270417 -4.214839][-4.2240415 -4.20806 -4.1557579 -4.0873289 -4.0253563 -3.9827945 -3.9788687 -4.0154419 -4.0671191 -4.1042724 -4.1252747 -4.1434722 -4.1815844 -4.2099929 -4.1858854][-4.2127843 -4.1787133 -4.1162481 -4.0448613 -3.982352 -3.9429095 -3.95152 -3.9999409 -4.0487604 -4.0789356 -4.10471 -4.1287518 -4.1678963 -4.1918068 -4.1604476][-4.1967664 -4.1553197 -4.0950732 -4.0321021 -3.9808354 -3.9510093 -3.9681687 -4.0110145 -4.039238 -4.0557814 -4.0862336 -4.1227264 -4.1602135 -4.1734452 -4.1362662][-4.1979918 -4.1594186 -4.103334 -4.0482855 -4.0137024 -3.9987576 -4.0221834 -4.0534863 -4.0551476 -4.0547833 -4.0850081 -4.119853 -4.1437469 -4.1416187 -4.0973845][-4.2125139 -4.1757669 -4.1272 -4.0876412 -4.0727139 -4.0750189 -4.0995865 -4.1158934 -4.09886 -4.090703 -4.1128249 -4.1251497 -4.1253157 -4.1123757 -4.061677][-4.24413 -4.2098446 -4.17003 -4.1443577 -4.1404991 -4.1492834 -4.1667261 -4.168591 -4.1458621 -4.1386375 -4.1505103 -4.1439562 -4.125967 -4.1046252 -4.0509815][-4.275281 -4.2487826 -4.2189641 -4.202517 -4.203042 -4.2128839 -4.2186365 -4.2065625 -4.1843591 -4.1782713 -4.1832089 -4.1708946 -4.1415148 -4.111743 -4.0621443][-4.2902675 -4.2759705 -4.2602258 -4.2537856 -4.2557697 -4.2610211 -4.25618 -4.2378683 -4.2194386 -4.212925 -4.2103295 -4.195755 -4.1633797 -4.1288838 -4.0888619][-4.290967 -4.2888169 -4.2840214 -4.2853327 -4.2901263 -4.2918506 -4.2835741 -4.2662411 -4.2522912 -4.2463012 -4.236918 -4.217577 -4.1836872 -4.1515007 -4.12496][-4.2842908 -4.2907839 -4.2921953 -4.2951322 -4.3001585 -4.3026118 -4.2976017 -4.2854438 -4.2773776 -4.2734118 -4.2621565 -4.241076 -4.2097926 -4.1827126 -4.166985]]...]
INFO - root - 2017-12-05 20:36:05.583493: step 41910, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 67h:51m:06s remains)
INFO - root - 2017-12-05 20:36:14.077194: step 41920, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:20m:09s remains)
INFO - root - 2017-12-05 20:36:22.601739: step 41930, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 68h:45m:08s remains)
INFO - root - 2017-12-05 20:36:31.155357: step 41940, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 69h:09m:54s remains)
INFO - root - 2017-12-05 20:36:39.721220: step 41950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 69h:20m:31s remains)
INFO - root - 2017-12-05 20:36:48.465644: step 41960, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 69h:42m:51s remains)
INFO - root - 2017-12-05 20:36:57.009579: step 41970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 69h:28m:51s remains)
INFO - root - 2017-12-05 20:37:05.406211: step 41980, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.871 sec/batch; 70h:17m:45s remains)
INFO - root - 2017-12-05 20:37:14.018018: step 41990, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 70h:09m:36s remains)
INFO - root - 2017-12-05 20:37:22.684623: step 42000, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 68h:46m:29s remains)
2017-12-05 20:37:23.519629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2472653 -4.2515879 -4.2509851 -4.2397547 -4.2241654 -4.2055383 -4.1857591 -4.158555 -4.1289425 -4.1023393 -4.0837851 -4.0958562 -4.1309834 -4.1673903 -4.1888361][-4.2476258 -4.2467079 -4.2417493 -4.2333779 -4.2232633 -4.2112575 -4.1931386 -4.1691208 -4.1456385 -4.1224465 -4.1016159 -4.1040506 -4.1288109 -4.1518774 -4.1599579][-4.233429 -4.2281747 -4.2190824 -4.2131906 -4.2103076 -4.2061334 -4.1921906 -4.1701045 -4.153708 -4.137598 -4.1210904 -4.1137867 -4.1207232 -4.1242352 -4.120544][-4.2127604 -4.2015796 -4.1897058 -4.1883564 -4.1908607 -4.1897712 -4.1801891 -4.160532 -4.1500635 -4.1424394 -4.1328025 -4.1202483 -4.1085186 -4.091424 -4.072752][-4.1875296 -4.1699171 -4.1610436 -4.1658416 -4.1728692 -4.1763749 -4.1649966 -4.1392641 -4.13735 -4.149313 -4.1506371 -4.1349888 -4.1086 -4.0735245 -4.043859][-4.1609063 -4.1357164 -4.1291714 -4.135838 -4.1372852 -4.1355782 -4.1066146 -4.0616097 -4.0686636 -4.108408 -4.1322656 -4.1253409 -4.0907183 -4.0571141 -4.0369763][-4.1308727 -4.0945477 -4.0746908 -4.064218 -4.0463595 -4.0193448 -3.9484839 -3.8659387 -3.8987906 -3.999908 -4.0708737 -4.08984 -4.0680423 -4.0530448 -4.0555835][-4.1026974 -4.0491548 -4.0048332 -3.9660964 -3.9182653 -3.8484149 -3.7192152 -3.5836146 -3.6605549 -3.8508043 -3.983171 -4.0411344 -4.0439448 -4.0535178 -4.078259][-4.0967135 -4.0416646 -3.9851081 -3.9271328 -3.8547544 -3.7612958 -3.614306 -3.474391 -3.5808289 -3.7966261 -3.9364305 -3.9985693 -4.0097318 -4.03618 -4.0808406][-4.1047158 -4.0634851 -4.0197377 -3.9706964 -3.9143023 -3.8522801 -3.7605495 -3.6892993 -3.7692676 -3.9053383 -3.9811242 -4.01081 -4.014163 -4.0458474 -4.0955915][-4.1175418 -4.0943928 -4.0721703 -4.0491128 -4.027133 -4.0012069 -3.9561853 -3.9231682 -3.9685409 -4.0355434 -4.0616632 -4.0666323 -4.066638 -4.0884495 -4.1197639][-4.1550703 -4.1424751 -4.133306 -4.1272192 -4.1253824 -4.1178179 -4.0938487 -4.0726681 -4.0899787 -4.1183724 -4.126183 -4.1248779 -4.1229019 -4.1307507 -4.1372304][-4.2098389 -4.199976 -4.1950688 -4.1958537 -4.1964588 -4.1917114 -4.1774316 -4.1618733 -4.1661758 -4.1794648 -4.1857009 -4.1838017 -4.1784563 -4.1722679 -4.163733][-4.2623458 -4.2551432 -4.2511921 -4.2524991 -4.2513623 -4.2452564 -4.2365484 -4.2281337 -4.2309251 -4.2411613 -4.246603 -4.2440457 -4.2360024 -4.22118 -4.2056956][-4.3055434 -4.3023214 -4.302434 -4.30523 -4.3021564 -4.2964244 -4.2947693 -4.2935352 -4.2970576 -4.3055606 -4.3094473 -4.3054986 -4.2978415 -4.2820859 -4.2667527]]...]
INFO - root - 2017-12-05 20:37:32.093293: step 42010, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 67h:26m:46s remains)
INFO - root - 2017-12-05 20:37:40.547566: step 42020, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 65h:36m:20s remains)
INFO - root - 2017-12-05 20:37:49.039482: step 42030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:51m:49s remains)
INFO - root - 2017-12-05 20:37:57.639472: step 42040, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 71h:14m:10s remains)
INFO - root - 2017-12-05 20:38:06.075222: step 42050, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 69h:20m:44s remains)
INFO - root - 2017-12-05 20:38:14.550084: step 42060, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.817 sec/batch; 65h:57m:06s remains)
INFO - root - 2017-12-05 20:38:23.030386: step 42070, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 70h:29m:48s remains)
INFO - root - 2017-12-05 20:38:31.539876: step 42080, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.829 sec/batch; 66h:52m:34s remains)
INFO - root - 2017-12-05 20:38:40.002629: step 42090, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 68h:40m:40s remains)
INFO - root - 2017-12-05 20:38:48.595046: step 42100, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 69h:32m:55s remains)
2017-12-05 20:38:49.374004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1502686 -4.14622 -4.1358452 -4.1272364 -4.1252122 -4.1305838 -4.1440535 -4.1619997 -4.1751814 -4.171176 -4.17117 -4.1842036 -4.2071533 -4.2323747 -4.2581825][-4.1101608 -4.0973377 -4.0882158 -4.0822711 -4.0853047 -4.0956178 -4.1172595 -4.14699 -4.1733823 -4.1821985 -4.1850948 -4.1951938 -4.2128496 -4.2328253 -4.2551575][-4.0641456 -4.0413542 -4.0345259 -4.0380383 -4.0504708 -4.0679731 -4.0969839 -4.1309071 -4.1618452 -4.1811819 -4.1909 -4.2053294 -4.2215314 -4.2348313 -4.2497239][-4.0311852 -4.0003219 -3.9978764 -4.0109439 -4.0323691 -4.0584354 -4.0922742 -4.1230206 -4.1484575 -4.1679015 -4.1795235 -4.1997232 -4.2199121 -4.2305117 -4.239346][-4.0291247 -3.9951904 -3.9960132 -4.0120411 -4.0325642 -4.0567203 -4.0884757 -4.1131 -4.1312113 -4.1476712 -4.1592007 -4.183363 -4.207561 -4.216424 -4.222507][-4.0526271 -4.0152721 -4.01611 -4.03454 -4.0501103 -4.0652905 -4.0872512 -4.1083689 -4.12213 -4.1321893 -4.1367846 -4.1540723 -4.1769581 -4.1885829 -4.2036777][-4.0711503 -4.0390248 -4.0444765 -4.0679793 -4.0835991 -4.0907679 -4.1035767 -4.1189814 -4.1239195 -4.1228585 -4.1140027 -4.12291 -4.1464992 -4.1681581 -4.1958551][-4.0818253 -4.059957 -4.0700955 -4.0967932 -4.1133204 -4.11894 -4.1285806 -4.1366081 -4.1287675 -4.1123877 -4.0929894 -4.1011786 -4.1290908 -4.1572204 -4.1931548][-4.094604 -4.0852447 -4.1019826 -4.1283975 -4.1440873 -4.1525064 -4.1583748 -4.1576066 -4.1390157 -4.1077514 -4.082098 -4.0944934 -4.1278534 -4.1584415 -4.195641][-4.1065521 -4.1095948 -4.1312194 -4.1541505 -4.16861 -4.17824 -4.1843581 -4.1802664 -4.1556988 -4.1155992 -4.0853124 -4.0992188 -4.1334572 -4.15874 -4.1915607][-4.121212 -4.1299767 -4.148984 -4.1649542 -4.1739321 -4.1833944 -4.1929693 -4.1911359 -4.1673379 -4.131146 -4.1052957 -4.115581 -4.1382003 -4.151032 -4.1770172][-4.1371889 -4.1450329 -4.1556549 -4.1636853 -4.1642246 -4.1707582 -4.1824927 -4.1856914 -4.1716762 -4.1448059 -4.1211152 -4.1224217 -4.1281614 -4.13143 -4.1584005][-4.1429558 -4.1484308 -4.1520085 -4.1557884 -4.1513824 -4.1535573 -4.1611276 -4.1643424 -4.154108 -4.132472 -4.1097751 -4.1052341 -4.1058817 -4.114265 -4.1565785][-4.1526628 -4.1578231 -4.1598291 -4.1623945 -4.155858 -4.1531687 -4.1536846 -4.1531868 -4.1408257 -4.1156282 -4.0968776 -4.0953326 -4.105628 -4.1304917 -4.1826749][-4.1720171 -4.1807218 -4.1839333 -4.1869864 -4.1785221 -4.1710148 -4.16562 -4.1605711 -4.1473589 -4.1218057 -4.1093884 -4.1162925 -4.1397982 -4.175025 -4.2256055]]...]
INFO - root - 2017-12-05 20:38:57.967023: step 42110, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.887 sec/batch; 71h:31m:15s remains)
INFO - root - 2017-12-05 20:39:06.557296: step 42120, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 68h:57m:23s remains)
INFO - root - 2017-12-05 20:39:15.232837: step 42130, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 70h:16m:53s remains)
INFO - root - 2017-12-05 20:39:23.673198: step 42140, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.854 sec/batch; 68h:54m:13s remains)
INFO - root - 2017-12-05 20:39:31.998070: step 42150, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 69h:58m:54s remains)
INFO - root - 2017-12-05 20:39:40.666345: step 42160, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 67h:43m:55s remains)
INFO - root - 2017-12-05 20:39:49.238351: step 42170, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 68h:15m:22s remains)
INFO - root - 2017-12-05 20:39:57.924593: step 42180, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 68h:49m:38s remains)
INFO - root - 2017-12-05 20:40:06.384976: step 42190, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 69h:04m:27s remains)
INFO - root - 2017-12-05 20:40:14.936021: step 42200, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 71h:17m:19s remains)
2017-12-05 20:40:15.712051: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1820397 -4.1463881 -4.1013088 -4.0675631 -4.0524197 -4.05148 -4.0597658 -4.0782351 -4.0999255 -4.1162357 -4.1253986 -4.130126 -4.1393957 -4.1505418 -4.1542563][-4.1342311 -4.1040182 -4.0732584 -4.0543909 -4.0509071 -4.055975 -4.0617828 -4.0737042 -4.0867209 -4.0958204 -4.0965796 -4.0905895 -4.0930748 -4.1008906 -4.1029997][-4.1090655 -4.092638 -4.0838938 -4.0821862 -4.0908728 -4.1049709 -4.1135406 -4.118886 -4.1206942 -4.1222243 -4.1167116 -4.1054945 -4.0990381 -4.0947642 -4.0903082][-4.1268868 -4.1257906 -4.1296659 -4.1347127 -4.1460629 -4.1576123 -4.1588597 -4.1573715 -4.1553526 -4.1569123 -4.1571689 -4.1534224 -4.1447978 -4.1302972 -4.1184378][-4.16272 -4.164012 -4.1684284 -4.1707582 -4.1767731 -4.1795931 -4.1696844 -4.1617336 -4.1612477 -4.165834 -4.174674 -4.1800823 -4.1732669 -4.1526051 -4.1326365][-4.1831884 -4.1779027 -4.1727176 -4.1637239 -4.1582241 -4.1497693 -4.1340203 -4.1315675 -4.1436272 -4.1566625 -4.1753755 -4.1884627 -4.1846023 -4.1572847 -4.1277046][-4.1678996 -4.150579 -4.1342697 -4.1154804 -4.1005845 -4.0818777 -4.0628066 -4.0736074 -4.1030121 -4.1267686 -4.1527996 -4.1740212 -4.1786594 -4.1543665 -4.1205921][-4.1477761 -4.1184268 -4.0942163 -4.0743952 -4.0610404 -4.0435767 -4.0276508 -4.0440621 -4.0772815 -4.0978808 -4.1227064 -4.1477709 -4.158978 -4.1441917 -4.1159043][-4.1583376 -4.1318879 -4.1100378 -4.0957923 -4.0901961 -4.0805168 -4.0671473 -4.0763688 -4.0989232 -4.1100554 -4.1261039 -4.1439695 -4.1558409 -4.1518564 -4.1358175][-4.1989031 -4.1838346 -4.1686387 -4.1620374 -4.1621261 -4.1561055 -4.1434708 -4.1424079 -4.1503248 -4.1536927 -4.1607289 -4.1711111 -4.1824942 -4.1844997 -4.179996][-4.2413554 -4.2351813 -4.2244539 -4.2233329 -4.2276945 -4.2250104 -4.2158585 -4.20929 -4.2083344 -4.2079287 -4.2083325 -4.2145972 -4.2252679 -4.2290487 -4.23056][-4.2584143 -4.2560434 -4.2495375 -4.25356 -4.2615128 -4.2635617 -4.2606273 -4.2544618 -4.2496653 -4.2450829 -4.2407455 -4.24491 -4.2539163 -4.2572064 -4.2595978][-4.2527337 -4.2508674 -4.2495432 -4.2596855 -4.27086 -4.2773943 -4.2792926 -4.2741151 -4.266232 -4.2568526 -4.2493777 -4.2513804 -4.2572136 -4.2583356 -4.2596407][-4.2367954 -4.23559 -4.240273 -4.2556696 -4.2681718 -4.2763734 -4.281498 -4.2772226 -4.2665362 -4.2521644 -4.2401462 -4.2394209 -4.242188 -4.2405448 -4.2405334][-4.2288351 -4.2300425 -4.2372069 -4.2525072 -4.2617459 -4.2685213 -4.2750692 -4.273869 -4.2649541 -4.2496791 -4.2351589 -4.2311597 -4.2310257 -4.229578 -4.2298703]]...]
INFO - root - 2017-12-05 20:40:24.285328: step 42210, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 65h:15m:52s remains)
INFO - root - 2017-12-05 20:40:32.861946: step 42220, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 68h:56m:46s remains)
INFO - root - 2017-12-05 20:40:41.363019: step 42230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 68h:25m:40s remains)
INFO - root - 2017-12-05 20:40:49.919074: step 42240, loss = 2.01, batch loss = 1.96 (9.5 examples/sec; 0.845 sec/batch; 68h:08m:15s remains)
INFO - root - 2017-12-05 20:40:58.240826: step 42250, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 68h:30m:55s remains)
INFO - root - 2017-12-05 20:41:06.779174: step 42260, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 68h:10m:17s remains)
INFO - root - 2017-12-05 20:41:15.366013: step 42270, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 66h:59m:18s remains)
INFO - root - 2017-12-05 20:41:23.891251: step 42280, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:43m:37s remains)
INFO - root - 2017-12-05 20:41:32.406482: step 42290, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 70h:07m:46s remains)
INFO - root - 2017-12-05 20:41:40.845053: step 42300, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 69h:10m:40s remains)
2017-12-05 20:41:41.577261: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1095037 -4.0646396 -4.0339041 -4.02328 -4.0455122 -4.0970759 -4.15453 -4.1987772 -4.2257886 -4.2366214 -4.2345653 -4.2253852 -4.2119637 -4.2021623 -4.202467][-4.1316543 -4.0992393 -4.0790272 -4.0742764 -4.0924897 -4.13636 -4.1824241 -4.2113461 -4.2299223 -4.2400866 -4.2398863 -4.229744 -4.2148018 -4.2027416 -4.1983032][-4.1537304 -4.1317253 -4.1188369 -4.1196508 -4.1345534 -4.1661825 -4.196795 -4.2132621 -4.2257123 -4.2361736 -4.2411237 -4.2347565 -4.2226763 -4.2107911 -4.2025075][-4.1627688 -4.1411252 -4.12715 -4.1228619 -4.1238813 -4.1381631 -4.1609058 -4.1799827 -4.1996913 -4.2195745 -4.2361732 -4.2390628 -4.2315826 -4.2214909 -4.2098546][-4.16593 -4.1353536 -4.1062822 -4.0806189 -4.0549636 -4.0455337 -4.0630383 -4.0959449 -4.13872 -4.1818032 -4.2195873 -4.2390342 -4.2387075 -4.2293854 -4.2137647][-4.1680059 -4.1365972 -4.0951705 -4.0484638 -3.9908986 -3.9422538 -3.92667 -3.9535975 -4.0236435 -4.1042514 -4.1738515 -4.2175574 -4.2279196 -4.2167182 -4.1978045][-4.1642108 -4.1440029 -4.1085291 -4.062541 -3.9931583 -3.9092338 -3.8294234 -3.8080289 -3.8868089 -4.0042963 -4.1071682 -4.1787 -4.2044067 -4.1922455 -4.1648111][-4.1535554 -4.1494112 -4.130609 -4.1054492 -4.0565648 -3.9779935 -3.8757584 -3.8075776 -3.8465569 -3.9503949 -4.0557976 -4.1372495 -4.1740961 -4.1679659 -4.1352959][-4.1312771 -4.1317978 -4.1274791 -4.1280675 -4.1163311 -4.0790267 -4.0102453 -3.9478185 -3.9344046 -3.9720545 -4.0406394 -4.1103344 -4.1482496 -4.1488309 -4.1219888][-4.1110272 -4.0954041 -4.087276 -4.1001291 -4.115624 -4.1225104 -4.1060724 -4.079566 -4.0548735 -4.0501475 -4.0775795 -4.1180077 -4.1419592 -4.143229 -4.1273928][-4.1168547 -4.0813675 -4.0549922 -4.0532112 -4.0709896 -4.0989885 -4.1187348 -4.13319 -4.133584 -4.1313152 -4.1415172 -4.1569061 -4.1621962 -4.15953 -4.1510558][-4.1501269 -4.1096978 -4.0684333 -4.0409012 -4.0346832 -4.0537081 -4.0833569 -4.1216974 -4.1483045 -4.165648 -4.1798325 -4.1892872 -4.19166 -4.191987 -4.1880574][-4.1825809 -4.1581511 -4.118793 -4.0769539 -4.0450177 -4.0418591 -4.0582042 -4.088182 -4.1162605 -4.1463327 -4.1724696 -4.1886191 -4.1994181 -4.2110677 -4.2193122][-4.1970859 -4.1972289 -4.1770639 -4.1432824 -4.1011195 -4.0734687 -4.065557 -4.0690594 -4.0813565 -4.10844 -4.1405292 -4.1670632 -4.1858573 -4.2043447 -4.2226892][-4.1834269 -4.2074537 -4.2139831 -4.2037916 -4.1730819 -4.136498 -4.10513 -4.0812726 -4.0759926 -4.0916729 -4.1158066 -4.1388221 -4.1600766 -4.182869 -4.20692]]...]
INFO - root - 2017-12-05 20:41:50.243571: step 42310, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 70h:41m:47s remains)
INFO - root - 2017-12-05 20:41:58.836202: step 42320, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 69h:05m:39s remains)
INFO - root - 2017-12-05 20:42:07.452250: step 42330, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 69h:46m:45s remains)
INFO - root - 2017-12-05 20:42:16.034936: step 42340, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 67h:51m:30s remains)
INFO - root - 2017-12-05 20:42:24.583171: step 42350, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 68h:40m:31s remains)
INFO - root - 2017-12-05 20:42:33.093624: step 42360, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 70h:16m:47s remains)
INFO - root - 2017-12-05 20:42:41.654654: step 42370, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 70h:40m:56s remains)
INFO - root - 2017-12-05 20:42:50.239182: step 42380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:17m:19s remains)
INFO - root - 2017-12-05 20:42:58.712414: step 42390, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 68h:00m:21s remains)
INFO - root - 2017-12-05 20:43:07.332285: step 42400, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:50m:28s remains)
2017-12-05 20:43:08.066213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3137784 -4.3190031 -4.3220549 -4.3235688 -4.3249383 -4.3276258 -4.3296895 -4.3307209 -4.33154 -4.3313069 -4.3314238 -4.331975 -4.332757 -4.3323545 -4.3322577][-4.291121 -4.295805 -4.2996259 -4.3014894 -4.3043466 -4.3090167 -4.3127856 -4.3149962 -4.3167143 -4.31744 -4.3188548 -4.3224845 -4.3258767 -4.3266048 -4.3268981][-4.2484927 -4.2487464 -4.2515278 -4.2537365 -4.2608552 -4.2693114 -4.2753553 -4.27974 -4.2835121 -4.2846045 -4.2896733 -4.3005648 -4.3084359 -4.31216 -4.3153687][-4.2088013 -4.2045612 -4.2040534 -4.2016792 -4.2052784 -4.2097716 -4.2117219 -4.2204919 -4.2323227 -4.238399 -4.2508569 -4.2704754 -4.28375 -4.29114 -4.2977819][-4.183002 -4.1704092 -4.1602716 -4.1470923 -4.1412377 -4.1371522 -4.1362128 -4.156702 -4.1817513 -4.197021 -4.2192788 -4.2457767 -4.263515 -4.2730274 -4.2809896][-4.1635451 -4.1370234 -4.1138444 -4.08779 -4.0719876 -4.0634656 -4.0643258 -4.0958314 -4.1340652 -4.1577454 -4.1873288 -4.2224283 -4.2466149 -4.2582278 -4.2674818][-4.1428432 -4.1027451 -4.0676203 -4.0261612 -3.9992876 -3.9863267 -3.9800141 -4.0114288 -4.0611658 -4.097579 -4.1410384 -4.1940894 -4.2299852 -4.2475219 -4.2591825][-4.1175818 -4.0665569 -4.0208659 -3.9668076 -3.9350576 -3.9233797 -3.9154358 -3.9465501 -4.0037642 -4.051764 -4.1073666 -4.1752777 -4.2196259 -4.2415929 -4.2551279][-4.1038561 -4.0435462 -3.9910526 -3.929039 -3.8923154 -3.8833504 -3.8763652 -3.9048548 -3.9633119 -4.0142627 -4.0766959 -4.14978 -4.2002773 -4.2277622 -4.2444186][-4.1076746 -4.0561109 -4.0117974 -3.9565384 -3.9198616 -3.9091177 -3.9001863 -3.9205496 -3.96678 -4.0078468 -4.0683823 -4.1369514 -4.1861014 -4.2157702 -4.2360864][-4.1329145 -4.094491 -4.0681038 -4.0307665 -4.0040841 -3.9954035 -3.9875467 -3.9998176 -4.0316439 -4.0625205 -4.1118226 -4.1638222 -4.2018867 -4.2261639 -4.2430372][-4.1760912 -4.1400647 -4.1195054 -4.094821 -4.0770831 -4.0741749 -4.0711355 -4.0825686 -4.1064792 -4.134439 -4.1753688 -4.2124104 -4.2388921 -4.2536755 -4.2622352][-4.2258391 -4.1906676 -4.1718988 -4.1580496 -4.1506653 -4.154469 -4.16075 -4.1760125 -4.1968355 -4.2192283 -4.2460971 -4.266716 -4.2798929 -4.2858148 -4.2895169][-4.2786188 -4.252357 -4.2387362 -4.2333646 -4.233243 -4.2421131 -4.254168 -4.2678041 -4.2830272 -4.297719 -4.3108644 -4.3194766 -4.3231559 -4.32381 -4.3240113][-4.3166518 -4.3047419 -4.2982836 -4.295814 -4.2980046 -4.3061323 -4.3154798 -4.3220439 -4.328424 -4.3370605 -4.34376 -4.3477144 -4.3479896 -4.3466258 -4.3449731]]...]
INFO - root - 2017-12-05 20:43:16.734589: step 42410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 68h:31m:16s remains)
INFO - root - 2017-12-05 20:43:25.268189: step 42420, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.848 sec/batch; 68h:21m:58s remains)
INFO - root - 2017-12-05 20:43:33.861459: step 42430, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 68h:09m:56s remains)
INFO - root - 2017-12-05 20:43:42.495076: step 42440, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 70h:17m:49s remains)
INFO - root - 2017-12-05 20:43:50.893838: step 42450, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 67h:29m:36s remains)
INFO - root - 2017-12-05 20:43:59.504014: step 42460, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 68h:19m:22s remains)
INFO - root - 2017-12-05 20:44:07.989791: step 42470, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.881 sec/batch; 70h:57m:32s remains)
INFO - root - 2017-12-05 20:44:16.521653: step 42480, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 69h:35m:50s remains)
INFO - root - 2017-12-05 20:44:25.060463: step 42490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 71h:18m:36s remains)
INFO - root - 2017-12-05 20:44:33.555216: step 42500, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 66h:57m:31s remains)
2017-12-05 20:44:34.288884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1560974 -4.1601858 -4.1611924 -4.1515222 -4.1441255 -4.1368055 -4.1356807 -4.1404719 -4.140882 -4.1276455 -4.112072 -4.10453 -4.1118784 -4.13416 -4.1627884][-4.1540375 -4.1545806 -4.149076 -4.137723 -4.132452 -4.1298671 -4.13545 -4.1497207 -4.1569505 -4.1438189 -4.1222515 -4.1050835 -4.0992775 -4.1127439 -4.139751][-4.1661711 -4.1610765 -4.1507592 -4.1439471 -4.1453638 -4.147059 -4.15563 -4.1710858 -4.1770163 -4.1619391 -4.1411052 -4.1251264 -4.1106167 -4.1081414 -4.1231604][-4.1779485 -4.170577 -4.1582441 -4.1567264 -4.1617341 -4.166708 -4.1792459 -4.1976728 -4.2024865 -4.1853952 -4.1627545 -4.1478581 -4.1295996 -4.1136341 -4.1138592][-4.1808038 -4.1704721 -4.1561012 -4.1526117 -4.1535211 -4.1572003 -4.1730928 -4.1975074 -4.2074194 -4.19161 -4.1639538 -4.1463709 -4.1281838 -4.10963 -4.1036835][-4.1754045 -4.1605215 -4.1422825 -4.1336946 -4.1292992 -4.128768 -4.1417069 -4.1662707 -4.1771317 -4.160965 -4.1310086 -4.114265 -4.107336 -4.1042442 -4.10402][-4.1807165 -4.1635151 -4.1410732 -4.1302834 -4.1259561 -4.1207304 -4.122108 -4.135942 -4.1398368 -4.1225123 -4.0987444 -4.0900192 -4.0984349 -4.1152072 -4.1249194][-4.2079229 -4.197865 -4.1783204 -4.1687322 -4.1642675 -4.153852 -4.1424866 -4.1395288 -4.1334772 -4.1170092 -4.1029115 -4.102911 -4.1186185 -4.1405077 -4.1510334][-4.2401767 -4.2378407 -4.225801 -4.2177653 -4.2108874 -4.2004967 -4.18599 -4.1740203 -4.1605906 -4.1445594 -4.138474 -4.1443081 -4.1571326 -4.1739206 -4.1807513][-4.2669253 -4.2738423 -4.268518 -4.2632623 -4.2564211 -4.2472706 -4.2327218 -4.2175136 -4.2040668 -4.1923342 -4.1935525 -4.2011762 -4.205626 -4.21149 -4.2111354][-4.28185 -4.2937403 -4.2927208 -4.2893696 -4.2831082 -4.2740831 -4.2605639 -4.24586 -4.2373476 -4.2338405 -4.2389174 -4.2454276 -4.2440925 -4.2403984 -4.2319417][-4.2744327 -4.2868752 -4.2890477 -4.2889919 -4.2850385 -4.2782946 -4.2689972 -4.258287 -4.2533832 -4.2532539 -4.259604 -4.265306 -4.2604203 -4.2507782 -4.2397585][-4.2558041 -4.2639413 -4.2638397 -4.2653441 -4.2662854 -4.2643838 -4.2588181 -4.251389 -4.2491856 -4.2520108 -4.2604609 -4.2681761 -4.2650752 -4.2545114 -4.2432613][-4.24202 -4.2455592 -4.2432265 -4.2443314 -4.2478094 -4.2493167 -4.2452507 -4.2391205 -4.2377143 -4.2408876 -4.2477202 -4.2556949 -4.256566 -4.2502117 -4.2431355][-4.2350221 -4.2359166 -4.2321038 -4.2314892 -4.2352509 -4.2386827 -4.2374396 -4.2346435 -4.2344809 -4.2367563 -4.2411933 -4.2476778 -4.250638 -4.2496696 -4.2477531]]...]
INFO - root - 2017-12-05 20:44:42.778841: step 42510, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 68h:04m:24s remains)
INFO - root - 2017-12-05 20:44:51.316442: step 42520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 69h:11m:18s remains)
INFO - root - 2017-12-05 20:44:59.847649: step 42530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 70h:17m:22s remains)
INFO - root - 2017-12-05 20:45:08.490383: step 42540, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 70h:15m:54s remains)
INFO - root - 2017-12-05 20:45:16.967075: step 42550, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 67h:51m:14s remains)
INFO - root - 2017-12-05 20:45:25.448557: step 42560, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 68h:34m:57s remains)
INFO - root - 2017-12-05 20:45:34.032908: step 42570, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 69h:08m:41s remains)
INFO - root - 2017-12-05 20:45:42.429984: step 42580, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 68h:03m:03s remains)
INFO - root - 2017-12-05 20:45:50.994508: step 42590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:52m:05s remains)
INFO - root - 2017-12-05 20:45:59.646456: step 42600, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 69h:43m:23s remains)
2017-12-05 20:46:00.453281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2764482 -4.2758994 -4.2676034 -4.2509208 -4.229733 -4.213881 -4.2089634 -4.2171693 -4.2358685 -4.2580218 -4.2880287 -4.3139663 -4.3227153 -4.3173566 -4.30745][-4.2696486 -4.2642031 -4.2526703 -4.2311621 -4.2019038 -4.1747308 -4.1581745 -4.1588883 -4.1782703 -4.2113671 -4.2565556 -4.2992864 -4.322329 -4.3223796 -4.3127027][-4.2546744 -4.2451434 -4.23111 -4.2060056 -4.1696024 -4.1304512 -4.1009245 -4.0931754 -4.1153374 -4.1604114 -4.2141004 -4.270586 -4.3091989 -4.319313 -4.3128505][-4.2336059 -4.2218504 -4.2055731 -4.1801362 -4.1411381 -4.0915475 -4.046927 -4.0278668 -4.0515065 -4.1082873 -4.1757803 -4.2440481 -4.2923141 -4.3110843 -4.3095202][-4.2061338 -4.1956439 -4.1847663 -4.170835 -4.1430688 -4.0905118 -4.0291638 -3.9849274 -3.9908321 -4.0495176 -4.1335282 -4.2153821 -4.2700334 -4.2941451 -4.2990294][-4.1856384 -4.1797004 -4.1739521 -4.17269 -4.1567073 -4.1028328 -4.018487 -3.9357042 -3.9054229 -3.9583082 -4.0641065 -4.1653585 -4.2316637 -4.2643738 -4.2772565][-4.1787558 -4.1748705 -4.1676211 -4.1645908 -4.14975 -4.0938792 -3.9940934 -3.8688638 -3.7944651 -3.848547 -3.9820147 -4.1041546 -4.1853204 -4.2319441 -4.25511][-4.1765914 -4.1691318 -4.15626 -4.1430116 -4.1235256 -4.0722947 -3.9814425 -3.851335 -3.7667019 -3.8214178 -3.9555023 -4.072948 -4.1529179 -4.2089534 -4.2416959][-4.1773653 -4.1648841 -4.1481295 -4.1282 -4.1074414 -4.07414 -4.0167785 -3.9270451 -3.8691709 -3.9058681 -4.0037413 -4.0935497 -4.1572261 -4.2101045 -4.24677][-4.193325 -4.1798086 -4.1581192 -4.1326241 -4.1116033 -4.09839 -4.074542 -4.0317159 -4.0025191 -4.0189915 -4.0772214 -4.139214 -4.1895943 -4.235323 -4.2698154][-4.2159324 -4.2057743 -4.1857796 -4.1601 -4.1410165 -4.1387982 -4.1361456 -4.121757 -4.1141825 -4.1237388 -4.1555686 -4.1944242 -4.2325425 -4.2693563 -4.2959161][-4.2432251 -4.2351913 -4.2201219 -4.2001061 -4.1871805 -4.1916976 -4.1970062 -4.1938791 -4.1969881 -4.2055855 -4.2226777 -4.244401 -4.2685032 -4.2925153 -4.3066854][-4.2680683 -4.2591867 -4.2467709 -4.236042 -4.23196 -4.2394223 -4.2463989 -4.248045 -4.2524738 -4.2578344 -4.2656331 -4.275033 -4.2850652 -4.2963576 -4.3017054][-4.294642 -4.2855058 -4.2741532 -4.2684903 -4.269876 -4.2756476 -4.2810006 -4.2846813 -4.2866459 -4.2871256 -4.2890387 -4.293035 -4.2973704 -4.3022833 -4.3045092][-4.316463 -4.3080144 -4.2973123 -4.2935438 -4.2964849 -4.3011565 -4.3057637 -4.3102551 -4.3113942 -4.310955 -4.3116093 -4.3128567 -4.3126779 -4.3117366 -4.3117847]]...]
INFO - root - 2017-12-05 20:46:08.948605: step 42610, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 71h:15m:51s remains)
INFO - root - 2017-12-05 20:46:17.411856: step 42620, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 68h:41m:25s remains)
INFO - root - 2017-12-05 20:46:25.918699: step 42630, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 67h:18m:04s remains)
INFO - root - 2017-12-05 20:46:34.414859: step 42640, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 70h:09m:15s remains)
INFO - root - 2017-12-05 20:46:42.939327: step 42650, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 69h:16m:13s remains)
INFO - root - 2017-12-05 20:46:51.463164: step 42660, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 70h:07m:03s remains)
INFO - root - 2017-12-05 20:46:59.972917: step 42670, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 66h:16m:50s remains)
INFO - root - 2017-12-05 20:47:08.491278: step 42680, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 66h:00m:21s remains)
INFO - root - 2017-12-05 20:47:17.015400: step 42690, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 68h:51m:53s remains)
INFO - root - 2017-12-05 20:47:25.586140: step 42700, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 69h:58m:37s remains)
2017-12-05 20:47:26.396714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2235322 -4.2029719 -4.1591725 -4.1346126 -4.1426606 -4.1733251 -4.199348 -4.2103071 -4.2167244 -4.216413 -4.2124825 -4.2015219 -4.1809883 -4.1562247 -4.1560664][-4.2224431 -4.1961441 -4.1515193 -4.1312394 -4.146575 -4.181314 -4.204535 -4.2142711 -4.2187705 -4.2111683 -4.200727 -4.1894674 -4.1773038 -4.1668696 -4.178721][-4.2150159 -4.1874828 -4.1525922 -4.1409836 -4.1548491 -4.1804032 -4.1974516 -4.207232 -4.2076206 -4.194459 -4.1871367 -4.1832452 -4.1814566 -4.1830764 -4.1963096][-4.2065735 -4.1811123 -4.1621537 -4.1598773 -4.1690078 -4.1844769 -4.19223 -4.1968431 -4.1901965 -4.1731353 -4.1717615 -4.1806703 -4.1870856 -4.19105 -4.194582][-4.1964664 -4.1680546 -4.1562133 -4.1590233 -4.16239 -4.1682415 -4.1640611 -4.1556149 -4.1439276 -4.1297474 -4.1412425 -4.1645927 -4.1790519 -4.1810317 -4.1761947][-4.1834211 -4.1545839 -4.1430783 -4.145462 -4.1417294 -4.1365228 -4.1212296 -4.0965676 -4.0831871 -4.082777 -4.1144638 -4.1524048 -4.1718397 -4.1658268 -4.1521878][-4.1742353 -4.1542077 -4.1467795 -4.1485691 -4.1408329 -4.1296825 -4.1098132 -4.0843821 -4.0793376 -4.0886669 -4.124496 -4.160109 -4.1724391 -4.1538563 -4.1332459][-4.1777048 -4.1671257 -4.1637635 -4.1678433 -4.1600041 -4.1486993 -4.1312504 -4.1180677 -4.12467 -4.1352043 -4.1575794 -4.1732435 -4.1648259 -4.1349874 -4.1129966][-4.1880307 -4.1817074 -4.1806364 -4.18533 -4.1801615 -4.1727629 -4.1601353 -4.1538363 -4.1647725 -4.1717048 -4.1726408 -4.1639161 -4.1385174 -4.108295 -4.0968361][-4.207972 -4.2033596 -4.2043343 -4.2084489 -4.2061915 -4.2035356 -4.1960955 -4.1883206 -4.1886511 -4.1797934 -4.1632118 -4.1388121 -4.1117358 -4.0943184 -4.0994587][-4.2268977 -4.22645 -4.2308283 -4.2341323 -4.2322884 -4.23059 -4.22401 -4.2095151 -4.1952729 -4.1721783 -4.1460304 -4.1208177 -4.1028805 -4.1042871 -4.1192627][-4.2376475 -4.2401848 -4.2452655 -4.2479019 -4.2469563 -4.2448249 -4.2363415 -4.2146955 -4.1888142 -4.1592584 -4.1310081 -4.1082892 -4.1032472 -4.1147728 -4.1309948][-4.2453041 -4.2484803 -4.2536292 -4.2555742 -4.252306 -4.2462392 -4.2309756 -4.2051997 -4.1769476 -4.1491513 -4.127284 -4.1101627 -4.1090083 -4.1217928 -4.1355863][-4.2475767 -4.2536006 -4.2587833 -4.2573562 -4.2481489 -4.2364588 -4.2180705 -4.19574 -4.17604 -4.1617947 -4.1514745 -4.141366 -4.1390324 -4.1468315 -4.1530709][-4.2358618 -4.2481513 -4.2588673 -4.2598062 -4.25011 -4.2365222 -4.2170897 -4.1992922 -4.1932735 -4.1932907 -4.1930184 -4.18833 -4.184113 -4.1841269 -4.1805634]]...]
INFO - root - 2017-12-05 20:47:34.927118: step 42710, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 68h:30m:07s remains)
INFO - root - 2017-12-05 20:47:43.320352: step 42720, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.797 sec/batch; 64h:09m:39s remains)
INFO - root - 2017-12-05 20:47:51.829125: step 42730, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 70h:06m:05s remains)
INFO - root - 2017-12-05 20:48:00.451151: step 42740, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:49m:47s remains)
INFO - root - 2017-12-05 20:48:09.065912: step 42750, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 70h:40m:58s remains)
INFO - root - 2017-12-05 20:48:17.629950: step 42760, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 68h:23m:10s remains)
INFO - root - 2017-12-05 20:48:26.197493: step 42770, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:50m:57s remains)
INFO - root - 2017-12-05 20:48:34.750430: step 42780, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 69h:36m:40s remains)
INFO - root - 2017-12-05 20:48:43.278567: step 42790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 68h:27m:30s remains)
INFO - root - 2017-12-05 20:48:51.770997: step 42800, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 69h:20m:11s remains)
2017-12-05 20:48:52.526696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1597033 -4.1694326 -4.173831 -4.1738248 -4.1744194 -4.1713209 -4.1643066 -4.1672659 -4.1738415 -4.1727009 -4.1665797 -4.1589141 -4.1539588 -4.1576 -4.1730981][-4.1880388 -4.1966043 -4.2022 -4.2023368 -4.2033157 -4.198966 -4.1797009 -4.1646976 -4.156858 -4.145092 -4.1350403 -4.1331806 -4.1383305 -4.1499104 -4.1731968][-4.2151785 -4.2203884 -4.2211337 -4.2179208 -4.2121911 -4.1967068 -4.1626959 -4.1292863 -4.1106176 -4.09064 -4.0829134 -4.0964093 -4.1171937 -4.1368775 -4.1652632][-4.2244277 -4.2275734 -4.22269 -4.2116752 -4.1928644 -4.1610508 -4.1116662 -4.0664759 -4.0515852 -4.0447359 -4.0528259 -4.0872583 -4.1186185 -4.1372356 -4.1599145][-4.2200222 -4.2199678 -4.2087107 -4.187324 -4.1593766 -4.1244383 -4.0799427 -4.0434666 -4.0443125 -4.0519958 -4.0652952 -4.1010518 -4.1294913 -4.1425543 -4.1574035][-4.2137694 -4.2099857 -4.1960487 -4.1695189 -4.1415877 -4.1193419 -4.0959344 -4.073246 -4.0774846 -4.0857739 -4.0948968 -4.12281 -4.1427112 -4.1479063 -4.1580234][-4.2179332 -4.2067652 -4.1889462 -4.1595097 -4.1372995 -4.1311607 -4.1254821 -4.11434 -4.11826 -4.1257339 -4.1332216 -4.152071 -4.1624384 -4.1624212 -4.1702156][-4.2286253 -4.2099509 -4.1876092 -4.161788 -4.1520519 -4.157578 -4.1623263 -4.1601133 -4.1623116 -4.170136 -4.1766653 -4.1883535 -4.1930647 -4.1904564 -4.1948338][-4.2349386 -4.2203231 -4.2026849 -4.1876078 -4.1880336 -4.1983757 -4.2044129 -4.2026515 -4.2054577 -4.2146144 -4.2200584 -4.2276669 -4.230927 -4.2271204 -4.2254171][-4.2357693 -4.2294221 -4.2181721 -4.2118216 -4.2161574 -4.2270594 -4.2355337 -4.2383757 -4.2413993 -4.2511687 -4.2570167 -4.262444 -4.2628708 -4.2584252 -4.2529774][-4.2289681 -4.2313886 -4.2267547 -4.2301855 -4.2429276 -4.2596083 -4.2716289 -4.27486 -4.27319 -4.276104 -4.278954 -4.283957 -4.2862086 -4.2840238 -4.279139][-4.2318735 -4.241725 -4.2429214 -4.2538095 -4.27209 -4.2906103 -4.3004479 -4.3002658 -4.293263 -4.2897844 -4.2889867 -4.2953644 -4.3021469 -4.3035622 -4.3008118][-4.25158 -4.2644558 -4.2680321 -4.2786469 -4.29465 -4.3088374 -4.3137994 -4.3108497 -4.3011131 -4.2941937 -4.2943497 -4.3034134 -4.31494 -4.3197455 -4.3188992][-4.2716427 -4.283618 -4.2864056 -4.2942905 -4.3053989 -4.3154407 -4.31797 -4.3154297 -4.3078451 -4.3010397 -4.3030615 -4.3137116 -4.3266344 -4.3329968 -4.3327465][-4.2868409 -4.2921586 -4.2928572 -4.2983246 -4.3070893 -4.3163524 -4.3210425 -4.3219657 -4.3183546 -4.3135262 -4.316956 -4.3264413 -4.33698 -4.343132 -4.3427982]]...]
INFO - root - 2017-12-05 20:49:01.074449: step 42810, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:11m:36s remains)
INFO - root - 2017-12-05 20:49:09.565652: step 42820, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 68h:53m:49s remains)
INFO - root - 2017-12-05 20:49:18.187518: step 42830, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 69h:05m:10s remains)
INFO - root - 2017-12-05 20:49:26.774359: step 42840, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 67h:30m:01s remains)
INFO - root - 2017-12-05 20:49:35.259077: step 42850, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 70h:35m:51s remains)
INFO - root - 2017-12-05 20:49:43.853594: step 42860, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 66h:34m:46s remains)
INFO - root - 2017-12-05 20:49:52.360486: step 42870, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 69h:41m:28s remains)
INFO - root - 2017-12-05 20:50:00.899801: step 42880, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 67h:53m:15s remains)
INFO - root - 2017-12-05 20:50:09.487198: step 42890, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 67h:44m:52s remains)
INFO - root - 2017-12-05 20:50:17.973342: step 42900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:14m:05s remains)
2017-12-05 20:50:18.680171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2918067 -4.2957325 -4.3012891 -4.302793 -4.3012409 -4.2988887 -4.296977 -4.2957873 -4.2958727 -4.2952991 -4.295074 -4.2955022 -4.2935138 -4.2906823 -4.2909641][-4.2876081 -4.287178 -4.2915745 -4.2931213 -4.2909904 -4.2878246 -4.283464 -4.2785974 -4.2750053 -4.2709184 -4.2681365 -4.2657332 -4.2619538 -4.2588048 -4.2582307][-4.26217 -4.2564726 -4.2597809 -4.2641096 -4.2645431 -4.2625594 -4.2562895 -4.2463789 -4.2380548 -4.23014 -4.2248187 -4.2208524 -4.2190962 -4.2190032 -4.2196684][-4.2133951 -4.2061553 -4.2136173 -4.2263956 -4.2336464 -4.2346973 -4.2265968 -4.2113428 -4.1968889 -4.181685 -4.1710277 -4.1672873 -4.1726971 -4.1795611 -4.1858635][-4.1378355 -4.1284742 -4.1408219 -4.1612515 -4.1738362 -4.1772842 -4.1677504 -4.1472588 -4.1256762 -4.1054006 -4.0950594 -4.0975146 -4.112977 -4.1294732 -4.1441879][-4.0522623 -4.0393972 -4.0505452 -4.0711074 -4.0854421 -4.0843916 -4.0663271 -4.0399618 -4.0197611 -4.01119 -4.0195551 -4.0393467 -4.0649438 -4.08255 -4.0945239][-4.0195761 -4.0046387 -4.0085478 -4.0226011 -4.0320191 -4.0183458 -3.9862037 -3.9525557 -3.9441228 -3.9620829 -3.9987326 -4.0361233 -4.06719 -4.0793214 -4.0762324][-4.0795908 -4.0640278 -4.0596757 -4.0673532 -4.0722423 -4.0528636 -4.0177455 -3.9857726 -3.9878538 -4.0197592 -4.0640759 -4.1011844 -4.125082 -4.1305089 -4.117661][-4.158555 -4.1496539 -4.1438103 -4.146307 -4.1469288 -4.1304045 -4.1037865 -4.0824704 -4.0888004 -4.1152878 -4.1480894 -4.1719866 -4.1833239 -4.1815019 -4.1668992][-4.204464 -4.2030578 -4.1993561 -4.198503 -4.1965895 -4.1863828 -4.1719751 -4.1615725 -4.1667709 -4.1837883 -4.2030826 -4.2134418 -4.2135668 -4.2061415 -4.1936417][-4.2214694 -4.22191 -4.2210836 -4.2198696 -4.2203431 -4.2174163 -4.2122159 -4.2077289 -4.2110047 -4.2189317 -4.2260208 -4.2267871 -4.2224874 -4.2156758 -4.2063894][-4.2437162 -4.2426262 -4.243259 -4.2450976 -4.2483339 -4.2500072 -4.2501621 -4.2486553 -4.2493005 -4.2511144 -4.2508049 -4.2470241 -4.241241 -4.2357812 -4.2307496][-4.271132 -4.26923 -4.2702222 -4.2722898 -4.2756066 -4.2779446 -4.2789788 -4.2780867 -4.2781382 -4.2781448 -4.277051 -4.2737732 -4.2706733 -4.2682405 -4.2665458][-4.280962 -4.2801056 -4.2814465 -4.2826748 -4.2844524 -4.2860913 -4.2875333 -4.2885323 -4.2897077 -4.2907925 -4.2909913 -4.2899475 -4.2893257 -4.2894449 -4.2894855][-4.2971168 -4.2977982 -4.2989922 -4.2994895 -4.3006511 -4.3018928 -4.3029184 -4.3037758 -4.3046584 -4.3055696 -4.3058977 -4.3058414 -4.3062129 -4.3068361 -4.3071766]]...]
INFO - root - 2017-12-05 20:50:27.202118: step 42910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 68h:50m:28s remains)
INFO - root - 2017-12-05 20:50:35.751991: step 42920, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 69h:48m:13s remains)
INFO - root - 2017-12-05 20:50:44.400450: step 42930, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:11m:59s remains)
INFO - root - 2017-12-05 20:50:52.819547: step 42940, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.837 sec/batch; 67h:21m:44s remains)
INFO - root - 2017-12-05 20:51:01.221987: step 42950, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 66h:20m:43s remains)
INFO - root - 2017-12-05 20:51:09.676793: step 42960, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 70h:17m:00s remains)
INFO - root - 2017-12-05 20:51:18.188759: step 42970, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 67h:46m:07s remains)
INFO - root - 2017-12-05 20:51:26.723622: step 42980, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 69h:01m:00s remains)
INFO - root - 2017-12-05 20:51:35.277093: step 42990, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 66h:29m:30s remains)
INFO - root - 2017-12-05 20:51:43.855381: step 43000, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 68h:34m:04s remains)
2017-12-05 20:51:44.631641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3094721 -4.2983241 -4.2836256 -4.2685695 -4.2574735 -4.2604518 -4.2722559 -4.2843695 -4.2832775 -4.2720871 -4.2536578 -4.231102 -4.2027979 -4.1799083 -4.1894045][-4.2992783 -4.285882 -4.2666974 -4.24559 -4.2276263 -4.2240005 -4.2339754 -4.2496624 -4.2520032 -4.2402544 -4.2168131 -4.1824422 -4.1392264 -4.1075416 -4.1232071][-4.2817836 -4.2665639 -4.2495952 -4.230722 -4.2096796 -4.1989694 -4.2035604 -4.2162843 -4.2210722 -4.2112069 -4.1880741 -4.1467419 -4.0951662 -4.0616875 -4.0872335][-4.2646828 -4.2538347 -4.2435455 -4.2294793 -4.2054777 -4.1872063 -4.1822209 -4.1854348 -4.1895308 -4.1857047 -4.1707311 -4.1356926 -4.0897694 -4.0660357 -4.1033492][-4.2507982 -4.2463889 -4.2378006 -4.2190518 -4.1878748 -4.1588111 -4.1365347 -4.1232181 -4.1330504 -4.1493597 -4.1530948 -4.1354771 -4.1069384 -4.1006513 -4.1480055][-4.2412248 -4.24028 -4.2284784 -4.2038074 -4.1630216 -4.1186996 -4.068593 -4.0275464 -4.040648 -4.087182 -4.1192889 -4.1292591 -4.1271868 -4.1418834 -4.194418][-4.2272029 -4.2272477 -4.2146559 -4.1871319 -4.1405196 -4.0799327 -3.9944334 -3.909059 -3.9132407 -3.9920764 -4.0630627 -4.1078048 -4.1349297 -4.1707311 -4.22644][-4.2157497 -4.214664 -4.2041 -4.17921 -4.1357036 -4.0674105 -3.9594986 -3.8387153 -3.8306398 -3.9287529 -4.0222754 -4.0901079 -4.1395307 -4.1885071 -4.2430615][-4.2189779 -4.2114396 -4.1991954 -4.1773496 -4.1431303 -4.0901618 -4.0043712 -3.9081721 -3.9019907 -3.9733863 -4.0478082 -4.1088181 -4.160182 -4.2100139 -4.25769][-4.2180443 -4.2053266 -4.194047 -4.1794195 -4.1573629 -4.128479 -4.0786357 -4.026546 -4.0329289 -4.074975 -4.1189437 -4.1592283 -4.1976509 -4.237174 -4.2758641][-4.2089353 -4.1941648 -4.1868005 -4.1833019 -4.1735315 -4.1630754 -4.140399 -4.1221776 -4.1392422 -4.1676011 -4.1946239 -4.2193441 -4.2446337 -4.2716074 -4.2993016][-4.1954627 -4.1835117 -4.1794267 -4.1846375 -4.1847858 -4.1873064 -4.1828527 -4.1818371 -4.2009144 -4.2253838 -4.2482681 -4.2669969 -4.284276 -4.3015656 -4.319602][-4.1858649 -4.1779423 -4.1750097 -4.1828742 -4.1922035 -4.2053456 -4.2137146 -4.222713 -4.2429562 -4.2652903 -4.2850819 -4.3004432 -4.3127642 -4.3229795 -4.3340292][-4.196928 -4.1927471 -4.1901364 -4.1983943 -4.2122707 -4.2301764 -4.2468338 -4.26233 -4.2791886 -4.2959075 -4.310606 -4.3216195 -4.329936 -4.33634 -4.3431396][-4.2439361 -4.2433929 -4.24306 -4.24896 -4.2597032 -4.2736778 -4.2874703 -4.3004422 -4.3115816 -4.3212075 -4.3297429 -4.3363333 -4.3413572 -4.345046 -4.3484635]]...]
INFO - root - 2017-12-05 20:51:53.213524: step 43010, loss = 2.04, batch loss = 1.98 (10.0 examples/sec; 0.801 sec/batch; 64h:25m:23s remains)
INFO - root - 2017-12-05 20:52:01.821134: step 43020, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:08m:10s remains)
INFO - root - 2017-12-05 20:52:10.325003: step 43030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:34m:57s remains)
INFO - root - 2017-12-05 20:52:18.831129: step 43040, loss = 2.07, batch loss = 2.02 (10.5 examples/sec; 0.762 sec/batch; 61h:17m:49s remains)
INFO - root - 2017-12-05 20:52:27.263458: step 43050, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 67h:47m:20s remains)
INFO - root - 2017-12-05 20:52:35.874200: step 43060, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.874 sec/batch; 70h:17m:38s remains)
INFO - root - 2017-12-05 20:52:44.393072: step 43070, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 68h:11m:11s remains)
INFO - root - 2017-12-05 20:52:52.975163: step 43080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:05m:11s remains)
INFO - root - 2017-12-05 20:53:01.460814: step 43090, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 67h:01m:39s remains)
INFO - root - 2017-12-05 20:53:10.012742: step 43100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 68h:07m:31s remains)
2017-12-05 20:53:10.783774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0672679 -4.0644293 -4.0817909 -4.1138878 -4.1484613 -4.1806493 -4.1922445 -4.1887693 -4.1822138 -4.1766038 -4.1767926 -4.1861129 -4.1912107 -4.1805377 -4.1714172][-4.0937371 -4.0640888 -4.0462532 -4.0569372 -4.0940156 -4.1291118 -4.1423073 -4.1462483 -4.1534576 -4.1534319 -4.1471214 -4.1562042 -4.1702495 -4.1708364 -4.1658287][-4.1146736 -4.0810933 -4.039463 -4.0154333 -4.0248322 -4.04664 -4.05582 -4.0753651 -4.1050777 -4.1194477 -4.1189628 -4.1308413 -4.1460276 -4.1499496 -4.1497593][-4.1278667 -4.107491 -4.0668011 -4.0195618 -3.9837909 -3.9615688 -3.9599445 -3.9963531 -4.052001 -4.0844126 -4.0968151 -4.119257 -4.1404462 -4.1454391 -4.143991][-4.1277494 -4.12973 -4.107244 -4.0584564 -3.9996862 -3.9346797 -3.9022548 -3.9422128 -4.0116496 -4.0602632 -4.0878725 -4.120904 -4.148334 -4.1557293 -4.1525807][-4.1221666 -4.1416807 -4.1430573 -4.1121144 -4.058713 -3.9825814 -3.9321587 -3.9496715 -4.0010753 -4.0481176 -4.0832109 -4.1259589 -4.1579108 -4.1691933 -4.1688013][-4.1278586 -4.1532345 -4.1676183 -4.1534524 -4.1155028 -4.0521765 -4.0067029 -4.0048575 -4.0237885 -4.0532875 -4.0893846 -4.1389313 -4.17642 -4.1889672 -4.1884422][-4.1487513 -4.1691837 -4.1797423 -4.1704621 -4.1433759 -4.0991139 -4.0673862 -4.0587864 -4.0594 -4.0747819 -4.1070032 -4.1560626 -4.1928945 -4.2045283 -4.199892][-4.1712523 -4.1793065 -4.184762 -4.1742787 -4.1483126 -4.1190429 -4.100193 -4.0904632 -4.0832281 -4.08614 -4.1087818 -4.1553626 -4.193397 -4.2055893 -4.2007937][-4.1831374 -4.1826005 -4.1862216 -4.1754246 -4.1483521 -4.1258931 -4.117126 -4.1075745 -4.089889 -4.0714746 -4.0773425 -4.1240244 -4.1701736 -4.1926751 -4.1943393][-4.20843 -4.2030344 -4.2024179 -4.1886511 -4.163075 -4.1434331 -4.1349568 -4.1225262 -4.090549 -4.0480647 -4.0323772 -4.0676966 -4.1177173 -4.15638 -4.1711941][-4.2443528 -4.2370152 -4.2294164 -4.2099471 -4.1869869 -4.1719475 -4.162478 -4.1463652 -4.105494 -4.0462193 -4.0140891 -4.0297351 -4.0728369 -4.1168857 -4.1430478][-4.2659793 -4.2582021 -4.2493076 -4.2294216 -4.208045 -4.1955709 -4.1887136 -4.1749859 -4.136241 -4.0765076 -4.0440617 -4.046402 -4.0713825 -4.1028261 -4.1259871][-4.2761045 -4.2678227 -4.256453 -4.2352672 -4.2174363 -4.2072959 -4.2014933 -4.1926332 -4.1591973 -4.1104641 -4.0869246 -4.0844932 -4.0958924 -4.110858 -4.1261487][-4.2713943 -4.261394 -4.2451549 -4.2198262 -4.2025819 -4.1983185 -4.1980247 -4.1948938 -4.1692791 -4.1329613 -4.1170745 -4.112987 -4.1176991 -4.1219683 -4.13172]]...]
INFO - root - 2017-12-05 20:53:19.306857: step 43110, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 67h:54m:32s remains)
INFO - root - 2017-12-05 20:53:27.678887: step 43120, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 70h:11m:28s remains)
INFO - root - 2017-12-05 20:53:36.273308: step 43130, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 70h:07m:34s remains)
INFO - root - 2017-12-05 20:53:44.881516: step 43140, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 67h:23m:01s remains)
INFO - root - 2017-12-05 20:53:53.253795: step 43150, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 69h:07m:45s remains)
INFO - root - 2017-12-05 20:54:01.810891: step 43160, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 67h:35m:53s remains)
INFO - root - 2017-12-05 20:54:10.310324: step 43170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 69h:03m:54s remains)
INFO - root - 2017-12-05 20:54:18.767508: step 43180, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 68h:48m:07s remains)
INFO - root - 2017-12-05 20:54:27.319502: step 43190, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.837 sec/batch; 67h:14m:47s remains)
INFO - root - 2017-12-05 20:54:35.694193: step 43200, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 64h:52m:19s remains)
2017-12-05 20:54:36.487589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3190184 -4.31164 -4.3016872 -4.29411 -4.290885 -4.2954879 -4.3054428 -4.3150573 -4.3208408 -4.3247042 -4.3284535 -4.3322282 -4.3348475 -4.335247 -4.3330584][-4.2923079 -4.2805023 -4.2656507 -4.2540932 -4.2501073 -4.2571268 -4.2702246 -4.2831879 -4.2920027 -4.2984848 -4.3043485 -4.3113189 -4.318049 -4.3209558 -4.3186736][-4.2651567 -4.2470455 -4.2258916 -4.2074289 -4.1989946 -4.2055836 -4.2192473 -4.2359924 -4.2504916 -4.2615829 -4.270185 -4.281424 -4.2956905 -4.3043675 -4.3037519][-4.2428007 -4.2177005 -4.1897688 -4.16526 -4.1502481 -4.1475582 -4.1537423 -4.1723619 -4.1947083 -4.2131948 -4.2266736 -4.2429786 -4.2667971 -4.2843351 -4.2886624][-4.2251053 -4.1953697 -4.1620078 -4.1353426 -4.1145072 -4.0963645 -4.0880718 -4.1076322 -4.1389513 -4.1644835 -4.1828432 -4.2037187 -4.2353668 -4.2621536 -4.2737513][-4.2101669 -4.1764278 -4.1363239 -4.1044 -4.0740042 -4.0381985 -4.01746 -4.0438223 -4.0875278 -4.1187987 -4.1410937 -4.1664066 -4.2019644 -4.2359304 -4.2552075][-4.2010956 -4.1682315 -4.1283374 -4.0911436 -4.0446377 -3.985153 -3.9497075 -3.983855 -4.0398169 -4.079689 -4.1100974 -4.1414485 -4.1792593 -4.2168217 -4.243053][-4.207211 -4.181622 -4.1457043 -4.104116 -4.0436263 -3.9659083 -3.9172854 -3.9512699 -4.0145607 -4.0634127 -4.1027231 -4.1395521 -4.1785841 -4.2149119 -4.2431827][-4.207087 -4.1924548 -4.1643867 -4.1249661 -4.0646505 -3.9889362 -3.9403961 -3.9610887 -4.0160179 -4.0691166 -4.1148543 -4.15465 -4.19265 -4.2248363 -4.2509465][-4.1930041 -4.1931543 -4.1783648 -4.1468863 -4.0917926 -4.02703 -3.9855411 -3.9905438 -4.0292358 -4.0837502 -4.1338758 -4.1764927 -4.214148 -4.2427254 -4.2647719][-4.1666145 -4.1791234 -4.1834278 -4.1691484 -4.1278758 -4.0743713 -4.0349555 -4.0215707 -4.0400825 -4.0942459 -4.1507816 -4.1976008 -4.2386227 -4.2678781 -4.2875991][-4.1454458 -4.163434 -4.183784 -4.1883063 -4.1632743 -4.11878 -4.076509 -4.0468354 -4.0488067 -4.099071 -4.1598034 -4.2104979 -4.2552538 -4.2873354 -4.3074775][-4.1422668 -4.1592851 -4.1866479 -4.2048092 -4.1928344 -4.1557055 -4.113935 -4.0769138 -4.0656447 -4.1034827 -4.1588635 -4.2090969 -4.2550826 -4.2916355 -4.315486][-4.1529188 -4.1688414 -4.1965547 -4.2172737 -4.2112865 -4.1838317 -4.1490564 -4.1136427 -4.0956416 -4.1185508 -4.1624136 -4.204843 -4.2464466 -4.2834158 -4.310348][-4.1741624 -4.1900692 -4.2143846 -4.2319312 -4.2295008 -4.2101493 -4.1841903 -4.1569543 -4.1402507 -4.1501613 -4.1771746 -4.2053523 -4.237536 -4.2706623 -4.2977905]]...]
INFO - root - 2017-12-05 20:54:44.951770: step 43210, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 66h:33m:48s remains)
INFO - root - 2017-12-05 20:54:53.442961: step 43220, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 67h:43m:51s remains)
INFO - root - 2017-12-05 20:55:02.008642: step 43230, loss = 2.06, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 72h:09m:12s remains)
INFO - root - 2017-12-05 20:55:10.423781: step 43240, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 67h:13m:45s remains)
INFO - root - 2017-12-05 20:55:18.904683: step 43250, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:44m:10s remains)
INFO - root - 2017-12-05 20:55:27.131345: step 43260, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 68h:20m:55s remains)
INFO - root - 2017-12-05 20:55:35.760797: step 43270, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 68h:48m:49s remains)
INFO - root - 2017-12-05 20:55:44.325569: step 43280, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 67h:33m:13s remains)
INFO - root - 2017-12-05 20:55:52.917510: step 43290, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 71h:23m:34s remains)
INFO - root - 2017-12-05 20:56:01.467700: step 43300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 68h:27m:27s remains)
2017-12-05 20:56:02.239311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1958113 -4.2177238 -4.2227893 -4.2125649 -4.20159 -4.1905928 -4.1865392 -4.1879711 -4.1903644 -4.1829052 -4.1684384 -4.16132 -4.1568384 -4.1359615 -4.1159678][-4.17541 -4.2116303 -4.2296395 -4.2213426 -4.2043328 -4.1845431 -4.1724615 -4.1651707 -4.163435 -4.1558952 -4.1395063 -4.1279564 -4.1241198 -4.108871 -4.099781][-4.1665478 -4.2146053 -4.2464132 -4.2380505 -4.2126555 -4.1824956 -4.1579852 -4.1391678 -4.1377463 -4.1374269 -4.1220841 -4.1093373 -4.104495 -4.0906219 -4.0847449][-4.1622486 -4.2189245 -4.2573366 -4.243926 -4.2041116 -4.1572313 -4.1171432 -4.0947104 -4.1007123 -4.1101151 -4.102427 -4.09654 -4.0919037 -4.0798807 -4.0722394][-4.16098 -4.2194276 -4.2524314 -4.2270088 -4.1717167 -4.1086779 -4.0545511 -4.0267997 -4.0371323 -4.0609818 -4.0727735 -4.0807304 -4.0808716 -4.0695839 -4.0571189][-4.164372 -4.2131529 -4.2286749 -4.1891437 -4.1221838 -4.0446706 -3.9722667 -3.9339004 -3.95366 -4.0025239 -4.0430613 -4.0727 -4.08716 -4.07535 -4.0515475][-4.1611748 -4.1958313 -4.1927476 -4.1391025 -4.0620561 -3.9672828 -3.8633211 -3.8094928 -3.8554118 -3.9454553 -4.0192556 -4.0691772 -4.098361 -4.0926723 -4.0685277][-4.16142 -4.1823435 -4.1658459 -4.104764 -4.0272655 -3.9240901 -3.7953613 -3.728147 -3.8009982 -3.9213641 -4.013279 -4.0677323 -4.1026211 -4.1096387 -4.0957031][-4.174901 -4.1875992 -4.1680579 -4.1113739 -4.0471234 -3.9647248 -3.8640876 -3.816802 -3.8709519 -3.9610145 -4.0302439 -4.0711188 -4.1043425 -4.1230216 -4.1216178][-4.1934257 -4.1988654 -4.17957 -4.1332464 -4.0846295 -4.0314751 -3.9760826 -3.9542732 -3.9805934 -4.0271525 -4.0595536 -4.0805173 -4.1097813 -4.1321831 -4.1349497][-4.20135 -4.20115 -4.1863728 -4.153904 -4.1218185 -4.090569 -4.06248 -4.0515184 -4.059062 -4.0741272 -4.0832734 -4.09474 -4.1187034 -4.1413288 -4.1433721][-4.1999645 -4.2035422 -4.1952391 -4.1725078 -4.1509633 -4.1345348 -4.1218882 -4.1123819 -4.1071477 -4.1011333 -4.0982952 -4.1008797 -4.11594 -4.1385059 -4.1369691][-4.1898117 -4.2018871 -4.20225 -4.1861615 -4.1711745 -4.167325 -4.1615553 -4.1514258 -4.1366205 -4.1171584 -4.1005244 -4.0861135 -4.0874977 -4.1066794 -4.1041279][-4.179337 -4.1991224 -4.2050052 -4.1915283 -4.1849232 -4.1876874 -4.1838174 -4.1721287 -4.1494665 -4.12181 -4.0927973 -4.0594549 -4.0474544 -4.063354 -4.0581746][-4.1779833 -4.2054162 -4.2146225 -4.1991463 -4.1940174 -4.1995044 -4.19519 -4.1837492 -4.1611385 -4.1339064 -4.0999312 -4.0583444 -4.0454831 -4.0608344 -4.05679]]...]
INFO - root - 2017-12-05 20:56:10.894309: step 43310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 69h:50m:48s remains)
INFO - root - 2017-12-05 20:56:19.325335: step 43320, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.824 sec/batch; 66h:13m:12s remains)
INFO - root - 2017-12-05 20:56:27.923854: step 43330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 68h:50m:49s remains)
INFO - root - 2017-12-05 20:56:36.389947: step 43340, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 68h:59m:16s remains)
INFO - root - 2017-12-05 20:56:44.829659: step 43350, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 66h:19m:41s remains)
INFO - root - 2017-12-05 20:56:53.305095: step 43360, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 68h:20m:53s remains)
INFO - root - 2017-12-05 20:57:01.734796: step 43370, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.870 sec/batch; 69h:50m:38s remains)
INFO - root - 2017-12-05 20:57:10.251844: step 43380, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 65h:25m:10s remains)
INFO - root - 2017-12-05 20:57:18.858865: step 43390, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 70h:01m:09s remains)
INFO - root - 2017-12-05 20:57:27.462187: step 43400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 70h:05m:04s remains)
2017-12-05 20:57:28.241878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0129614 -3.9864237 -3.9932103 -4.0582666 -4.0990486 -4.0739717 -4.0369 -4.03548 -4.0656857 -4.0710168 -4.0600491 -4.0401268 -4.0186238 -4.034152 -4.0700483][-4.0125036 -3.9771731 -3.9846044 -4.055809 -4.0953636 -4.0650687 -4.0248 -4.031105 -4.0758147 -4.0786924 -4.0509057 -4.0147381 -3.9902108 -4.016129 -4.0578823][-4.0252361 -3.9813225 -3.9908428 -4.0603476 -4.0884457 -4.0506825 -4.007585 -4.0188732 -4.0775595 -4.08081 -4.0468006 -4.0020561 -3.9768267 -4.010376 -4.0514][-4.0340004 -3.9826076 -3.9939818 -4.0605292 -4.075428 -4.024334 -3.9718332 -3.9896472 -4.0638928 -4.0771456 -4.0509443 -4.0059824 -3.9828982 -4.0221586 -4.056675][-4.0248075 -3.9683418 -3.9814236 -4.0498857 -4.0522785 -3.9822471 -3.9182947 -3.9494076 -4.0432415 -4.07456 -4.0594897 -4.0194988 -4.0035076 -4.0462685 -4.0726576][-4.0024881 -3.9413238 -3.9585428 -4.0293779 -4.0241513 -3.9312947 -3.8471084 -3.892678 -4.0077419 -4.0603118 -4.0585327 -4.0323119 -4.0258732 -4.0634241 -4.0783944][-3.9943609 -3.9290071 -3.9464414 -4.015646 -3.995646 -3.8754125 -3.7650347 -3.8250804 -3.9629991 -4.0319481 -4.0374985 -4.0161324 -4.0158763 -4.0471234 -4.0547881][-4.0218668 -3.9490066 -3.9614968 -4.0248094 -3.9945819 -3.8607657 -3.7427149 -3.8061843 -3.944937 -4.0141349 -4.013412 -3.9820466 -3.980855 -4.007854 -4.0123754][-4.0685911 -3.9946859 -4.0025215 -4.0607977 -4.0341415 -3.9204559 -3.8305855 -3.8818097 -3.9861395 -4.0327597 -4.0148168 -3.9719193 -3.9680309 -3.9908774 -3.9928393][-4.1115093 -4.0447912 -4.0476642 -4.0971675 -4.0761871 -3.9875178 -3.9317102 -3.9830611 -4.060905 -4.0858374 -4.04908 -4.0012755 -4.0038357 -4.0197 -4.0115123][-4.136322 -4.0726442 -4.0712175 -4.1125093 -4.0972481 -4.0309386 -4.0037146 -4.062151 -4.1232195 -4.1370945 -4.0945063 -4.0519085 -4.0606565 -4.0658736 -4.0457559][-4.1350856 -4.076324 -4.0735893 -4.1148181 -4.1099515 -4.0660119 -4.0553761 -4.1105213 -4.1597438 -4.16996 -4.1297946 -4.09065 -4.0955238 -4.0924964 -4.066617][-4.1183858 -4.0720344 -4.0748143 -4.1167831 -4.1230807 -4.09666 -4.0946026 -4.1412549 -4.1761217 -4.1789646 -4.1432214 -4.1032987 -4.0992117 -4.0877357 -4.0567884][-4.1141462 -4.0817223 -4.0896325 -4.128098 -4.1360245 -4.1190877 -4.1222081 -4.1635752 -4.18771 -4.1827726 -4.1478539 -4.1040468 -4.0895576 -4.0718565 -4.0400081][-4.1196208 -4.103653 -4.1145983 -4.1470394 -4.1506538 -4.1369352 -4.1392293 -4.174273 -4.1915483 -4.1802974 -4.1454983 -4.1007113 -4.0758395 -4.0545964 -4.0336852]]...]
INFO - root - 2017-12-05 20:57:36.702098: step 43410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 69h:05m:07s remains)
INFO - root - 2017-12-05 20:57:45.126701: step 43420, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 67h:53m:14s remains)
INFO - root - 2017-12-05 20:57:53.634724: step 43430, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 68h:27m:04s remains)
INFO - root - 2017-12-05 20:58:02.163862: step 43440, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:54m:18s remains)
INFO - root - 2017-12-05 20:58:10.521960: step 43450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 67h:48m:36s remains)
INFO - root - 2017-12-05 20:58:19.062740: step 43460, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:42m:02s remains)
INFO - root - 2017-12-05 20:58:27.376927: step 43470, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 68h:38m:31s remains)
INFO - root - 2017-12-05 20:58:35.909883: step 43480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:33m:31s remains)
INFO - root - 2017-12-05 20:58:44.403602: step 43490, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 68h:24m:54s remains)
INFO - root - 2017-12-05 20:58:52.899744: step 43500, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 68h:25m:29s remains)
2017-12-05 20:58:53.679282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3081808 -4.3078408 -4.2972631 -4.2884035 -4.2809515 -4.2743039 -4.2708483 -4.2677522 -4.2515368 -4.2251239 -4.2022004 -4.193871 -4.1987252 -4.2123737 -4.2127347][-4.2857537 -4.2833524 -4.2712827 -4.2596016 -4.2494116 -4.2409987 -4.2405882 -4.2441978 -4.2332287 -4.2082114 -4.1842113 -4.1706104 -4.1694193 -4.1821995 -4.1907096][-4.2596664 -4.25173 -4.2363043 -4.2246356 -4.2161736 -4.2122192 -4.2174149 -4.2253618 -4.2188625 -4.1977782 -4.1747394 -4.1551132 -4.1474314 -4.160603 -4.178978][-4.2346954 -4.2195563 -4.2026296 -4.1960635 -4.1959038 -4.1968331 -4.2003961 -4.2032228 -4.19757 -4.1823092 -4.163096 -4.1447186 -4.1361923 -4.1515865 -4.1825409][-4.2109275 -4.1924686 -4.1784062 -4.1774054 -4.1816421 -4.1788483 -4.1696482 -4.1611104 -4.1585245 -4.1514235 -4.1403069 -4.1270676 -4.1203194 -4.1383371 -4.1822643][-4.1913443 -4.1729946 -4.1602497 -4.1592722 -4.1611867 -4.1526375 -4.1316857 -4.1050243 -4.09548 -4.0999622 -4.10172 -4.0984683 -4.0991464 -4.1219783 -4.1744051][-4.17334 -4.1554041 -4.1442375 -4.1404762 -4.137166 -4.1161003 -4.0732117 -4.0111752 -3.9856191 -4.0189509 -4.0529618 -4.0787725 -4.096839 -4.1206832 -4.1665821][-4.1561236 -4.1369128 -4.124733 -4.1186028 -4.1090875 -4.0715623 -4.001524 -3.9054019 -3.8768811 -3.9608266 -4.0409093 -4.0953536 -4.1243362 -4.1425595 -4.1715536][-4.1418595 -4.1252756 -4.1133261 -4.1080661 -4.096724 -4.0541887 -3.9862 -3.9129679 -3.907234 -3.9987454 -4.0791731 -4.1255193 -4.1481032 -4.1611428 -4.1787724][-4.1428275 -4.1300483 -4.1191106 -4.1207108 -4.1152306 -4.0836239 -4.0401034 -4.0119352 -4.0199733 -4.0797744 -4.1316571 -4.1550083 -4.1630964 -4.1737714 -4.1889439][-4.1587553 -4.1484046 -4.1372585 -4.1435666 -4.14237 -4.1222463 -4.0929189 -4.0852256 -4.0990434 -4.1354003 -4.1685967 -4.1787224 -4.1763382 -4.1847067 -4.196136][-4.1708379 -4.1603446 -4.1481628 -4.1518483 -4.1525693 -4.14083 -4.1184869 -4.1159129 -4.1332636 -4.1623921 -4.1857538 -4.1909423 -4.1865354 -4.18875 -4.1962309][-4.171155 -4.1564159 -4.1379108 -4.1337457 -4.1349411 -4.131474 -4.1203 -4.1202526 -4.1418977 -4.1722665 -4.1910605 -4.1975155 -4.19839 -4.1984019 -4.203063][-4.168819 -4.1525517 -4.1279769 -4.117898 -4.1199007 -4.1211953 -4.1165271 -4.1170187 -4.1387529 -4.172771 -4.1911612 -4.2004542 -4.2058625 -4.2071166 -4.21402][-4.1860108 -4.1773553 -4.1559539 -4.1404819 -4.1380043 -4.133677 -4.1254168 -4.1250706 -4.142498 -4.1748843 -4.1942635 -4.2029386 -4.2069378 -4.2093019 -4.2195845]]...]
INFO - root - 2017-12-05 20:59:02.178587: step 43510, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 69h:39m:28s remains)
INFO - root - 2017-12-05 20:59:10.665946: step 43520, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 68h:20m:21s remains)
INFO - root - 2017-12-05 20:59:19.233667: step 43530, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:50m:43s remains)
INFO - root - 2017-12-05 20:59:27.727878: step 43540, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 67h:39m:17s remains)
INFO - root - 2017-12-05 20:59:36.135450: step 43550, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.811 sec/batch; 65h:05m:28s remains)
INFO - root - 2017-12-05 20:59:44.589202: step 43560, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 66h:13m:41s remains)
INFO - root - 2017-12-05 20:59:53.057122: step 43570, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 66h:38m:09s remains)
INFO - root - 2017-12-05 21:00:01.556548: step 43580, loss = 2.01, batch loss = 1.96 (9.0 examples/sec; 0.887 sec/batch; 71h:13m:02s remains)
INFO - root - 2017-12-05 21:00:10.048621: step 43590, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.846 sec/batch; 67h:55m:51s remains)
INFO - root - 2017-12-05 21:00:18.617850: step 43600, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 68h:10m:38s remains)
2017-12-05 21:00:19.366925: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.170877 -4.1887989 -4.1887107 -4.173183 -4.1631837 -4.1620722 -4.1779675 -4.2130938 -4.2388182 -4.2373514 -4.2207446 -4.1935153 -4.1652403 -4.1293855 -4.0679283][-4.1687765 -4.1912613 -4.1950731 -4.1815925 -4.1745071 -4.1741076 -4.1858988 -4.214746 -4.2378125 -4.2441454 -4.23816 -4.2164984 -4.1920371 -4.1656027 -4.1159544][-4.1703262 -4.1968703 -4.2051706 -4.1940055 -4.1859121 -4.1809959 -4.1836686 -4.2015114 -4.221951 -4.2333927 -4.2349429 -4.2180176 -4.1995821 -4.1827645 -4.1466079][-4.1750751 -4.2033844 -4.2086396 -4.1925406 -4.1777105 -4.1624074 -4.15268 -4.15935 -4.1816211 -4.2000861 -4.2113667 -4.2052283 -4.1984572 -4.1947775 -4.1748161][-4.1779351 -4.2023382 -4.1985359 -4.1689157 -4.1381927 -4.1069522 -4.081162 -4.0818586 -4.1162329 -4.1512136 -4.1780357 -4.1907458 -4.2011623 -4.2130208 -4.2105][-4.1780124 -4.1952238 -4.1813693 -4.1348314 -4.0841017 -4.0361547 -3.9949577 -3.9936635 -4.0461926 -4.1039662 -4.1480765 -4.1799283 -4.2066975 -4.2303944 -4.2406926][-4.1813192 -4.1930046 -4.1751485 -4.1222062 -4.0628486 -4.0126581 -3.9698839 -3.9661245 -4.0218334 -4.085258 -4.1333566 -4.1710944 -4.2025757 -4.2280364 -4.245048][-4.191206 -4.2012663 -4.1863661 -4.1397357 -4.0865846 -4.0482221 -4.0181255 -4.0110078 -4.04558 -4.0927448 -4.1317954 -4.16049 -4.1859007 -4.2091632 -4.2320557][-4.2013755 -4.2110329 -4.2028508 -4.1679583 -4.1236439 -4.0947847 -4.0761023 -4.0673079 -4.0818572 -4.1127419 -4.142591 -4.1617942 -4.1786327 -4.1979542 -4.2235446][-4.1978035 -4.2066741 -4.2060957 -4.1844053 -4.1483111 -4.1238585 -4.1082158 -4.0987926 -4.1064773 -4.1312037 -4.1587467 -4.174624 -4.1863246 -4.1986394 -4.2193718][-4.1705813 -4.1802349 -4.1890373 -4.1813617 -4.1591806 -4.1416254 -4.1263609 -4.1155457 -4.1208329 -4.142951 -4.1693206 -4.1856356 -4.1945477 -4.2005091 -4.2127724][-4.1318293 -4.1415696 -4.1583562 -4.1654296 -4.159893 -4.1521449 -4.1419511 -4.1349211 -4.1397481 -4.1567574 -4.1769528 -4.1869669 -4.1908784 -4.1915588 -4.1958175][-4.0872803 -4.1011982 -4.1263652 -4.1488075 -4.159019 -4.1610932 -4.1570649 -4.1526904 -4.1547923 -4.1626978 -4.1720467 -4.1746416 -4.1758237 -4.1760764 -4.1760683][-4.0605612 -4.0801425 -4.1105571 -4.1408763 -4.1571383 -4.16208 -4.1605754 -4.1579375 -4.158586 -4.1605268 -4.1627312 -4.1624475 -4.1642165 -4.1659336 -4.1648173][-4.0779672 -4.1012654 -4.1321578 -4.1615691 -4.1758871 -4.1795163 -4.1783743 -4.1767263 -4.1767783 -4.1756873 -4.1750731 -4.1752987 -4.1792855 -4.1828079 -4.1822996]]...]
INFO - root - 2017-12-05 21:00:27.859238: step 43610, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 70h:34m:04s remains)
INFO - root - 2017-12-05 21:00:36.401586: step 43620, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 67h:41m:49s remains)
INFO - root - 2017-12-05 21:00:44.869519: step 43630, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 67h:46m:07s remains)
INFO - root - 2017-12-05 21:00:53.560029: step 43640, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 67h:48m:19s remains)
INFO - root - 2017-12-05 21:01:01.896051: step 43650, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 67h:29m:31s remains)
INFO - root - 2017-12-05 21:01:10.424159: step 43660, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.835 sec/batch; 66h:59m:03s remains)
INFO - root - 2017-12-05 21:01:18.836269: step 43670, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 69h:37m:53s remains)
INFO - root - 2017-12-05 21:01:27.413736: step 43680, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 68h:00m:11s remains)
INFO - root - 2017-12-05 21:01:35.751073: step 43690, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 67h:51m:10s remains)
INFO - root - 2017-12-05 21:01:44.282642: step 43700, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 67h:04m:45s remains)
2017-12-05 21:01:45.072209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2947111 -4.2670217 -4.2475452 -4.2349968 -4.2249765 -4.2013655 -4.1797657 -4.1811924 -4.1785746 -4.1761475 -4.1718922 -4.1649289 -4.1650586 -4.1686821 -4.1731186][-4.2895174 -4.2590027 -4.2373233 -4.2253904 -4.2190056 -4.1971264 -4.1751161 -4.1737776 -4.1674337 -4.1651125 -4.1671915 -4.1647305 -4.1653442 -4.1765089 -4.1912818][-4.2870712 -4.2529788 -4.2278638 -4.2186093 -4.217607 -4.1997471 -4.1732445 -4.1644273 -4.1534677 -4.1555171 -4.1655941 -4.1682305 -4.1725144 -4.188386 -4.2075019][-4.2854452 -4.24845 -4.2223949 -4.2180982 -4.2220273 -4.2064629 -4.1756682 -4.1567373 -4.1412182 -4.1503572 -4.1660862 -4.1745806 -4.1808057 -4.1950045 -4.2106934][-4.286397 -4.2509418 -4.2284961 -4.2269106 -4.23082 -4.2133312 -4.1820631 -4.1513658 -4.1314054 -4.1450486 -4.1634336 -4.1717892 -4.1772513 -4.1893139 -4.2035155][-4.2885537 -4.260056 -4.2407694 -4.2351146 -4.2312894 -4.2092552 -4.1767454 -4.131938 -4.0999627 -4.10913 -4.1249962 -4.1371622 -4.1502924 -4.1705875 -4.1981559][-4.2903352 -4.2690387 -4.2502952 -4.2343674 -4.2186847 -4.1822662 -4.1308117 -4.0597057 -4.008491 -4.0186195 -4.0493116 -4.0797496 -4.1101933 -4.1441393 -4.1902003][-4.2941618 -4.2762 -4.251761 -4.2214441 -4.1876664 -4.1262684 -4.0427551 -3.9390087 -3.8779626 -3.9168527 -3.9846611 -4.0450783 -4.0929694 -4.1337366 -4.1837726][-4.2910924 -4.2723746 -4.2396069 -4.1952829 -4.1421947 -4.0619154 -3.9596007 -3.8532469 -3.8188009 -3.89961 -3.992517 -4.0571537 -4.102541 -4.1380959 -4.182848][-4.2812114 -4.2587771 -4.2202826 -4.1701255 -4.1138258 -4.0364375 -3.9568479 -3.8980942 -3.9076865 -3.989737 -4.0566578 -4.0894685 -4.1153007 -4.1428342 -4.1818824][-4.2755809 -4.2525716 -4.2140112 -4.1649895 -4.1158862 -4.0569916 -4.016139 -3.999856 -4.01778 -4.0694289 -4.1007357 -4.1022105 -4.108232 -4.1288848 -4.1672812][-4.2769608 -4.253695 -4.2165375 -4.1704426 -4.1299624 -4.0877681 -4.0624456 -4.049983 -4.0514388 -4.0741816 -4.0879245 -4.0784597 -4.0734348 -4.0938339 -4.1388459][-4.2847333 -4.2598124 -4.2242923 -4.1780796 -4.1367598 -4.0979614 -4.0702968 -4.0488524 -4.03018 -4.0423932 -4.0602975 -4.0552082 -4.0468154 -4.0720329 -4.1226273][-4.2913542 -4.2650084 -4.2302566 -4.1807694 -4.1370916 -4.0993361 -4.0655818 -4.0360279 -4.004972 -4.0219555 -4.0561905 -4.06606 -4.0625567 -4.0838895 -4.1277823][-4.2960176 -4.2684655 -4.2347174 -4.1854844 -4.1414866 -4.1051087 -4.0745664 -4.0491929 -4.0207443 -4.0430036 -4.0825272 -4.1056771 -4.1090293 -4.117064 -4.14214]]...]
INFO - root - 2017-12-05 21:01:53.620127: step 43710, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 67h:44m:50s remains)
INFO - root - 2017-12-05 21:02:02.185067: step 43720, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 70h:50m:29s remains)
INFO - root - 2017-12-05 21:02:10.692860: step 43730, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.838 sec/batch; 67h:12m:12s remains)
INFO - root - 2017-12-05 21:02:19.281599: step 43740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 69h:21m:29s remains)
INFO - root - 2017-12-05 21:02:27.741220: step 43750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 68h:28m:30s remains)
INFO - root - 2017-12-05 21:02:36.253644: step 43760, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 66h:56m:50s remains)
INFO - root - 2017-12-05 21:02:44.796684: step 43770, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 67h:17m:32s remains)
INFO - root - 2017-12-05 21:02:53.285337: step 43780, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 70h:19m:46s remains)
INFO - root - 2017-12-05 21:03:01.745802: step 43790, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.841 sec/batch; 67h:24m:21s remains)
INFO - root - 2017-12-05 21:03:10.288430: step 43800, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 67h:12m:21s remains)
2017-12-05 21:03:11.042649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.31604 -4.3196239 -4.3205829 -4.3232932 -4.3253751 -4.3271089 -4.3284492 -4.3284793 -4.3254986 -4.3201804 -4.3056045 -4.2850156 -4.265007 -4.2422991 -4.2152061][-4.3217692 -4.3244886 -4.3247871 -4.3262792 -4.327548 -4.3278961 -4.327498 -4.3266883 -4.3228412 -4.3164363 -4.3029919 -4.2841229 -4.264781 -4.2406907 -4.2094064][-4.3178124 -4.3198094 -4.3169332 -4.3169932 -4.3181591 -4.3169718 -4.3155074 -4.3156786 -4.310823 -4.2999907 -4.2859259 -4.272428 -4.2583227 -4.2425513 -4.2201686][-4.3012776 -4.3037362 -4.2979741 -4.2963805 -4.2951627 -4.2896371 -4.2848525 -4.285902 -4.2822719 -4.2669282 -4.2558346 -4.2540784 -4.25173 -4.2521052 -4.2455797][-4.2815003 -4.285347 -4.2773485 -4.2733717 -4.2671247 -4.2564278 -4.245739 -4.2441306 -4.2358084 -4.2125831 -4.205708 -4.2137356 -4.2225714 -4.2387691 -4.2457604][-4.2635679 -4.2632642 -4.2498026 -4.2410979 -4.2292962 -4.2139211 -4.1970611 -4.1909652 -4.1768894 -4.1466665 -4.1444845 -4.1633844 -4.1809521 -4.2070894 -4.2244749][-4.240489 -4.2287712 -4.2043004 -4.185637 -4.1651573 -4.13525 -4.1063452 -4.0951462 -4.0815716 -4.053751 -4.0579295 -4.08965 -4.1215849 -4.1564503 -4.1792717][-4.20595 -4.182498 -4.1441059 -4.1109548 -4.0778913 -4.0265613 -3.977191 -3.9564753 -3.9492929 -3.9375668 -3.9602597 -4.0133691 -4.0634952 -4.1081114 -4.13438][-4.1792707 -4.1499543 -4.1026344 -4.0569191 -4.0088644 -3.9392283 -3.8781929 -3.861304 -3.8765621 -3.8941376 -3.941504 -4.0149918 -4.0736089 -4.1146016 -4.1316376][-4.1729679 -4.148562 -4.1047559 -4.0619497 -4.0148411 -3.9536915 -3.9101682 -3.9118836 -3.9423225 -3.9735248 -4.0248575 -4.0890374 -4.1379209 -4.1653471 -4.1730714][-4.1899629 -4.176765 -4.14705 -4.1150417 -4.0800519 -4.0423222 -4.0266643 -4.0421267 -4.0732675 -4.10396 -4.1457767 -4.1915746 -4.2229328 -4.2356977 -4.23583][-4.2126136 -4.2114062 -4.1961727 -4.1764555 -4.1569543 -4.1407595 -4.1425691 -4.1614933 -4.1885552 -4.2143426 -4.2432752 -4.2723422 -4.2900534 -4.2957206 -4.2932038][-4.2346811 -4.2485919 -4.2486806 -4.2420311 -4.2347069 -4.2319894 -4.2374649 -4.24926 -4.2651482 -4.2801709 -4.296669 -4.3111362 -4.3176141 -4.3179688 -4.3159914][-4.2560163 -4.2771335 -4.2848744 -4.2872343 -4.2864556 -4.2889276 -4.2922421 -4.2947011 -4.2985034 -4.3046513 -4.3138046 -4.3215728 -4.3243256 -4.3229451 -4.3205786][-4.2810316 -4.3017812 -4.3061509 -4.3060188 -4.30375 -4.3041945 -4.30341 -4.3010616 -4.3024321 -4.3074379 -4.314496 -4.3198128 -4.3225307 -4.3222437 -4.32099]]...]
INFO - root - 2017-12-05 21:03:19.514802: step 43810, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.836 sec/batch; 67h:01m:27s remains)
INFO - root - 2017-12-05 21:03:28.084194: step 43820, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 69h:57m:52s remains)
INFO - root - 2017-12-05 21:03:36.553971: step 43830, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:53m:36s remains)
INFO - root - 2017-12-05 21:03:45.103584: step 43840, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 68h:38m:45s remains)
INFO - root - 2017-12-05 21:03:53.479615: step 43850, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 69h:06m:24s remains)
INFO - root - 2017-12-05 21:04:01.967799: step 43860, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 67h:19m:19s remains)
INFO - root - 2017-12-05 21:04:10.500764: step 43870, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 65h:20m:59s remains)
INFO - root - 2017-12-05 21:04:19.073461: step 43880, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-05 21:04:27.601390: step 43890, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.884 sec/batch; 70h:51m:21s remains)
INFO - root - 2017-12-05 21:04:36.031952: step 43900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 67h:55m:16s remains)
2017-12-05 21:04:36.820569: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2431626 -4.2380261 -4.2308769 -4.2134724 -4.1975765 -4.1889949 -4.1868386 -4.1848593 -4.1875896 -4.191957 -4.193274 -4.1908174 -4.1868825 -4.1792412 -4.1728354][-4.2349205 -4.2291808 -4.2200632 -4.1963224 -4.1712508 -4.1589346 -4.1542916 -4.14819 -4.1520777 -4.1665363 -4.1766305 -4.1809416 -4.1791596 -4.1688051 -4.1592093][-4.2252226 -4.2198772 -4.2085242 -4.1803608 -4.146915 -4.1287985 -4.1176472 -4.1078143 -4.1174812 -4.1460862 -4.1720686 -4.1857047 -4.1872869 -4.1750822 -4.1633782][-4.2061267 -4.2016835 -4.18693 -4.1585393 -4.1219249 -4.093873 -4.0685606 -4.049624 -4.0650764 -4.1139112 -4.1633129 -4.1896849 -4.1968851 -4.1894484 -4.180223][-4.1915426 -4.1880379 -4.1696277 -4.1413465 -4.1047034 -4.0626497 -4.0125074 -3.9757357 -3.9934282 -4.0676212 -4.1448021 -4.1900964 -4.2086205 -4.213479 -4.2109261][-4.1977344 -4.19674 -4.1748514 -4.1449819 -4.1055212 -4.0479913 -3.9727273 -3.9140029 -3.9333134 -4.035224 -4.1361661 -4.1994934 -4.2330322 -4.25321 -4.2551942][-4.2136731 -4.2168465 -4.1948576 -4.1636052 -4.1190658 -4.047246 -3.9547052 -3.8767846 -3.8939641 -4.0191097 -4.1361833 -4.2089605 -4.2486672 -4.2777472 -4.2823567][-4.221508 -4.2284269 -4.2119584 -4.1768365 -4.125308 -4.0430551 -3.940022 -3.8488452 -3.8659461 -4.0072503 -4.1360378 -4.21569 -4.2554235 -4.2812934 -4.2841921][-4.2314587 -4.2412281 -4.2310228 -4.1942959 -4.1378517 -4.0554323 -3.9505064 -3.8550103 -3.8729765 -4.014801 -4.142385 -4.224299 -4.262537 -4.2807531 -4.2818575][-4.2413621 -4.2500577 -4.2423663 -4.2100916 -4.1611643 -4.0943274 -4.0104923 -3.9379487 -3.9557803 -4.0691209 -4.1680555 -4.2326231 -4.2626305 -4.2746272 -4.2729111][-4.2413969 -4.2466207 -4.2386637 -4.2158136 -4.1862049 -4.1452408 -4.0959687 -4.055212 -4.0686765 -4.144186 -4.2065296 -4.2460461 -4.2643962 -4.2713566 -4.2677774][-4.2409477 -4.2439117 -4.2354755 -4.221036 -4.211843 -4.1943231 -4.1690464 -4.1493025 -4.1605625 -4.2065105 -4.2386613 -4.2556415 -4.2608194 -4.2609186 -4.2545204][-4.246017 -4.2460475 -4.2386465 -4.2347484 -4.2372003 -4.2316704 -4.2184739 -4.2097082 -4.2172842 -4.2389593 -4.2485743 -4.2492495 -4.2462034 -4.2413416 -4.2351923][-4.2584243 -4.25501 -4.2484946 -4.250411 -4.2576351 -4.2576532 -4.2492604 -4.2451158 -4.2460752 -4.2511234 -4.24795 -4.2384658 -4.2276597 -4.2227845 -4.2226086][-4.2654505 -4.2618494 -4.2589846 -4.2634721 -4.2721143 -4.2738924 -4.2655458 -4.2582674 -4.2525773 -4.2460189 -4.236784 -4.2258692 -4.2150455 -4.2133789 -4.2170577]]...]
INFO - root - 2017-12-05 21:04:45.331183: step 43910, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 68h:02m:55s remains)
INFO - root - 2017-12-05 21:04:53.926514: step 43920, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 69h:10m:24s remains)
INFO - root - 2017-12-05 21:05:02.424062: step 43930, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 69h:54m:07s remains)
INFO - root - 2017-12-05 21:05:11.059782: step 43940, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:55m:33s remains)
INFO - root - 2017-12-05 21:05:19.549134: step 43950, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 69h:47m:31s remains)
INFO - root - 2017-12-05 21:05:28.093652: step 43960, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 68h:59m:17s remains)
INFO - root - 2017-12-05 21:05:36.560687: step 43970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 68h:58m:06s remains)
INFO - root - 2017-12-05 21:05:45.049885: step 43980, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 68h:29m:40s remains)
INFO - root - 2017-12-05 21:05:53.568981: step 43990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 67h:18m:53s remains)
INFO - root - 2017-12-05 21:06:02.003818: step 44000, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 70h:18m:04s remains)
2017-12-05 21:06:02.837603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1121926 -4.0744367 -4.0391006 -4.0179186 -4.0233746 -4.0493255 -4.0606995 -4.0640535 -4.0944285 -4.1584668 -4.2203588 -4.266551 -4.294631 -4.3098507 -4.3269935][-4.1154523 -4.0767779 -4.0363526 -4.0105567 -4.0194931 -4.0504379 -4.0652347 -4.0703139 -4.0998311 -4.1612287 -4.2194395 -4.2608628 -4.28484 -4.2973909 -4.3152866][-4.1222162 -4.094399 -4.0628514 -4.0452352 -4.05925 -4.0911608 -4.1102686 -4.116344 -4.1360931 -4.1854191 -4.2361693 -4.26996 -4.286819 -4.2926211 -4.3075371][-4.1304746 -4.1095748 -4.092442 -4.0902414 -4.1088438 -4.13778 -4.1555419 -4.1561608 -4.1612473 -4.19694 -4.2435732 -4.2776761 -4.2929363 -4.295723 -4.3072972][-4.1394763 -4.1183043 -4.1135106 -4.1236844 -4.1388106 -4.1532931 -4.1600533 -4.1483264 -4.1396179 -4.167223 -4.2178555 -4.2633786 -4.2877426 -4.2957549 -4.30918][-4.1568389 -4.1342535 -4.1357546 -4.1497893 -4.1505089 -4.1410065 -4.1252785 -4.0930133 -4.0693645 -4.0944414 -4.1578608 -4.2234821 -4.2639079 -4.2839985 -4.3049674][-4.1716509 -4.1461945 -4.1455293 -4.1523595 -4.1349869 -4.106534 -4.0743117 -4.0282407 -3.995167 -4.0206571 -4.095871 -4.1787343 -4.2330389 -4.2659197 -4.2955627][-4.1706266 -4.1432295 -4.1370707 -4.1301107 -4.1008568 -4.0729752 -4.0464187 -4.004621 -3.9735739 -4.0013885 -4.0798216 -4.1681795 -4.2264085 -4.2642026 -4.2946405][-4.1591244 -4.131536 -4.1200142 -4.1023345 -4.0725389 -4.0550647 -4.0471663 -4.0266991 -4.0150528 -4.0487223 -4.12267 -4.2040658 -4.2561626 -4.2886348 -4.3105388][-4.1445475 -4.1189537 -4.1047487 -4.0857487 -4.0648689 -4.059176 -4.0650315 -4.0639868 -4.0721684 -4.1107297 -4.1767416 -4.2482877 -4.2928391 -4.31585 -4.3284073][-4.1528258 -4.1271157 -4.1117477 -4.0931416 -4.0834641 -4.0887818 -4.1008911 -4.1116362 -4.1302252 -4.1663818 -4.2205687 -4.2798033 -4.3161969 -4.3311424 -4.3384762][-4.1892185 -4.1637039 -4.1441855 -4.1224771 -4.114336 -4.1238565 -4.13739 -4.1539569 -4.1785583 -4.2085934 -4.2497292 -4.2954564 -4.3234067 -4.3331757 -4.3405237][-4.2155342 -4.1930137 -4.1708817 -4.1471014 -4.1342888 -4.1417289 -4.1562562 -4.1789432 -4.2093854 -4.2358303 -4.2656384 -4.29791 -4.3166676 -4.3255281 -4.3368039][-4.2220216 -4.2056484 -4.1881213 -4.1682162 -4.1552787 -4.1585746 -4.1693544 -4.1929426 -4.2252994 -4.2500834 -4.2704606 -4.2897482 -4.3015194 -4.312119 -4.3290396][-4.2121768 -4.1991086 -4.186202 -4.1747108 -4.1676245 -4.1688924 -4.175921 -4.1973243 -4.2286253 -4.2517548 -4.2653327 -4.2754612 -4.2838588 -4.295516 -4.3176918]]...]
INFO - root - 2017-12-05 21:06:11.170007: step 44010, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 66h:33m:06s remains)
INFO - root - 2017-12-05 21:06:19.698903: step 44020, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.874 sec/batch; 70h:03m:50s remains)
INFO - root - 2017-12-05 21:06:28.284239: step 44030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 68h:26m:31s remains)
INFO - root - 2017-12-05 21:06:36.841359: step 44040, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 68h:15m:19s remains)
INFO - root - 2017-12-05 21:06:45.423059: step 44050, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 68h:02m:33s remains)
INFO - root - 2017-12-05 21:06:53.998977: step 44060, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 69h:22m:07s remains)
INFO - root - 2017-12-05 21:07:02.606044: step 44070, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 70h:11m:00s remains)
INFO - root - 2017-12-05 21:07:11.148675: step 44080, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.896 sec/batch; 71h:46m:46s remains)
INFO - root - 2017-12-05 21:07:19.726021: step 44090, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:33m:59s remains)
INFO - root - 2017-12-05 21:07:28.275986: step 44100, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 67h:59m:23s remains)
2017-12-05 21:07:29.155207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2113986 -4.2091985 -4.20065 -4.1916375 -4.2007689 -4.1973577 -4.1852942 -4.1713905 -4.1568241 -4.1463456 -4.1511965 -4.1619925 -4.1780457 -4.1857696 -4.1920805][-4.1881714 -4.18709 -4.1794128 -4.1669645 -4.1649952 -4.1601114 -4.16361 -4.1614485 -4.1484289 -4.1426878 -4.1584086 -4.1807961 -4.2017632 -4.2043409 -4.2053289][-4.1814203 -4.178957 -4.172924 -4.1616168 -4.1564984 -4.161067 -4.1824937 -4.1847286 -4.1676507 -4.1637368 -4.1835823 -4.2072396 -4.2251148 -4.2220235 -4.2138333][-4.1804175 -4.1737361 -4.1726184 -4.1687832 -4.175529 -4.190834 -4.21465 -4.2105408 -4.1814427 -4.1784916 -4.2006679 -4.2273545 -4.2382622 -4.2242131 -4.2084336][-4.1771121 -4.1696558 -4.1716027 -4.1761522 -4.1877513 -4.2052646 -4.2174788 -4.1951671 -4.1494255 -4.143292 -4.1707487 -4.2018051 -4.2049952 -4.1821289 -4.1684127][-4.1843438 -4.1762266 -4.1830392 -4.1847353 -4.1876378 -4.1976867 -4.1872773 -4.1354609 -4.0639529 -4.0583916 -4.0965824 -4.1313033 -4.1327705 -4.1150918 -4.1078048][-4.1677818 -4.1601825 -4.170423 -4.16568 -4.1578331 -4.1476369 -4.1055536 -4.0199738 -3.9242525 -3.9385552 -4.0052533 -4.0516143 -4.0652084 -4.0624366 -4.0637264][-4.1299891 -4.11787 -4.12872 -4.1188383 -4.1022811 -4.0734391 -4.016572 -3.9149652 -3.8165488 -3.854178 -3.9415872 -3.998065 -4.022613 -4.0375185 -4.0570126][-4.1153574 -4.1018753 -4.1106815 -4.1019487 -4.0867858 -4.0610309 -4.0203815 -3.9502177 -3.8853405 -3.9214919 -3.9936492 -4.0435367 -4.059269 -4.0664916 -4.0862617][-4.1417685 -4.1289463 -4.1330123 -4.1206903 -4.1086411 -4.0978074 -4.0787454 -4.0438809 -4.0151052 -4.0386977 -4.082551 -4.1147065 -4.1223383 -4.1200275 -4.1283741][-4.182198 -4.17156 -4.17122 -4.1541004 -4.1456065 -4.1439724 -4.1392665 -4.1223917 -4.1087213 -4.1219745 -4.1468515 -4.1674652 -4.1747046 -4.176445 -4.1796045][-4.2172165 -4.2094 -4.2045321 -4.1885333 -4.1850438 -4.1915927 -4.1926985 -4.1844392 -4.1793928 -4.1895037 -4.2066827 -4.2225461 -4.2287264 -4.2310119 -4.2325125][-4.2330518 -4.2286773 -4.2208705 -4.2109342 -4.2157297 -4.2265625 -4.22953 -4.2253728 -4.2232685 -4.228713 -4.2425866 -4.257308 -4.2630038 -4.2635279 -4.2645278][-4.2343049 -4.2344866 -4.2311959 -4.2292743 -4.2380395 -4.2476892 -4.25265 -4.2534604 -4.2519479 -4.2512136 -4.2582459 -4.2658539 -4.2669973 -4.26467 -4.2645111][-4.2453651 -4.2470074 -4.2459936 -4.2445226 -4.2485027 -4.2552414 -4.2623396 -4.2682934 -4.268743 -4.2654495 -4.2655187 -4.2664776 -4.2645159 -4.2600012 -4.2572322]]...]
INFO - root - 2017-12-05 21:07:37.531124: step 44110, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 70h:05m:20s remains)
INFO - root - 2017-12-05 21:07:46.071574: step 44120, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 67h:39m:42s remains)
INFO - root - 2017-12-05 21:07:54.581567: step 44130, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 68h:09m:53s remains)
INFO - root - 2017-12-05 21:08:03.034831: step 44140, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 67h:10m:47s remains)
INFO - root - 2017-12-05 21:08:11.583563: step 44150, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 68h:21m:51s remains)
INFO - root - 2017-12-05 21:08:20.251616: step 44160, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 69h:03m:17s remains)
INFO - root - 2017-12-05 21:08:28.864507: step 44170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 67h:48m:20s remains)
INFO - root - 2017-12-05 21:08:37.381427: step 44180, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 70h:16m:05s remains)
INFO - root - 2017-12-05 21:08:45.952379: step 44190, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 69h:12m:00s remains)
INFO - root - 2017-12-05 21:08:54.564866: step 44200, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:32m:47s remains)
2017-12-05 21:08:55.405153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2435336 -4.232738 -4.2351952 -4.2275634 -4.2079844 -4.1991754 -4.1964164 -4.1945882 -4.2041354 -4.2080693 -4.1937213 -4.174593 -4.1615992 -4.1620803 -4.1721253][-4.2438035 -4.2356834 -4.241694 -4.2371488 -4.2163043 -4.2020712 -4.1958447 -4.1917334 -4.1995015 -4.2020745 -4.1899633 -4.1798859 -4.1758881 -4.1790061 -4.1924219][-4.245091 -4.2404857 -4.2501068 -4.2521992 -4.2343011 -4.2187009 -4.2099061 -4.2022567 -4.1980748 -4.1921029 -4.1790428 -4.1774073 -4.1850195 -4.1953297 -4.2087307][-4.2398353 -4.2359996 -4.2466712 -4.2550173 -4.2442484 -4.2278891 -4.2159767 -4.2023563 -4.1901174 -4.1806378 -4.1767335 -4.1852365 -4.203784 -4.2224207 -4.234158][-4.2291636 -4.2225451 -4.2317481 -4.242269 -4.2355795 -4.2177582 -4.1988068 -4.1791191 -4.1664081 -4.1626458 -4.1709466 -4.1947393 -4.2261529 -4.2490296 -4.2571721][-4.2221766 -4.2134962 -4.2146592 -4.213923 -4.2023015 -4.1815195 -4.1605515 -4.1407952 -4.1368375 -4.1467786 -4.1686921 -4.204206 -4.2411838 -4.2618346 -4.2658796][-4.2216477 -4.213408 -4.2090645 -4.195209 -4.1689191 -4.1391487 -4.1171913 -4.1023989 -4.1100817 -4.1379557 -4.174314 -4.2170472 -4.2537904 -4.270987 -4.2702122][-4.2238722 -4.2174377 -4.2139349 -4.1947947 -4.1582026 -4.1232991 -4.1023707 -4.0934296 -4.1102433 -4.1497912 -4.1936512 -4.2363234 -4.2672615 -4.2769403 -4.2659254][-4.2315736 -4.2280464 -4.2275743 -4.21451 -4.1807823 -4.1507382 -4.1349888 -4.1279168 -4.1448674 -4.1808476 -4.2177896 -4.2499347 -4.269424 -4.2693043 -4.2492094][-4.2406731 -4.2373595 -4.2370296 -4.2292643 -4.2030783 -4.1833839 -4.1748056 -4.1697707 -4.1824212 -4.2081318 -4.2336407 -4.2540541 -4.264133 -4.2575297 -4.2353768][-4.2460136 -4.2412224 -4.2397351 -4.2328167 -4.2114663 -4.2006254 -4.1994357 -4.2013245 -4.215385 -4.2319174 -4.2474713 -4.2609711 -4.2660313 -4.2578964 -4.239614][-4.2527008 -4.2473879 -4.2498517 -4.2484331 -4.2342181 -4.227932 -4.2303681 -4.2348323 -4.2489886 -4.2638521 -4.2730012 -4.2791471 -4.2799287 -4.2730918 -4.2617135][-4.2691717 -4.2663512 -4.2713604 -4.2715335 -4.2622662 -4.2577472 -4.2607641 -4.2667942 -4.2785864 -4.2898407 -4.2944608 -4.2970662 -4.2967834 -4.2926559 -4.2856145][-4.2864037 -4.2837763 -4.2854028 -4.2849932 -4.2811007 -4.2791157 -4.2806816 -4.2860169 -4.2957673 -4.3043818 -4.3076053 -4.3079495 -4.3063149 -4.3027191 -4.2976036][-4.292316 -4.2877502 -4.2845125 -4.2833395 -4.28384 -4.284873 -4.2873673 -4.2912107 -4.297894 -4.3038182 -4.3049679 -4.303432 -4.3012772 -4.2990074 -4.2956767]]...]
INFO - root - 2017-12-05 21:09:03.917736: step 44210, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 68h:03m:21s remains)
INFO - root - 2017-12-05 21:09:12.384029: step 44220, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 70h:15m:47s remains)
INFO - root - 2017-12-05 21:09:20.975457: step 44230, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 70h:10m:29s remains)
INFO - root - 2017-12-05 21:09:29.474343: step 44240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:52m:09s remains)
INFO - root - 2017-12-05 21:09:37.956078: step 44250, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:54m:54s remains)
INFO - root - 2017-12-05 21:09:46.658500: step 44260, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 68h:30m:59s remains)
INFO - root - 2017-12-05 21:09:55.302823: step 44270, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 70h:22m:43s remains)
INFO - root - 2017-12-05 21:10:03.841029: step 44280, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.823 sec/batch; 65h:51m:31s remains)
INFO - root - 2017-12-05 21:10:12.380217: step 44290, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 68h:32m:13s remains)
INFO - root - 2017-12-05 21:10:20.953926: step 44300, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 67h:44m:53s remains)
2017-12-05 21:10:21.713490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3323164 -4.3310471 -4.3264432 -4.3223529 -4.3195796 -4.3197331 -4.3220921 -4.3251681 -4.3279953 -4.3304925 -4.3322992 -4.3318305 -4.329319 -4.326026 -4.3239017][-4.3337364 -4.3316922 -4.3229756 -4.3152633 -4.3093023 -4.3073616 -4.3103356 -4.3171844 -4.3244829 -4.3303165 -4.333643 -4.330986 -4.3246279 -4.3171067 -4.311203][-4.3304396 -4.3296123 -4.319056 -4.3071923 -4.2956376 -4.2898765 -4.2930174 -4.3054266 -4.3180766 -4.3274207 -4.3308263 -4.323297 -4.3104739 -4.2967377 -4.2880235][-4.2994981 -4.2975054 -4.2832274 -4.2661209 -4.2476797 -4.2374783 -4.2419491 -4.2625246 -4.2858458 -4.3012562 -4.3035035 -4.288506 -4.2693276 -4.2527838 -4.2467937][-4.2522192 -4.2453446 -4.2240353 -4.1995845 -4.1708946 -4.1515994 -4.1546192 -4.1836753 -4.2212315 -4.2484841 -4.2517805 -4.2329354 -4.2102962 -4.1968131 -4.1988096][-4.2135482 -4.2031016 -4.179183 -4.1513114 -4.1151381 -4.0872068 -4.081974 -4.1082811 -4.1518669 -4.1858044 -4.1915526 -4.1763868 -4.1566138 -4.1511779 -4.1626687][-4.1731396 -4.164238 -4.1433172 -4.1174583 -4.0844183 -4.0570087 -4.0439463 -4.0604954 -4.09904 -4.1313171 -4.1391611 -4.1309905 -4.1185908 -4.1229625 -4.1441689][-4.1254048 -4.117826 -4.10229 -4.0846457 -4.0631127 -4.0429382 -4.027081 -4.0364585 -4.0674534 -4.092627 -4.1008091 -4.1038628 -4.1043673 -4.1188855 -4.1482549][-4.1023045 -4.0959229 -4.08705 -4.0789371 -4.0656314 -4.0526257 -4.0425892 -4.0519261 -4.0742226 -4.0902071 -4.0987034 -4.1123824 -4.1231427 -4.1427116 -4.1739082][-4.113039 -4.1070766 -4.1030173 -4.1043854 -4.0989866 -4.0917139 -4.0895696 -4.1028023 -4.1208906 -4.1302133 -4.1389985 -4.1567578 -4.1687336 -4.1843076 -4.2087646][-4.1417723 -4.1407127 -4.1432972 -4.1496096 -4.1496572 -4.1481285 -4.1508207 -4.1635165 -4.17533 -4.1801553 -4.1863947 -4.2005215 -4.2096014 -4.2208486 -4.2369633][-4.1712189 -4.1768508 -4.1869712 -4.1983552 -4.203949 -4.2069745 -4.2104006 -4.2169008 -4.2183909 -4.2141309 -4.214407 -4.2236977 -4.230783 -4.2400875 -4.2499323][-4.205668 -4.2174382 -4.23035 -4.2386851 -4.2398372 -4.2392788 -4.2410789 -4.2441511 -4.2394838 -4.2317038 -4.2290626 -4.2346578 -4.237999 -4.2431664 -4.2486506][-4.2340755 -4.246603 -4.2553663 -4.2552557 -4.248385 -4.2438016 -4.2450752 -4.2501693 -4.248394 -4.243052 -4.2389131 -4.2409678 -4.2402267 -4.2414026 -4.2447782][-4.2479658 -4.2612004 -4.2675123 -4.262435 -4.2514606 -4.2447643 -4.2453532 -4.2524357 -4.2547946 -4.2528706 -4.2473674 -4.2459092 -4.2407103 -4.23831 -4.2416086]]...]
INFO - root - 2017-12-05 21:10:30.346994: step 44310, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 69h:06m:39s remains)
INFO - root - 2017-12-05 21:10:38.950176: step 44320, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.883 sec/batch; 70h:41m:19s remains)
INFO - root - 2017-12-05 21:10:47.403745: step 44330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 68h:58m:48s remains)
INFO - root - 2017-12-05 21:10:56.051041: step 44340, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 68h:09m:14s remains)
INFO - root - 2017-12-05 21:11:04.493086: step 44350, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 68h:39m:28s remains)
INFO - root - 2017-12-05 21:11:12.915126: step 44360, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:23m:14s remains)
INFO - root - 2017-12-05 21:11:21.469384: step 44370, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 68h:21m:58s remains)
INFO - root - 2017-12-05 21:11:30.049481: step 44380, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 67h:01m:12s remains)
INFO - root - 2017-12-05 21:11:38.605398: step 44390, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 68h:30m:34s remains)
INFO - root - 2017-12-05 21:11:47.133556: step 44400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:50m:40s remains)
2017-12-05 21:11:47.895333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2496324 -4.2423115 -4.2308574 -4.2201905 -4.2107635 -4.2045188 -4.1978211 -4.1935282 -4.1909204 -4.1918721 -4.1978908 -4.2060604 -4.2128167 -4.2193747 -4.2228284][-4.2471428 -4.237886 -4.2232866 -4.2096539 -4.2006059 -4.1974592 -4.1923747 -4.1870985 -4.1820545 -4.180244 -4.1840158 -4.1913109 -4.199276 -4.2082667 -4.2145863][-4.2447348 -4.2340064 -4.2161217 -4.1984348 -4.1889358 -4.1887741 -4.1883082 -4.1867557 -4.1836576 -4.1806846 -4.1791658 -4.1800723 -4.1849194 -4.1940918 -4.2041349][-4.2443 -4.2330518 -4.2134395 -4.1924658 -4.1808062 -4.1817517 -4.1860309 -4.1907005 -4.1935573 -4.1926866 -4.18679 -4.1792893 -4.1781697 -4.1844807 -4.19506][-4.2459159 -4.2356715 -4.2165313 -4.1946158 -4.1794415 -4.1776953 -4.1825581 -4.1912231 -4.2007995 -4.2051368 -4.1987877 -4.1869488 -4.1809053 -4.182436 -4.1890597][-4.2494383 -4.2434354 -4.2289615 -4.209331 -4.1893978 -4.1784148 -4.17447 -4.1780372 -4.1898479 -4.2011938 -4.2015076 -4.1934133 -4.1874604 -4.1852064 -4.1857491][-4.2495809 -4.2470317 -4.238153 -4.2229552 -4.2006907 -4.1803527 -4.1639605 -4.1565466 -4.1664104 -4.1823678 -4.1899805 -4.1884055 -4.1854262 -4.1819091 -4.1793208][-4.2471232 -4.24477 -4.2382483 -4.2271552 -4.2073011 -4.185585 -4.1647797 -4.1513867 -4.157619 -4.1715794 -4.179585 -4.1787519 -4.17592 -4.1733589 -4.17343][-4.2428503 -4.237865 -4.2297606 -4.2208171 -4.2073245 -4.1950488 -4.1835442 -4.1762538 -4.1797624 -4.1838403 -4.1828165 -4.1757135 -4.1688194 -4.1655326 -4.1680169][-4.2381864 -4.2288532 -4.2158632 -4.2072625 -4.2013593 -4.2019091 -4.2033496 -4.205368 -4.207695 -4.20357 -4.1932116 -4.1789756 -4.1664419 -4.158937 -4.1603074][-4.2369232 -4.2228103 -4.2026148 -4.1911249 -4.1892552 -4.1969118 -4.2061834 -4.21422 -4.21689 -4.21085 -4.1996622 -4.1851473 -4.1710138 -4.1595073 -4.1558971][-4.2430663 -4.2245526 -4.1972556 -4.1796918 -4.1758614 -4.1830683 -4.1931729 -4.2039247 -4.2090197 -4.2062254 -4.2005367 -4.1928058 -4.1834383 -4.1719127 -4.1638885][-4.252089 -4.2315936 -4.2015247 -4.1799574 -4.1716404 -4.1740894 -4.1798959 -4.190269 -4.1968746 -4.1960187 -4.1935263 -4.1934209 -4.1920748 -4.1865616 -4.1800423][-4.2604303 -4.242672 -4.2149372 -4.1939025 -4.1832089 -4.1812434 -4.1812506 -4.1873193 -4.1914816 -4.18879 -4.1862669 -4.1907578 -4.1983624 -4.2028856 -4.2030258][-4.2633896 -4.2488832 -4.22452 -4.2069831 -4.1984539 -4.1967411 -4.1944451 -4.1958337 -4.1961827 -4.1911945 -4.1881413 -4.1927586 -4.2049079 -4.2167196 -4.2217083]]...]
INFO - root - 2017-12-05 21:11:56.423779: step 44410, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 69h:05m:06s remains)
INFO - root - 2017-12-05 21:12:05.149755: step 44420, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:22m:13s remains)
INFO - root - 2017-12-05 21:12:13.567484: step 44430, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 67h:46m:36s remains)
INFO - root - 2017-12-05 21:12:22.002380: step 44440, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 67h:37m:40s remains)
INFO - root - 2017-12-05 21:12:30.536243: step 44450, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 70h:22m:06s remains)
INFO - root - 2017-12-05 21:12:39.158734: step 44460, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 69h:45m:16s remains)
INFO - root - 2017-12-05 21:12:47.771643: step 44470, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 70h:06m:57s remains)
INFO - root - 2017-12-05 21:12:56.266394: step 44480, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 65h:34m:33s remains)
INFO - root - 2017-12-05 21:13:04.780585: step 44490, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 69h:58m:31s remains)
INFO - root - 2017-12-05 21:13:13.298610: step 44500, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 69h:38m:43s remains)
2017-12-05 21:13:14.094535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2970786 -4.2786732 -4.2512832 -4.223877 -4.2116566 -4.2191181 -4.2320595 -4.240859 -4.2493081 -4.2554269 -4.2596354 -4.2622504 -4.2610459 -4.2588038 -4.2519083][-4.2787004 -4.2505369 -4.2093749 -4.169044 -4.1590061 -4.1781473 -4.1981707 -4.2086997 -4.2185173 -4.2251244 -4.2276063 -4.2282777 -4.2281504 -4.2274055 -4.2224689][-4.2493448 -4.2135973 -4.1606593 -4.1112618 -4.109026 -4.141789 -4.1649771 -4.1736054 -4.1851921 -4.1953392 -4.1977448 -4.1972008 -4.1964674 -4.1954603 -4.1911411][-4.2240653 -4.1864233 -4.1305127 -4.0830178 -4.0907035 -4.1271348 -4.1453166 -4.146646 -4.1543355 -4.168047 -4.1749363 -4.17739 -4.178555 -4.1748133 -4.1661263][-4.2158589 -4.1846333 -4.1395197 -4.1062632 -4.1175742 -4.1383996 -4.1377869 -4.1225529 -4.1245179 -4.1398444 -4.1552973 -4.1678638 -4.1746073 -4.1706862 -4.1580024][-4.2166233 -4.1966724 -4.1673503 -4.1477342 -4.1514964 -4.1462502 -4.1210318 -4.0827436 -4.0760717 -4.0956774 -4.1241627 -4.1521273 -4.1667438 -4.1627135 -4.1465545][-4.2183342 -4.2103305 -4.1933551 -4.178576 -4.1669359 -4.1381407 -4.0846958 -4.0140643 -3.9924004 -4.0270996 -4.0768356 -4.1271982 -4.1508379 -4.1464682 -4.1282663][-4.2267375 -4.2250295 -4.2157803 -4.2019043 -4.1761136 -4.125433 -4.0477834 -3.9539165 -3.9192743 -3.9698203 -4.0403237 -4.110672 -4.1433334 -4.1417212 -4.1257362][-4.2445445 -4.2445722 -4.2378674 -4.2235169 -4.1874342 -4.1270332 -4.0472631 -3.9611869 -3.9313617 -3.9816282 -4.0524325 -4.11938 -4.1482863 -4.1442575 -4.1303358][-4.2630153 -4.2637906 -4.2576709 -4.243751 -4.2083869 -4.1508307 -4.0827732 -4.024426 -4.0149789 -4.054306 -4.0988469 -4.1379786 -4.1481 -4.1320243 -4.1160946][-4.2803741 -4.2813473 -4.2771258 -4.2658887 -4.2369442 -4.1881719 -4.1324191 -4.0981917 -4.1021762 -4.1341672 -4.1532364 -4.1576743 -4.13932 -4.1042962 -4.0866489][-4.2944078 -4.2964373 -4.2958879 -4.28926 -4.2680717 -4.2301259 -4.1852221 -4.1623888 -4.1646714 -4.1872492 -4.1892471 -4.1731195 -4.1368761 -4.0922127 -4.07682][-4.3064466 -4.3105412 -4.3122487 -4.3092442 -4.295454 -4.2674184 -4.2331915 -4.214839 -4.20666 -4.2138414 -4.2065244 -4.1869364 -4.1515074 -4.1113691 -4.1003003][-4.3154106 -4.3212585 -4.3243575 -4.3228383 -4.31347 -4.2933722 -4.2691164 -4.250824 -4.2327123 -4.2268386 -4.2153063 -4.1987576 -4.1681519 -4.1346545 -4.1276417][-4.3214335 -4.3276076 -4.3314047 -4.3305297 -4.3245978 -4.3105288 -4.2932138 -4.2757607 -4.2537842 -4.2422066 -4.2299085 -4.214848 -4.186799 -4.152494 -4.142405]]...]
INFO - root - 2017-12-05 21:13:22.682068: step 44510, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 68h:43m:45s remains)
INFO - root - 2017-12-05 21:13:31.391265: step 44520, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.815 sec/batch; 65h:10m:41s remains)
INFO - root - 2017-12-05 21:13:40.038480: step 44530, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 67h:59m:31s remains)
INFO - root - 2017-12-05 21:13:48.552188: step 44540, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 66h:36m:06s remains)
INFO - root - 2017-12-05 21:13:57.033099: step 44550, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 70h:31m:39s remains)
INFO - root - 2017-12-05 21:14:05.579753: step 44560, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 66h:33m:17s remains)
INFO - root - 2017-12-05 21:14:14.106609: step 44570, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 69h:16m:28s remains)
INFO - root - 2017-12-05 21:14:22.766096: step 44580, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 66h:11m:23s remains)
INFO - root - 2017-12-05 21:14:31.289826: step 44590, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 68h:47m:15s remains)
INFO - root - 2017-12-05 21:14:39.796778: step 44600, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 67h:02m:02s remains)
2017-12-05 21:14:40.584312: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2752819 -4.2747183 -4.2646327 -4.2503538 -4.2327366 -4.2159295 -4.21062 -4.2168374 -4.22449 -4.2301307 -4.2282324 -4.20582 -4.1759982 -4.1459465 -4.1178741][-4.2641683 -4.2667122 -4.2594018 -4.2471342 -4.2291622 -4.2095633 -4.2001729 -4.2058859 -4.2188206 -4.2305603 -4.2326236 -4.2107611 -4.1760211 -4.1368079 -4.093874][-4.239707 -4.2462487 -4.2441707 -4.2371993 -4.2226572 -4.2016435 -4.1856451 -4.1853046 -4.20095 -4.2192936 -4.2260332 -4.20573 -4.1689019 -4.1271615 -4.0792065][-4.2270341 -4.2334762 -4.232923 -4.2299256 -4.2201204 -4.2008038 -4.178019 -4.1690812 -4.184154 -4.211267 -4.2255211 -4.2083254 -4.1755381 -4.1446195 -4.1092997][-4.2047787 -4.2097726 -4.2114444 -4.2132983 -4.2073669 -4.1877742 -4.1546183 -4.1341524 -4.1468067 -4.1864643 -4.2140818 -4.2050629 -4.1823883 -4.1676841 -4.1539016][-4.1899548 -4.1910567 -4.1916761 -4.1923728 -4.1788149 -4.1469026 -4.0983987 -4.0636444 -4.0722442 -4.1277328 -4.1746392 -4.17888 -4.1700573 -4.1699185 -4.172061][-4.2010069 -4.1969748 -4.1917796 -4.182436 -4.1516523 -4.0988317 -4.0309105 -3.9765091 -3.9743674 -4.0423121 -4.1104078 -4.1332893 -4.1408272 -4.1484365 -4.1611214][-4.2269549 -4.2191768 -4.204812 -4.1778116 -4.126297 -4.05632 -3.9729974 -3.8988891 -3.878022 -3.9525192 -4.0407457 -4.0871868 -4.1129875 -4.1239552 -4.141901][-4.2494011 -4.240005 -4.21751 -4.1756406 -4.1133347 -4.0385408 -3.9556367 -3.8801637 -3.8560708 -3.9274507 -4.0154457 -4.070117 -4.1024261 -4.1104069 -4.1263037][-4.2575378 -4.2510157 -4.2276635 -4.1839147 -4.1262536 -4.0630441 -3.9967222 -3.9419856 -3.9296982 -3.9846187 -4.0490365 -4.0932136 -4.1202173 -4.1247582 -4.1369648][-4.2431164 -4.2458177 -4.2330246 -4.1983743 -4.1518283 -4.1039491 -4.0591722 -4.0299211 -4.0301142 -4.0649457 -4.1032457 -4.135705 -4.1563635 -4.1619205 -4.1722503][-4.2075415 -4.2246704 -4.2285066 -4.209476 -4.1781116 -4.1447334 -4.1190267 -4.1103549 -4.11762 -4.1366687 -4.1565952 -4.1771221 -4.1895781 -4.1938515 -4.201097][-4.1832514 -4.2115684 -4.2277613 -4.2234745 -4.2057581 -4.1831713 -4.1714687 -4.1769538 -4.1887159 -4.1995106 -4.207871 -4.2171078 -4.2206883 -4.2188787 -4.215004][-4.1889391 -4.2170672 -4.2324972 -4.2325463 -4.2202296 -4.2025132 -4.1972747 -4.2103386 -4.2267642 -4.2374516 -4.2411985 -4.2427077 -4.2399755 -4.231986 -4.2207708][-4.2269473 -4.2473693 -4.25309 -4.2461247 -4.2289777 -4.2076731 -4.2052159 -4.2233996 -4.2421746 -4.253346 -4.2569318 -4.255764 -4.2505746 -4.2424173 -4.2324629]]...]
INFO - root - 2017-12-05 21:14:49.104571: step 44610, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.877 sec/batch; 70h:10m:19s remains)
INFO - root - 2017-12-05 21:14:57.677912: step 44620, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 69h:26m:54s remains)
INFO - root - 2017-12-05 21:15:06.173367: step 44630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 68h:46m:15s remains)
INFO - root - 2017-12-05 21:15:14.737017: step 44640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 67h:48m:08s remains)
INFO - root - 2017-12-05 21:15:23.202545: step 44650, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.879 sec/batch; 70h:15m:54s remains)
INFO - root - 2017-12-05 21:15:31.668449: step 44660, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 67h:58m:03s remains)
INFO - root - 2017-12-05 21:15:40.197433: step 44670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 68h:48m:53s remains)
INFO - root - 2017-12-05 21:15:48.923399: step 44680, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.868 sec/batch; 69h:25m:52s remains)
INFO - root - 2017-12-05 21:15:57.509766: step 44690, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 69h:16m:12s remains)
INFO - root - 2017-12-05 21:16:06.053206: step 44700, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 68h:11m:09s remains)
2017-12-05 21:16:06.823151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.276042 -4.2787895 -4.276515 -4.2744684 -4.2689023 -4.2613988 -4.2504897 -4.2395926 -4.2345467 -4.2363424 -4.240901 -4.2454605 -4.2510929 -4.2650771 -4.2817788][-4.2406111 -4.2413387 -4.2398763 -4.2402573 -4.2367873 -4.2270455 -4.2068172 -4.1864057 -4.1768265 -4.1820741 -4.1941509 -4.2038317 -4.2116051 -4.2314405 -4.2550511][-4.2144823 -4.21296 -4.2113466 -4.2113047 -4.2079506 -4.1922364 -4.1619182 -4.1361194 -4.129065 -4.1402488 -4.1563168 -4.1716657 -4.1854115 -4.2125373 -4.2435126][-4.18876 -4.1895633 -4.1867185 -4.1839967 -4.17655 -4.1487226 -4.1089816 -4.0889406 -4.0939612 -4.1080637 -4.1183414 -4.1346192 -4.1629133 -4.2022328 -4.2391148][-4.1394348 -4.1476827 -4.1472917 -4.1408544 -4.1235247 -4.0810075 -4.0389843 -4.0373278 -4.0634031 -4.0791388 -4.0766468 -4.0896063 -4.1311245 -4.1838627 -4.2275357][-4.0822477 -4.0962062 -4.0925426 -4.0786219 -4.0476532 -3.987931 -3.9416401 -3.9549513 -3.9988015 -4.0168924 -4.0081458 -4.0214763 -4.0761514 -4.1430697 -4.1954188][-4.0471282 -4.0623269 -4.0547709 -4.0336528 -3.988867 -3.9086721 -3.8461137 -3.8582621 -3.9170351 -3.94108 -3.9312613 -3.951896 -4.0209203 -4.0989685 -4.1593862][-4.0530005 -4.0648575 -4.0535488 -4.0348821 -3.989409 -3.9036341 -3.8269124 -3.8264782 -3.882988 -3.9050729 -3.890734 -3.9156833 -3.9916315 -4.066474 -4.1241436][-4.0707464 -4.0833797 -4.0791693 -4.0799942 -4.0574708 -3.9939365 -3.9309371 -3.9237773 -3.9620266 -3.9684083 -3.9438529 -3.9546909 -4.0136118 -4.0690908 -4.1147904][-4.0851569 -4.1023207 -4.1103611 -4.1304989 -4.1376605 -4.1113005 -4.0814323 -4.0867171 -4.104557 -4.0870614 -4.0433831 -4.0339355 -4.0677657 -4.1041942 -4.1401405][-4.0909061 -4.1133275 -4.1365232 -4.1703491 -4.1927662 -4.1925378 -4.1904182 -4.20706 -4.211122 -4.1739259 -4.113749 -4.0916739 -4.1102562 -4.1374187 -4.1709728][-4.0971909 -4.1234779 -4.1595426 -4.2020812 -4.2296495 -4.2403259 -4.2523823 -4.2714396 -4.2659335 -4.2191467 -4.1518188 -4.1238179 -4.1377153 -4.1637955 -4.1970158][-4.1215038 -4.147203 -4.1857157 -4.2278824 -4.2542543 -4.2655187 -4.2816191 -4.299067 -4.2914624 -4.2474823 -4.1833849 -4.1539845 -4.1629739 -4.1855025 -4.2154379][-4.1714916 -4.1961341 -4.2262464 -4.25817 -4.2818241 -4.29443 -4.3103175 -4.3265085 -4.3218689 -4.2875481 -4.233932 -4.2075438 -4.2111664 -4.2280736 -4.2509632][-4.232276 -4.2484064 -4.2663388 -4.2850862 -4.3039241 -4.3141718 -4.3272047 -4.3418756 -4.3413343 -4.3204675 -4.28723 -4.2709193 -4.2718544 -4.2818775 -4.2959251]]...]
INFO - root - 2017-12-05 21:16:15.464755: step 44710, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 69h:59m:50s remains)
INFO - root - 2017-12-05 21:16:24.000824: step 44720, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 67h:23m:49s remains)
INFO - root - 2017-12-05 21:16:32.706054: step 44730, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 70h:40m:49s remains)
INFO - root - 2017-12-05 21:16:41.252596: step 44740, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 69h:00m:20s remains)
INFO - root - 2017-12-05 21:16:49.660119: step 44750, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 68h:19m:23s remains)
INFO - root - 2017-12-05 21:16:58.265211: step 44760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 66h:31m:55s remains)
INFO - root - 2017-12-05 21:17:06.790588: step 44770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 68h:17m:39s remains)
INFO - root - 2017-12-05 21:17:15.484882: step 44780, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 70h:33m:03s remains)
INFO - root - 2017-12-05 21:17:24.160497: step 44790, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.900 sec/batch; 71h:56m:12s remains)
INFO - root - 2017-12-05 21:17:32.801413: step 44800, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 69h:22m:58s remains)
2017-12-05 21:17:33.539394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1365337 -4.1235623 -4.1203794 -4.1402721 -4.1696115 -4.1943793 -4.2126241 -4.2228074 -4.2150631 -4.1978817 -4.1939654 -4.2106724 -4.2214842 -4.2213621 -4.2197146][-4.0631509 -4.072011 -4.0966005 -4.1352692 -4.1691937 -4.1957731 -4.2116723 -4.2121067 -4.1995296 -4.182507 -4.1860204 -4.2100759 -4.22701 -4.2317533 -4.2321343][-4.0074325 -4.0334344 -4.0715733 -4.1181355 -4.1508074 -4.1745496 -4.1839457 -4.17289 -4.156445 -4.1493182 -4.1745043 -4.216835 -4.245657 -4.2574425 -4.2602034][-3.998451 -4.0290804 -4.0573082 -4.0899119 -4.1121197 -4.1285839 -4.1310945 -4.1055269 -4.0829191 -4.0934358 -4.1436181 -4.2065887 -4.2480364 -4.2699084 -4.2796464][-4.0175185 -4.0338526 -4.0405884 -4.0511627 -4.0582495 -4.0625787 -4.056818 -4.02366 -4.0111713 -4.044383 -4.1130528 -4.1833515 -4.2338061 -4.2644854 -4.2820182][-4.0580893 -4.05429 -4.0393176 -4.0258093 -4.0142756 -4.0096564 -4.0047684 -3.9823823 -3.9876289 -4.0350566 -4.108067 -4.1745882 -4.2263522 -4.262713 -4.2843304][-4.1043892 -4.0875216 -4.0574021 -4.0244184 -3.9968486 -3.9871402 -3.9822812 -3.9641743 -3.977809 -4.0275183 -4.0972452 -4.1612749 -4.2197995 -4.265265 -4.2883792][-4.1504197 -4.1267729 -4.0898461 -4.045455 -4.0094142 -3.9934597 -3.9797456 -3.9557893 -3.9665825 -4.0140023 -4.0803537 -4.1400385 -4.1997404 -4.2475581 -4.2646651][-4.1887388 -4.161181 -4.1222525 -4.0813875 -4.0493994 -4.0303769 -4.0099478 -3.9821503 -3.9895358 -4.0292912 -4.086185 -4.1361175 -4.1835709 -4.21856 -4.2216058][-4.2000551 -4.1764746 -4.1472816 -4.1207986 -4.0976982 -4.0752139 -4.0479221 -4.0208259 -4.0210085 -4.0448828 -4.08918 -4.1293058 -4.1682291 -4.1919432 -4.1882777][-4.1831045 -4.16731 -4.1518207 -4.1409082 -4.1270337 -4.1072607 -4.0823278 -4.0600791 -4.0587864 -4.074398 -4.1094217 -4.1430755 -4.17511 -4.191885 -4.1830549][-4.1556835 -4.1451573 -4.1372457 -4.1342659 -4.1266732 -4.1149263 -4.1001678 -4.0906062 -4.0945654 -4.1136956 -4.1473107 -4.1748028 -4.1968575 -4.2049203 -4.1914968][-4.1230807 -4.1189494 -4.1169257 -4.117373 -4.1160183 -4.1147718 -4.1138868 -4.1191249 -4.1323876 -4.1581907 -4.1910825 -4.2138667 -4.2282829 -4.23047 -4.218163][-4.0643616 -4.0756688 -4.0877485 -4.0986075 -4.108654 -4.1202207 -4.135242 -4.1547213 -4.1764007 -4.2025671 -4.2277737 -4.2430034 -4.2530274 -4.2549148 -4.2486286][-4.0123806 -4.0364003 -4.05956 -4.0783343 -4.0985441 -4.1254897 -4.1554604 -4.1847157 -4.2112622 -4.2362804 -4.2531667 -4.2606668 -4.2668238 -4.2700405 -4.2699952]]...]
INFO - root - 2017-12-05 21:17:42.031833: step 44810, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 69h:42m:35s remains)
INFO - root - 2017-12-05 21:17:50.694157: step 44820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 68h:47m:33s remains)
INFO - root - 2017-12-05 21:17:59.320468: step 44830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:43m:20s remains)
INFO - root - 2017-12-05 21:18:07.865692: step 44840, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 68h:58m:25s remains)
INFO - root - 2017-12-05 21:18:16.418361: step 44850, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 67h:11m:59s remains)
INFO - root - 2017-12-05 21:18:24.939520: step 44860, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 68h:12m:31s remains)
INFO - root - 2017-12-05 21:18:33.433966: step 44870, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 67h:42m:03s remains)
INFO - root - 2017-12-05 21:18:41.998254: step 44880, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.874 sec/batch; 69h:47m:45s remains)
INFO - root - 2017-12-05 21:18:50.531069: step 44890, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 65h:30m:48s remains)
INFO - root - 2017-12-05 21:18:58.946366: step 44900, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 67h:47m:29s remains)
2017-12-05 21:18:59.697034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2583814 -4.2588968 -4.2475882 -4.2393003 -4.231503 -4.22308 -4.2071419 -4.1899681 -4.1892247 -4.2057114 -4.2278042 -4.2438722 -4.2489271 -4.2461619 -4.2444363][-4.2396832 -4.2406054 -4.2284083 -4.2182956 -4.2088046 -4.1917324 -4.166997 -4.1473861 -4.1459565 -4.1645055 -4.1906543 -4.2132616 -4.2258244 -4.2251034 -4.2254753][-4.2139268 -4.2159996 -4.2027974 -4.1925793 -4.1796379 -4.1528177 -4.120163 -4.1012716 -4.1012936 -4.1160617 -4.1426659 -4.1717114 -4.1921473 -4.1937151 -4.1960459][-4.1875305 -4.1944695 -4.1839628 -4.168365 -4.1449671 -4.1066971 -4.0665827 -4.0524874 -4.0654888 -4.0865574 -4.1202922 -4.1575642 -4.1846147 -4.1871424 -4.1882114][-4.155242 -4.1665769 -4.1594887 -4.1385064 -4.1061258 -4.0478392 -3.9832604 -3.9674041 -4.0049157 -4.0459738 -4.0901365 -4.1395755 -4.1774888 -4.187036 -4.1890073][-4.1132646 -4.1239848 -4.1221657 -4.1003966 -4.0536275 -3.9586992 -3.8413663 -3.8206258 -3.9062428 -3.9855361 -4.0461278 -4.1119628 -4.1637654 -4.1822329 -4.1837764][-4.0712047 -4.082027 -4.0829225 -4.0557866 -3.9833019 -3.836062 -3.6498086 -3.6431422 -3.7963009 -3.9167776 -3.9966404 -4.0801907 -4.1489925 -4.1769819 -4.1763778][-4.0625777 -4.0757456 -4.0716071 -4.03232 -3.9388077 -3.7665858 -3.5532997 -3.5762556 -3.7711473 -3.9084811 -3.99484 -4.0808082 -4.1527505 -4.180407 -4.1758542][-4.0989084 -4.1110048 -4.1050014 -4.0666766 -3.9867666 -3.858479 -3.7228982 -3.7498221 -3.8880377 -3.9871786 -4.0515656 -4.11549 -4.1707811 -4.1903038 -4.1814418][-4.1460152 -4.1526427 -4.1459675 -4.1119428 -4.054832 -3.9800787 -3.9184058 -3.939065 -4.0107012 -4.0684667 -4.1162777 -4.1628671 -4.1977334 -4.2078557 -4.1949844][-4.1933475 -4.1951551 -4.1783724 -4.1397228 -4.0966983 -4.0514936 -4.0277762 -4.0490618 -4.0910387 -4.131001 -4.1739788 -4.2121487 -4.2319679 -4.2330723 -4.2196493][-4.2266984 -4.2261019 -4.2009773 -4.157845 -4.1197071 -4.0848808 -4.0736618 -4.0941458 -4.1311712 -4.1705241 -4.2123866 -4.24565 -4.255795 -4.2520952 -4.2446876][-4.2438712 -4.2441006 -4.2209311 -4.1807804 -4.1479449 -4.1171603 -4.1069746 -4.125237 -4.1643562 -4.2054205 -4.2439675 -4.2712331 -4.2766929 -4.2729874 -4.2701988][-4.2514172 -4.2540026 -4.23805 -4.20701 -4.1815152 -4.157856 -4.1471462 -4.1616211 -4.1985221 -4.2349491 -4.2643113 -4.2849369 -4.2899628 -4.2877212 -4.2873192][-4.2558732 -4.2598119 -4.2499237 -4.2303896 -4.2140326 -4.1960497 -4.1832418 -4.1917496 -4.2244487 -4.2541037 -4.2744832 -4.2905536 -4.295886 -4.294364 -4.2935991]]...]
INFO - root - 2017-12-05 21:19:08.187426: step 44910, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 69h:45m:41s remains)
INFO - root - 2017-12-05 21:19:16.645792: step 44920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 67h:54m:55s remains)
INFO - root - 2017-12-05 21:19:25.294567: step 44930, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 68h:47m:40s remains)
INFO - root - 2017-12-05 21:19:33.870096: step 44940, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 69h:07m:24s remains)
INFO - root - 2017-12-05 21:19:42.229061: step 44950, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 68h:58m:21s remains)
INFO - root - 2017-12-05 21:19:50.775912: step 44960, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 67h:25m:09s remains)
INFO - root - 2017-12-05 21:19:59.157538: step 44970, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 68h:08m:49s remains)
INFO - root - 2017-12-05 21:20:07.720085: step 44980, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:46m:46s remains)
INFO - root - 2017-12-05 21:20:16.268498: step 44990, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.890 sec/batch; 71h:06m:44s remains)
INFO - root - 2017-12-05 21:20:24.824711: step 45000, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 67h:34m:10s remains)
2017-12-05 21:20:25.625482: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1782269 -4.1583772 -4.1579256 -4.1806049 -4.2021179 -4.2231083 -4.2348742 -4.2375951 -4.2349286 -4.2269998 -4.2140441 -4.2099495 -4.216136 -4.23093 -4.2419081][-4.1557651 -4.1368408 -4.1394415 -4.1633325 -4.1794181 -4.1962252 -4.2076545 -4.2107782 -4.2080979 -4.2020555 -4.1888094 -4.181911 -4.1876459 -4.2007031 -4.2080421][-4.1408086 -4.1197791 -4.1189132 -4.1345954 -4.1425772 -4.1533203 -4.1693826 -4.1781516 -4.1810966 -4.1763873 -4.16434 -4.1538944 -4.155354 -4.1683617 -4.1779423][-4.1481853 -4.1278796 -4.1244769 -4.1281881 -4.1239328 -4.1281734 -4.1451955 -4.1565156 -4.16389 -4.157321 -4.1421885 -4.126379 -4.1246815 -4.1374855 -4.1512251][-4.1726856 -4.1586461 -4.1507769 -4.1371007 -4.1189013 -4.1167188 -4.1293769 -4.140336 -4.1471915 -4.1346183 -4.1095338 -4.087286 -4.0894923 -4.1118193 -4.1343641][-4.1877217 -4.175488 -4.153789 -4.112885 -4.0769544 -4.0678449 -4.0830607 -4.0981622 -4.1107936 -4.0988822 -4.0700827 -4.0498986 -4.0590758 -4.0905418 -4.11329][-4.1847129 -4.1718607 -4.1388435 -4.0779152 -4.0273743 -4.0125818 -4.0352421 -4.0635095 -4.0843353 -4.081706 -4.060473 -4.049747 -4.0615969 -4.0854907 -4.0984225][-4.1839137 -4.179646 -4.1506915 -4.0926003 -4.0440817 -4.0208197 -4.0400119 -4.0713205 -4.0929623 -4.1002479 -4.0907879 -4.0841432 -4.0884671 -4.1042542 -4.1156983][-4.2055097 -4.2193165 -4.2088714 -4.1704631 -4.1259685 -4.0930419 -4.0956 -4.1140971 -4.1267843 -4.1349773 -4.1289992 -4.1197734 -4.1196809 -4.1385527 -4.1538677][-4.2272744 -4.2539291 -4.2624168 -4.2426753 -4.204669 -4.1632671 -4.1501179 -4.1554694 -4.1627507 -4.1702871 -4.1673861 -4.16033 -4.1590734 -4.1784034 -4.192421][-4.2300544 -4.2595282 -4.27703 -4.2701645 -4.2422366 -4.2036462 -4.1849895 -4.1844144 -4.1912432 -4.1985488 -4.1930962 -4.1859708 -4.1879549 -4.2076812 -4.2207432][-4.2178588 -4.2420025 -4.260838 -4.2597895 -4.2450104 -4.2200646 -4.20465 -4.2021794 -4.2031751 -4.202065 -4.1946554 -4.1931763 -4.1991034 -4.2169962 -4.2276087][-4.2109618 -4.2243547 -4.2359972 -4.2364988 -4.2330689 -4.2227659 -4.2133646 -4.2129645 -4.2081933 -4.1973891 -4.1851878 -4.1815381 -4.1860795 -4.2009206 -4.2167645][-4.2139945 -4.217833 -4.2244129 -4.2248282 -4.22386 -4.2184591 -4.2139411 -4.2162161 -4.2104793 -4.1943803 -4.1769018 -4.169549 -4.1732316 -4.1850538 -4.2016544][-4.2380342 -4.2366581 -4.2362752 -4.2320843 -4.2282248 -4.2246642 -4.2232 -4.2293396 -4.2272773 -4.2120013 -4.1939392 -4.1875029 -4.1899395 -4.1960964 -4.2078352]]...]
INFO - root - 2017-12-05 21:20:34.257817: step 45010, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 68h:07m:27s remains)
INFO - root - 2017-12-05 21:20:42.767301: step 45020, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 69h:37m:20s remains)
INFO - root - 2017-12-05 21:20:51.330636: step 45030, loss = 2.02, batch loss = 1.97 (9.5 examples/sec; 0.843 sec/batch; 67h:20m:50s remains)
INFO - root - 2017-12-05 21:20:59.864181: step 45040, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 67h:06m:25s remains)
INFO - root - 2017-12-05 21:21:08.415101: step 45050, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 68h:47m:53s remains)
INFO - root - 2017-12-05 21:21:16.924252: step 45060, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 68h:34m:14s remains)
INFO - root - 2017-12-05 21:21:25.309314: step 45070, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 66h:18m:38s remains)
INFO - root - 2017-12-05 21:21:33.874619: step 45080, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 68h:24m:29s remains)
INFO - root - 2017-12-05 21:21:42.311625: step 45090, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 67h:21m:25s remains)
INFO - root - 2017-12-05 21:21:50.639598: step 45100, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 66h:36m:58s remains)
2017-12-05 21:21:51.540535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1092281 -4.1155505 -4.1242347 -4.1360068 -4.1563182 -4.1772094 -4.17119 -4.1526561 -4.1335874 -4.1261826 -4.12761 -4.14858 -4.1800942 -4.190166 -4.1614933][-3.9906988 -4.003263 -4.0272055 -4.0521121 -4.1001053 -4.1618719 -4.1937871 -4.191834 -4.1688852 -4.1474943 -4.1278338 -4.1223841 -4.1324768 -4.1299672 -4.1061311][-3.9269991 -3.941047 -3.9757116 -4.0195546 -4.0869117 -4.1610789 -4.1969843 -4.19523 -4.1684666 -4.1353655 -4.094296 -4.062171 -4.0507522 -4.0428586 -4.0394382][-3.9722333 -3.9880652 -4.0230947 -4.0665822 -4.1235285 -4.1826162 -4.2067962 -4.1987658 -4.1674457 -4.1238542 -4.0669441 -4.022737 -3.9953163 -3.9883306 -4.0093875][-4.0663276 -4.0784717 -4.0998888 -4.1243677 -4.1578612 -4.194273 -4.2069407 -4.2019167 -4.176806 -4.145576 -4.1047039 -4.0696874 -4.0423493 -4.039041 -4.065691][-4.1553736 -4.1569715 -4.1525283 -4.1451349 -4.148519 -4.1668749 -4.1754489 -4.1735444 -4.1665144 -4.1684537 -4.1630898 -4.147686 -4.128037 -4.1257715 -4.1444221][-4.2174735 -4.204144 -4.1700282 -4.1241388 -4.0922904 -4.0904922 -4.0971918 -4.101099 -4.1182432 -4.1582828 -4.1857109 -4.1891227 -4.1768594 -4.1829567 -4.203424][-4.2475233 -4.2174993 -4.1514559 -4.0642509 -4.00207 -3.9864607 -3.9960356 -4.0158739 -4.0615335 -4.1311736 -4.1797175 -4.1950655 -4.1969485 -4.2189431 -4.2468791][-4.2589059 -4.2186565 -4.136683 -4.0321445 -3.9607913 -3.9419391 -3.961843 -3.9984317 -4.0568953 -4.1300745 -4.1817141 -4.2064877 -4.2239318 -4.2553191 -4.2854929][-4.2620444 -4.2260618 -4.1543717 -4.0667539 -4.012567 -4.0007191 -4.0238357 -4.0570655 -4.1058431 -4.1663027 -4.2132068 -4.2411504 -4.2627168 -4.2897506 -4.3097825][-4.2687025 -4.2477756 -4.2028565 -4.1489048 -4.1163464 -4.1069098 -4.1215425 -4.1443615 -4.1783838 -4.2212982 -4.25715 -4.2803717 -4.2958932 -4.3125062 -4.32134][-4.2767816 -4.2710166 -4.2510209 -4.2259874 -4.2101436 -4.2039814 -4.2115974 -4.2267957 -4.2488165 -4.2753758 -4.29878 -4.3129673 -4.3203425 -4.3254309 -4.324645][-4.2900934 -4.2973733 -4.2948766 -4.2882628 -4.2841034 -4.2824774 -4.2854652 -4.2926154 -4.303112 -4.31426 -4.3232617 -4.3268738 -4.3275294 -4.3251896 -4.3165932][-4.3057852 -4.321753 -4.3284616 -4.3302011 -4.3313131 -4.3315144 -4.3321061 -4.331902 -4.332324 -4.3335543 -4.3337154 -4.32988 -4.324532 -4.3156486 -4.3030586][-4.3235474 -4.3447609 -4.3541751 -4.3569489 -4.357399 -4.3556461 -4.3518138 -4.3457022 -4.3404117 -4.3377409 -4.3352056 -4.3310819 -4.3244123 -4.3136444 -4.3017368]]...]
INFO - root - 2017-12-05 21:22:00.030537: step 45110, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.837 sec/batch; 66h:50m:55s remains)
INFO - root - 2017-12-05 21:22:08.578628: step 45120, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 69h:07m:12s remains)
INFO - root - 2017-12-05 21:22:17.132205: step 45130, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 65h:43m:09s remains)
INFO - root - 2017-12-05 21:22:25.602928: step 45140, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:43m:44s remains)
INFO - root - 2017-12-05 21:22:33.986705: step 45150, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 65h:42m:56s remains)
INFO - root - 2017-12-05 21:22:42.468242: step 45160, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 67h:18m:43s remains)
INFO - root - 2017-12-05 21:22:50.937522: step 45170, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 70h:51m:39s remains)
INFO - root - 2017-12-05 21:22:59.270164: step 45180, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 67h:34m:53s remains)
INFO - root - 2017-12-05 21:23:07.786150: step 45190, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 68h:21m:30s remains)
INFO - root - 2017-12-05 21:23:16.383255: step 45200, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 69h:45m:31s remains)
2017-12-05 21:23:17.164833: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1072893 -4.1199055 -4.1300411 -4.1120429 -4.09068 -4.0588846 -4.03469 -4.0250387 -4.0397797 -4.0853758 -4.1391339 -4.2038331 -4.2662859 -4.305469 -4.3273106][-4.0982032 -4.1056137 -4.1137733 -4.0985622 -4.0877275 -4.060782 -4.0341706 -4.0242786 -4.0326357 -4.0767417 -4.1439404 -4.2133493 -4.274488 -4.3114247 -4.333436][-4.0816846 -4.093226 -4.1045728 -4.0893869 -4.0773053 -4.0537157 -4.0255675 -4.0148025 -4.028358 -4.0729651 -4.1472015 -4.2203655 -4.2782283 -4.31284 -4.3344369][-4.0722294 -4.0909705 -4.1027884 -4.0733609 -4.0509472 -4.0374146 -4.0174584 -4.0071535 -4.0323954 -4.0821047 -4.152575 -4.226974 -4.2822757 -4.3127775 -4.3339891][-4.0597219 -4.0883431 -4.0891137 -4.0421143 -4.0112066 -4.0113459 -3.9963834 -3.9856131 -4.0278997 -4.0898104 -4.1616755 -4.2368388 -4.2896042 -4.3167048 -4.3364105][-4.0389557 -4.0624876 -4.0505929 -3.9906294 -3.9645944 -3.9726377 -3.9403145 -3.912822 -3.98068 -4.068377 -4.1515512 -4.2340117 -4.2908092 -4.3181896 -4.3379812][-4.028574 -4.0386119 -4.0233645 -3.96256 -3.933188 -3.926085 -3.8502111 -3.7786973 -3.8800333 -4.0021625 -4.1048269 -4.2059469 -4.2762485 -4.312583 -4.335815][-4.0653563 -4.06852 -4.0661473 -4.0230761 -3.981987 -3.9383407 -3.8223951 -3.708261 -3.8215308 -3.9597342 -4.0650167 -4.1743274 -4.2563281 -4.30305 -4.3308849][-4.1281796 -4.1333733 -4.1438837 -4.1243954 -4.0811195 -4.0252581 -3.9302726 -3.8448234 -3.9096169 -4.00535 -4.082963 -4.1738558 -4.2504311 -4.2989988 -4.328341][-4.1645856 -4.1728964 -4.189579 -4.1848946 -4.1548023 -4.1108675 -4.0467806 -3.9942765 -4.0203848 -4.073091 -4.1268287 -4.1935315 -4.2546139 -4.2971673 -4.3253331][-4.1696243 -4.186964 -4.2085085 -4.2125072 -4.1947665 -4.16193 -4.114284 -4.0786562 -4.0874925 -4.1221066 -4.1663389 -4.2195039 -4.2676353 -4.2999191 -4.3233819][-4.1784143 -4.1968 -4.2172003 -4.2235632 -4.2118106 -4.1848693 -4.1497145 -4.1275649 -4.1325374 -4.1601496 -4.1986866 -4.2436948 -4.2829905 -4.3055792 -4.3246183][-4.1993351 -4.2097583 -4.223587 -4.2276835 -4.2186503 -4.1982861 -4.17378 -4.1620197 -4.1664863 -4.1908679 -4.2281456 -4.2687259 -4.2999153 -4.3144054 -4.3280835][-4.2339697 -4.2364755 -4.2447963 -4.2446117 -4.2360072 -4.2196236 -4.2032242 -4.1970897 -4.2007103 -4.2211657 -4.2535777 -4.2866364 -4.3125191 -4.3240376 -4.3354611][-4.2739286 -4.2736826 -4.277359 -4.2744179 -4.2664709 -4.2544661 -4.2445345 -4.2392669 -4.23958 -4.2521687 -4.2758956 -4.3000207 -4.3205056 -4.33261 -4.34407]]...]
INFO - root - 2017-12-05 21:23:25.584941: step 45210, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 67h:34m:21s remains)
INFO - root - 2017-12-05 21:23:34.097375: step 45220, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 69h:11m:11s remains)
INFO - root - 2017-12-05 21:23:42.652693: step 45230, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 68h:01m:05s remains)
INFO - root - 2017-12-05 21:23:51.257547: step 45240, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 71h:17m:44s remains)
INFO - root - 2017-12-05 21:23:59.624816: step 45250, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 69h:07m:22s remains)
INFO - root - 2017-12-05 21:24:08.057373: step 45260, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 66h:44m:58s remains)
INFO - root - 2017-12-05 21:24:16.536954: step 45270, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 66h:40m:20s remains)
INFO - root - 2017-12-05 21:24:25.048082: step 45280, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 69h:08m:41s remains)
INFO - root - 2017-12-05 21:24:33.471444: step 45290, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 65h:27m:31s remains)
INFO - root - 2017-12-05 21:24:41.941897: step 45300, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 68h:08m:19s remains)
2017-12-05 21:24:42.717222: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.159481 -4.1417694 -4.1342106 -4.145503 -4.1673551 -4.1809239 -4.1684084 -4.1245694 -4.0906758 -4.0988631 -4.1305442 -4.1849909 -4.2386971 -4.2897491 -4.3237357][-4.1441526 -4.1200438 -4.1066031 -4.1172009 -4.1419029 -4.1517138 -4.1308246 -4.0744867 -4.03139 -4.0374451 -4.077877 -4.1482248 -4.2163038 -4.279367 -4.3193493][-4.1532121 -4.1278658 -4.1127577 -4.1224413 -4.1452894 -4.1416807 -4.1061258 -4.0410256 -3.9937088 -4.0027418 -4.0541024 -4.1380081 -4.2153473 -4.2824445 -4.3209538][-4.1787219 -4.1587181 -4.1474485 -4.1600094 -4.1757183 -4.1504946 -4.0925422 -4.0155706 -3.9617558 -3.9805508 -4.0478144 -4.1415491 -4.2262373 -4.2931647 -4.3271608][-4.2011452 -4.1884956 -4.1815629 -4.1953244 -4.1991844 -4.1495881 -4.0695944 -3.9727917 -3.9091313 -3.9447653 -4.0301909 -4.13371 -4.2252755 -4.2946825 -4.3287711][-4.1993127 -4.1932044 -4.1890054 -4.1970229 -4.1842375 -4.1105423 -4.0073652 -3.8871078 -3.8173165 -3.8745222 -3.9827166 -4.1043539 -4.2058139 -4.2811 -4.3194547][-4.1488137 -4.1542668 -4.1536841 -4.1519542 -4.1248007 -4.0348253 -3.9048529 -3.7520804 -3.6765306 -3.7670991 -3.9104409 -4.0560627 -4.1724029 -4.2570295 -4.3030047][-4.0675225 -4.086484 -4.09746 -4.0976677 -4.061203 -3.9641159 -3.8231313 -3.6556637 -3.5856719 -3.7004905 -3.8626564 -4.02174 -4.1487575 -4.241487 -4.2956][-4.0381694 -4.0743289 -4.10307 -4.1142311 -4.0822616 -3.9993682 -3.883167 -3.7465529 -3.690433 -3.7731194 -3.8977146 -4.03486 -4.1551585 -4.2461929 -4.3006754][-4.0858159 -4.1232653 -4.153729 -4.1709385 -4.1466632 -4.0831113 -4.0035229 -3.9055943 -3.8591902 -3.9055967 -3.98186 -4.0817857 -4.183197 -4.2648783 -4.3130131][-4.15071 -4.1808105 -4.204627 -4.2219181 -4.2011786 -4.150435 -4.0952044 -4.023221 -3.9832785 -4.0066228 -4.0522327 -4.126545 -4.2133 -4.2847419 -4.3244915][-4.2037091 -4.2230749 -4.2395396 -4.2549653 -4.2398658 -4.2024417 -4.1645803 -4.11118 -4.0756993 -4.0831704 -4.1101494 -4.16975 -4.2441278 -4.3033867 -4.3347673][-4.2457652 -4.2568011 -4.2693987 -4.2811146 -4.268774 -4.2446208 -4.2226281 -4.1851134 -4.1540685 -4.1501722 -4.164362 -4.21001 -4.2725086 -4.3209772 -4.34518][-4.2770724 -4.2846541 -4.2946196 -4.2971425 -4.282887 -4.2692838 -4.2547436 -4.2237434 -4.1944938 -4.1871982 -4.1997156 -4.2376351 -4.289547 -4.330287 -4.3491116][-4.2888556 -4.2929087 -4.2971249 -4.2897873 -4.2714839 -4.2652988 -4.2572551 -4.2296987 -4.2047215 -4.2013083 -4.2176356 -4.251277 -4.2936845 -4.3287969 -4.3447576]]...]
INFO - root - 2017-12-05 21:24:51.291933: step 45310, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 68h:13m:15s remains)
INFO - root - 2017-12-05 21:24:59.725985: step 45320, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.830 sec/batch; 66h:11m:51s remains)
INFO - root - 2017-12-05 21:25:08.190199: step 45330, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.833 sec/batch; 66h:28m:44s remains)
INFO - root - 2017-12-05 21:25:16.696399: step 45340, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.832 sec/batch; 66h:23m:23s remains)
INFO - root - 2017-12-05 21:25:25.211130: step 45350, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 69h:38m:39s remains)
INFO - root - 2017-12-05 21:25:33.778003: step 45360, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 66h:35m:58s remains)
INFO - root - 2017-12-05 21:25:42.285853: step 45370, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 68h:25m:45s remains)
INFO - root - 2017-12-05 21:25:50.898528: step 45380, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.874 sec/batch; 69h:44m:09s remains)
INFO - root - 2017-12-05 21:25:59.459704: step 45390, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 70h:11m:58s remains)
INFO - root - 2017-12-05 21:26:07.860028: step 45400, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:40m:17s remains)
2017-12-05 21:26:08.655296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581019 -4.254673 -4.2532811 -4.2542939 -4.2577124 -4.2622991 -4.2661829 -4.2672234 -4.267211 -4.2678285 -4.2705693 -4.2686577 -4.2576351 -4.2348928 -4.214283][-4.2450781 -4.2405925 -4.2379584 -4.2385397 -4.2432051 -4.2506814 -4.2578783 -4.2607193 -4.2622862 -4.263566 -4.2657485 -4.2645149 -4.2558408 -4.2412009 -4.2329478][-4.218142 -4.2104945 -4.2052927 -4.2046843 -4.2131586 -4.2255359 -4.2372837 -4.2437167 -4.2479939 -4.2507544 -4.2546415 -4.2568479 -4.2532706 -4.2487292 -4.2521296][-4.180892 -4.16815 -4.1590562 -4.1583362 -4.1733274 -4.1917777 -4.2080679 -4.2186065 -4.2267652 -4.2315674 -4.238328 -4.2441983 -4.24341 -4.2445107 -4.2558107][-4.1340594 -4.1138477 -4.0996823 -4.1001582 -4.1201763 -4.1412992 -4.1581488 -4.1723762 -4.1852784 -4.1963582 -4.20912 -4.2198091 -4.2205248 -4.22229 -4.237464][-4.1020412 -4.0705457 -4.0482349 -4.0450835 -4.0651016 -4.0834403 -4.0965543 -4.1105633 -4.1287441 -4.1499348 -4.1730652 -4.1907015 -4.1921763 -4.1917691 -4.2105985][-4.0848823 -4.04485 -4.0137534 -4.0028262 -4.0192137 -4.0335851 -4.0424347 -4.0543256 -4.0757809 -4.1076484 -4.1415086 -4.1677618 -4.1712651 -4.1667347 -4.1848683][-4.0838418 -4.0510917 -4.0195069 -4.0022831 -4.0086188 -4.0183706 -4.0249319 -4.0354815 -4.0577259 -4.0964622 -4.1386003 -4.1698971 -4.1728754 -4.1608224 -4.1713543][-4.0986834 -4.0848274 -4.0650554 -4.0503564 -4.0505857 -4.057013 -4.0639539 -4.074604 -4.0925083 -4.1260185 -4.1658106 -4.1962943 -4.1967649 -4.1790719 -4.17957][-4.1319747 -4.1377563 -4.1334605 -4.1258903 -4.1232667 -4.1267042 -4.1345444 -4.14441 -4.1569104 -4.1783657 -4.2058163 -4.2293153 -4.2303848 -4.2141037 -4.2080312][-4.1855106 -4.1990371 -4.2042308 -4.2009683 -4.1958094 -4.1981382 -4.2062645 -4.2155542 -4.2244778 -4.234653 -4.2477751 -4.2605734 -4.2593756 -4.2454305 -4.2361584][-4.2374282 -4.2506976 -4.2571697 -4.2564359 -4.2525949 -4.2540331 -4.260324 -4.2694793 -4.2765279 -4.2804503 -4.2826509 -4.2853041 -4.2775793 -4.2618804 -4.2510056][-4.2671766 -4.2780643 -4.2824268 -4.2827005 -4.2800035 -4.2808728 -4.2853012 -4.2926979 -4.297904 -4.3006454 -4.2999377 -4.2963219 -4.2808447 -4.2599673 -4.2476287][-4.2636533 -4.2687688 -4.2702966 -4.2701907 -4.268856 -4.2692218 -4.2728271 -4.27763 -4.2798963 -4.281517 -4.2820177 -4.2775559 -4.2600474 -4.2388048 -4.2283645][-4.2487268 -4.2513747 -4.2508855 -4.2506881 -4.2504287 -4.2508388 -4.2540927 -4.2585173 -4.2606759 -4.2625189 -4.2643819 -4.2622037 -4.2447958 -4.22087 -4.2103662]]...]
INFO - root - 2017-12-05 21:26:17.298465: step 45410, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 69h:44m:23s remains)
INFO - root - 2017-12-05 21:26:25.866923: step 45420, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 68h:16m:09s remains)
INFO - root - 2017-12-05 21:26:34.447549: step 45430, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 68h:16m:03s remains)
INFO - root - 2017-12-05 21:26:42.985785: step 45440, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 64h:57m:52s remains)
INFO - root - 2017-12-05 21:26:51.445744: step 45450, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 68h:15m:20s remains)
INFO - root - 2017-12-05 21:27:00.033232: step 45460, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 70h:23m:20s remains)
INFO - root - 2017-12-05 21:27:08.525999: step 45470, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 67h:34m:12s remains)
INFO - root - 2017-12-05 21:27:17.196055: step 45480, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 67h:53m:38s remains)
INFO - root - 2017-12-05 21:27:25.877997: step 45490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 67h:00m:53s remains)
INFO - root - 2017-12-05 21:27:34.523276: step 45500, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.837 sec/batch; 66h:45m:47s remains)
2017-12-05 21:27:35.267863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2529197 -4.2427645 -4.2461257 -4.2537165 -4.25438 -4.25034 -4.2496634 -4.244977 -4.2432942 -4.2454205 -4.2445788 -4.240685 -4.2330623 -4.2274733 -4.2246017][-4.2406554 -4.2294841 -4.2332358 -4.23929 -4.234786 -4.2224112 -4.2140212 -4.2085543 -4.2171884 -4.236753 -4.2530036 -4.259758 -4.2566204 -4.2482996 -4.2395139][-4.2330508 -4.2197495 -4.2223787 -4.2277431 -4.2215948 -4.2054524 -4.18898 -4.1801767 -4.1971369 -4.2318568 -4.2616267 -4.2780004 -4.2794347 -4.269269 -4.2576036][-4.238349 -4.220602 -4.2173309 -4.2189431 -4.21224 -4.1931744 -4.1673727 -4.1510029 -4.1703687 -4.2166505 -4.25751 -4.280529 -4.283968 -4.2705235 -4.2600169][-4.2529078 -4.228498 -4.2162104 -4.2116566 -4.1986556 -4.1692257 -4.127902 -4.0959744 -4.1140089 -4.1752639 -4.230217 -4.2610612 -4.2681184 -4.2534194 -4.24627][-4.2667289 -4.2390409 -4.2194409 -4.2065825 -4.1827207 -4.1365337 -4.0706882 -4.0135779 -4.02459 -4.1071143 -4.1847577 -4.2305369 -4.2483983 -4.2370505 -4.2316332][-4.2701211 -4.2429452 -4.2216625 -4.2064691 -4.1788154 -4.1240273 -4.0441136 -3.967437 -3.9704792 -4.0665812 -4.1601162 -4.2177014 -4.245573 -4.241652 -4.2370062][-4.2635627 -4.2389359 -4.2215266 -4.214695 -4.1988969 -4.1579437 -4.0956163 -4.0338526 -4.0318851 -4.105937 -4.1835475 -4.2350092 -4.2633648 -4.2666621 -4.2642603][-4.2531152 -4.234726 -4.2260652 -4.230504 -4.2311392 -4.2137971 -4.1802769 -4.1430693 -4.1361914 -4.1753607 -4.220448 -4.2548623 -4.2789197 -4.2892127 -4.2913032][-4.2497654 -4.239531 -4.2389841 -4.249774 -4.2585678 -4.2558117 -4.241169 -4.2175388 -4.2037735 -4.2186885 -4.2413769 -4.2600079 -4.2776971 -4.2906609 -4.296411][-4.2447338 -4.2394567 -4.2436509 -4.2580142 -4.2674508 -4.2675633 -4.25986 -4.24127 -4.2258406 -4.2326617 -4.2483926 -4.2582188 -4.267417 -4.2794623 -4.2908683][-4.2335587 -4.2284408 -4.2364063 -4.2552419 -4.2640505 -4.264246 -4.2581296 -4.2423449 -4.2298927 -4.2372446 -4.2547474 -4.259583 -4.2610588 -4.270092 -4.2832704][-4.2192316 -4.2109437 -4.22182 -4.2457509 -4.2555737 -4.2590342 -4.2565055 -4.2450533 -4.2330208 -4.2363582 -4.2538562 -4.2591872 -4.2576084 -4.2625856 -4.2727857][-4.2033548 -4.1877956 -4.1990094 -4.2270174 -4.240591 -4.2467256 -4.2476721 -4.2397118 -4.2269311 -4.2270918 -4.2434335 -4.2536645 -4.2556286 -4.2576365 -4.2634134][-4.2005043 -4.1766982 -4.1833515 -4.212018 -4.2267995 -4.230351 -4.2277312 -4.2180953 -4.2070932 -4.20807 -4.2233477 -4.2371373 -4.2430148 -4.245945 -4.2518382]]...]
INFO - root - 2017-12-05 21:27:43.986500: step 45510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 69h:02m:01s remains)
INFO - root - 2017-12-05 21:27:52.530744: step 45520, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 66h:38m:47s remains)
INFO - root - 2017-12-05 21:28:01.226204: step 45530, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.898 sec/batch; 71h:32m:52s remains)
INFO - root - 2017-12-05 21:28:09.731383: step 45540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:33m:14s remains)
INFO - root - 2017-12-05 21:28:18.142287: step 45550, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 67h:16m:36s remains)
INFO - root - 2017-12-05 21:28:26.721023: step 45560, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 68h:09m:40s remains)
INFO - root - 2017-12-05 21:28:35.365742: step 45570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 68h:58m:03s remains)
INFO - root - 2017-12-05 21:28:44.036849: step 45580, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 69h:07m:15s remains)
INFO - root - 2017-12-05 21:28:52.569234: step 45590, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 68h:54m:22s remains)
INFO - root - 2017-12-05 21:29:01.105261: step 45600, loss = 2.09, batch loss = 2.04 (10.4 examples/sec; 0.768 sec/batch; 61h:14m:03s remains)
2017-12-05 21:29:01.877174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2604508 -4.2734785 -4.2688894 -4.2652164 -4.2676177 -4.2659359 -4.2623219 -4.2579756 -4.2617979 -4.2782545 -4.3006821 -4.3100996 -4.2968736 -4.2821269 -4.2710953][-4.2513905 -4.262064 -4.257421 -4.2557158 -4.2617345 -4.26622 -4.267487 -4.26224 -4.2589121 -4.2678013 -4.2867031 -4.2983346 -4.2889051 -4.2779675 -4.2708731][-4.2288294 -4.2405648 -4.2398233 -4.2407045 -4.2476058 -4.2545419 -4.2555447 -4.2498927 -4.2464104 -4.2503867 -4.2611036 -4.2684555 -4.2630782 -4.2573671 -4.2593031][-4.1938839 -4.2077556 -4.21096 -4.213798 -4.2202339 -4.2255831 -4.2220836 -4.2197061 -4.2230463 -4.2305241 -4.2342329 -4.2304149 -4.2209349 -4.21663 -4.2272148][-4.1742864 -4.1803651 -4.1773977 -4.1750159 -4.1763573 -4.174118 -4.1650524 -4.1676164 -4.1838551 -4.197866 -4.196094 -4.1849318 -4.1707306 -4.1675572 -4.1835647][-4.1678357 -4.1566043 -4.1380806 -4.1209326 -4.1067734 -4.0861 -4.0677643 -4.080071 -4.1133814 -4.1349621 -4.1343884 -4.1259508 -4.1208272 -4.1269531 -4.1520591][-4.1636147 -4.1337776 -4.0992541 -4.0683904 -4.036469 -3.9937792 -3.9662549 -3.9915254 -4.0452776 -4.0813322 -4.0910821 -4.0938764 -4.1043692 -4.1260514 -4.1613631][-4.1652036 -4.1360769 -4.1052332 -4.0779777 -4.0488005 -4.0094981 -3.9916718 -4.0249405 -4.0833178 -4.1261382 -4.1431246 -4.1504841 -4.1640563 -4.1881695 -4.2202229][-4.1881142 -4.173203 -4.1565928 -4.1421757 -4.1250887 -4.0991745 -4.091783 -4.1174726 -4.1584349 -4.19162 -4.2051568 -4.2112327 -4.2223835 -4.2426643 -4.2677155][-4.2131853 -4.2116923 -4.2071514 -4.2007709 -4.191246 -4.1751938 -4.1728997 -4.1874804 -4.2095981 -4.2311344 -4.2408266 -4.24634 -4.2570338 -4.2733493 -4.2912374][-4.238935 -4.2424464 -4.2419972 -4.2386255 -4.2315068 -4.2229357 -4.22344 -4.2312231 -4.2405076 -4.2518282 -4.2592969 -4.2648015 -4.2734966 -4.2868185 -4.3014607][-4.2639213 -4.2680521 -4.2697682 -4.2691197 -4.2645664 -4.2610826 -4.2625923 -4.2680936 -4.2739582 -4.2794003 -4.28243 -4.2828474 -4.2860579 -4.295022 -4.3060036][-4.2732706 -4.27619 -4.2807627 -4.2839346 -4.2830005 -4.2830591 -4.28468 -4.2877035 -4.2891359 -4.2891426 -4.2871909 -4.2831659 -4.2821274 -4.2872663 -4.2930837][-4.2820129 -4.2821527 -4.2864642 -4.2900195 -4.2900815 -4.2912884 -4.2927823 -4.2939911 -4.2933097 -4.2903085 -4.2857218 -4.2798719 -4.2773676 -4.2801046 -4.2815695][-4.2932897 -4.2921 -4.2939782 -4.2949681 -4.2940021 -4.294241 -4.2952785 -4.29669 -4.2974358 -4.2972732 -4.2958784 -4.2934165 -4.2923446 -4.2937388 -4.2926331]]...]
INFO - root - 2017-12-05 21:29:10.455781: step 45610, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 67h:09m:06s remains)
INFO - root - 2017-12-05 21:29:18.976894: step 45620, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 66h:43m:59s remains)
INFO - root - 2017-12-05 21:29:27.618985: step 45630, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 66h:29m:01s remains)
INFO - root - 2017-12-05 21:29:36.144657: step 45640, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 67h:17m:40s remains)
INFO - root - 2017-12-05 21:29:44.454695: step 45650, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 68h:10m:41s remains)
INFO - root - 2017-12-05 21:29:53.147376: step 45660, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 67h:27m:55s remains)
INFO - root - 2017-12-05 21:30:01.698097: step 45670, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 65h:34m:18s remains)
INFO - root - 2017-12-05 21:30:10.183771: step 45680, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 69h:11m:42s remains)
INFO - root - 2017-12-05 21:30:18.876763: step 45690, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 69h:09m:45s remains)
INFO - root - 2017-12-05 21:30:27.365851: step 45700, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:33m:04s remains)
2017-12-05 21:30:28.119363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2998571 -4.2889342 -4.2788081 -4.2697582 -4.2628851 -4.2579784 -4.256516 -4.2586331 -4.26229 -4.2645965 -4.2532396 -4.228909 -4.1959071 -4.1661205 -4.1464329][-4.3071012 -4.2927628 -4.2790256 -4.2682085 -4.26085 -4.2538075 -4.2498131 -4.2537022 -4.2636695 -4.2727022 -4.2679267 -4.2475948 -4.2135587 -4.1780024 -4.149456][-4.2987 -4.2801094 -4.2619157 -4.2471204 -4.23837 -4.2305789 -4.2278786 -4.2356358 -4.2501416 -4.2661791 -4.2687883 -4.2526937 -4.2178016 -4.1770868 -4.1429086][-4.2861333 -4.26005 -4.23146 -4.2043881 -4.187026 -4.17621 -4.1727209 -4.1839886 -4.20912 -4.2387705 -4.2530856 -4.2412958 -4.2079458 -4.1672883 -4.1360364][-4.2759438 -4.2434492 -4.2028117 -4.1615224 -4.1306477 -4.1049571 -4.0908785 -4.10206 -4.1422238 -4.1916685 -4.2188606 -4.2124643 -4.1843996 -4.1506739 -4.1279993][-4.2721076 -4.2398324 -4.1941872 -4.14385 -4.0992785 -4.052516 -4.0143375 -4.0125165 -4.0593424 -4.125422 -4.1691165 -4.1740022 -4.1569414 -4.1376586 -4.1244721][-4.276185 -4.2510858 -4.2104959 -4.1570797 -4.0988097 -4.0305276 -3.9618473 -3.933882 -3.97652 -4.0554175 -4.1247029 -4.1503267 -4.1451626 -4.1347942 -4.1224146][-4.2886806 -4.2717385 -4.2401843 -4.189311 -4.123198 -4.0400834 -3.9514079 -3.9016886 -3.9282947 -4.0072784 -4.0946031 -4.1424265 -4.1544523 -4.1502438 -4.1335845][-4.3079128 -4.2989683 -4.2762313 -4.2326636 -4.1708245 -4.0885 -3.996702 -3.9351692 -3.944504 -4.0098767 -4.0972075 -4.1556959 -4.1828642 -4.1867051 -4.1677713][-4.3257928 -4.3211493 -4.3030925 -4.2664647 -4.2146754 -4.1450338 -4.0669084 -4.008769 -4.0059109 -4.0547032 -4.1320276 -4.1944771 -4.230998 -4.239614 -4.2222366][-4.3380585 -4.332675 -4.3161535 -4.2869225 -4.2456822 -4.19202 -4.1323862 -4.0831728 -4.0774813 -4.1176786 -4.1837735 -4.2465477 -4.2863312 -4.2995319 -4.2881722][-4.3428245 -4.33728 -4.3227048 -4.2998986 -4.2717085 -4.2355933 -4.1952448 -4.1580353 -4.1519852 -4.184443 -4.2371573 -4.2929683 -4.3317022 -4.3473277 -4.3409557][-4.3408484 -4.3363743 -4.3267078 -4.3127656 -4.2978425 -4.2781148 -4.2528543 -4.2273297 -4.2216458 -4.2447548 -4.2819452 -4.3241243 -4.3548808 -4.3675747 -4.3628945][-4.3332314 -4.3322182 -4.3285174 -4.32328 -4.3187551 -4.3112507 -4.2967629 -4.2802196 -4.2769489 -4.2937012 -4.3194475 -4.34585 -4.3619909 -4.3656449 -4.3595233][-4.3256922 -4.3303061 -4.3321829 -4.3314166 -4.3294225 -4.3239241 -4.3139424 -4.3055892 -4.30757 -4.3214731 -4.3383036 -4.3527632 -4.3571348 -4.3537469 -4.3468289]]...]
INFO - root - 2017-12-05 21:30:36.821710: step 45710, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 69h:52m:22s remains)
INFO - root - 2017-12-05 21:30:45.413231: step 45720, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 69h:18m:45s remains)
INFO - root - 2017-12-05 21:30:54.015347: step 45730, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.811 sec/batch; 64h:34m:22s remains)
INFO - root - 2017-12-05 21:31:02.502529: step 45740, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 68h:20m:55s remains)
INFO - root - 2017-12-05 21:31:10.954107: step 45750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 69h:29m:57s remains)
INFO - root - 2017-12-05 21:31:19.503333: step 45760, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 68h:13m:48s remains)
INFO - root - 2017-12-05 21:31:28.138235: step 45770, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:34m:23s remains)
INFO - root - 2017-12-05 21:31:36.647058: step 45780, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 69h:10m:28s remains)
INFO - root - 2017-12-05 21:31:45.305591: step 45790, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 68h:58m:00s remains)
INFO - root - 2017-12-05 21:31:53.892830: step 45800, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 68h:39m:51s remains)
2017-12-05 21:31:54.697329: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3171773 -4.2873693 -4.245039 -4.2094154 -4.1986647 -4.189394 -4.1604595 -4.1582627 -4.1859636 -4.2001438 -4.1987448 -4.159502 -4.1125712 -4.0768371 -4.0720878][-4.3174262 -4.2871919 -4.2447062 -4.2087541 -4.1939049 -4.1753025 -4.1442857 -4.1481886 -4.1833205 -4.2043605 -4.2039661 -4.1605225 -4.110621 -4.0779781 -4.0825076][-4.3073397 -4.2757297 -4.2334957 -4.19607 -4.1762838 -4.1505175 -4.1183019 -4.1310811 -4.1773944 -4.2036304 -4.2008829 -4.1529608 -4.1005421 -4.0732312 -4.0859919][-4.2940292 -4.2584219 -4.2138562 -4.1756024 -4.1494441 -4.1115017 -4.0746689 -4.1036367 -4.1698461 -4.2084389 -4.2102485 -4.1622157 -4.104012 -4.0764947 -4.0858669][-4.2832317 -4.2403955 -4.1885738 -4.1413803 -4.0984974 -4.0371637 -3.9889116 -4.0388527 -4.1331887 -4.1917686 -4.2126417 -4.1727734 -4.1165648 -4.0933275 -4.0966711][-4.2743392 -4.2255878 -4.1637535 -4.0984 -4.0277934 -3.9292729 -3.8546491 -3.9280429 -4.0615621 -4.14728 -4.1946936 -4.1720524 -4.1294751 -4.1152172 -4.1130815][-4.2688479 -4.2185626 -4.1481495 -4.0633063 -3.9680362 -3.8341064 -3.7154946 -3.7970419 -3.9710979 -4.0845408 -4.1624141 -4.1704125 -4.1498976 -4.1396985 -4.1216412][-4.2667408 -4.2161169 -4.1436214 -4.04993 -3.9466143 -3.7981427 -3.6413095 -3.702843 -3.8944743 -4.0266356 -4.1279774 -4.166307 -4.1672864 -4.1577682 -4.1244812][-4.2701988 -4.2208219 -4.15433 -4.0703321 -3.9858034 -3.8687291 -3.7254822 -3.7470829 -3.9071152 -4.0282869 -4.1251831 -4.1722927 -4.1837416 -4.1732121 -4.1354818][-4.2784967 -4.2336178 -4.1758986 -4.1061411 -4.0419269 -3.9654856 -3.8645978 -3.8677213 -3.9772031 -4.0699172 -4.1452842 -4.1855645 -4.1988616 -4.189539 -4.1559367][-4.28764 -4.2478247 -4.1985321 -4.1406522 -4.0914216 -4.0419006 -3.9773061 -3.9772511 -4.0478773 -4.1120977 -4.1619964 -4.19198 -4.2084713 -4.2105756 -4.1901989][-4.2960587 -4.262476 -4.2209134 -4.1734238 -4.1396995 -4.1117496 -4.0751967 -4.0765948 -4.1185741 -4.1552844 -4.182601 -4.2023835 -4.2217488 -4.2349796 -4.2301555][-4.3008361 -4.2698264 -4.2302313 -4.1876669 -4.1613626 -4.1451354 -4.126606 -4.135726 -4.1659579 -4.1843486 -4.1971092 -4.211494 -4.2344851 -4.2526164 -4.2540112][-4.3030157 -4.2709394 -4.2294555 -4.1855 -4.159142 -4.1444993 -4.1324944 -4.1523423 -4.1853461 -4.1954851 -4.199625 -4.2118006 -4.2397041 -4.2580066 -4.2601242][-4.3032002 -4.2686 -4.2243409 -4.1779351 -4.1453576 -4.1231456 -4.1068277 -4.1317463 -4.17243 -4.1802979 -4.1791468 -4.1925712 -4.2240467 -4.2422886 -4.2473965]]...]
INFO - root - 2017-12-05 21:32:03.293567: step 45810, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 67h:01m:38s remains)
INFO - root - 2017-12-05 21:32:11.842576: step 45820, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.878 sec/batch; 69h:55m:24s remains)
INFO - root - 2017-12-05 21:32:20.317740: step 45830, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 66h:19m:20s remains)
INFO - root - 2017-12-05 21:32:29.057028: step 45840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 67h:48m:37s remains)
INFO - root - 2017-12-05 21:32:37.566729: step 45850, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 68h:17m:09s remains)
INFO - root - 2017-12-05 21:32:46.048293: step 45860, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 67h:22m:51s remains)
INFO - root - 2017-12-05 21:32:54.601852: step 45870, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 70h:10m:01s remains)
INFO - root - 2017-12-05 21:33:03.223084: step 45880, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 68h:01m:27s remains)
INFO - root - 2017-12-05 21:33:11.797470: step 45890, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 69h:05m:49s remains)
INFO - root - 2017-12-05 21:33:20.312127: step 45900, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 69h:10m:21s remains)
2017-12-05 21:33:21.044080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3188071 -4.2978678 -4.2579069 -4.2039237 -4.148962 -4.0987639 -4.0660849 -4.0682273 -4.0930195 -4.1228991 -4.1531224 -4.166749 -4.1697078 -4.1694856 -4.1673284][-4.3167591 -4.2979426 -4.2667274 -4.2290163 -4.1943359 -4.1609344 -4.1354327 -4.1267295 -4.1320434 -4.1421294 -4.1524849 -4.15031 -4.1419091 -4.1352353 -4.1258144][-4.3159733 -4.296948 -4.2705226 -4.24387 -4.2226963 -4.2016253 -4.1791172 -4.162704 -4.1588421 -4.1614847 -4.1610188 -4.1524162 -4.141994 -4.1321068 -4.1117978][-4.3152294 -4.2951579 -4.2670836 -4.239028 -4.2177148 -4.1960406 -4.1694503 -4.1498895 -4.1535835 -4.1671543 -4.1703134 -4.1677694 -4.1655526 -4.1578922 -4.1316872][-4.3149805 -4.29209 -4.257278 -4.2206769 -4.1908517 -4.1595435 -4.118103 -4.0883546 -4.1029062 -4.1387711 -4.1609774 -4.1749659 -4.1863813 -4.1863422 -4.1668525][-4.31295 -4.2850418 -4.2398205 -4.1897163 -4.1469531 -4.09712 -4.0324259 -3.99032 -4.0169778 -4.082849 -4.1325388 -4.1678915 -4.1906805 -4.1960368 -4.1802526][-4.3091464 -4.272646 -4.2109604 -4.1440072 -4.0875311 -4.0219574 -3.9358261 -3.8867943 -3.9323831 -4.0262623 -4.0964847 -4.1428704 -4.1682277 -4.1725044 -4.1568642][-4.3048515 -4.2612615 -4.190321 -4.1125579 -4.0465946 -3.9731917 -3.88463 -3.844111 -3.9078827 -4.012239 -4.0862617 -4.1276116 -4.1444855 -4.1413641 -4.1247439][-4.302536 -4.2599716 -4.1904216 -4.1094084 -4.0419722 -3.9752879 -3.9053941 -3.8848891 -3.9476795 -4.0364861 -4.0977736 -4.1309156 -4.1418548 -4.1351409 -4.1223111][-4.3073788 -4.2720962 -4.2116165 -4.1359978 -4.0700259 -4.0077949 -3.9493966 -3.9268732 -3.9674244 -4.0331721 -4.0876427 -4.1272759 -4.1512437 -4.1578388 -4.1569533][-4.3165517 -4.2916384 -4.2430987 -4.1783667 -4.1155019 -4.0543475 -3.9954925 -3.9554358 -3.9619532 -4.0001597 -4.0508657 -4.1067171 -4.1551223 -4.1836128 -4.195272][-4.3256655 -4.3098497 -4.2744656 -4.226059 -4.1758723 -4.12568 -4.0721931 -4.0227604 -4.002862 -4.0152469 -4.0540876 -4.1084 -4.1623282 -4.200531 -4.2211757][-4.3316045 -4.319829 -4.2922091 -4.25581 -4.2197 -4.18558 -4.148962 -4.1120105 -4.0903487 -4.0873508 -4.1075654 -4.1443338 -4.1864996 -4.2223806 -4.2484465][-4.3342109 -4.32163 -4.2931032 -4.25812 -4.226069 -4.2001109 -4.1762714 -4.1554828 -4.1404128 -4.1328745 -4.1439118 -4.1687975 -4.2032928 -4.2400637 -4.2693806][-4.3337545 -4.3181672 -4.2854953 -4.246769 -4.2119904 -4.1828818 -4.1582522 -4.1454988 -4.1382585 -4.1344614 -4.148777 -4.1743784 -4.2067766 -4.2445855 -4.2738252]]...]
INFO - root - 2017-12-05 21:33:29.728517: step 45910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 68h:33m:55s remains)
INFO - root - 2017-12-05 21:33:38.164480: step 45920, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 66h:22m:26s remains)
INFO - root - 2017-12-05 21:33:46.728428: step 45930, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 66h:47m:01s remains)
INFO - root - 2017-12-05 21:33:55.367245: step 45940, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.838 sec/batch; 66h:40m:20s remains)
INFO - root - 2017-12-05 21:34:03.832532: step 45950, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.857 sec/batch; 68h:12m:07s remains)
INFO - root - 2017-12-05 21:34:12.396987: step 45960, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 67h:38m:05s remains)
INFO - root - 2017-12-05 21:34:20.975672: step 45970, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 71h:11m:02s remains)
INFO - root - 2017-12-05 21:34:29.315690: step 45980, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:04m:21s remains)
INFO - root - 2017-12-05 21:34:37.924169: step 45990, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 68h:55m:21s remains)
INFO - root - 2017-12-05 21:34:46.538877: step 46000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 68h:51m:45s remains)
2017-12-05 21:34:47.303036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0456114 -4.0686436 -4.0788784 -4.0725803 -4.0843654 -4.1122289 -4.1340828 -4.1548996 -4.1547685 -4.1249404 -4.0966191 -4.0864911 -4.0759411 -4.0489054 -4.0292296][-4.0748353 -4.0962148 -4.103415 -4.08793 -4.08262 -4.0969787 -4.114254 -4.1382303 -4.1560678 -4.1490521 -4.1358209 -4.1281872 -4.1191039 -4.1036816 -4.099544][-4.073174 -4.0929933 -4.0954595 -4.0789661 -4.0726471 -4.0800881 -4.0939522 -4.1243768 -4.1590176 -4.1770105 -4.1858974 -4.1859918 -4.1829934 -4.1836829 -4.198463][-4.0651875 -4.0830364 -4.080266 -4.0659733 -4.0630641 -4.0673666 -4.0799708 -4.1167603 -4.1639719 -4.2023954 -4.2314072 -4.2459865 -4.2547011 -4.2655396 -4.2866778][-4.0737028 -4.0843673 -4.0731206 -4.056356 -4.0518789 -4.0515475 -4.0615149 -4.1022549 -4.1567087 -4.2082391 -4.2539287 -4.2846942 -4.3053241 -4.3187428 -4.33481][-4.0984755 -4.097302 -4.0794425 -4.0594029 -4.04405 -4.0321226 -4.0365334 -4.0784464 -4.1323681 -4.1895266 -4.2473378 -4.2935114 -4.3243842 -4.33813 -4.3467321][-4.1381841 -4.1219921 -4.0953612 -4.0669045 -4.0350704 -4.0016475 -3.9952588 -4.04178 -4.1022334 -4.1642714 -4.2311678 -4.2847214 -4.321733 -4.3357296 -4.3360496][-4.1761594 -4.1453004 -4.1092043 -4.0694103 -4.0218172 -3.9696484 -3.950845 -4.0002623 -4.0677052 -4.1374803 -4.2116265 -4.2674747 -4.3064036 -4.3213892 -4.31631][-4.2057152 -4.1620874 -4.1135206 -4.0632157 -4.0113158 -3.9576664 -3.936444 -3.9796634 -4.0454936 -4.1208358 -4.1997666 -4.2556534 -4.2955112 -4.3090382 -4.2996893][-4.22689 -4.1737523 -4.1141667 -4.057652 -4.0105162 -3.9670355 -3.9519746 -3.9874277 -4.0433469 -4.1186161 -4.196568 -4.251451 -4.2905192 -4.2996264 -4.2869139][-4.249537 -4.1948571 -4.1337361 -4.0778246 -4.0306873 -3.9879537 -3.9733138 -4.00128 -4.0526156 -4.1280885 -4.2045641 -4.2576966 -4.2929893 -4.2959914 -4.2789059][-4.2760191 -4.2302976 -4.1795268 -4.1307807 -4.0825295 -4.0364881 -4.0192623 -4.0407047 -4.0891948 -4.1608076 -4.2307539 -4.2766013 -4.3043838 -4.2990384 -4.2757726][-4.2984333 -4.2682233 -4.2351193 -4.1996741 -4.161232 -4.1222596 -4.1067791 -4.1209292 -4.1575403 -4.2127509 -4.2651091 -4.2980552 -4.3139338 -4.3008275 -4.2717662][-4.3130889 -4.297895 -4.2802129 -4.2580876 -4.2337165 -4.209 -4.1993279 -4.20739 -4.2291975 -4.2629657 -4.2921791 -4.307272 -4.3089409 -4.2890496 -4.255815][-4.3168597 -4.307981 -4.2960091 -4.280755 -4.2661834 -4.2533073 -4.249722 -4.2565155 -4.2689252 -4.2860727 -4.2965956 -4.2947607 -4.2830777 -4.2591295 -4.2287922]]...]
INFO - root - 2017-12-05 21:34:55.823177: step 46010, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:46m:21s remains)
INFO - root - 2017-12-05 21:35:04.288894: step 46020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 68h:07m:57s remains)
INFO - root - 2017-12-05 21:35:12.854542: step 46030, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 69h:30m:17s remains)
INFO - root - 2017-12-05 21:35:21.460381: step 46040, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 65h:24m:15s remains)
INFO - root - 2017-12-05 21:35:29.951874: step 46050, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 66h:33m:52s remains)
INFO - root - 2017-12-05 21:35:38.441458: step 46060, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.876 sec/batch; 69h:42m:30s remains)
INFO - root - 2017-12-05 21:35:46.959273: step 46070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 68h:05m:45s remains)
INFO - root - 2017-12-05 21:35:55.600296: step 46080, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 68h:43m:32s remains)
INFO - root - 2017-12-05 21:36:04.075176: step 46090, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 65h:54m:14s remains)
INFO - root - 2017-12-05 21:36:12.597135: step 46100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 67h:43m:04s remains)
2017-12-05 21:36:13.414133: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2599955 -4.2500634 -4.2383108 -4.2194972 -4.2068729 -4.1970081 -4.1962185 -4.2106347 -4.2379441 -4.2622957 -4.279695 -4.2855897 -4.2797236 -4.2734718 -4.2678275][-4.2581792 -4.2482152 -4.23277 -4.20663 -4.1874928 -4.1752172 -4.1696115 -4.1833138 -4.2199063 -4.2577825 -4.2853031 -4.2968554 -4.2928786 -4.2873831 -4.2830229][-4.2488418 -4.2350521 -4.2166243 -4.1844459 -4.1562395 -4.1372948 -4.126853 -4.1386528 -4.1856093 -4.2352519 -4.2717085 -4.2886906 -4.289495 -4.2846913 -4.28119][-4.2445316 -4.229269 -4.2075558 -4.1744819 -4.1415524 -4.1114492 -4.0876532 -4.0897441 -4.139617 -4.2000976 -4.2463183 -4.2668271 -4.2710686 -4.26695 -4.2660208][-4.2486687 -4.2340326 -4.2101026 -4.1766734 -4.1408806 -4.0989437 -4.0496531 -4.0302782 -4.0773716 -4.1476555 -4.2012138 -4.2271585 -4.2390709 -4.2451 -4.2513013][-4.2520804 -4.2375913 -4.2124696 -4.1803207 -4.146945 -4.0936832 -4.0072246 -3.9527652 -4.0012488 -4.0832963 -4.1373115 -4.1671796 -4.1899662 -4.2153816 -4.236876][-4.247344 -4.2277265 -4.1967363 -4.1597872 -4.1210051 -4.0458479 -3.9073529 -3.8087773 -3.8822291 -3.997719 -4.0607257 -4.0939751 -4.1276846 -4.1759334 -4.2195797][-4.2367277 -4.2064943 -4.1642594 -4.1163788 -4.0652585 -3.9711123 -3.8006735 -3.6846738 -3.793792 -3.9403563 -4.017046 -4.0663767 -4.1155977 -4.1769824 -4.2274947][-4.2218175 -4.1816754 -4.1258836 -4.0750284 -4.0240927 -3.9391274 -3.8024116 -3.7332659 -3.8442996 -3.9706366 -4.0441089 -4.1012468 -4.1544433 -4.2110109 -4.2513542][-4.2088246 -4.1669765 -4.1133752 -4.0723667 -4.0336275 -3.9780173 -3.9093382 -3.9011488 -3.9923289 -4.0755024 -4.129118 -4.1721263 -4.2115445 -4.2524452 -4.2782249][-4.1978297 -4.1625028 -4.1270118 -4.1085052 -4.0904784 -4.060133 -4.03644 -4.0611806 -4.1344476 -4.1840229 -4.2136073 -4.2378187 -4.258863 -4.2784891 -4.2901263][-4.1793737 -4.1492829 -4.1286211 -4.1253076 -4.1202545 -4.1033459 -4.097898 -4.13237 -4.1922312 -4.2199497 -4.232336 -4.2430568 -4.2505875 -4.2562957 -4.2583408][-4.1617332 -4.1322036 -4.118577 -4.115592 -4.1095743 -4.0986967 -4.1033316 -4.13979 -4.1868892 -4.2018991 -4.2016253 -4.199923 -4.198298 -4.1974349 -4.1955872][-4.1617055 -4.1319966 -4.1174135 -4.1078858 -4.0986843 -4.0922146 -4.098865 -4.1282706 -4.1605716 -4.166718 -4.1617889 -4.1563821 -4.1526136 -4.1512938 -4.1509442][-4.1780167 -4.1544352 -4.1452012 -4.1372328 -4.1271744 -4.1202455 -4.1271057 -4.1503363 -4.1728358 -4.1764145 -4.1726527 -4.1698651 -4.1691532 -4.1693392 -4.1696472]]...]
INFO - root - 2017-12-05 21:36:21.995191: step 46110, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 66h:53m:32s remains)
INFO - root - 2017-12-05 21:36:30.751024: step 46120, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:31m:22s remains)
INFO - root - 2017-12-05 21:36:39.320272: step 46130, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 70h:44m:58s remains)
INFO - root - 2017-12-05 21:36:47.805975: step 46140, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 67h:07m:31s remains)
INFO - root - 2017-12-05 21:36:56.364734: step 46150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 69h:19m:16s remains)
INFO - root - 2017-12-05 21:37:04.821988: step 46160, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.881 sec/batch; 70h:03m:06s remains)
INFO - root - 2017-12-05 21:37:13.527206: step 46170, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 69h:45m:36s remains)
INFO - root - 2017-12-05 21:37:22.067895: step 46180, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.875 sec/batch; 69h:35m:55s remains)
INFO - root - 2017-12-05 21:37:30.582792: step 46190, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 68h:16m:25s remains)
INFO - root - 2017-12-05 21:37:39.077070: step 46200, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:44m:12s remains)
2017-12-05 21:37:39.876237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2183461 -4.2184691 -4.2144518 -4.2107697 -4.1986547 -4.188961 -4.2027688 -4.2191758 -4.2213969 -4.2096353 -4.1883578 -4.1646457 -4.1643295 -4.1738725 -4.1864357][-4.2108674 -4.2033405 -4.1976719 -4.1986761 -4.1865087 -4.1702919 -4.1774635 -4.1881247 -4.1881657 -4.1746058 -4.1522303 -4.1295986 -4.1386285 -4.1530614 -4.1686926][-4.2051392 -4.1955705 -4.1890593 -4.1916642 -4.1774869 -4.158762 -4.1600437 -4.1633382 -4.1600962 -4.1498766 -4.1273823 -4.1036472 -4.1162748 -4.1354904 -4.1522622][-4.2111106 -4.2021651 -4.1945639 -4.19315 -4.172514 -4.1508284 -4.1422119 -4.1375523 -4.1310759 -4.1264367 -4.1085043 -4.0915222 -4.1073 -4.1301417 -4.1442919][-4.225596 -4.2172527 -4.2075319 -4.1959939 -4.1624393 -4.1290817 -4.1009312 -4.0815306 -4.07845 -4.0945339 -4.092483 -4.0840693 -4.104712 -4.1317263 -4.1498933][-4.24965 -4.2387385 -4.2171 -4.1848626 -4.1285114 -4.061655 -3.9986491 -3.9667861 -3.9850326 -4.0453715 -4.0773087 -4.0885324 -4.1185021 -4.1528516 -4.1751785][-4.275764 -4.2580614 -4.2219329 -4.163228 -4.0722175 -3.9531708 -3.8494635 -3.8134837 -3.8678267 -3.9880648 -4.0667686 -4.1079683 -4.1471896 -4.1835017 -4.20746][-4.297771 -4.2700629 -4.2217832 -4.1413989 -4.02971 -3.8819852 -3.761636 -3.7286882 -3.8075304 -3.9624245 -4.068881 -4.1334963 -4.18231 -4.2186942 -4.2417126][-4.30952 -4.2778411 -4.227067 -4.14835 -4.0486779 -3.9238436 -3.8326726 -3.8156056 -3.8866031 -4.0246263 -4.1230497 -4.1874285 -4.2297435 -4.2559714 -4.2725668][-4.3001008 -4.2733879 -4.2338934 -4.1728988 -4.0982232 -4.0139232 -3.9658983 -3.9707608 -4.031136 -4.1365237 -4.21317 -4.2601538 -4.2805185 -4.2900214 -4.2972012][-4.2829504 -4.2619028 -4.2370558 -4.1970363 -4.1518579 -4.1030393 -4.0840874 -4.1011467 -4.1558309 -4.2340736 -4.2889886 -4.3135643 -4.3113756 -4.3064222 -4.3056617][-4.2718391 -4.2575378 -4.247262 -4.2272453 -4.2038469 -4.1776695 -4.1699691 -4.1883078 -4.2309213 -4.2837744 -4.3183775 -4.3248658 -4.3079295 -4.2920609 -4.2878575][-4.2662134 -4.2613778 -4.264513 -4.2649131 -4.257668 -4.2401514 -4.2295661 -4.236537 -4.2628536 -4.2959657 -4.3174992 -4.3132296 -4.2885323 -4.2659836 -4.2598252][-4.2630272 -4.2665949 -4.2818637 -4.2979512 -4.3005567 -4.2871666 -4.2705727 -4.2622051 -4.2726216 -4.2929425 -4.3077283 -4.2952137 -4.2642055 -4.237309 -4.2311754][-4.2547569 -4.2629924 -4.2831178 -4.3065138 -4.3167906 -4.3117056 -4.2946706 -4.2789812 -4.2802844 -4.2929087 -4.2997494 -4.281754 -4.2488689 -4.2172394 -4.2090173]]...]
INFO - root - 2017-12-05 21:37:48.401158: step 46210, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 68h:35m:54s remains)
INFO - root - 2017-12-05 21:37:56.883221: step 46220, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 68h:37m:46s remains)
INFO - root - 2017-12-05 21:38:05.362965: step 46230, loss = 2.10, batch loss = 2.04 (9.1 examples/sec; 0.880 sec/batch; 69h:57m:50s remains)
INFO - root - 2017-12-05 21:38:13.836319: step 46240, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.852 sec/batch; 67h:44m:31s remains)
INFO - root - 2017-12-05 21:38:22.299986: step 46250, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.858 sec/batch; 68h:13m:06s remains)
INFO - root - 2017-12-05 21:38:30.911784: step 46260, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 69h:56m:30s remains)
INFO - root - 2017-12-05 21:38:39.587297: step 46270, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 68h:57m:59s remains)
INFO - root - 2017-12-05 21:38:48.198214: step 46280, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.870 sec/batch; 69h:08m:45s remains)
INFO - root - 2017-12-05 21:38:56.709760: step 46290, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 66h:55m:21s remains)
INFO - root - 2017-12-05 21:39:05.184245: step 46300, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.875 sec/batch; 69h:32m:09s remains)
2017-12-05 21:39:06.000954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1381173 -4.1426129 -4.1306214 -4.1087193 -4.0946345 -4.0811987 -4.0692244 -4.0684361 -4.0864697 -4.1055069 -4.1177182 -4.1326675 -4.1456394 -4.1521139 -4.1545739][-4.1610389 -4.1590176 -4.1440105 -4.1279988 -4.1236658 -4.1184864 -4.1158485 -4.1186323 -4.1326146 -4.1395326 -4.1396232 -4.1409469 -4.1397824 -4.135582 -4.1351919][-4.1657472 -4.1628675 -4.1561575 -4.1516781 -4.1567526 -4.156961 -4.1532745 -4.1477671 -4.1481395 -4.1459746 -4.1364574 -4.1209908 -4.1044264 -4.0939784 -4.0932689][-4.1550961 -4.1583271 -4.1639833 -4.1686244 -4.168426 -4.1602225 -4.1455245 -4.1287146 -4.1206303 -4.1149387 -4.0974569 -4.0717225 -4.0461268 -4.0379624 -4.0425148][-4.1262665 -4.1405263 -4.1535373 -4.1610947 -4.1533294 -4.1273017 -4.0915556 -4.0518847 -4.0315318 -4.0250907 -4.0176353 -4.0024981 -3.9899874 -3.9977236 -4.0128136][-4.1041188 -4.1288152 -4.1425533 -4.1368618 -4.1088176 -4.0624142 -3.9994078 -3.9236114 -3.8885591 -3.903054 -3.9325795 -3.9484479 -3.9629972 -3.9876707 -4.0167594][-4.1112561 -4.13355 -4.1368246 -4.1089277 -4.0544558 -3.9850354 -3.8843465 -3.7590306 -3.7207778 -3.7874784 -3.8693252 -3.9279137 -3.9731674 -4.0154057 -4.0583267][-4.1203709 -4.1271882 -4.1195068 -4.0806866 -4.0155368 -3.9331074 -3.8176372 -3.6797984 -3.6642816 -3.7766581 -3.885103 -3.9622369 -4.0213618 -4.0671387 -4.1092825][-4.0995197 -4.090939 -4.0776477 -4.04388 -3.9989009 -3.9464545 -3.8761663 -3.8011327 -3.8000479 -3.874877 -3.9544394 -4.0165548 -4.0663834 -4.1056461 -4.1409721][-4.051012 -4.0408559 -4.036561 -4.03061 -4.0223627 -4.0122814 -3.994746 -3.9688983 -3.9588337 -3.9813476 -4.0173197 -4.0529575 -4.0852547 -4.1168737 -4.154551][-4.0120096 -4.0198584 -4.0311832 -4.0478759 -4.0634804 -4.0724421 -4.0783148 -4.068409 -4.0458937 -4.0372066 -4.045094 -4.0605397 -4.0835624 -4.1175671 -4.1622181][-4.0010171 -4.0188179 -4.0301943 -4.0439863 -4.0630355 -4.0815053 -4.0939193 -4.088336 -4.0681477 -4.0524831 -4.0521283 -4.0607958 -4.0852866 -4.1245775 -4.1707368][-4.0049925 -4.0147953 -4.0189795 -4.0278506 -4.0400782 -4.0550871 -4.0669675 -4.0697689 -4.0659294 -4.0624261 -4.0654449 -4.0763512 -4.1029253 -4.1359038 -4.1694713][-4.0239992 -4.0245547 -4.0223889 -4.0268278 -4.0329003 -4.0430775 -4.0514879 -4.0580549 -4.0706763 -4.0853815 -4.0991678 -4.1141267 -4.1311722 -4.1441331 -4.1544576][-4.046679 -4.0427132 -4.0383139 -4.040256 -4.0450683 -4.0580177 -4.0689077 -4.0787182 -4.0943208 -4.1100659 -4.12861 -4.14462 -4.1461997 -4.1385546 -4.1293182]]...]
INFO - root - 2017-12-05 21:39:14.558811: step 46310, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 69h:00m:45s remains)
INFO - root - 2017-12-05 21:39:23.101436: step 46320, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.819 sec/batch; 65h:05m:03s remains)
INFO - root - 2017-12-05 21:39:31.689086: step 46330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 68h:25m:58s remains)
INFO - root - 2017-12-05 21:39:40.254803: step 46340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 67h:41m:47s remains)
INFO - root - 2017-12-05 21:39:48.693172: step 46350, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 67h:55m:53s remains)
INFO - root - 2017-12-05 21:39:57.260617: step 46360, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:49m:43s remains)
INFO - root - 2017-12-05 21:40:05.702260: step 46370, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 67h:56m:53s remains)
INFO - root - 2017-12-05 21:40:14.179979: step 46380, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 66h:34m:57s remains)
INFO - root - 2017-12-05 21:40:22.711513: step 46390, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.873 sec/batch; 69h:25m:05s remains)
INFO - root - 2017-12-05 21:40:31.188311: step 46400, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 67h:44m:15s remains)
2017-12-05 21:40:32.000263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2649951 -4.2698865 -4.2760816 -4.2748504 -4.2720757 -4.268106 -4.2604346 -4.249927 -4.2492409 -4.267169 -4.2849169 -4.2901664 -4.2805047 -4.2606091 -4.2461829][-4.242754 -4.252357 -4.2647181 -4.2650127 -4.2617121 -4.25085 -4.2315865 -4.20705 -4.2004533 -4.2257452 -4.2505155 -4.2588658 -4.2465167 -4.2219863 -4.206358][-4.2209983 -4.2342854 -4.2517567 -4.2529364 -4.2433081 -4.2205305 -4.1877584 -4.1449971 -4.1378555 -4.1762719 -4.2074862 -4.215559 -4.2029624 -4.1776848 -4.1594725][-4.197896 -4.2120275 -4.2363887 -4.236578 -4.2199435 -4.1885304 -4.1354127 -4.075448 -4.0761991 -4.1355476 -4.1745448 -4.1800675 -4.1630378 -4.133956 -4.1155252][-4.1745334 -4.1871562 -4.2148123 -4.2122707 -4.1889706 -4.1434221 -4.0679531 -3.9950705 -4.0069785 -4.0934429 -4.1452689 -4.1467514 -4.120975 -4.0895491 -4.0734787][-4.16101 -4.1712551 -4.1973395 -4.1926069 -4.1617 -4.0973144 -3.9979823 -3.9118793 -3.9409473 -4.0487905 -4.1059608 -4.10152 -4.0718307 -4.047152 -4.0422115][-4.157486 -4.1639385 -4.1874304 -4.1863723 -4.1492562 -4.06417 -3.948827 -3.8680158 -3.9209566 -4.0318513 -4.0822959 -4.0706 -4.0463672 -4.0413222 -4.0584822][-4.1620979 -4.1636748 -4.1907997 -4.2011962 -4.1681628 -4.0814157 -3.978704 -3.9180682 -3.9658389 -4.0551343 -4.0948448 -4.08412 -4.0731258 -4.0867777 -4.1137347][-4.1781907 -4.1775327 -4.2102966 -4.2300959 -4.2095385 -4.1419325 -4.0627542 -4.0176692 -4.0551119 -4.1213422 -4.1538706 -4.1496935 -4.1477547 -4.1586804 -4.1707592][-4.1992497 -4.1947165 -4.2223415 -4.2404642 -4.2312064 -4.1848307 -4.1250319 -4.0934172 -4.1264834 -4.1835752 -4.2135296 -4.212141 -4.2095537 -4.2102823 -4.2019806][-4.2170382 -4.2040243 -4.2196326 -4.2346811 -4.2333097 -4.2034531 -4.1613059 -4.1402249 -4.1715927 -4.2227664 -4.247613 -4.2464833 -4.2420959 -4.2361331 -4.2198653][-4.2278323 -4.20609 -4.2109418 -4.2229252 -4.2257724 -4.207171 -4.1826839 -4.174191 -4.2016873 -4.2410145 -4.2560573 -4.2541666 -4.2496839 -4.2433381 -4.2289157][-4.2383232 -4.2134767 -4.2117653 -4.2207346 -4.2242193 -4.213048 -4.2045321 -4.20718 -4.2272863 -4.2521114 -4.2594466 -4.2569056 -4.2515707 -4.2455587 -4.2346649][-4.2609377 -4.2380247 -4.2340741 -4.24088 -4.2425089 -4.2380605 -4.2393794 -4.2477059 -4.261683 -4.2742133 -4.275948 -4.2725968 -4.2675261 -4.2633691 -4.2570405][-4.2891893 -4.2738848 -4.2714095 -4.2759442 -4.2776766 -4.2775974 -4.281004 -4.2890115 -4.2972889 -4.3017869 -4.3009219 -4.2967148 -4.2927256 -4.2914495 -4.2888937]]...]
INFO - root - 2017-12-05 21:40:40.618977: step 46410, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 68h:47m:21s remains)
INFO - root - 2017-12-05 21:40:49.055174: step 46420, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 68h:39m:35s remains)
INFO - root - 2017-12-05 21:40:57.650156: step 46430, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.828 sec/batch; 65h:47m:39s remains)
INFO - root - 2017-12-05 21:41:06.211446: step 46440, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 66h:51m:08s remains)
INFO - root - 2017-12-05 21:41:14.700317: step 46450, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 69h:45m:00s remains)
INFO - root - 2017-12-05 21:41:23.122952: step 46460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 67h:23m:53s remains)
INFO - root - 2017-12-05 21:41:31.718429: step 46470, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.882 sec/batch; 70h:02m:33s remains)
INFO - root - 2017-12-05 21:41:40.204759: step 46480, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 67h:13m:57s remains)
INFO - root - 2017-12-05 21:41:48.648998: step 46490, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 66h:19m:18s remains)
INFO - root - 2017-12-05 21:41:57.101169: step 46500, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 66h:51m:41s remains)
2017-12-05 21:41:57.968175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.316793 -4.3072634 -4.3025494 -4.2979965 -4.2942247 -4.2907844 -4.2852058 -4.281189 -4.2806458 -4.2855673 -4.2907925 -4.2956109 -4.2996974 -4.302546 -4.3069673][-4.3054042 -4.2910776 -4.2846484 -4.2804294 -4.2745624 -4.2671 -4.2558117 -4.24758 -4.248971 -4.259264 -4.2666678 -4.2710934 -4.2749877 -4.2767739 -4.281929][-4.2931252 -4.2715063 -4.2595782 -4.2500978 -4.2362518 -4.2207713 -4.2011461 -4.1903925 -4.1974359 -4.219213 -4.2343597 -4.2415318 -4.2468877 -4.24812 -4.2550869][-4.2834516 -4.2555709 -4.2365456 -4.2161894 -4.1927958 -4.1662154 -4.1349559 -4.1185513 -4.1306934 -4.1710076 -4.2043552 -4.2215896 -4.2313581 -4.2352419 -4.2426844][-4.2776675 -4.2449584 -4.2168212 -4.1852717 -4.1516275 -4.1118064 -4.0642228 -4.0346932 -4.0507855 -4.1152043 -4.1751118 -4.2096386 -4.2304592 -4.2407174 -4.2472177][-4.2727661 -4.230638 -4.1886277 -4.1431594 -4.0941248 -4.0342422 -3.9655883 -3.9222164 -3.9486623 -4.0447826 -4.1367383 -4.1938038 -4.23124 -4.2499437 -4.2575512][-4.2662125 -4.2127323 -4.1567707 -4.0973291 -4.0300078 -3.9507632 -3.8660467 -3.8156214 -3.8569958 -3.9836297 -4.10389 -4.1792474 -4.2289305 -4.2548151 -4.2654881][-4.2672195 -4.2071962 -4.1434774 -4.0759225 -4.0007486 -3.9164271 -3.834703 -3.7881129 -3.8322325 -3.9651034 -4.0906172 -4.1678715 -4.2196336 -4.2506828 -4.2657104][-4.2790232 -4.222661 -4.1622486 -4.1002483 -4.0336289 -3.959893 -3.891011 -3.8559153 -3.8931022 -4.0036077 -4.1092262 -4.173336 -4.2169838 -4.2472763 -4.2621861][-4.2938709 -4.2465582 -4.1969705 -4.1486797 -4.0984783 -4.0428891 -3.9867377 -3.9618688 -3.994452 -4.076335 -4.1523337 -4.1976004 -4.2282619 -4.250463 -4.2605925][-4.3082333 -4.2719312 -4.2355819 -4.2032423 -4.171032 -4.1338544 -4.0948391 -4.0772448 -4.1039963 -4.1554608 -4.2011867 -4.22936 -4.246726 -4.258779 -4.2660222][-4.3198423 -4.2922311 -4.2664328 -4.2450538 -4.2288766 -4.2101407 -4.1874447 -4.1740465 -4.192935 -4.2252965 -4.2506409 -4.264905 -4.2711744 -4.2736459 -4.2772422][-4.33006 -4.3118362 -4.2948637 -4.2811093 -4.2749882 -4.2702818 -4.2594643 -4.2494574 -4.2589159 -4.2780104 -4.2904191 -4.2952981 -4.2961903 -4.2946391 -4.2952][-4.3385148 -4.328589 -4.3200665 -4.3127551 -4.3108807 -4.3118505 -4.3084908 -4.3047481 -4.3104119 -4.3207664 -4.3252449 -4.3236327 -4.31971 -4.3154979 -4.3131881][-4.3418851 -4.3382654 -4.3361411 -4.3335238 -4.3328104 -4.33433 -4.3345771 -4.3353677 -4.3395267 -4.34323 -4.3435049 -4.3406343 -4.3353195 -4.3291016 -4.3252735]]...]
INFO - root - 2017-12-05 21:42:06.485272: step 46510, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 67h:14m:37s remains)
INFO - root - 2017-12-05 21:42:15.090825: step 46520, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 69h:26m:04s remains)
INFO - root - 2017-12-05 21:42:23.400863: step 46530, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 65h:37m:48s remains)
INFO - root - 2017-12-05 21:42:31.999862: step 46540, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 66h:52m:08s remains)
INFO - root - 2017-12-05 21:42:40.420427: step 46550, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 68h:19m:57s remains)
INFO - root - 2017-12-05 21:42:48.899643: step 46560, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 68h:10m:58s remains)
INFO - root - 2017-12-05 21:42:57.499923: step 46570, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 65h:55m:30s remains)
INFO - root - 2017-12-05 21:43:05.999843: step 46580, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 67h:56m:46s remains)
INFO - root - 2017-12-05 21:43:14.605994: step 46590, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 66h:06m:38s remains)
INFO - root - 2017-12-05 21:43:23.124657: step 46600, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 66h:00m:25s remains)
2017-12-05 21:43:23.966202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0972118 -4.1243429 -4.1367064 -4.1460228 -4.145381 -4.1310377 -4.117836 -4.1069012 -4.0939746 -4.1036468 -4.1313114 -4.1621485 -4.1883664 -4.2109189 -4.2457905][-4.0952835 -4.118248 -4.1316714 -4.1425757 -4.1439009 -4.13177 -4.1191282 -4.1076574 -4.0974836 -4.1071844 -4.1287861 -4.1637573 -4.1987214 -4.2240152 -4.2552571][-4.0954819 -4.1162992 -4.1276178 -4.1383233 -4.1407862 -4.1256971 -4.108058 -4.0923972 -4.0811729 -4.0921955 -4.1148634 -4.1554713 -4.1966014 -4.2298803 -4.2643151][-4.099267 -4.1132479 -4.1188154 -4.127389 -4.1330667 -4.1144528 -4.08313 -4.0531979 -4.0495629 -4.0760045 -4.1072893 -4.147274 -4.1867051 -4.2226672 -4.2650566][-4.1098442 -4.1121469 -4.106204 -4.1117125 -4.1210394 -4.0964561 -4.0421195 -3.9908919 -3.9974408 -4.0486021 -4.0938206 -4.1328688 -4.1682887 -4.2040744 -4.253274][-4.1157775 -4.1028366 -4.0806684 -4.0776482 -4.0860868 -4.0532951 -3.9660325 -3.8737326 -3.8906639 -3.979835 -4.0507154 -4.0983706 -4.1386089 -4.1771641 -4.2303605][-4.0980792 -4.0747352 -4.040257 -4.0316591 -4.0432391 -4.0009241 -3.8799181 -3.7365317 -3.7561755 -3.889524 -3.990696 -4.05508 -4.1065621 -4.1501923 -4.2044268][-4.0903726 -4.0653162 -4.03142 -4.02629 -4.0452847 -4.0205431 -3.9263682 -3.8043766 -3.8028536 -3.9067771 -3.9898896 -4.0548553 -4.1090126 -4.1477365 -4.19431][-4.1217852 -4.104804 -4.0816402 -4.0775967 -4.0962319 -4.0904989 -4.04363 -3.9661984 -3.9459095 -3.9926567 -4.0333109 -4.0794621 -4.1268654 -4.1627932 -4.2039976][-4.1519837 -4.1452084 -4.1325665 -4.1274366 -4.1385446 -4.1348844 -4.1117969 -4.0644059 -4.0335894 -4.0452986 -4.0627308 -4.1007247 -4.1444011 -4.1814418 -4.2245817][-4.1807671 -4.1814232 -4.1751642 -4.16938 -4.1720238 -4.1634941 -4.1447253 -4.1111603 -4.0812349 -4.0773396 -4.08529 -4.1203794 -4.1644654 -4.2038345 -4.2454338][-4.2110281 -4.2156496 -4.2093706 -4.2021551 -4.1995587 -4.1889381 -4.1702709 -4.1446323 -4.119616 -4.1143675 -4.124114 -4.1538582 -4.1942239 -4.2298555 -4.2652965][-4.2379689 -4.2422361 -4.2376528 -4.2318611 -4.2269158 -4.2165494 -4.2023926 -4.1847529 -4.1671634 -4.1617408 -4.1701446 -4.1938133 -4.2252917 -4.2529812 -4.2813082][-4.2566395 -4.2592874 -4.25789 -4.25429 -4.2495613 -4.2418122 -4.2347145 -4.2259331 -4.2159443 -4.2116604 -4.2177806 -4.2353206 -4.2582874 -4.2765083 -4.2957692][-4.2676468 -4.2691231 -4.2678189 -4.265276 -4.2616749 -4.2579689 -4.2557621 -4.2542958 -4.2525272 -4.2527261 -4.257277 -4.2685604 -4.2825804 -4.2942424 -4.308022]]...]
INFO - root - 2017-12-05 21:43:32.500822: step 46610, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 67h:53m:20s remains)
INFO - root - 2017-12-05 21:43:40.989438: step 46620, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 66h:38m:28s remains)
INFO - root - 2017-12-05 21:43:49.459822: step 46630, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 66h:49m:27s remains)
INFO - root - 2017-12-05 21:43:57.973060: step 46640, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.862 sec/batch; 68h:28m:16s remains)
INFO - root - 2017-12-05 21:44:06.264088: step 46650, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 67h:26m:28s remains)
INFO - root - 2017-12-05 21:44:14.853836: step 46660, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 67h:58m:15s remains)
INFO - root - 2017-12-05 21:44:23.268483: step 46670, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 67h:18m:23s remains)
INFO - root - 2017-12-05 21:44:31.831778: step 46680, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 68h:23m:49s remains)
INFO - root - 2017-12-05 21:44:40.351587: step 46690, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 66h:19m:08s remains)
INFO - root - 2017-12-05 21:44:49.071668: step 46700, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 69h:26m:21s remains)
2017-12-05 21:44:49.828967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2127619 -4.1881557 -4.1537786 -4.1209316 -4.11035 -4.12105 -4.1331415 -4.1380606 -4.1426349 -4.1435285 -4.1481571 -4.1574712 -4.1692753 -4.1832609 -4.1843996][-4.2133207 -4.1975536 -4.1688681 -4.1344824 -4.1122932 -4.1100478 -4.1091256 -4.1078057 -4.1209159 -4.1381874 -4.1564922 -4.1777987 -4.1977105 -4.2120724 -4.2123623][-4.216291 -4.2094722 -4.1897087 -4.1580935 -4.1243334 -4.0971012 -4.0728045 -4.0672212 -4.0985231 -4.1411581 -4.1799297 -4.2112393 -4.2313285 -4.2443004 -4.2420068][-4.2194481 -4.2209716 -4.2125468 -4.1890693 -4.146626 -4.0901408 -4.030952 -4.012536 -4.0582247 -4.1269956 -4.1874361 -4.2287569 -4.2503071 -4.2623458 -4.2577691][-4.2182961 -4.2282987 -4.2319064 -4.2176294 -4.1702819 -4.09249 -4.0017977 -3.9600644 -4.0108361 -4.0984707 -4.1772532 -4.2299972 -4.257309 -4.2706318 -4.2637749][-4.2207708 -4.2382293 -4.2451396 -4.229054 -4.1770663 -4.0856915 -3.9772489 -3.919749 -3.9754837 -4.0838981 -4.1800046 -4.2409258 -4.2706981 -4.2802162 -4.270308][-4.2343688 -4.24632 -4.2447863 -4.2188325 -4.1637936 -4.0775814 -3.9806633 -3.9318261 -3.9943938 -4.1089334 -4.2049422 -4.2590942 -4.2800922 -4.2805281 -4.2677779][-4.2516394 -4.25488 -4.2404318 -4.2057767 -4.1499543 -4.0809817 -4.0194569 -4.0010548 -4.0639658 -4.1642 -4.2410655 -4.2775397 -4.2825413 -4.2690058 -4.2507944][-4.2571673 -4.2540827 -4.2295861 -4.1836023 -4.1193128 -4.0618 -4.0376024 -4.0539312 -4.1207256 -4.2086864 -4.2687058 -4.2875366 -4.2765193 -4.2458129 -4.2197752][-4.2526116 -4.2459874 -4.2168217 -4.164773 -4.09041 -4.0349817 -4.0332718 -4.0768495 -4.1534886 -4.2366829 -4.2833867 -4.2906713 -4.2649541 -4.2178979 -4.182158][-4.242435 -4.2416825 -4.2131491 -4.15996 -4.0831528 -4.0278382 -4.0337982 -4.0928025 -4.1745849 -4.2530313 -4.2919364 -4.2946987 -4.2617664 -4.2035613 -4.1582069][-4.2388439 -4.2427039 -4.2162285 -4.167901 -4.102901 -4.0576205 -4.0699458 -4.1325407 -4.2090378 -4.2747564 -4.3059196 -4.3054237 -4.2731991 -4.2163148 -4.1676674][-4.2321982 -4.2410264 -4.2215848 -4.1835103 -4.135808 -4.1059022 -4.122931 -4.1785727 -4.2390213 -4.2868896 -4.3094416 -4.3095207 -4.28652 -4.2453685 -4.2039156][-4.2074151 -4.2221522 -4.2098455 -4.1810355 -4.147862 -4.1304693 -4.1540618 -4.2040839 -4.2494731 -4.2813411 -4.2960124 -4.2944384 -4.2802749 -4.2581811 -4.2325797][-4.1721916 -4.1939497 -4.18385 -4.15136 -4.118432 -4.1096821 -4.1398215 -4.1911087 -4.2321348 -4.2584605 -4.265789 -4.2552786 -4.2423658 -4.2363791 -4.231617]]...]
INFO - root - 2017-12-05 21:44:58.443686: step 46710, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.902 sec/batch; 71h:35m:10s remains)
INFO - root - 2017-12-05 21:45:07.020436: step 46720, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 69h:48m:38s remains)
INFO - root - 2017-12-05 21:45:15.504380: step 46730, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 69h:29m:22s remains)
INFO - root - 2017-12-05 21:45:23.892471: step 46740, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 67h:35m:04s remains)
INFO - root - 2017-12-05 21:45:32.314432: step 46750, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 66h:06m:22s remains)
INFO - root - 2017-12-05 21:45:40.935262: step 46760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.856 sec/batch; 67h:54m:13s remains)
INFO - root - 2017-12-05 21:45:49.479002: step 46770, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:44m:34s remains)
INFO - root - 2017-12-05 21:45:57.983947: step 46780, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 69h:33m:39s remains)
INFO - root - 2017-12-05 21:46:06.546111: step 46790, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 68h:44m:49s remains)
INFO - root - 2017-12-05 21:46:15.135473: step 46800, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 67h:29m:27s remains)
2017-12-05 21:46:15.864114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.198204 -4.2216349 -4.2481446 -4.2705646 -4.2756839 -4.2686725 -4.2611446 -4.2608171 -4.2634215 -4.263257 -4.2631721 -4.2660589 -4.2688675 -4.2747121 -4.2768][-4.1965275 -4.2245669 -4.2539964 -4.2773705 -4.2796283 -4.27016 -4.2588949 -4.2517719 -4.2444863 -4.2369876 -4.235033 -4.2421923 -4.2549143 -4.26423 -4.2654281][-4.19671 -4.2173657 -4.240591 -4.2610211 -4.2635264 -4.2558303 -4.2427249 -4.234683 -4.2258129 -4.2143564 -4.2103057 -4.2183819 -4.2364526 -4.2524986 -4.2554426][-4.2028794 -4.2138691 -4.2236505 -4.2337537 -4.2328038 -4.2243662 -4.2080073 -4.2014108 -4.2007341 -4.1973138 -4.1996632 -4.2118793 -4.2311397 -4.2489862 -4.2521067][-4.2178607 -4.2204475 -4.2146626 -4.2100091 -4.2007542 -4.1865621 -4.1620655 -4.1495361 -4.1564617 -4.172266 -4.1940331 -4.2183881 -4.2385497 -4.2505832 -4.2496552][-4.2279978 -4.225203 -4.2129688 -4.2007031 -4.1840777 -4.1582532 -4.11796 -4.0965409 -4.1078491 -4.1405692 -4.1749983 -4.2101145 -4.23374 -4.2405276 -4.2339087][-4.2208014 -4.216125 -4.2089772 -4.2030869 -4.1911874 -4.1640944 -4.1134763 -4.0774684 -4.0765576 -4.1026735 -4.1313157 -4.1690893 -4.20248 -4.2175417 -4.2151194][-4.2129283 -4.2086854 -4.2108269 -4.2172484 -4.2185955 -4.2062421 -4.1666679 -4.1249542 -4.0978117 -4.0865235 -4.088933 -4.119894 -4.1640468 -4.196229 -4.2075396][-4.2134433 -4.2122965 -4.2201538 -4.231647 -4.242353 -4.2455149 -4.2289276 -4.2025833 -4.1709418 -4.1336188 -4.1072564 -4.1188374 -4.1547556 -4.1903067 -4.2104988][-4.2236705 -4.2252455 -4.2319026 -4.2393231 -4.2529845 -4.2631984 -4.2642441 -4.2622786 -4.2481008 -4.2146993 -4.1793642 -4.1724744 -4.1856918 -4.2015824 -4.2141886][-4.2240558 -4.22786 -4.2318511 -4.2350483 -4.2464638 -4.261425 -4.276299 -4.2906585 -4.2931314 -4.2756157 -4.2485409 -4.2297096 -4.2208567 -4.2169948 -4.2206435][-4.2184687 -4.2239065 -4.2255206 -4.224679 -4.2313328 -4.2483306 -4.2717671 -4.2960491 -4.3088427 -4.3028097 -4.2855148 -4.2660384 -4.2486873 -4.2349043 -4.2324553][-4.2126012 -4.2216611 -4.2242227 -4.2179613 -4.2168622 -4.2325268 -4.2632055 -4.29454 -4.3126674 -4.3157597 -4.3041592 -4.2870588 -4.267992 -4.2469954 -4.2298141][-4.2191482 -4.2313032 -4.2347012 -4.2263341 -4.2159247 -4.2202611 -4.2481003 -4.2841997 -4.3093238 -4.3187494 -4.3110323 -4.2941704 -4.2747245 -4.2453289 -4.2113161][-4.2345052 -4.2445722 -4.24659 -4.2372384 -4.219286 -4.2115011 -4.2280631 -4.2611828 -4.29146 -4.3089108 -4.3087106 -4.2985587 -4.283361 -4.2528467 -4.2095041]]...]
INFO - root - 2017-12-05 21:46:24.441333: step 46810, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 69h:02m:54s remains)
INFO - root - 2017-12-05 21:46:32.968959: step 46820, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 68h:33m:05s remains)
INFO - root - 2017-12-05 21:46:41.563204: step 46830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 68h:02m:15s remains)
INFO - root - 2017-12-05 21:46:50.125861: step 46840, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 70h:05m:38s remains)
INFO - root - 2017-12-05 21:46:58.737292: step 46850, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 66h:48m:01s remains)
INFO - root - 2017-12-05 21:47:07.132250: step 46860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 67h:03m:27s remains)
INFO - root - 2017-12-05 21:47:15.682271: step 46870, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 66h:30m:41s remains)
INFO - root - 2017-12-05 21:47:24.360617: step 46880, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 66h:54m:40s remains)
INFO - root - 2017-12-05 21:47:32.964812: step 46890, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 68h:55m:44s remains)
INFO - root - 2017-12-05 21:47:41.478887: step 46900, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:14m:56s remains)
2017-12-05 21:47:42.288144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1651497 -4.1541409 -4.1336417 -4.1329064 -4.15267 -4.1647329 -4.1596847 -4.1406946 -4.1207037 -4.1166391 -4.1194854 -4.1088185 -4.0981321 -4.112927 -4.1571507][-4.2041383 -4.1958451 -4.1740179 -4.1657257 -4.17647 -4.1816859 -4.1713529 -4.1533346 -4.1389856 -4.1394739 -4.1416154 -4.1246357 -4.1055236 -4.1136851 -4.1553707][-4.2281046 -4.2212467 -4.2014303 -4.1889753 -4.1957626 -4.1978583 -4.1857567 -4.1748977 -4.1687837 -4.176919 -4.1822753 -4.1633158 -4.1362667 -4.1327963 -4.1635261][-4.2237849 -4.2166233 -4.2008095 -4.1927395 -4.2035642 -4.2089682 -4.1974721 -4.1896887 -4.1889725 -4.2050056 -4.2160969 -4.1978564 -4.1637135 -4.1498275 -4.1658669][-4.1806464 -4.1717768 -4.1593213 -4.162816 -4.187129 -4.198688 -4.18057 -4.161355 -4.1616359 -4.189661 -4.2149243 -4.2054768 -4.1743445 -4.1597042 -4.1658792][-4.1126719 -4.102634 -4.0955205 -4.1164875 -4.1555848 -4.1713176 -4.1422749 -4.1041923 -4.0985174 -4.1385784 -4.1812506 -4.1860752 -4.168674 -4.1623425 -4.1627865][-4.0537176 -4.0401411 -4.0362058 -4.0674028 -4.1157355 -4.1314735 -4.0928574 -4.0379443 -4.0279479 -4.0781283 -4.1370058 -4.1555133 -4.1461744 -4.1401114 -4.1374359][-4.050808 -4.0280395 -4.0122533 -4.029357 -4.0662413 -4.0667377 -4.0058966 -3.9268332 -3.9068124 -3.9742565 -4.0557508 -4.0948858 -4.0983076 -4.0962505 -4.0950594][-4.1209583 -4.0992332 -4.0793667 -4.0809965 -4.0912342 -4.0605254 -3.9696124 -3.8629034 -3.830725 -3.9060023 -3.9958804 -4.0470309 -4.0629911 -4.0689487 -4.0743718][-4.2058811 -4.1927414 -4.1770396 -4.1728024 -4.16818 -4.1298513 -4.0487952 -3.9619117 -3.9325874 -3.9756069 -4.0337567 -4.0684671 -4.0798597 -4.0815239 -4.0916123][-4.2585897 -4.2541871 -4.2435226 -4.2372274 -4.2276435 -4.19216 -4.1295724 -4.067678 -4.0431137 -4.061173 -4.0923047 -4.1112309 -4.113719 -4.1108451 -4.1217656][-4.288065 -4.2904539 -4.2844524 -4.279798 -4.2679834 -4.2345128 -4.1837149 -4.1386876 -4.1206632 -4.1305695 -4.1498618 -4.1601467 -4.1562452 -4.1490979 -4.1540322][-4.3005648 -4.3074145 -4.3061666 -4.3065972 -4.2987108 -4.2719183 -4.2318568 -4.1994529 -4.189837 -4.1983232 -4.2117691 -4.21662 -4.2090731 -4.1988673 -4.1960268][-4.2858729 -4.2988505 -4.3011231 -4.3064823 -4.3056164 -4.2879424 -4.2576723 -4.2333355 -4.2276468 -4.2349453 -4.2439389 -4.2485456 -4.2443752 -4.2349424 -4.228363][-4.2451758 -4.26353 -4.2692294 -4.2798386 -4.2851963 -4.2782726 -4.2631135 -4.2479205 -4.2443633 -4.2496247 -4.2554383 -4.2594657 -4.2582536 -4.2524257 -4.2453809]]...]
INFO - root - 2017-12-05 21:47:50.752922: step 46910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 67h:15m:53s remains)
INFO - root - 2017-12-05 21:47:59.301665: step 46920, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 67h:26m:52s remains)
INFO - root - 2017-12-05 21:48:07.894776: step 46930, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.873 sec/batch; 69h:16m:06s remains)
INFO - root - 2017-12-05 21:48:16.468187: step 46940, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 67h:38m:53s remains)
INFO - root - 2017-12-05 21:48:24.895939: step 46950, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 67h:54m:09s remains)
INFO - root - 2017-12-05 21:48:33.480617: step 46960, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 66h:29m:25s remains)
INFO - root - 2017-12-05 21:48:41.905720: step 46970, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 68h:51m:58s remains)
INFO - root - 2017-12-05 21:48:50.473122: step 46980, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 69h:46m:48s remains)
INFO - root - 2017-12-05 21:48:58.938067: step 46990, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 67h:50m:37s remains)
INFO - root - 2017-12-05 21:49:07.444801: step 47000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 68h:38m:53s remains)
2017-12-05 21:49:08.179124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2950325 -4.2537026 -4.1990638 -4.1603727 -4.1566553 -4.1717663 -4.18738 -4.2009778 -4.1950464 -4.1709366 -4.1296673 -4.0962629 -4.10423 -4.147944 -4.2058549][-4.2804947 -4.2321033 -4.1670566 -4.1206236 -4.1157436 -4.1345038 -4.1523876 -4.1667337 -4.1631274 -4.1417494 -4.0999703 -4.0665922 -4.0850039 -4.1354413 -4.1917882][-4.2716236 -4.22077 -4.1513314 -4.1003509 -4.0935307 -4.1131229 -4.1239309 -4.1329594 -4.134778 -4.1219225 -4.08965 -4.0668907 -4.0948997 -4.1465349 -4.1936536][-4.2686229 -4.2187748 -4.150485 -4.1000233 -4.0919619 -4.1069465 -4.1049705 -4.101325 -4.1053696 -4.1011686 -4.0869789 -4.0864258 -4.1258054 -4.176784 -4.2133064][-4.2695894 -4.2231407 -4.160892 -4.1159115 -4.1054053 -4.108573 -4.0887747 -4.0682054 -4.0689373 -4.0750928 -4.0839109 -4.1143432 -4.1683865 -4.2183514 -4.246006][-4.2696571 -4.2300143 -4.1793094 -4.1432996 -4.12662 -4.1118159 -4.0724249 -4.026793 -4.0150156 -4.0369382 -4.0765195 -4.1392035 -4.204812 -4.2540393 -4.2768879][-4.2678213 -4.2342811 -4.1953268 -4.168745 -4.1473842 -4.1181827 -4.0541348 -3.9728374 -3.9365606 -3.9732897 -4.0517292 -4.1456647 -4.2219062 -4.2710881 -4.2927485][-4.2655368 -4.236691 -4.2059369 -4.1868486 -4.1654825 -4.126852 -4.0442638 -3.9274721 -3.8652463 -3.913394 -4.030663 -4.149909 -4.2317791 -4.27996 -4.3008509][-4.2663536 -4.242238 -4.2171044 -4.2025189 -4.1849394 -4.1459889 -4.0559635 -3.9228058 -3.8521028 -3.91361 -4.0489378 -4.1737585 -4.2526541 -4.2958708 -4.3118768][-4.2716537 -4.2508545 -4.22672 -4.2128825 -4.2009344 -4.1692867 -4.0904889 -3.9727707 -3.9158282 -3.9826307 -4.1097183 -4.2195649 -4.2846622 -4.3148375 -4.3194284][-4.2819157 -4.2624149 -4.2358503 -4.2186193 -4.211329 -4.1934962 -4.137548 -4.0520077 -4.01673 -4.0790834 -4.1843276 -4.2704659 -4.3136797 -4.3254356 -4.3162341][-4.296874 -4.2783809 -4.2474184 -4.2241592 -4.2178297 -4.2107816 -4.175653 -4.1191616 -4.1023984 -4.1562972 -4.2379088 -4.3004966 -4.3243065 -4.319869 -4.3007812][-4.3112831 -4.2943964 -4.259479 -4.2286534 -4.2179928 -4.21523 -4.1928897 -4.1547232 -4.1473513 -4.192183 -4.2563891 -4.302002 -4.3134341 -4.2988005 -4.27571][-4.321672 -4.3064251 -4.2701335 -4.2359848 -4.2218289 -4.2191997 -4.201426 -4.1692419 -4.1639075 -4.2027178 -4.2535024 -4.2836375 -4.2848015 -4.26593 -4.2472315][-4.3234243 -4.3066568 -4.2706933 -4.2376862 -4.2232881 -4.2205515 -4.2038536 -4.17416 -4.1684527 -4.20147 -4.2403431 -4.2572651 -4.2516885 -4.23528 -4.2288194]]...]
INFO - root - 2017-12-05 21:49:16.706316: step 47010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 68h:26m:40s remains)
INFO - root - 2017-12-05 21:49:25.234097: step 47020, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 66h:56m:05s remains)
INFO - root - 2017-12-05 21:49:33.736939: step 47030, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 68h:20m:33s remains)
INFO - root - 2017-12-05 21:49:42.337637: step 47040, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 67h:02m:40s remains)
INFO - root - 2017-12-05 21:49:50.857584: step 47050, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 69h:09m:27s remains)
INFO - root - 2017-12-05 21:49:59.290502: step 47060, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 68h:38m:10s remains)
INFO - root - 2017-12-05 21:50:07.929449: step 47070, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 67h:40m:45s remains)
INFO - root - 2017-12-05 21:50:16.346270: step 47080, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 66h:13m:52s remains)
INFO - root - 2017-12-05 21:50:24.948097: step 47090, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 67h:33m:14s remains)
INFO - root - 2017-12-05 21:50:33.328275: step 47100, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 67h:55m:16s remains)
2017-12-05 21:50:34.126722: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.310142 -4.3089876 -4.3079352 -4.30751 -4.3083715 -4.3097272 -4.3115778 -4.3126125 -4.31231 -4.3110456 -4.3093948 -4.3085585 -4.3087769 -4.3092656 -4.3094158][-4.3131928 -4.31142 -4.3091793 -4.3077216 -4.30855 -4.3111124 -4.314405 -4.3170834 -4.3178625 -4.3171587 -4.3155808 -4.3147149 -4.3149147 -4.3150706 -4.3143382][-4.317894 -4.3145876 -4.3092895 -4.3047938 -4.3040009 -4.3066964 -4.3107705 -4.3153782 -4.3185186 -4.3201728 -4.3206983 -4.3218479 -4.3235068 -4.3242073 -4.3228583][-4.3210044 -4.3148985 -4.30407 -4.293375 -4.2882781 -4.2878523 -4.2892814 -4.2939315 -4.3006568 -4.3080282 -4.3151555 -4.3230882 -4.3304453 -4.3340187 -4.3329992][-4.31876 -4.3052616 -4.2832718 -4.2601929 -4.2455273 -4.2369828 -4.230989 -4.2321196 -4.24193 -4.2606192 -4.2823262 -4.3057318 -4.3267145 -4.3389583 -4.3406506][-4.308919 -4.2838173 -4.2449942 -4.203517 -4.17248 -4.14777 -4.1251731 -4.114676 -4.1275578 -4.1643758 -4.212213 -4.2627897 -4.3071656 -4.3341417 -4.3417239][-4.2817168 -4.2469325 -4.1942449 -4.1359816 -4.0883703 -4.0437193 -3.9942713 -3.9613419 -3.9747005 -4.0352955 -4.1161656 -4.1986346 -4.269958 -4.3152552 -4.3314185][-4.23665 -4.2024379 -4.1484785 -4.0819845 -4.0206442 -3.9537287 -3.8697855 -3.8080015 -3.8204656 -3.9084809 -4.0251789 -4.1393704 -4.2349644 -4.296174 -4.3197718][-4.184741 -4.1650591 -4.1257544 -4.0651808 -4.0012426 -3.9206572 -3.8098485 -3.724586 -3.7349892 -3.8417773 -3.9795625 -4.1108251 -4.2181993 -4.286222 -4.3136244][-4.1462917 -4.1525178 -4.140964 -4.1039238 -4.055243 -3.9814851 -3.8732228 -3.7890916 -3.7955627 -3.8916869 -4.0174465 -4.13734 -4.2337084 -4.2930646 -4.3160162][-4.1589642 -4.1842403 -4.1932034 -4.1788735 -4.1499615 -4.0975389 -4.0188141 -3.9587002 -3.9654846 -4.0346394 -4.1237516 -4.2088923 -4.2764072 -4.3143859 -4.3249249][-4.2140808 -4.2423887 -4.2567225 -4.2543969 -4.2421918 -4.2153659 -4.1742 -4.1432304 -4.1488976 -4.1876297 -4.2362037 -4.282434 -4.3170791 -4.3307414 -4.3266721][-4.2713785 -4.2916279 -4.2991571 -4.2959723 -4.2899723 -4.2807131 -4.2675228 -4.2581987 -4.2633567 -4.2816834 -4.3019018 -4.3185768 -4.3263488 -4.321701 -4.3091335][-4.3013477 -4.3109818 -4.3083596 -4.2992859 -4.2941432 -4.293282 -4.2938142 -4.2948995 -4.2997794 -4.3082337 -4.3133593 -4.3118119 -4.3043795 -4.2926259 -4.2804847][-4.3011088 -4.3025751 -4.2931743 -4.280395 -4.2741003 -4.274961 -4.2799549 -4.2847958 -4.2888203 -4.2916522 -4.2885737 -4.2785549 -4.2658253 -4.2555404 -4.2493749]]...]
INFO - root - 2017-12-05 21:50:42.659706: step 47110, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 66h:06m:30s remains)
INFO - root - 2017-12-05 21:50:51.129832: step 47120, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 68h:03m:16s remains)
INFO - root - 2017-12-05 21:50:59.599391: step 47130, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 67h:42m:13s remains)
INFO - root - 2017-12-05 21:51:08.107477: step 47140, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 66h:54m:49s remains)
INFO - root - 2017-12-05 21:51:16.542116: step 47150, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 66h:30m:05s remains)
INFO - root - 2017-12-05 21:51:25.076415: step 47160, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 66h:23m:36s remains)
INFO - root - 2017-12-05 21:51:33.742892: step 47170, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 67h:32m:24s remains)
INFO - root - 2017-12-05 21:51:42.324270: step 47180, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.821 sec/batch; 65h:02m:43s remains)
INFO - root - 2017-12-05 21:51:50.838781: step 47190, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 67h:45m:44s remains)
INFO - root - 2017-12-05 21:51:59.227009: step 47200, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 67h:36m:32s remains)
2017-12-05 21:52:00.009793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219725 -4.2143521 -4.2074971 -4.1973062 -4.1892176 -4.18759 -4.1892638 -4.1919017 -4.1955404 -4.2018485 -4.2093959 -4.2174249 -4.2273993 -4.2358923 -4.2449064][-4.2006516 -4.1911025 -4.1842375 -4.1734447 -4.164608 -4.1627169 -4.164423 -4.164916 -4.1657147 -4.174778 -4.1882849 -4.1996932 -4.2122121 -4.2228537 -4.2346144][-4.167428 -4.1597404 -4.1536608 -4.14248 -4.1323953 -4.1310568 -4.1341128 -4.1307158 -4.129899 -4.14362 -4.1623893 -4.1761284 -4.1927557 -4.2087417 -4.22402][-4.1453233 -4.1415057 -4.1377072 -4.1249194 -4.1099458 -4.106184 -4.1093049 -4.1062994 -4.10509 -4.1181889 -4.1342168 -4.147234 -4.1683974 -4.1911025 -4.2113929][-4.1310635 -4.1272659 -4.1208482 -4.100379 -4.0735941 -4.0616207 -4.0683441 -4.0759411 -4.0813961 -4.0959425 -4.1111674 -4.1239328 -4.1446013 -4.1696472 -4.1931129][-4.1131015 -4.1070838 -4.0930123 -4.0571618 -4.0082488 -3.9776247 -3.98722 -4.0095272 -4.0272427 -4.0510788 -4.0762572 -4.0981846 -4.1226783 -4.1498308 -4.1735654][-4.1125951 -4.1064239 -4.0908227 -4.0495729 -3.9906015 -3.9466977 -3.9504776 -3.971034 -3.9876533 -4.0120273 -4.0424514 -4.0710874 -4.0983443 -4.1302385 -4.1547489][-4.13287 -4.1273274 -4.1140184 -4.0800338 -4.0319262 -3.995038 -3.9877086 -3.9898169 -3.993556 -4.0107102 -4.0394778 -4.0688257 -4.0976548 -4.131669 -4.1560817][-4.1708469 -4.1692638 -4.1611423 -4.1379132 -4.10816 -4.0842085 -4.0732255 -4.0656719 -4.06069 -4.0693221 -4.0888972 -4.1090746 -4.1305127 -4.1577921 -4.1774178][-4.2166672 -4.2161341 -4.2075634 -4.1873512 -4.1661744 -4.1527743 -4.1483445 -4.1398969 -4.1328092 -4.1363583 -4.1516128 -4.1635885 -4.1784205 -4.2009072 -4.2178349][-4.2605672 -4.2590671 -4.24807 -4.22928 -4.2112613 -4.1994462 -4.1960959 -4.1887388 -4.1824718 -4.1865926 -4.19938 -4.2104568 -4.2255278 -4.2457266 -4.2576323][-4.2720089 -4.2689281 -4.2581892 -4.2438493 -4.234869 -4.2314882 -4.2326465 -4.2328119 -4.2330256 -4.2358522 -4.241219 -4.2491794 -4.2612715 -4.2752523 -4.2791319][-4.254951 -4.2523475 -4.2439065 -4.2357278 -4.2356744 -4.2408466 -4.247272 -4.2537642 -4.2592897 -4.2635288 -4.2667713 -4.27125 -4.2773 -4.2828851 -4.2803903][-4.2402091 -4.2366714 -4.2303877 -4.22738 -4.2334924 -4.2440872 -4.2538056 -4.2630033 -4.2709236 -4.2771749 -4.281589 -4.284441 -4.2852397 -4.2838845 -4.2777085][-4.2494946 -4.2461267 -4.2399073 -4.2384191 -4.2455039 -4.2551913 -4.264214 -4.2718258 -4.27946 -4.285634 -4.2893391 -4.2907777 -4.2896423 -4.28724 -4.2825489]]...]
INFO - root - 2017-12-05 21:52:08.620361: step 47210, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.896 sec/batch; 70h:58m:37s remains)
INFO - root - 2017-12-05 21:52:17.143381: step 47220, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.804 sec/batch; 63h:43m:14s remains)
INFO - root - 2017-12-05 21:52:25.737352: step 47230, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 67h:22m:34s remains)
INFO - root - 2017-12-05 21:52:34.213960: step 47240, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 67h:20m:35s remains)
INFO - root - 2017-12-05 21:52:42.790734: step 47250, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 68h:12m:00s remains)
INFO - root - 2017-12-05 21:52:51.255233: step 47260, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 69h:44m:45s remains)
INFO - root - 2017-12-05 21:52:59.770465: step 47270, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.882 sec/batch; 69h:53m:41s remains)
INFO - root - 2017-12-05 21:53:08.421942: step 47280, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 66h:46m:20s remains)
INFO - root - 2017-12-05 21:53:16.984144: step 47290, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.898 sec/batch; 71h:07m:16s remains)
INFO - root - 2017-12-05 21:53:25.496124: step 47300, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 68h:09m:32s remains)
2017-12-05 21:53:26.312521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2598886 -4.2528753 -4.2424631 -4.2359309 -4.245255 -4.2615032 -4.2673583 -4.26155 -4.2489347 -4.2287879 -4.2013426 -4.1745582 -4.1621494 -4.1666875 -4.1842394][-4.2449942 -4.2387586 -4.2255855 -4.213366 -4.2158008 -4.2279391 -4.2347565 -4.2324157 -4.2233062 -4.2051287 -4.179131 -4.1523156 -4.1411037 -4.145575 -4.1637888][-4.2268033 -4.2213192 -4.2060075 -4.1868649 -4.1787953 -4.1855459 -4.1946197 -4.1963062 -4.1907868 -4.1782813 -4.1611938 -4.1474595 -4.1513014 -4.1614642 -4.1773729][-4.2062941 -4.2011724 -4.188262 -4.1657462 -4.1451964 -4.1399107 -4.1462293 -4.1485291 -4.146791 -4.1476359 -4.1482162 -4.15758 -4.1816745 -4.1989336 -4.2115059][-4.1938524 -4.1902933 -4.1790061 -4.15346 -4.1203256 -4.0975842 -4.0882564 -4.0837011 -4.0927629 -4.116281 -4.1396484 -4.1690416 -4.2064118 -4.2290211 -4.2404122][-4.1941853 -4.1955538 -4.1855555 -4.1543159 -4.1096554 -4.0678997 -4.0300741 -4.005836 -4.0297618 -4.083118 -4.1289244 -4.1708016 -4.2127361 -4.2382512 -4.249433][-4.1968031 -4.2040725 -4.1995873 -4.1659007 -4.1113319 -4.0509253 -3.9800818 -3.9275432 -3.9643729 -4.0484285 -4.1137867 -4.1618419 -4.2013459 -4.2253571 -4.2350979][-4.21082 -4.2206078 -4.2195978 -4.1874595 -4.1312141 -4.0611029 -3.9739211 -3.9060776 -3.9487045 -4.0454755 -4.1185927 -4.1636372 -4.1927953 -4.2076855 -4.2095346][-4.2403746 -4.2475505 -4.2454982 -4.2188663 -4.1719313 -4.1082149 -4.0300961 -3.97321 -4.0077868 -4.0901694 -4.1537309 -4.1880336 -4.2034917 -4.2036862 -4.1954565][-4.2815466 -4.2850828 -4.2797732 -4.2577806 -4.2221894 -4.1684136 -4.1042643 -4.0624275 -4.0871782 -4.1471491 -4.1962543 -4.2229009 -4.2314157 -4.2239232 -4.2108817][-4.3159504 -4.3177781 -4.3098054 -4.2913113 -4.265327 -4.2227249 -4.1718049 -4.1400118 -4.1582022 -4.1998172 -4.2339993 -4.255218 -4.2624784 -4.2573404 -4.2461362][-4.3318939 -4.3330827 -4.3249297 -4.3088775 -4.2911525 -4.2617779 -4.2242675 -4.198667 -4.209343 -4.2373962 -4.2607989 -4.2794027 -4.2900758 -4.292552 -4.2868719][-4.3367147 -4.3370495 -4.3292913 -4.3133116 -4.2993212 -4.2831616 -4.2586365 -4.237793 -4.2414789 -4.2590785 -4.2755318 -4.2921448 -4.3071346 -4.3153529 -4.3138762][-4.3366251 -4.3358326 -4.327467 -4.311759 -4.3003578 -4.2942505 -4.2811666 -4.2661505 -4.2653651 -4.2749557 -4.2846408 -4.2961879 -4.3105526 -4.3211446 -4.3239584][-4.33266 -4.3312473 -4.3238263 -4.3109727 -4.3030457 -4.3036141 -4.3005152 -4.2929516 -4.2916579 -4.2961359 -4.2991004 -4.3028173 -4.3108053 -4.3193159 -4.3237238]]...]
INFO - root - 2017-12-05 21:53:34.670069: step 47310, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 67h:40m:28s remains)
INFO - root - 2017-12-05 21:53:43.233765: step 47320, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 68h:54m:34s remains)
INFO - root - 2017-12-05 21:53:51.684275: step 47330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 66h:21m:29s remains)
INFO - root - 2017-12-05 21:54:00.198114: step 47340, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 65h:25m:17s remains)
INFO - root - 2017-12-05 21:54:08.794366: step 47350, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 69h:09m:26s remains)
INFO - root - 2017-12-05 21:54:17.345553: step 47360, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 68h:02m:08s remains)
INFO - root - 2017-12-05 21:54:26.000558: step 47370, loss = 2.09, batch loss = 2.03 (8.7 examples/sec; 0.917 sec/batch; 72h:37m:13s remains)
INFO - root - 2017-12-05 21:54:34.547582: step 47380, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 69h:45m:44s remains)
INFO - root - 2017-12-05 21:54:43.176853: step 47390, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 68h:11m:32s remains)
INFO - root - 2017-12-05 21:54:51.789603: step 47400, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 68h:36m:56s remains)
2017-12-05 21:54:52.578363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0498114 -4.0508704 -4.0779581 -4.1064949 -4.1296844 -4.1326327 -4.1065693 -4.0634832 -4.0267115 -4.0111871 -4.0181751 -4.0397968 -4.0539513 -4.0529838 -4.0366817][-4.0357027 -4.0302405 -4.0539579 -4.0877714 -4.1150928 -4.1198363 -4.0886979 -4.037456 -3.9915528 -3.9690125 -3.9830921 -4.0104914 -4.0317345 -4.0368276 -4.0301294][-4.0373693 -4.0327535 -4.0471449 -4.0742054 -4.0986695 -4.1035156 -4.0728593 -4.0273004 -3.9864171 -3.9654238 -3.978303 -4.0024481 -4.0203028 -4.028265 -4.0311918][-4.0563149 -4.0596418 -4.0670438 -4.0812893 -4.0956888 -4.0976954 -4.0733151 -4.0401649 -4.0136127 -4.0004988 -4.0037842 -4.0136728 -4.0199051 -4.02886 -4.041997][-4.0805454 -4.0901775 -4.0913467 -4.0912571 -4.0935669 -4.0939455 -4.0773897 -4.0567708 -4.0386214 -4.0291939 -4.0270958 -4.0293908 -4.0320077 -4.0471525 -4.0716033][-4.0928841 -4.104394 -4.1068463 -4.1014385 -4.0932908 -4.09044 -4.0786142 -4.0601645 -4.0433397 -4.0372066 -4.0390086 -4.0443769 -4.0531321 -4.077631 -4.107769][-4.1014462 -4.113903 -4.1197319 -4.1186371 -4.1074142 -4.0975647 -4.082016 -4.0611577 -4.0439897 -4.0422888 -4.0510416 -4.061378 -4.0750251 -4.1017022 -4.1276445][-4.11443 -4.1290779 -4.1354861 -4.135798 -4.1212311 -4.106616 -4.0911288 -4.0673776 -4.0494409 -4.0498786 -4.0563717 -4.0664072 -4.0825696 -4.1053581 -4.1264963][-4.1280289 -4.1500711 -4.1588087 -4.1513791 -4.1263041 -4.1042762 -4.0864916 -4.0631967 -4.0488863 -4.0511279 -4.0510097 -4.0564008 -4.0726266 -4.0950246 -4.1112537][-4.13612 -4.1645217 -4.1707997 -4.15551 -4.122478 -4.0931835 -4.07301 -4.0493922 -4.0396805 -4.0484433 -4.0498018 -4.051465 -4.0651255 -4.086647 -4.1001244][-4.1238451 -4.1545496 -4.1583233 -4.1436572 -4.1097355 -4.0768938 -4.0598826 -4.0446525 -4.0414267 -4.05214 -4.0589385 -4.0607042 -4.0728073 -4.0933285 -4.1068645][-4.0968742 -4.12649 -4.1295805 -4.1217427 -4.0919394 -4.0680652 -4.0635076 -4.0592222 -4.057641 -4.067862 -4.0798397 -4.0851583 -4.1007376 -4.1231914 -4.1341929][-4.0762696 -4.095 -4.0948548 -4.0912862 -4.0763493 -4.0703931 -4.0795531 -4.0779767 -4.0720015 -4.0842886 -4.1013212 -4.1106153 -4.1272864 -4.1496711 -4.159339][-4.0705638 -4.0707822 -4.0656424 -4.0665293 -4.0679851 -4.0750008 -4.0879722 -4.0829859 -4.0763025 -4.0943303 -4.1143327 -4.1287284 -4.1452632 -4.163846 -4.1696234][-4.0856872 -4.07462 -4.065218 -4.0665617 -4.0777822 -4.0852952 -4.0908284 -4.0810084 -4.0770845 -4.0989194 -4.1208076 -4.14219 -4.1546893 -4.1608658 -4.1583729]]...]
INFO - root - 2017-12-05 21:55:00.978247: step 47410, loss = 2.04, batch loss = 1.99 (9.8 examples/sec; 0.812 sec/batch; 64h:19m:18s remains)
INFO - root - 2017-12-05 21:55:09.451803: step 47420, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 68h:03m:34s remains)
INFO - root - 2017-12-05 21:55:17.977680: step 47430, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 67h:53m:52s remains)
INFO - root - 2017-12-05 21:55:26.580117: step 47440, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 67h:34m:48s remains)
INFO - root - 2017-12-05 21:55:34.987595: step 47450, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 67h:19m:20s remains)
INFO - root - 2017-12-05 21:55:43.674760: step 47460, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 67h:50m:43s remains)
INFO - root - 2017-12-05 21:55:52.154750: step 47470, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 66h:06m:25s remains)
INFO - root - 2017-12-05 21:56:00.781570: step 47480, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.818 sec/batch; 64h:45m:05s remains)
INFO - root - 2017-12-05 21:56:09.437272: step 47490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.894 sec/batch; 70h:44m:37s remains)
INFO - root - 2017-12-05 21:56:17.902447: step 47500, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 67h:08m:45s remains)
2017-12-05 21:56:18.656468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2242332 -4.2188973 -4.20609 -4.2022009 -4.2028294 -4.2028728 -4.1964269 -4.193119 -4.1879592 -4.1808758 -4.183208 -4.1898546 -4.2011418 -4.2021132 -4.1934233][-4.2190809 -4.2041321 -4.180819 -4.1651683 -4.1605525 -4.1612091 -4.1627932 -4.1729884 -4.182364 -4.1905336 -4.199533 -4.2020822 -4.2086549 -4.2069244 -4.1970205][-4.2076535 -4.1818981 -4.1512589 -4.1320844 -4.12525 -4.1250472 -4.1323838 -4.1517506 -4.1706133 -4.188447 -4.1995964 -4.1963663 -4.1984062 -4.1968756 -4.1926613][-4.1854186 -4.1540403 -4.1277676 -4.1177421 -4.1143808 -4.1105771 -4.1168222 -4.1370478 -4.1605253 -4.183218 -4.1950841 -4.1849132 -4.1779041 -4.1743884 -4.1748395][-4.1457868 -4.1130586 -4.1000552 -4.1100764 -4.1193352 -4.1123128 -4.1135387 -4.1304479 -4.1563482 -4.1808095 -4.1940684 -4.1798482 -4.1627345 -4.1553617 -4.1605458][-4.0985985 -4.0678277 -4.0676589 -4.0924797 -4.1104741 -4.1002941 -4.0935936 -4.1080332 -4.1357331 -4.1626616 -4.1823111 -4.1744213 -4.1561852 -4.1510143 -4.1605067][-4.0876908 -4.0612087 -4.0649934 -4.0850911 -4.09297 -4.0699258 -4.0454617 -4.0537891 -4.0831122 -4.1209288 -4.157342 -4.1667557 -4.1559253 -4.1595592 -4.1732492][-4.1263938 -4.1067162 -4.1092649 -4.1143084 -4.1005096 -4.054471 -4.0040064 -4.0022826 -4.0341311 -4.0829763 -4.1310954 -4.1524625 -4.1527491 -4.1630573 -4.1794152][-4.1874523 -4.173737 -4.1723781 -4.1661272 -4.14292 -4.0887861 -4.02942 -4.0185041 -4.0429153 -4.0846829 -4.1239696 -4.1440992 -4.155849 -4.1695318 -4.1829147][-4.2292318 -4.2183323 -4.2133794 -4.2029195 -4.1819649 -4.1384144 -4.0955286 -4.0842977 -4.0976281 -4.1205144 -4.1419044 -4.1544957 -4.1704278 -4.181911 -4.1878366][-4.2485743 -4.2401681 -4.2352109 -4.2275543 -4.2128305 -4.1828446 -4.1572194 -4.1486216 -4.1515694 -4.1582527 -4.165699 -4.17163 -4.1825013 -4.1856394 -4.1866188][-4.2559805 -4.24752 -4.2411327 -4.2364011 -4.2296491 -4.2149854 -4.201591 -4.195096 -4.1899652 -4.1841431 -4.183629 -4.1857071 -4.1892209 -4.18626 -4.1818051][-4.2566175 -4.2461243 -4.237112 -4.2324033 -4.2295432 -4.2250428 -4.2215781 -4.2200718 -4.2121091 -4.2008877 -4.196353 -4.1945906 -4.1913486 -4.1826782 -4.1713338][-4.2669487 -4.2558312 -4.2445016 -4.2353954 -4.2288275 -4.2255569 -4.2252693 -4.2265916 -4.2200947 -4.2080145 -4.2012491 -4.1951127 -4.1848812 -4.1682415 -4.1490712][-4.28527 -4.27554 -4.2639837 -4.251543 -4.2393818 -4.2307076 -4.2251029 -4.2214522 -4.2132134 -4.2000918 -4.1939754 -4.1845126 -4.1682038 -4.1457877 -4.1206651]]...]
INFO - root - 2017-12-05 21:56:27.199126: step 47510, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 66h:51m:46s remains)
INFO - root - 2017-12-05 21:56:35.538512: step 47520, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 68h:30m:40s remains)
INFO - root - 2017-12-05 21:56:44.073978: step 47530, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 68h:37m:17s remains)
INFO - root - 2017-12-05 21:56:52.617231: step 47540, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 67h:11m:32s remains)
INFO - root - 2017-12-05 21:57:01.047272: step 47550, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:56m:03s remains)
INFO - root - 2017-12-05 21:57:09.516729: step 47560, loss = 2.08, batch loss = 2.03 (9.7 examples/sec; 0.829 sec/batch; 65h:35m:48s remains)
INFO - root - 2017-12-05 21:57:18.121019: step 47570, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 68h:26m:20s remains)
INFO - root - 2017-12-05 21:57:26.578797: step 47580, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.871 sec/batch; 68h:56m:39s remains)
INFO - root - 2017-12-05 21:57:35.142935: step 47590, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 69h:00m:59s remains)
INFO - root - 2017-12-05 21:57:43.669659: step 47600, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 68h:36m:33s remains)
2017-12-05 21:57:44.437984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1408758 -4.1419821 -4.1186428 -4.081964 -4.06945 -4.1056075 -4.145514 -4.1429939 -4.1252851 -4.1341496 -4.1636143 -4.193038 -4.215497 -4.2436647 -4.2808495][-4.14724 -4.157186 -4.1362524 -4.0913925 -4.0680509 -4.100153 -4.1446238 -4.1428328 -4.1270809 -4.1430922 -4.1767869 -4.2077427 -4.2253437 -4.244669 -4.2773318][-4.1575694 -4.1742091 -4.1598587 -4.1179957 -4.086957 -4.1053529 -4.1445289 -4.1391621 -4.1261191 -4.1496491 -4.1850219 -4.2127404 -4.2256427 -4.2414789 -4.2717633][-4.1538467 -4.1772165 -4.1749959 -4.1482329 -4.118547 -4.1232 -4.1455803 -4.1267481 -4.1128526 -4.1437449 -4.1801257 -4.2043319 -4.21857 -4.2376103 -4.2684813][-4.1341062 -4.1554637 -4.1595006 -4.1486783 -4.1277914 -4.1231303 -4.1292825 -4.094964 -4.0769658 -4.11794 -4.1590614 -4.1836247 -4.2044935 -4.2315125 -4.2657661][-4.1170292 -4.1325817 -4.1327062 -4.1267209 -4.1123285 -4.1047788 -4.1028533 -4.0575991 -4.0407505 -4.0911217 -4.1405382 -4.1695061 -4.19511 -4.2281556 -4.2644329][-4.1156483 -4.12296 -4.1177006 -4.11073 -4.1014285 -4.0910869 -4.08298 -4.0317559 -4.0167203 -4.0745435 -4.1308308 -4.1644559 -4.1932783 -4.2290349 -4.2647214][-4.1298261 -4.1261349 -4.1119041 -4.0981565 -4.0851178 -4.0656719 -4.0490675 -3.997112 -3.9877479 -4.0564718 -4.1201134 -4.157249 -4.1890144 -4.2263551 -4.2609248][-4.1631513 -4.1486912 -4.1246719 -4.1019559 -4.0872908 -4.0657225 -4.0483441 -4.0077353 -4.0129843 -4.0878878 -4.1483679 -4.1795735 -4.2046251 -4.235579 -4.2636104][-4.1944366 -4.1775074 -4.1497793 -4.1247187 -4.1137195 -4.0996852 -4.0855 -4.0520325 -4.0632772 -4.1316934 -4.1805539 -4.2023621 -4.2210422 -4.2461066 -4.2712622][-4.2225332 -4.2081957 -4.1837006 -4.1607161 -4.1514974 -4.1438122 -4.13467 -4.1092072 -4.1199126 -4.1730781 -4.2104378 -4.2274542 -4.2425985 -4.2628727 -4.2849808][-4.2424121 -4.2262974 -4.2064643 -4.1908131 -4.1855068 -4.1808681 -4.174726 -4.1555071 -4.1616645 -4.2018671 -4.2345176 -4.250772 -4.2662764 -4.2837009 -4.3008528][-4.2578812 -4.2407427 -4.2234263 -4.2116485 -4.2084355 -4.2089596 -4.2075696 -4.1967325 -4.1990905 -4.2258062 -4.2507014 -4.265605 -4.2816887 -4.2983985 -4.3123236][-4.2743382 -4.2586279 -4.2443986 -4.2371817 -4.2374444 -4.2413135 -4.2421837 -4.2367392 -4.2365522 -4.250411 -4.2679672 -4.2819767 -4.2958241 -4.3099947 -4.3202567][-4.2889929 -4.2778754 -4.2689366 -4.2645645 -4.2639742 -4.2659564 -4.26706 -4.2656746 -4.2655587 -4.2719536 -4.28326 -4.295197 -4.3071604 -4.3180184 -4.324646]]...]
INFO - root - 2017-12-05 21:57:53.091779: step 47610, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 65h:57m:25s remains)
INFO - root - 2017-12-05 21:58:01.679611: step 47620, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 67h:53m:28s remains)
INFO - root - 2017-12-05 21:58:10.098923: step 47630, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 69h:00m:01s remains)
INFO - root - 2017-12-05 21:58:18.659128: step 47640, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 68h:21m:29s remains)
INFO - root - 2017-12-05 21:58:27.136335: step 47650, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 70h:00m:17s remains)
INFO - root - 2017-12-05 21:58:35.709068: step 47660, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 69h:05m:41s remains)
INFO - root - 2017-12-05 21:58:44.322515: step 47670, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 68h:23m:11s remains)
INFO - root - 2017-12-05 21:58:52.911622: step 47680, loss = 2.09, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 68h:37m:20s remains)
INFO - root - 2017-12-05 21:59:01.472929: step 47690, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 67h:21m:16s remains)
INFO - root - 2017-12-05 21:59:09.977019: step 47700, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 68h:16m:55s remains)
2017-12-05 21:59:10.692849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2550235 -4.2443018 -4.2359834 -4.2358966 -4.236939 -4.2379365 -4.2370377 -4.2391086 -4.2453504 -4.2506471 -4.2554321 -4.2593188 -4.2649369 -4.2679653 -4.2682161][-4.2341533 -4.2197866 -4.2109132 -4.2115784 -4.2125316 -4.209733 -4.2040267 -4.1983933 -4.2019444 -4.207016 -4.2116528 -4.2183304 -4.2319722 -4.2448134 -4.250843][-4.2222805 -4.2117958 -4.207046 -4.2058816 -4.1999364 -4.1883979 -4.1775002 -4.1656709 -4.1677194 -4.16857 -4.1673756 -4.1732588 -4.1925011 -4.2163219 -4.2317572][-4.21205 -4.2046313 -4.2010016 -4.19462 -4.1823435 -4.1636171 -4.1467161 -4.1332312 -4.1324267 -4.1306891 -4.1299777 -4.136394 -4.1533651 -4.1796002 -4.19791][-4.2067137 -4.1981583 -4.1909747 -4.1814265 -4.1678576 -4.146246 -4.1200976 -4.1001143 -4.100286 -4.10561 -4.1124015 -4.1122484 -4.1207929 -4.141808 -4.1631203][-4.2007608 -4.1938334 -4.1870165 -4.1752381 -4.1574917 -4.1282077 -4.0872927 -4.0582733 -4.0670147 -4.085887 -4.098824 -4.0926652 -4.0924268 -4.1079097 -4.1348314][-4.19462 -4.1903839 -4.1803651 -4.162158 -4.1365123 -4.0949955 -4.0281339 -3.9781222 -4.00218 -4.0403295 -4.0640416 -4.0624809 -4.0635271 -4.0750709 -4.1012635][-4.1722217 -4.1625113 -4.1480842 -4.1301584 -4.1005344 -4.0501957 -3.9561787 -3.87873 -3.9190414 -3.9835956 -4.0246329 -4.0294304 -4.0298495 -4.0386496 -4.0610161][-4.153666 -4.1404772 -4.1280642 -4.1147485 -4.0923996 -4.0499368 -3.9724116 -3.9144073 -3.9414418 -3.9872222 -4.0183778 -4.0192528 -4.0177484 -4.0285249 -4.052927][-4.1630096 -4.1504226 -4.1389952 -4.1265411 -4.1139832 -4.0885305 -4.0449786 -4.0137687 -4.0178986 -4.0319061 -4.0451355 -4.0467191 -4.0488052 -4.0625215 -4.087441][-4.1934862 -4.1794591 -4.165678 -4.1493382 -4.1370573 -4.1193323 -4.0945415 -4.0824876 -4.0819545 -4.0854058 -4.089273 -4.0907106 -4.0971775 -4.1111879 -4.1320019][-4.2281432 -4.2117658 -4.193666 -4.1725888 -4.15881 -4.1453805 -4.1319265 -4.1348324 -4.1400003 -4.1425581 -4.1442275 -4.14608 -4.1536188 -4.1636682 -4.174726][-4.2505465 -4.2339344 -4.2139416 -4.1935315 -4.1803613 -4.1737905 -4.1673732 -4.1720076 -4.1792154 -4.1828709 -4.18638 -4.1900525 -4.1951995 -4.2000561 -4.2066069][-4.2681012 -4.2518725 -4.234817 -4.220705 -4.2110825 -4.2091756 -4.2062249 -4.210216 -4.2165756 -4.2208858 -4.2258549 -4.2311826 -4.234889 -4.2357273 -4.2409348][-4.2802253 -4.2687058 -4.2575579 -4.25122 -4.2472477 -4.2484288 -4.2503648 -4.2542849 -4.2581925 -4.2609577 -4.2643642 -4.2688165 -4.2713847 -4.2705145 -4.2729068]]...]
INFO - root - 2017-12-05 21:59:19.235391: step 47710, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 69h:23m:53s remains)
INFO - root - 2017-12-05 21:59:27.734923: step 47720, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 65h:42m:30s remains)
INFO - root - 2017-12-05 21:59:36.284892: step 47730, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.827 sec/batch; 65h:26m:32s remains)
INFO - root - 2017-12-05 21:59:44.664587: step 47740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 68h:07m:20s remains)
INFO - root - 2017-12-05 21:59:53.208081: step 47750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:21m:27s remains)
INFO - root - 2017-12-05 22:00:01.775015: step 47760, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 68h:27m:05s remains)
INFO - root - 2017-12-05 22:00:10.320987: step 47770, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 68h:32m:36s remains)
INFO - root - 2017-12-05 22:00:18.818056: step 47780, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:33m:37s remains)
INFO - root - 2017-12-05 22:00:27.546380: step 47790, loss = 2.06, batch loss = 2.00 (8.7 examples/sec; 0.922 sec/batch; 72h:54m:05s remains)
INFO - root - 2017-12-05 22:00:36.060623: step 47800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 66h:37m:28s remains)
2017-12-05 22:00:36.941441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2850323 -4.2763762 -4.2590942 -4.2536678 -4.2467022 -4.2326975 -4.2436628 -4.270072 -4.2785077 -4.2826533 -4.2804875 -4.2812915 -4.2878308 -4.2902832 -4.2869568][-4.2797379 -4.2693753 -4.2559228 -4.2568316 -4.2510519 -4.2338033 -4.2424359 -4.2702885 -4.2798624 -4.2842841 -4.2849159 -4.2871361 -4.2905636 -4.2900963 -4.2862964][-4.2730041 -4.261425 -4.2506509 -4.2551918 -4.2493415 -4.2306004 -4.2396383 -4.2703824 -4.2827692 -4.2887874 -4.2935185 -4.2962208 -4.2945881 -4.2896795 -4.2866564][-4.2691541 -4.2556386 -4.2431564 -4.2464247 -4.2395945 -4.2194271 -4.2299814 -4.2614322 -4.2738833 -4.2816443 -4.291822 -4.297195 -4.293479 -4.2885 -4.2882056][-4.2671404 -4.2529974 -4.2378206 -4.2371883 -4.2281528 -4.2079444 -4.2206759 -4.2501969 -4.2616282 -4.2717633 -4.2859964 -4.2930751 -4.2889948 -4.28583 -4.288538][-4.26388 -4.2478461 -4.2295551 -4.2253323 -4.21463 -4.1972985 -4.2131882 -4.2396355 -4.2488885 -4.2612138 -4.2783031 -4.2841725 -4.2800212 -4.280045 -4.2836757][-4.2591 -4.2404256 -4.2189703 -4.2127547 -4.20225 -4.1876988 -4.2062035 -4.2292533 -4.2356725 -4.2460933 -4.2594852 -4.2594981 -4.2518997 -4.2521625 -4.2573342][-4.2539768 -4.2320614 -4.2064853 -4.1981516 -4.1872382 -4.1745377 -4.1955404 -4.2176046 -4.2208095 -4.2270646 -4.234674 -4.2287292 -4.2159457 -4.2159629 -4.2267985][-4.2481227 -4.2235422 -4.19444 -4.1817622 -4.1681166 -4.1542206 -4.178297 -4.2043424 -4.2064137 -4.2086778 -4.2135353 -4.2062073 -4.1907988 -4.1919 -4.2085104][-4.2420058 -4.2151871 -4.1833482 -4.1644788 -4.1454844 -4.1294012 -4.1571255 -4.1895485 -4.1931014 -4.1915951 -4.1943874 -4.1876292 -4.1707931 -4.1718507 -4.1936646][-4.2380548 -4.2091141 -4.1743145 -4.1503592 -4.1275611 -4.1095133 -4.1383853 -4.1740441 -4.1772265 -4.1709938 -4.1711469 -4.164515 -4.1480207 -4.1527228 -4.1826706][-4.2356439 -4.2039504 -4.1650753 -4.1377082 -4.1149035 -4.0983214 -4.1271605 -4.163413 -4.1655817 -4.1549854 -4.1512322 -4.1441922 -4.131094 -4.1399846 -4.1772556][-4.2349358 -4.2010489 -4.1589918 -4.1298418 -4.109334 -4.0983295 -4.1280465 -4.1628761 -4.16304 -4.1492543 -4.1419659 -4.1351638 -4.1271329 -4.1394453 -4.1804371][-4.2346811 -4.1985331 -4.1549497 -4.1245031 -4.1065106 -4.1011152 -4.1320553 -4.1638842 -4.1624246 -4.1468887 -4.1360745 -4.1292958 -4.1256456 -4.1383691 -4.1755447][-4.2359753 -4.198153 -4.1539149 -4.1215396 -4.1041079 -4.1022253 -4.1330295 -4.1610737 -4.1609025 -4.1477857 -4.1371374 -4.1333289 -4.1350279 -4.1463943 -4.1705337]]...]
INFO - root - 2017-12-05 22:00:45.534281: step 47810, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 67h:06m:00s remains)
INFO - root - 2017-12-05 22:00:54.081607: step 47820, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:51m:40s remains)
INFO - root - 2017-12-05 22:01:02.585939: step 47830, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 68h:32m:59s remains)
INFO - root - 2017-12-05 22:01:11.010106: step 47840, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 66h:06m:54s remains)
INFO - root - 2017-12-05 22:01:19.374765: step 47850, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:19m:46s remains)
INFO - root - 2017-12-05 22:01:27.919311: step 47860, loss = 2.02, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 66h:22m:26s remains)
INFO - root - 2017-12-05 22:01:36.528701: step 47870, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 67h:54m:08s remains)
INFO - root - 2017-12-05 22:01:45.027097: step 47880, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 68h:40m:22s remains)
INFO - root - 2017-12-05 22:01:53.559439: step 47890, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 66h:55m:19s remains)
INFO - root - 2017-12-05 22:02:02.152535: step 47900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 67h:37m:25s remains)
2017-12-05 22:02:02.914250: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.25754 -4.2647948 -4.268229 -4.2717447 -4.2727365 -4.2700214 -4.2613792 -4.2495108 -4.2407465 -4.2398362 -4.2442632 -4.2448254 -4.2459989 -4.2430711 -4.236423][-4.2243509 -4.2404037 -4.2495022 -4.2539191 -4.2571135 -4.2605805 -4.2626529 -4.2652555 -4.2691765 -4.275197 -4.27974 -4.2761021 -4.2685122 -4.2569795 -4.2420373][-4.1828508 -4.2068887 -4.2241621 -4.2328997 -4.2376323 -4.2438626 -4.2547641 -4.2696409 -4.2841578 -4.2929134 -4.2955203 -4.2887039 -4.2724261 -4.2516494 -4.2295294][-4.1648774 -4.1915083 -4.2106342 -4.2181144 -4.2188573 -4.2207336 -4.2341795 -4.2579794 -4.2816997 -4.2943745 -4.2974896 -4.288733 -4.2646527 -4.2347693 -4.2084355][-4.1767697 -4.1966529 -4.2049489 -4.2012763 -4.1893468 -4.1811905 -4.18863 -4.2131195 -4.2444439 -4.2668748 -4.2777996 -4.2747793 -4.2510691 -4.2195559 -4.1967254][-4.205945 -4.2108655 -4.1998944 -4.1783252 -4.149694 -4.1271772 -4.1221514 -4.1403742 -4.1804395 -4.2154956 -4.2376723 -4.2470245 -4.2328458 -4.2064686 -4.19253][-4.2426796 -4.2295017 -4.1956663 -4.1536713 -4.1040654 -4.0607462 -4.0368915 -4.0423717 -4.0919232 -4.14252 -4.1817579 -4.21055 -4.2145205 -4.2002163 -4.1926832][-4.269042 -4.2407103 -4.1823411 -4.1144085 -4.0378928 -3.9649985 -3.9110677 -3.8941422 -3.95492 -4.0308557 -4.0998759 -4.1593437 -4.1940422 -4.2041163 -4.20821][-4.2732234 -4.2375259 -4.1664858 -4.0823407 -3.9894271 -3.8998325 -3.8267298 -3.7930183 -3.8569479 -3.9489512 -4.0403996 -4.1236429 -4.1852303 -4.2224627 -4.2416725][-4.2643681 -4.235486 -4.1710129 -4.0938444 -4.0129466 -3.9429314 -3.8863792 -3.8574574 -3.9063442 -3.9799721 -4.0614386 -4.1421652 -4.2104745 -4.2576652 -4.2823586][-4.2569094 -4.2427716 -4.1965384 -4.1366405 -4.0789795 -4.0391779 -4.0146041 -4.0032034 -4.0393271 -4.0895886 -4.1456847 -4.2056608 -4.2603688 -4.2972903 -4.3149195][-4.2658916 -4.2658477 -4.2392979 -4.200069 -4.166121 -4.1508965 -4.1485548 -4.1504478 -4.17767 -4.2076759 -4.2395644 -4.2744207 -4.3061213 -4.3250318 -4.3309584][-4.2884378 -4.2980452 -4.2892785 -4.2707758 -4.2559 -4.2521615 -4.2552695 -4.2590485 -4.2758741 -4.29201 -4.30781 -4.323061 -4.3346338 -4.3383384 -4.3355327][-4.3107882 -4.3235936 -4.3255363 -4.32208 -4.3166566 -4.3145871 -4.31375 -4.3147836 -4.3238435 -4.3325071 -4.3404078 -4.3451056 -4.3463778 -4.3431077 -4.336998][-4.3239813 -4.336318 -4.3423157 -4.3456445 -4.3446908 -4.3413486 -4.3358855 -4.3326511 -4.3357949 -4.3408523 -4.346036 -4.3482523 -4.34732 -4.3435054 -4.3382335]]...]
INFO - root - 2017-12-05 22:02:11.372409: step 47910, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 66h:41m:06s remains)
INFO - root - 2017-12-05 22:02:19.986369: step 47920, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 67h:13m:19s remains)
INFO - root - 2017-12-05 22:02:28.569079: step 47930, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 68h:45m:08s remains)
INFO - root - 2017-12-05 22:02:37.105081: step 47940, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 66h:56m:33s remains)
INFO - root - 2017-12-05 22:02:45.566891: step 47950, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.882 sec/batch; 69h:42m:29s remains)
INFO - root - 2017-12-05 22:02:53.940083: step 47960, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 66h:18m:17s remains)
INFO - root - 2017-12-05 22:03:02.467324: step 47970, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 65h:54m:04s remains)
INFO - root - 2017-12-05 22:03:11.025373: step 47980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 68h:09m:57s remains)
INFO - root - 2017-12-05 22:03:19.581858: step 47990, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.856 sec/batch; 67h:37m:04s remains)
INFO - root - 2017-12-05 22:03:28.119291: step 48000, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 66h:09m:51s remains)
2017-12-05 22:03:28.888029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1247034 -4.1295509 -4.1331329 -4.1397462 -4.1411967 -4.1450567 -4.1409349 -4.1236386 -4.1170154 -4.1157894 -4.115283 -4.1099653 -4.1018667 -4.1000032 -4.0992517][-4.1142492 -4.1234303 -4.1295104 -4.1380053 -4.1404605 -4.1478057 -4.1499262 -4.1370745 -4.124517 -4.11584 -4.1144934 -4.1072035 -4.0950065 -4.0916238 -4.0982413][-4.1113963 -4.1272664 -4.133111 -4.1374693 -4.1360068 -4.1445079 -4.1464591 -4.1372433 -4.1258249 -4.1156244 -4.1116056 -4.0991373 -4.0844774 -4.0826235 -4.0935106][-4.10677 -4.1234045 -4.1270819 -4.1290145 -4.1253171 -4.1280079 -4.123292 -4.1152849 -4.1148181 -4.1133866 -4.1083951 -4.0923476 -4.0770426 -4.074357 -4.0837312][-4.0897894 -4.1039438 -4.1069093 -4.1064091 -4.1023788 -4.0940428 -4.0738282 -4.0567493 -4.0668397 -4.0791712 -4.0777483 -4.0573535 -4.0427361 -4.0447969 -4.0569367][-4.0672584 -4.0807476 -4.0847611 -4.0796766 -4.0663252 -4.0439134 -4.0077753 -3.9820259 -3.9962263 -4.0196624 -4.0217032 -4.0018063 -3.994354 -4.0033445 -4.0177341][-4.0421906 -4.0532374 -4.0589533 -4.0528221 -4.0316606 -4.0009212 -3.9645793 -3.9425294 -3.9570503 -3.9831355 -3.9895391 -3.9840219 -3.9900985 -3.9994588 -3.9999628][-4.0267491 -4.0318465 -4.039619 -4.0390029 -4.0199986 -3.9939857 -3.9744651 -3.9658513 -3.9774866 -3.9980235 -4.0053906 -4.0112839 -4.0281353 -4.0386715 -4.0256958][-4.0237522 -4.0264692 -4.0363379 -4.0444813 -4.0389256 -4.0276737 -4.0236549 -4.0255423 -4.0327716 -4.0463381 -4.0498791 -4.0567851 -4.0754766 -4.0906496 -4.0820026][-4.0268927 -4.0276337 -4.0375109 -4.0522537 -4.0641208 -4.07264 -4.0794926 -4.08629 -4.0971026 -4.1088891 -4.1110911 -4.1156282 -4.1315594 -4.148231 -4.1481447][-4.0452161 -4.042417 -4.0491991 -4.069767 -4.092351 -4.1106114 -4.1242309 -4.13847 -4.1574945 -4.1702547 -4.170125 -4.1707869 -4.1788034 -4.1900826 -4.1913896][-4.0847497 -4.0792642 -4.0817814 -4.0962753 -4.1158934 -4.132822 -4.1484985 -4.1652069 -4.1834555 -4.1936321 -4.1902766 -4.1859975 -4.1874881 -4.1951365 -4.198308][-4.1162453 -4.1101651 -4.1111989 -4.119813 -4.1266394 -4.1321068 -4.1409106 -4.1515427 -4.1597877 -4.1622643 -4.1597576 -4.1518879 -4.1491365 -4.1571264 -4.1673236][-4.1250644 -4.1182647 -4.1144805 -4.1167693 -4.113112 -4.1090441 -4.11069 -4.1135054 -4.1165819 -4.1203556 -4.12146 -4.112433 -4.1018381 -4.106421 -4.1184068][-4.1175137 -4.099915 -4.0838923 -4.0812893 -4.078968 -4.0752921 -4.0754972 -4.072731 -4.0740061 -4.0850472 -4.093852 -4.0837717 -4.0652914 -4.065681 -4.0719514]]...]
INFO - root - 2017-12-05 22:03:37.421698: step 48010, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 66h:52m:07s remains)
INFO - root - 2017-12-05 22:03:45.837175: step 48020, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 65h:42m:30s remains)
INFO - root - 2017-12-05 22:03:54.282387: step 48030, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.853 sec/batch; 67h:24m:38s remains)
INFO - root - 2017-12-05 22:04:02.817125: step 48040, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 68h:22m:52s remains)
INFO - root - 2017-12-05 22:04:11.227808: step 48050, loss = 2.08, batch loss = 2.03 (10.1 examples/sec; 0.795 sec/batch; 62h:50m:12s remains)
INFO - root - 2017-12-05 22:04:19.728799: step 48060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 67h:39m:03s remains)
INFO - root - 2017-12-05 22:04:28.085588: step 48070, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 67h:07m:27s remains)
INFO - root - 2017-12-05 22:04:36.596960: step 48080, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 67h:48m:00s remains)
INFO - root - 2017-12-05 22:04:45.104729: step 48090, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 67h:57m:46s remains)
INFO - root - 2017-12-05 22:04:53.619752: step 48100, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 66h:41m:03s remains)
2017-12-05 22:04:54.444800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2164598 -4.2098327 -4.2110772 -4.2209024 -4.2336822 -4.2374988 -4.2341175 -4.2260361 -4.2173758 -4.2197838 -4.2277431 -4.2340493 -4.23633 -4.2338314 -4.2326446][-4.1993957 -4.183444 -4.1698689 -4.172442 -4.1855054 -4.1943474 -4.2003427 -4.2030096 -4.2010345 -4.2055345 -4.2158446 -4.2234254 -4.2266145 -4.2251906 -4.2269421][-4.174098 -4.1547337 -4.1260476 -4.1184454 -4.1316113 -4.1494594 -4.1697068 -4.1861506 -4.1954 -4.2034025 -4.2114153 -4.2177773 -4.2212524 -4.2223849 -4.2273259][-4.1582217 -4.14633 -4.1119952 -4.092278 -4.0986404 -4.1165228 -4.1396804 -4.1632023 -4.1802874 -4.1949258 -4.204463 -4.2114038 -4.2143769 -4.2177696 -4.2256312][-4.1502566 -4.15398 -4.1221242 -4.09176 -4.085197 -4.0862503 -4.0969172 -4.1134095 -4.1314559 -4.1529751 -4.1700292 -4.1856065 -4.1971788 -4.2116132 -4.224884][-4.1407995 -4.157372 -4.1319242 -4.0950131 -4.0695133 -4.0465851 -4.0404778 -4.0494347 -4.0709419 -4.1001611 -4.1306481 -4.1610169 -4.1863642 -4.2183809 -4.2436695][-4.1213517 -4.1350541 -4.10809 -4.0622196 -4.0197248 -3.9891484 -3.9855456 -4.0056739 -4.034574 -4.068583 -4.1110692 -4.1548228 -4.1931338 -4.2383933 -4.2704439][-4.1017232 -4.1053481 -4.0753555 -4.0286 -3.9891856 -3.9727004 -3.9867454 -4.0169535 -4.0465617 -4.077764 -4.1209464 -4.1648874 -4.2033467 -4.2444377 -4.2703152][-4.1146169 -4.1030641 -4.0672908 -4.02528 -3.9982555 -3.994025 -4.0172625 -4.0493431 -4.0769978 -4.1030183 -4.1389294 -4.1767898 -4.2093191 -4.2372594 -4.2522073][-4.1433868 -4.1201906 -4.0808959 -4.04432 -4.0228286 -4.0259786 -4.049963 -4.0802789 -4.1065831 -4.1249976 -4.1521363 -4.1842823 -4.2091842 -4.2240186 -4.2326622][-4.1655393 -4.1390595 -4.1033187 -4.076869 -4.0590563 -4.0608578 -4.0841002 -4.111135 -4.1330295 -4.1463795 -4.1659565 -4.1893196 -4.2023697 -4.2085876 -4.2159853][-4.1645427 -4.1401935 -4.1143737 -4.1000018 -4.0857987 -4.0879412 -4.1119261 -4.1374931 -4.1551557 -4.1606736 -4.1690207 -4.180172 -4.1862073 -4.1925068 -4.2037086][-4.1540132 -4.12913 -4.1111665 -4.1082907 -4.1031728 -4.10814 -4.1324062 -4.1564374 -4.169497 -4.1665931 -4.1598983 -4.1598458 -4.1619558 -4.1694851 -4.1841621][-4.1569095 -4.1245456 -4.1036625 -4.1015992 -4.101501 -4.10725 -4.1298122 -4.1509638 -4.16151 -4.1562009 -4.1447496 -4.1415319 -4.1403847 -4.1484971 -4.1636653][-4.1647367 -4.1309 -4.1054759 -4.0987773 -4.0970721 -4.103117 -4.1265087 -4.1484127 -4.1628647 -4.1622066 -4.1491814 -4.1397824 -4.133543 -4.1372247 -4.1504416]]...]
INFO - root - 2017-12-05 22:05:02.938417: step 48110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 67h:39m:28s remains)
INFO - root - 2017-12-05 22:05:11.484998: step 48120, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.844 sec/batch; 66h:38m:22s remains)
INFO - root - 2017-12-05 22:05:19.932965: step 48130, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 64h:02m:34s remains)
INFO - root - 2017-12-05 22:05:28.386551: step 48140, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 64h:33m:27s remains)
INFO - root - 2017-12-05 22:05:36.822109: step 48150, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.838 sec/batch; 66h:11m:07s remains)
INFO - root - 2017-12-05 22:05:45.255837: step 48160, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 66h:48m:27s remains)
INFO - root - 2017-12-05 22:05:53.826192: step 48170, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 66h:31m:52s remains)
INFO - root - 2017-12-05 22:06:02.350282: step 48180, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 68h:22m:36s remains)
INFO - root - 2017-12-05 22:06:10.788546: step 48190, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 65h:49m:26s remains)
INFO - root - 2017-12-05 22:06:19.354567: step 48200, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 68h:01m:35s remains)
2017-12-05 22:06:20.112644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.246984 -4.2580013 -4.263309 -4.2683811 -4.271246 -4.2733583 -4.2738013 -4.2665591 -4.2558475 -4.2471333 -4.2461853 -4.2566547 -4.2716222 -4.2833791 -4.2861457][-4.2480717 -4.2572389 -4.2636604 -4.2728996 -4.2788682 -4.2795677 -4.2736068 -4.2589316 -4.2412934 -4.2276678 -4.2284617 -4.2447619 -4.2651963 -4.2804856 -4.2860589][-4.2512264 -4.2571926 -4.2636356 -4.2768526 -4.2874064 -4.2873058 -4.2738819 -4.2495718 -4.2228417 -4.2024889 -4.2043014 -4.2262559 -4.2514486 -4.270359 -4.280035][-4.2562847 -4.2580853 -4.2643232 -4.2805772 -4.2944522 -4.2921953 -4.2703767 -4.2353354 -4.1970615 -4.1679788 -4.1714745 -4.2008109 -4.2326703 -4.2571821 -4.273519][-4.254993 -4.2564592 -4.2649617 -4.2838631 -4.298656 -4.2920632 -4.2603779 -4.2125845 -4.1582069 -4.1175785 -4.1238818 -4.1638403 -4.2074337 -4.2426987 -4.2686181][-4.2401924 -4.2460489 -4.2620273 -4.2860231 -4.3012404 -4.2893372 -4.2454939 -4.1792426 -4.1014423 -4.0490046 -4.0637989 -4.1204333 -4.1796551 -4.2265954 -4.2608137][-4.2182026 -4.2333527 -4.2608123 -4.2905364 -4.3057518 -4.2884712 -4.2297015 -4.1400108 -4.0363069 -3.9767284 -4.0081921 -4.0872827 -4.1613569 -4.2141151 -4.251709][-4.1944671 -4.2249479 -4.2656169 -4.3001828 -4.3151245 -4.2937675 -4.2215552 -4.1099157 -3.9869053 -3.9299884 -3.9841442 -4.0823593 -4.1636996 -4.215364 -4.2507148][-4.1736865 -4.2220531 -4.2749991 -4.3134518 -4.3273134 -4.3015642 -4.2206912 -4.0999641 -3.9787052 -3.9377675 -4.0086751 -4.1087961 -4.1819391 -4.2236743 -4.2520518][-4.1624546 -4.2232409 -4.2832842 -4.323328 -4.3363357 -4.3085814 -4.2283649 -4.1177568 -4.021266 -4.0043039 -4.077559 -4.1593113 -4.2107286 -4.2353964 -4.2518687][-4.1699414 -4.2296557 -4.2871127 -4.3252535 -4.3376284 -4.3128958 -4.2443633 -4.1579709 -4.0960155 -4.1040478 -4.1652322 -4.2168489 -4.2418613 -4.2480664 -4.2508426][-4.1886358 -4.2383208 -4.2868576 -4.3215089 -4.3337541 -4.3171358 -4.2676911 -4.2083459 -4.1756177 -4.195796 -4.2370257 -4.2588615 -4.2602119 -4.2515955 -4.2455425][-4.2050071 -4.2424726 -4.2805762 -4.3102622 -4.324038 -4.3159475 -4.2837009 -4.2462792 -4.2326794 -4.2550521 -4.2799735 -4.2820807 -4.266572 -4.2474027 -4.2362919][-4.2145534 -4.2402391 -4.2687559 -4.2953053 -4.3130355 -4.3111486 -4.2892284 -4.2654114 -4.2615428 -4.2818007 -4.295815 -4.2875128 -4.263967 -4.2392526 -4.224226][-4.2153482 -4.2323527 -4.2565827 -4.2823648 -4.3027258 -4.3015137 -4.283474 -4.2683544 -4.2691579 -4.2862034 -4.2938023 -4.2813139 -4.2550554 -4.2277732 -4.2117643]]...]
INFO - root - 2017-12-05 22:06:28.584847: step 48210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 66h:25m:58s remains)
INFO - root - 2017-12-05 22:06:37.005698: step 48220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 68h:53m:44s remains)
INFO - root - 2017-12-05 22:06:45.466875: step 48230, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.859 sec/batch; 67h:48m:19s remains)
INFO - root - 2017-12-05 22:06:53.828850: step 48240, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.818 sec/batch; 64h:35m:03s remains)
INFO - root - 2017-12-05 22:07:02.211223: step 48250, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 66h:22m:59s remains)
INFO - root - 2017-12-05 22:07:10.703818: step 48260, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 66h:47m:28s remains)
INFO - root - 2017-12-05 22:07:19.175232: step 48270, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 68h:18m:20s remains)
INFO - root - 2017-12-05 22:07:27.680072: step 48280, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 68h:42m:17s remains)
INFO - root - 2017-12-05 22:07:36.101259: step 48290, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 67h:18m:37s remains)
INFO - root - 2017-12-05 22:07:44.523497: step 48300, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 68h:26m:53s remains)
2017-12-05 22:07:45.341532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.307435 -4.3122673 -4.3134718 -4.3065081 -4.297893 -4.2958403 -4.3022623 -4.3145089 -4.3256388 -4.32612 -4.31833 -4.3142395 -4.3148775 -4.3164206 -4.3141975][-4.3060274 -4.3078012 -4.3079882 -4.3060689 -4.3026891 -4.3001347 -4.3017712 -4.3080573 -4.3150034 -4.31197 -4.3015313 -4.2951889 -4.2952685 -4.2982874 -4.3002][-4.2898469 -4.2960768 -4.2969332 -4.2971139 -4.2923808 -4.2830453 -4.2739468 -4.2705288 -4.2741671 -4.2755008 -4.2710714 -4.2699938 -4.2756577 -4.2848959 -4.2921162][-4.2679439 -4.2846994 -4.2911172 -4.2924037 -4.2805386 -4.25388 -4.2239628 -4.2075291 -4.2137332 -4.2273593 -4.2369828 -4.2469354 -4.2625136 -4.277689 -4.2868719][-4.24224 -4.270051 -4.2823429 -4.280344 -4.2538042 -4.2015743 -4.1421981 -4.1108065 -4.1246634 -4.1616116 -4.19518 -4.2234573 -4.2513285 -4.2722344 -4.2819171][-4.2125158 -4.2489281 -4.2608924 -4.250803 -4.2055798 -4.1159692 -4.0098228 -3.9596946 -3.9950402 -4.0730953 -4.1460814 -4.20131 -4.2431283 -4.2681684 -4.2778645][-4.1888828 -4.2253518 -4.2306428 -4.210381 -4.1480308 -4.0193887 -3.8592334 -3.7918668 -3.8691282 -4.0001903 -4.1076665 -4.1811838 -4.2301731 -4.2580829 -4.2689605][-4.1791034 -4.2017846 -4.1937795 -4.1714759 -4.108356 -3.9709144 -3.7997596 -3.7429614 -3.8537683 -4.0035558 -4.1104507 -4.1773891 -4.21985 -4.2453036 -4.257298][-4.1908631 -4.193294 -4.17732 -4.1667037 -4.1297398 -4.0368452 -3.9254527 -3.8979597 -3.9815292 -4.0877266 -4.1598597 -4.2028189 -4.2300878 -4.2488465 -4.2592468][-4.1996813 -4.1910143 -4.1766672 -4.1836286 -4.1830468 -4.1509805 -4.1054869 -4.0940647 -4.1337314 -4.1857181 -4.2223368 -4.2455535 -4.2609868 -4.2724781 -4.2823167][-4.2026587 -4.1945553 -4.1880212 -4.2078123 -4.229816 -4.2333879 -4.2229633 -4.2198787 -4.23256 -4.2468219 -4.2588639 -4.2736573 -4.2874503 -4.2989173 -4.3089771][-4.1979027 -4.1939554 -4.1936579 -4.2145929 -4.2419639 -4.25745 -4.2610564 -4.2637448 -4.2662077 -4.2614655 -4.2611032 -4.2739282 -4.2899146 -4.3028173 -4.3106766][-4.1897297 -4.188189 -4.1894274 -4.2049303 -4.2262764 -4.242949 -4.2542939 -4.260962 -4.2598782 -4.2458458 -4.2385311 -4.2512226 -4.2692242 -4.2811074 -4.2848053][-4.1948171 -4.1919136 -4.1893296 -4.1950932 -4.2054672 -4.2193413 -4.2348967 -4.2446871 -4.2384481 -4.2154145 -4.2022004 -4.2147074 -4.234406 -4.2459264 -4.2478609][-4.2267871 -4.2226949 -4.2153473 -4.2083616 -4.2019062 -4.2055116 -4.2184262 -4.2316661 -4.2282867 -4.2076263 -4.1948204 -4.2026067 -4.2162943 -4.2237844 -4.2227678]]...]
INFO - root - 2017-12-05 22:07:53.822486: step 48310, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 66h:31m:45s remains)
INFO - root - 2017-12-05 22:08:02.349074: step 48320, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 67h:08m:26s remains)
INFO - root - 2017-12-05 22:08:10.868848: step 48330, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 64h:44m:27s remains)
INFO - root - 2017-12-05 22:08:19.281546: step 48340, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 64h:19m:56s remains)
INFO - root - 2017-12-05 22:08:27.685343: step 48350, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 66h:30m:27s remains)
INFO - root - 2017-12-05 22:08:36.147438: step 48360, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 66h:26m:00s remains)
INFO - root - 2017-12-05 22:08:44.675953: step 48370, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 66h:56m:52s remains)
INFO - root - 2017-12-05 22:08:53.116395: step 48380, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.817 sec/batch; 64h:27m:52s remains)
INFO - root - 2017-12-05 22:09:01.738777: step 48390, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 68h:59m:19s remains)
INFO - root - 2017-12-05 22:09:10.139671: step 48400, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 64h:14m:04s remains)
2017-12-05 22:09:10.874124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2697315 -4.2770085 -4.2919083 -4.3018703 -4.2999692 -4.2907786 -4.2716112 -4.2487464 -4.2166581 -4.1717577 -4.1248198 -4.1010532 -4.0979571 -4.0959749 -4.0987554][-4.2735496 -4.2767782 -4.2850456 -4.2864847 -4.2789621 -4.2681217 -4.2528667 -4.2379413 -4.2123857 -4.1705465 -4.1274738 -4.1093788 -4.1120729 -4.1097746 -4.1086273][-4.2568121 -4.25888 -4.2604008 -4.2515044 -4.2380443 -4.2273073 -4.2177072 -4.2156568 -4.2071996 -4.1823339 -4.1512256 -4.1385942 -4.1400714 -4.1330495 -4.1269031][-4.2363296 -4.2336426 -4.2272625 -4.2093759 -4.18948 -4.1751657 -4.16635 -4.1758938 -4.1928225 -4.1952729 -4.1811414 -4.17353 -4.1748037 -4.1690407 -4.1633482][-4.2217007 -4.2121572 -4.1991014 -4.1732082 -4.1462808 -4.1223526 -4.1020083 -4.1134934 -4.1556439 -4.1899505 -4.1984286 -4.1991558 -4.2052612 -4.2072792 -4.2073464][-4.196629 -4.1822782 -4.1691885 -4.1428123 -4.1109447 -4.0744562 -4.0372534 -4.0408034 -4.09912 -4.1615663 -4.1976557 -4.2134404 -4.2259383 -4.2331161 -4.2386866][-4.1806564 -4.1638751 -4.1547179 -4.1378407 -4.1068759 -4.05957 -4.0008349 -3.9829209 -4.039206 -4.1219459 -4.1831808 -4.2153211 -4.2349658 -4.246273 -4.2575374][-4.1772351 -4.15987 -4.1586647 -4.1584415 -4.1356106 -4.0845046 -4.0100908 -3.9642928 -3.9960628 -4.0778112 -4.1529431 -4.1980648 -4.2230344 -4.2370143 -4.2500982][-4.1847563 -4.1669445 -4.1745515 -4.1910419 -4.1836658 -4.1385403 -4.0650682 -4.0056534 -4.0067606 -4.0676875 -4.1411672 -4.192019 -4.2176785 -4.2258067 -4.2305527][-4.1905193 -4.1724019 -4.1842437 -4.2113457 -4.2191749 -4.1904497 -4.1354671 -4.08465 -4.0701542 -4.106688 -4.1650844 -4.2084975 -4.227284 -4.2205358 -4.2086344][-4.2021127 -4.18402 -4.1919589 -4.2180014 -4.2332277 -4.2226772 -4.1941991 -4.1640682 -4.1512184 -4.170073 -4.2073812 -4.2351189 -4.2409363 -4.2180657 -4.1895742][-4.2287335 -4.2135944 -4.2144136 -4.2288451 -4.2423511 -4.2448444 -4.2377462 -4.2261825 -4.2192669 -4.2281 -4.2480087 -4.2628489 -4.2591195 -4.2280054 -4.1914663][-4.2540293 -4.2448907 -4.2411609 -4.2445107 -4.2529416 -4.2606049 -4.2649908 -4.264452 -4.2622161 -4.265 -4.2722812 -4.2787342 -4.2702837 -4.2372017 -4.2001395][-4.2669158 -4.2640719 -4.2600694 -4.2581744 -4.2622671 -4.2703137 -4.2774563 -4.2806158 -4.2806334 -4.2808652 -4.2817435 -4.2831049 -4.2736859 -4.2434783 -4.211091][-4.2711883 -4.272027 -4.2703714 -4.2683206 -4.2698054 -4.2748475 -4.2795062 -4.2822948 -4.283289 -4.2829967 -4.28121 -4.2806644 -4.2727628 -4.246738 -4.2197742]]...]
INFO - root - 2017-12-05 22:09:19.470999: step 48410, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 68h:01m:07s remains)
INFO - root - 2017-12-05 22:09:27.839228: step 48420, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:27m:51s remains)
INFO - root - 2017-12-05 22:09:36.356464: step 48430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:46m:09s remains)
INFO - root - 2017-12-05 22:09:44.834810: step 48440, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 66h:03m:38s remains)
INFO - root - 2017-12-05 22:09:53.295069: step 48450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 68h:02m:33s remains)
INFO - root - 2017-12-05 22:10:01.922210: step 48460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 67h:31m:40s remains)
INFO - root - 2017-12-05 22:10:10.509158: step 48470, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 66h:36m:38s remains)
INFO - root - 2017-12-05 22:10:18.826542: step 48480, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 65h:48m:17s remains)
INFO - root - 2017-12-05 22:10:27.340200: step 48490, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 68h:00m:48s remains)
INFO - root - 2017-12-05 22:10:35.897190: step 48500, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 68h:43m:23s remains)
2017-12-05 22:10:36.767101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0643883 -4.1051903 -4.1127057 -4.0967631 -4.0854158 -4.0774441 -4.0821724 -4.1017752 -4.134439 -4.1634564 -4.1769443 -4.1699915 -4.1448975 -4.1194692 -4.1013985][-4.076746 -4.1041164 -4.103754 -4.092061 -4.0865431 -4.0804462 -4.0844884 -4.103116 -4.1360149 -4.1636424 -4.1728063 -4.1607184 -4.1319575 -4.103548 -4.0880179][-4.1029992 -4.1167564 -4.1110649 -4.1054316 -4.0999126 -4.0921555 -4.0918956 -4.1081281 -4.1372914 -4.1627946 -4.1747603 -4.1643295 -4.1300249 -4.0989141 -4.0844417][-4.1370878 -4.1368871 -4.1251545 -4.1107936 -4.0951552 -4.0806084 -4.0694866 -4.0806594 -4.1140194 -4.1448431 -4.1666675 -4.1660595 -4.1364946 -4.1030121 -4.0892558][-4.1641712 -4.1516519 -4.1279869 -4.09344 -4.054955 -4.0221138 -3.9985404 -4.022922 -4.0719409 -4.1166873 -4.1485877 -4.1634312 -4.1458454 -4.1186914 -4.1077695][-4.1784782 -4.161469 -4.1274934 -4.0642729 -3.9959178 -3.9318259 -3.8942966 -3.9391985 -4.0221815 -4.0936317 -4.1381912 -4.1635647 -4.1590157 -4.1413937 -4.1301894][-4.1874766 -4.1634927 -4.1060095 -4.0064354 -3.8975 -3.7975821 -3.7500257 -3.8263781 -3.9599161 -4.0650773 -4.1305251 -4.1716046 -4.1809187 -4.1695037 -4.1549692][-4.1954446 -4.1669493 -4.0957837 -3.9742639 -3.836585 -3.717639 -3.6815176 -3.788676 -3.9452996 -4.0599923 -4.1252413 -4.175137 -4.1975217 -4.1928797 -4.1743183][-4.1959014 -4.1728978 -4.1094913 -4.0027628 -3.8703756 -3.7552021 -3.7338097 -3.8472991 -3.9875259 -4.07946 -4.1320667 -4.1822662 -4.2084942 -4.203187 -4.1826324][-4.1863766 -4.1743536 -4.132504 -4.0613394 -3.9629593 -3.8661227 -3.8454509 -3.9355164 -4.0375333 -4.0994282 -4.1389036 -4.184298 -4.2057719 -4.197041 -4.17882][-4.1788807 -4.1825213 -4.1672525 -4.1230173 -4.0602374 -3.9964592 -3.9796984 -4.0346861 -4.0849261 -4.1152625 -4.1460891 -4.1812139 -4.1965995 -4.1906872 -4.1794128][-4.1657753 -4.1819897 -4.1798339 -4.1500535 -4.1123486 -4.0716753 -4.0593762 -4.0859938 -4.1006293 -4.1144753 -4.1394649 -4.1668744 -4.1811852 -4.1852841 -4.1814241][-4.1502337 -4.1776137 -4.1790142 -4.1575685 -4.1361866 -4.1128845 -4.10129 -4.1097355 -4.11933 -4.1354961 -4.1510773 -4.1629815 -4.1696506 -4.1806545 -4.181911][-4.1269832 -4.165607 -4.17247 -4.1563144 -4.1456347 -4.1361694 -4.1254921 -4.12646 -4.1442595 -4.1628814 -4.1660242 -4.162025 -4.1611781 -4.171237 -4.1705151][-4.094492 -4.1371717 -4.1524558 -4.1478562 -4.1482325 -4.1500425 -4.1438642 -4.1440387 -4.159863 -4.1771502 -4.1678514 -4.1511769 -4.1419325 -4.1464396 -4.143218]]...]
INFO - root - 2017-12-05 22:10:45.153373: step 48510, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 65h:50m:17s remains)
INFO - root - 2017-12-05 22:10:53.731163: step 48520, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 67h:55m:57s remains)
INFO - root - 2017-12-05 22:11:02.225355: step 48530, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 70h:00m:13s remains)
INFO - root - 2017-12-05 22:11:10.681487: step 48540, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 66h:20m:17s remains)
INFO - root - 2017-12-05 22:11:19.149384: step 48550, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 69h:00m:01s remains)
INFO - root - 2017-12-05 22:11:27.584143: step 48560, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 65h:36m:12s remains)
INFO - root - 2017-12-05 22:11:36.020507: step 48570, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.840 sec/batch; 66h:13m:07s remains)
INFO - root - 2017-12-05 22:11:44.491086: step 48580, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:52m:06s remains)
INFO - root - 2017-12-05 22:11:52.876822: step 48590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 66h:16m:05s remains)
INFO - root - 2017-12-05 22:12:01.295017: step 48600, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.807 sec/batch; 63h:37m:05s remains)
2017-12-05 22:12:02.052288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1999111 -4.2014375 -4.2100611 -4.2272167 -4.2489538 -4.265625 -4.2734928 -4.2778454 -4.2711148 -4.2530818 -4.2449703 -4.2595015 -4.2762156 -4.28052 -4.27519][-4.2089019 -4.2161875 -4.2312279 -4.2516146 -4.268744 -4.2749057 -4.2723823 -4.2641773 -4.2490172 -4.234374 -4.2381768 -4.2551212 -4.2667475 -4.2698307 -4.2645187][-4.2288713 -4.2413425 -4.2566156 -4.2717938 -4.2821622 -4.2790813 -4.2683153 -4.253705 -4.2407455 -4.2371535 -4.2499704 -4.2596545 -4.2558088 -4.252399 -4.2500896][-4.2425747 -4.2562003 -4.267869 -4.2743773 -4.2753854 -4.2653389 -4.2536759 -4.2446084 -4.2417593 -4.249332 -4.2639809 -4.2640977 -4.2515469 -4.2430253 -4.2441878][-4.2466931 -4.2578678 -4.2614779 -4.2573643 -4.2437153 -4.2215428 -4.2068081 -4.2076674 -4.2238135 -4.2485671 -4.26883 -4.2693377 -4.2563848 -4.2447414 -4.2425675][-4.2472515 -4.2539206 -4.2457752 -4.223547 -4.1840014 -4.1370149 -4.1119604 -4.1257639 -4.17017 -4.2204142 -4.25445 -4.2683225 -4.2643323 -4.2528682 -4.2454619][-4.2592459 -4.2562141 -4.2331657 -4.188828 -4.1167006 -4.0374441 -3.9977152 -4.0220561 -4.0937319 -4.1711669 -4.2220354 -4.24858 -4.2540045 -4.2461524 -4.2355609][-4.2798223 -4.2707548 -4.2374387 -4.1768727 -4.0814586 -3.9812322 -3.9303823 -3.9551971 -4.0379415 -4.1299696 -4.1916928 -4.2248082 -4.2329407 -4.2276874 -4.2160311][-4.2875252 -4.2780833 -4.2490911 -4.1934762 -4.1058731 -4.02103 -3.9819062 -4.0014324 -4.06767 -4.14054 -4.1905375 -4.215857 -4.2192454 -4.2171717 -4.2096572][-4.2790532 -4.2738414 -4.2551336 -4.2177095 -4.1568842 -4.1026692 -4.0835619 -4.0991545 -4.14575 -4.1930838 -4.2200766 -4.2290182 -4.2235379 -4.2211752 -4.2162633][-4.2594905 -4.2622461 -4.2578235 -4.2374492 -4.2038054 -4.1770978 -4.16994 -4.1806827 -4.2130384 -4.2452602 -4.260767 -4.2588739 -4.2435017 -4.2332888 -4.2231584][-4.2396674 -4.247704 -4.2520866 -4.2424092 -4.2290449 -4.224247 -4.2265124 -4.2325664 -4.2553182 -4.2791386 -4.2890105 -4.2842407 -4.2653632 -4.2474184 -4.2284784][-4.2224531 -4.2293372 -4.2318487 -4.2281461 -4.2284665 -4.239759 -4.2515745 -4.2603035 -4.2767782 -4.2912555 -4.2960072 -4.2880883 -4.2732005 -4.2584729 -4.2372][-4.2048464 -4.2070312 -4.2077069 -4.2102313 -4.2185769 -4.2362432 -4.2536178 -4.2665148 -4.2802873 -4.2854309 -4.2849965 -4.2780204 -4.2735004 -4.2711577 -4.258585][-4.199122 -4.19947 -4.1965919 -4.2004638 -4.2117033 -4.2263412 -4.2395806 -4.2538004 -4.2631726 -4.2631006 -4.2625713 -4.2639875 -4.2736 -4.2832251 -4.2820373]]...]
INFO - root - 2017-12-05 22:12:10.698780: step 48610, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 68h:41m:41s remains)
INFO - root - 2017-12-05 22:12:19.108287: step 48620, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 66h:33m:03s remains)
INFO - root - 2017-12-05 22:12:27.611951: step 48630, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 67h:20m:04s remains)
INFO - root - 2017-12-05 22:12:36.175624: step 48640, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.867 sec/batch; 68h:22m:03s remains)
INFO - root - 2017-12-05 22:12:44.631567: step 48650, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 68h:20m:37s remains)
INFO - root - 2017-12-05 22:12:53.147778: step 48660, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 65h:23m:41s remains)
INFO - root - 2017-12-05 22:13:01.655303: step 48670, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 67h:22m:48s remains)
INFO - root - 2017-12-05 22:13:10.123528: step 48680, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:15m:44s remains)
INFO - root - 2017-12-05 22:13:18.833084: step 48690, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 67h:28m:45s remains)
INFO - root - 2017-12-05 22:13:27.274679: step 48700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 66h:13m:27s remains)
2017-12-05 22:13:28.073434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1461058 -4.1690021 -4.1974907 -4.1939683 -4.1673384 -4.1531539 -4.1685414 -4.1863561 -4.1920862 -4.1946392 -4.1987581 -4.2031264 -4.2000937 -4.1873465 -4.1748686][-4.1308155 -4.1518393 -4.182466 -4.1873965 -4.1656456 -4.1462383 -4.15382 -4.17033 -4.1784592 -4.1801138 -4.1808715 -4.1857004 -4.18754 -4.1825171 -4.1821513][-4.1104178 -4.1294913 -4.1578836 -4.1729383 -4.1625462 -4.1390839 -4.1386919 -4.1544805 -4.1667137 -4.1684465 -4.1646824 -4.1708012 -4.17954 -4.1821256 -4.1907983][-4.0996757 -4.11678 -4.1364145 -4.1487875 -4.14271 -4.1197085 -4.1179638 -4.136323 -4.1542487 -4.160459 -4.1577797 -4.1675816 -4.18223 -4.1871495 -4.1949177][-4.0941358 -4.11191 -4.1221609 -4.1250772 -4.1194072 -4.1003795 -4.1013355 -4.1234837 -4.1457052 -4.1556292 -4.1607809 -4.175272 -4.1891937 -4.1891732 -4.1864786][-4.0845642 -4.1089597 -4.1175241 -4.1143746 -4.1070166 -4.0901537 -4.08504 -4.0983539 -4.1185384 -4.1363273 -4.1541357 -4.1749592 -4.1890225 -4.1847162 -4.1781907][-4.0739388 -4.1068754 -4.1186423 -4.113277 -4.1030345 -4.0821104 -4.0598378 -4.0535111 -4.0711837 -4.1025195 -4.1401758 -4.1704726 -4.1861091 -4.1814356 -4.1773844][-4.0719357 -4.1074653 -4.1237068 -4.1218367 -4.109695 -4.0813909 -4.0426254 -4.0194221 -4.0347438 -4.0755858 -4.1272268 -4.16146 -4.1726131 -4.1657891 -4.1651473][-4.0940862 -4.127841 -4.1432157 -4.1446714 -4.135232 -4.1084828 -4.0653019 -4.0312204 -4.0392432 -4.0764832 -4.1259112 -4.1553516 -4.1562228 -4.145 -4.148315][-4.1248784 -4.1576896 -4.1721659 -4.1749129 -4.1661334 -4.1399889 -4.0990353 -4.0635352 -4.0681152 -4.1005807 -4.140769 -4.160975 -4.1533327 -4.142859 -4.1519685][-4.1473379 -4.1783085 -4.1922622 -4.1953368 -4.1859436 -4.1589289 -4.1194944 -4.0863595 -4.0930634 -4.1257157 -4.1614203 -4.1748643 -4.1673179 -4.1625953 -4.176734][-4.1626086 -4.1872773 -4.2008204 -4.2041764 -4.1948223 -4.1696134 -4.1345348 -4.1052089 -4.1130056 -4.1466722 -4.182673 -4.1970921 -4.1926131 -4.1903338 -4.2018867][-4.1843452 -4.2038856 -4.2162848 -4.2196822 -4.2111964 -4.1913533 -4.1642027 -4.1384368 -4.1400423 -4.166985 -4.1977491 -4.2117925 -4.2068019 -4.2026386 -4.2075095][-4.2053423 -4.2173457 -4.2268748 -4.2318907 -4.2273064 -4.2124519 -4.1915841 -4.1671953 -4.1604228 -4.1779952 -4.2004 -4.2109294 -4.2058616 -4.2024841 -4.20278][-4.2052279 -4.210392 -4.2169132 -4.224308 -4.2254019 -4.2147741 -4.1966095 -4.1721282 -4.1603727 -4.1699948 -4.1873631 -4.1989012 -4.1991315 -4.1971025 -4.1918125]]...]
INFO - root - 2017-12-05 22:13:36.538978: step 48710, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.868 sec/batch; 68h:26m:22s remains)
INFO - root - 2017-12-05 22:13:45.092070: step 48720, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 67h:19m:53s remains)
INFO - root - 2017-12-05 22:13:53.518766: step 48730, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 66h:54m:26s remains)
INFO - root - 2017-12-05 22:14:02.062585: step 48740, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 65h:41m:00s remains)
INFO - root - 2017-12-05 22:14:10.592606: step 48750, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 68h:21m:48s remains)
INFO - root - 2017-12-05 22:14:19.171971: step 48760, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 68h:09m:33s remains)
INFO - root - 2017-12-05 22:14:27.643958: step 48770, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 67h:34m:55s remains)
INFO - root - 2017-12-05 22:14:36.171232: step 48780, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 67h:11m:52s remains)
INFO - root - 2017-12-05 22:14:44.695729: step 48790, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 66h:55m:14s remains)
INFO - root - 2017-12-05 22:14:53.095476: step 48800, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 65h:36m:00s remains)
2017-12-05 22:14:53.883542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1597438 -4.1059456 -4.0510964 -4.0027943 -3.9890032 -4.0293517 -4.1047897 -4.1832204 -4.2459431 -4.2870345 -4.3099408 -4.321465 -4.3250055 -4.3243227 -4.3224144][-4.1670341 -4.1407351 -4.1167026 -4.0842433 -4.0575752 -4.0671062 -4.1136136 -4.1768703 -4.2378092 -4.2823825 -4.3092647 -4.32226 -4.3267331 -4.3262062 -4.3233628][-4.1701651 -4.1687531 -4.170105 -4.1564531 -4.1353621 -4.1293468 -4.1491318 -4.1913667 -4.2407951 -4.2796464 -4.3055959 -4.3204689 -4.3268638 -4.328063 -4.32608][-4.1734014 -4.1869836 -4.20067 -4.1992226 -4.1843729 -4.1694579 -4.1691694 -4.1952777 -4.2357039 -4.2718592 -4.2969155 -4.3135958 -4.32384 -4.3290544 -4.3295279][-4.1834593 -4.1993389 -4.2117634 -4.2091165 -4.190022 -4.1646194 -4.1506753 -4.1648331 -4.2001028 -4.2399931 -4.2734804 -4.2985244 -4.3154445 -4.3258443 -4.330236][-4.1992669 -4.2055945 -4.206356 -4.1897154 -4.1546988 -4.1152644 -4.0910459 -4.0985274 -4.1375456 -4.1877093 -4.2365813 -4.273993 -4.2994528 -4.3166108 -4.3264828][-4.2146125 -4.2106686 -4.2012668 -4.1692786 -4.1111422 -4.0495176 -4.0092783 -4.0095453 -4.0529318 -4.118432 -4.1881528 -4.2413745 -4.2757883 -4.3008289 -4.3182364][-4.2322416 -4.2199874 -4.2066908 -4.1693878 -4.1048727 -4.0341306 -3.9757237 -3.9558291 -3.9878066 -4.0558438 -4.1362972 -4.2013173 -4.2454386 -4.2794142 -4.3052998][-4.2472291 -4.2314878 -4.2198558 -4.1900663 -4.141109 -4.086729 -4.038249 -4.0025816 -3.9969394 -4.03279 -4.097033 -4.1614423 -4.2119112 -4.2555871 -4.2886324][-4.2552819 -4.2413845 -4.2314587 -4.2091227 -4.1762524 -4.1442752 -4.1155529 -4.085959 -4.0655537 -4.0704165 -4.1057191 -4.1526332 -4.1970434 -4.2394938 -4.2729368][-4.2569718 -4.244483 -4.2329865 -4.2125778 -4.1884212 -4.1765943 -4.1699915 -4.1574545 -4.1422567 -4.1389728 -4.1560559 -4.1826472 -4.2114906 -4.2422156 -4.266922][-4.255971 -4.2420769 -4.2264705 -4.2029033 -4.1844287 -4.186306 -4.1986494 -4.2068949 -4.2063389 -4.2058287 -4.2145944 -4.2292185 -4.2430882 -4.2586136 -4.2706995][-4.2600756 -4.2432537 -4.2240357 -4.2013702 -4.1881576 -4.1946187 -4.2164078 -4.2384524 -4.2503371 -4.2557359 -4.26102 -4.26785 -4.2711067 -4.2751188 -4.2777796][-4.2728963 -4.2548838 -4.2345829 -4.2139139 -4.2036715 -4.2089791 -4.2301888 -4.2566566 -4.2748537 -4.2824664 -4.2853022 -4.2879205 -4.2859492 -4.2831578 -4.2813258][-4.2901516 -4.2726345 -4.2524924 -4.2305088 -4.2166214 -4.2169437 -4.2331209 -4.2590318 -4.2790418 -4.2865129 -4.2899885 -4.2909231 -4.287353 -4.2822852 -4.2797494]]...]
INFO - root - 2017-12-05 22:15:02.540186: step 48810, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 66h:46m:37s remains)
INFO - root - 2017-12-05 22:15:11.057496: step 48820, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 65h:41m:21s remains)
INFO - root - 2017-12-05 22:15:19.575794: step 48830, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 68h:52m:29s remains)
INFO - root - 2017-12-05 22:15:28.082303: step 48840, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 69h:14m:15s remains)
INFO - root - 2017-12-05 22:15:36.568571: step 48850, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 68h:35m:05s remains)
INFO - root - 2017-12-05 22:15:45.126223: step 48860, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 67h:25m:39s remains)
INFO - root - 2017-12-05 22:15:53.790017: step 48870, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 66h:54m:25s remains)
INFO - root - 2017-12-05 22:16:02.313776: step 48880, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 68h:23m:52s remains)
INFO - root - 2017-12-05 22:16:10.768386: step 48890, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 67h:05m:46s remains)
INFO - root - 2017-12-05 22:16:19.347748: step 48900, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 68h:26m:34s remains)
2017-12-05 22:16:20.130916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1694641 -4.1715736 -4.2035203 -4.2436409 -4.2658477 -4.274075 -4.2743392 -4.2646484 -4.2434525 -4.219089 -4.20055 -4.1831255 -4.1779318 -4.1958532 -4.2085247][-4.1692896 -4.175241 -4.2046862 -4.2413707 -4.2617974 -4.2677808 -4.271771 -4.2708774 -4.2614322 -4.249053 -4.2382407 -4.2255311 -4.2197652 -4.2247534 -4.2224517][-4.1640267 -4.1728768 -4.1964226 -4.2236772 -4.2377081 -4.2409997 -4.2460175 -4.2503114 -4.2538733 -4.2585125 -4.260005 -4.2548037 -4.248651 -4.2434893 -4.2277532][-4.1763778 -4.181921 -4.1954513 -4.2082348 -4.2119303 -4.2073631 -4.205699 -4.20758 -4.2187648 -4.2413459 -4.2593417 -4.2649021 -4.2628818 -4.2550507 -4.2331514][-4.1970963 -4.1977081 -4.1971335 -4.1940837 -4.1826949 -4.16154 -4.1406007 -4.1269269 -4.1382589 -4.1784635 -4.2179494 -4.2415128 -4.2522521 -4.2513847 -4.2332263][-4.2096062 -4.2098703 -4.1974697 -4.1772776 -4.1457572 -4.1002021 -4.0494165 -4.01088 -4.0205693 -4.083992 -4.1505442 -4.1952629 -4.219399 -4.225399 -4.2110343][-4.2234192 -4.2258239 -4.2041097 -4.1668143 -4.1167059 -4.0489855 -3.9684386 -3.9026694 -3.9099166 -3.9964554 -4.08965 -4.1549883 -4.1908722 -4.20287 -4.1957951][-4.2397861 -4.2469864 -4.2224083 -4.1778388 -4.1219797 -4.0477057 -3.9528255 -3.8725679 -3.8770452 -3.9696922 -4.0718451 -4.1458607 -4.1867895 -4.1990585 -4.1955385][-4.2610478 -4.2675791 -4.2428136 -4.2015624 -4.1527658 -4.0861721 -3.9987545 -3.9291592 -3.9351473 -4.0125933 -4.1002855 -4.165741 -4.2002521 -4.2065916 -4.201767][-4.27349 -4.2711959 -4.2448516 -4.2100081 -4.1738906 -4.1235862 -4.0580182 -4.0127411 -4.0251045 -4.0834007 -4.14887 -4.1991334 -4.22475 -4.2263923 -4.2206368][-4.2821331 -4.2695079 -4.2412796 -4.2137814 -4.1937594 -4.1635427 -4.12524 -4.1054955 -4.1224575 -4.1633372 -4.2049479 -4.2366743 -4.2523131 -4.250567 -4.243206][-4.28697 -4.263936 -4.2297945 -4.2061687 -4.1997037 -4.1898665 -4.1769471 -4.1783872 -4.19649 -4.2199359 -4.2378187 -4.2502747 -4.2558775 -4.2535372 -4.2479448][-4.27119 -4.2467852 -4.2136354 -4.1953907 -4.2004852 -4.2070389 -4.2120671 -4.2255154 -4.2424183 -4.2508335 -4.2472267 -4.2399507 -4.2349052 -4.2334566 -4.2311654][-4.2474651 -4.232111 -4.2097435 -4.2000289 -4.2119641 -4.2261147 -4.2392163 -4.2554431 -4.2669253 -4.2621212 -4.2405338 -4.2159576 -4.2018228 -4.201272 -4.2029982][-4.2315989 -4.2287135 -4.2189703 -4.2167339 -4.2305679 -4.2461338 -4.2601752 -4.2726116 -4.2757916 -4.2604632 -4.22889 -4.1965666 -4.1808424 -4.1846938 -4.1921797]]...]
INFO - root - 2017-12-05 22:16:28.519037: step 48910, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 67h:28m:17s remains)
INFO - root - 2017-12-05 22:16:37.036937: step 48920, loss = 2.10, batch loss = 2.05 (9.1 examples/sec; 0.874 sec/batch; 68h:52m:32s remains)
INFO - root - 2017-12-05 22:16:45.642944: step 48930, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 68h:01m:10s remains)
INFO - root - 2017-12-05 22:16:54.146611: step 48940, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 65h:15m:04s remains)
INFO - root - 2017-12-05 22:17:02.545038: step 48950, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 68h:19m:29s remains)
INFO - root - 2017-12-05 22:17:11.119068: step 48960, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 68h:24m:25s remains)
INFO - root - 2017-12-05 22:17:19.671798: step 48970, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 68h:00m:06s remains)
INFO - root - 2017-12-05 22:17:28.196161: step 48980, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 68h:16m:59s remains)
INFO - root - 2017-12-05 22:17:36.708316: step 48990, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 68h:41m:06s remains)
INFO - root - 2017-12-05 22:17:45.442395: step 49000, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 67h:08m:34s remains)
2017-12-05 22:17:46.375528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2087626 -4.2034082 -4.1985598 -4.1914644 -4.1722789 -4.1498003 -4.1437025 -4.1568789 -4.1834054 -4.2018886 -4.2120066 -4.2142639 -4.2131672 -4.2218094 -4.2294016][-4.2039289 -4.1966357 -4.1929951 -4.1904655 -4.1765566 -4.15467 -4.1343737 -4.1337051 -4.1626468 -4.1891322 -4.2080054 -4.212111 -4.2091265 -4.2127924 -4.2165103][-4.1970677 -4.1884279 -4.184885 -4.1869473 -4.1837969 -4.1670885 -4.1362472 -4.11969 -4.1459665 -4.1781163 -4.2025485 -4.2067804 -4.1990385 -4.1974044 -4.197124][-4.1889176 -4.17796 -4.1719985 -4.1770787 -4.1821003 -4.1722455 -4.1360765 -4.1033988 -4.1251316 -4.1609693 -4.1900387 -4.2016149 -4.1971464 -4.1987967 -4.1983328][-4.182909 -4.1682839 -4.15498 -4.1554556 -4.161056 -4.1537952 -4.1171446 -4.0753279 -4.0917068 -4.1385684 -4.1823993 -4.2077522 -4.2080584 -4.2066345 -4.2035651][-4.1793861 -4.160109 -4.1410365 -4.1314793 -4.1327677 -4.1257486 -4.0861158 -4.03192 -4.036972 -4.1016273 -4.166688 -4.2072735 -4.2095237 -4.2029791 -4.194334][-4.1773787 -4.1583505 -4.1396809 -4.1248803 -4.118257 -4.1023426 -4.0486484 -3.9649835 -3.9382443 -4.026341 -4.1235461 -4.1850362 -4.2008381 -4.1999636 -4.1941223][-4.1879735 -4.1756554 -4.1627502 -4.1442575 -4.1295352 -4.1013336 -4.0347991 -3.9293826 -3.8705497 -3.9757612 -4.0988059 -4.1768775 -4.2073054 -4.2164025 -4.21512][-4.2052193 -4.1996932 -4.1911592 -4.1743531 -4.1623254 -4.13818 -4.0850892 -4.0022197 -3.95054 -4.025702 -4.1257133 -4.1885023 -4.2172027 -4.2304225 -4.2267513][-4.2172461 -4.2179437 -4.2147231 -4.2040553 -4.1916647 -4.1741171 -4.1423903 -4.0977097 -4.0653205 -4.0984778 -4.1529651 -4.18963 -4.2136993 -4.2277894 -4.2261496][-4.246202 -4.2481208 -4.241662 -4.2243371 -4.2015953 -4.1821513 -4.1677055 -4.15493 -4.1411724 -4.1496964 -4.1732235 -4.196137 -4.2160521 -4.2319937 -4.2397513][-4.2871771 -4.2854409 -4.2708297 -4.2426357 -4.2076545 -4.1845417 -4.18543 -4.1988478 -4.2020111 -4.2011704 -4.20786 -4.2223535 -4.2378497 -4.251164 -4.2616625][-4.3161583 -4.3109913 -4.2947068 -4.26479 -4.2278967 -4.2076125 -4.2148318 -4.2393031 -4.2527943 -4.2537766 -4.252892 -4.2537751 -4.2591772 -4.2656031 -4.2738972][-4.3186493 -4.3153834 -4.3032537 -4.2779164 -4.2495265 -4.2356324 -4.2432213 -4.2653041 -4.2793317 -4.279357 -4.2744985 -4.2714629 -4.2716088 -4.27269 -4.2792554][-4.3150792 -4.3128366 -4.303381 -4.2835279 -4.2651978 -4.25699 -4.2617712 -4.2785945 -4.2873116 -4.2832685 -4.2795792 -4.2792797 -4.2805839 -4.283442 -4.2905126]]...]
INFO - root - 2017-12-05 22:17:54.829154: step 49010, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 69h:50m:35s remains)
INFO - root - 2017-12-05 22:18:03.367739: step 49020, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 66h:04m:47s remains)
INFO - root - 2017-12-05 22:18:11.819594: step 49030, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 65h:57m:26s remains)
INFO - root - 2017-12-05 22:18:20.225046: step 49040, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 65h:47m:55s remains)
INFO - root - 2017-12-05 22:18:28.687541: step 49050, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 66h:52m:17s remains)
INFO - root - 2017-12-05 22:18:37.218273: step 49060, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 68h:59m:24s remains)
INFO - root - 2017-12-05 22:18:45.822136: step 49070, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 69h:09m:24s remains)
INFO - root - 2017-12-05 22:18:54.402069: step 49080, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 67h:37m:09s remains)
INFO - root - 2017-12-05 22:19:02.889379: step 49090, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 69h:28m:51s remains)
INFO - root - 2017-12-05 22:19:11.388346: step 49100, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 68h:07m:51s remains)
2017-12-05 22:19:12.096105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765651 -4.277369 -4.28015 -4.2791147 -4.2756782 -4.270031 -4.2627072 -4.2547631 -4.2442727 -4.2345409 -4.2275896 -4.2289863 -4.2342811 -4.2403646 -4.2434034][-4.2648764 -4.2734261 -4.279726 -4.2787766 -4.2729483 -4.2662916 -4.2607417 -4.2573919 -4.2513089 -4.2439737 -4.2377553 -4.2385483 -4.2432332 -4.2479291 -4.2499881][-4.2523909 -4.26534 -4.2708855 -4.2668562 -4.2588606 -4.2530165 -4.2517438 -4.2557192 -4.2565327 -4.25417 -4.2487979 -4.2464409 -4.2469788 -4.2476678 -4.2463126][-4.2518911 -4.2627568 -4.2645655 -4.2557034 -4.2456923 -4.2393069 -4.2392521 -4.2467518 -4.2526016 -4.2558303 -4.2530932 -4.2489319 -4.2467537 -4.2434955 -4.2366924][-4.257031 -4.2614355 -4.2577844 -4.2436795 -4.2285624 -4.21692 -4.2129378 -4.2179694 -4.2234244 -4.2289648 -4.2303052 -4.2303257 -4.2313504 -4.2287989 -4.2210946][-4.255024 -4.25223 -4.2410569 -4.2208419 -4.1983528 -4.1777248 -4.16551 -4.1654091 -4.1693945 -4.1777253 -4.1878443 -4.1982522 -4.2073483 -4.2086806 -4.2044916][-4.2462764 -4.2338042 -4.2137389 -4.1892247 -4.1624122 -4.1367354 -4.12155 -4.1227264 -4.1326647 -4.1496506 -4.1699486 -4.1888447 -4.2004838 -4.2008052 -4.1956096][-4.2389565 -4.2210422 -4.1969438 -4.1741858 -4.1517639 -4.1313381 -4.1199412 -4.124783 -4.1398749 -4.1614485 -4.1850686 -4.2039609 -4.2116 -4.2091551 -4.2011][-4.2471485 -4.2338295 -4.2148228 -4.1983514 -4.1837234 -4.1699128 -4.1592169 -4.1605425 -4.1715579 -4.1875391 -4.2030692 -4.213695 -4.2160993 -4.2126827 -4.2056556][-4.2660937 -4.2620873 -4.2512021 -4.2412 -4.2315874 -4.2209177 -4.2096591 -4.2066994 -4.2118678 -4.219687 -4.2228136 -4.2208405 -4.215292 -4.208734 -4.2018118][-4.2780547 -4.2838926 -4.2829132 -4.2787371 -4.2731376 -4.2645049 -4.2536559 -4.2472138 -4.2479539 -4.2497993 -4.2446237 -4.2350068 -4.2237768 -4.2131066 -4.2045341][-4.2840023 -4.2968106 -4.302681 -4.3021894 -4.2985773 -4.2919984 -4.2835455 -4.2768722 -4.2753367 -4.2726574 -4.2627959 -4.2506394 -4.2386456 -4.2276278 -4.2192817][-4.2762465 -4.2915215 -4.3014936 -4.3036957 -4.3015995 -4.2981248 -4.2927179 -4.2871933 -4.2847123 -4.2798142 -4.269186 -4.2590184 -4.251256 -4.2447009 -4.2388945][-4.2546053 -4.2688031 -4.2821231 -4.2878714 -4.2883835 -4.2880559 -4.2860947 -4.2829103 -4.2815809 -4.2783117 -4.2706718 -4.2647595 -4.2622051 -4.2598615 -4.2543688][-4.2228427 -4.2362309 -4.2531352 -4.2648182 -4.2719231 -4.2764339 -4.2790418 -4.2784986 -4.2791595 -4.2790909 -4.2757072 -4.273066 -4.2729449 -4.2714291 -4.2620397]]...]
INFO - root - 2017-12-05 22:19:20.593437: step 49110, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 67h:06m:25s remains)
INFO - root - 2017-12-05 22:19:28.970431: step 49120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 67h:34m:13s remains)
INFO - root - 2017-12-05 22:19:37.437186: step 49130, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:08m:48s remains)
INFO - root - 2017-12-05 22:19:45.978862: step 49140, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 66h:55m:06s remains)
INFO - root - 2017-12-05 22:19:54.313898: step 49150, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 65h:58m:33s remains)
INFO - root - 2017-12-05 22:20:02.933705: step 49160, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 68h:28m:29s remains)
INFO - root - 2017-12-05 22:20:11.389536: step 49170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 67h:36m:48s remains)
INFO - root - 2017-12-05 22:20:20.009708: step 49180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 66h:20m:01s remains)
INFO - root - 2017-12-05 22:20:28.619962: step 49190, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:06m:52s remains)
INFO - root - 2017-12-05 22:20:37.090747: step 49200, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.866 sec/batch; 68h:06m:46s remains)
2017-12-05 22:20:37.888000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2791414 -4.2783718 -4.2735677 -4.2638021 -4.2507248 -4.2398353 -4.2382922 -4.2480588 -4.2628303 -4.2765217 -4.2853103 -4.2871428 -4.2853312 -4.2816296 -4.2777805][-4.2899866 -4.2853374 -4.2736516 -4.2544022 -4.2328286 -4.2158542 -4.2105126 -4.2219477 -4.2453661 -4.270689 -4.2873774 -4.2939377 -4.2955461 -4.2940774 -4.2919788][-4.29217 -4.2831864 -4.2632427 -4.2318263 -4.1999655 -4.17569 -4.1638732 -4.1726713 -4.2040515 -4.2409253 -4.263526 -4.2759657 -4.2853637 -4.291811 -4.2962489][-4.29393 -4.2818174 -4.2534728 -4.2103539 -4.1688142 -4.1381431 -4.1158986 -4.1147861 -4.1480565 -4.193161 -4.2192311 -4.2381682 -4.2618093 -4.2817969 -4.2969394][-4.2939067 -4.2808762 -4.2465849 -4.1952038 -4.144721 -4.1054034 -4.0651646 -4.0421605 -4.0705519 -4.1247268 -4.1573653 -4.184648 -4.2269764 -4.2650714 -4.2926493][-4.2925415 -4.2806258 -4.2459745 -4.1913266 -4.133719 -4.0828772 -4.02004 -3.962661 -3.9757018 -4.039999 -4.086729 -4.12706 -4.1876783 -4.2429109 -4.2826762][-4.291389 -4.2806048 -4.2489505 -4.19624 -4.1347733 -4.0713277 -3.9841485 -3.8846283 -3.8717208 -3.9530821 -4.0286574 -4.09201 -4.1688666 -4.2357135 -4.2819352][-4.2887707 -4.2780695 -4.2509546 -4.203043 -4.1370897 -4.0600514 -3.9544423 -3.8270631 -3.7908201 -3.8911834 -4.0004716 -4.0898957 -4.1780457 -4.2477155 -4.2902713][-4.2862897 -4.2749958 -4.2531314 -4.2148232 -4.1543212 -4.0792317 -3.9847193 -3.8772545 -3.8386941 -3.929255 -4.0431252 -4.1367116 -4.2173781 -4.2751966 -4.3046546][-4.2895017 -4.2780561 -4.2584586 -4.2267604 -4.1777735 -4.1170611 -4.0470719 -3.9737806 -3.9468994 -4.0212369 -4.1263165 -4.2124505 -4.273324 -4.3086953 -4.3195882][-4.2981243 -4.2886963 -4.2711864 -4.2423058 -4.2015963 -4.1541138 -4.1095986 -4.0696721 -4.060348 -4.1191363 -4.2058558 -4.2761421 -4.31607 -4.3300676 -4.3257728][-4.3010774 -4.2919703 -4.276927 -4.25213 -4.2193365 -4.1873174 -4.1680045 -4.160707 -4.1663947 -4.2067752 -4.2675648 -4.3160243 -4.3370023 -4.3361912 -4.323163][-4.2948318 -4.2825537 -4.2676358 -4.2483726 -4.2256122 -4.2107587 -4.2135024 -4.2249331 -4.2375789 -4.2617764 -4.2997594 -4.3299165 -4.33982 -4.3322134 -4.3170333][-4.2831192 -4.2637591 -4.2432466 -4.2243624 -4.2085047 -4.2058482 -4.2275562 -4.2551675 -4.2726245 -4.2873907 -4.3102183 -4.3285513 -4.334393 -4.3266463 -4.312922][-4.27079 -4.2411375 -4.2140322 -4.1921353 -4.1738181 -4.1714129 -4.2008276 -4.2395983 -4.2617331 -4.2745605 -4.2951436 -4.3139267 -4.3235388 -4.3210764 -4.3104119]]...]
INFO - root - 2017-12-05 22:20:46.447166: step 49210, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 64h:15m:22s remains)
INFO - root - 2017-12-05 22:20:54.885612: step 49220, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.853 sec/batch; 67h:08m:25s remains)
INFO - root - 2017-12-05 22:21:03.196891: step 49230, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 64h:29m:27s remains)
INFO - root - 2017-12-05 22:21:11.745059: step 49240, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.874 sec/batch; 68h:48m:11s remains)
INFO - root - 2017-12-05 22:21:20.200966: step 49250, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.877 sec/batch; 68h:57m:57s remains)
INFO - root - 2017-12-05 22:21:28.620790: step 49260, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 65h:52m:49s remains)
INFO - root - 2017-12-05 22:21:37.088821: step 49270, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 64h:31m:44s remains)
INFO - root - 2017-12-05 22:21:45.531289: step 49280, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.821 sec/batch; 64h:33m:39s remains)
INFO - root - 2017-12-05 22:21:53.946544: step 49290, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 64h:55m:41s remains)
INFO - root - 2017-12-05 22:22:02.469796: step 49300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 67h:15m:59s remains)
2017-12-05 22:22:03.259309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2860646 -4.2824445 -4.2777367 -4.2777128 -4.2796092 -4.2755079 -4.2691007 -4.2648778 -4.2705708 -4.2736936 -4.2679362 -4.2652745 -4.2661772 -4.2668371 -4.2688751][-4.2772532 -4.2724714 -4.2639184 -4.2590141 -4.2551761 -4.2440267 -4.23123 -4.2235947 -4.2335267 -4.238883 -4.2304487 -4.2323713 -4.2384696 -4.2391248 -4.2415361][-4.2611718 -4.2522616 -4.2381759 -4.2300148 -4.2219124 -4.2049093 -4.1847839 -4.1716785 -4.1839151 -4.191123 -4.1860662 -4.1973357 -4.2101502 -4.2092533 -4.213717][-4.2497773 -4.2410789 -4.2268219 -4.2178411 -4.207191 -4.1819348 -4.154387 -4.1336083 -4.1426544 -4.1513085 -4.15476 -4.1773996 -4.1951485 -4.193522 -4.20048][-4.2456784 -4.2402611 -4.2287927 -4.2177758 -4.2005243 -4.1640654 -4.1258712 -4.0933638 -4.0965614 -4.1098356 -4.1234474 -4.1574349 -4.1820111 -4.1831446 -4.1969471][-4.2479858 -4.2446418 -4.2313943 -4.2136831 -4.1842451 -4.134563 -4.0864072 -4.0449028 -4.0459285 -4.0628905 -4.088047 -4.1357107 -4.167757 -4.1772656 -4.1992126][-4.2529111 -4.249064 -4.2321296 -4.2059746 -4.1603584 -4.095727 -4.0375104 -3.9910619 -4.0017066 -4.0323219 -4.0761156 -4.1330261 -4.1657791 -4.1828837 -4.2100883][-4.2461243 -4.2369108 -4.2109771 -4.176641 -4.1238904 -4.0503783 -3.9861009 -3.9441257 -3.9777956 -4.0353336 -4.0963359 -4.1534472 -4.180058 -4.199151 -4.2268376][-4.2292738 -4.2110305 -4.1755624 -4.1396351 -4.0915365 -4.0203791 -3.9699616 -3.9533324 -4.0072203 -4.0770693 -4.1372385 -4.1847544 -4.2040868 -4.2209744 -4.2466831][-4.2187014 -4.1992822 -4.1648307 -4.1359892 -4.0991693 -4.0463266 -4.0212078 -4.0284667 -4.088798 -4.149519 -4.1902809 -4.2198024 -4.2322173 -4.2445011 -4.2659678][-4.2288218 -4.2163172 -4.1896152 -4.1674948 -4.1457734 -4.1172543 -4.1133528 -4.13395 -4.1852794 -4.2241836 -4.2411275 -4.25593 -4.2620482 -4.2690158 -4.284173][-4.2513852 -4.2469983 -4.2300878 -4.2147384 -4.2046313 -4.19581 -4.202343 -4.2238922 -4.2579203 -4.2773995 -4.2801952 -4.2864938 -4.2887573 -4.2889953 -4.2984939][-4.2750363 -4.2744217 -4.2634306 -4.2515607 -4.2480936 -4.2497277 -4.2594886 -4.2775745 -4.29718 -4.3055024 -4.3017383 -4.3038893 -4.3037472 -4.3012776 -4.30727][-4.2918735 -4.2898207 -4.2823415 -4.2734079 -4.2720413 -4.2768426 -4.2869325 -4.2995191 -4.3083873 -4.31129 -4.3067651 -4.3073764 -4.307282 -4.3051467 -4.309341][-4.2964153 -4.2916169 -4.28504 -4.278245 -4.27683 -4.2812619 -4.28962 -4.2974439 -4.3012075 -4.3022628 -4.3003592 -4.3007712 -4.3009467 -4.3004055 -4.3043704]]...]
INFO - root - 2017-12-05 22:22:11.648399: step 49310, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 64h:45m:04s remains)
INFO - root - 2017-12-05 22:22:20.264746: step 49320, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 69h:12m:02s remains)
INFO - root - 2017-12-05 22:22:28.735890: step 49330, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 67h:02m:24s remains)
INFO - root - 2017-12-05 22:22:37.276272: step 49340, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 66h:37m:48s remains)
INFO - root - 2017-12-05 22:22:45.576223: step 49350, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 67h:44m:46s remains)
INFO - root - 2017-12-05 22:22:54.097004: step 49360, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 66h:43m:50s remains)
INFO - root - 2017-12-05 22:23:02.529022: step 49370, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 65h:10m:51s remains)
INFO - root - 2017-12-05 22:23:11.116047: step 49380, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 66h:16m:09s remains)
INFO - root - 2017-12-05 22:23:19.602905: step 49390, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 68h:08m:08s remains)
INFO - root - 2017-12-05 22:23:28.142923: step 49400, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 67h:46m:35s remains)
2017-12-05 22:23:29.033287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.237628 -4.2361145 -4.229569 -4.2208452 -4.2152724 -4.2175846 -4.2273407 -4.2340627 -4.2309556 -4.2218308 -4.2201838 -4.2266483 -4.2349763 -4.2473335 -4.2692738][-4.2021594 -4.2010407 -4.1938624 -4.1855769 -4.1800284 -4.183291 -4.1929765 -4.1995611 -4.1944876 -4.1852493 -4.1922016 -4.206758 -4.2210379 -4.2367587 -4.262948][-4.1611362 -4.1590791 -4.1519852 -4.1444087 -4.141119 -4.145328 -4.1498408 -4.1512361 -4.1457844 -4.1412406 -4.1574388 -4.1800094 -4.2021174 -4.2254291 -4.255888][-4.1287303 -4.1257071 -4.1233335 -4.1172681 -4.113627 -4.1144423 -4.1103926 -4.104537 -4.101418 -4.1033807 -4.1243029 -4.1488309 -4.1749191 -4.2080417 -4.249455][-4.1149726 -4.1102867 -4.112473 -4.1069536 -4.0974255 -4.08369 -4.0619788 -4.0495605 -4.05919 -4.0788045 -4.1042824 -4.1299009 -4.1598415 -4.2037139 -4.251893][-4.1160564 -4.1033158 -4.0995255 -4.088522 -4.074193 -4.0491829 -4.0045767 -3.9771891 -3.9997787 -4.0450873 -4.0810103 -4.1090536 -4.14686 -4.2011967 -4.2530479][-4.09623 -4.0695691 -4.0522981 -4.0337648 -4.0170779 -3.9865336 -3.9112878 -3.8505037 -3.8894475 -3.9783044 -4.0407767 -4.0778761 -4.1247292 -4.1842642 -4.2419167][-4.0425358 -4.0038023 -3.9723206 -3.946203 -3.9236307 -3.8852339 -3.7758045 -3.6768286 -3.7427566 -3.8836043 -3.9728339 -4.0224247 -4.080307 -4.1563606 -4.2277431][-4.0120206 -3.9762883 -3.94515 -3.9198518 -3.8961158 -3.8576741 -3.7497189 -3.6504793 -3.7173283 -3.8603678 -3.9503303 -3.9979115 -4.06001 -4.1497717 -4.2313256][-4.0380735 -4.0098581 -3.986505 -3.9770219 -3.9642973 -3.9394443 -3.8686271 -3.8088827 -3.8487325 -3.9431283 -4.012115 -4.0509028 -4.1050105 -4.1865735 -4.2581654][-4.0939207 -4.0617065 -4.0411134 -4.03916 -4.0381103 -4.0296631 -3.9956167 -3.9685731 -3.9929137 -4.0507326 -4.1006923 -4.131495 -4.1750903 -4.2397237 -4.2901039][-4.1440234 -4.1110792 -4.0887036 -4.085341 -4.0919833 -4.0968575 -4.0855708 -4.0764432 -4.091918 -4.1273918 -4.1639781 -4.1894774 -4.2232547 -4.2735958 -4.3096323][-4.1781259 -4.1529818 -4.1309237 -4.123745 -4.1313272 -4.1426125 -4.1442113 -4.1379361 -4.141716 -4.1637192 -4.1919837 -4.2115741 -4.2338896 -4.2737231 -4.3056841][-4.1875653 -4.1719813 -4.1556811 -4.1475248 -4.1534438 -4.1652403 -4.168551 -4.1568751 -4.1536193 -4.1666188 -4.1839786 -4.1961617 -4.2131615 -4.248322 -4.28176][-4.180748 -4.1709132 -4.1607952 -4.1572943 -4.1674562 -4.1802139 -4.1768689 -4.15217 -4.1366768 -4.1391077 -4.1499429 -4.1599078 -4.17925 -4.2153049 -4.2544065]]...]
INFO - root - 2017-12-05 22:23:37.541669: step 49410, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 66h:11m:12s remains)
INFO - root - 2017-12-05 22:23:46.029122: step 49420, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 67h:39m:34s remains)
INFO - root - 2017-12-05 22:23:54.579069: step 49430, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.891 sec/batch; 70h:02m:27s remains)
INFO - root - 2017-12-05 22:24:02.977928: step 49440, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 65h:28m:36s remains)
INFO - root - 2017-12-05 22:24:11.384474: step 49450, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 68h:16m:02s remains)
INFO - root - 2017-12-05 22:24:20.018871: step 49460, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 68h:38m:12s remains)
INFO - root - 2017-12-05 22:24:28.654869: step 49470, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 65h:46m:10s remains)
INFO - root - 2017-12-05 22:24:37.206683: step 49480, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 65h:20m:06s remains)
INFO - root - 2017-12-05 22:24:45.757074: step 49490, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:54m:08s remains)
INFO - root - 2017-12-05 22:24:54.276637: step 49500, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 65h:18m:53s remains)
2017-12-05 22:24:55.042671: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1627989 -4.1680183 -4.1700168 -4.1555805 -4.1402979 -4.1415997 -4.1536274 -4.1595864 -4.1531682 -4.1480136 -4.1652665 -4.19162 -4.2224064 -4.2612772 -4.2984271][-4.1288643 -4.1440864 -4.1607785 -4.1600738 -4.1496 -4.1510878 -4.1670303 -4.1767306 -4.1672707 -4.1578703 -4.1758842 -4.201221 -4.2322187 -4.2698894 -4.30535][-4.1051521 -4.1307859 -4.1604738 -4.1771789 -4.1750255 -4.1764479 -4.1931291 -4.2064805 -4.1980634 -4.1865363 -4.203609 -4.2284594 -4.25546 -4.2874289 -4.3153205][-4.0942459 -4.1221719 -4.1551309 -4.1801291 -4.1835814 -4.1849713 -4.2015147 -4.2213445 -4.2221227 -4.2171597 -4.2377324 -4.2647023 -4.2900534 -4.3154254 -4.3333778][-4.1042147 -4.1215639 -4.142621 -4.1572456 -4.1531978 -4.14967 -4.1635766 -4.1887393 -4.2029676 -4.2142091 -4.2438588 -4.2773395 -4.3068008 -4.3319225 -4.34674][-4.1218472 -4.1172886 -4.1126966 -4.1010962 -4.0775933 -4.0626645 -4.072156 -4.0977607 -4.1209927 -4.15142 -4.2007675 -4.2498326 -4.289803 -4.3232126 -4.3440142][-4.1503658 -4.1264539 -4.0989375 -4.0629864 -4.0198455 -3.9839864 -3.9694877 -3.96913 -3.9817572 -4.0307412 -4.1073828 -4.1817341 -4.2418294 -4.2923532 -4.3258686][-4.1736116 -4.1429896 -4.1102905 -4.0716314 -4.0269961 -3.9833474 -3.952302 -3.921438 -3.909579 -3.956383 -4.04257 -4.1315274 -4.2035208 -4.2656364 -4.30902][-4.1968265 -4.1689458 -4.140945 -4.11296 -4.0800643 -4.0487156 -4.0278859 -3.9979777 -3.9804997 -4.0135741 -4.0780663 -4.15013 -4.2104073 -4.2663779 -4.3064137][-4.2230325 -4.2069864 -4.1915922 -4.17681 -4.156178 -4.1353927 -4.1238956 -4.1039286 -4.08974 -4.1109877 -4.1552758 -4.2065616 -4.2495294 -4.2935572 -4.3235703][-4.2397351 -4.2376485 -4.2353916 -4.231246 -4.2197556 -4.2066488 -4.1977468 -4.1815252 -4.167335 -4.1801934 -4.213026 -4.2520132 -4.2841473 -4.3191328 -4.3427439][-4.2442369 -4.2482953 -4.2515454 -4.2531018 -4.2482567 -4.2401853 -4.2307167 -4.2105646 -4.19409 -4.2042575 -4.2325172 -4.2663512 -4.2948976 -4.325274 -4.345015][-4.2364941 -4.2387161 -4.2388086 -4.2382069 -4.2349939 -4.2285504 -4.2157269 -4.186872 -4.1657434 -4.1767054 -4.204247 -4.2373328 -4.2646394 -4.2955046 -4.3210206][-4.2254009 -4.2245574 -4.2204413 -4.2156954 -4.210475 -4.2017875 -4.1847134 -4.1465054 -4.1207042 -4.1295819 -4.1581039 -4.1913939 -4.2175007 -4.2529984 -4.2874265][-4.2158632 -4.21633 -4.2128816 -4.2080441 -4.2029691 -4.19462 -4.1773286 -4.139924 -4.1168981 -4.1244678 -4.1490088 -4.1759629 -4.197938 -4.2344232 -4.27161]]...]
INFO - root - 2017-12-05 22:25:03.525177: step 49510, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.847 sec/batch; 66h:32m:33s remains)
INFO - root - 2017-12-05 22:25:12.030502: step 49520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 66h:35m:48s remains)
INFO - root - 2017-12-05 22:25:20.564852: step 49530, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.874 sec/batch; 68h:43m:54s remains)
INFO - root - 2017-12-05 22:25:29.043724: step 49540, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 66h:06m:29s remains)
INFO - root - 2017-12-05 22:25:37.531701: step 49550, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 66h:03m:05s remains)
INFO - root - 2017-12-05 22:25:46.006073: step 49560, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.816 sec/batch; 64h:06m:18s remains)
INFO - root - 2017-12-05 22:25:54.596181: step 49570, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 69h:07m:44s remains)
INFO - root - 2017-12-05 22:26:03.223050: step 49580, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 65h:04m:31s remains)
INFO - root - 2017-12-05 22:26:11.837222: step 49590, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 68h:05m:08s remains)
INFO - root - 2017-12-05 22:26:20.547061: step 49600, loss = 2.06, batch loss = 2.00 (8.5 examples/sec; 0.942 sec/batch; 74h:00m:55s remains)
2017-12-05 22:26:21.317291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2476358 -4.2564316 -4.253736 -4.2493639 -4.2570739 -4.2621875 -4.2626343 -4.2544045 -4.2429962 -4.2218165 -4.1910644 -4.1725636 -4.1766305 -4.1838207 -4.1958518][-4.24342 -4.2493377 -4.2524853 -4.254806 -4.2661614 -4.2697234 -4.2621861 -4.2451639 -4.2251053 -4.2000914 -4.1719909 -4.1591091 -4.169951 -4.1862736 -4.2106051][-4.21449 -4.2133288 -4.2257891 -4.2369771 -4.250937 -4.2575293 -4.2428775 -4.21573 -4.1895752 -4.1650882 -4.1425071 -4.1322975 -4.146173 -4.1737633 -4.2087574][-4.1764579 -4.1675706 -4.1895146 -4.2090659 -4.2227526 -4.2283406 -4.208323 -4.1780095 -4.1523776 -4.1352544 -4.1230135 -4.1157861 -4.1293159 -4.1629052 -4.2038493][-4.1486759 -4.1387024 -4.1652513 -4.1847591 -4.1950531 -4.1915522 -4.1608787 -4.1251235 -4.10348 -4.0962625 -4.1039977 -4.1099129 -4.1273928 -4.1615977 -4.1981187][-4.1341639 -4.1315279 -4.1587949 -4.1683488 -4.1641927 -4.1420927 -4.0895824 -4.0392 -4.0160489 -4.0299969 -4.0684628 -4.0989685 -4.1253181 -4.1593 -4.191133][-4.1419954 -4.152597 -4.174665 -4.1675563 -4.1370292 -4.08691 -4.0028849 -3.924572 -3.9018776 -3.9480696 -4.0234842 -4.0779033 -4.1096125 -4.1415997 -4.1737113][-4.1750956 -4.198276 -4.2125568 -4.1883249 -4.1316223 -4.0570312 -3.9488943 -3.843787 -3.8246498 -3.9035099 -4.0055814 -4.0713673 -4.0999866 -4.1295943 -4.1579585][-4.2144694 -4.2440662 -4.2467227 -4.208569 -4.1375003 -4.0559554 -3.9495678 -3.8539786 -3.8543792 -3.9430084 -4.0394754 -4.0930052 -4.1127992 -4.1366725 -4.1559939][-4.234817 -4.25746 -4.253016 -4.208292 -4.1370072 -4.063509 -3.9833717 -3.9274201 -3.9528139 -4.0280876 -4.0989861 -4.1319518 -4.14149 -4.15777 -4.1724968][-4.2386417 -4.24659 -4.2396851 -4.2003284 -4.13021 -4.0620961 -4.0104661 -3.9981277 -4.0405693 -4.1012731 -4.1524596 -4.1727352 -4.1762447 -4.1882725 -4.2034745][-4.2341142 -4.2315192 -4.2261734 -4.1903563 -4.1214414 -4.0508332 -4.0172429 -4.0367508 -4.0925965 -4.1470914 -4.1876493 -4.2032423 -4.2066622 -4.2182064 -4.2346258][-4.2320404 -4.2244573 -4.2142415 -4.1748919 -4.10275 -4.0319657 -4.0105028 -4.0495763 -4.1173105 -4.1725359 -4.2058897 -4.219018 -4.2244654 -4.2368283 -4.2532449][-4.2338462 -4.2211308 -4.198576 -4.1507 -4.0731254 -4.0100784 -4.00143 -4.0531597 -4.1279383 -4.1863313 -4.2157159 -4.2262096 -4.2307096 -4.2418461 -4.2562485][-4.2375216 -4.2205443 -4.1889277 -4.1338468 -4.0603929 -4.0127611 -4.0172205 -4.0758352 -4.1469045 -4.2009263 -4.2266464 -4.2342582 -4.2372971 -4.2458863 -4.2545552]]...]
INFO - root - 2017-12-05 22:26:29.857801: step 49610, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 65h:02m:32s remains)
INFO - root - 2017-12-05 22:26:38.374017: step 49620, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 69h:10m:54s remains)
INFO - root - 2017-12-05 22:26:47.019349: step 49630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 66h:56m:51s remains)
INFO - root - 2017-12-05 22:26:55.622462: step 49640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:51m:33s remains)
INFO - root - 2017-12-05 22:27:04.021892: step 49650, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 69h:45m:57s remains)
INFO - root - 2017-12-05 22:27:12.695000: step 49660, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 67h:21m:13s remains)
INFO - root - 2017-12-05 22:27:21.200657: step 49670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 67h:51m:30s remains)
INFO - root - 2017-12-05 22:27:29.800373: step 49680, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 65h:58m:08s remains)
INFO - root - 2017-12-05 22:27:38.344871: step 49690, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 69h:08m:46s remains)
INFO - root - 2017-12-05 22:27:46.738123: step 49700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 66h:09m:56s remains)
2017-12-05 22:27:47.534445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.16847 -4.1503587 -4.11618 -4.107842 -4.1118135 -4.1139121 -4.1049094 -4.0837507 -4.0823655 -4.1161137 -4.1649013 -4.2240248 -4.2739887 -4.3098526 -4.3283472][-4.1537256 -4.1247768 -4.0844445 -4.0784659 -4.0943651 -4.1050463 -4.094738 -4.0719748 -4.0759335 -4.1151996 -4.1647191 -4.224329 -4.2756748 -4.3125238 -4.3297949][-4.1463904 -4.1143155 -4.0697088 -4.0622134 -4.0835342 -4.0962911 -4.0823178 -4.0576968 -4.0707707 -4.120625 -4.1749773 -4.2379975 -4.2902188 -4.323535 -4.3351827][-4.1610088 -4.1303058 -4.0863833 -4.0807171 -4.1015015 -4.1062651 -4.0774732 -4.0474744 -4.065249 -4.1224022 -4.18574 -4.2535772 -4.3083458 -4.3384571 -4.3433933][-4.1935072 -4.1679997 -4.1304588 -4.1289935 -4.1408944 -4.1360731 -4.0958991 -4.0596952 -4.0768914 -4.1348357 -4.203331 -4.2700963 -4.3244028 -4.352356 -4.3515763][-4.2252817 -4.20783 -4.1786761 -4.175703 -4.1744609 -4.1658711 -4.1209044 -4.0761638 -4.0847673 -4.14451 -4.2162495 -4.2782292 -4.328886 -4.3565793 -4.3553147][-4.2488914 -4.2361307 -4.2140551 -4.2077813 -4.1927977 -4.1791677 -4.1319509 -4.0824881 -4.082634 -4.1425695 -4.213922 -4.2730384 -4.3229141 -4.3536711 -4.3560982][-4.25677 -4.2443738 -4.2246428 -4.217936 -4.19973 -4.1803761 -4.1314645 -4.0829797 -4.0811763 -4.1371093 -4.2042961 -4.2625074 -4.3127651 -4.3468194 -4.3541441][-4.2508364 -4.2415137 -4.2253814 -4.2194033 -4.2044225 -4.1789441 -4.1291566 -4.0874553 -4.0890222 -4.1382565 -4.2002344 -4.2570438 -4.3066831 -4.3414669 -4.3511043][-4.2450566 -4.2388887 -4.2255392 -4.2174468 -4.2033315 -4.1772466 -4.1357861 -4.105895 -4.1093931 -4.1524434 -4.2070436 -4.2615838 -4.3086448 -4.3388505 -4.3479686][-4.2435284 -4.2360806 -4.223742 -4.2146997 -4.2067327 -4.1896377 -4.158123 -4.1345716 -4.1388364 -4.1777425 -4.2245026 -4.2760062 -4.3178935 -4.3397064 -4.3449154][-4.2419734 -4.233161 -4.2203269 -4.2128019 -4.2118082 -4.2073216 -4.1873374 -4.1682243 -4.1742878 -4.2115579 -4.25216 -4.2956882 -4.3291078 -4.3434753 -4.3434625][-4.2418818 -4.2293463 -4.2179341 -4.2141271 -4.2161579 -4.2150106 -4.2044144 -4.1932368 -4.2053137 -4.2444739 -4.280323 -4.3135614 -4.3379683 -4.3478608 -4.3437424][-4.2564363 -4.2442636 -4.2316656 -4.2293005 -4.230319 -4.2248993 -4.2164221 -4.210135 -4.2272382 -4.2667227 -4.3002844 -4.3272204 -4.3452525 -4.3513589 -4.3446517][-4.283236 -4.27672 -4.26712 -4.2629085 -4.2583838 -4.2468166 -4.2351513 -4.2290807 -4.2474775 -4.2847848 -4.3144388 -4.33656 -4.349247 -4.3522539 -4.3450203]]...]
INFO - root - 2017-12-05 22:27:56.218730: step 49710, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 67h:29m:29s remains)
INFO - root - 2017-12-05 22:28:04.780190: step 49720, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 69h:06m:40s remains)
INFO - root - 2017-12-05 22:28:13.269713: step 49730, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 66h:33m:02s remains)
INFO - root - 2017-12-05 22:28:21.800994: step 49740, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 66h:21m:26s remains)
INFO - root - 2017-12-05 22:28:30.270129: step 49750, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 67h:34m:32s remains)
INFO - root - 2017-12-05 22:28:38.668068: step 49760, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 68h:23m:33s remains)
INFO - root - 2017-12-05 22:28:47.207189: step 49770, loss = 2.04, batch loss = 1.99 (10.0 examples/sec; 0.799 sec/batch; 62h:43m:26s remains)
INFO - root - 2017-12-05 22:28:55.713872: step 49780, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 67h:04m:07s remains)
INFO - root - 2017-12-05 22:29:04.244026: step 49790, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 69h:04m:53s remains)
INFO - root - 2017-12-05 22:29:12.870177: step 49800, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 68h:51m:52s remains)
2017-12-05 22:29:13.717092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.290607 -4.2977014 -4.3031759 -4.3091264 -4.3130727 -4.31425 -4.310842 -4.3013515 -4.2920237 -4.2876582 -4.2895093 -4.2959404 -4.303267 -4.3086009 -4.3097014][-4.3081527 -4.3141375 -4.3166008 -4.3180094 -4.3146229 -4.3052468 -4.29065 -4.2745409 -4.2630119 -4.2595129 -4.2657132 -4.2802553 -4.2977657 -4.3128247 -4.3201423][-4.3154125 -4.321835 -4.3222866 -4.3182688 -4.3039126 -4.279304 -4.2506046 -4.2258816 -4.2086592 -4.2010517 -4.208817 -4.2338409 -4.263658 -4.2899427 -4.3051109][-4.300117 -4.3077722 -4.3084712 -4.3009505 -4.2781816 -4.2424331 -4.2029734 -4.169076 -4.1429729 -4.1278539 -4.1355605 -4.1700983 -4.2101507 -4.2446494 -4.2668753][-4.2783003 -4.28619 -4.2867236 -4.276391 -4.2497869 -4.2103944 -4.1654854 -4.1221819 -4.0857749 -4.0648103 -4.0730858 -4.1127081 -4.1572433 -4.1946311 -4.2208095][-4.2516189 -4.2602172 -4.2620196 -4.2527275 -4.2284861 -4.1919274 -4.14601 -4.0993876 -4.0604296 -4.040061 -4.0504642 -4.0865245 -4.1249709 -4.1557918 -4.1768408][-4.2342873 -4.2428207 -4.2463431 -4.2401175 -4.2214694 -4.1914926 -4.1501946 -4.1094584 -4.0768619 -4.0635781 -4.076117 -4.1029439 -4.1284852 -4.1470909 -4.1560388][-4.2400832 -4.2469382 -4.2502208 -4.2471256 -4.2356253 -4.2158966 -4.1876354 -4.16041 -4.1390872 -4.1338396 -4.1453047 -4.161406 -4.1742411 -4.1805162 -4.1766949][-4.2585049 -4.2622862 -4.2636456 -4.2620997 -4.2567081 -4.2467184 -4.2317743 -4.2173572 -4.2068772 -4.207252 -4.2163844 -4.2257795 -4.2307777 -4.2282162 -4.2155671][-4.2806482 -4.2818131 -4.2808995 -4.2797842 -4.2768192 -4.2713704 -4.2633395 -4.2550964 -4.2509546 -4.2549953 -4.2642941 -4.2723222 -4.2743883 -4.2680283 -4.2531147][-4.2983584 -4.2986541 -4.2966104 -4.2947936 -4.2917833 -4.2868218 -4.2800832 -4.2723885 -4.2693872 -4.2737446 -4.2831144 -4.2913632 -4.2926445 -4.2869806 -4.2762394][-4.306222 -4.306787 -4.3051052 -4.3036575 -4.3006849 -4.2953882 -4.28821 -4.2793055 -4.2746091 -4.2782135 -4.2881746 -4.2968044 -4.2983003 -4.2938733 -4.2873731][-4.3113484 -4.3131032 -4.3129239 -4.3131323 -4.3112907 -4.3049989 -4.2953262 -4.28399 -4.277544 -4.2806449 -4.2909055 -4.299139 -4.3001904 -4.2968092 -4.2937779][-4.3184204 -4.3215084 -4.322937 -4.324739 -4.3245878 -4.3189945 -4.3081169 -4.2955904 -4.2882328 -4.290493 -4.2985744 -4.3045311 -4.3044553 -4.3018188 -4.3010783][-4.3275084 -4.3303089 -4.3323045 -4.3356419 -4.3378453 -4.3344607 -4.3244934 -4.3127975 -4.305584 -4.3058639 -4.3093791 -4.3108516 -4.3089767 -4.306848 -4.3077145]]...]
INFO - root - 2017-12-05 22:29:22.331454: step 49810, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.869 sec/batch; 68h:15m:37s remains)
INFO - root - 2017-12-05 22:29:30.788381: step 49820, loss = 2.09, batch loss = 2.03 (10.5 examples/sec; 0.762 sec/batch; 59h:51m:55s remains)
INFO - root - 2017-12-05 22:29:39.313474: step 49830, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 65h:39m:10s remains)
INFO - root - 2017-12-05 22:29:47.925179: step 49840, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 66h:13m:40s remains)
INFO - root - 2017-12-05 22:29:56.335531: step 49850, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 65h:23m:49s remains)
INFO - root - 2017-12-05 22:30:04.866321: step 49860, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 68h:24m:41s remains)
INFO - root - 2017-12-05 22:30:13.342248: step 49870, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.880 sec/batch; 69h:07m:12s remains)
INFO - root - 2017-12-05 22:30:21.964863: step 49880, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:43m:33s remains)
INFO - root - 2017-12-05 22:30:30.506963: step 49890, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 66h:34m:09s remains)
INFO - root - 2017-12-05 22:30:39.040954: step 49900, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.818 sec/batch; 64h:14m:44s remains)
2017-12-05 22:30:39.833683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3374777 -4.3390846 -4.3401394 -4.3412528 -4.3425074 -4.3444533 -4.3475971 -4.3507266 -4.3506565 -4.3462749 -4.3388157 -4.3309407 -4.3252587 -4.3229685 -4.3247342][-4.3401508 -4.3442488 -4.3453593 -4.3448896 -4.3432493 -4.342288 -4.3429589 -4.3443341 -4.3427076 -4.3374648 -4.3314934 -4.327765 -4.3274236 -4.3300667 -4.334609][-4.3305354 -4.3373342 -4.339385 -4.3385549 -4.3358607 -4.33389 -4.3338966 -4.334548 -4.3323851 -4.3268418 -4.3222609 -4.3222284 -4.3259482 -4.3317924 -4.3384485][-4.3087626 -4.3177938 -4.3209515 -4.3202195 -4.3174696 -4.3157568 -4.3161798 -4.3164144 -4.311974 -4.3027544 -4.2964754 -4.2992191 -4.3091316 -4.3218184 -4.3335848][-4.2683744 -4.27748 -4.2786326 -4.2746325 -4.2694607 -4.2675934 -4.2696857 -4.2706771 -4.2623553 -4.2459106 -4.2361045 -4.2445555 -4.26609 -4.291255 -4.3137717][-4.2054787 -4.2104273 -4.20631 -4.1965628 -4.1873832 -4.1848006 -4.1896839 -4.1936293 -4.1846695 -4.1649103 -4.1541724 -4.170197 -4.2050467 -4.2439809 -4.2810555][-4.1383057 -4.139245 -4.1297464 -4.1141291 -4.0999656 -4.0963149 -4.1059265 -4.1177106 -4.1141992 -4.0960226 -4.0854406 -4.1061625 -4.1503654 -4.1994829 -4.2469387][-4.1227055 -4.1234488 -4.1126709 -4.0943065 -4.0751586 -4.0672956 -4.0790534 -4.0990648 -4.1030531 -4.0864353 -4.0718288 -4.0856042 -4.125124 -4.1726279 -4.2205868][-4.1652832 -4.1673055 -4.1589174 -4.1417041 -4.1190934 -4.1014438 -4.1022487 -4.1146755 -4.1156564 -4.098218 -4.080328 -4.0821524 -4.1056814 -4.1394925 -4.1785097][-4.2084484 -4.2107673 -4.2027774 -4.1855793 -4.1583042 -4.1275926 -4.1095552 -4.105967 -4.0981097 -4.0761123 -4.0514975 -4.0427194 -4.0574236 -4.0863934 -4.1230388][-4.2288842 -4.2318306 -4.2239428 -4.2059836 -4.174571 -4.1342916 -4.1022887 -4.0872226 -4.075676 -4.0533915 -4.02401 -4.0036149 -4.007544 -4.0314 -4.0656533][-4.2319231 -4.2361617 -4.2320366 -4.2188873 -4.1929154 -4.1571031 -4.1257396 -4.1086059 -4.0978465 -4.0820203 -4.0605106 -4.0407257 -4.0343156 -4.0433011 -4.0625558][-4.2438173 -4.2473922 -4.2465043 -4.2405362 -4.2260561 -4.2036605 -4.1827564 -4.1709776 -4.1637473 -4.1526122 -4.1368213 -4.1210556 -4.1121159 -4.1114879 -4.1188617][-4.2757311 -4.2777019 -4.2788539 -4.2775335 -4.2721982 -4.2624068 -4.2519717 -4.245749 -4.2414308 -4.2342615 -4.2237163 -4.2119775 -4.2026792 -4.1967783 -4.1957397][-4.3122244 -4.3137445 -4.3157105 -4.3164983 -4.3157535 -4.3134766 -4.3103271 -4.3093228 -4.3089542 -4.3063712 -4.3007979 -4.2931242 -4.2843151 -4.2749863 -4.2682257]]...]
INFO - root - 2017-12-05 22:30:48.401952: step 49910, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 65h:24m:34s remains)
INFO - root - 2017-12-05 22:30:57.035341: step 49920, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 68h:51m:09s remains)
INFO - root - 2017-12-05 22:31:05.550445: step 49930, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.829 sec/batch; 65h:02m:16s remains)
INFO - root - 2017-12-05 22:31:14.084496: step 49940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 67h:19m:04s remains)
INFO - root - 2017-12-05 22:31:22.532111: step 49950, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 65h:25m:10s remains)
INFO - root - 2017-12-05 22:31:31.069048: step 49960, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 68h:19m:28s remains)
INFO - root - 2017-12-05 22:31:39.512941: step 49970, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 66h:54m:11s remains)
INFO - root - 2017-12-05 22:31:48.035813: step 49980, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 66h:27m:06s remains)
INFO - root - 2017-12-05 22:31:56.635776: step 49990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 66h:47m:57s remains)
INFO - root - 2017-12-05 22:32:05.158437: step 50000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 67h:54m:31s remains)
2017-12-05 22:32:06.057406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3082185 -4.2906952 -4.2727923 -4.253087 -4.2294631 -4.2118397 -4.210144 -4.2238736 -4.2343426 -4.2374077 -4.2472868 -4.258316 -4.2635689 -4.2651243 -4.2765541][-4.2980623 -4.2747154 -4.2490945 -4.2217722 -4.1892095 -4.1599989 -4.1516423 -4.1647596 -4.1783996 -4.1850591 -4.1986051 -4.2131348 -4.2213044 -4.2251725 -4.2396884][-4.2902565 -4.2625237 -4.2318521 -4.1969252 -4.1521106 -4.1077948 -4.0869412 -4.09702 -4.1123919 -4.1246085 -4.1457329 -4.1663356 -4.185 -4.1962385 -4.2132192][-4.2796078 -4.2468882 -4.2070074 -4.1615963 -4.1054025 -4.0501904 -4.0207314 -4.0277138 -4.0412817 -4.0576215 -4.0892153 -4.1222353 -4.156631 -4.1813121 -4.2011914][-4.2635441 -4.217391 -4.1634007 -4.1090012 -4.0510573 -3.9919209 -3.9523709 -3.9563203 -3.9780715 -4.0044808 -4.05088 -4.1094613 -4.1570997 -4.1876712 -4.2013321][-4.2434974 -4.1820855 -4.1151381 -4.053309 -3.9894836 -3.914964 -3.8528748 -3.8575363 -3.9114666 -3.9701829 -4.046155 -4.1279149 -4.1795197 -4.2026978 -4.2074652][-4.2238188 -4.145637 -4.0619044 -3.9795425 -3.8885083 -3.7769408 -3.682301 -3.7100146 -3.8331738 -3.9480133 -4.05125 -4.1397753 -4.185895 -4.2035818 -4.2075653][-4.1979694 -4.1006155 -3.9899232 -3.8699541 -3.7371175 -3.5895641 -3.4708004 -3.5432718 -3.7364645 -3.889971 -4.0023136 -4.0847931 -4.1375542 -4.1663618 -4.1868329][-4.1857672 -4.0834823 -3.9601667 -3.8275464 -3.6998367 -3.5807784 -3.498718 -3.58844 -3.7493451 -3.8683412 -3.953501 -4.020968 -4.08173 -4.129529 -4.1672578][-4.1938505 -4.1021957 -3.990871 -3.8876495 -3.8107066 -3.7514338 -3.7158329 -3.7742379 -3.8506064 -3.9080589 -3.9629862 -4.0182886 -4.0812054 -4.136447 -4.1744652][-4.21763 -4.1402373 -4.0511851 -3.9787076 -3.9329567 -3.8998013 -3.8829975 -3.9121618 -3.9443839 -3.9741325 -4.0123787 -4.0561137 -4.1102939 -4.1576209 -4.1884685][-4.2556996 -4.1984234 -4.1340537 -4.0835643 -4.052352 -4.0262203 -4.0155029 -4.0324469 -4.0546441 -4.0782318 -4.1056705 -4.1345687 -4.1676903 -4.1966233 -4.2145839][-4.2896061 -4.2503695 -4.2033672 -4.1671324 -4.1461916 -4.1327243 -4.132514 -4.1480513 -4.1682587 -4.1905236 -4.2063904 -4.2175455 -4.2265897 -4.2341485 -4.2413821][-4.3182592 -4.2936831 -4.2615633 -4.237896 -4.2276278 -4.2293549 -4.2379184 -4.2501769 -4.2638607 -4.2773504 -4.2841182 -4.2811441 -4.2750788 -4.273901 -4.2792315][-4.3374953 -4.3230743 -4.3043232 -4.291779 -4.2896781 -4.296597 -4.3054433 -4.3131232 -4.3216686 -4.3275008 -4.327394 -4.3196092 -4.3104115 -4.309526 -4.315]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 22:32:15.245687: step 50010, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 68h:25m:33s remains)
INFO - root - 2017-12-05 22:32:23.670770: step 50020, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 64h:51m:42s remains)
INFO - root - 2017-12-05 22:32:32.362791: step 50030, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 67h:19m:16s remains)
INFO - root - 2017-12-05 22:32:40.740048: step 50040, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 69h:03m:09s remains)
INFO - root - 2017-12-05 22:32:49.163682: step 50050, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 65h:56m:25s remains)
INFO - root - 2017-12-05 22:32:57.783802: step 50060, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 65h:50m:42s remains)
INFO - root - 2017-12-05 22:33:06.315840: step 50070, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 66h:42m:27s remains)
INFO - root - 2017-12-05 22:33:14.721301: step 50080, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:40m:26s remains)
INFO - root - 2017-12-05 22:33:23.271346: step 50090, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 65h:59m:10s remains)
INFO - root - 2017-12-05 22:33:31.832640: step 50100, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 67h:35m:09s remains)
2017-12-05 22:33:32.616665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1646514 -4.1819072 -4.2148523 -4.2396851 -4.226779 -4.1740093 -4.1110172 -4.0798531 -4.095706 -4.1324091 -4.16358 -4.1786494 -4.1762867 -4.1872039 -4.189683][-4.1784868 -4.2081203 -4.2384629 -4.2492719 -4.2270737 -4.1716495 -4.1114397 -4.0860806 -4.1044989 -4.1342077 -4.1567078 -4.1705256 -4.1775136 -4.195333 -4.1992669][-4.1823506 -4.2188268 -4.2471743 -4.24501 -4.2117858 -4.1527796 -4.0951376 -4.0751243 -4.0960608 -4.1265459 -4.1491241 -4.16623 -4.182477 -4.2017474 -4.2044139][-4.1814761 -4.2219286 -4.2496047 -4.2408576 -4.1992397 -4.1365185 -4.0792513 -4.0625734 -4.0862722 -4.1191282 -4.1430182 -4.16894 -4.1927042 -4.2141008 -4.215764][-4.1650863 -4.2056527 -4.2323036 -4.2236047 -4.185926 -4.1303349 -4.0771937 -4.0639143 -4.0918465 -4.1270933 -4.1519537 -4.180923 -4.2093987 -4.2326632 -4.2319994][-4.1290021 -4.1679735 -4.1977243 -4.1954575 -4.1676736 -4.1243258 -4.0798831 -4.075768 -4.1089525 -4.1474152 -4.177846 -4.2037787 -4.2293983 -4.2460289 -4.2422633][-4.0854707 -4.1225753 -4.1566234 -4.1636915 -4.1461015 -4.1081676 -4.0618734 -4.0656962 -4.1072507 -4.1514249 -4.1901913 -4.2177258 -4.2371135 -4.2460861 -4.2413397][-4.0473728 -4.0866327 -4.1258574 -4.1403189 -4.1246815 -4.0825324 -4.0318017 -4.0458345 -4.0981264 -4.1525717 -4.1942463 -4.2211585 -4.2359324 -4.2428417 -4.2379961][-4.0531106 -4.0879655 -4.1221323 -4.1347218 -4.1167197 -4.0754433 -4.0319953 -4.0513644 -4.1105533 -4.1714487 -4.212904 -4.2355695 -4.248229 -4.2536077 -4.2450585][-4.0998673 -4.1249933 -4.1467443 -4.1554437 -4.1435194 -4.1121922 -4.0804629 -4.0955439 -4.1491075 -4.204164 -4.24019 -4.2588363 -4.2713346 -4.27442 -4.266315][-4.1501069 -4.1720967 -4.185297 -4.1963129 -4.1992235 -4.1814284 -4.159327 -4.1658311 -4.2022958 -4.242588 -4.2704878 -4.2864437 -4.2952156 -4.2935529 -4.286458][-4.200963 -4.2196817 -4.221036 -4.2325659 -4.2435846 -4.2350435 -4.2183604 -4.221621 -4.2404976 -4.2636495 -4.2829695 -4.29977 -4.3077154 -4.3038163 -4.3002052][-4.2440133 -4.2563229 -4.2443867 -4.2485776 -4.258925 -4.2565975 -4.2456684 -4.2486262 -4.2570734 -4.2708616 -4.2876887 -4.3069839 -4.3204432 -4.3214717 -4.3210225][-4.2610369 -4.2686396 -4.2479653 -4.2435441 -4.2573056 -4.2626524 -4.2570319 -4.2594309 -4.2627792 -4.2720246 -4.2874017 -4.30595 -4.3203025 -4.3260164 -4.3302088][-4.2603145 -4.2602277 -4.2380629 -4.2306571 -4.2483821 -4.2608705 -4.2627892 -4.2657757 -4.2675486 -4.2741971 -4.2861285 -4.2987275 -4.3101978 -4.3182259 -4.3236194]]...]
INFO - root - 2017-12-05 22:33:41.235389: step 50110, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 68h:29m:22s remains)
INFO - root - 2017-12-05 22:33:49.711747: step 50120, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 67h:16m:15s remains)
INFO - root - 2017-12-05 22:33:58.218663: step 50130, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 64h:19m:34s remains)
INFO - root - 2017-12-05 22:34:06.699408: step 50140, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 64h:23m:27s remains)
INFO - root - 2017-12-05 22:34:15.081744: step 50150, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 67h:43m:36s remains)
INFO - root - 2017-12-05 22:34:23.684373: step 50160, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 66h:22m:27s remains)
INFO - root - 2017-12-05 22:34:32.095765: step 50170, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 65h:45m:11s remains)
INFO - root - 2017-12-05 22:34:40.599543: step 50180, loss = 2.07, batch loss = 2.01 (10.0 examples/sec; 0.799 sec/batch; 62h:38m:10s remains)
INFO - root - 2017-12-05 22:34:48.992448: step 50190, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.847 sec/batch; 66h:25m:38s remains)
INFO - root - 2017-12-05 22:34:57.545782: step 50200, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 67h:09m:11s remains)
2017-12-05 22:34:58.410997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2912521 -4.2930031 -4.2966628 -4.3011651 -4.30364 -4.3020325 -4.2982726 -4.2960362 -4.2926693 -4.2855172 -4.2785993 -4.284976 -4.2954879 -4.2973218 -4.2890778][-4.2759442 -4.2789354 -4.2842412 -4.28925 -4.2898426 -4.2827253 -4.2724748 -4.2667537 -4.2648463 -4.2591844 -4.2539034 -4.265583 -4.2835832 -4.28763 -4.2734375][-4.2675886 -4.267199 -4.2694139 -4.2712297 -4.2686868 -4.2545786 -4.2350283 -4.2250242 -4.2288327 -4.2334242 -4.2376714 -4.2538633 -4.2743812 -4.28292 -4.2703094][-4.2651076 -4.2559509 -4.2487721 -4.2427168 -4.2334595 -4.2138233 -4.1874614 -4.1745887 -4.1851864 -4.2046561 -4.2250195 -4.2464886 -4.2721138 -4.2908053 -4.2822704][-4.2691031 -4.2488694 -4.2272253 -4.2092228 -4.1912255 -4.161191 -4.119936 -4.0953956 -4.1122303 -4.1533532 -4.1967039 -4.2336445 -4.270051 -4.30037 -4.2988067][-4.2838488 -4.2580757 -4.22206 -4.1871333 -4.1491876 -4.0939608 -4.0153031 -3.9568703 -3.9750798 -4.0572472 -4.1431584 -4.2076888 -4.257792 -4.2968264 -4.3042808][-4.2961903 -4.2717786 -4.23123 -4.1837716 -4.1250682 -4.0346451 -3.9053254 -3.7836912 -3.7902446 -3.9289837 -4.0732036 -4.1712518 -4.2342219 -4.2733197 -4.28657][-4.3002863 -4.2837353 -4.2505865 -4.20208 -4.1347761 -4.0226331 -3.8546619 -3.6799965 -3.6685131 -3.8442984 -4.0241814 -4.1374636 -4.1989226 -4.2316966 -4.250361][-4.3004594 -4.291326 -4.2671523 -4.2234941 -4.1626158 -4.0640678 -3.9135988 -3.7633605 -3.7478819 -3.8856368 -4.0309267 -4.1184058 -4.161068 -4.181253 -4.2058344][-4.2917733 -4.2872858 -4.2674165 -4.2355533 -4.194705 -4.1322703 -4.0341597 -3.940002 -3.9244034 -4.0008245 -4.0860825 -4.1286974 -4.1366806 -4.1332989 -4.1515126][-4.2644792 -4.2669215 -4.25848 -4.2449675 -4.2289081 -4.2030482 -4.15078 -4.0935879 -4.0752897 -4.1094723 -4.1538429 -4.1649246 -4.1417875 -4.11049 -4.1097803][-4.2274256 -4.23913 -4.2445259 -4.2498903 -4.2526932 -4.2546811 -4.2376242 -4.2052436 -4.1866322 -4.1985283 -4.2191787 -4.215663 -4.178906 -4.1324673 -4.1106634][-4.17968 -4.2030025 -4.2238665 -4.2435722 -4.2575774 -4.27377 -4.2827787 -4.2757072 -4.2647314 -4.266201 -4.2728987 -4.2650981 -4.2296376 -4.187161 -4.1576467][-4.1056709 -4.1369619 -4.1802044 -4.2204556 -4.2520361 -4.2824006 -4.3089061 -4.3183231 -4.3120236 -4.307755 -4.308897 -4.3039527 -4.2776022 -4.2473435 -4.2169166][-3.9781871 -4.0169063 -4.0982432 -4.1722074 -4.2269731 -4.2732511 -4.3110065 -4.329987 -4.3309112 -4.3264928 -4.3271618 -4.3270488 -4.3127289 -4.2967267 -4.2702065]]...]
INFO - root - 2017-12-05 22:35:06.882762: step 50210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 67h:50m:51s remains)
INFO - root - 2017-12-05 22:35:15.403649: step 50220, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 68h:07m:51s remains)
INFO - root - 2017-12-05 22:35:23.950656: step 50230, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.873 sec/batch; 68h:27m:11s remains)
INFO - root - 2017-12-05 22:35:32.483193: step 50240, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.826 sec/batch; 64h:45m:10s remains)
INFO - root - 2017-12-05 22:35:40.920666: step 50250, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 65h:34m:59s remains)
INFO - root - 2017-12-05 22:35:49.420382: step 50260, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 66h:30m:23s remains)
INFO - root - 2017-12-05 22:35:58.131904: step 50270, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 68h:33m:45s remains)
INFO - root - 2017-12-05 22:36:06.650234: step 50280, loss = 2.03, batch loss = 1.97 (9.8 examples/sec; 0.812 sec/batch; 63h:41m:11s remains)
INFO - root - 2017-12-05 22:36:15.149424: step 50290, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.829 sec/batch; 65h:01m:06s remains)
INFO - root - 2017-12-05 22:36:23.674805: step 50300, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 68h:39m:46s remains)
2017-12-05 22:36:24.503461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1810007 -4.1954551 -4.2091484 -4.2119889 -4.208046 -4.2074986 -4.208005 -4.2122836 -4.2236538 -4.2297559 -4.2260857 -4.2284508 -4.2324128 -4.2343836 -4.2431684][-4.1449523 -4.1628418 -4.17311 -4.1720419 -4.1654944 -4.1600428 -4.1556959 -4.1615729 -4.1813059 -4.1981363 -4.2019491 -4.2122955 -4.2208467 -4.2202005 -4.2245669][-4.1168408 -4.1341758 -4.141078 -4.1372056 -4.1249008 -4.1111484 -4.1013913 -4.1068439 -4.1344552 -4.1636658 -4.1782131 -4.1977668 -4.209404 -4.20402 -4.206975][-4.1082625 -4.1291251 -4.1357884 -4.1281557 -4.1038375 -4.0788178 -4.0616331 -4.0676656 -4.1040807 -4.1438527 -4.1675305 -4.1937628 -4.204433 -4.19616 -4.1991787][-4.1113586 -4.1371012 -4.1427703 -4.1269507 -4.09114 -4.0506229 -4.02149 -4.0269914 -4.0726547 -4.1250305 -4.1540833 -4.1797047 -4.1926837 -4.189147 -4.1954727][-4.1092954 -4.1362977 -4.1408567 -4.12059 -4.0732903 -4.0107341 -3.9559531 -3.9552765 -4.0194764 -4.0933104 -4.1347361 -4.1634283 -4.1805391 -4.1846161 -4.1916523][-4.0939503 -4.112649 -4.11442 -4.0917878 -4.032517 -3.9348316 -3.8380694 -3.839541 -3.9454997 -4.0513163 -4.1132369 -4.1492305 -4.17317 -4.1840067 -4.1895628][-4.0684924 -4.0706172 -4.0633659 -4.0367236 -3.9662974 -3.8292565 -3.6854877 -3.6999974 -3.8715816 -4.0196991 -4.1034765 -4.1403089 -4.1642356 -4.1788564 -4.1829042][-4.0697985 -4.0628538 -4.0511637 -4.0153074 -3.9424813 -3.8003674 -3.6491382 -3.6808708 -3.87381 -4.0294719 -4.1092072 -4.140089 -4.1584415 -4.1725526 -4.1702967][-4.102262 -4.1010118 -4.0852237 -4.0500879 -3.9968042 -3.8989081 -3.8052628 -3.8386588 -3.9752457 -4.0862832 -4.1374741 -4.1526895 -4.1622992 -4.1678333 -4.1560235][-4.1484523 -4.15225 -4.1342254 -4.1035337 -4.0724916 -4.0219603 -3.9808202 -4.0070405 -4.0881157 -4.153965 -4.1779656 -4.1790314 -4.1787052 -4.1709013 -4.1509495][-4.1757808 -4.1845808 -4.1729174 -4.1565981 -4.1406345 -4.1185956 -4.10368 -4.11552 -4.1588759 -4.1975126 -4.2085681 -4.1980972 -4.1883736 -4.1733251 -4.1525979][-4.1936593 -4.1974983 -4.1915784 -4.1863642 -4.1794214 -4.1697125 -4.1635766 -4.1669626 -4.1932459 -4.2212815 -4.2265859 -4.2110147 -4.1959863 -4.1806216 -4.1656857][-4.2180037 -4.2153544 -4.2088737 -4.2062955 -4.2027435 -4.1983733 -4.1968436 -4.1993208 -4.2173419 -4.2387209 -4.2424369 -4.2279496 -4.211452 -4.1962323 -4.1831179][-4.2508788 -4.2419257 -4.2311425 -4.2292643 -4.2283349 -4.2258468 -4.2243595 -4.2253637 -4.2364516 -4.2513251 -4.255796 -4.2459893 -4.2320342 -4.2182126 -4.2059846]]...]
INFO - root - 2017-12-05 22:36:32.929192: step 50310, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 65h:45m:21s remains)
INFO - root - 2017-12-05 22:36:41.420428: step 50320, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.851 sec/batch; 66h:41m:21s remains)
INFO - root - 2017-12-05 22:36:50.002274: step 50330, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 68h:39m:43s remains)
INFO - root - 2017-12-05 22:36:58.596385: step 50340, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.884 sec/batch; 69h:15m:28s remains)
INFO - root - 2017-12-05 22:37:07.121676: step 50350, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 66h:47m:41s remains)
INFO - root - 2017-12-05 22:37:15.695014: step 50360, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:37m:59s remains)
INFO - root - 2017-12-05 22:37:24.034209: step 50370, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 67h:52m:31s remains)
INFO - root - 2017-12-05 22:37:32.611177: step 50380, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 67h:34m:01s remains)
INFO - root - 2017-12-05 22:37:41.127103: step 50390, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.811 sec/batch; 63h:34m:44s remains)
INFO - root - 2017-12-05 22:37:49.509021: step 50400, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 67h:54m:56s remains)
2017-12-05 22:37:50.331155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1485863 -4.1570125 -4.1408663 -4.1319671 -4.1396327 -4.1497159 -4.1418171 -4.1271682 -4.1320934 -4.1379986 -4.126483 -4.1091366 -4.1155386 -4.1310592 -4.1281247][-4.1575885 -4.1847048 -4.1903634 -4.1935167 -4.2018657 -4.206274 -4.1977386 -4.1854544 -4.1852255 -4.184144 -4.1723938 -4.1594872 -4.1622639 -4.1694231 -4.1633086][-4.1759005 -4.2137675 -4.2367897 -4.2479329 -4.251646 -4.249598 -4.2436442 -4.2432356 -4.2441397 -4.2360163 -4.2264509 -4.2220345 -4.2264357 -4.2286706 -4.2218289][-4.1811647 -4.215982 -4.2414646 -4.2507443 -4.2432928 -4.2269149 -4.2197461 -4.2347503 -4.2440572 -4.2365355 -4.2296305 -4.23718 -4.2510905 -4.2561073 -4.2527165][-4.1717329 -4.1903443 -4.2013922 -4.1982551 -4.1731911 -4.1356463 -4.1298409 -4.1700583 -4.1948605 -4.1895123 -4.183372 -4.1999946 -4.2248721 -4.23685 -4.2340808][-4.1368079 -4.12958 -4.1176319 -4.0965672 -4.0463357 -3.9753118 -3.9692442 -4.0438309 -4.0908537 -4.0892129 -4.0833406 -4.1096358 -4.1489582 -4.1680942 -4.1627231][-4.095799 -4.064424 -4.0327024 -3.9973907 -3.9237342 -3.8164012 -3.8100603 -3.9195809 -3.9934218 -4.0030079 -4.00014 -4.029387 -4.0764203 -4.0997853 -4.0927773][-4.0867372 -4.0547147 -4.024425 -3.9945669 -3.9328871 -3.8456707 -3.848403 -3.9462976 -4.0165372 -4.0298133 -4.0200367 -4.0368128 -4.0729427 -4.0932317 -4.0852475][-4.1017456 -4.0822 -4.0637536 -4.0484562 -4.0185561 -3.9787147 -3.9914944 -4.0570531 -4.1041512 -4.1079 -4.0899944 -4.0925565 -4.1114335 -4.120914 -4.11224][-4.1365213 -4.1294475 -4.12299 -4.1194825 -4.1122136 -4.1067405 -4.1242218 -4.1635547 -4.189712 -4.183187 -4.1573558 -4.1486697 -4.1556897 -4.1595092 -4.1546059][-4.1807818 -4.1813841 -4.1796985 -4.1800652 -4.1816559 -4.1893153 -4.2055678 -4.2276239 -4.2383661 -4.2268481 -4.2003779 -4.18784 -4.19304 -4.1989646 -4.2012444][-4.21493 -4.2168427 -4.2146788 -4.2149696 -4.2195792 -4.2295332 -4.2430248 -4.2535338 -4.2541647 -4.2426414 -4.2227893 -4.2145667 -4.2216544 -4.2293921 -4.2327313][-4.2348833 -4.2389235 -4.2392659 -4.2392683 -4.2414217 -4.24669 -4.2536726 -4.2552781 -4.2478385 -4.2378907 -4.2276626 -4.2268162 -4.2353764 -4.2407656 -4.2430258][-4.2441773 -4.2507811 -4.255105 -4.2574854 -4.2564821 -4.2546926 -4.2540879 -4.250524 -4.2396078 -4.2307062 -4.2263794 -4.2290859 -4.2366171 -4.2411156 -4.2443876][-4.2492638 -4.2574124 -4.2641816 -4.2677135 -4.2662258 -4.2608109 -4.255815 -4.249825 -4.2402363 -4.2331748 -4.2313919 -4.2340503 -4.2382417 -4.2404861 -4.243278]]...]
INFO - root - 2017-12-05 22:37:58.871382: step 50410, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 65h:48m:49s remains)
INFO - root - 2017-12-05 22:38:07.374122: step 50420, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 65h:06m:41s remains)
INFO - root - 2017-12-05 22:38:15.962512: step 50430, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 68h:50m:06s remains)
INFO - root - 2017-12-05 22:38:24.463448: step 50440, loss = 2.06, batch loss = 2.01 (9.9 examples/sec; 0.812 sec/batch; 63h:36m:40s remains)
INFO - root - 2017-12-05 22:38:32.942295: step 50450, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 66h:12m:20s remains)
INFO - root - 2017-12-05 22:38:41.626295: step 50460, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 68h:18m:58s remains)
INFO - root - 2017-12-05 22:38:50.227237: step 50470, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 67h:20m:06s remains)
INFO - root - 2017-12-05 22:38:58.698956: step 50480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 68h:14m:01s remains)
INFO - root - 2017-12-05 22:39:07.334279: step 50490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 67h:22m:18s remains)
INFO - root - 2017-12-05 22:39:15.832860: step 50500, loss = 2.07, batch loss = 2.01 (10.4 examples/sec; 0.767 sec/batch; 60h:03m:33s remains)
2017-12-05 22:39:16.586320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3120766 -4.3046188 -4.2995863 -4.2975106 -4.2985435 -4.302814 -4.3077955 -4.3113437 -4.3133674 -4.3157635 -4.3178821 -4.3201547 -4.3224044 -4.3229012 -4.3233643][-4.3017011 -4.2897296 -4.2804675 -4.2714033 -4.26831 -4.2717991 -4.2785764 -4.2874789 -4.2922177 -4.296576 -4.2998738 -4.3040152 -4.306963 -4.3064094 -4.3054361][-4.2797627 -4.2630949 -4.2509542 -4.2347517 -4.2245612 -4.2248883 -4.2348509 -4.2535872 -4.261951 -4.2670131 -4.2720094 -4.2797871 -4.2857275 -4.2856135 -4.2837281][-4.2436981 -4.2128806 -4.1913757 -4.1634493 -4.1447878 -4.1471686 -4.1678462 -4.1985874 -4.2096643 -4.2152991 -4.22502 -4.2430425 -4.25493 -4.259151 -4.2615061][-4.2074394 -4.1565623 -4.1169086 -4.0657692 -4.0343361 -4.0483918 -4.0920181 -4.1337218 -4.141283 -4.1408772 -4.1572413 -4.1920176 -4.2169948 -4.2311869 -4.2419653][-4.186625 -4.1192536 -4.0632329 -3.9946065 -3.9526191 -3.9718041 -4.03616 -4.0843868 -4.0799456 -4.0684934 -4.0860186 -4.1365075 -4.1754951 -4.1999989 -4.2219582][-4.1900287 -4.1160126 -4.0517073 -3.984529 -3.9434958 -3.9579217 -4.0250134 -4.0643067 -4.038096 -4.011117 -4.0287986 -4.0923061 -4.14201 -4.1745563 -4.2083168][-4.2089782 -4.1383786 -4.0740366 -4.0186219 -3.9863009 -3.9984598 -4.0529027 -4.0697837 -4.0189619 -3.9729149 -3.9884512 -4.0627885 -4.1250668 -4.163321 -4.1974831][-4.2281613 -4.1679344 -4.1113153 -4.0672607 -4.0404911 -4.0491643 -4.0833893 -4.0786304 -4.01582 -3.9612997 -3.9752657 -4.0549674 -4.1274114 -4.168839 -4.1976418][-4.2420158 -4.1979289 -4.1552687 -4.125639 -4.1112652 -4.118432 -4.1299 -4.1122012 -4.04889 -3.9950509 -4.0030265 -4.0767283 -4.1486425 -4.1877847 -4.2142062][-4.2597575 -4.2293477 -4.2002034 -4.1853552 -4.1852694 -4.1873679 -4.1773052 -4.1473179 -4.0937977 -4.0484686 -4.0497766 -4.1074505 -4.1678109 -4.2010503 -4.22749][-4.27817 -4.2554169 -4.2341161 -4.2257857 -4.2288032 -4.2250252 -4.1996584 -4.1578722 -4.1124787 -4.0796738 -4.083746 -4.130229 -4.1785693 -4.2061505 -4.2325454][-4.2977877 -4.2784805 -4.2585783 -4.2470064 -4.2442737 -4.2337289 -4.1989646 -4.1517015 -4.1101747 -4.0889206 -4.1014338 -4.1458368 -4.193007 -4.217741 -4.2359219][-4.3186474 -4.3028126 -4.2810273 -4.2622375 -4.2523756 -4.2395453 -4.2074137 -4.1657906 -4.1302619 -4.1173792 -4.1345797 -4.1746526 -4.2198019 -4.2399116 -4.2487297][-4.3321643 -4.3196864 -4.2995358 -4.2801375 -4.2704649 -4.2612858 -4.23672 -4.2049141 -4.1786928 -4.1754937 -4.1952457 -4.2271471 -4.2619476 -4.273005 -4.2741809]]...]
INFO - root - 2017-12-05 22:39:25.116869: step 50510, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 65h:28m:28s remains)
INFO - root - 2017-12-05 22:39:33.689832: step 50520, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 64h:45m:23s remains)
INFO - root - 2017-12-05 22:39:42.246654: step 50530, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 67h:18m:41s remains)
INFO - root - 2017-12-05 22:39:50.816430: step 50540, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 67h:08m:29s remains)
INFO - root - 2017-12-05 22:39:59.262253: step 50550, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 66h:57m:14s remains)
INFO - root - 2017-12-05 22:40:07.854328: step 50560, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 67h:20m:15s remains)
INFO - root - 2017-12-05 22:40:16.471244: step 50570, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 67h:50m:17s remains)
INFO - root - 2017-12-05 22:40:25.071198: step 50580, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 67h:32m:46s remains)
INFO - root - 2017-12-05 22:40:33.576606: step 50590, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 68h:12m:15s remains)
INFO - root - 2017-12-05 22:40:42.082417: step 50600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:50m:34s remains)
2017-12-05 22:40:42.889145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2226472 -4.2179303 -4.2093096 -4.2005486 -4.1945977 -4.1906071 -4.1864672 -4.1860394 -4.1904707 -4.1972766 -4.1999936 -4.1959009 -4.1869159 -4.1746378 -4.1631608][-4.2399516 -4.2330155 -4.221365 -4.2089796 -4.2003226 -4.194674 -4.1895547 -4.1871552 -4.1870122 -4.1881495 -4.1878405 -4.1836939 -4.1756859 -4.1627131 -4.1484585][-4.2519789 -4.2414265 -4.2251124 -4.208137 -4.1966619 -4.1896992 -4.1819587 -4.1731863 -4.1646252 -4.1582165 -4.1575232 -4.1571641 -4.1525846 -4.1425819 -4.13124][-4.2556982 -4.2370248 -4.2105236 -4.1856523 -4.1706147 -4.1627469 -4.1532321 -4.138669 -4.1220508 -4.1104131 -4.1142921 -4.1241894 -4.1268187 -4.1215515 -4.1139927][-4.2380934 -4.2083621 -4.1690283 -4.1341424 -4.116046 -4.11405 -4.1156092 -4.1066895 -4.0883374 -4.0745916 -4.0849848 -4.1064715 -4.1157289 -4.1132774 -4.104073][-4.2003527 -4.1621294 -4.1090975 -4.060914 -4.0422106 -4.0573363 -4.0851216 -4.09452 -4.0805173 -4.0662045 -4.0808449 -4.1102877 -4.1219535 -4.11165 -4.0889783][-4.1574216 -4.116281 -4.0551033 -3.9967744 -3.9807942 -4.0169835 -4.0729847 -4.1008677 -4.0914211 -4.0781 -4.0952291 -4.1237574 -4.1301031 -4.1083984 -4.0714755][-4.129745 -4.0961514 -4.0413432 -3.9857643 -3.9730854 -4.0125875 -4.075222 -4.1136651 -4.11471 -4.1066341 -4.1195173 -4.1393447 -4.140749 -4.118216 -4.0826645][-4.1213078 -4.1019797 -4.0701656 -4.0330205 -4.0220141 -4.0474033 -4.0945988 -4.1318669 -4.1417561 -4.1391497 -4.1452713 -4.1539249 -4.1495109 -4.1327291 -4.1151571][-4.1229568 -4.1146197 -4.1067991 -4.092658 -4.0845089 -4.0971794 -4.1235218 -4.150178 -4.1635938 -4.164937 -4.1625075 -4.1573548 -4.1467447 -4.1375575 -4.1410685][-4.1281366 -4.1266341 -4.1342673 -4.1371307 -4.1358471 -4.1417031 -4.1532168 -4.166008 -4.1758909 -4.1779065 -4.1685762 -4.1542506 -4.1423659 -4.1455269 -4.1672745][-4.1385908 -4.1418066 -4.157433 -4.1698394 -4.1744442 -4.178122 -4.1806731 -4.1825662 -4.1843209 -4.1829133 -4.1692791 -4.1542072 -4.1485734 -4.1611891 -4.187449][-4.1543179 -4.1608214 -4.1781235 -4.1917763 -4.1992736 -4.2062888 -4.2045636 -4.197886 -4.1887116 -4.18206 -4.1688566 -4.1580515 -4.1571517 -4.1716671 -4.19473][-4.1693535 -4.1785913 -4.193821 -4.20499 -4.213933 -4.2214084 -4.2188058 -4.2081966 -4.1931372 -4.1808863 -4.1665106 -4.1575174 -4.15933 -4.1748672 -4.1984692][-4.1827435 -4.1949973 -4.2089972 -4.2178278 -4.2236581 -4.227809 -4.2269125 -4.2181206 -4.2016988 -4.1842785 -4.1686206 -4.1617618 -4.1694822 -4.1882186 -4.210597]]...]
INFO - root - 2017-12-05 22:40:51.426626: step 50610, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 65h:46m:20s remains)
INFO - root - 2017-12-05 22:40:59.964094: step 50620, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 67h:42m:15s remains)
INFO - root - 2017-12-05 22:41:08.530726: step 50630, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.866 sec/batch; 67h:46m:59s remains)
INFO - root - 2017-12-05 22:41:17.063352: step 50640, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.826 sec/batch; 64h:38m:49s remains)
INFO - root - 2017-12-05 22:41:25.557646: step 50650, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 67h:45m:03s remains)
INFO - root - 2017-12-05 22:41:34.182826: step 50660, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 66h:51m:02s remains)
INFO - root - 2017-12-05 22:41:42.768158: step 50670, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:35m:48s remains)
INFO - root - 2017-12-05 22:41:51.330801: step 50680, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 67h:40m:41s remains)
INFO - root - 2017-12-05 22:41:59.934064: step 50690, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 65h:24m:20s remains)
INFO - root - 2017-12-05 22:42:08.425302: step 50700, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.871 sec/batch; 68h:10m:11s remains)
2017-12-05 22:42:09.165460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0906782 -4.1335049 -4.1621451 -4.1820116 -4.1949811 -4.2001414 -4.1952763 -4.1927071 -4.196208 -4.2035275 -4.2018609 -4.1931767 -4.1780777 -4.1512628 -4.1070561][-4.0829382 -4.104249 -4.1135888 -4.1251049 -4.1426654 -4.1565485 -4.1581416 -4.1623979 -4.1748085 -4.1916008 -4.1966228 -4.1926227 -4.1764736 -4.1408162 -4.0852289][-4.0716243 -4.0749059 -4.0715923 -4.078547 -4.0959325 -4.1096277 -4.1105042 -4.1185122 -4.1373825 -4.1629558 -4.1772041 -4.1836743 -4.1754122 -4.1440477 -4.0908542][-4.07983 -4.0753617 -4.0689082 -4.071568 -4.0796423 -4.0817504 -4.0770974 -4.0857067 -4.1091609 -4.1421957 -4.1674109 -4.1843629 -4.1891193 -4.1740685 -4.13756][-4.1237669 -4.1190233 -4.1141024 -4.1110239 -4.10286 -4.0861487 -4.0698771 -4.070941 -4.0914907 -4.1269994 -4.1637073 -4.1920991 -4.2111015 -4.2116451 -4.18972][-4.1805596 -4.1783938 -4.171349 -4.1608043 -4.1369929 -4.0998454 -4.0672145 -4.0565791 -4.0732713 -4.1099443 -4.152925 -4.1868749 -4.213573 -4.2238727 -4.2161098][-4.2179813 -4.2164021 -4.2058558 -4.1889248 -4.1572757 -4.1101933 -4.0655661 -4.0415592 -4.0511618 -4.0816851 -4.1249785 -4.1649709 -4.1974463 -4.2163763 -4.2232733][-4.2326856 -4.2324772 -4.22039 -4.2019396 -4.1715169 -4.1238165 -4.074264 -4.0429153 -4.0448303 -4.0653248 -4.1048374 -4.1515503 -4.1917205 -4.2165947 -4.2305503][-4.2389154 -4.2392964 -4.2271595 -4.2102542 -4.1818709 -4.1369877 -4.0900993 -4.061214 -4.0589719 -4.0737863 -4.1112738 -4.1631279 -4.2066436 -4.2335448 -4.247273][-4.2465158 -4.2458334 -4.2334452 -4.2156153 -4.1854324 -4.1431627 -4.1035309 -4.0813656 -4.0791388 -4.0937624 -4.1308494 -4.18085 -4.2207613 -4.2437873 -4.2551827][-4.2491431 -4.2457085 -4.2319446 -4.20866 -4.1720233 -4.1288252 -4.0929756 -4.07806 -4.0829916 -4.1031327 -4.1410513 -4.1835151 -4.2165785 -4.2364912 -4.2472849][-4.2408619 -4.2330079 -4.214499 -4.1843929 -4.1413555 -4.0991907 -4.0673895 -4.057178 -4.0660291 -4.0897613 -4.1289206 -4.1671171 -4.1952643 -4.2153211 -4.2288685][-4.2180591 -4.2072721 -4.184679 -4.1508431 -4.1057177 -4.06595 -4.036294 -4.029314 -4.0429592 -4.0710788 -4.1109562 -4.1442924 -4.1664219 -4.1851234 -4.2003284][-4.1817026 -4.1715021 -4.1488385 -4.1153951 -4.0712094 -4.03182 -4.0043693 -4.0049844 -4.0267277 -4.0579529 -4.0936632 -4.1185045 -4.1334596 -4.1483507 -4.1621766][-4.1437664 -4.1370783 -4.1180234 -4.0891857 -4.0510631 -4.0161443 -3.9971619 -4.0061035 -4.030992 -4.058219 -4.083941 -4.0994077 -4.1073356 -4.1169987 -4.1280427]]...]
INFO - root - 2017-12-05 22:42:17.802022: step 50710, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 67h:17m:51s remains)
INFO - root - 2017-12-05 22:42:26.259346: step 50720, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 68h:22m:09s remains)
INFO - root - 2017-12-05 22:42:34.852889: step 50730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 67h:38m:00s remains)
INFO - root - 2017-12-05 22:42:43.359715: step 50740, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 67h:04m:44s remains)
INFO - root - 2017-12-05 22:42:51.747324: step 50750, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 65h:58m:07s remains)
INFO - root - 2017-12-05 22:43:00.237114: step 50760, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 63h:21m:04s remains)
INFO - root - 2017-12-05 22:43:08.874769: step 50770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:32m:08s remains)
INFO - root - 2017-12-05 22:43:17.401616: step 50780, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.835 sec/batch; 65h:21m:13s remains)
INFO - root - 2017-12-05 22:43:25.938756: step 50790, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 66h:00m:45s remains)
INFO - root - 2017-12-05 22:43:34.484963: step 50800, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:21m:22s remains)
2017-12-05 22:43:35.263905: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.330883 -4.316421 -4.30909 -4.3080621 -4.3103614 -4.3122048 -4.3096776 -4.3034964 -4.2976747 -4.298739 -4.3066907 -4.3137 -4.3180537 -4.3182988 -4.31348][-4.3136234 -4.2928376 -4.2806597 -4.2770038 -4.2757673 -4.2741933 -4.2704415 -4.2622547 -4.2525015 -4.2540689 -4.2680645 -4.2803321 -4.2890573 -4.2925491 -4.2844591][-4.29555 -4.2669377 -4.2488651 -4.2430463 -4.2406135 -4.2387495 -4.2346544 -4.2233043 -4.2081084 -4.2105265 -4.2319283 -4.2486305 -4.2600894 -4.2663145 -4.2564483][-4.2832861 -4.2483397 -4.2268391 -4.2195954 -4.2156534 -4.212903 -4.2068 -4.189959 -4.16924 -4.174943 -4.2044077 -4.2258582 -4.23902 -4.2473578 -4.2369318][-4.2754331 -4.2358789 -4.2135787 -4.2024984 -4.193644 -4.1870732 -4.1744547 -4.1511278 -4.1266351 -4.1364241 -4.1759052 -4.20347 -4.2204375 -4.2309561 -4.2207747][-4.2728934 -4.2334008 -4.2116289 -4.1966062 -4.1802697 -4.1624851 -4.1381421 -4.103755 -4.07643 -4.0922256 -4.1444 -4.1828451 -4.2069492 -4.2221518 -4.2149849][-4.2808843 -4.248342 -4.2281656 -4.2074456 -4.1824565 -4.1544161 -4.12057 -4.0745444 -4.0402703 -4.0598245 -4.1222878 -4.17119 -4.2047181 -4.2265868 -4.2250109][-4.2923145 -4.2679844 -4.2502394 -4.2255335 -4.1954379 -4.1611261 -4.1229296 -4.0752559 -4.0344005 -4.0455089 -4.106215 -4.1603861 -4.2011638 -4.2287326 -4.2326488][-4.2983236 -4.2780852 -4.2603083 -4.2320256 -4.1999741 -4.1614065 -4.119905 -4.0792284 -4.039813 -4.0397224 -4.0952115 -4.1539726 -4.1971445 -4.2250471 -4.2307906][-4.2978315 -4.2776423 -4.2587118 -4.2266521 -4.1933837 -4.15308 -4.1102991 -4.079361 -4.0567627 -4.0555058 -4.1026039 -4.1573668 -4.1931696 -4.2169957 -4.2222857][-4.2960954 -4.2747149 -4.2565842 -4.2269692 -4.1964226 -4.162652 -4.125658 -4.104701 -4.1015716 -4.1058016 -4.1405077 -4.1785579 -4.1993561 -4.2140193 -4.2178903][-4.2957993 -4.272016 -4.2565489 -4.2360163 -4.2142158 -4.1912351 -4.1663556 -4.1523743 -4.1567512 -4.1642237 -4.1874056 -4.2081385 -4.2142687 -4.2175817 -4.217854][-4.2972965 -4.2715406 -4.2593665 -4.249197 -4.2383647 -4.2253408 -4.2095618 -4.2008462 -4.2078886 -4.2166009 -4.2306261 -4.2401142 -4.2387028 -4.2352304 -4.2322631][-4.3086252 -4.2831078 -4.2733445 -4.2698851 -4.2662835 -4.2603784 -4.251698 -4.24883 -4.2567463 -4.2668672 -4.2788463 -4.2842479 -4.2833605 -4.2779512 -4.2716093][-4.3263216 -4.306663 -4.2999206 -4.2999921 -4.3002815 -4.3004708 -4.298614 -4.2989507 -4.3053937 -4.3135581 -4.3209419 -4.3235765 -4.3235173 -4.3216958 -4.31758]]...]
INFO - root - 2017-12-05 22:43:43.438252: step 50810, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.829 sec/batch; 64h:50m:54s remains)
INFO - root - 2017-12-05 22:43:51.921290: step 50820, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 67h:00m:54s remains)
INFO - root - 2017-12-05 22:44:00.450019: step 50830, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 64h:30m:18s remains)
INFO - root - 2017-12-05 22:44:08.978421: step 50840, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:31m:34s remains)
INFO - root - 2017-12-05 22:44:17.408080: step 50850, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 68h:03m:14s remains)
INFO - root - 2017-12-05 22:44:25.967486: step 50860, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 65h:38m:28s remains)
INFO - root - 2017-12-05 22:44:34.537963: step 50870, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 66h:46m:04s remains)
INFO - root - 2017-12-05 22:44:43.083005: step 50880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 68h:16m:17s remains)
INFO - root - 2017-12-05 22:44:51.680037: step 50890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 66h:37m:18s remains)
INFO - root - 2017-12-05 22:45:00.161745: step 50900, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:30m:32s remains)
2017-12-05 22:45:00.942586: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.238626 -4.2113981 -4.1971221 -4.1967244 -4.1950788 -4.1903467 -4.1869946 -4.1836681 -4.172823 -4.1667271 -4.1622086 -4.1513186 -4.1514568 -4.1559567 -4.1562619][-4.2241526 -4.1941962 -4.178369 -4.1805773 -4.1831708 -4.1865535 -4.1936855 -4.1973686 -4.1863894 -4.1802306 -4.1726747 -4.1551962 -4.1569209 -4.1658568 -4.1652346][-4.2082996 -4.1788173 -4.1682792 -4.1775069 -4.1819243 -4.1836257 -4.188508 -4.1895995 -4.1802473 -4.1797194 -4.1730475 -4.1534715 -4.1565304 -4.1687288 -4.165864][-4.1989174 -4.1755071 -4.175981 -4.1882143 -4.1839519 -4.1729021 -4.1673059 -4.1620545 -4.1570935 -4.1640215 -4.1629028 -4.1451025 -4.1493669 -4.1671929 -4.1699233][-4.2012568 -4.183485 -4.1893382 -4.1939864 -4.1745787 -4.1464186 -4.1270523 -4.1166883 -4.1224194 -4.13956 -4.1473408 -4.1362081 -4.13969 -4.1632223 -4.1764479][-4.1966243 -4.1724691 -4.1712995 -4.1649532 -4.1348095 -4.0940027 -4.0630732 -4.049644 -4.0699096 -4.1019745 -4.1225057 -4.1240187 -4.1327014 -4.1584854 -4.1804695][-4.1622267 -4.125154 -4.1095977 -4.0919881 -4.0556808 -4.0038462 -3.961092 -3.9474595 -3.985553 -4.0351815 -4.0679502 -4.0844865 -4.109426 -4.1443539 -4.1759362][-4.11575 -4.0707674 -4.0420146 -4.0154643 -3.9778481 -3.9136004 -3.8567181 -3.8483863 -3.9079876 -3.9736781 -4.0162196 -4.046587 -4.09206 -4.1379204 -4.1696734][-4.0974393 -4.0508313 -4.0183692 -3.9969656 -3.9771671 -3.9284697 -3.884762 -3.8905497 -3.9503376 -4.0082498 -4.0423536 -4.0702834 -4.1183071 -4.1603551 -4.1794972][-4.1054411 -4.0661 -4.0428076 -4.0399389 -4.0507326 -4.0346122 -4.0143471 -4.0208979 -4.0582128 -4.0916061 -4.1119647 -4.1333265 -4.1709857 -4.1974249 -4.2011409][-4.1113133 -4.0759511 -4.0631514 -4.0792694 -4.1128755 -4.1204724 -4.1121163 -4.1154609 -4.1346383 -4.1520381 -4.1669779 -4.1863713 -4.2129769 -4.2233605 -4.2144308][-4.0988965 -4.0574369 -4.04542 -4.0780611 -4.1308956 -4.1530418 -4.1511893 -4.1574607 -4.1707969 -4.1859221 -4.2009015 -4.2176151 -4.2335691 -4.2328157 -4.2181416][-4.0833578 -4.0380559 -4.0264406 -4.069098 -4.1324816 -4.1638808 -4.1699982 -4.1820383 -4.1980023 -4.2140665 -4.2253127 -4.235672 -4.2412896 -4.2342839 -4.2184772][-4.0969667 -4.0592937 -4.0489254 -4.0885019 -4.1484985 -4.1819763 -4.1919079 -4.2070556 -4.222548 -4.2336326 -4.2385821 -4.2412786 -4.2390375 -4.2270026 -4.2109013][-4.1297445 -4.0994644 -4.0870438 -4.1180959 -4.1685133 -4.1998839 -4.2108641 -4.2248673 -4.236814 -4.24164 -4.2411151 -4.2388868 -4.2313447 -4.2177563 -4.2052107]]...]
INFO - root - 2017-12-05 22:45:09.507076: step 50910, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 67h:18m:20s remains)
INFO - root - 2017-12-05 22:45:17.889412: step 50920, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 67h:14m:25s remains)
INFO - root - 2017-12-05 22:45:26.342835: step 50930, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 66h:54m:40s remains)
INFO - root - 2017-12-05 22:45:34.813008: step 50940, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 65h:59m:49s remains)
INFO - root - 2017-12-05 22:45:43.240931: step 50950, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 65h:35m:34s remains)
INFO - root - 2017-12-05 22:45:51.805775: step 50960, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 67h:32m:58s remains)
INFO - root - 2017-12-05 22:46:00.371031: step 50970, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.823 sec/batch; 64h:20m:25s remains)
INFO - root - 2017-12-05 22:46:08.886026: step 50980, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 65h:51m:56s remains)
INFO - root - 2017-12-05 22:46:17.359367: step 50990, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 65h:18m:46s remains)
INFO - root - 2017-12-05 22:46:25.875138: step 51000, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.824 sec/batch; 64h:24m:23s remains)
2017-12-05 22:46:26.635262: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.176827 -4.1578488 -4.1374855 -4.1290855 -4.1390123 -4.1582842 -4.16899 -4.1663313 -4.1535296 -4.1449304 -4.1502638 -4.1606436 -4.1688247 -4.1705046 -4.167356][-4.1825938 -4.1653218 -4.1461926 -4.1445842 -4.1638136 -4.1845641 -4.1895585 -4.1775417 -4.154027 -4.1370821 -4.1421738 -4.1548886 -4.1605172 -4.15959 -4.1535425][-4.1817102 -4.165493 -4.1439843 -4.150804 -4.1851406 -4.2078271 -4.203548 -4.1776252 -4.145102 -4.1236787 -4.1294627 -4.1488929 -4.1552839 -4.1553268 -4.1510468][-4.1627445 -4.1478434 -4.1241956 -4.1368408 -4.1837487 -4.2097139 -4.200572 -4.168004 -4.1303749 -4.1025991 -4.1037955 -4.1276312 -4.1393967 -4.1456156 -4.1475415][-4.1232882 -4.1121922 -4.0917645 -4.1097045 -4.164156 -4.1927824 -4.1810684 -4.149168 -4.1139708 -4.0822406 -4.0758119 -4.0983777 -4.1126819 -4.1243877 -4.1346321][-4.0795064 -4.0720553 -4.056582 -4.0765638 -4.1300783 -4.1560831 -4.1382785 -4.1104112 -4.087492 -4.0590887 -4.0493288 -4.0657382 -4.0765743 -4.0875173 -4.1044736][-4.0345035 -4.0226192 -4.0094547 -4.0326104 -4.0854197 -4.0986409 -4.0648179 -4.0398645 -4.03806 -4.0284743 -4.0224586 -4.0256257 -4.0302892 -4.0377254 -4.057992][-4.0109034 -3.9885533 -3.9695094 -3.9945571 -4.0435495 -4.0359364 -3.9896488 -3.9723382 -3.9940779 -4.006216 -4.0042529 -3.9932303 -3.9924459 -3.998318 -4.0196152][-4.0355258 -4.004529 -3.9780076 -3.9976807 -4.0388079 -4.023108 -3.9813685 -3.9732959 -4.0073166 -4.030127 -4.0270147 -4.0088344 -3.9986737 -3.9992363 -4.0207529][-4.0728807 -4.0422454 -4.0213523 -4.0389457 -4.071022 -4.0580387 -4.0239897 -4.015717 -4.0499792 -4.0781207 -4.0753207 -4.0589962 -4.0413704 -4.034411 -4.0534492][-4.1138477 -4.0893264 -4.0768452 -4.0892978 -4.1141438 -4.1039209 -4.0745964 -4.0656757 -4.0996394 -4.1338296 -4.1354141 -4.11989 -4.0995874 -4.0874457 -4.1048861][-4.1639252 -4.1476831 -4.1416831 -4.1518674 -4.1710219 -4.1624122 -4.1400666 -4.1329255 -4.1620388 -4.19469 -4.2035718 -4.1945062 -4.1773467 -4.1652 -4.1762486][-4.2132645 -4.2038407 -4.1996565 -4.20776 -4.2214217 -4.2165346 -4.2014256 -4.1989379 -4.2183371 -4.2464576 -4.26013 -4.2564206 -4.2425022 -4.2331185 -4.238615][-4.2471528 -4.2410173 -4.2373085 -4.2417121 -4.2507114 -4.2496076 -4.241343 -4.2419062 -4.2555966 -4.2771354 -4.2895632 -4.2880878 -4.2759209 -4.2677236 -4.2699189][-4.2594891 -4.2577677 -4.255919 -4.2577853 -4.2640076 -4.2662683 -4.2630172 -4.2672014 -4.2798882 -4.293396 -4.2994289 -4.2984929 -4.2892714 -4.2824554 -4.2825441]]...]
INFO - root - 2017-12-05 22:46:35.212512: step 51010, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 69h:17m:26s remains)
INFO - root - 2017-12-05 22:46:43.804535: step 51020, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 66h:17m:08s remains)
INFO - root - 2017-12-05 22:46:52.172517: step 51030, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 68h:22m:36s remains)
INFO - root - 2017-12-05 22:47:00.567323: step 51040, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.851 sec/batch; 66h:33m:38s remains)
INFO - root - 2017-12-05 22:47:09.042145: step 51050, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 66h:52m:42s remains)
INFO - root - 2017-12-05 22:47:17.572943: step 51060, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 67h:52m:17s remains)
INFO - root - 2017-12-05 22:47:26.136398: step 51070, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.843 sec/batch; 65h:55m:11s remains)
INFO - root - 2017-12-05 22:47:34.719020: step 51080, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.839 sec/batch; 65h:34m:03s remains)
INFO - root - 2017-12-05 22:47:43.273158: step 51090, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:54m:26s remains)
INFO - root - 2017-12-05 22:47:51.789162: step 51100, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 68h:30m:58s remains)
2017-12-05 22:47:52.585445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2252846 -4.2434254 -4.2587943 -4.2630563 -4.244638 -4.2015324 -4.1472278 -4.1021695 -4.073688 -4.0511541 -4.0412035 -4.0483804 -4.0838189 -4.1319494 -4.1779342][-4.2566156 -4.2601528 -4.2620606 -4.2604847 -4.2513118 -4.2242689 -4.1868773 -4.1631665 -4.1600652 -4.1618648 -4.1704531 -4.181519 -4.2009063 -4.2296162 -4.2576156][-4.2691121 -4.2592268 -4.2516618 -4.2488117 -4.2489572 -4.23621 -4.211647 -4.1972566 -4.2011547 -4.2098274 -4.2197323 -4.2298541 -4.2441082 -4.2701149 -4.2951274][-4.2651792 -4.2462711 -4.2285366 -4.2202621 -4.2227125 -4.2152157 -4.1934295 -4.1742053 -4.172956 -4.1829715 -4.1954942 -4.2088389 -4.2273121 -4.2627559 -4.2963185][-4.2531343 -4.2259207 -4.1980181 -4.182498 -4.1782389 -4.1644793 -4.1307979 -4.095119 -4.084137 -4.0988603 -4.1265793 -4.1555996 -4.1866636 -4.2350731 -4.2816234][-4.2454414 -4.2159467 -4.1815 -4.1533756 -4.1320815 -4.0995955 -4.0416508 -3.9825509 -3.9652414 -3.9999042 -4.0596008 -4.1153946 -4.16277 -4.2197032 -4.2709446][-4.2381096 -4.2107291 -4.1722941 -4.1290097 -4.0852318 -4.0276937 -3.9474652 -3.8718276 -3.8586326 -3.9282892 -4.0274162 -4.1086783 -4.169805 -4.2282357 -4.2761245][-4.2264719 -4.1987467 -4.1550093 -4.0991006 -4.0424695 -3.9782979 -3.9048123 -3.8435984 -3.8470669 -3.9373784 -4.0496655 -4.1343474 -4.1973419 -4.2494473 -4.2880144][-4.2131376 -4.1854196 -4.13844 -4.0787225 -4.0284848 -3.9871054 -3.9535797 -3.9345202 -3.9537792 -4.0296364 -4.11817 -4.1834254 -4.23114 -4.2693138 -4.2986393][-4.2123995 -4.1912308 -4.15176 -4.1059847 -4.0777407 -4.0676045 -4.0707316 -4.0811372 -4.1025085 -4.149087 -4.1999197 -4.2355218 -4.2614584 -4.28302 -4.3026485][-4.2344723 -4.223207 -4.1991177 -4.1753416 -4.1669955 -4.173409 -4.1893845 -4.2057018 -4.2202706 -4.24229 -4.2620811 -4.2701106 -4.2729712 -4.277657 -4.2898121][-4.255991 -4.2527728 -4.2447486 -4.2388496 -4.2400184 -4.2493553 -4.2655339 -4.2765908 -4.2821927 -4.2889395 -4.286881 -4.2717247 -4.2508855 -4.2390904 -4.246181][-4.253449 -4.2541485 -4.2582936 -4.2660084 -4.2744017 -4.2848878 -4.2974925 -4.303256 -4.3029757 -4.3003697 -4.2802348 -4.2405243 -4.1957941 -4.1695929 -4.1767797][-4.2326779 -4.233233 -4.2462897 -4.2653337 -4.2811522 -4.291554 -4.2995539 -4.2986507 -4.2918425 -4.2811432 -4.2462559 -4.1867909 -4.1250811 -4.0904541 -4.1034923][-4.21819 -4.2169733 -4.23231 -4.2548027 -4.2686839 -4.2728791 -4.2711511 -4.2618346 -4.2492652 -4.236196 -4.1988592 -4.1334472 -4.0682182 -4.0331779 -4.0491281]]...]
INFO - root - 2017-12-05 22:48:01.108583: step 51110, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:28m:08s remains)
INFO - root - 2017-12-05 22:48:09.620437: step 51120, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 67h:16m:25s remains)
INFO - root - 2017-12-05 22:48:18.059505: step 51130, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 64h:16m:51s remains)
INFO - root - 2017-12-05 22:48:26.420998: step 51140, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 67h:36m:22s remains)
INFO - root - 2017-12-05 22:48:34.963541: step 51150, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 66h:47m:54s remains)
INFO - root - 2017-12-05 22:48:43.497269: step 51160, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 65h:51m:12s remains)
INFO - root - 2017-12-05 22:48:52.073310: step 51170, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 67h:17m:23s remains)
INFO - root - 2017-12-05 22:49:00.524990: step 51180, loss = 2.12, batch loss = 2.06 (9.8 examples/sec; 0.820 sec/batch; 64h:04m:21s remains)
INFO - root - 2017-12-05 22:49:08.965704: step 51190, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 64h:34m:29s remains)
INFO - root - 2017-12-05 22:49:17.509305: step 51200, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 68h:10m:33s remains)
2017-12-05 22:49:18.275535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3013377 -4.3005948 -4.2985921 -4.2969289 -4.2949562 -4.2933903 -4.29296 -4.2935529 -4.2952704 -4.2974362 -4.29927 -4.2992234 -4.2966723 -4.2935429 -4.2925987][-4.2885332 -4.2869782 -4.2854524 -4.2842107 -4.2811151 -4.2770019 -4.2744551 -4.2749348 -4.2782736 -4.282558 -4.2869673 -4.2886767 -4.2875376 -4.2857151 -4.28524][-4.2676411 -4.2675557 -4.2670555 -4.2646809 -4.2581158 -4.2495475 -4.2441072 -4.2457051 -4.2530489 -4.2617841 -4.2705822 -4.274744 -4.2746911 -4.2731867 -4.2727051][-4.2533951 -4.2536631 -4.2526989 -4.2470169 -4.2342706 -4.220048 -4.2117867 -4.2165885 -4.2300024 -4.2448788 -4.2573714 -4.2629051 -4.2623425 -4.2586837 -4.255981][-4.2395339 -4.23792 -4.2341204 -4.2233176 -4.2030115 -4.1811914 -4.1698008 -4.1799212 -4.20404 -4.2277355 -4.2438807 -4.2506065 -4.2482491 -4.2390509 -4.2286277][-4.2152443 -4.2106752 -4.2001171 -4.1805048 -4.1495523 -4.1178484 -4.1024079 -4.1201134 -4.1596961 -4.1977139 -4.2232332 -4.233861 -4.2286372 -4.2115993 -4.1907372][-4.1887813 -4.1803861 -4.160183 -4.1284466 -4.0848179 -4.038764 -4.0145893 -4.0398765 -4.0999136 -4.15821 -4.1987228 -4.2173061 -4.2128015 -4.1904778 -4.1591377][-4.1669216 -4.1551557 -4.1282353 -4.088047 -4.0314169 -3.9669402 -3.9277785 -3.9592316 -4.0406241 -4.1180906 -4.1738997 -4.2022943 -4.2012897 -4.1753922 -4.1339049][-4.1730418 -4.1626287 -4.1391735 -4.1001048 -4.0382142 -3.9635067 -3.9180369 -3.952316 -4.0386672 -4.1180334 -4.1752582 -4.2044744 -4.2058916 -4.1788621 -4.1310005][-4.1869469 -4.1839981 -4.1743879 -4.1479459 -4.0935822 -4.0268912 -3.9905856 -4.0231533 -4.0953484 -4.1563468 -4.2000151 -4.2222452 -4.2208648 -4.1953087 -4.1485081][-4.1817231 -4.19154 -4.19859 -4.1860056 -4.142725 -4.0899925 -4.0650358 -4.0921712 -4.1457767 -4.1885624 -4.2203016 -4.2394028 -4.2382627 -4.2180815 -4.1782455][-4.1561842 -4.1751223 -4.1954193 -4.1937122 -4.1605158 -4.117784 -4.0979671 -4.1186452 -4.1619844 -4.198801 -4.2304096 -4.2545786 -4.2590709 -4.2470632 -4.2136421][-4.1186724 -4.1403675 -4.1698928 -4.1794062 -4.1611495 -4.1300783 -4.1085844 -4.1183338 -4.1548743 -4.1934156 -4.2297726 -4.2596226 -4.2710705 -4.2665977 -4.2381616][-4.090239 -4.1134977 -4.1484003 -4.169476 -4.1724577 -4.1587548 -4.138083 -4.1365829 -4.1617823 -4.1933246 -4.2270155 -4.2570176 -4.2730036 -4.2730374 -4.2508597][-4.0942178 -4.1148787 -4.1467438 -4.1716452 -4.1909366 -4.1969137 -4.1856151 -4.175879 -4.1863031 -4.2037106 -4.2261286 -4.2515888 -4.2671504 -4.2692904 -4.2563372]]...]
INFO - root - 2017-12-05 22:49:26.832536: step 51210, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 67h:41m:40s remains)
INFO - root - 2017-12-05 22:49:35.395231: step 51220, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 68h:07m:43s remains)
INFO - root - 2017-12-05 22:49:43.872646: step 51230, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 64h:06m:39s remains)
INFO - root - 2017-12-05 22:49:52.314533: step 51240, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 65h:13m:21s remains)
INFO - root - 2017-12-05 22:50:00.482053: step 51250, loss = 2.11, batch loss = 2.05 (9.9 examples/sec; 0.811 sec/batch; 63h:23m:00s remains)
INFO - root - 2017-12-05 22:50:08.968032: step 51260, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 67h:55m:40s remains)
INFO - root - 2017-12-05 22:50:17.444846: step 51270, loss = 2.02, batch loss = 1.97 (9.4 examples/sec; 0.850 sec/batch; 66h:25m:49s remains)
INFO - root - 2017-12-05 22:50:26.039498: step 51280, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 66h:20m:17s remains)
INFO - root - 2017-12-05 22:50:34.589694: step 51290, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 66h:18m:26s remains)
INFO - root - 2017-12-05 22:50:43.119467: step 51300, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 68h:00m:12s remains)
2017-12-05 22:50:43.989425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1873126 -4.1598148 -4.1450729 -4.1469297 -4.1468811 -4.1384964 -4.1226635 -4.1081786 -4.0902829 -4.0676508 -4.0453434 -4.0303092 -4.0266314 -4.0449042 -4.08964][-4.1710653 -4.1366458 -4.1278529 -4.1341982 -4.1394224 -4.1415095 -4.1361055 -4.1344557 -4.13322 -4.119926 -4.0951347 -4.0683069 -4.0489573 -4.0530128 -4.084775][-4.150979 -4.1139245 -4.11489 -4.1248522 -4.1298862 -4.1361432 -4.1387458 -4.1428585 -4.1457958 -4.1413608 -4.12581 -4.1024985 -4.0765886 -4.0699368 -4.0889091][-4.1411691 -4.1081009 -4.1133981 -4.1199017 -4.1203489 -4.1240735 -4.1250572 -4.1200032 -4.1195779 -4.1319065 -4.1315846 -4.1190619 -4.0976949 -4.087358 -4.0905538][-4.1501794 -4.1290274 -4.131412 -4.1251822 -4.1118393 -4.1022735 -4.0828485 -4.054493 -4.0476127 -4.0818548 -4.1100907 -4.1215305 -4.1152163 -4.1025128 -4.0926285][-4.16728 -4.1673503 -4.1679759 -4.1459165 -4.1108351 -4.076858 -4.022481 -3.9504597 -3.9277325 -3.9956298 -4.06717 -4.1086059 -4.1225777 -4.1191626 -4.1073623][-4.186717 -4.20079 -4.1970243 -4.1627517 -4.108912 -4.0443931 -3.9528065 -3.8344107 -3.7891376 -3.8947237 -4.0155921 -4.0857043 -4.1206594 -4.1307411 -4.1207361][-4.2114639 -4.2259312 -4.2115235 -4.17083 -4.1094627 -4.0294771 -3.9252448 -3.7961545 -3.7441306 -3.8577018 -3.9872551 -4.0612888 -4.0981884 -4.114852 -4.1081448][-4.2177539 -4.2262344 -4.2031012 -4.1617842 -4.111793 -4.0528059 -3.9835653 -3.8984222 -3.8653042 -3.9388301 -4.0203753 -4.0604372 -4.0733185 -4.0824256 -4.0795717][-4.1944542 -4.1888857 -4.1566358 -4.1238594 -4.1019974 -4.0864382 -4.0721035 -4.0423727 -4.0275526 -4.0588832 -4.0858579 -4.084949 -4.0631704 -4.0512452 -4.0442863][-4.1569061 -4.1338229 -4.0998969 -4.0792851 -4.0859418 -4.1125975 -4.140213 -4.1489024 -4.1513648 -4.1554542 -4.1480002 -4.1213012 -4.0737472 -4.0356503 -4.0129457][-4.1355481 -4.1072965 -4.0782652 -4.0714812 -4.0935154 -4.1367965 -4.1789222 -4.2015481 -4.2116566 -4.2061887 -4.1850023 -4.1499181 -4.0970626 -4.0451517 -4.00484][-4.1408787 -4.1149955 -4.0953965 -4.1010733 -4.128016 -4.1655016 -4.2013741 -4.2199392 -4.2232394 -4.2091794 -4.1857471 -4.15454 -4.11528 -4.067039 -4.0220284][-4.159019 -4.1399894 -4.1288238 -4.1358538 -4.1542277 -4.1764493 -4.1961036 -4.2041693 -4.1992974 -4.1788206 -4.15455 -4.132937 -4.1104074 -4.0794244 -4.0517268][-4.1818147 -4.1683993 -4.1626887 -4.1635261 -4.16806 -4.1722994 -4.1713634 -4.1678338 -4.1591845 -4.13874 -4.1171613 -4.1100488 -4.1127582 -4.1094694 -4.1021929]]...]
INFO - root - 2017-12-05 22:50:52.446765: step 51310, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 65h:53m:49s remains)
INFO - root - 2017-12-05 22:51:01.100982: step 51320, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 67h:41m:01s remains)
INFO - root - 2017-12-05 22:51:09.659150: step 51330, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 67h:26m:42s remains)
INFO - root - 2017-12-05 22:51:18.175513: step 51340, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 65h:05m:12s remains)
INFO - root - 2017-12-05 22:51:26.567757: step 51350, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 68h:09m:29s remains)
INFO - root - 2017-12-05 22:51:34.974217: step 51360, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 65h:41m:00s remains)
INFO - root - 2017-12-05 22:51:43.576091: step 51370, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 68h:21m:55s remains)
INFO - root - 2017-12-05 22:51:52.062284: step 51380, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 67h:59m:19s remains)
INFO - root - 2017-12-05 22:52:00.631560: step 51390, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 66h:15m:06s remains)
INFO - root - 2017-12-05 22:52:09.120404: step 51400, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 65h:50m:15s remains)
2017-12-05 22:52:09.957895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2079449 -4.2211876 -4.2463508 -4.267983 -4.2778163 -4.2730708 -4.256135 -4.231266 -4.2049227 -4.1912346 -4.198575 -4.2253461 -4.2578588 -4.2852139 -4.3020363][-4.2201324 -4.235981 -4.2593412 -4.2754316 -4.2795849 -4.2685485 -4.24176 -4.2067208 -4.1714339 -4.1527596 -4.1628804 -4.1983619 -4.2400856 -4.2749672 -4.2999344][-4.2438893 -4.2568326 -4.2715473 -4.2785511 -4.2754588 -4.2559557 -4.2161927 -4.1690588 -4.1243224 -4.1019382 -4.1164412 -4.1632342 -4.2160606 -4.2592311 -4.2909732][-4.2674012 -4.2729311 -4.2793059 -4.2796264 -4.2710748 -4.2448163 -4.1968269 -4.1444368 -4.0953307 -4.0681386 -4.0796103 -4.129951 -4.1901054 -4.2398405 -4.2772131][-4.2853985 -4.285315 -4.2885518 -4.2885537 -4.2795053 -4.2528877 -4.204648 -4.15011 -4.0962448 -4.0617452 -4.0639691 -4.1083369 -4.1689415 -4.2212391 -4.2625642][-4.2988052 -4.297266 -4.3008437 -4.3036094 -4.2953792 -4.2673883 -4.2174883 -4.1621895 -4.1084762 -4.0707526 -4.0642076 -4.0989118 -4.1548591 -4.2062788 -4.2493653][-4.3090773 -4.3089437 -4.3133488 -4.316927 -4.30739 -4.2771173 -4.226738 -4.1730194 -4.1243572 -4.0862689 -4.0710917 -4.0945463 -4.14344 -4.1945405 -4.2384448][-4.3128548 -4.3142843 -4.3206792 -4.3253789 -4.3160067 -4.287859 -4.24117 -4.1898489 -4.1412954 -4.098794 -4.0749092 -4.0894227 -4.1318493 -4.1837459 -4.2287326][-4.3077593 -4.3095889 -4.3183508 -4.3261242 -4.3198495 -4.2963314 -4.2536931 -4.2012024 -4.1496882 -4.1045356 -4.0778918 -4.0904436 -4.1285992 -4.1789393 -4.2232809][-4.2989459 -4.3004889 -4.3095388 -4.3207464 -4.3196373 -4.3018508 -4.2633204 -4.2101612 -4.1611204 -4.1207962 -4.0975041 -4.108542 -4.140408 -4.1843824 -4.2262163][-4.296093 -4.2985048 -4.3068886 -4.3178577 -4.3211989 -4.3105335 -4.2781353 -4.2291346 -4.1875973 -4.154995 -4.1347103 -4.1416564 -4.1638365 -4.1983113 -4.236464][-4.30142 -4.3055348 -4.3134575 -4.3222122 -4.3269324 -4.3218021 -4.2966933 -4.2541504 -4.2190003 -4.1944122 -4.1781464 -4.1814027 -4.195498 -4.2205782 -4.2515354][-4.3120108 -4.3166075 -4.3227911 -4.3278131 -4.3299875 -4.3262734 -4.3080683 -4.2769771 -4.2501645 -4.2332072 -4.22134 -4.2212086 -4.2288113 -4.2456121 -4.268189][-4.3214388 -4.3243451 -4.3273649 -4.3289237 -4.3284121 -4.3256488 -4.3150191 -4.2960949 -4.2800326 -4.2687755 -4.2574964 -4.2537293 -4.256494 -4.2670364 -4.2823653][-4.3222003 -4.322794 -4.3231931 -4.3229103 -4.3215709 -4.3201742 -4.3166924 -4.3074393 -4.2988324 -4.2898755 -4.2779589 -4.2726932 -4.2739887 -4.280952 -4.2913585]]...]
INFO - root - 2017-12-05 22:52:18.523195: step 51410, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 67h:06m:11s remains)
INFO - root - 2017-12-05 22:52:26.977669: step 51420, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 65h:20m:04s remains)
INFO - root - 2017-12-05 22:52:35.510170: step 51430, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:19m:41s remains)
INFO - root - 2017-12-05 22:52:44.057940: step 51440, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 67h:21m:15s remains)
INFO - root - 2017-12-05 22:52:52.572323: step 51450, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 67h:37m:15s remains)
INFO - root - 2017-12-05 22:53:00.899434: step 51460, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 65h:06m:45s remains)
INFO - root - 2017-12-05 22:53:09.208667: step 51470, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.840 sec/batch; 65h:33m:04s remains)
INFO - root - 2017-12-05 22:53:17.746691: step 51480, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 66h:10m:27s remains)
INFO - root - 2017-12-05 22:53:26.150598: step 51490, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 67h:34m:16s remains)
INFO - root - 2017-12-05 22:53:34.649995: step 51500, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 67h:48m:29s remains)
2017-12-05 22:53:35.468614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2551875 -4.2506037 -4.2464604 -4.241221 -4.2397733 -4.245575 -4.2475786 -4.2424564 -4.2433853 -4.2448263 -4.2475209 -4.2567344 -4.2693791 -4.2794113 -4.2817583][-4.2601776 -4.2522097 -4.2431831 -4.2324166 -4.2253871 -4.2272449 -4.2280359 -4.22322 -4.2289715 -4.2334471 -4.2331147 -4.2384348 -4.2521744 -4.2638297 -4.26673][-4.2408772 -4.230906 -4.22278 -4.2124734 -4.2072859 -4.2099538 -4.2072606 -4.1983976 -4.2048554 -4.215838 -4.2161474 -4.2177434 -4.2319574 -4.24188 -4.2419662][-4.2157187 -4.201757 -4.1932311 -4.1824589 -4.1787262 -4.1812387 -4.17298 -4.1658154 -4.1826615 -4.2022734 -4.206151 -4.2045269 -4.2145 -4.2205691 -4.2177963][-4.1923671 -4.1714611 -4.1589508 -4.1475577 -4.13552 -4.1221957 -4.09585 -4.0883465 -4.121007 -4.1583309 -4.1752954 -4.1813884 -4.1907945 -4.1915655 -4.1851416][-4.171494 -4.1459265 -4.1276054 -4.1090016 -4.0778546 -4.0363894 -3.98345 -3.9672513 -4.0165257 -4.0804434 -4.1198997 -4.1465607 -4.163259 -4.163372 -4.1596932][-4.1504216 -4.126308 -4.106914 -4.0812535 -4.035943 -3.9737227 -3.8914685 -3.857574 -3.9205689 -4.0124598 -4.0724998 -4.1130404 -4.1416478 -4.1464391 -4.1489935][-4.137382 -4.1123562 -4.096303 -4.0713196 -4.0286956 -3.965776 -3.880487 -3.8350632 -3.889971 -3.9788079 -4.0402875 -4.0862517 -4.1281357 -4.1435041 -4.1559744][-4.1754742 -4.149549 -4.1348748 -4.1148829 -4.0851769 -4.0451069 -3.9913564 -3.959811 -3.995919 -4.0558119 -4.0983162 -4.1360312 -4.1804576 -4.2027211 -4.2153611][-4.2195559 -4.199018 -4.1856909 -4.1651716 -4.1436129 -4.1227827 -4.0987639 -4.0838132 -4.1039267 -4.1357379 -4.1619368 -4.1900969 -4.2239623 -4.2418566 -4.2523379][-4.2388992 -4.2244854 -4.2102728 -4.1895342 -4.1718659 -4.1625261 -4.1532927 -4.1469545 -4.1587052 -4.1766524 -4.1960454 -4.21951 -4.2424107 -4.25558 -4.2638459][-4.2238421 -4.21172 -4.1948652 -4.1775789 -4.169857 -4.1715951 -4.1725965 -4.1755829 -4.1873345 -4.2002277 -4.2155766 -4.2351217 -4.2537904 -4.2625065 -4.2655606][-4.2016749 -4.1837888 -4.1620164 -4.1473169 -4.1464748 -4.1542563 -4.1558027 -4.1603885 -4.1733141 -4.1854873 -4.1977835 -4.215539 -4.2353468 -4.2462273 -4.2476983][-4.201653 -4.179347 -4.1574578 -4.1460614 -4.1427159 -4.1455374 -4.1421571 -4.1429973 -4.1541696 -4.1677747 -4.1784697 -4.1908407 -4.2048168 -4.2118344 -4.2157564][-4.241653 -4.2206717 -4.2027264 -4.1923056 -4.1860805 -4.1812043 -4.1709423 -4.1678505 -4.1785154 -4.1942592 -4.2041407 -4.2080622 -4.2086787 -4.2053552 -4.2066655]]...]
INFO - root - 2017-12-05 22:53:43.985765: step 51510, loss = 2.04, batch loss = 1.99 (9.9 examples/sec; 0.811 sec/batch; 63h:18m:38s remains)
INFO - root - 2017-12-05 22:53:52.464638: step 51520, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.842 sec/batch; 65h:45m:20s remains)
INFO - root - 2017-12-05 22:54:00.942400: step 51530, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 68h:19m:11s remains)
INFO - root - 2017-12-05 22:54:09.482389: step 51540, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 69h:14m:58s remains)
INFO - root - 2017-12-05 22:54:17.878926: step 51550, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 64h:14m:42s remains)
INFO - root - 2017-12-05 22:54:26.517872: step 51560, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:46m:54s remains)
INFO - root - 2017-12-05 22:54:35.004299: step 51570, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 67h:45m:45s remains)
INFO - root - 2017-12-05 22:54:43.519514: step 51580, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 68h:34m:17s remains)
INFO - root - 2017-12-05 22:54:52.110171: step 51590, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 66h:01m:30s remains)
INFO - root - 2017-12-05 22:55:00.530893: step 51600, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 64h:31m:43s remains)
2017-12-05 22:55:01.354698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2367721 -4.26456 -4.2844343 -4.2874928 -4.2742119 -4.252336 -4.2309384 -4.2165737 -4.2072115 -4.2038093 -4.2016115 -4.1998882 -4.2032671 -4.204134 -4.1942635][-4.2240896 -4.2594757 -4.2849693 -4.2885742 -4.2738304 -4.2514105 -4.2268219 -4.2142949 -4.2131352 -4.2150917 -4.2140484 -4.2096424 -4.2076359 -4.2006946 -4.1840143][-4.2164831 -4.2533703 -4.275598 -4.2734571 -4.2577004 -4.2336259 -4.2040949 -4.1909251 -4.1985803 -4.209527 -4.2120824 -4.2110677 -4.207449 -4.1946411 -4.1713915][-4.2218657 -4.2484779 -4.2604613 -4.2514181 -4.2354403 -4.2088747 -4.1710176 -4.1500649 -4.162004 -4.1875587 -4.2095151 -4.2188988 -4.2157316 -4.1983886 -4.1696229][-4.2349429 -4.2471213 -4.245944 -4.232729 -4.2152891 -4.18764 -4.1369109 -4.0965281 -4.1036944 -4.1502018 -4.2046642 -4.23354 -4.2336583 -4.2146878 -4.1833444][-4.2395892 -4.2421789 -4.2323127 -4.2148223 -4.193017 -4.1610165 -4.0939822 -4.0228739 -4.0108333 -4.0796504 -4.1679311 -4.215168 -4.2249894 -4.2129602 -4.1892853][-4.2374363 -4.2354565 -4.2250218 -4.2038178 -4.1730189 -4.1298475 -4.0423632 -3.924412 -3.8723679 -3.9676433 -4.099864 -4.1687393 -4.1881452 -4.1904764 -4.1837921][-4.2343416 -4.2370586 -4.2324376 -4.2127481 -4.1757298 -4.1193938 -4.007288 -3.8370721 -3.732779 -3.8551548 -4.0325656 -4.1240792 -4.14992 -4.164423 -4.1786814][-4.2330451 -4.2453418 -4.2509584 -4.2390542 -4.2068987 -4.1503267 -4.0370646 -3.8650012 -3.7459967 -3.8455548 -4.0167265 -4.1124115 -4.1400075 -4.1567068 -4.1830659][-4.2313762 -4.249927 -4.2625976 -4.2619376 -4.2461834 -4.2041154 -4.1182137 -3.9875183 -3.8937869 -3.9341455 -4.0456967 -4.1243849 -4.1514492 -4.1675205 -4.1941233][-4.2240057 -4.2408381 -4.2520742 -4.2569089 -4.2564664 -4.2342753 -4.1827526 -4.0999784 -4.0334411 -4.0321803 -4.0889688 -4.1476688 -4.1754556 -4.1905494 -4.209094][-4.1938763 -4.2033834 -4.2123504 -4.22189 -4.2330041 -4.2324252 -4.2138648 -4.1716752 -4.12831 -4.1118784 -4.1366911 -4.1766124 -4.1993446 -4.2126637 -4.2227592][-4.1491756 -4.1505494 -4.1631904 -4.1782174 -4.1962667 -4.2146211 -4.224514 -4.2132225 -4.190134 -4.1797333 -4.1933875 -4.2171807 -4.2307463 -4.2391677 -4.241086][-4.1119237 -4.1047311 -4.1228356 -4.1478434 -4.175529 -4.2059822 -4.2307472 -4.2375894 -4.2321382 -4.2327075 -4.2432022 -4.2574759 -4.2647429 -4.2682996 -4.265099][-4.0786371 -4.0619526 -4.086154 -4.1284409 -4.172338 -4.211339 -4.2411613 -4.25425 -4.2588754 -4.2657971 -4.275754 -4.2856841 -4.2922688 -4.2944217 -4.287405]]...]
INFO - root - 2017-12-05 22:55:10.003052: step 51610, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 69h:02m:30s remains)
INFO - root - 2017-12-05 22:55:18.486809: step 51620, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 65h:20m:16s remains)
INFO - root - 2017-12-05 22:55:27.025646: step 51630, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 65h:52m:57s remains)
INFO - root - 2017-12-05 22:55:35.637607: step 51640, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 66h:20m:56s remains)
INFO - root - 2017-12-05 22:55:44.250739: step 51650, loss = 2.07, batch loss = 2.01 (8.1 examples/sec; 0.986 sec/batch; 76h:53m:58s remains)
INFO - root - 2017-12-05 22:55:52.817938: step 51660, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 67h:35m:05s remains)
INFO - root - 2017-12-05 22:56:01.302486: step 51670, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 66h:55m:09s remains)
INFO - root - 2017-12-05 22:56:09.782225: step 51680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 66h:58m:30s remains)
INFO - root - 2017-12-05 22:56:18.261102: step 51690, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 65h:48m:23s remains)
INFO - root - 2017-12-05 22:56:26.786203: step 51700, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 66h:57m:20s remains)
2017-12-05 22:56:27.672977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1592441 -4.149292 -4.1333094 -4.1308012 -4.1306615 -4.1432495 -4.1678138 -4.1759448 -4.1408458 -4.0860548 -4.0582414 -4.0691805 -4.1270566 -4.1988492 -4.2603731][-4.1628642 -4.1451259 -4.126946 -4.1232409 -4.1130157 -4.1138453 -4.1314082 -4.1397891 -4.1116467 -4.0673265 -4.0517735 -4.069994 -4.1276221 -4.1968303 -4.2540178][-4.17871 -4.1539407 -4.1314611 -4.1212182 -4.0995159 -4.0825357 -4.0845537 -4.093267 -4.078208 -4.0515804 -4.0525494 -4.075686 -4.1274834 -4.1897182 -4.2401776][-4.2001767 -4.1722059 -4.1482754 -4.1317654 -4.09885 -4.0602288 -4.0402155 -4.0518141 -4.0559797 -4.0467849 -4.0614367 -4.0893068 -4.1356292 -4.1902552 -4.2324085][-4.2174349 -4.1906157 -4.1665578 -4.1439457 -4.0980129 -4.0383177 -3.9969435 -4.0123744 -4.0387082 -4.0467167 -4.0731258 -4.1089959 -4.1551056 -4.2030435 -4.2374244][-4.2236633 -4.1981282 -4.17501 -4.1468959 -4.0868568 -4.0010905 -3.9346657 -3.9522395 -4.006639 -4.0359311 -4.0748224 -4.1200619 -4.1708632 -4.2160544 -4.2462325][-4.2289996 -4.2036872 -4.1801043 -4.1459131 -4.0703406 -3.9516015 -3.8485341 -3.8592427 -3.9438267 -4.0007319 -4.0593481 -4.1171179 -4.1706395 -4.2157354 -4.2461834][-4.2400742 -4.220263 -4.1976051 -4.1601915 -4.0704961 -3.9244998 -3.7848113 -3.7814991 -3.8906369 -3.9712529 -4.0457091 -4.1131124 -4.16742 -4.2123427 -4.2424393][-4.2679048 -4.25308 -4.233788 -4.2013965 -4.11536 -3.9736602 -3.8369017 -3.8192873 -3.9183137 -3.9941645 -4.0663166 -4.1348968 -4.1853347 -4.2232537 -4.2488704][-4.2958627 -4.2846775 -4.2668447 -4.2431707 -4.1762309 -4.0643253 -3.9605012 -3.9395621 -4.0077243 -4.0586944 -4.1113019 -4.1680341 -4.2110434 -4.244565 -4.2655087][-4.3081045 -4.2980523 -4.2790585 -4.2638206 -4.2173238 -4.1387787 -4.0682678 -4.0477858 -4.0903087 -4.11699 -4.1450272 -4.187099 -4.2223029 -4.255022 -4.2752104][-4.3038807 -4.2952828 -4.2766957 -4.2675672 -4.2373137 -4.1837249 -4.1348977 -4.1142011 -4.1373534 -4.1477566 -4.1587362 -4.1904731 -4.2196484 -4.2502041 -4.2725296][-4.3013797 -4.2951984 -4.2782459 -4.2721653 -4.2526307 -4.2156863 -4.1800356 -4.1605625 -4.169589 -4.167326 -4.1673865 -4.1921225 -4.2192583 -4.2473192 -4.2693472][-4.3063087 -4.3029728 -4.2877774 -4.2806392 -4.2656207 -4.2397051 -4.2146373 -4.1981373 -4.1984611 -4.1888103 -4.1850944 -4.2046256 -4.2287512 -4.2525597 -4.2713785][-4.3188324 -4.3174858 -4.303278 -4.2937341 -4.2800512 -4.260376 -4.2427826 -4.2306824 -4.2274318 -4.2177582 -4.2146373 -4.2299151 -4.2475753 -4.2644396 -4.2788157]]...]
INFO - root - 2017-12-05 22:56:36.236812: step 51710, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 66h:44m:21s remains)
INFO - root - 2017-12-05 22:56:44.787960: step 51720, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 65h:27m:43s remains)
INFO - root - 2017-12-05 22:56:53.377678: step 51730, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 63h:47m:06s remains)
INFO - root - 2017-12-05 22:57:02.014756: step 51740, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:57m:18s remains)
INFO - root - 2017-12-05 22:57:10.502879: step 51750, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 67h:52m:03s remains)
INFO - root - 2017-12-05 22:57:19.053332: step 51760, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 65h:35m:42s remains)
INFO - root - 2017-12-05 22:57:27.516635: step 51770, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 63h:58m:20s remains)
INFO - root - 2017-12-05 22:57:35.956282: step 51780, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 68h:04m:04s remains)
INFO - root - 2017-12-05 22:57:44.404910: step 51790, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 65h:58m:38s remains)
INFO - root - 2017-12-05 22:57:52.970831: step 51800, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.864 sec/batch; 67h:20m:07s remains)
2017-12-05 22:57:53.768591: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2326455 -4.2436018 -4.2525167 -4.2554955 -4.2528639 -4.2465868 -4.2359204 -4.2257986 -4.2216763 -4.2215753 -4.2227526 -4.2270761 -4.2348356 -4.2427354 -4.250998][-4.2261977 -4.2417855 -4.2512 -4.25019 -4.2431483 -4.2342815 -4.2221413 -4.2104826 -4.2053361 -4.2043314 -4.2047672 -4.2113218 -4.2231884 -4.2345753 -4.2450671][-4.2226796 -4.2371473 -4.2405477 -4.2324066 -4.2201123 -4.2087569 -4.1988568 -4.1898332 -4.1857638 -4.1847305 -4.1864634 -4.1965 -4.2138143 -4.23133 -4.2450061][-4.2178011 -4.2255 -4.2200551 -4.2048554 -4.1896095 -4.17829 -4.1705346 -4.1663122 -4.1661944 -4.1658125 -4.1686578 -4.1810827 -4.203465 -4.2272258 -4.2445569][-4.207459 -4.2084565 -4.1987143 -4.1788082 -4.159236 -4.1399655 -4.1268559 -4.1246996 -4.1330156 -4.1399302 -4.145402 -4.1593037 -4.1880722 -4.2188196 -4.241281][-4.18974 -4.1852155 -4.1761055 -4.16149 -4.1353016 -4.0955977 -4.0614142 -4.0578837 -4.0829382 -4.107903 -4.12212 -4.1359253 -4.1648369 -4.2002053 -4.2305665][-4.172718 -4.1658206 -4.1608939 -4.1519189 -4.1168113 -4.0469422 -3.9721041 -3.9616585 -4.0171027 -4.07462 -4.1064725 -4.1228142 -4.1476831 -4.179975 -4.2106204][-4.15518 -4.1503325 -4.1489835 -4.1427264 -4.1019249 -4.0041537 -3.8819311 -3.8529582 -3.9422035 -4.0375967 -4.09719 -4.1274118 -4.1478963 -4.1680474 -4.189352][-4.1273651 -4.1216273 -4.1184134 -4.119647 -4.0921688 -4.0086818 -3.8898785 -3.8425863 -3.9224305 -4.0254421 -4.1006422 -4.1440592 -4.1682763 -4.1805863 -4.1901278][-4.1000743 -4.0832839 -4.07342 -4.0858727 -4.0912461 -4.0589409 -3.9973567 -3.9577329 -3.9818506 -4.0445266 -4.1092415 -4.15589 -4.1866894 -4.2011709 -4.2076254][-4.1009989 -4.0795522 -4.0638885 -4.0774617 -4.1028366 -4.108429 -4.0899892 -4.0647497 -4.0497761 -4.0682273 -4.1153526 -4.1631064 -4.1964021 -4.2132587 -4.2189484][-4.1358671 -4.1202645 -4.1036773 -4.1057587 -4.1233134 -4.1384058 -4.1407504 -4.1297388 -4.109086 -4.1085844 -4.1434679 -4.1886258 -4.2169976 -4.2237053 -4.2196517][-4.1814094 -4.170886 -4.15249 -4.1388621 -4.1394396 -4.1532035 -4.1709471 -4.1802468 -4.1731238 -4.1702867 -4.1925983 -4.2251692 -4.2416258 -4.2327309 -4.2164583][-4.20791 -4.1971455 -4.1788697 -4.1591158 -4.1486831 -4.1593657 -4.1859789 -4.2115417 -4.2165837 -4.2160749 -4.2298188 -4.2503757 -4.2565 -4.2417169 -4.2209544][-4.2178531 -4.2025027 -4.1819406 -4.1608572 -4.1497374 -4.1634779 -4.1954536 -4.2252774 -4.2359648 -4.2380967 -4.2459235 -4.2581949 -4.2607794 -4.248229 -4.230474]]...]
INFO - root - 2017-12-05 22:58:02.339539: step 51810, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 67h:49m:57s remains)
INFO - root - 2017-12-05 22:58:11.010195: step 51820, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 65h:42m:50s remains)
INFO - root - 2017-12-05 22:58:19.479412: step 51830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 66h:51m:37s remains)
INFO - root - 2017-12-05 22:58:27.993477: step 51840, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 68h:04m:41s remains)
INFO - root - 2017-12-05 22:58:36.553205: step 51850, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 68h:17m:14s remains)
INFO - root - 2017-12-05 22:58:45.136411: step 51860, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 67h:11m:47s remains)
INFO - root - 2017-12-05 22:58:53.683136: step 51870, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.832 sec/batch; 64h:53m:18s remains)
INFO - root - 2017-12-05 22:59:02.320443: step 51880, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 68h:21m:19s remains)
INFO - root - 2017-12-05 22:59:10.766420: step 51890, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 66h:38m:30s remains)
INFO - root - 2017-12-05 22:59:19.274957: step 51900, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 66h:38m:45s remains)
2017-12-05 22:59:20.260087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1494694 -4.1376038 -4.1321521 -4.1365919 -4.1415424 -4.1511593 -4.154676 -4.1513619 -4.1614943 -4.1846857 -4.1994438 -4.1893106 -4.1661592 -4.1541657 -4.156332][-4.1527295 -4.1413994 -4.1359544 -4.1394157 -4.1442566 -4.1518674 -4.1515 -4.1472921 -4.1542387 -4.1777906 -4.1928811 -4.1867476 -4.163763 -4.1445231 -4.1464372][-4.1643066 -4.1540537 -4.1498094 -4.1491294 -4.143558 -4.1367483 -4.1311841 -4.1305375 -4.13695 -4.1577663 -4.1738648 -4.1712537 -4.1517572 -4.133779 -4.1385][-4.17893 -4.1693339 -4.1631551 -4.1554723 -4.1341691 -4.1102839 -4.0965466 -4.0998149 -4.1157928 -4.1371183 -4.1508 -4.1471305 -4.1318078 -4.121829 -4.1304183][-4.1872578 -4.1781936 -4.1695242 -4.1553516 -4.1248631 -4.0895834 -4.0643144 -4.0662055 -4.0877233 -4.1097035 -4.1201825 -4.1167626 -4.110168 -4.1110616 -4.1250615][-4.1858754 -4.1804295 -4.1766515 -4.1637516 -4.1290636 -4.0859871 -4.04917 -4.0409389 -4.0569615 -4.0709925 -4.0808411 -4.0871954 -4.098021 -4.1126223 -4.1311221][-4.1759105 -4.1754537 -4.1798558 -4.1714411 -4.1347637 -4.0827947 -4.03279 -4.0101662 -4.0176654 -4.0262561 -4.0414166 -4.0659218 -4.0964985 -4.1237054 -4.1436262][-4.1537209 -4.1584711 -4.1712122 -4.1679111 -4.1318645 -4.0727811 -4.009264 -3.9714286 -3.9744749 -3.9936285 -4.0247693 -4.0658584 -4.1088009 -4.142756 -4.1616554][-4.120894 -4.1277428 -4.1445465 -4.148118 -4.1191554 -4.0592752 -3.9879274 -3.947655 -3.9579856 -3.9956026 -4.0402579 -4.0861068 -4.1264648 -4.1570349 -4.1754556][-4.102674 -4.1078453 -4.1205096 -4.12443 -4.102005 -4.0473156 -3.9811814 -3.9550769 -3.9800959 -4.0280814 -4.0727582 -4.109581 -4.1413717 -4.1658545 -4.1821585][-4.1124773 -4.1133442 -4.1189756 -4.1209574 -4.1044078 -4.0610237 -4.0103111 -4.0001826 -4.02836 -4.0705452 -4.1081219 -4.1353955 -4.1583042 -4.1759 -4.18924][-4.1464944 -4.1444669 -4.1429048 -4.1414056 -4.1310949 -4.1026344 -4.0700688 -4.0678997 -4.086781 -4.116488 -4.144865 -4.1656842 -4.1838188 -4.1974425 -4.2087765][-4.1770597 -4.1769166 -4.1717596 -4.1666169 -4.1578622 -4.1410618 -4.1227808 -4.1232481 -4.1353607 -4.1555281 -4.1776934 -4.1983242 -4.2151861 -4.2262368 -4.2322416][-4.189044 -4.1906848 -4.185545 -4.1754513 -4.1673288 -4.1598206 -4.1566753 -4.1631 -4.1744061 -4.1913009 -4.2093434 -4.2266321 -4.2388878 -4.2433414 -4.2402964][-4.1938553 -4.1940126 -4.1897979 -4.1793928 -4.1728649 -4.1735816 -4.1817532 -4.1932006 -4.2035041 -4.2161803 -4.2271757 -4.2349982 -4.2406316 -4.2382784 -4.2292919]]...]
INFO - root - 2017-12-05 22:59:28.853133: step 51910, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 67h:40m:21s remains)
INFO - root - 2017-12-05 22:59:37.442401: step 51920, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:19m:31s remains)
INFO - root - 2017-12-05 22:59:45.994443: step 51930, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 67h:40m:43s remains)
INFO - root - 2017-12-05 22:59:54.508023: step 51940, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 66h:22m:52s remains)
INFO - root - 2017-12-05 23:00:03.002255: step 51950, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 65h:56m:37s remains)
INFO - root - 2017-12-05 23:00:11.559112: step 51960, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 68h:05m:21s remains)
INFO - root - 2017-12-05 23:00:20.052939: step 51970, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 65h:08m:29s remains)
INFO - root - 2017-12-05 23:00:28.699886: step 51980, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 65h:01m:22s remains)
INFO - root - 2017-12-05 23:00:37.251102: step 51990, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:31m:04s remains)
INFO - root - 2017-12-05 23:00:45.671261: step 52000, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 65h:49m:05s remains)
2017-12-05 23:00:46.436237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3279114 -4.3322549 -4.3356237 -4.3388762 -4.3393631 -4.3382339 -4.3370814 -4.333756 -4.329926 -4.3275685 -4.3304329 -4.3354449 -4.33682 -4.3359008 -4.3316131][-4.3202891 -4.3272934 -4.3302369 -4.3303089 -4.3256807 -4.3200779 -4.31735 -4.3118281 -4.3059287 -4.3041263 -4.309618 -4.3180623 -4.3242269 -4.3287735 -4.3283329][-4.3077664 -4.3122368 -4.3112392 -4.305553 -4.2959762 -4.2864733 -4.2830024 -4.2783689 -4.2725506 -4.2756233 -4.2878842 -4.3017712 -4.3110847 -4.3192048 -4.3220291][-4.2855411 -4.2857785 -4.2821651 -4.2726622 -4.2566442 -4.2407684 -4.235 -4.2317061 -4.2253323 -4.2325191 -4.25554 -4.2807884 -4.2962875 -4.3070464 -4.3115926][-4.261076 -4.2552047 -4.2462053 -4.2274728 -4.198782 -4.1702042 -4.1568394 -4.1483502 -4.1372905 -4.1477442 -4.1912742 -4.237874 -4.2688208 -4.2871718 -4.293222][-4.2511873 -4.240653 -4.2212005 -4.1849341 -4.1331348 -4.0770578 -4.0390239 -4.0109463 -3.9840066 -4.0010033 -4.0786281 -4.1609488 -4.2181997 -4.2532291 -4.2661114][-4.2498665 -4.2403917 -4.2166891 -4.1691 -4.099781 -4.0145726 -3.9367964 -3.869287 -3.8209572 -3.8504708 -3.9671273 -4.087235 -4.1716681 -4.2249665 -4.248621][-4.24762 -4.2432432 -4.2270517 -4.1901755 -4.130127 -4.0456181 -3.9486768 -3.8476825 -3.7797022 -3.8070626 -3.9304094 -4.0641928 -4.1613369 -4.2237463 -4.2535667][-4.2437487 -4.2480903 -4.2463903 -4.2314806 -4.1975393 -4.1416135 -4.0598564 -3.964005 -3.8949041 -3.9004056 -3.9871898 -4.0954123 -4.1786246 -4.2347765 -4.2608552][-4.2247152 -4.2368979 -4.2524738 -4.2598033 -4.2493587 -4.2202435 -4.1657372 -4.0986094 -4.0452137 -4.0361362 -4.0802174 -4.1488223 -4.2050624 -4.2437477 -4.2598968][-4.1919255 -4.2036877 -4.2282929 -4.2526584 -4.2620516 -4.2547131 -4.2300067 -4.1964946 -4.1624217 -4.147666 -4.1575494 -4.1901035 -4.219708 -4.2397161 -4.2442784][-4.122654 -4.1208525 -4.152307 -4.2003231 -4.2319875 -4.2493067 -4.2519007 -4.2471657 -4.2351127 -4.2240486 -4.215035 -4.2214375 -4.2290959 -4.2321391 -4.2278361][-4.0008678 -3.9765131 -4.02437 -4.1088419 -4.1740561 -4.2189775 -4.2449903 -4.2609482 -4.2704058 -4.2749782 -4.2671094 -4.2579393 -4.2478967 -4.2367396 -4.2241149][-3.8536711 -3.8075225 -3.8819747 -4.0096445 -4.1162376 -4.1891327 -4.2358546 -4.2672205 -4.2927947 -4.31044 -4.3110256 -4.295578 -4.2726831 -4.2495456 -4.2311711][-3.7863212 -3.7405479 -3.8292594 -3.9777269 -4.104557 -4.192111 -4.2447882 -4.2761683 -4.3029556 -4.3246784 -4.33162 -4.3206739 -4.2973876 -4.2705355 -4.2484026]]...]
INFO - root - 2017-12-05 23:00:54.889129: step 52010, loss = 2.05, batch loss = 1.99 (10.6 examples/sec; 0.758 sec/batch; 59h:03m:43s remains)
INFO - root - 2017-12-05 23:01:03.473848: step 52020, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 65h:31m:02s remains)
INFO - root - 2017-12-05 23:01:12.037992: step 52030, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 66h:21m:17s remains)
INFO - root - 2017-12-05 23:01:20.570858: step 52040, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 65h:27m:16s remains)
INFO - root - 2017-12-05 23:01:29.032136: step 52050, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.851 sec/batch; 66h:15m:29s remains)
INFO - root - 2017-12-05 23:01:37.580080: step 52060, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 67h:38m:53s remains)
INFO - root - 2017-12-05 23:01:46.128935: step 52070, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 65h:58m:54s remains)
INFO - root - 2017-12-05 23:01:54.743928: step 52080, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:16m:36s remains)
INFO - root - 2017-12-05 23:02:03.322460: step 52090, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 68h:11m:17s remains)
INFO - root - 2017-12-05 23:02:11.750413: step 52100, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 64h:23m:17s remains)
2017-12-05 23:02:12.529163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1605091 -4.1665077 -4.1743584 -4.1860733 -4.2007465 -4.2140937 -4.2158327 -4.208005 -4.1888008 -4.1794858 -4.1825056 -4.1800137 -4.166029 -4.1669655 -4.1805415][-4.1517272 -4.1527066 -4.1593237 -4.173738 -4.1892433 -4.1974773 -4.1925015 -4.184484 -4.1757579 -4.1755991 -4.1871 -4.193769 -4.1866822 -4.1889639 -4.2021785][-4.1624904 -4.1627 -4.1667056 -4.1715779 -4.1764126 -4.1721158 -4.1554794 -4.1504717 -4.1563497 -4.1728654 -4.1966467 -4.210474 -4.2074957 -4.206769 -4.2142839][-4.1905479 -4.1951427 -4.1976762 -4.1905489 -4.1744971 -4.1458378 -4.1185722 -4.1231494 -4.1488981 -4.184351 -4.2145624 -4.2261696 -4.220345 -4.212194 -4.2138247][-4.218442 -4.223753 -4.2204213 -4.201086 -4.1662278 -4.1176672 -4.0827427 -4.0988278 -4.1446919 -4.193644 -4.2284222 -4.2380185 -4.2271819 -4.2143135 -4.2101369][-4.2392416 -4.2383175 -4.2269912 -4.2012892 -4.1557918 -4.0927758 -4.0517268 -4.0728993 -4.1266227 -4.1822977 -4.2248435 -4.2372308 -4.2249131 -4.2061777 -4.192555][-4.2434268 -4.2376585 -4.2228737 -4.1926394 -4.1408176 -4.0704541 -4.0249457 -4.0420685 -4.0926566 -4.1532221 -4.2098413 -4.2332592 -4.2243643 -4.20671 -4.1887655][-4.2403603 -4.23219 -4.2131014 -4.177702 -4.1228094 -4.0472131 -3.9959946 -4.0038261 -4.0512171 -4.118928 -4.1881008 -4.2235937 -4.2233653 -4.2118721 -4.193347][-4.2410069 -4.23031 -4.206738 -4.1665926 -4.1097465 -4.0329022 -3.9756835 -3.9723186 -4.0115356 -4.0839434 -4.163888 -4.2156663 -4.2292166 -4.2249737 -4.2058558][-4.2455482 -4.2270613 -4.1918869 -4.1409817 -4.0785275 -4.0000582 -3.9401257 -3.9287305 -3.9630985 -4.0418715 -4.13415 -4.2014494 -4.2307568 -4.2372594 -4.2198911][-4.248837 -4.2243834 -4.1780105 -4.1171937 -4.0526853 -3.977422 -3.9210777 -3.9135423 -3.950258 -4.030477 -4.1264329 -4.1971669 -4.2341409 -4.2477732 -4.2320633][-4.2487459 -4.2220407 -4.169261 -4.1056671 -4.0448313 -3.976754 -3.9242585 -3.9204829 -3.9588566 -4.0355067 -4.1266928 -4.1977563 -4.2403469 -4.2598495 -4.2474713][-4.253089 -4.2275333 -4.177753 -4.1201639 -4.0659814 -4.0085444 -3.9628582 -3.95863 -3.9902804 -4.0517182 -4.132453 -4.1990047 -4.2433028 -4.2687235 -4.2637987][-4.2634358 -4.242671 -4.2035432 -4.161787 -4.12211 -4.0802741 -4.0484447 -4.0457859 -4.0641556 -4.10273 -4.1639295 -4.2191138 -4.2572641 -4.2804174 -4.2803693][-4.2814159 -4.2673693 -4.2412019 -4.2159605 -4.191339 -4.1631236 -4.1439371 -4.1409383 -4.1486111 -4.1698871 -4.2115331 -4.2534351 -4.2826505 -4.2992625 -4.3008852]]...]
INFO - root - 2017-12-05 23:02:20.989028: step 52110, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 65h:48m:43s remains)
INFO - root - 2017-12-05 23:02:29.403604: step 52120, loss = 2.06, batch loss = 2.00 (10.7 examples/sec; 0.749 sec/batch; 58h:18m:18s remains)
INFO - root - 2017-12-05 23:02:37.823751: step 52130, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 65h:14m:06s remains)
INFO - root - 2017-12-05 23:02:46.474263: step 52140, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 67h:38m:10s remains)
INFO - root - 2017-12-05 23:02:54.867843: step 52150, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 66h:19m:50s remains)
INFO - root - 2017-12-05 23:03:03.498876: step 52160, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 63h:48m:11s remains)
INFO - root - 2017-12-05 23:03:12.003958: step 52170, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.829 sec/batch; 64h:31m:20s remains)
INFO - root - 2017-12-05 23:03:20.434719: step 52180, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 65h:44m:04s remains)
INFO - root - 2017-12-05 23:03:28.966867: step 52190, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 65h:12m:49s remains)
INFO - root - 2017-12-05 23:03:37.499406: step 52200, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 68h:16m:29s remains)
2017-12-05 23:03:38.374979: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2654991 -4.2703352 -4.2715492 -4.2736492 -4.2766085 -4.2773294 -4.272438 -4.2778292 -4.2906909 -4.2949691 -4.2839603 -4.268528 -4.2645988 -4.272234 -4.2844214][-4.2681971 -4.2669725 -4.2593889 -4.2552123 -4.2588634 -4.261138 -4.2527709 -4.256156 -4.2735071 -4.2826238 -4.272902 -4.2598634 -4.2615547 -4.2725768 -4.2863259][-4.2737732 -4.2664256 -4.2515378 -4.2411866 -4.2426486 -4.2448688 -4.2349534 -4.2365017 -4.2590861 -4.2723556 -4.2623544 -4.2482219 -4.2489848 -4.2600517 -4.2757092][-4.2809629 -4.2700362 -4.2495551 -4.22974 -4.2257924 -4.2232561 -4.207077 -4.20632 -4.2365913 -4.2602091 -4.2571177 -4.2438669 -4.242003 -4.251791 -4.2699533][-4.2837043 -4.2728229 -4.2468047 -4.2134595 -4.1953988 -4.1814051 -4.1554413 -4.1462488 -4.1864777 -4.2324696 -4.2468247 -4.2403526 -4.2394381 -4.2532825 -4.2761374][-4.2637129 -4.2542958 -4.2233243 -4.1739645 -4.1368771 -4.1010146 -4.0542178 -4.0332441 -4.0935631 -4.1761618 -4.2190719 -4.2325072 -4.2472491 -4.2708 -4.2967629][-4.2199564 -4.2088418 -4.1740327 -4.115088 -4.0576997 -3.9888053 -3.8990004 -3.860522 -3.9565167 -4.0869522 -4.1701746 -4.2166939 -4.2562537 -4.2919912 -4.3184514][-4.1798191 -4.165153 -4.1298046 -4.0658793 -3.9859438 -3.8724458 -3.7200897 -3.6550708 -3.786063 -3.9611347 -4.0880713 -4.1732254 -4.244206 -4.2963576 -4.3234062][-4.1620069 -4.1403813 -4.1048427 -4.0492015 -3.9664311 -3.8358829 -3.6536858 -3.5684817 -3.6947637 -3.8746469 -4.0196605 -4.1288362 -4.2256041 -4.290072 -4.3154607][-4.1709781 -4.1403303 -4.1043496 -4.0635529 -4.0037394 -3.9052196 -3.7619829 -3.683903 -3.7545333 -3.8834419 -4.0059505 -4.1122303 -4.2173986 -4.2843981 -4.3052912][-4.20753 -4.1782789 -4.1434345 -4.1105704 -4.0690289 -4.0054536 -3.9137099 -3.8544159 -3.8777688 -3.9531131 -4.040267 -4.1244249 -4.2169914 -4.279057 -4.2964215][-4.2372088 -4.219553 -4.1927309 -4.1660318 -4.1346951 -4.0922003 -4.0349417 -3.9948025 -3.9973078 -4.0371885 -4.0898719 -4.1466084 -4.2159948 -4.2707348 -4.2895947][-4.2347975 -4.2310557 -4.21906 -4.203948 -4.1882453 -4.1616235 -4.125958 -4.0986757 -4.0954165 -4.1162291 -4.1439977 -4.1779284 -4.2269325 -4.2695956 -4.2861238][-4.2036314 -4.2067385 -4.207356 -4.2070541 -4.2078829 -4.1962538 -4.174921 -4.1550074 -4.1513748 -4.161345 -4.1760073 -4.1961322 -4.2309561 -4.2597036 -4.269011][-4.1346421 -4.1354918 -4.1437464 -4.1598105 -4.1808619 -4.1878395 -4.1819267 -4.171268 -4.1708555 -4.1778274 -4.1884632 -4.2039194 -4.2284255 -4.2455907 -4.2503533]]...]
INFO - root - 2017-12-05 23:03:46.918155: step 52210, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 67h:26m:29s remains)
INFO - root - 2017-12-05 23:03:55.550817: step 52220, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 69h:13m:42s remains)
INFO - root - 2017-12-05 23:04:03.833480: step 52230, loss = 2.08, batch loss = 2.02 (10.7 examples/sec; 0.746 sec/batch; 58h:03m:03s remains)
INFO - root - 2017-12-05 23:04:12.411420: step 52240, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 67h:06m:48s remains)
INFO - root - 2017-12-05 23:04:20.913960: step 52250, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 69h:51m:50s remains)
INFO - root - 2017-12-05 23:04:29.440266: step 52260, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:11m:13s remains)
INFO - root - 2017-12-05 23:04:38.008721: step 52270, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 67h:51m:22s remains)
INFO - root - 2017-12-05 23:04:46.613423: step 52280, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 66h:27m:15s remains)
INFO - root - 2017-12-05 23:04:55.178861: step 52290, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 67h:42m:49s remains)
INFO - root - 2017-12-05 23:05:03.778162: step 52300, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.829 sec/batch; 64h:33m:21s remains)
2017-12-05 23:05:04.559902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2994876 -4.2890468 -4.2749438 -4.2628126 -4.2575097 -4.2636461 -4.2780566 -4.2916512 -4.3047972 -4.3196092 -4.3320389 -4.3363123 -4.3345747 -4.3321238 -4.3305235][-4.2795329 -4.2598825 -4.2373962 -4.2200084 -4.2097106 -4.2180586 -4.2412376 -4.2606735 -4.2794394 -4.3016691 -4.3217063 -4.3322387 -4.335104 -4.3348751 -4.3330855][-4.2516356 -4.2166314 -4.182888 -4.162291 -4.1472545 -4.155077 -4.1864724 -4.2135777 -4.2364826 -4.2651048 -4.2945094 -4.314703 -4.3268375 -4.3326297 -4.3331528][-4.2308884 -4.1847773 -4.1430464 -4.119566 -4.0989289 -4.0969782 -4.1250453 -4.1541314 -4.1783004 -4.2106819 -4.2509274 -4.2851081 -4.3105955 -4.3259983 -4.3311038][-4.2153268 -4.1658974 -4.120213 -4.0896544 -4.0537672 -4.0286212 -4.0411777 -4.0737319 -4.1072845 -4.14655 -4.1987662 -4.2508035 -4.29172 -4.31688 -4.3268046][-4.2043934 -4.1544089 -4.0983887 -4.0497336 -3.9877684 -3.9362237 -3.935096 -3.9801362 -4.0380917 -4.0948195 -4.1559839 -4.22064 -4.2741737 -4.3069491 -4.3214884][-4.1850452 -4.1280236 -4.0611939 -4.0014906 -3.9257925 -3.859762 -3.8562212 -3.919003 -3.999393 -4.0709949 -4.134922 -4.2012625 -4.2604747 -4.2979584 -4.3166814][-4.1695037 -4.105732 -4.037384 -3.986727 -3.9237218 -3.8622122 -3.8575504 -3.9241478 -4.0055337 -4.0771589 -4.1373138 -4.1968875 -4.2555218 -4.2956 -4.3154416][-4.1746712 -4.11876 -4.0694723 -4.0424113 -4.002882 -3.9493012 -3.9319971 -3.9830325 -4.0515175 -4.1119332 -4.16138 -4.2099957 -4.2632561 -4.30245 -4.3209362][-4.1990423 -4.1612725 -4.1369228 -4.13024 -4.1089206 -4.0611753 -4.0289459 -4.0607209 -4.119029 -4.1676755 -4.2028646 -4.2366452 -4.2798004 -4.3147349 -4.3299346][-4.2243247 -4.2011471 -4.1933465 -4.2009397 -4.1905937 -4.1540585 -4.1195183 -4.1376486 -4.187324 -4.2257547 -4.2493534 -4.2691064 -4.2995911 -4.3259368 -4.3359909][-4.2425861 -4.2250013 -4.2224188 -4.2375016 -4.2374125 -4.2180333 -4.1960821 -4.2082314 -4.2461948 -4.2736411 -4.2881241 -4.2986112 -4.3170562 -4.3332205 -4.3384233][-4.2576218 -4.2449389 -4.2437744 -4.2612362 -4.2704329 -4.2647409 -4.2543221 -4.2623839 -4.2868128 -4.304955 -4.315588 -4.3229556 -4.3337717 -4.3415961 -4.3424163][-4.2690911 -4.2640543 -4.2647548 -4.2804308 -4.2946277 -4.2960949 -4.2914677 -4.2970572 -4.3116641 -4.324049 -4.3334069 -4.3406081 -4.3470888 -4.3492761 -4.3470149][-4.2757006 -4.2787681 -4.2819157 -4.29368 -4.3069844 -4.3105087 -4.3091512 -4.3137593 -4.3220196 -4.330822 -4.3379078 -4.3450432 -4.3493066 -4.348875 -4.3454762]]...]
INFO - root - 2017-12-05 23:05:13.206188: step 52310, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 66h:18m:57s remains)
INFO - root - 2017-12-05 23:05:21.740800: step 52320, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.890 sec/batch; 69h:16m:55s remains)
INFO - root - 2017-12-05 23:05:30.203616: step 52330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:30m:55s remains)
INFO - root - 2017-12-05 23:05:38.686994: step 52340, loss = 2.07, batch loss = 2.01 (10.3 examples/sec; 0.778 sec/batch; 60h:34m:46s remains)
INFO - root - 2017-12-05 23:05:47.163280: step 52350, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 67h:50m:22s remains)
INFO - root - 2017-12-05 23:05:55.775114: step 52360, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 67h:01m:32s remains)
INFO - root - 2017-12-05 23:06:04.445360: step 52370, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 66h:32m:23s remains)
INFO - root - 2017-12-05 23:06:13.068828: step 52380, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 66h:14m:07s remains)
INFO - root - 2017-12-05 23:06:21.579774: step 52390, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 66h:51m:37s remains)
INFO - root - 2017-12-05 23:06:30.180049: step 52400, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 67h:27m:55s remains)
2017-12-05 23:06:31.021823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.182478 -4.1540456 -4.1262531 -4.1116934 -4.1021543 -4.100091 -4.1011672 -4.0918465 -4.081851 -4.0943294 -4.1281962 -4.1448069 -4.1457157 -4.1376452 -4.1135845][-4.1703153 -4.1392279 -4.112937 -4.1000519 -4.0858312 -4.07242 -4.0668454 -4.0575037 -4.0482183 -4.0624771 -4.0952134 -4.108345 -4.1077471 -4.1028109 -4.0815291][-4.1570578 -4.1296053 -4.1097817 -4.1043286 -4.0919542 -4.0690494 -4.0487318 -4.0364213 -4.0277081 -4.0415635 -4.0692511 -4.0855989 -4.0862508 -4.0810819 -4.0629644][-4.1495566 -4.1316705 -4.1188149 -4.1181231 -4.1111908 -4.0883026 -4.0557942 -4.0353079 -4.020659 -4.0338964 -4.0588818 -4.08009 -4.0839252 -4.0741882 -4.055305][-4.1517854 -4.1378384 -4.1277752 -4.1290522 -4.121809 -4.0950937 -4.0543752 -4.0263729 -4.0044451 -4.0153155 -4.0471973 -4.0766063 -4.0834 -4.0710979 -4.0517435][-4.1663394 -4.1554546 -4.1413856 -4.1337366 -4.1186533 -4.086205 -4.043293 -4.016552 -3.9882898 -3.9918168 -4.0331216 -4.0751376 -4.0889707 -4.0777187 -4.057302][-4.1836429 -4.1756015 -4.1596494 -4.1428885 -4.1189394 -4.0839553 -4.0437126 -4.0176463 -3.9824739 -3.9753444 -4.02771 -4.0913806 -4.1146059 -4.1006193 -4.0765562][-4.1962175 -4.1897473 -4.1741595 -4.1539216 -4.1278648 -4.1035228 -4.066493 -4.037365 -3.9982798 -3.989254 -4.0431881 -4.1134281 -4.1440926 -4.1350608 -4.1086073][-4.2048659 -4.201292 -4.1896162 -4.175056 -4.1585546 -4.1491923 -4.1185393 -4.0898695 -4.0590458 -4.0561814 -4.0964146 -4.1482573 -4.1723828 -4.1685004 -4.14328][-4.2170315 -4.2132688 -4.2011352 -4.1875925 -4.1791754 -4.1799736 -4.1611128 -4.1413302 -4.1265087 -4.1329422 -4.1591215 -4.1913791 -4.2078724 -4.2046776 -4.1827483][-4.2304893 -4.2253137 -4.2127037 -4.1983705 -4.190865 -4.19443 -4.18224 -4.1723256 -4.16897 -4.1796112 -4.1987023 -4.2201815 -4.2342319 -4.2311816 -4.2158837][-4.2411909 -4.2342763 -4.2226896 -4.212184 -4.2081146 -4.211329 -4.2051654 -4.1981521 -4.1975417 -4.2068958 -4.2255268 -4.2445703 -4.257915 -4.2579026 -4.2457428][-4.2602 -4.2541046 -4.2455177 -4.237536 -4.2346721 -4.2378607 -4.2368331 -4.2342091 -4.2352066 -4.2425032 -4.2594481 -4.2771573 -4.2880788 -4.287652 -4.2768331][-4.2895293 -4.2858272 -4.2808719 -4.2765036 -4.2753 -4.277894 -4.27881 -4.2777 -4.2792215 -4.2860889 -4.2986255 -4.3100657 -4.3168373 -4.3161983 -4.3078508][-4.3100829 -4.3066435 -4.3041792 -4.302722 -4.3037462 -4.3074865 -4.309279 -4.3095093 -4.3119855 -4.3173957 -4.3245811 -4.3295107 -4.3310142 -4.3296804 -4.3247294]]...]
INFO - root - 2017-12-05 23:06:39.522225: step 52410, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 66h:45m:56s remains)
INFO - root - 2017-12-05 23:06:48.024163: step 52420, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 67h:32m:37s remains)
INFO - root - 2017-12-05 23:06:56.597200: step 52430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:37m:05s remains)
INFO - root - 2017-12-05 23:07:05.377886: step 52440, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.881 sec/batch; 68h:30m:44s remains)
INFO - root - 2017-12-05 23:07:13.849438: step 52450, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.818 sec/batch; 63h:37m:32s remains)
INFO - root - 2017-12-05 23:07:22.405641: step 52460, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 65h:58m:13s remains)
INFO - root - 2017-12-05 23:07:30.922176: step 52470, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 67h:12m:39s remains)
INFO - root - 2017-12-05 23:07:39.404384: step 52480, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 67h:53m:32s remains)
INFO - root - 2017-12-05 23:07:47.994761: step 52490, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 65h:52m:24s remains)
INFO - root - 2017-12-05 23:07:56.567380: step 52500, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 66h:05m:19s remains)
2017-12-05 23:07:57.353418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2476468 -4.226193 -4.2119455 -4.2039881 -4.1915879 -4.1849923 -4.1991334 -4.21068 -4.2158227 -4.2247276 -4.2356539 -4.2481766 -4.2473817 -4.2406287 -4.2474837][-4.2396469 -4.2161613 -4.1987743 -4.1841593 -4.1677389 -4.1609216 -4.1780753 -4.1904087 -4.1943212 -4.2076893 -4.2230444 -4.2352133 -4.2291803 -4.2183905 -4.2245889][-4.2293515 -4.196476 -4.1715326 -4.1477404 -4.1289315 -4.1313772 -4.1547508 -4.1719561 -4.1821074 -4.2061076 -4.2250566 -4.2287683 -4.2130213 -4.1948195 -4.1999526][-4.2189326 -4.1769066 -4.1435919 -4.1079378 -4.0876665 -4.1031485 -4.1321397 -4.1497965 -4.1687284 -4.2052708 -4.2244916 -4.2208261 -4.1980896 -4.1747832 -4.17998][-4.2113857 -4.1618886 -4.1193085 -4.0745835 -4.0541391 -4.0749259 -4.1016006 -4.1134796 -4.1420937 -4.1865835 -4.2010074 -4.1946831 -4.170754 -4.1476607 -4.1577077][-4.2106953 -4.1583109 -4.108706 -4.0594373 -4.0370584 -4.0503254 -4.0693064 -4.0741062 -4.1082153 -4.1511421 -4.156332 -4.1500096 -4.1364 -4.1255956 -4.1415844][-4.2143693 -4.1636825 -4.1085782 -4.0563469 -4.0302939 -4.0332217 -4.0444593 -4.0423265 -4.0734963 -4.1112409 -4.1128225 -4.112349 -4.1175923 -4.122818 -4.1388712][-4.2156253 -4.166007 -4.106956 -4.0540595 -4.0228958 -4.0166655 -4.0262775 -4.0243931 -4.0541725 -4.0886555 -4.0935025 -4.0980844 -4.1176076 -4.133482 -4.1445785][-4.2182713 -4.1681151 -4.11118 -4.0634351 -4.0316744 -4.0225692 -4.0353422 -4.0412111 -4.0667224 -4.0910034 -4.0977883 -4.1008458 -4.1173291 -4.1290007 -4.1308861][-4.2225766 -4.1735477 -4.1222095 -4.0811205 -4.0540552 -4.0479712 -4.0606561 -4.0682349 -4.0829158 -4.0944695 -4.1002135 -4.1011372 -4.1086845 -4.1111269 -4.1035352][-4.2361016 -4.1900759 -4.1454782 -4.1109295 -4.0898108 -4.0857029 -4.0942321 -4.09992 -4.1047077 -4.1052256 -4.1069856 -4.1104169 -4.1168594 -4.1139946 -4.1025505][-4.2563496 -4.212986 -4.1695743 -4.1359663 -4.1184778 -4.1192603 -4.1290402 -4.1343727 -4.1316495 -4.1245017 -4.1235271 -4.1307464 -4.1416564 -4.1415548 -4.1288157][-4.2793145 -4.2412906 -4.2021585 -4.1719747 -4.1577144 -4.1607108 -4.1727405 -4.17836 -4.1715341 -4.1604996 -4.1584086 -4.1685176 -4.1800103 -4.1781635 -4.1615791][-4.3027606 -4.2726908 -4.2418365 -4.2198176 -4.2089491 -4.2096047 -4.2194433 -4.2251444 -4.2204847 -4.21255 -4.2110415 -4.2195873 -4.2272682 -4.2228346 -4.2074628][-4.3214049 -4.3005939 -4.2797389 -4.2665467 -4.2588029 -4.2575459 -4.2639027 -4.268219 -4.2663536 -4.2619247 -4.2628813 -4.2683973 -4.2695456 -4.2636743 -4.2545691]]...]
INFO - root - 2017-12-05 23:08:06.113822: step 52510, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 68h:23m:52s remains)
INFO - root - 2017-12-05 23:08:14.891144: step 52520, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 67h:08m:18s remains)
INFO - root - 2017-12-05 23:08:23.354316: step 52530, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 67h:51m:29s remains)
INFO - root - 2017-12-05 23:08:31.958482: step 52540, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.838 sec/batch; 65h:12m:05s remains)
INFO - root - 2017-12-05 23:08:40.463804: step 52550, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 65h:23m:56s remains)
INFO - root - 2017-12-05 23:08:48.963067: step 52560, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.856 sec/batch; 66h:35m:05s remains)
INFO - root - 2017-12-05 23:08:57.567043: step 52570, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 67h:41m:21s remains)
INFO - root - 2017-12-05 23:09:06.085821: step 52580, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.814 sec/batch; 63h:17m:26s remains)
INFO - root - 2017-12-05 23:09:14.768532: step 52590, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 66h:31m:42s remains)
INFO - root - 2017-12-05 23:09:23.316773: step 52600, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 66h:01m:16s remains)
2017-12-05 23:09:24.070463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2939653 -4.297595 -4.2891154 -4.2706504 -4.2571378 -4.2517524 -4.255465 -4.2646308 -4.2768207 -4.2842035 -4.2810206 -4.2756987 -4.280098 -4.291965 -4.3005996][-4.2967539 -4.2991834 -4.2900367 -4.2733183 -4.2629137 -4.2588787 -4.2602158 -4.26635 -4.2775083 -4.2864633 -4.2849674 -4.2785368 -4.2789454 -4.28646 -4.2922807][-4.2977214 -4.2990012 -4.289773 -4.2762146 -4.2713633 -4.2699585 -4.26737 -4.2665648 -4.2745843 -4.2837071 -4.2846088 -4.2787118 -4.276052 -4.2799792 -4.2842827][-4.2988434 -4.2998939 -4.2924337 -4.2835865 -4.2816381 -4.2782598 -4.2687068 -4.2604771 -4.2657537 -4.2765527 -4.2827563 -4.2802024 -4.2764788 -4.2783804 -4.2823958][-4.2992887 -4.3011594 -4.2971897 -4.2921472 -4.2880516 -4.2775073 -4.2594976 -4.2430973 -4.2445021 -4.2577744 -4.2704763 -4.2749443 -4.276299 -4.2801514 -4.2856812][-4.2967825 -4.3003588 -4.3004479 -4.2989006 -4.29123 -4.26997 -4.2390757 -4.2123837 -4.209918 -4.2272921 -4.2489176 -4.2643647 -4.2745361 -4.2835817 -4.2906189][-4.2931314 -4.2991385 -4.302515 -4.303247 -4.2921777 -4.2607279 -4.216835 -4.1818414 -4.1792707 -4.2015138 -4.2298245 -4.2528329 -4.2706628 -4.2858119 -4.2940731][-4.2923408 -4.2986846 -4.3027868 -4.3048048 -4.2927055 -4.2578249 -4.2119031 -4.17741 -4.1750827 -4.1950693 -4.2204409 -4.2432785 -4.263113 -4.2822952 -4.2926035][-4.2949677 -4.2996087 -4.3015008 -4.3031487 -4.2932777 -4.26336 -4.2269211 -4.200954 -4.19441 -4.1999521 -4.21108 -4.2260175 -4.2448468 -4.2693119 -4.28496][-4.3018856 -4.3047419 -4.3038158 -4.3042049 -4.2969513 -4.2747488 -4.2485962 -4.2297897 -4.2168512 -4.206038 -4.1979551 -4.2003188 -4.2185678 -4.2510018 -4.2753944][-4.3096161 -4.3130245 -4.3089876 -4.306098 -4.2986426 -4.281198 -4.2628622 -4.251307 -4.2376556 -4.2150741 -4.1907635 -4.1817894 -4.1991725 -4.2371287 -4.2692356][-4.3135829 -4.3185415 -4.3113451 -4.3019276 -4.2889647 -4.2723074 -4.2611446 -4.2585869 -4.2512631 -4.2278209 -4.19639 -4.1801982 -4.1956663 -4.235271 -4.2696648][-4.3152485 -4.3186088 -4.3074574 -4.2915254 -4.2727094 -4.2551079 -4.2506013 -4.2579293 -4.2592168 -4.2418656 -4.2133474 -4.1970081 -4.2104564 -4.2448988 -4.2748275][-4.3146386 -4.3150477 -4.3010297 -4.2802439 -4.2568579 -4.2397842 -4.241714 -4.2572494 -4.2652621 -4.2558374 -4.2338729 -4.2186227 -4.226615 -4.2517586 -4.2750826][-4.3124003 -4.3125448 -4.2987213 -4.2746043 -4.2465234 -4.2297907 -4.2360306 -4.2568703 -4.2695875 -4.2645597 -4.2455235 -4.2304015 -4.2316246 -4.2472234 -4.265976]]...]
INFO - root - 2017-12-05 23:09:32.578124: step 52610, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.832 sec/batch; 64h:40m:56s remains)
INFO - root - 2017-12-05 23:09:41.198565: step 52620, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 67h:32m:00s remains)
INFO - root - 2017-12-05 23:09:49.739323: step 52630, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 64h:33m:49s remains)
INFO - root - 2017-12-05 23:09:58.314752: step 52640, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:23m:23s remains)
INFO - root - 2017-12-05 23:10:06.727680: step 52650, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 66h:11m:18s remains)
INFO - root - 2017-12-05 23:10:15.336137: step 52660, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:53m:06s remains)
INFO - root - 2017-12-05 23:10:23.791572: step 52670, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.878 sec/batch; 68h:13m:48s remains)
INFO - root - 2017-12-05 23:10:32.310700: step 52680, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 66h:49m:27s remains)
INFO - root - 2017-12-05 23:10:40.910455: step 52690, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 68h:00m:41s remains)
INFO - root - 2017-12-05 23:10:49.573057: step 52700, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 65h:48m:09s remains)
2017-12-05 23:10:50.324348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2389183 -4.2374773 -4.2365932 -4.2401814 -4.2449923 -4.251615 -4.2580466 -4.2640405 -4.2708912 -4.2792788 -4.2828083 -4.2748642 -4.2613358 -4.2547836 -4.2571292][-4.2684617 -4.2646122 -4.2606754 -4.2597623 -4.2574229 -4.2544107 -4.2509756 -4.2545514 -4.2656488 -4.2829018 -4.2969532 -4.2979021 -4.2899766 -4.2819691 -4.2765045][-4.292573 -4.2880435 -4.2821665 -4.2768207 -4.2673578 -4.2539506 -4.2390046 -4.2363997 -4.2489924 -4.2738576 -4.2971621 -4.3082213 -4.3096924 -4.3053637 -4.2970052][-4.3063836 -4.3004818 -4.2899618 -4.277422 -4.2561741 -4.2310915 -4.2074528 -4.1980653 -4.2110472 -4.2432904 -4.2786541 -4.3037992 -4.3188314 -4.32535 -4.3198996][-4.3020349 -4.2949109 -4.2765503 -4.25374 -4.2222714 -4.1909657 -4.1679387 -4.1608744 -4.1797242 -4.2168579 -4.2588153 -4.292479 -4.3160706 -4.3308897 -4.3298607][-4.2851205 -4.2720118 -4.2446671 -4.2160363 -4.1854105 -4.1601539 -4.1474867 -4.1524081 -4.1796021 -4.2172151 -4.2539167 -4.2820148 -4.3033147 -4.3193417 -4.3226185][-4.2616653 -4.2410192 -4.209702 -4.1850872 -4.1648941 -4.1517897 -4.1509066 -4.1663494 -4.19983 -4.2350688 -4.2594876 -4.2700176 -4.2797542 -4.2921643 -4.2983603][-4.2348561 -4.211009 -4.18331 -4.1680932 -4.1595187 -4.1558909 -4.1633205 -4.1856394 -4.2194085 -4.2483144 -4.2582827 -4.2498126 -4.246315 -4.2532349 -4.2624416][-4.2106228 -4.1889324 -4.169085 -4.16259 -4.161099 -4.16143 -4.1722679 -4.1943216 -4.2230554 -4.2436829 -4.2426796 -4.22487 -4.2171383 -4.2247343 -4.2381845][-4.19029 -4.1688871 -4.1537442 -4.1497374 -4.1467524 -4.14587 -4.1562071 -4.1740279 -4.1965113 -4.2108736 -4.2062793 -4.1912336 -4.1865926 -4.1986303 -4.2177482][-4.1841011 -4.1652613 -4.1554642 -4.1518903 -4.1439171 -4.1367426 -4.1394262 -4.1472244 -4.1588035 -4.1685882 -4.1671362 -4.1597829 -4.1614842 -4.1799879 -4.2067156][-4.2016726 -4.187892 -4.18328 -4.1803775 -4.1690764 -4.1553311 -4.1482658 -4.1474319 -4.1520309 -4.1597333 -4.1642265 -4.1639447 -4.1701894 -4.190279 -4.2187238][-4.2393765 -4.2279482 -4.2252197 -4.2246094 -4.2153792 -4.2014289 -4.1898909 -4.1857409 -4.1877279 -4.1942358 -4.1995053 -4.2008224 -4.2051535 -4.2181239 -4.2364178][-4.2750897 -4.2671704 -4.2664304 -4.2693348 -4.2652817 -4.2556291 -4.2452865 -4.2401366 -4.2402949 -4.2429476 -4.2455859 -4.2455935 -4.2458682 -4.2502742 -4.2564197][-4.2978573 -4.296926 -4.3002219 -4.3066554 -4.3069706 -4.3017635 -4.2937961 -4.2889605 -4.2878113 -4.2883596 -4.289824 -4.2891541 -4.286109 -4.2842903 -4.2828484]]...]
INFO - root - 2017-12-05 23:10:58.867340: step 52710, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:26m:18s remains)
INFO - root - 2017-12-05 23:11:07.443742: step 52720, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 67h:47m:45s remains)
INFO - root - 2017-12-05 23:11:16.049611: step 52730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 67h:11m:43s remains)
INFO - root - 2017-12-05 23:11:24.493825: step 52740, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 66h:40m:33s remains)
INFO - root - 2017-12-05 23:11:32.958347: step 52750, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:46m:37s remains)
INFO - root - 2017-12-05 23:11:41.535196: step 52760, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 66h:46m:08s remains)
INFO - root - 2017-12-05 23:11:50.095546: step 52770, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 65h:50m:49s remains)
INFO - root - 2017-12-05 23:11:58.680431: step 52780, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 66h:40m:36s remains)
INFO - root - 2017-12-05 23:12:07.227391: step 52790, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 66h:56m:10s remains)
INFO - root - 2017-12-05 23:12:15.686662: step 52800, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 66h:00m:05s remains)
2017-12-05 23:12:16.460404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.05529 -4.069313 -4.0815 -4.0733171 -4.0595608 -4.0606351 -4.0700765 -4.0855722 -4.1001639 -4.1268325 -4.1687894 -4.1961136 -4.2064319 -4.2146463 -4.21358][-4.0485859 -4.0674005 -4.0789051 -4.0653563 -4.0402656 -4.0356836 -4.0550737 -4.0876303 -4.1119957 -4.1274319 -4.1507244 -4.1661267 -4.1757007 -4.1914654 -4.204124][-4.0658207 -4.0948782 -4.1073966 -4.080833 -4.0387959 -4.0190477 -4.0275364 -4.0592647 -4.0895724 -4.1027956 -4.1130605 -4.11775 -4.1270289 -4.1482925 -4.1768026][-4.0963087 -4.1263657 -4.135807 -4.097971 -4.0441332 -4.0110412 -4.0029836 -4.0184779 -4.0406394 -4.0457358 -4.0449948 -4.0442095 -4.0583673 -4.0896716 -4.1329427][-4.1207738 -4.1403365 -4.1345577 -4.0892749 -4.0273681 -3.9836581 -3.9598224 -3.965755 -3.9850056 -3.9854562 -3.9756064 -3.9710622 -3.9938972 -4.0405607 -4.0965028][-4.142765 -4.1483579 -4.1257639 -4.0654163 -3.9843302 -3.9094839 -3.8570335 -3.8673329 -3.9214318 -3.9484658 -3.9387784 -3.9282491 -3.957828 -4.0200567 -4.0890956][-4.1593256 -4.1559997 -4.1293068 -4.0615172 -3.9522474 -3.8274195 -3.7143412 -3.7285726 -3.8450763 -3.9206014 -3.92558 -3.9081657 -3.9371307 -4.0028429 -4.0884013][-4.1591067 -4.1480904 -4.1254811 -4.0685406 -3.9612443 -3.8219614 -3.6675668 -3.6558475 -3.7991672 -3.9031656 -3.9308081 -3.9225433 -3.9460711 -4.0057058 -4.0917139][-4.1501737 -4.1320567 -4.1104746 -4.0698261 -4.0027695 -3.9166267 -3.8093164 -3.7840598 -3.8605046 -3.9317441 -3.9602787 -3.9581361 -3.9700809 -4.0221691 -4.1051254][-4.1457114 -4.1278043 -4.1055036 -4.0739679 -4.0366492 -3.9990962 -3.9519651 -3.9408896 -3.9687047 -4.00069 -4.0092354 -3.9936297 -3.982868 -4.0150805 -4.0843706][-4.1294518 -4.1249285 -4.1154261 -4.0947018 -4.0717826 -4.0519962 -4.0296507 -4.0305328 -4.0449004 -4.0560465 -4.04393 -4.0126786 -3.990397 -4.0089984 -4.0509052][-4.0911808 -4.1022196 -4.1032448 -4.0944948 -4.0847473 -4.077457 -4.0682321 -4.0712833 -4.0812874 -4.0858555 -4.0626903 -4.0276885 -4.0103393 -4.016902 -4.0283856][-4.067821 -4.0827012 -4.0876508 -4.0767908 -4.0702419 -4.0714579 -4.0721097 -4.0747309 -4.0775986 -4.0769596 -4.0578032 -4.0335627 -4.0251026 -4.0262313 -4.0203357][-4.0918794 -4.1006641 -4.0950689 -4.0801916 -4.0713358 -4.0692377 -4.0627789 -4.0555491 -4.0455303 -4.0354323 -4.0155373 -3.9956465 -3.9998705 -4.0133104 -4.0098743][-4.1223173 -4.1277261 -4.1220393 -4.1101832 -4.10048 -4.0902705 -4.0645618 -4.037117 -4.0162516 -4.0044041 -3.9859831 -3.9675889 -3.9753 -3.9927671 -3.9867375]]...]
INFO - root - 2017-12-05 23:12:25.115446: step 52810, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 64h:59m:32s remains)
INFO - root - 2017-12-05 23:12:33.709120: step 52820, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:44m:58s remains)
INFO - root - 2017-12-05 23:12:42.153578: step 52830, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 67h:57m:59s remains)
INFO - root - 2017-12-05 23:12:50.814038: step 52840, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 66h:11m:26s remains)
INFO - root - 2017-12-05 23:12:59.245341: step 52850, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 63h:43m:35s remains)
INFO - root - 2017-12-05 23:13:07.829764: step 52860, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.850 sec/batch; 66h:02m:44s remains)
INFO - root - 2017-12-05 23:13:16.486328: step 52870, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 67h:37m:39s remains)
INFO - root - 2017-12-05 23:13:25.045383: step 52880, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.856 sec/batch; 66h:27m:02s remains)
INFO - root - 2017-12-05 23:13:33.542992: step 52890, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.822 sec/batch; 63h:49m:09s remains)
INFO - root - 2017-12-05 23:13:42.110808: step 52900, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 67h:27m:07s remains)
2017-12-05 23:13:42.874922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3328047 -4.3203688 -4.2951136 -4.2549663 -4.2136769 -4.1849985 -4.1701183 -4.1733713 -4.1902914 -4.2053037 -4.2069874 -4.2038059 -4.1992593 -4.1956162 -4.1949515][-4.3354883 -4.3233824 -4.294868 -4.2451158 -4.1954875 -4.1615405 -4.13903 -4.1377335 -4.1570983 -4.1783552 -4.1878681 -4.1917419 -4.1880546 -4.1807971 -4.174726][-4.3348975 -4.32292 -4.2923536 -4.2371173 -4.1869226 -4.1557431 -4.129571 -4.1236105 -4.1426816 -4.1660123 -4.1836338 -4.1975632 -4.1972561 -4.1880121 -4.1800494][-4.3311825 -4.3184223 -4.2861729 -4.2298579 -4.1829462 -4.1580672 -4.1341414 -4.1259131 -4.1442828 -4.1664662 -4.1894512 -4.211453 -4.214293 -4.2032828 -4.1970143][-4.3249216 -4.3087263 -4.27175 -4.213017 -4.1665 -4.1438351 -4.1223707 -4.1137857 -4.1300464 -4.1494055 -4.1783843 -4.2086873 -4.2167454 -4.2078886 -4.2065091][-4.3183804 -4.2969341 -4.2529373 -4.1894865 -4.139039 -4.1133146 -4.0911813 -4.0819335 -4.0941644 -4.1094623 -4.1452603 -4.1847286 -4.2017145 -4.199409 -4.2058983][-4.3135653 -4.2865777 -4.2358751 -4.1697588 -4.115417 -4.0857606 -4.0616956 -4.0527835 -4.0624042 -4.0725985 -4.1101017 -4.1566467 -4.1841784 -4.1916466 -4.204576][-4.31102 -4.2809143 -4.2288494 -4.1672082 -4.1164675 -4.089963 -4.0680757 -4.0607147 -4.0712719 -4.0794563 -4.1141934 -4.1591473 -4.1887956 -4.202733 -4.2186184][-4.3116198 -4.2838359 -4.239295 -4.1908197 -4.1521807 -4.13377 -4.1152396 -4.1063929 -4.1159053 -4.1271605 -4.1572261 -4.1922441 -4.2147341 -4.2271104 -4.2419024][-4.3145161 -4.2922206 -4.2564931 -4.2190733 -4.1903982 -4.1767411 -4.1597552 -4.1504874 -4.1578732 -4.171926 -4.1999922 -4.2296724 -4.2474332 -4.2581449 -4.2734957][-4.3185487 -4.3016963 -4.2724466 -4.23987 -4.2167664 -4.2054448 -4.1893635 -4.179688 -4.1860266 -4.1999278 -4.2279797 -4.255228 -4.2728863 -4.286067 -4.3004608][-4.3221436 -4.3082218 -4.2823272 -4.2518692 -4.2310457 -4.2205248 -4.2086515 -4.2030578 -4.212677 -4.2276134 -4.2544479 -4.2796669 -4.2959852 -4.3078952 -4.3176675][-4.3236046 -4.3098259 -4.2856512 -4.2566094 -4.2374544 -4.2271256 -4.2191138 -4.2195477 -4.2313251 -4.2459188 -4.2681665 -4.2892237 -4.3039927 -4.3136296 -4.3179641][-4.3231721 -4.306601 -4.2815948 -4.251255 -4.2299695 -4.21831 -4.2125258 -4.2190533 -4.2328463 -4.2468166 -4.2650757 -4.2826343 -4.2962565 -4.3049569 -4.3069415][-4.3238826 -4.3047504 -4.2762237 -4.2417526 -4.2154694 -4.2011609 -4.197216 -4.2077346 -4.2244148 -4.2398505 -4.2560172 -4.27057 -4.2837439 -4.29251 -4.294364]]...]
INFO - root - 2017-12-05 23:13:51.406224: step 52910, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 66h:58m:46s remains)
INFO - root - 2017-12-05 23:14:00.028726: step 52920, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 63h:53m:36s remains)
INFO - root - 2017-12-05 23:14:08.633940: step 52930, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 68h:35m:45s remains)
INFO - root - 2017-12-05 23:14:17.223809: step 52940, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 66h:19m:36s remains)
INFO - root - 2017-12-05 23:14:25.637431: step 52950, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 66h:11m:58s remains)
INFO - root - 2017-12-05 23:14:34.184798: step 52960, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 65h:10m:43s remains)
INFO - root - 2017-12-05 23:14:42.672019: step 52970, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 65h:54m:11s remains)
INFO - root - 2017-12-05 23:14:51.322886: step 52980, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 65h:15m:52s remains)
INFO - root - 2017-12-05 23:14:59.876515: step 52990, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 65h:48m:02s remains)
INFO - root - 2017-12-05 23:15:08.316374: step 53000, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 65h:25m:31s remains)
2017-12-05 23:15:09.035736: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2887192 -4.2920361 -4.2964454 -4.2971711 -4.296524 -4.2922382 -4.2875056 -4.2819009 -4.275991 -4.2716174 -4.2730174 -4.28479 -4.3010421 -4.31541 -4.3274217][-4.266808 -4.2721105 -4.2774091 -4.278985 -4.27947 -4.2759948 -4.2710834 -4.2642956 -4.2549696 -4.2450247 -4.2422867 -4.2537546 -4.2715826 -4.2871695 -4.3063617][-4.246624 -4.2517405 -4.2570724 -4.2608051 -4.2627125 -4.2598791 -4.254756 -4.2495384 -4.2367892 -4.221364 -4.21599 -4.2275276 -4.2433343 -4.2579675 -4.2828507][-4.2311568 -4.233047 -4.2376461 -4.2449288 -4.2506766 -4.2509389 -4.2483153 -4.24735 -4.2335839 -4.2151141 -4.208281 -4.21578 -4.2256889 -4.2359042 -4.2635665][-4.2178521 -4.2151389 -4.2160892 -4.2261167 -4.2353115 -4.2382464 -4.2375612 -4.2386923 -4.225215 -4.2061906 -4.1977592 -4.2030983 -4.2115474 -4.2222509 -4.2523513][-4.2091422 -4.1985631 -4.1913767 -4.2000113 -4.2105656 -4.216958 -4.2178535 -4.219595 -4.2067156 -4.1848679 -4.1716542 -4.175139 -4.1899595 -4.2071767 -4.2407856][-4.2083435 -4.1899676 -4.1729817 -4.1760917 -4.1865416 -4.1970005 -4.2007861 -4.2047396 -4.1951447 -4.1736441 -4.1537023 -4.1496696 -4.1680517 -4.1926961 -4.2302694][-4.2110868 -4.1883388 -4.1642404 -4.160893 -4.1713805 -4.1863861 -4.1948633 -4.2052507 -4.2040319 -4.1867938 -4.1629715 -4.1470795 -4.1595616 -4.1855245 -4.2249703][-4.22434 -4.2054691 -4.1817446 -4.1735277 -4.1821647 -4.1990457 -4.2110481 -4.2246575 -4.2263284 -4.2130661 -4.1912227 -4.1718426 -4.1776576 -4.1970563 -4.2313585][-4.2425995 -4.2315617 -4.2140431 -4.2057586 -4.2153807 -4.2344179 -4.2477608 -4.2594824 -4.2598586 -4.2473559 -4.2288918 -4.2113709 -4.2127118 -4.2240672 -4.2470241][-4.2713928 -4.2660184 -4.2504416 -4.2404857 -4.2476945 -4.2653012 -4.2764125 -4.2853856 -4.2854753 -4.2750845 -4.2587309 -4.2419343 -4.2376189 -4.2424335 -4.2568693][-4.2919016 -4.2881122 -4.2725434 -4.2609329 -4.2652669 -4.2806726 -4.2893777 -4.2949929 -4.2944531 -4.2866979 -4.2716131 -4.2558713 -4.2484536 -4.2503762 -4.2607059][-4.3021679 -4.29792 -4.2842784 -4.2736406 -4.2763476 -4.2880716 -4.2942533 -4.297214 -4.296145 -4.2896047 -4.2766886 -4.2628927 -4.2552562 -4.2564306 -4.2654314][-4.3006649 -4.2972593 -4.2876515 -4.2815938 -4.2840705 -4.2901177 -4.2939792 -4.2960415 -4.2949462 -4.2896633 -4.2808371 -4.2715449 -4.266295 -4.2687497 -4.2788682][-4.2955446 -4.2932205 -4.2887812 -4.2864647 -4.2868295 -4.2871633 -4.2879505 -4.2898707 -4.2907925 -4.2890778 -4.285212 -4.2815843 -4.2803917 -4.2848825 -4.2963543]]...]
INFO - root - 2017-12-05 23:15:17.633148: step 53010, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 66h:28m:22s remains)
INFO - root - 2017-12-05 23:15:26.274651: step 53020, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 67h:11m:54s remains)
INFO - root - 2017-12-05 23:15:34.762404: step 53030, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 68h:44m:56s remains)
INFO - root - 2017-12-05 23:15:43.346798: step 53040, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 67h:18m:51s remains)
INFO - root - 2017-12-05 23:15:51.826630: step 53050, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 66h:45m:24s remains)
INFO - root - 2017-12-05 23:16:00.313016: step 53060, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 66h:55m:08s remains)
INFO - root - 2017-12-05 23:16:08.956226: step 53070, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 67h:15m:20s remains)
INFO - root - 2017-12-05 23:16:17.410363: step 53080, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 65h:57m:28s remains)
INFO - root - 2017-12-05 23:16:26.023988: step 53090, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.838 sec/batch; 65h:02m:10s remains)
INFO - root - 2017-12-05 23:16:34.519213: step 53100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:59m:04s remains)
2017-12-05 23:16:35.320230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3229089 -4.3233671 -4.3232093 -4.3246589 -4.3270583 -4.3278279 -4.3275523 -4.3272996 -4.3263869 -4.326282 -4.326407 -4.3250661 -4.3236179 -4.3214397 -4.3191905][-4.3189178 -4.3191156 -4.3168368 -4.3188071 -4.3238964 -4.3247743 -4.323082 -4.3229222 -4.3222241 -4.3229327 -4.3242469 -4.32294 -4.3224883 -4.32109 -4.3172984][-4.3041162 -4.3034091 -4.2983732 -4.301208 -4.3125734 -4.3153229 -4.3123884 -4.3121066 -4.3110347 -4.310308 -4.3116131 -4.3109574 -4.31309 -4.3133221 -4.3071404][-4.2791743 -4.2792382 -4.2709951 -4.271534 -4.2875891 -4.2921796 -4.2860184 -4.2833962 -4.2821875 -4.2787504 -4.2790036 -4.2824674 -4.2909212 -4.2914939 -4.2807302][-4.2382693 -4.2393689 -4.2247634 -4.2182035 -4.2380662 -4.2485952 -4.2436318 -4.2407422 -4.238564 -4.2291074 -4.2230663 -4.2319093 -4.2516632 -4.259656 -4.2505388][-4.1851792 -4.1840205 -4.154192 -4.1340003 -4.1577969 -4.1821465 -4.1880817 -4.1899209 -4.18923 -4.1719255 -4.1545362 -4.168869 -4.2049527 -4.2263389 -4.2215772][-4.1250405 -4.1164703 -4.0684505 -4.0327597 -4.06171 -4.1015897 -4.1195168 -4.1273432 -4.1277547 -4.1053567 -4.0798607 -4.1036949 -4.1567149 -4.1901746 -4.1927648][-4.0891175 -4.07506 -4.0115232 -3.9582736 -3.9811537 -4.0234637 -4.0468321 -4.0593748 -4.0641813 -4.0403938 -4.0086641 -4.0379419 -4.1034713 -4.147459 -4.1625142][-4.0985656 -4.0832734 -4.015811 -3.9524522 -3.9635742 -4.002296 -4.0313663 -4.0460472 -4.0506415 -4.0301681 -3.9960325 -4.0207362 -4.0859895 -4.134232 -4.1524229][-4.1291027 -4.1211114 -4.0661025 -4.0100474 -4.0157919 -4.0483222 -4.0792413 -4.09384 -4.0975523 -4.0871563 -4.0615735 -4.0770497 -4.1270709 -4.1658945 -4.176898][-4.1616464 -4.1627817 -4.1257439 -4.0853429 -4.0912056 -4.11953 -4.1486549 -4.1626787 -4.1683989 -4.1701751 -4.15697 -4.1637568 -4.1953816 -4.2185478 -4.2203684][-4.1774707 -4.1806884 -4.15727 -4.1295886 -4.1357851 -4.1609025 -4.1902337 -4.2077074 -4.2179022 -4.22796 -4.2257771 -4.23109 -4.2511358 -4.2644911 -4.261785][-4.1895289 -4.1918006 -4.1777291 -4.1618357 -4.1713114 -4.1942396 -4.2211246 -4.2385654 -4.2488966 -4.2587481 -4.2615247 -4.2671294 -4.279181 -4.2880411 -4.2875195][-4.2136064 -4.2160311 -4.2110014 -4.2072935 -4.2199941 -4.2386732 -4.2615948 -4.2764645 -4.28149 -4.2846179 -4.2857113 -4.2887011 -4.2954092 -4.2999468 -4.3008785][-4.235014 -4.2375975 -4.2339287 -4.2349787 -4.2504053 -4.2685647 -4.2888193 -4.302268 -4.3013883 -4.2968688 -4.2938819 -4.2968526 -4.3023539 -4.304811 -4.3064408]]...]
INFO - root - 2017-12-05 23:16:43.750479: step 53110, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 67h:16m:30s remains)
INFO - root - 2017-12-05 23:16:52.309366: step 53120, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.863 sec/batch; 66h:59m:30s remains)
INFO - root - 2017-12-05 23:17:00.827235: step 53130, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.870 sec/batch; 67h:33m:11s remains)
INFO - root - 2017-12-05 23:17:09.394790: step 53140, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:46m:14s remains)
INFO - root - 2017-12-05 23:17:17.980213: step 53150, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 66h:03m:28s remains)
INFO - root - 2017-12-05 23:17:26.547171: step 53160, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 66h:59m:29s remains)
INFO - root - 2017-12-05 23:17:35.053560: step 53170, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 65h:40m:43s remains)
INFO - root - 2017-12-05 23:17:43.697352: step 53180, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 66h:55m:36s remains)
INFO - root - 2017-12-05 23:17:52.253756: step 53190, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 68h:03m:31s remains)
INFO - root - 2017-12-05 23:18:00.816284: step 53200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 66h:05m:24s remains)
2017-12-05 23:18:01.615456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1949983 -4.1938744 -4.2093778 -4.2301574 -4.2347794 -4.2250185 -4.2176137 -4.2170553 -4.2101908 -4.2008624 -4.1937089 -4.2037678 -4.2253213 -4.2439837 -4.2587986][-4.1225777 -4.1146512 -4.141901 -4.1890955 -4.2192712 -4.2283764 -4.2381063 -4.2483859 -4.2482462 -4.2425218 -4.2366319 -4.2455349 -4.2598839 -4.2679482 -4.2744789][-4.05847 -4.0397034 -4.0772514 -4.1483612 -4.2025504 -4.2243681 -4.243773 -4.2623448 -4.271883 -4.2722416 -4.2715411 -4.2807465 -4.2883949 -4.2886982 -4.2872348][-4.0462685 -4.0264606 -4.06737 -4.1428003 -4.2010994 -4.2226276 -4.2380056 -4.2526913 -4.2676764 -4.278039 -4.2867317 -4.2997346 -4.3066735 -4.3038259 -4.2954459][-4.1181388 -4.106389 -4.1371226 -4.1888657 -4.2230458 -4.222795 -4.2158585 -4.2134919 -4.226161 -4.2501922 -4.2749829 -4.2968979 -4.3108339 -4.3102155 -4.2988572][-4.2095704 -4.2054658 -4.2188129 -4.2341385 -4.2279458 -4.1874728 -4.1393619 -4.1184726 -4.1395512 -4.1858258 -4.23606 -4.2737656 -4.2998 -4.306355 -4.2958274][-4.2701917 -4.265501 -4.2579842 -4.2376304 -4.1869078 -4.0920296 -3.9937437 -3.9732921 -4.031496 -4.1158004 -4.19459 -4.2495294 -4.2858262 -4.2962422 -4.2889218][-4.2879324 -4.27622 -4.2508945 -4.2121992 -4.1359439 -4.0033641 -3.8730364 -3.8728037 -3.9770534 -4.0899425 -4.1781373 -4.2384763 -4.275146 -4.2845912 -4.2781663][-4.2681422 -4.2502337 -4.2203565 -4.1883554 -4.1285338 -4.021852 -3.9273214 -3.9423492 -4.0406203 -4.1380892 -4.2080369 -4.253932 -4.2789297 -4.2830434 -4.2739429][-4.2193651 -4.2066441 -4.1936994 -4.1883473 -4.1681619 -4.1207385 -4.0799332 -4.0954084 -4.155395 -4.2173882 -4.2584882 -4.2842822 -4.2927761 -4.2864165 -4.2722054][-4.1743646 -4.17749 -4.1876864 -4.2062855 -4.2151651 -4.2091665 -4.2008734 -4.2115488 -4.2411966 -4.2751942 -4.2982464 -4.3085513 -4.2995882 -4.2849121 -4.2709465][-4.1655335 -4.1804242 -4.1998076 -4.2206135 -4.2373447 -4.2480426 -4.2518773 -4.25591 -4.2673111 -4.2870269 -4.3018808 -4.3019786 -4.2862544 -4.2752891 -4.2715092][-4.19508 -4.2112136 -4.2200789 -4.2258172 -4.2356892 -4.2488489 -4.2526679 -4.2444673 -4.2407341 -4.2542424 -4.265203 -4.2639384 -4.2558532 -4.2593532 -4.2696457][-4.221046 -4.2292833 -4.2195735 -4.2085423 -4.2137527 -4.2274094 -4.2253685 -4.204288 -4.1866817 -4.1921473 -4.2022266 -4.2122045 -4.2229505 -4.2450819 -4.2641764][-4.22444 -4.2207856 -4.1977644 -4.1783738 -4.1824374 -4.1971045 -4.189889 -4.1575775 -4.121542 -4.1128478 -4.1280522 -4.1586232 -4.1925159 -4.2302179 -4.2567449]]...]
INFO - root - 2017-12-05 23:18:10.100946: step 53210, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 65h:11m:06s remains)
INFO - root - 2017-12-05 23:18:18.568643: step 53220, loss = 2.05, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 68h:44m:22s remains)
INFO - root - 2017-12-05 23:18:27.196358: step 53230, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 67h:16m:00s remains)
INFO - root - 2017-12-05 23:18:35.662328: step 53240, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.893 sec/batch; 69h:16m:42s remains)
INFO - root - 2017-12-05 23:18:44.251495: step 53250, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 64h:40m:49s remains)
INFO - root - 2017-12-05 23:18:52.749930: step 53260, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 64h:25m:49s remains)
INFO - root - 2017-12-05 23:19:01.241546: step 53270, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 67h:16m:05s remains)
INFO - root - 2017-12-05 23:19:09.633846: step 53280, loss = 2.11, batch loss = 2.05 (9.3 examples/sec; 0.863 sec/batch; 66h:57m:51s remains)
INFO - root - 2017-12-05 23:19:18.087062: step 53290, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 66h:34m:43s remains)
INFO - root - 2017-12-05 23:19:26.595919: step 53300, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 66h:59m:08s remains)
2017-12-05 23:19:27.389147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3246822 -4.3247828 -4.3248677 -4.3249388 -4.3248749 -4.3245883 -4.3241572 -4.3236136 -4.3232436 -4.3229713 -4.3226566 -4.3222237 -4.3217177 -4.3210115 -4.3203897][-4.3122149 -4.3139229 -4.3153195 -4.3162508 -4.316381 -4.3156362 -4.3145838 -4.3135867 -4.3130789 -4.3126383 -4.3120852 -4.3108687 -4.3091269 -4.307076 -4.3049393][-4.2942376 -4.2994261 -4.3039393 -4.3063459 -4.3067379 -4.3057022 -4.3042326 -4.3027997 -4.3022165 -4.302145 -4.3021431 -4.3005815 -4.2968249 -4.2913804 -4.2854667][-4.2786913 -4.287076 -4.29411 -4.2973084 -4.2969384 -4.2957015 -4.2946033 -4.2925563 -4.2910333 -4.2905211 -4.2919779 -4.2921271 -4.287847 -4.2785077 -4.2679524][-4.2684407 -4.2770309 -4.2844768 -4.2871156 -4.2848105 -4.283154 -4.2837853 -4.2819633 -4.2775564 -4.273809 -4.2747521 -4.2753568 -4.270853 -4.2596416 -4.2460432][-4.265214 -4.270967 -4.2751961 -4.2746468 -4.2689118 -4.2664967 -4.2699947 -4.2708516 -4.2631726 -4.2531638 -4.2515 -4.2538557 -4.2517333 -4.2434788 -4.232697][-4.2627707 -4.2666168 -4.2689333 -4.2661967 -4.2587175 -4.2555037 -4.2607355 -4.2636256 -4.2529 -4.2368984 -4.2312627 -4.2346072 -4.236321 -4.2358503 -4.2307949][-4.256979 -4.2621031 -4.2641368 -4.2586408 -4.2486105 -4.2441573 -4.2502337 -4.2547092 -4.2424245 -4.2203832 -4.205699 -4.2050881 -4.2105494 -4.2213068 -4.22802][-4.2327089 -4.2432041 -4.2468157 -4.2378664 -4.2248158 -4.2198038 -4.2263026 -4.23153 -4.220129 -4.1945262 -4.1727228 -4.1694875 -4.1813383 -4.2062411 -4.225924][-4.1974893 -4.213273 -4.2163892 -4.2031293 -4.1882792 -4.1842837 -4.19125 -4.1962876 -4.18742 -4.1649671 -4.1484694 -4.151196 -4.1704512 -4.2019548 -4.2243981][-4.1633406 -4.17896 -4.1816468 -4.1685152 -4.1538987 -4.1484113 -4.15362 -4.1610842 -4.1600657 -4.1506572 -4.1491117 -4.1593852 -4.1802068 -4.2072997 -4.2239885][-4.1537642 -4.1650681 -4.167727 -4.1575465 -4.1437173 -4.1374078 -4.14297 -4.1556697 -4.1648293 -4.168714 -4.1775975 -4.1883745 -4.2018275 -4.216661 -4.224216][-4.1568284 -4.16512 -4.1687393 -4.1649623 -4.157196 -4.1555128 -4.1651912 -4.1821775 -4.1960711 -4.204257 -4.2138195 -4.2193327 -4.2245274 -4.2290115 -4.2268729][-4.1572003 -4.1676784 -4.17704 -4.1816773 -4.1795692 -4.1781325 -4.1866503 -4.2028518 -4.2165527 -4.2254605 -4.232512 -4.2337713 -4.2333136 -4.2314553 -4.2230072][-4.1627674 -4.1776218 -4.1899366 -4.1966386 -4.1956673 -4.1922073 -4.1990881 -4.2135954 -4.2231264 -4.2281904 -4.2338457 -4.2360706 -4.2331219 -4.2274284 -4.2154589]]...]
INFO - root - 2017-12-05 23:19:35.885824: step 53310, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 65h:44m:16s remains)
INFO - root - 2017-12-05 23:19:44.426702: step 53320, loss = 2.10, batch loss = 2.05 (9.2 examples/sec; 0.872 sec/batch; 67h:36m:07s remains)
INFO - root - 2017-12-05 23:19:52.820068: step 53330, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 65h:10m:02s remains)
INFO - root - 2017-12-05 23:20:01.436743: step 53340, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 66h:08m:13s remains)
INFO - root - 2017-12-05 23:20:09.842396: step 53350, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 63h:08m:07s remains)
INFO - root - 2017-12-05 23:20:18.515604: step 53360, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 67h:42m:24s remains)
INFO - root - 2017-12-05 23:20:27.052753: step 53370, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.869 sec/batch; 67h:21m:57s remains)
INFO - root - 2017-12-05 23:20:35.436421: step 53380, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 64h:59m:27s remains)
INFO - root - 2017-12-05 23:20:44.007601: step 53390, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 67h:52m:46s remains)
INFO - root - 2017-12-05 23:20:52.637692: step 53400, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 66h:15m:12s remains)
2017-12-05 23:20:53.580131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2510324 -4.225009 -4.2045302 -4.1931667 -4.1957006 -4.2065787 -4.2181916 -4.2317996 -4.24318 -4.2521029 -4.2546754 -4.2504182 -4.2421665 -4.2309494 -4.2237129][-4.2517929 -4.2219777 -4.1944814 -4.1741767 -4.1690373 -4.1741037 -4.1819348 -4.1952672 -4.2106776 -4.2255607 -4.2327113 -4.2295389 -4.2221727 -4.2134 -4.212173][-4.2665105 -4.2330294 -4.1987915 -4.169805 -4.1528397 -4.1490669 -4.1494784 -4.158349 -4.1771317 -4.2000418 -4.2156162 -4.2150288 -4.2079396 -4.2022657 -4.2088103][-4.2832747 -4.2476568 -4.2101865 -4.1750989 -4.1429615 -4.1243606 -4.1127114 -4.1124258 -4.1322088 -4.1651568 -4.1942449 -4.203608 -4.2017131 -4.1991963 -4.2106714][-4.2924976 -4.2549405 -4.2127624 -4.166625 -4.1154404 -4.0758228 -4.0436997 -4.0293283 -4.0466027 -4.0909467 -4.1400781 -4.16733 -4.1752162 -4.1781178 -4.193964][-4.2918215 -4.2495885 -4.2006392 -4.139565 -4.0686135 -4.006597 -3.9501741 -3.9143877 -3.9266443 -3.9828279 -4.0502405 -4.0952578 -4.11238 -4.1211948 -4.1445746][-4.2779083 -4.2282944 -4.1693592 -4.0934443 -4.00689 -3.9251354 -3.8457634 -3.7871697 -3.7958038 -3.8683255 -3.9556441 -4.0166383 -4.0419121 -4.0571814 -4.0884366][-4.2576876 -4.2039289 -4.1414919 -4.061235 -3.9779844 -3.9020205 -3.8284013 -3.770376 -3.7744422 -3.8404093 -3.9252918 -3.988132 -4.0161953 -4.0349345 -4.0693555][-4.2527895 -4.2049146 -4.1497393 -4.0803313 -4.0196748 -3.9712658 -3.9283071 -3.8975475 -3.9040549 -3.9425948 -3.9979653 -4.0452886 -4.0710154 -4.0893197 -4.1200233][-4.2669864 -4.2312737 -4.1897831 -4.1365933 -4.0968766 -4.0715041 -4.0526395 -4.0432572 -4.0552611 -4.079082 -4.1128654 -4.1453924 -4.167635 -4.1849165 -4.2088456][-4.2889953 -4.2647409 -4.2363877 -4.1970391 -4.1702061 -4.15791 -4.1524148 -4.1546397 -4.1692486 -4.1875539 -4.2116356 -4.2358985 -4.2542181 -4.2699761 -4.2890296][-4.3061705 -4.2884989 -4.270206 -4.2447128 -4.2298803 -4.2257562 -4.2273946 -4.2366276 -4.252717 -4.2683907 -4.2861304 -4.3024321 -4.3149967 -4.3252988 -4.3372679][-4.3236279 -4.3128042 -4.3049707 -4.2949286 -4.2917995 -4.2938695 -4.297914 -4.3065438 -4.3185496 -4.3290825 -4.3400168 -4.3493705 -4.3555694 -4.3585491 -4.3616614][-4.3406487 -4.3344107 -4.3327026 -4.3311925 -4.33303 -4.337184 -4.3410082 -4.3461719 -4.3533134 -4.3590603 -4.3644891 -4.3690567 -4.3724651 -4.3728905 -4.3711376][-4.3502045 -4.3465972 -4.3458481 -4.34596 -4.3481107 -4.351563 -4.3545671 -4.3577666 -4.3618045 -4.3647013 -4.3672252 -4.3695321 -4.3710513 -4.3697209 -4.365438]]...]
INFO - root - 2017-12-05 23:21:02.040105: step 53410, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:50m:14s remains)
INFO - root - 2017-12-05 23:21:10.569031: step 53420, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.892 sec/batch; 69h:11m:12s remains)
INFO - root - 2017-12-05 23:21:19.101850: step 53430, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:33m:52s remains)
INFO - root - 2017-12-05 23:21:27.661231: step 53440, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 66h:16m:07s remains)
INFO - root - 2017-12-05 23:21:36.021535: step 53450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:12m:16s remains)
INFO - root - 2017-12-05 23:21:44.557662: step 53460, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 65h:19m:19s remains)
INFO - root - 2017-12-05 23:21:53.099388: step 53470, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 66h:32m:38s remains)
INFO - root - 2017-12-05 23:22:01.714914: step 53480, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 66h:00m:49s remains)
INFO - root - 2017-12-05 23:22:10.259546: step 53490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 66h:26m:09s remains)
INFO - root - 2017-12-05 23:22:18.905944: step 53500, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 67h:48m:31s remains)
2017-12-05 23:22:19.646502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.19672 -4.1860805 -4.1958179 -4.2161851 -4.2441587 -4.2643304 -4.26968 -4.2719431 -4.265 -4.2568564 -4.2520695 -4.2447333 -4.2449389 -4.2462096 -4.2399411][-4.1963024 -4.19047 -4.1989579 -4.2148323 -4.2368765 -4.2506366 -4.2478738 -4.243372 -4.2359786 -4.2308922 -4.2298441 -4.2239728 -4.2242694 -4.2290349 -4.2295923][-4.1989956 -4.2029338 -4.2109962 -4.2190371 -4.2297273 -4.2333436 -4.2259269 -4.2212682 -4.2156353 -4.2157593 -4.2189522 -4.2139254 -4.2130575 -4.2166548 -4.219521][-4.2018242 -4.2147722 -4.2225509 -4.2221327 -4.2221937 -4.2185588 -4.2095518 -4.2053566 -4.2008739 -4.2024422 -4.203383 -4.1951718 -4.1943846 -4.1984282 -4.1990805][-4.1890421 -4.20553 -4.214294 -4.20899 -4.2012854 -4.1938319 -4.1863484 -4.1884804 -4.1903729 -4.1942687 -4.1934886 -4.1836958 -4.1838055 -4.1860504 -4.1796994][-4.1552563 -4.1733379 -4.1820889 -4.1756811 -4.167345 -4.1611428 -4.1571531 -4.165513 -4.17875 -4.18902 -4.1931272 -4.1898017 -4.1940188 -4.1956177 -4.1771264][-4.1154985 -4.130651 -4.1327057 -4.1184568 -4.1008983 -4.0878987 -4.0875106 -4.1089234 -4.1400132 -4.16407 -4.1805525 -4.189455 -4.2032709 -4.2098112 -4.1910295][-4.09511 -4.1016073 -4.0928721 -4.0695567 -4.0392041 -4.0158 -4.0136847 -4.0454831 -4.0895562 -4.1286707 -4.1599793 -4.1841702 -4.2079673 -4.2192736 -4.2055454][-4.1108704 -4.1135535 -4.0953135 -4.066958 -4.0354028 -4.01043 -4.0012321 -4.019053 -4.0558438 -4.1027718 -4.1429906 -4.1722355 -4.1988134 -4.2142491 -4.2135043][-4.14142 -4.1455212 -4.1247473 -4.0863128 -4.0495625 -4.0268865 -4.0175056 -4.024395 -4.0497379 -4.0938387 -4.13924 -4.1685181 -4.19075 -4.2065868 -4.2153044][-4.1820593 -4.1822343 -4.1615148 -4.1253071 -4.0919294 -4.0754886 -4.0700045 -4.0762181 -4.0947042 -4.125536 -4.1618438 -4.18588 -4.1999745 -4.2101612 -4.2189293][-4.2035494 -4.2026095 -4.1883903 -4.1657505 -4.1448612 -4.1359816 -4.1334152 -4.1378865 -4.1470284 -4.1625009 -4.187223 -4.2037959 -4.2118235 -4.2189279 -4.2264357][-4.2084966 -4.2082543 -4.1993666 -4.1892185 -4.1821189 -4.1782808 -4.1730251 -4.170012 -4.1700835 -4.1774845 -4.2006788 -4.2180209 -4.2258973 -4.232338 -4.2364149][-4.2092633 -4.2144928 -4.2153492 -4.2153358 -4.2148333 -4.2119212 -4.202786 -4.1912303 -4.1831107 -4.18374 -4.2012472 -4.2166786 -4.2254457 -4.2316575 -4.2323813][-4.2186584 -4.2259011 -4.2296534 -4.2341909 -4.2340488 -4.2281861 -4.2138004 -4.197145 -4.1858506 -4.18443 -4.1980557 -4.2094955 -4.2164726 -4.222054 -4.2204909]]...]
INFO - root - 2017-12-05 23:22:28.148866: step 53510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 66h:09m:02s remains)
INFO - root - 2017-12-05 23:22:36.831460: step 53520, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 67h:26m:04s remains)
INFO - root - 2017-12-05 23:22:45.343849: step 53530, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 67h:33m:25s remains)
INFO - root - 2017-12-05 23:22:53.963718: step 53540, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.889 sec/batch; 68h:52m:42s remains)
INFO - root - 2017-12-05 23:23:02.418797: step 53550, loss = 2.09, batch loss = 2.03 (8.9 examples/sec; 0.898 sec/batch; 69h:36m:01s remains)
INFO - root - 2017-12-05 23:23:10.968764: step 53560, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:22m:55s remains)
INFO - root - 2017-12-05 23:23:19.558566: step 53570, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 65h:55m:00s remains)
INFO - root - 2017-12-05 23:23:28.206530: step 53580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:26m:20s remains)
INFO - root - 2017-12-05 23:23:36.665070: step 53590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 67h:24m:09s remains)
INFO - root - 2017-12-05 23:23:45.220035: step 53600, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 67h:01m:54s remains)
2017-12-05 23:23:46.016450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2055049 -4.1816955 -4.162354 -4.1655283 -4.195128 -4.2398629 -4.2719789 -4.2851748 -4.295414 -4.3043242 -4.3077292 -4.3038754 -4.2882233 -4.2642922 -4.2381463][-4.2000594 -4.1714406 -4.1480408 -4.15039 -4.17967 -4.2208452 -4.2520485 -4.2721877 -4.2942605 -4.3147626 -4.3272095 -4.3296514 -4.3175397 -4.2978587 -4.2783322][-4.1837354 -4.1596384 -4.1390333 -4.1431141 -4.1673393 -4.1914358 -4.2063894 -4.2288289 -4.2641735 -4.3009763 -4.3264041 -4.33811 -4.33316 -4.3194447 -4.3063741][-4.1685162 -4.1585608 -4.1455989 -4.14731 -4.1538424 -4.1482944 -4.1330862 -4.1544366 -4.20644 -4.2642097 -4.3038936 -4.3272228 -4.3315353 -4.3223844 -4.308691][-4.1777859 -4.1765037 -4.1657944 -4.1551976 -4.1330562 -4.0884023 -4.0371046 -4.0571938 -4.1302094 -4.2116466 -4.2685905 -4.3045292 -4.3195062 -4.3120065 -4.29083][-4.195066 -4.1949458 -4.1794424 -4.1456842 -4.0854125 -3.9966764 -3.9017477 -3.9283123 -4.033196 -4.1407795 -4.2183213 -4.2694654 -4.2968721 -4.2925687 -4.2641349][-4.2076588 -4.2071466 -4.1887512 -4.1368194 -4.043005 -3.9132671 -3.7751029 -3.8121483 -3.9485464 -4.0769081 -4.1699405 -4.2336149 -4.2726669 -4.2744851 -4.2434535][-4.2201672 -4.2185588 -4.2001729 -4.1470222 -4.04717 -3.9077661 -3.7552464 -3.7920117 -3.9324727 -4.0584688 -4.1509213 -4.216651 -4.2624383 -4.2714276 -4.243989][-4.2388239 -4.233602 -4.2167153 -4.1785464 -4.1032329 -3.9956558 -3.8810956 -3.9057598 -4.0085697 -4.1034217 -4.1756482 -4.2298317 -4.2724972 -4.2864456 -4.2685227][-4.2543392 -4.2474866 -4.233779 -4.2118626 -4.1630907 -4.0961952 -4.0293493 -4.0478287 -4.1096344 -4.1679435 -4.2163672 -4.2559767 -4.2912397 -4.308023 -4.301868][-4.2675071 -4.2627563 -4.2540774 -4.242815 -4.213058 -4.17614 -4.1432133 -4.1557369 -4.1899819 -4.2215781 -4.2513404 -4.2784386 -4.3048368 -4.3220677 -4.324471][-4.2870545 -4.2859321 -4.280272 -4.2734065 -4.2557664 -4.2387009 -4.2254863 -4.2335405 -4.2489562 -4.2613926 -4.2751861 -4.2931066 -4.3130975 -4.3279028 -4.3322744][-4.3012114 -4.3045945 -4.3024254 -4.2987452 -4.29087 -4.2875514 -4.2838211 -4.2848067 -4.286 -4.2889309 -4.2956333 -4.3055377 -4.3177028 -4.3268738 -4.3287721][-4.3006306 -4.3057456 -4.3039012 -4.3027344 -4.303165 -4.3070564 -4.3058133 -4.2991123 -4.2907896 -4.2921014 -4.3005018 -4.30676 -4.3132792 -4.3180156 -4.317493][-4.2925787 -4.2967629 -4.2935376 -4.2930789 -4.2969041 -4.3009706 -4.297718 -4.2838173 -4.2674084 -4.2678676 -4.2797832 -4.2888193 -4.296371 -4.3017263 -4.2994647]]...]
INFO - root - 2017-12-05 23:23:54.642435: step 53610, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 68h:37m:27s remains)
INFO - root - 2017-12-05 23:24:03.294248: step 53620, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 67h:35m:30s remains)
INFO - root - 2017-12-05 23:24:11.848809: step 53630, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 64h:53m:54s remains)
INFO - root - 2017-12-05 23:24:20.404581: step 53640, loss = 2.08, batch loss = 2.02 (9.8 examples/sec; 0.818 sec/batch; 63h:23m:57s remains)
INFO - root - 2017-12-05 23:24:28.850804: step 53650, loss = 2.02, batch loss = 1.97 (9.4 examples/sec; 0.854 sec/batch; 66h:09m:46s remains)
INFO - root - 2017-12-05 23:24:37.485365: step 53660, loss = 2.07, batch loss = 2.01 (8.5 examples/sec; 0.945 sec/batch; 73h:13m:19s remains)
INFO - root - 2017-12-05 23:24:46.132951: step 53670, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 67h:59m:34s remains)
INFO - root - 2017-12-05 23:24:54.765742: step 53680, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 66h:45m:40s remains)
INFO - root - 2017-12-05 23:25:03.321973: step 53690, loss = 2.07, batch loss = 2.02 (9.8 examples/sec; 0.820 sec/batch; 63h:32m:08s remains)
INFO - root - 2017-12-05 23:25:11.799532: step 53700, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 65h:10m:55s remains)
2017-12-05 23:25:12.543751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2840557 -4.2829747 -4.2822824 -4.2832379 -4.2873688 -4.29205 -4.2968163 -4.2994328 -4.29849 -4.2967443 -4.295795 -4.2955685 -4.2964363 -4.297729 -4.2987146][-4.286726 -4.28416 -4.2813368 -4.2789068 -4.2804337 -4.2826972 -4.286232 -4.2887645 -4.2868495 -4.2850018 -4.284183 -4.2841463 -4.2865009 -4.2904983 -4.2962022][-4.2798591 -4.2690711 -4.2569966 -4.2464705 -4.2403908 -4.2390552 -4.2432151 -4.2520313 -4.2553973 -4.2564306 -4.2580338 -4.2610726 -4.26718 -4.2745242 -4.2853031][-4.2557554 -4.2320914 -4.2080851 -4.1892781 -4.1764126 -4.1714334 -4.1781278 -4.1940603 -4.2039385 -4.2107635 -4.2193365 -4.2280359 -4.2400956 -4.2520518 -4.2669306][-4.2174149 -4.180809 -4.1462827 -4.1202049 -4.1007433 -4.0853481 -4.0831308 -4.1051483 -4.129437 -4.1488905 -4.1686716 -4.1835766 -4.2029257 -4.223248 -4.2396555][-4.1782036 -4.1369138 -4.1002088 -4.0688519 -4.0330467 -3.9873559 -3.9564295 -3.9876986 -4.0396309 -4.0830688 -4.1208124 -4.1431084 -4.168364 -4.1938148 -4.2091427][-4.1426625 -4.1000152 -4.0637784 -4.0209179 -3.9580958 -3.865761 -3.788995 -3.8387303 -3.9427671 -4.0255637 -4.0834179 -4.1105576 -4.1390133 -4.1693416 -4.1848788][-4.1215172 -4.0885072 -4.0613508 -4.0168982 -3.9428284 -3.8243411 -3.7210283 -3.7889309 -3.9300637 -4.0364165 -4.0978527 -4.1175361 -4.1377869 -4.1648006 -4.1762123][-4.1251516 -4.1119361 -4.1039391 -4.0788012 -4.03253 -3.9515867 -3.8788161 -3.9293985 -4.0306883 -4.1025581 -4.1364923 -4.1323929 -4.1312513 -4.1484909 -4.15399][-4.1385593 -4.1405206 -4.1544309 -4.1567373 -4.1436772 -4.1057515 -4.0635376 -4.0881867 -4.1348825 -4.1560717 -4.1520157 -4.123857 -4.10645 -4.1138983 -4.11475][-4.1342773 -4.1474481 -4.1775718 -4.1979585 -4.2031288 -4.1915207 -4.1737485 -4.18795 -4.2020516 -4.1896338 -4.1618209 -4.1265755 -4.1037593 -4.1037135 -4.1014633][-4.110261 -4.1324625 -4.169414 -4.1972871 -4.213263 -4.2203422 -4.2183537 -4.2290525 -4.2312903 -4.2126021 -4.1879239 -4.1620541 -4.1430874 -4.143723 -4.1444964][-4.0898571 -4.1217647 -4.1591287 -4.1903553 -4.2164459 -4.2375646 -4.245543 -4.2579865 -4.2644906 -4.25641 -4.2427244 -4.2230291 -4.2044525 -4.2065754 -4.2100821][-4.1249976 -4.1608038 -4.1916041 -4.2147808 -4.2386837 -4.2638 -4.2751122 -4.2871933 -4.2959838 -4.2975841 -4.2913775 -4.2741489 -4.2573252 -4.2612967 -4.2680459][-4.205523 -4.2330847 -4.2501516 -4.2596283 -4.2728691 -4.2904596 -4.2970881 -4.3049922 -4.3117819 -4.3183212 -4.317976 -4.3077946 -4.2982483 -4.3035512 -4.3112907]]...]
INFO - root - 2017-12-05 23:25:21.127758: step 53710, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 66h:59m:40s remains)
INFO - root - 2017-12-05 23:25:29.750092: step 53720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 66h:53m:16s remains)
INFO - root - 2017-12-05 23:25:38.383738: step 53730, loss = 2.05, batch loss = 1.99 (8.8 examples/sec; 0.906 sec/batch; 70h:11m:13s remains)
INFO - root - 2017-12-05 23:25:46.899858: step 53740, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 64h:13m:00s remains)
INFO - root - 2017-12-05 23:25:55.473173: step 53750, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:21m:30s remains)
INFO - root - 2017-12-05 23:26:04.076580: step 53760, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 66h:27m:12s remains)
INFO - root - 2017-12-05 23:26:12.540851: step 53770, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 64h:46m:00s remains)
INFO - root - 2017-12-05 23:26:21.089415: step 53780, loss = 2.11, batch loss = 2.05 (9.1 examples/sec; 0.879 sec/batch; 68h:02m:21s remains)
INFO - root - 2017-12-05 23:26:29.629004: step 53790, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 64h:47m:49s remains)
INFO - root - 2017-12-05 23:26:38.225502: step 53800, loss = 2.05, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 63h:14m:35s remains)
2017-12-05 23:26:39.019567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2684946 -4.295023 -4.3055611 -4.3069906 -4.3099337 -4.3161511 -4.320416 -4.3219309 -4.3215728 -4.3188586 -4.3149362 -4.3112135 -4.3088717 -4.3075581 -4.306561][-4.2644038 -4.29485 -4.3061085 -4.3060889 -4.3091311 -4.313961 -4.317481 -4.3205104 -4.3212914 -4.3208055 -4.3186364 -4.3152089 -4.3126173 -4.3115115 -4.3103113][-4.248054 -4.2734013 -4.2791367 -4.2733731 -4.2741694 -4.2739077 -4.273726 -4.2799549 -4.2883573 -4.2953444 -4.2997532 -4.3011475 -4.3018012 -4.3032079 -4.3034506][-4.2155633 -4.2343545 -4.2341137 -4.2265496 -4.2270784 -4.2179885 -4.2113657 -4.2181921 -4.2317462 -4.246429 -4.2577953 -4.2659178 -4.2721119 -4.2772918 -4.2796474][-4.1872063 -4.2028418 -4.1997786 -4.1942449 -4.192647 -4.1690598 -4.1446028 -4.1439009 -4.1668367 -4.1952958 -4.2185059 -4.2343407 -4.2433109 -4.246387 -4.246295][-4.1659303 -4.1773214 -4.1729064 -4.1648746 -4.1520715 -4.1016035 -4.0421381 -4.0309582 -4.0776186 -4.1358337 -4.1806097 -4.2057095 -4.2164788 -4.2175555 -4.21422][-4.1350245 -4.1422763 -4.1328039 -4.10678 -4.0604544 -3.9608545 -3.8495781 -3.8301466 -3.9217191 -4.0269246 -4.1006689 -4.1408825 -4.1644034 -4.172647 -4.1708455][-4.10495 -4.1146936 -4.1005683 -4.05436 -3.9723508 -3.8222055 -3.6590672 -3.6404946 -3.7848167 -3.9306135 -4.0240211 -4.075007 -4.1068258 -4.1201687 -4.1195445][-4.0896506 -4.1062274 -4.0980244 -4.0571923 -3.9817798 -3.8498371 -3.7194738 -3.7274628 -3.8595273 -3.9828897 -4.0549426 -4.090312 -4.1130972 -4.1195025 -4.1142454][-4.117857 -4.1317682 -4.1280155 -4.0994177 -4.0526204 -3.9785216 -3.9188468 -3.944407 -4.029304 -4.1037335 -4.1459184 -4.1661973 -4.1801891 -4.1807032 -4.1705327][-4.1729517 -4.1750402 -4.1686049 -4.14786 -4.11996 -4.0863934 -4.0678391 -4.0897493 -4.1340866 -4.173049 -4.19873 -4.2113175 -4.2186136 -4.2185063 -4.2131643][-4.2077923 -4.2055311 -4.1997843 -4.1864376 -4.1706424 -4.1562572 -4.15065 -4.1626334 -4.1815834 -4.2018971 -4.2177305 -4.2228465 -4.2254128 -4.2248716 -4.2212572][-4.2132788 -4.2157445 -4.2181325 -4.2144709 -4.205616 -4.194984 -4.1883698 -4.1902595 -4.1938643 -4.2002664 -4.2065377 -4.2056661 -4.2081008 -4.2111721 -4.2088466][-4.1926622 -4.2049346 -4.2193542 -4.2271981 -4.2255821 -4.2169218 -4.2057667 -4.1972322 -4.1884909 -4.1840467 -4.1792092 -4.16945 -4.17253 -4.1854858 -4.1891437][-4.16031 -4.1839857 -4.2085385 -4.2231631 -4.2270203 -4.2216668 -4.210062 -4.1958742 -4.1783261 -4.1627779 -4.1437969 -4.1249547 -4.1309843 -4.1563134 -4.1727481]]...]
INFO - root - 2017-12-05 23:26:47.610136: step 53810, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 65h:35m:34s remains)
INFO - root - 2017-12-05 23:26:56.141756: step 53820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 65h:50m:47s remains)
INFO - root - 2017-12-05 23:27:04.627081: step 53830, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 65h:12m:06s remains)
INFO - root - 2017-12-05 23:27:13.192828: step 53840, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 64h:25m:21s remains)
INFO - root - 2017-12-05 23:27:21.685704: step 53850, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.899 sec/batch; 69h:35m:34s remains)
INFO - root - 2017-12-05 23:27:30.120466: step 53860, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.878 sec/batch; 67h:58m:10s remains)
INFO - root - 2017-12-05 23:27:38.754432: step 53870, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 66h:33m:05s remains)
INFO - root - 2017-12-05 23:27:47.219386: step 53880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 66h:32m:16s remains)
INFO - root - 2017-12-05 23:27:55.818636: step 53890, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 67h:24m:15s remains)
INFO - root - 2017-12-05 23:28:04.458756: step 53900, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.905 sec/batch; 70h:03m:47s remains)
2017-12-05 23:28:05.249081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2671175 -4.2670422 -4.2659445 -4.265974 -4.2686095 -4.2728672 -4.2771568 -4.280715 -4.2833986 -4.2841311 -4.2832255 -4.2816038 -4.2794323 -4.2782326 -4.2780895][-4.2464013 -4.2469459 -4.2460966 -4.2455797 -4.2482152 -4.2527652 -4.2576823 -4.2627668 -4.2673478 -4.2685461 -4.2662935 -4.2623286 -4.2572212 -4.253232 -4.2500319][-4.233551 -4.2343893 -4.23264 -4.2291279 -4.228085 -4.2293582 -4.2336326 -4.2411675 -4.2494078 -4.2545285 -4.2533927 -4.248558 -4.2411885 -4.2341862 -4.2271175][-4.2254834 -4.2233047 -4.2168813 -4.20714 -4.1987019 -4.1927052 -4.19424 -4.2032766 -4.2164168 -4.2285318 -4.2341166 -4.2349472 -4.2304444 -4.2242093 -4.2166915][-4.2084122 -4.2036495 -4.1922107 -4.1763754 -4.1612449 -4.1470037 -4.1412573 -4.1460958 -4.1600251 -4.1783886 -4.1950464 -4.20875 -4.2150831 -4.21786 -4.21733][-4.1701274 -4.163177 -4.1473784 -4.1277094 -4.1070428 -4.0852356 -4.0716658 -4.0702848 -4.0795012 -4.0974483 -4.1204839 -4.145504 -4.1655374 -4.18082 -4.1927238][-4.1349516 -4.1272836 -4.1124096 -4.0953555 -4.0738029 -4.0475149 -4.0284104 -4.0190754 -4.0200067 -4.0314417 -4.0535808 -4.0831418 -4.1108127 -4.1335621 -4.1574521][-4.1339569 -4.1287503 -4.1195011 -4.1104074 -4.0923491 -4.0669317 -4.0460129 -4.0330033 -4.0291362 -4.0334744 -4.0491161 -4.072845 -4.093996 -4.1125431 -4.1382751][-4.1547914 -4.1567216 -4.1554327 -4.156 -4.14598 -4.1263347 -4.1060495 -4.09166 -4.0868464 -4.0863948 -4.0942054 -4.1075649 -4.1176343 -4.1268039 -4.147418][-4.1812811 -4.1908183 -4.2005725 -4.2118139 -4.2119622 -4.2022014 -4.1893892 -4.1787319 -4.1717649 -4.1642046 -4.1626992 -4.164825 -4.1636519 -4.1622291 -4.1739478][-4.1996479 -4.2108335 -4.2272959 -4.2453451 -4.2525649 -4.252007 -4.249577 -4.2466164 -4.2406664 -4.2302904 -4.2238317 -4.2204638 -4.2139387 -4.2055488 -4.20671][-4.2010603 -4.2071347 -4.2210383 -4.2387862 -4.2489171 -4.2529225 -4.258954 -4.264967 -4.2639537 -4.2563391 -4.2507896 -4.246634 -4.2377443 -4.2234626 -4.217103][-4.2092509 -4.2081614 -4.2134476 -4.2255583 -4.23468 -4.2393389 -4.2499561 -4.2613134 -4.2645488 -4.2598181 -4.2542739 -4.2502184 -4.2420225 -4.2298841 -4.2237411][-4.2241316 -4.2178388 -4.2143779 -4.2210321 -4.2277241 -4.2299385 -4.2403212 -4.2549424 -4.2640233 -4.2626305 -4.2577043 -4.2546735 -4.2507567 -4.2452288 -4.240994][-4.2350392 -4.2268643 -4.2206984 -4.2237649 -4.226768 -4.2242746 -4.2289934 -4.24159 -4.2540622 -4.2574911 -4.2551031 -4.2540407 -4.2551694 -4.2570758 -4.2564459]]...]
INFO - root - 2017-12-05 23:28:13.838297: step 53910, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 65h:08m:28s remains)
INFO - root - 2017-12-05 23:28:22.259716: step 53920, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 65h:42m:43s remains)
INFO - root - 2017-12-05 23:28:30.791867: step 53930, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 65h:17m:31s remains)
INFO - root - 2017-12-05 23:28:39.363784: step 53940, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 66h:52m:04s remains)
INFO - root - 2017-12-05 23:28:47.855537: step 53950, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 65h:02m:24s remains)
INFO - root - 2017-12-05 23:28:56.501913: step 53960, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 67h:33m:28s remains)
INFO - root - 2017-12-05 23:29:05.073699: step 53970, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 66h:46m:51s remains)
INFO - root - 2017-12-05 23:29:13.713085: step 53980, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 67h:54m:29s remains)
INFO - root - 2017-12-05 23:29:22.187927: step 53990, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 65h:23m:49s remains)
INFO - root - 2017-12-05 23:29:30.670636: step 54000, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 68h:19m:13s remains)
2017-12-05 23:29:31.491502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2621908 -4.2616882 -4.2612448 -4.2606096 -4.2592936 -4.2574167 -4.2552576 -4.2535825 -4.2536945 -4.256165 -4.2599554 -4.2645063 -4.2688022 -4.272192 -4.2744184][-4.2573204 -4.2553663 -4.25404 -4.2526746 -4.2501855 -4.2462783 -4.2424555 -4.2401009 -4.2406659 -4.2445159 -4.2500687 -4.2566285 -4.2629395 -4.267817 -4.270885][-4.2542939 -4.250473 -4.2472935 -4.2441635 -4.2394 -4.23264 -4.226346 -4.2228065 -4.2232547 -4.227809 -4.2352824 -4.2445993 -4.2540154 -4.2618318 -4.2674294][-4.2474432 -4.24115 -4.2350841 -4.2294269 -4.222321 -4.2133946 -4.2055693 -4.2010312 -4.2004862 -4.2045269 -4.2129006 -4.2249632 -4.2383852 -4.2502575 -4.2598739][-4.2335844 -4.2273746 -4.2204638 -4.2131453 -4.2032428 -4.1913996 -4.18153 -4.174736 -4.171258 -4.172823 -4.1796322 -4.1930447 -4.2102828 -4.2271233 -4.2424331][-4.2137384 -4.211359 -4.2079639 -4.2029862 -4.1930494 -4.1787705 -4.1652989 -4.1544127 -4.1461678 -4.1427522 -4.145462 -4.1569805 -4.1741567 -4.1927991 -4.2112384][-4.1996737 -4.2024078 -4.2044477 -4.2044225 -4.1972737 -4.1835904 -4.1686082 -4.1537123 -4.1398759 -4.1311069 -4.1286077 -4.1336923 -4.1448927 -4.15925 -4.175056][-4.2063022 -4.2105265 -4.2144713 -4.2150965 -4.2077837 -4.1951556 -4.1818581 -4.1679459 -4.1540651 -4.1455588 -4.141304 -4.1399822 -4.1423306 -4.1471958 -4.1543317][-4.2268634 -4.2301893 -4.2339506 -4.2338605 -4.2270379 -4.2170458 -4.2080173 -4.1986732 -4.1889935 -4.1838117 -4.1795192 -4.1746163 -4.1707706 -4.1677136 -4.166656][-4.2393069 -4.2429218 -4.2481117 -4.250741 -4.2485895 -4.2444754 -4.2406721 -4.2366076 -4.2321329 -4.230063 -4.226243 -4.2199645 -4.2135668 -4.2075243 -4.2027826][-4.2533 -4.2587132 -4.2657442 -4.271666 -4.2740526 -4.27388 -4.2725019 -4.2711382 -4.269382 -4.2682285 -4.264658 -4.2591491 -4.252739 -4.2471147 -4.2425771][-4.2712827 -4.2778373 -4.2850509 -4.2920904 -4.2965512 -4.2977395 -4.2967639 -4.2944641 -4.2914572 -4.288794 -4.2858047 -4.2829542 -4.2795277 -4.2771521 -4.2748308][-4.2900605 -4.2972169 -4.3027868 -4.3077278 -4.3106918 -4.3109264 -4.3086443 -4.3029218 -4.2965631 -4.29097 -4.2880945 -4.2882833 -4.2888103 -4.2904787 -4.2916393][-4.3060384 -4.3122396 -4.3134966 -4.3140178 -4.3147564 -4.3155694 -4.3136129 -4.3067946 -4.297905 -4.2887816 -4.2829585 -4.2819538 -4.283422 -4.28716 -4.2907763][-4.3103814 -4.3146019 -4.3116093 -4.3083777 -4.307384 -4.3087921 -4.308392 -4.3027191 -4.2939157 -4.283154 -4.2742286 -4.270401 -4.2712817 -4.275991 -4.2803826]]...]
INFO - root - 2017-12-05 23:29:40.152560: step 54010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:13m:26s remains)
INFO - root - 2017-12-05 23:29:48.630505: step 54020, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 66h:26m:55s remains)
INFO - root - 2017-12-05 23:29:57.037575: step 54030, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.835 sec/batch; 64h:37m:00s remains)
INFO - root - 2017-12-05 23:30:05.672593: step 54040, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 65h:54m:35s remains)
INFO - root - 2017-12-05 23:30:14.199072: step 54050, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.865 sec/batch; 66h:55m:50s remains)
INFO - root - 2017-12-05 23:30:22.795530: step 54060, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 65h:32m:48s remains)
INFO - root - 2017-12-05 23:30:31.372771: step 54070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 66h:10m:35s remains)
INFO - root - 2017-12-05 23:30:39.987533: step 54080, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.875 sec/batch; 67h:41m:01s remains)
INFO - root - 2017-12-05 23:30:48.475848: step 54090, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 66h:25m:26s remains)
INFO - root - 2017-12-05 23:30:56.938214: step 54100, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 67h:12m:28s remains)
2017-12-05 23:30:57.706647: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2134671 -4.2070274 -4.1988091 -4.201354 -4.206254 -4.21089 -4.21284 -4.2160153 -4.2224703 -4.2293963 -4.2361984 -4.2436624 -4.24836 -4.2486954 -4.252419][-4.206574 -4.1983547 -4.188108 -4.1893177 -4.1915679 -4.1909919 -4.18851 -4.191256 -4.1983819 -4.2068396 -4.2151313 -4.221633 -4.22198 -4.2162948 -4.218287][-4.189115 -4.1847796 -4.1796927 -4.1835728 -4.1818457 -4.1758947 -4.1715989 -4.1770487 -4.1857991 -4.1948633 -4.2048826 -4.2101121 -4.2049465 -4.1903663 -4.1891122][-4.1598005 -4.163383 -4.16887 -4.1753974 -4.1689415 -4.1566133 -4.15262 -4.164042 -4.1755624 -4.1858811 -4.1960135 -4.1979766 -4.1854382 -4.1648316 -4.1625614][-4.12906 -4.134995 -4.1489425 -4.158422 -4.1494417 -4.1363921 -4.1360455 -4.1498389 -4.1599307 -4.1686463 -4.1774058 -4.1758852 -4.1596646 -4.1379542 -4.1366329][-4.1066384 -4.10324 -4.1200848 -4.1362271 -4.1328025 -4.1280432 -4.1321936 -4.1397171 -4.1384678 -4.140636 -4.147861 -4.1480045 -4.1311312 -4.1122441 -4.1149321][-4.1031909 -4.0845151 -4.0972691 -4.1185789 -4.1277051 -4.1368871 -4.1439152 -4.13401 -4.1125636 -4.1076956 -4.1215396 -4.1310287 -4.1219244 -4.1094542 -4.1122231][-4.1229768 -4.092164 -4.0927472 -4.1122169 -4.1332989 -4.1557736 -4.16212 -4.1337061 -4.0942655 -4.0879765 -4.1117353 -4.1315336 -4.1344504 -4.1308336 -4.1330109][-4.1589336 -4.1273756 -4.117321 -4.1277184 -4.1457486 -4.1681952 -4.1737137 -4.14077 -4.1001625 -4.0969777 -4.1249323 -4.150434 -4.1608658 -4.1644034 -4.1682315][-4.1938462 -4.1682925 -4.1521821 -4.1520104 -4.1592441 -4.176095 -4.1825056 -4.1565404 -4.1251569 -4.1241941 -4.1505761 -4.1759095 -4.1902857 -4.1997552 -4.2049804][-4.2164488 -4.1975408 -4.1795969 -4.1715703 -4.17189 -4.1831131 -4.1907845 -4.1765561 -4.1572261 -4.1579418 -4.1773534 -4.1981721 -4.2142544 -4.2266755 -4.2312527][-4.2234268 -4.2119303 -4.1967773 -4.1868057 -4.1838379 -4.1927381 -4.2024908 -4.199728 -4.1917167 -4.1953545 -4.2115135 -4.2305403 -4.2454925 -4.2551646 -4.2553477][-4.2224069 -4.2212553 -4.2135925 -4.2073321 -4.2057176 -4.2128768 -4.2241139 -4.2301 -4.2318153 -4.2376008 -4.2507052 -4.2661738 -4.2771549 -4.2818065 -4.2799082][-4.2296386 -4.2359152 -4.2348247 -4.2318883 -4.229815 -4.2323828 -4.2393994 -4.247323 -4.2527246 -4.2577949 -4.2682219 -4.282228 -4.2933006 -4.2988825 -4.2994561][-4.2399259 -4.2464786 -4.246984 -4.2453604 -4.2440758 -4.2451906 -4.2496243 -4.2571068 -4.2637925 -4.2691731 -4.27807 -4.2899232 -4.300899 -4.3090043 -4.3133454]]...]
INFO - root - 2017-12-05 23:31:06.257308: step 54110, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 68h:18m:28s remains)
INFO - root - 2017-12-05 23:31:14.794466: step 54120, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 64h:52m:22s remains)
INFO - root - 2017-12-05 23:31:23.243829: step 54130, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 66h:27m:46s remains)
INFO - root - 2017-12-05 23:31:31.776814: step 54140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 66h:45m:27s remains)
INFO - root - 2017-12-05 23:31:40.330318: step 54150, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 66h:16m:57s remains)
INFO - root - 2017-12-05 23:31:48.927644: step 54160, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 67h:00m:36s remains)
INFO - root - 2017-12-05 23:31:57.559462: step 54170, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 67h:50m:11s remains)
INFO - root - 2017-12-05 23:32:06.174507: step 54180, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:10m:21s remains)
INFO - root - 2017-12-05 23:32:14.753027: step 54190, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 66h:05m:49s remains)
INFO - root - 2017-12-05 23:32:23.298190: step 54200, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 66h:57m:25s remains)
2017-12-05 23:32:24.141159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1903811 -4.1342859 -4.0927992 -4.0717463 -4.0447321 -4.0212903 -4.0016131 -3.9663002 -3.9227765 -3.9477229 -4.0115128 -4.0460458 -4.0591879 -4.0765886 -4.0872011][-4.1844068 -4.1223464 -4.0750604 -4.0436788 -4.00385 -3.9789436 -3.9775889 -3.960511 -3.92267 -3.9488006 -4.0151591 -4.048533 -4.0642266 -4.0865569 -4.0970874][-4.1792192 -4.1144533 -4.0667729 -4.0316844 -3.986846 -3.9681659 -3.9894383 -3.9893074 -3.9476283 -3.9599743 -4.0280943 -4.0688281 -4.084764 -4.0984793 -4.106081][-4.1812258 -4.1187372 -4.0716953 -4.0391827 -3.9985068 -3.980922 -4.0074706 -4.0085273 -3.9486628 -3.9476669 -4.0276408 -4.0875883 -4.1115541 -4.1210122 -4.1249852][-4.1876545 -4.1281762 -4.0800414 -4.0519228 -4.0170755 -3.9979231 -4.014451 -3.9964759 -3.9106798 -3.9037452 -4.0092926 -4.0891557 -4.118042 -4.1331458 -4.1422133][-4.1938138 -4.13736 -4.0889955 -4.0634627 -4.0345221 -4.0123162 -4.0024128 -3.9496462 -3.8299551 -3.8197405 -3.9532163 -4.0571818 -4.1032157 -4.1349607 -4.1616569][-4.201756 -4.1490755 -4.1008592 -4.0803809 -4.0597963 -4.034152 -3.9940741 -3.8969765 -3.7407703 -3.7489965 -3.9131804 -4.0323792 -4.0875959 -4.1326385 -4.170444][-4.2095084 -4.1609325 -4.1145859 -4.0988607 -4.0866675 -4.0658693 -4.0208769 -3.9197845 -3.7874203 -3.814275 -3.9577277 -4.053154 -4.091908 -4.127183 -4.1635504][-4.2161636 -4.1713181 -4.128788 -4.1188736 -4.1185379 -4.1130943 -4.0917463 -4.0255666 -3.9403574 -3.9656787 -4.0628147 -4.1190896 -4.1345596 -4.147963 -4.1669264][-4.2240224 -4.18363 -4.1452713 -4.1412778 -4.15357 -4.1622114 -4.1582837 -4.1203213 -4.0714731 -4.0923142 -4.1530576 -4.1784997 -4.1750216 -4.1684427 -4.1661773][-4.2320724 -4.1962848 -4.162303 -4.1627488 -4.1807718 -4.1954827 -4.2004604 -4.1793709 -4.1474066 -4.1597095 -4.1940608 -4.1951437 -4.1819444 -4.1683884 -4.1559772][-4.237711 -4.2053714 -4.1752658 -4.1765995 -4.19201 -4.2073069 -4.2180624 -4.2041078 -4.1718946 -4.1725478 -4.1921821 -4.183588 -4.1661639 -4.1531887 -4.1412668][-4.2426724 -4.2124643 -4.1854758 -4.1843424 -4.1948042 -4.2086883 -4.2232018 -4.2098813 -4.1746325 -4.169704 -4.1861677 -4.1779146 -4.1627584 -4.1557326 -4.14954][-4.2474961 -4.2184434 -4.1934409 -4.1889386 -4.1943626 -4.206038 -4.2194586 -4.2039719 -4.170208 -4.1710858 -4.1941829 -4.1941533 -4.1866169 -4.1842647 -4.1811862][-4.2511716 -4.2231383 -4.1995707 -4.1932392 -4.1961174 -4.2062788 -4.2171793 -4.201766 -4.1743584 -4.1841908 -4.2140384 -4.2213283 -4.2194376 -4.2205167 -4.2201381]]...]
INFO - root - 2017-12-05 23:32:32.507032: step 54210, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.827 sec/batch; 63h:55m:16s remains)
INFO - root - 2017-12-05 23:32:41.178959: step 54220, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 68h:01m:08s remains)
INFO - root - 2017-12-05 23:32:49.698756: step 54230, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.876 sec/batch; 67h:44m:13s remains)
INFO - root - 2017-12-05 23:32:58.293924: step 54240, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 66h:14m:16s remains)
INFO - root - 2017-12-05 23:33:06.792185: step 54250, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.884 sec/batch; 68h:17m:32s remains)
INFO - root - 2017-12-05 23:33:15.354143: step 54260, loss = 2.09, batch loss = 2.04 (9.6 examples/sec; 0.833 sec/batch; 64h:20m:51s remains)
INFO - root - 2017-12-05 23:33:23.960672: step 54270, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.848 sec/batch; 65h:34m:29s remains)
INFO - root - 2017-12-05 23:33:32.569067: step 54280, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.863 sec/batch; 66h:39m:29s remains)
INFO - root - 2017-12-05 23:33:41.196612: step 54290, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 67h:06m:40s remains)
INFO - root - 2017-12-05 23:33:49.704855: step 54300, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:54m:08s remains)
2017-12-05 23:33:50.480248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.23396 -4.2186794 -4.1981344 -4.1644721 -4.1366739 -4.0961876 -4.0574961 -4.0581837 -4.0930829 -4.14023 -4.1363044 -4.0958524 -4.0685964 -4.079783 -4.1225166][-4.2334423 -4.221118 -4.2008123 -4.1636877 -4.1357813 -4.092504 -4.047823 -4.0458384 -4.0853229 -4.140008 -4.15507 -4.1351643 -4.111259 -4.1096931 -4.1336975][-4.2348156 -4.2206063 -4.197875 -4.1617446 -4.1311622 -4.0753593 -4.0121636 -3.999368 -4.0441203 -4.1120267 -4.1454406 -4.143456 -4.1296821 -4.1304779 -4.1445894][-4.2357321 -4.2162619 -4.1885109 -4.1484561 -4.1136684 -4.0476708 -3.9649127 -3.9391522 -3.9877508 -4.0682292 -4.1228728 -4.1447978 -4.1483884 -4.1535978 -4.1548553][-4.2362456 -4.2160115 -4.1880627 -4.1460171 -4.1014218 -4.0174131 -3.9082363 -3.8686128 -3.9263737 -4.0231934 -4.1032386 -4.1509571 -4.16972 -4.1745462 -4.1632018][-4.2365427 -4.2152338 -4.1869788 -4.1438093 -4.0881577 -3.9763391 -3.8333559 -3.7856362 -3.8663187 -3.9874218 -4.0968795 -4.1632466 -4.1904879 -4.1906924 -4.1676593][-4.2350807 -4.2103772 -4.1795211 -4.1343875 -4.0659723 -3.9265738 -3.7556202 -3.7101593 -3.8228989 -3.9766164 -4.1100426 -4.1834078 -4.2104206 -4.2000475 -4.1636219][-4.2332711 -4.2070622 -4.1718507 -4.1217737 -4.0449133 -3.8964815 -3.7291689 -3.7108953 -3.8397057 -4.0035977 -4.1392126 -4.20576 -4.222168 -4.1960721 -4.1415462][-4.2313614 -4.2045259 -4.171916 -4.1260557 -4.0548177 -3.91881 -3.7823007 -3.7948847 -3.9157472 -4.0592084 -4.1731405 -4.2216234 -4.2209039 -4.1793971 -4.1074119][-4.2306767 -4.204721 -4.1792569 -4.1419888 -4.0822783 -3.9717741 -3.8776832 -3.9078574 -4.007966 -4.1218028 -4.2091875 -4.2357674 -4.2179737 -4.1678772 -4.0973077][-4.2311287 -4.20841 -4.1904545 -4.1615572 -4.1122594 -4.0320678 -3.9792488 -4.0138407 -4.0906897 -4.1783457 -4.24139 -4.2513576 -4.2240458 -4.1734614 -4.1152735][-4.2317567 -4.2115545 -4.1988916 -4.1768537 -4.1404004 -4.091619 -4.07067 -4.1074677 -4.1664953 -4.2318058 -4.2722735 -4.2714386 -4.2370024 -4.1874452 -4.1403279][-4.2357016 -4.2178721 -4.2106237 -4.19593 -4.172904 -4.1528182 -4.1546278 -4.1914086 -4.2365513 -4.2802382 -4.2982817 -4.2866869 -4.2486544 -4.2051244 -4.1684451][-4.2411642 -4.226871 -4.2258239 -4.2181697 -4.2071633 -4.2068882 -4.2219553 -4.2550807 -4.2865763 -4.3072643 -4.3074017 -4.2896776 -4.2569666 -4.2245545 -4.2012253][-4.2479229 -4.2382364 -4.241581 -4.2373786 -4.2337637 -4.2429104 -4.2603278 -4.2855344 -4.3030224 -4.3067918 -4.2990746 -4.2843986 -4.2644639 -4.246747 -4.2361116]]...]
INFO - root - 2017-12-05 23:33:58.950120: step 54310, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 65h:32m:27s remains)
INFO - root - 2017-12-05 23:34:07.345238: step 54320, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 67h:09m:05s remains)
INFO - root - 2017-12-05 23:34:15.927821: step 54330, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 66h:46m:45s remains)
INFO - root - 2017-12-05 23:34:24.301959: step 54340, loss = 2.08, batch loss = 2.02 (10.3 examples/sec; 0.774 sec/batch; 59h:46m:39s remains)
INFO - root - 2017-12-05 23:34:32.858948: step 54350, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.880 sec/batch; 68h:01m:13s remains)
INFO - root - 2017-12-05 23:34:41.472217: step 54360, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.888 sec/batch; 68h:34m:48s remains)
INFO - root - 2017-12-05 23:34:50.028435: step 54370, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:52m:52s remains)
INFO - root - 2017-12-05 23:34:58.699991: step 54380, loss = 2.06, batch loss = 2.00 (8.3 examples/sec; 0.966 sec/batch; 74h:37m:53s remains)
INFO - root - 2017-12-05 23:35:07.186805: step 54390, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 64h:06m:00s remains)
INFO - root - 2017-12-05 23:35:15.734754: step 54400, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.835 sec/batch; 64h:28m:07s remains)
2017-12-05 23:35:16.525308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.07253 -4.0871267 -4.0968451 -4.1062436 -4.115098 -4.1253738 -4.1309814 -4.1260114 -4.1288152 -4.1544805 -4.1829367 -4.19965 -4.20428 -4.2090979 -4.2059932][-4.0629969 -4.0874043 -4.1086297 -4.1207762 -4.1244473 -4.1251163 -4.1213727 -4.1095581 -4.107965 -4.130877 -4.1550522 -4.1718111 -4.1849017 -4.2077622 -4.223062][-4.053092 -4.0889192 -4.1203609 -4.1370907 -4.1441188 -4.1476836 -4.1462951 -4.1379805 -4.1379037 -4.1531224 -4.1615038 -4.1659141 -4.17543 -4.20644 -4.2331414][-4.07904 -4.1177416 -4.1479549 -4.165184 -4.1757727 -4.1826725 -4.1834559 -4.1805911 -4.1866379 -4.1996107 -4.1979871 -4.188951 -4.1915169 -4.2215428 -4.2490487][-4.1297889 -4.155807 -4.1738119 -4.18288 -4.1884971 -4.1914606 -4.1882944 -4.1888809 -4.2026587 -4.220849 -4.2220473 -4.2129712 -4.21519 -4.2423339 -4.2636385][-4.1681461 -4.1801195 -4.1826839 -4.17761 -4.1689677 -4.1563082 -4.1373663 -4.1316667 -4.1513524 -4.1786928 -4.1939087 -4.1973815 -4.2137237 -4.2452459 -4.2619328][-4.1771994 -4.1768579 -4.1664062 -4.1481652 -4.125361 -4.0942078 -4.0535393 -4.0355725 -4.0588074 -4.0969911 -4.13167 -4.1574783 -4.1948686 -4.2342782 -4.2492876][-4.1573853 -4.1459031 -4.126689 -4.1042533 -4.0816178 -4.0517716 -4.0125141 -3.9975204 -4.0222955 -4.0633512 -4.108912 -4.150116 -4.1966519 -4.2350249 -4.2448463][-4.133357 -4.1141353 -4.0912027 -4.0709615 -4.0585623 -4.0439487 -4.0285478 -4.0340943 -4.0693803 -4.1135564 -4.1604791 -4.20062 -4.238771 -4.2641373 -4.2624946][-4.1257553 -4.1045036 -4.0826006 -4.0664439 -4.0610943 -4.0610628 -4.0705962 -4.0997849 -4.1461668 -4.189919 -4.2305789 -4.2599096 -4.2831392 -4.2934222 -4.280652][-4.1234274 -4.1001673 -4.0767584 -4.062283 -4.0610061 -4.0722246 -4.1002522 -4.1463227 -4.2016959 -4.244184 -4.2738671 -4.2888193 -4.2956867 -4.2918 -4.26927][-4.1186929 -4.0932035 -4.0684686 -4.0557833 -4.0584674 -4.0789351 -4.11943 -4.1752977 -4.2345624 -4.2751503 -4.2946777 -4.2975936 -4.29096 -4.2757659 -4.2461662][-4.1208363 -4.0962944 -4.0748963 -4.0679226 -4.0740876 -4.0960364 -4.1362381 -4.1904778 -4.2448955 -4.2794433 -4.292747 -4.2909212 -4.2807522 -4.2625942 -4.2318172][-4.14462 -4.1276951 -4.1127977 -4.1078734 -4.1115789 -4.1267753 -4.1569943 -4.1990538 -4.2424545 -4.2707911 -4.2814207 -4.2788591 -4.2706637 -4.2552013 -4.22997][-4.171288 -4.1635509 -4.1569633 -4.1545181 -4.1555839 -4.1625304 -4.179369 -4.2054539 -4.2334423 -4.2528949 -4.2596631 -4.2568064 -4.2512832 -4.2394295 -4.22064]]...]
INFO - root - 2017-12-05 23:35:25.108492: step 54410, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 65h:12m:59s remains)
INFO - root - 2017-12-05 23:35:33.702482: step 54420, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 66h:24m:19s remains)
INFO - root - 2017-12-05 23:35:42.304501: step 54430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 66h:23m:39s remains)
INFO - root - 2017-12-05 23:35:50.725669: step 54440, loss = 2.09, batch loss = 2.03 (10.2 examples/sec; 0.782 sec/batch; 60h:24m:57s remains)
INFO - root - 2017-12-05 23:35:59.100265: step 54450, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 64h:47m:20s remains)
INFO - root - 2017-12-05 23:36:07.593744: step 54460, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.865 sec/batch; 66h:49m:39s remains)
INFO - root - 2017-12-05 23:36:16.167186: step 54470, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 66h:42m:43s remains)
INFO - root - 2017-12-05 23:36:24.735412: step 54480, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 68h:30m:44s remains)
INFO - root - 2017-12-05 23:36:33.334251: step 54490, loss = 2.08, batch loss = 2.03 (8.4 examples/sec; 0.951 sec/batch; 73h:28m:10s remains)
INFO - root - 2017-12-05 23:36:41.936475: step 54500, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 63h:20m:49s remains)
2017-12-05 23:36:42.748525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1203346 -4.1213665 -4.1186876 -4.1177979 -4.1209278 -4.1297164 -4.1300292 -4.1376653 -4.1537962 -4.1711674 -4.1859674 -4.1957173 -4.19354 -4.1740284 -4.141305][-4.1157775 -4.1194797 -4.1211195 -4.122355 -4.1219163 -4.1276917 -4.1353736 -4.1456518 -4.1516247 -4.1598239 -4.1689825 -4.1805673 -4.1818242 -4.1628838 -4.1326394][-4.1157422 -4.1191177 -4.1228251 -4.124166 -4.124186 -4.1321454 -4.1444654 -4.1502194 -4.1451688 -4.1454105 -4.1548762 -4.1703334 -4.1748395 -4.1622314 -4.138916][-4.1162491 -4.1178555 -4.1214461 -4.1238513 -4.1254916 -4.1307192 -4.13582 -4.1280727 -4.1124167 -4.1113276 -4.1247683 -4.1440649 -4.1505985 -4.1468711 -4.1351089][-4.126832 -4.1255956 -4.1267262 -4.1245008 -4.1187439 -4.1128244 -4.1021528 -4.0762386 -4.0539122 -4.058929 -4.080173 -4.1019979 -4.1118436 -4.1200638 -4.1290069][-4.1381316 -4.1334529 -4.1281481 -4.1169276 -4.0995555 -4.0767474 -4.04273 -3.9955745 -3.9715712 -3.9967093 -4.0356 -4.0691519 -4.0889587 -4.1127887 -4.1367145][-4.1509094 -4.1464486 -4.1372337 -4.1171436 -4.0875068 -4.0446506 -3.9751892 -3.8941698 -3.874507 -3.9378061 -4.0119309 -4.0691195 -4.1032615 -4.131125 -4.1594462][-4.1568875 -4.1543589 -4.1428475 -4.1203413 -4.0875158 -4.0323 -3.9375992 -3.8294787 -3.8180158 -3.9158466 -4.0198016 -4.0990653 -4.1451492 -4.1727138 -4.198411][-4.1671991 -4.1630526 -4.1520381 -4.1342597 -4.1069174 -4.05659 -3.9727356 -3.8853941 -3.878408 -3.9634533 -4.0579433 -4.13367 -4.1817346 -4.2070909 -4.2315168][-4.1788378 -4.1741157 -4.1675406 -4.1579504 -4.1372733 -4.0924706 -4.031003 -3.9786758 -3.9783566 -4.039535 -4.1107349 -4.1676383 -4.2067943 -4.230967 -4.25248][-4.171082 -4.1702518 -4.1707668 -4.1659017 -4.1463318 -4.1040149 -4.0631471 -4.0466003 -4.0611444 -4.1048393 -4.1595464 -4.1977921 -4.2278748 -4.2512393 -4.268692][-4.1605573 -4.1639 -4.1686511 -4.1654429 -4.143075 -4.0984674 -4.0712452 -4.0812635 -4.11315 -4.1495705 -4.1920061 -4.2174826 -4.2405171 -4.2593522 -4.2723989][-4.1708975 -4.1699691 -4.1680508 -4.1628413 -4.1388025 -4.1020184 -4.0904231 -4.1181264 -4.1573844 -4.1892662 -4.2179289 -4.2330346 -4.2474608 -4.2599125 -4.2679477][-4.18362 -4.1792064 -4.1731248 -4.1658192 -4.1473093 -4.1280918 -4.13155 -4.1609387 -4.1959653 -4.2209768 -4.2401142 -4.248683 -4.2543788 -4.2577333 -4.2582493][-4.1967072 -4.1950588 -4.1882529 -4.1811485 -4.1696877 -4.1619697 -4.1691065 -4.1949568 -4.2231994 -4.242094 -4.2547874 -4.257834 -4.2576261 -4.2543244 -4.2476387]]...]
INFO - root - 2017-12-05 23:36:51.265391: step 54510, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 68h:11m:47s remains)
INFO - root - 2017-12-05 23:36:59.860467: step 54520, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 66h:39m:46s remains)
INFO - root - 2017-12-05 23:37:08.462524: step 54530, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.843 sec/batch; 65h:07m:31s remains)
INFO - root - 2017-12-05 23:37:16.833829: step 54540, loss = 2.03, batch loss = 1.98 (10.9 examples/sec; 0.733 sec/batch; 56h:35m:10s remains)
INFO - root - 2017-12-05 23:37:25.397395: step 54550, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 66h:21m:38s remains)
INFO - root - 2017-12-05 23:37:33.925719: step 54560, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 66h:11m:43s remains)
INFO - root - 2017-12-05 23:37:42.499230: step 54570, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 66h:33m:48s remains)
INFO - root - 2017-12-05 23:37:51.112016: step 54580, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.882 sec/batch; 68h:03m:17s remains)
INFO - root - 2017-12-05 23:37:59.624092: step 54590, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 67h:04m:29s remains)
INFO - root - 2017-12-05 23:38:08.156580: step 54600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 66h:21m:41s remains)
2017-12-05 23:38:08.933634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2783275 -4.2689672 -4.2588353 -4.2520576 -4.2413535 -4.2315526 -4.2283869 -4.2350268 -4.2546353 -4.281785 -4.3062778 -4.325201 -4.338274 -4.3439121 -4.3450732][-4.2374215 -4.2288771 -4.2168946 -4.2070537 -4.1951318 -4.1797748 -4.1694427 -4.1758223 -4.2025971 -4.2429829 -4.2756166 -4.3002925 -4.3203959 -4.33136 -4.3366532][-4.1960411 -4.1901574 -4.1793294 -4.1688361 -4.1609168 -4.1436539 -4.1265206 -4.133122 -4.1681881 -4.2187214 -4.2557058 -4.2836075 -4.3065753 -4.3203564 -4.3298306][-4.1771193 -4.17039 -4.1570807 -4.1458759 -4.1386852 -4.11927 -4.1028085 -4.1177106 -4.1624641 -4.2155085 -4.252985 -4.2820368 -4.3056188 -4.3193693 -4.3289413][-4.178009 -4.1643672 -4.1440554 -4.12728 -4.1178527 -4.0955091 -4.0876317 -4.11976 -4.1761837 -4.2290397 -4.2630181 -4.2924929 -4.3145289 -4.3251081 -4.3311243][-4.175427 -4.1456423 -4.1081715 -4.0806704 -4.0641828 -4.0444555 -4.0550117 -4.1120186 -4.1825438 -4.2379017 -4.2695017 -4.2999353 -4.3231 -4.329205 -4.3309836][-4.1531615 -4.1082654 -4.0481653 -4.0036497 -3.9754746 -3.9522822 -3.9749472 -4.058979 -4.1535277 -4.2233529 -4.2588 -4.2936139 -4.3197794 -4.3261924 -4.3275332][-4.1172357 -4.0706921 -3.9961433 -3.9348183 -3.8910179 -3.854465 -3.87944 -3.9813695 -4.1005878 -4.1878424 -4.2328892 -4.2747517 -4.3080058 -4.320013 -4.3235717][-4.094511 -4.0537958 -3.9788342 -3.9139555 -3.8711379 -3.8358545 -3.8618417 -3.9650848 -4.0852294 -4.1707754 -4.2154093 -4.263351 -4.3026934 -4.3185716 -4.3225174][-4.1058941 -4.0646911 -3.9965756 -3.9412951 -3.90688 -3.8789752 -3.9096754 -4.0031557 -4.1042438 -4.1768365 -4.2190995 -4.2688775 -4.3097696 -4.3232636 -4.3253417][-4.1460376 -4.1043935 -4.04766 -4.0029292 -3.9719179 -3.9521334 -3.9848125 -4.0645552 -4.1449013 -4.2056975 -4.2449455 -4.2889681 -4.3226252 -4.3294425 -4.3293252][-4.203588 -4.1682696 -4.127358 -4.0941019 -4.0684762 -4.0550733 -4.082058 -4.1423779 -4.2023821 -4.246861 -4.27715 -4.3096366 -4.3318391 -4.3329296 -4.331459][-4.2544451 -4.2274289 -4.2013173 -4.1813607 -4.1643338 -4.1569729 -4.1759458 -4.215414 -4.2552357 -4.28504 -4.3041205 -4.3230848 -4.3355365 -4.3336 -4.3331265][-4.2909317 -4.27491 -4.2624369 -4.2517157 -4.2405767 -4.2356153 -4.2454181 -4.2680426 -4.2931237 -4.312346 -4.32326 -4.333271 -4.3386593 -4.3357024 -4.3358378][-4.3110981 -4.3024087 -4.2963967 -4.291069 -4.2845521 -4.2811365 -4.2845054 -4.2953339 -4.3088841 -4.320353 -4.3277235 -4.3344264 -4.3381119 -4.337079 -4.3381557]]...]
INFO - root - 2017-12-05 23:38:17.459278: step 54610, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.858 sec/batch; 66h:12m:38s remains)
INFO - root - 2017-12-05 23:38:26.107030: step 54620, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 64h:08m:02s remains)
INFO - root - 2017-12-05 23:38:34.612282: step 54630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 65h:56m:04s remains)
INFO - root - 2017-12-05 23:38:43.072294: step 54640, loss = 2.09, batch loss = 2.03 (10.3 examples/sec; 0.779 sec/batch; 60h:05m:15s remains)
INFO - root - 2017-12-05 23:38:51.523998: step 54650, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 63h:16m:06s remains)
INFO - root - 2017-12-05 23:39:00.017565: step 54660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:16m:57s remains)
INFO - root - 2017-12-05 23:39:08.547828: step 54670, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 66h:04m:49s remains)
INFO - root - 2017-12-05 23:39:17.177691: step 54680, loss = 2.09, batch loss = 2.04 (9.1 examples/sec; 0.881 sec/batch; 67h:58m:48s remains)
INFO - root - 2017-12-05 23:39:25.744561: step 54690, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 65h:32m:51s remains)
INFO - root - 2017-12-05 23:39:34.308115: step 54700, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 66h:59m:14s remains)
2017-12-05 23:39:35.058204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2854056 -4.2662 -4.24044 -4.2109084 -4.1859441 -4.1667418 -4.1625414 -4.1731391 -4.1983209 -4.2206116 -4.2259989 -4.2289462 -4.2234554 -4.2168751 -4.2167621][-4.2785063 -4.254776 -4.2233267 -4.1854982 -4.1497231 -4.1229467 -4.1171231 -4.1289659 -4.1636438 -4.1992822 -4.2143412 -4.2216067 -4.2174149 -4.2069979 -4.2013564][-4.2716141 -4.2448697 -4.2137914 -4.1730218 -4.1300159 -4.0962005 -4.0824919 -4.0883789 -4.1261287 -4.1733065 -4.1998668 -4.21318 -4.21436 -4.2069526 -4.2034569][-4.2711482 -4.2465596 -4.218977 -4.1803041 -4.1363015 -4.0965 -4.0721922 -4.0636153 -4.0936866 -4.1441031 -4.1776857 -4.19805 -4.2069535 -4.2106466 -4.2162743][-4.2743335 -4.2545261 -4.232008 -4.2005463 -4.1632323 -4.1240344 -4.0914021 -4.0611892 -4.0705996 -4.1106024 -4.1421151 -4.1679969 -4.1873956 -4.2049265 -4.2224336][-4.273479 -4.2547374 -4.2329435 -4.2094054 -4.1821523 -4.1474872 -4.1111345 -4.0656395 -4.0512242 -4.0693941 -4.0904794 -4.1211028 -4.1529264 -4.1842709 -4.2141647][-4.2662582 -4.2436719 -4.2203045 -4.1993871 -4.1768036 -4.1460347 -4.1142035 -4.0728474 -4.0490417 -4.0454068 -4.0463414 -4.0671449 -4.0997949 -4.1404462 -4.1856508][-4.2541375 -4.2210193 -4.1905575 -4.1691284 -4.1471806 -4.1194897 -4.095787 -4.0732164 -4.0612135 -4.051487 -4.03638 -4.038043 -4.0595913 -4.1035514 -4.1592622][-4.2458415 -4.2045741 -4.1676335 -4.1434035 -4.1183591 -4.0922093 -4.0774479 -4.0763087 -4.0814223 -4.0771837 -4.0604019 -4.0520539 -4.0627022 -4.1031346 -4.1579061][-4.2520208 -4.2123575 -4.1746178 -4.1423907 -4.1057239 -4.0752048 -4.0677352 -4.0837655 -4.1044607 -4.1122689 -4.1055608 -4.1008868 -4.1072054 -4.1350994 -4.1755753][-4.272017 -4.240757 -4.2082171 -4.1719651 -4.1290607 -4.0967216 -4.0901561 -4.1089592 -4.1359324 -4.1564054 -4.1636944 -4.1670313 -4.171257 -4.1837916 -4.2065072][-4.2993064 -4.2773528 -4.2514296 -4.2223973 -4.1909871 -4.1676335 -4.1570039 -4.16263 -4.1810946 -4.2034345 -4.2158718 -4.2225218 -4.22567 -4.2297964 -4.24059][-4.3191314 -4.3037758 -4.2857 -4.2679648 -4.2501831 -4.2347012 -4.2201433 -4.2143745 -4.2205324 -4.2371044 -4.2474256 -4.2546263 -4.2579 -4.25984 -4.26598][-4.331109 -4.3217516 -4.3103709 -4.3003149 -4.289854 -4.2799106 -4.2683773 -4.26093 -4.2613864 -4.2697511 -4.2750154 -4.2794089 -4.28105 -4.2803311 -4.2831235][-4.33642 -4.3309326 -4.3241887 -4.3181758 -4.3117886 -4.3060942 -4.3009562 -4.2970428 -4.2960072 -4.2983751 -4.2993989 -4.3005433 -4.3007956 -4.3000708 -4.3017583]]...]
INFO - root - 2017-12-05 23:39:43.587331: step 54710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 66h:18m:12s remains)
INFO - root - 2017-12-05 23:39:52.236668: step 54720, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 66h:56m:51s remains)
INFO - root - 2017-12-05 23:40:00.732196: step 54730, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 66h:12m:17s remains)
INFO - root - 2017-12-05 23:40:09.253935: step 54740, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.774 sec/batch; 59h:43m:59s remains)
INFO - root - 2017-12-05 23:40:17.777703: step 54750, loss = 2.07, batch loss = 2.02 (8.9 examples/sec; 0.899 sec/batch; 69h:22m:30s remains)
INFO - root - 2017-12-05 23:40:26.295820: step 54760, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.825 sec/batch; 63h:40m:36s remains)
INFO - root - 2017-12-05 23:40:34.739570: step 54770, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 64h:57m:43s remains)
INFO - root - 2017-12-05 23:40:43.350123: step 54780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 64h:46m:08s remains)
INFO - root - 2017-12-05 23:40:51.971792: step 54790, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 66h:04m:39s remains)
INFO - root - 2017-12-05 23:41:00.572016: step 54800, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.887 sec/batch; 68h:23m:02s remains)
2017-12-05 23:41:01.361130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1725006 -4.1727767 -4.1660151 -4.1574583 -4.1396275 -4.1047387 -4.0759659 -4.0757661 -4.0972023 -4.1232843 -4.1504312 -4.165545 -4.1680036 -4.1596951 -4.1396585][-4.1684251 -4.1617756 -4.1477737 -4.132803 -4.1058679 -4.0596776 -4.0259652 -4.0306168 -4.0692329 -4.1067042 -4.1290812 -4.1301179 -4.1162825 -4.1075397 -4.105372][-4.1717796 -4.1540051 -4.1275635 -4.0994291 -4.0587759 -3.998848 -3.9596126 -3.9690883 -4.0214977 -4.06707 -4.0860438 -4.0741882 -4.0422859 -4.0319533 -4.0540581][-4.1624961 -4.1323237 -4.0920982 -4.0455484 -3.9832475 -3.9106088 -3.8697462 -3.8902221 -3.9553418 -4.0023251 -4.0190768 -4.0031009 -3.9599411 -3.9456756 -3.9919062][-4.1418371 -4.0998526 -4.0464654 -3.985862 -3.9092839 -3.8260965 -3.7861443 -3.8226318 -3.896651 -3.9425771 -3.9566522 -3.9413655 -3.8983686 -3.883986 -3.9456308][-4.1235147 -4.0807781 -4.0289268 -3.9643276 -3.8772047 -3.7746062 -3.7196622 -3.7664249 -3.8557577 -3.9056284 -3.9215825 -3.91279 -3.8792434 -3.8720932 -3.9370906][-4.1240945 -4.0811453 -4.0395184 -3.9790268 -3.8836102 -3.7592876 -3.6816914 -3.7345438 -3.8404143 -3.8947902 -3.9166579 -3.9218068 -3.9026442 -3.9019303 -3.9576204][-4.1391578 -4.101 -4.0678997 -4.0163531 -3.9234495 -3.7981176 -3.7108693 -3.7553196 -3.8584213 -3.9137268 -3.9406159 -3.9556513 -3.950026 -3.9497364 -3.9872203][-4.1665277 -4.1342735 -4.1022544 -4.0607481 -3.9864385 -3.8919525 -3.825079 -3.8500171 -3.9256566 -3.9701254 -3.9890199 -4.0032668 -4.0076008 -4.0102558 -4.0249677][-4.1848755 -4.1553555 -4.1220994 -4.0864129 -4.0374451 -3.9807408 -3.9432275 -3.9615591 -4.0127993 -4.0456896 -4.0537906 -4.0621114 -4.0699039 -4.0749764 -4.0742][-4.1896124 -4.16332 -4.1313996 -4.101665 -4.071187 -4.0445209 -4.0321412 -4.0479832 -4.0822654 -4.1005511 -4.0986176 -4.1004024 -4.1137643 -4.1255941 -4.1197929][-4.1939673 -4.1690664 -4.1374779 -4.1129065 -4.0963016 -4.0863466 -4.0868564 -4.1001062 -4.1237679 -4.1322331 -4.1203856 -4.1133094 -4.1174297 -4.1268053 -4.1205745][-4.1965985 -4.1732373 -4.1417933 -4.1167674 -4.1037631 -4.0982924 -4.1024408 -4.1179762 -4.1407413 -4.1524134 -4.1422644 -4.1270046 -4.1155281 -4.1146631 -4.1087236][-4.1996727 -4.1756878 -4.1457605 -4.1212258 -4.1094437 -4.1040063 -4.108602 -4.1243033 -4.1446505 -4.1610827 -4.160871 -4.1468654 -4.128406 -4.1163926 -4.108901][-4.2049923 -4.1833115 -4.1590075 -4.1417494 -4.1331892 -4.1245365 -4.1232882 -4.1319571 -4.1441383 -4.1600504 -4.1687837 -4.1620708 -4.1440444 -4.1280975 -4.1201997]]...]
INFO - root - 2017-12-05 23:41:09.896261: step 54810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 65h:30m:33s remains)
INFO - root - 2017-12-05 23:41:18.412660: step 54820, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 66h:31m:54s remains)
INFO - root - 2017-12-05 23:41:26.962190: step 54830, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 63h:16m:41s remains)
INFO - root - 2017-12-05 23:41:35.290656: step 54840, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 65h:04m:43s remains)
INFO - root - 2017-12-05 23:41:43.785471: step 54850, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 63h:36m:00s remains)
INFO - root - 2017-12-05 23:41:52.294858: step 54860, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 65h:42m:18s remains)
INFO - root - 2017-12-05 23:42:00.689997: step 54870, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 66h:21m:20s remains)
INFO - root - 2017-12-05 23:42:09.155250: step 54880, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:20m:54s remains)
INFO - root - 2017-12-05 23:42:17.757047: step 54890, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.880 sec/batch; 67h:50m:06s remains)
INFO - root - 2017-12-05 23:42:26.313721: step 54900, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 65h:25m:20s remains)
2017-12-05 23:42:27.112044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1874819 -4.1882606 -4.195992 -4.2122474 -4.232852 -4.2401114 -4.2256289 -4.2063212 -4.193305 -4.1751122 -4.1626968 -4.1553121 -4.1436892 -4.1462736 -4.17665][-4.2063818 -4.210216 -4.2118559 -4.22029 -4.2299304 -4.222353 -4.1999135 -4.17256 -4.1500087 -4.128089 -4.1216664 -4.1245322 -4.1215816 -4.128705 -4.1616931][-4.2278714 -4.2358422 -4.2342634 -4.2358966 -4.2344832 -4.2125835 -4.1843748 -4.1555657 -4.1349783 -4.1199317 -4.1185517 -4.1278696 -4.1348081 -4.1421266 -4.1632361][-4.2267528 -4.2361579 -4.23238 -4.2320476 -4.2256465 -4.1979318 -4.1675949 -4.1437459 -4.1304374 -4.118434 -4.1160936 -4.1253953 -4.1375875 -4.1421366 -4.1472497][-4.2067366 -4.2152886 -4.2077813 -4.1992283 -4.1765275 -4.1351752 -4.1034603 -4.0976982 -4.0977144 -4.0871654 -4.0830941 -4.0899215 -4.0992708 -4.0991874 -4.0943928][-4.1701131 -4.1768889 -4.1661625 -4.1423521 -4.09197 -4.0222864 -3.9913101 -4.0204916 -4.0422359 -4.0390944 -4.0407014 -4.0540533 -4.0576911 -4.05247 -4.0418053][-4.1326952 -4.1338005 -4.1231065 -4.0847611 -4.0054607 -3.9094508 -3.8990088 -3.9706888 -4.0197411 -4.0321693 -4.0431266 -4.0635152 -4.0628586 -4.0542178 -4.0441389][-4.1206055 -4.1229367 -4.1174188 -4.085073 -4.0203118 -3.9520175 -3.9620719 -4.0297713 -4.0756297 -4.0918336 -4.10286 -4.1165657 -4.1105309 -4.0996456 -4.092617][-4.1387019 -4.1481223 -4.15327 -4.141541 -4.1138463 -4.082408 -4.095562 -4.1359634 -4.1641736 -4.1722922 -4.1755795 -4.1814361 -4.1745381 -4.1628461 -4.1585827][-4.1458907 -4.1599131 -4.1736736 -4.1825304 -4.1850271 -4.1796422 -4.1964664 -4.2235107 -4.2411089 -4.2439532 -4.2428722 -4.2414966 -4.2342205 -4.2236929 -4.2209735][-4.1504 -4.1657796 -4.1811304 -4.2014623 -4.2233772 -4.2342834 -4.2583833 -4.2829309 -4.2953916 -4.2945075 -4.2902741 -4.283639 -4.2749319 -4.2649822 -4.2612495][-4.1505814 -4.1697788 -4.1866193 -4.2126322 -4.2438121 -4.2636337 -4.2904949 -4.3133655 -4.3223586 -4.3215919 -4.3173103 -4.3109241 -4.303463 -4.29455 -4.2881608][-4.173173 -4.1979246 -4.2177439 -4.2425766 -4.27118 -4.2919588 -4.3157706 -4.3342962 -4.3386488 -4.334208 -4.3287268 -4.321629 -4.3121715 -4.3028889 -4.2951603][-4.221745 -4.2492018 -4.2677875 -4.2865872 -4.305469 -4.3191853 -4.33344 -4.3435016 -4.34173 -4.3346648 -4.32764 -4.3191361 -4.3090668 -4.3002791 -4.2933178][-4.2711196 -4.3002796 -4.3143725 -4.3251987 -4.3344393 -4.3390279 -4.3439064 -4.3435245 -4.3356729 -4.3279324 -4.3216314 -4.3144226 -4.3059239 -4.2980609 -4.2894831]]...]
INFO - root - 2017-12-05 23:42:35.633647: step 54910, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 67h:52m:53s remains)
INFO - root - 2017-12-05 23:42:44.249492: step 54920, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 65h:27m:57s remains)
INFO - root - 2017-12-05 23:42:52.760259: step 54930, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 63h:18m:57s remains)
INFO - root - 2017-12-05 23:43:01.152450: step 54940, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:48m:55s remains)
INFO - root - 2017-12-05 23:43:09.691243: step 54950, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.837 sec/batch; 64h:29m:58s remains)
INFO - root - 2017-12-05 23:43:18.423217: step 54960, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.883 sec/batch; 68h:06m:06s remains)
INFO - root - 2017-12-05 23:43:26.959233: step 54970, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 65h:07m:50s remains)
INFO - root - 2017-12-05 23:43:35.213862: step 54980, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 65h:37m:57s remains)
INFO - root - 2017-12-05 23:43:43.688364: step 54990, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 63h:48m:19s remains)
INFO - root - 2017-12-05 23:43:52.283262: step 55000, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 68h:14m:42s remains)
2017-12-05 23:43:53.103208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2385178 -4.2208762 -4.2090464 -4.2020726 -4.2013068 -4.2039289 -4.2072287 -4.2103691 -4.2123647 -4.2163587 -4.222568 -4.2259574 -4.2270155 -4.2232828 -4.2139049][-4.2209859 -4.2081561 -4.2005243 -4.1987715 -4.2045293 -4.2131343 -4.2201614 -4.2251139 -4.2277522 -4.2328629 -4.2392888 -4.2426481 -4.2441292 -4.2399492 -4.2304988][-4.2102928 -4.2045016 -4.2041926 -4.20844 -4.217267 -4.2267556 -4.2345552 -4.2382274 -4.2377868 -4.2421966 -4.2509604 -4.2568293 -4.2607961 -4.2594447 -4.24846][-4.2116466 -4.2130632 -4.2185974 -4.225564 -4.2328148 -4.24199 -4.2489853 -4.2467871 -4.2352839 -4.2302918 -4.2402983 -4.2568011 -4.2722383 -4.2765346 -4.2648497][-4.2222362 -4.2319012 -4.2403889 -4.2437677 -4.2457814 -4.2488561 -4.2501984 -4.237102 -4.2072325 -4.1877646 -4.2001143 -4.231792 -4.2601285 -4.2771454 -4.2776923][-4.2411246 -4.2520003 -4.2580705 -4.25502 -4.2463865 -4.2389 -4.2284055 -4.19746 -4.1470356 -4.1133261 -4.125927 -4.1698923 -4.2220569 -4.2664995 -4.2903185][-4.2601733 -4.2673626 -4.2647042 -4.2515059 -4.2298121 -4.2055588 -4.1704555 -4.1134834 -4.044116 -4.0046039 -4.0269713 -4.0954704 -4.1789947 -4.2533169 -4.2965622][-4.2629061 -4.2695003 -4.2633839 -4.2382927 -4.1993008 -4.1518745 -4.0916147 -4.0110469 -3.9326344 -3.9067175 -3.9534879 -4.0493188 -4.1536674 -4.2414794 -4.2927556][-4.2401443 -4.257062 -4.2531242 -4.2195978 -4.1684542 -4.1017447 -4.0244994 -3.9388251 -3.8750732 -3.8826852 -3.9587486 -4.0680962 -4.1703715 -4.2497792 -4.2958012][-4.191257 -4.229897 -4.2348394 -4.2029572 -4.1511226 -4.0806875 -4.0084734 -3.9448564 -3.9217768 -3.9686127 -4.0582881 -4.1566987 -4.2344356 -4.2883744 -4.3168507][-4.117444 -4.1837816 -4.2111616 -4.1994638 -4.1657305 -4.114121 -4.0699134 -4.0461259 -4.0592928 -4.1179013 -4.1914434 -4.2581882 -4.30339 -4.3289504 -4.3370442][-4.0643296 -4.1548738 -4.2090812 -4.223465 -4.2161889 -4.1905708 -4.172431 -4.1725216 -4.1975636 -4.2437124 -4.2876554 -4.32179 -4.3410339 -4.34758 -4.3439169][-4.0882254 -4.1832833 -4.2467175 -4.2733774 -4.2785506 -4.2690482 -4.2610779 -4.2672377 -4.2872982 -4.3140478 -4.3337941 -4.3452687 -4.3490162 -4.3460712 -4.3384342][-4.1811876 -4.2552719 -4.3029542 -4.3222528 -4.3251481 -4.3182054 -4.3104143 -4.3137965 -4.3245645 -4.3366542 -4.3433309 -4.3440585 -4.3424358 -4.3377242 -4.3310485][-4.2736259 -4.3177357 -4.3417826 -4.3478832 -4.3434744 -4.3347931 -4.326582 -4.3270993 -4.3317156 -4.3363581 -4.33785 -4.3362322 -4.3345113 -4.3313456 -4.3270106]]...]
INFO - root - 2017-12-05 23:44:01.574671: step 55010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 66h:31m:10s remains)
INFO - root - 2017-12-05 23:44:10.294028: step 55020, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.871 sec/batch; 67h:06m:20s remains)
INFO - root - 2017-12-05 23:44:18.893052: step 55030, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:55m:04s remains)
INFO - root - 2017-12-05 23:44:27.499829: step 55040, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:52m:27s remains)
INFO - root - 2017-12-05 23:44:36.037113: step 55050, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 66h:24m:12s remains)
INFO - root - 2017-12-05 23:44:44.601249: step 55060, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 67h:11m:02s remains)
INFO - root - 2017-12-05 23:44:53.166344: step 55070, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.819 sec/batch; 63h:08m:19s remains)
INFO - root - 2017-12-05 23:45:01.646372: step 55080, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:42m:18s remains)
INFO - root - 2017-12-05 23:45:09.964513: step 55090, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 65h:39m:28s remains)
INFO - root - 2017-12-05 23:45:18.533476: step 55100, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 66h:22m:41s remains)
2017-12-05 23:45:19.248763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.352787 -4.3495555 -4.3467641 -4.3451939 -4.345068 -4.34499 -4.3450227 -4.3446918 -4.3438268 -4.3434234 -4.3445196 -4.3454123 -4.3455739 -4.3430853 -4.3354864][-4.3503857 -4.347436 -4.3454838 -4.3440289 -4.3434381 -4.3425307 -4.3406968 -4.3375106 -4.3340859 -4.3326192 -4.3349066 -4.3386221 -4.3426056 -4.3445339 -4.3426781][-4.34736 -4.34347 -4.34106 -4.339057 -4.3371081 -4.3323922 -4.3241644 -4.3138185 -4.3061938 -4.3048577 -4.3115029 -4.3229733 -4.3352919 -4.3445544 -4.349246][-4.3449965 -4.3402524 -4.3366046 -4.3316169 -4.3229113 -4.3052621 -4.2801604 -4.2541847 -4.2388172 -4.2410331 -4.2590251 -4.2879829 -4.31697 -4.3383107 -4.3509159][-4.3427353 -4.3359146 -4.3275571 -4.313684 -4.2876616 -4.2419267 -4.1839757 -4.1321697 -4.1081996 -4.1229005 -4.1676941 -4.2279997 -4.284133 -4.3238306 -4.3469133][-4.3394489 -4.3278093 -4.3090873 -4.2773442 -4.2205873 -4.1311808 -4.0308332 -3.9530597 -3.930619 -3.9737873 -4.05905 -4.1573882 -4.2430544 -4.3015924 -4.3359008][-4.334578 -4.3150659 -4.2824268 -4.2275214 -4.13428 -4.0017662 -3.8689129 -3.781224 -3.7792237 -3.863225 -3.98886 -4.1161003 -4.21781 -4.2854304 -4.3260922][-4.3286591 -4.301621 -4.2564168 -4.1820765 -4.0647688 -3.9166543 -3.7885127 -3.7261086 -3.7597299 -3.8698406 -4.0060434 -4.13299 -4.228682 -4.2906623 -4.326951][-4.32443 -4.2947373 -4.246614 -4.1711564 -4.0628219 -3.942169 -3.8554265 -3.8324995 -3.8832133 -3.9827321 -4.0954089 -4.1972923 -4.2697225 -4.3144593 -4.33788][-4.3242593 -4.2980404 -4.2585959 -4.2017055 -4.1281967 -4.0548534 -4.0127759 -4.0152645 -4.0627933 -4.13566 -4.2127771 -4.2787242 -4.3210721 -4.3436494 -4.35217][-4.3284278 -4.3103614 -4.2862654 -4.2557106 -4.2196975 -4.1876578 -4.1772051 -4.1916676 -4.2281094 -4.2735977 -4.3166776 -4.3482537 -4.3629847 -4.3666229 -4.3631988][-4.3355141 -4.3262053 -4.316021 -4.3062754 -4.2965655 -4.2900848 -4.2956152 -4.3131938 -4.3386989 -4.3631525 -4.3802929 -4.386982 -4.383904 -4.3758731 -4.3662858][-4.3429465 -4.340096 -4.3385005 -4.3392782 -4.3418703 -4.3462167 -4.3555512 -4.3686852 -4.3830986 -4.39297 -4.3951249 -4.3900175 -4.3807893 -4.370573 -4.3611722][-4.3478928 -4.3476176 -4.3489656 -4.3524284 -4.357048 -4.36178 -4.3679175 -4.3745861 -4.38003 -4.3819594 -4.3792729 -4.3731222 -4.3657684 -4.3589621 -4.3534031][-4.3499331 -4.3495426 -4.3504233 -4.3520255 -4.3541608 -4.3560357 -4.35806 -4.3602805 -4.361599 -4.361537 -4.3595629 -4.3563194 -4.3527536 -4.3498716 -4.34757]]...]
INFO - root - 2017-12-05 23:45:27.714336: step 55110, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 65h:19m:59s remains)
INFO - root - 2017-12-05 23:45:36.233117: step 55120, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 65h:41m:05s remains)
INFO - root - 2017-12-05 23:45:44.766203: step 55130, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 67h:41m:25s remains)
INFO - root - 2017-12-05 23:45:53.227380: step 55140, loss = 2.08, batch loss = 2.03 (8.7 examples/sec; 0.925 sec/batch; 71h:14m:12s remains)
INFO - root - 2017-12-05 23:46:01.801442: step 55150, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 67h:01m:47s remains)
INFO - root - 2017-12-05 23:46:10.517380: step 55160, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 66h:19m:33s remains)
INFO - root - 2017-12-05 23:46:19.009239: step 55170, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 67h:35m:15s remains)
INFO - root - 2017-12-05 23:46:27.658203: step 55180, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 66h:08m:57s remains)
INFO - root - 2017-12-05 23:46:36.081689: step 55190, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 65h:31m:15s remains)
INFO - root - 2017-12-05 23:46:44.438484: step 55200, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 65h:44m:46s remains)
2017-12-05 23:46:45.264103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1996412 -4.2082372 -4.2158508 -4.223805 -4.2349524 -4.2423162 -4.2423382 -4.2360625 -4.2257209 -4.2133718 -4.2043996 -4.2042642 -4.2145972 -4.2366333 -4.26018][-4.189415 -4.20528 -4.2177386 -4.2294436 -4.243042 -4.249948 -4.2477827 -4.2414279 -4.2336445 -4.2249622 -4.2175422 -4.214838 -4.2183347 -4.23144 -4.2463841][-4.1909151 -4.2092075 -4.2221613 -4.2338123 -4.2461863 -4.2499614 -4.2464046 -4.2430305 -4.2414002 -4.2409463 -4.2396164 -4.2370634 -4.2330937 -4.23121 -4.2296772][-4.208384 -4.22216 -4.2288189 -4.2343607 -4.23995 -4.2379661 -4.2326393 -4.2333832 -4.2397137 -4.2496843 -4.2581716 -4.2602911 -4.2528577 -4.2384005 -4.2206488][-4.2165771 -4.2207713 -4.2162042 -4.2102757 -4.2036314 -4.1925316 -4.1846695 -4.191391 -4.2076416 -4.2284756 -4.2470984 -4.2568736 -4.2527914 -4.235167 -4.2091537][-4.2017331 -4.1913342 -4.1688924 -4.1446028 -4.1177592 -4.0878539 -4.0683126 -4.0804944 -4.1140842 -4.1571145 -4.19611 -4.2218776 -4.2276864 -4.2119164 -4.1802492][-4.175797 -4.155273 -4.1236525 -4.0868273 -4.0398564 -3.9854794 -3.9435546 -3.9484384 -3.9927883 -4.0579953 -4.1209922 -4.168057 -4.189714 -4.18113 -4.148859][-4.1699114 -4.1513796 -4.1273155 -4.0995569 -4.0604239 -4.0091705 -3.9618952 -3.950633 -3.9769757 -4.0308838 -4.09202 -4.144639 -4.1767368 -4.1797757 -4.1582365][-4.1856141 -4.1744566 -4.162385 -4.1488752 -4.1290264 -4.1005082 -4.0688524 -4.0564423 -4.0666943 -4.097177 -4.13619 -4.1748128 -4.2024307 -4.210568 -4.2004704][-4.2024803 -4.1969934 -4.1944466 -4.191329 -4.1847134 -4.1713457 -4.1518612 -4.1412244 -4.14533 -4.1631923 -4.185823 -4.2087097 -4.2252192 -4.2310877 -4.2249594][-4.2187204 -4.2157054 -4.2175589 -4.2196827 -4.2200518 -4.2166567 -4.2065167 -4.198123 -4.1982994 -4.2070546 -4.2183619 -4.2297225 -4.2362385 -4.235889 -4.2285237][-4.2285185 -4.2271352 -4.2288108 -4.2316346 -4.2343044 -4.2369704 -4.2361817 -4.2348089 -4.2364759 -4.2404051 -4.2435884 -4.2433276 -4.2371273 -4.2281294 -4.2186341][-4.2249975 -4.2247963 -4.2248 -4.2255993 -4.227262 -4.2317805 -4.2362943 -4.2400446 -4.2425709 -4.2431474 -4.2399697 -4.2306914 -4.2145038 -4.2003531 -4.1939754][-4.2164292 -4.2185416 -4.2175255 -4.2163463 -4.2161484 -4.2192845 -4.2241578 -4.2291756 -4.2319875 -4.2314844 -4.2267485 -4.2152009 -4.1957841 -4.1795273 -4.1760902][-4.2179604 -4.2193551 -4.2163148 -4.212656 -4.2099562 -4.2107472 -4.2146578 -4.220211 -4.2243428 -4.2256145 -4.2229323 -4.2139912 -4.1969242 -4.1825089 -4.180542]]...]
INFO - root - 2017-12-05 23:46:53.818966: step 55210, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.830 sec/batch; 63h:57m:55s remains)
INFO - root - 2017-12-05 23:47:02.418992: step 55220, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 69h:07m:02s remains)
INFO - root - 2017-12-05 23:47:11.007220: step 55230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 65h:15m:31s remains)
INFO - root - 2017-12-05 23:47:19.606840: step 55240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:28m:53s remains)
INFO - root - 2017-12-05 23:47:28.220717: step 55250, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 67h:01m:58s remains)
INFO - root - 2017-12-05 23:47:36.965411: step 55260, loss = 2.04, batch loss = 1.99 (9.0 examples/sec; 0.890 sec/batch; 68h:33m:28s remains)
INFO - root - 2017-12-05 23:47:45.424851: step 55270, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 65h:37m:55s remains)
INFO - root - 2017-12-05 23:47:54.074051: step 55280, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 67h:09m:18s remains)
INFO - root - 2017-12-05 23:48:02.669941: step 55290, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.866 sec/batch; 66h:39m:35s remains)
INFO - root - 2017-12-05 23:48:11.184892: step 55300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 65h:58m:43s remains)
2017-12-05 23:48:11.927447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1507444 -4.1400671 -4.1162629 -4.1136675 -4.1261878 -4.1467113 -4.1723909 -4.1926641 -4.1965532 -4.1895261 -4.1806808 -4.1638775 -4.14718 -4.1352558 -4.1357546][-4.1877561 -4.1837654 -4.1648703 -4.166193 -4.1833506 -4.2018023 -4.220058 -4.2337523 -4.2315841 -4.2189975 -4.2065463 -4.1873589 -4.1746969 -4.171349 -4.1784515][-4.204608 -4.2084522 -4.1974311 -4.2035108 -4.2230778 -4.2367697 -4.244812 -4.2480965 -4.2418909 -4.2265925 -4.2118387 -4.1956296 -4.1885295 -4.1883388 -4.1939063][-4.1946068 -4.2021327 -4.1961432 -4.2029943 -4.2204866 -4.2300882 -4.2301517 -4.2241745 -4.2141137 -4.2011313 -4.1900511 -4.1788831 -4.1776094 -4.1732903 -4.1731172][-4.1571932 -4.1583147 -4.148252 -4.1520844 -4.1695232 -4.1806421 -4.175765 -4.1571093 -4.1387343 -4.128387 -4.1256671 -4.1260991 -4.1334085 -4.1311522 -4.1316571][-4.0873084 -4.071661 -4.0528932 -4.0595016 -4.0830321 -4.0958109 -4.083158 -4.0470347 -4.0182519 -4.0144529 -4.0259919 -4.0441523 -4.0696564 -4.082603 -4.0962148][-4.020452 -4.0004354 -3.9791706 -3.9853239 -4.0048656 -4.0110173 -3.984374 -3.9257512 -3.88169 -3.8851891 -3.9148769 -3.9550161 -4.0067468 -4.0447488 -4.0796318][-4.0023785 -3.9875216 -3.9708283 -3.975033 -3.9859362 -3.9816153 -3.9523509 -3.8908057 -3.844785 -3.8464246 -3.8740439 -3.92027 -3.9791658 -4.0270691 -4.06859][-4.0347424 -4.0251465 -4.0142918 -4.0180163 -4.0238533 -4.0190573 -3.9992282 -3.9608247 -3.9333813 -3.9322152 -3.9479423 -3.979579 -4.0162005 -4.0445385 -4.0671177][-4.0927334 -4.089747 -4.0867391 -4.0894666 -4.0906892 -4.0868645 -4.07292 -4.0504842 -4.0380459 -4.037313 -4.0449314 -4.0601597 -4.0745964 -4.0765605 -4.0684][-4.1537986 -4.1540804 -4.1552219 -4.1573009 -4.1578026 -4.1560969 -4.1481314 -4.1347733 -4.126193 -4.1215105 -4.121232 -4.1250253 -4.1220551 -4.1030264 -4.0744405][-4.2034154 -4.2029147 -4.2017636 -4.2018051 -4.2032003 -4.2034869 -4.2001348 -4.19071 -4.1800485 -4.170589 -4.1655984 -4.1611457 -4.1469569 -4.1185193 -4.0889544][-4.2356257 -4.2340021 -4.2308722 -4.2288485 -4.2294335 -4.2304816 -4.228817 -4.2207685 -4.2084756 -4.1945391 -4.1849761 -4.1740365 -4.1544709 -4.1279798 -4.10763][-4.2500191 -4.2467227 -4.2432141 -4.24059 -4.2407608 -4.2425966 -4.24224 -4.2356 -4.2236776 -4.2088289 -4.1948562 -4.1781735 -4.1548967 -4.1332855 -4.1222086][-4.2516117 -4.2467632 -4.2448049 -4.2438684 -4.2439365 -4.2438717 -4.2417717 -4.2357736 -4.2261186 -4.2134752 -4.1992054 -4.1827788 -4.1623187 -4.1475439 -4.1440973]]...]
INFO - root - 2017-12-05 23:48:20.504281: step 55310, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.881 sec/batch; 67h:51m:33s remains)
INFO - root - 2017-12-05 23:48:28.957005: step 55320, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 66h:11m:32s remains)
INFO - root - 2017-12-05 23:48:37.510439: step 55330, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 66h:04m:16s remains)
INFO - root - 2017-12-05 23:48:45.930011: step 55340, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 64h:29m:47s remains)
INFO - root - 2017-12-05 23:48:54.472824: step 55350, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.850 sec/batch; 65h:27m:53s remains)
INFO - root - 2017-12-05 23:49:03.092701: step 55360, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 66h:53m:21s remains)
INFO - root - 2017-12-05 23:49:11.703778: step 55370, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.815 sec/batch; 62h:43m:53s remains)
INFO - root - 2017-12-05 23:49:20.108068: step 55380, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 64h:33m:31s remains)
INFO - root - 2017-12-05 23:49:28.960314: step 55390, loss = 2.05, batch loss = 1.99 (8.6 examples/sec; 0.934 sec/batch; 71h:54m:02s remains)
INFO - root - 2017-12-05 23:49:37.443962: step 55400, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 66h:42m:32s remains)
2017-12-05 23:49:38.260234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1512489 -4.1843424 -4.215508 -4.2446108 -4.2660351 -4.2650008 -4.2475057 -4.2131166 -4.1704636 -4.1255693 -4.09314 -4.0835338 -4.123055 -4.1757956 -4.209197][-4.1802998 -4.2158904 -4.2444496 -4.2648249 -4.2773852 -4.2787433 -4.2661238 -4.2434292 -4.2157893 -4.1789 -4.1517806 -4.1448 -4.1737494 -4.2147441 -4.2435141][-4.2072735 -4.2411771 -4.2640624 -4.2747383 -4.2752218 -4.2731972 -4.2624364 -4.2521925 -4.2430305 -4.222528 -4.2071671 -4.2064009 -4.2226467 -4.2499781 -4.2748017][-4.2204461 -4.249929 -4.2642231 -4.2622457 -4.2464671 -4.2316456 -4.2203722 -4.22821 -4.2438278 -4.2463832 -4.2453542 -4.2460175 -4.2474456 -4.2591424 -4.2770228][-4.2173562 -4.2364159 -4.2386928 -4.2226644 -4.184494 -4.1441793 -4.1239009 -4.1549239 -4.2043242 -4.2379541 -4.25523 -4.2552919 -4.24266 -4.238904 -4.2480516][-4.20223 -4.2138581 -4.2050543 -4.1699204 -4.1047754 -4.0220938 -3.9660244 -4.0221872 -4.1163177 -4.1867385 -4.2245712 -4.2320843 -4.2117009 -4.1945577 -4.1986742][-4.1957564 -4.2012525 -4.1831551 -4.1331658 -4.0412917 -3.9059649 -3.7957928 -3.8810141 -4.0233474 -4.126617 -4.1816669 -4.1978297 -4.1741652 -4.1486154 -4.1512647][-4.2200089 -4.2178512 -4.1975842 -4.1502366 -4.0585632 -3.9185145 -3.810353 -3.8841016 -4.0129209 -4.1125717 -4.1651735 -4.1799407 -4.1574273 -4.12944 -4.1335411][-4.2605767 -4.2519107 -4.23579 -4.2011952 -4.1308632 -4.0323329 -3.9678972 -4.0068913 -4.0779719 -4.1354747 -4.1605873 -4.1620064 -4.1362896 -4.1049047 -4.1110344][-4.30263 -4.2906737 -4.2785473 -4.2573681 -4.2084904 -4.1464543 -4.1140037 -4.1289997 -4.1534963 -4.1683826 -4.1632576 -4.1423569 -4.0979466 -4.0483742 -4.0513272][-4.3323593 -4.321342 -4.3126068 -4.3014197 -4.2735896 -4.2394428 -4.221756 -4.2190433 -4.2159724 -4.2081718 -4.1874828 -4.1468272 -4.0784059 -4.0075569 -4.0095258][-4.3416615 -4.3319216 -4.3252797 -4.3204045 -4.3115497 -4.2996817 -4.2940326 -4.2859178 -4.2723632 -4.2541609 -4.2258348 -4.1768069 -4.0997381 -4.0256004 -4.0301423][-4.3362913 -4.3277926 -4.3228288 -4.3238211 -4.3280044 -4.3292236 -4.3301449 -4.323669 -4.3100057 -4.2922716 -4.2692027 -4.226161 -4.1639342 -4.1082387 -4.1160345][-4.3280768 -4.32034 -4.3170943 -4.3205271 -4.3284988 -4.3350248 -4.3387361 -4.3343344 -4.3248558 -4.3129258 -4.2986393 -4.2695413 -4.2291346 -4.198597 -4.2095852][-4.3205724 -4.312273 -4.3087816 -4.3112955 -4.3177338 -4.3250017 -4.3294821 -4.3274317 -4.3216004 -4.3162746 -4.3120232 -4.2992482 -4.277791 -4.2640572 -4.2746921]]...]
INFO - root - 2017-12-05 23:49:46.667294: step 55410, loss = 2.05, batch loss = 2.00 (10.5 examples/sec; 0.760 sec/batch; 58h:31m:22s remains)
INFO - root - 2017-12-05 23:49:55.278897: step 55420, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 67h:31m:01s remains)
INFO - root - 2017-12-05 23:50:03.839284: step 55430, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 65h:53m:57s remains)
INFO - root - 2017-12-05 23:50:12.308017: step 55440, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 64h:32m:36s remains)
INFO - root - 2017-12-05 23:50:20.903708: step 55450, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 65h:32m:33s remains)
INFO - root - 2017-12-05 23:50:29.481151: step 55460, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 64h:17m:52s remains)
INFO - root - 2017-12-05 23:50:38.142663: step 55470, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 65h:19m:43s remains)
INFO - root - 2017-12-05 23:50:46.723334: step 55480, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 66h:44m:59s remains)
INFO - root - 2017-12-05 23:50:55.328480: step 55490, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 66h:43m:37s remains)
INFO - root - 2017-12-05 23:51:03.946861: step 55500, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 66h:15m:51s remains)
2017-12-05 23:51:04.721975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1017785 -4.1306009 -4.1611786 -4.1657381 -4.1285424 -4.0833993 -4.086297 -4.112648 -4.14958 -4.2015181 -4.2539091 -4.2958813 -4.3183165 -4.32514 -4.3229656][-4.1404381 -4.1634808 -4.1834049 -4.1793542 -4.1443958 -4.1031575 -4.0907779 -4.092236 -4.1223984 -4.1799226 -4.242938 -4.2943869 -4.3228655 -4.3317542 -4.3264055][-4.1756921 -4.1920023 -4.2027717 -4.1937881 -4.1615739 -4.1252084 -4.0976849 -4.0809484 -4.1093469 -4.1700544 -4.2340989 -4.286324 -4.317718 -4.3286653 -4.3219223][-4.205977 -4.2173743 -4.218987 -4.2074728 -4.1795182 -4.1453695 -4.1068754 -4.0826321 -4.1155944 -4.1756005 -4.235188 -4.2855878 -4.3159986 -4.3271847 -4.3200531][-4.2267981 -4.2338262 -4.2292218 -4.2157531 -4.1874733 -4.144609 -4.0865345 -4.0613132 -4.1119838 -4.18478 -4.2511759 -4.2990689 -4.3234816 -4.3314042 -4.323051][-4.2450514 -4.24662 -4.2355795 -4.2165804 -4.1762366 -4.1055574 -4.0136189 -4.0006218 -4.0924354 -4.192914 -4.2702079 -4.3141494 -4.3315296 -4.3350482 -4.3256445][-4.2439761 -4.2439141 -4.2324705 -4.2056189 -4.1438193 -4.0288725 -3.8984323 -3.9139092 -4.0618567 -4.1946135 -4.2774744 -4.3174314 -4.3301296 -4.3311033 -4.3216419][-4.2245874 -4.2238007 -4.2181377 -4.1922784 -4.1197095 -3.9838107 -3.854373 -3.9040928 -4.0666013 -4.1990294 -4.2746968 -4.3083992 -4.3180852 -4.3194509 -4.3117557][-4.2147465 -4.2180858 -4.2208591 -4.2048469 -4.1460919 -4.0348535 -3.9497318 -4.0034347 -4.1262579 -4.2232218 -4.278367 -4.3052535 -4.314239 -4.3157597 -4.3084912][-4.2332029 -4.2374282 -4.2416234 -4.2301512 -4.1884923 -4.1089692 -4.058712 -4.1056118 -4.1908131 -4.2567873 -4.2936382 -4.3126326 -4.3202238 -4.3195639 -4.3096132][-4.2514424 -4.2516227 -4.2517953 -4.2420697 -4.21173 -4.1561327 -4.1310763 -4.1722145 -4.2377915 -4.283947 -4.3041563 -4.3138194 -4.3170381 -4.3134522 -4.3024096][-4.2516274 -4.2501836 -4.2484875 -4.2433176 -4.2226481 -4.1858368 -4.177711 -4.2131534 -4.2626023 -4.293015 -4.301806 -4.3044348 -4.30544 -4.3008857 -4.2895617][-4.2381592 -4.2408504 -4.2390819 -4.2366877 -4.2261686 -4.2073483 -4.2087822 -4.2375493 -4.2705379 -4.2861958 -4.2885227 -4.2906513 -4.293201 -4.2877722 -4.2761259][-4.2331233 -4.2346773 -4.2326727 -4.2326593 -4.2297387 -4.2231488 -4.2260175 -4.2463365 -4.2662783 -4.2734284 -4.2747722 -4.2802181 -4.2835274 -4.2777009 -4.2658772][-4.2452049 -4.2449865 -4.2443657 -4.2467742 -4.2458396 -4.2415161 -4.2425566 -4.25496 -4.2663913 -4.2695403 -4.2730417 -4.2812195 -4.2844415 -4.278688 -4.2679834]]...]
INFO - root - 2017-12-05 23:51:13.298804: step 55510, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 66h:56m:33s remains)
INFO - root - 2017-12-05 23:51:21.660962: step 55520, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.763 sec/batch; 58h:43m:56s remains)
INFO - root - 2017-12-05 23:51:30.152313: step 55530, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 67h:18m:00s remains)
INFO - root - 2017-12-05 23:51:38.594211: step 55540, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 65h:41m:35s remains)
INFO - root - 2017-12-05 23:51:47.193733: step 55550, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 65h:53m:45s remains)
INFO - root - 2017-12-05 23:51:55.742082: step 55560, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 66h:38m:28s remains)
INFO - root - 2017-12-05 23:52:04.330867: step 55570, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 66h:41m:19s remains)
INFO - root - 2017-12-05 23:52:12.793395: step 55580, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 65h:38m:17s remains)
INFO - root - 2017-12-05 23:52:21.263552: step 55590, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 66h:38m:49s remains)
INFO - root - 2017-12-05 23:52:29.878509: step 55600, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.829 sec/batch; 63h:45m:43s remains)
2017-12-05 23:52:30.693415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0972476 -4.0836806 -4.0778689 -4.0798435 -4.0692778 -4.0626373 -4.0687513 -4.0829973 -4.1062021 -4.1337142 -4.1637793 -4.1712775 -4.1680131 -4.1531129 -4.1378822][-4.0880136 -4.0618148 -4.045907 -4.0440083 -4.0338588 -4.0340562 -4.0472221 -4.0673647 -4.099227 -4.1326451 -4.1632185 -4.1726274 -4.1697326 -4.1526928 -4.1296372][-4.0712919 -4.0406995 -4.0168471 -4.0102973 -4.0044208 -4.0097351 -4.0278072 -4.0594921 -4.0990105 -4.1353817 -4.1637034 -4.17002 -4.161489 -4.1390486 -4.1095357][-4.0715423 -4.0499134 -4.0308146 -4.0208817 -4.0103254 -4.0118222 -4.0260468 -4.0593596 -4.09363 -4.1230283 -4.1503239 -4.1574802 -4.145299 -4.122849 -4.0868473][-4.0960455 -4.0831695 -4.0773573 -4.0645838 -4.0394182 -4.0233989 -4.0250278 -4.0438638 -4.060297 -4.0803843 -4.1085248 -4.1224346 -4.117301 -4.0979352 -4.0576][-4.1374316 -4.1295366 -4.1276174 -4.1131635 -4.0745215 -4.0347157 -4.01682 -4.0137653 -4.0175776 -4.0282025 -4.0547509 -4.07995 -4.0923676 -4.0863514 -4.0569377][-4.1638422 -4.1565433 -4.1511383 -4.1334314 -4.0868955 -4.032382 -3.997823 -3.9803574 -3.9777973 -3.9866676 -4.013937 -4.0527639 -4.0837469 -4.0966911 -4.0914106][-4.1626358 -4.156436 -4.1494584 -4.1275353 -4.0754185 -4.0140057 -3.9670491 -3.9392326 -3.9403963 -3.9641719 -4.0067344 -4.0596294 -4.1018867 -4.1250892 -4.1360235][-4.153697 -4.1504593 -4.143168 -4.1200109 -4.067224 -4.0094423 -3.9641502 -3.9358027 -3.9412839 -3.9755619 -4.0333853 -4.09503 -4.1376262 -4.1586151 -4.1711559][-4.1396861 -4.1386619 -4.1299014 -4.1093807 -4.072711 -4.0338464 -4.0022697 -3.9866707 -3.9933803 -4.0214353 -4.0720687 -4.1259947 -4.1612034 -4.1742978 -4.1817288][-4.1265163 -4.1276407 -4.1232543 -4.1101832 -4.0907049 -4.0727677 -4.060277 -4.0558944 -4.05836 -4.0714726 -4.1000557 -4.1366897 -4.1619949 -4.1688609 -4.1698661][-4.112093 -4.1122808 -4.1151872 -4.108314 -4.0956178 -4.0927057 -4.0946617 -4.0983877 -4.0992632 -4.10263 -4.1102886 -4.1258087 -4.1384206 -4.14202 -4.1435757][-4.1217785 -4.117784 -4.1238461 -4.1177149 -4.1036057 -4.1053338 -4.1153269 -4.1252546 -4.1260428 -4.1220112 -4.1143637 -4.1156 -4.1198235 -4.1227622 -4.1304979][-4.1547246 -4.1482072 -4.1547112 -4.147521 -4.132473 -4.1380186 -4.1512914 -4.1620059 -4.1621432 -4.1536818 -4.1399817 -4.1369472 -4.1374454 -4.1368256 -4.1436443][-4.1803541 -4.1753826 -4.1835637 -4.1782475 -4.1660023 -4.1719184 -4.1837683 -4.1911945 -4.1904707 -4.1845107 -4.1749568 -4.1729918 -4.1717224 -4.1683421 -4.1735]]...]
INFO - root - 2017-12-05 23:52:39.199388: step 55610, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 64h:16m:51s remains)
INFO - root - 2017-12-05 23:52:47.679228: step 55620, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 64h:31m:30s remains)
INFO - root - 2017-12-05 23:52:56.227611: step 55630, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.807 sec/batch; 62h:04m:48s remains)
INFO - root - 2017-12-05 23:53:04.560002: step 55640, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 65h:04m:58s remains)
INFO - root - 2017-12-05 23:53:13.265072: step 55650, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 65h:54m:32s remains)
INFO - root - 2017-12-05 23:53:21.958613: step 55660, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.892 sec/batch; 68h:37m:30s remains)
INFO - root - 2017-12-05 23:53:30.494032: step 55670, loss = 2.02, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 65h:49m:13s remains)
INFO - root - 2017-12-05 23:53:38.981488: step 55680, loss = 2.04, batch loss = 1.99 (9.6 examples/sec; 0.838 sec/batch; 64h:24m:18s remains)
INFO - root - 2017-12-05 23:53:47.484917: step 55690, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 63h:57m:19s remains)
INFO - root - 2017-12-05 23:53:56.008928: step 55700, loss = 2.02, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 65h:57m:23s remains)
2017-12-05 23:53:56.730727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.34259 -4.3471231 -4.3497386 -4.3511858 -4.3507185 -4.3486338 -4.3467555 -4.3440819 -4.338532 -4.3328924 -4.3276916 -4.3248024 -4.32575 -4.330894 -4.3374081][-4.33036 -4.3362217 -4.339922 -4.3411818 -4.3389783 -4.3358889 -4.3353314 -4.3323331 -4.3249393 -4.3168187 -4.3082323 -4.3015447 -4.2999582 -4.3076472 -4.31848][-4.297328 -4.3012657 -4.3021674 -4.300478 -4.2945242 -4.2907014 -4.2934141 -4.292685 -4.2875652 -4.2817845 -4.2733994 -4.2638593 -4.25822 -4.2662787 -4.2808247][-4.2513709 -4.2488909 -4.2425113 -4.2329779 -4.2173038 -4.20958 -4.2143388 -4.2207527 -4.2289214 -4.2355065 -4.2327189 -4.2235126 -4.2137556 -4.2169313 -4.2305155][-4.2155561 -4.2032886 -4.1883883 -4.1665287 -4.1310496 -4.1106181 -4.1122556 -4.1292925 -4.1615052 -4.1905293 -4.20173 -4.2020245 -4.1959095 -4.1947465 -4.2042642][-4.2052259 -4.1840162 -4.1598668 -4.1195135 -4.0549264 -4.0093393 -3.9943252 -4.0180411 -4.0798297 -4.1420565 -4.1798477 -4.1997576 -4.2049618 -4.2053623 -4.2099786][-4.2047763 -4.1797066 -4.1494741 -4.091867 -3.9999342 -3.925498 -3.8859882 -3.9107833 -3.9982922 -4.0923557 -4.1594071 -4.2027669 -4.2208915 -4.2235827 -4.2220039][-4.204289 -4.183341 -4.1538868 -4.0897756 -3.9910526 -3.9054992 -3.8550837 -3.8779469 -3.9709971 -4.0732093 -4.1522689 -4.2048564 -4.2248254 -4.2245665 -4.2180777][-4.2087054 -4.1967392 -4.1725659 -4.117528 -4.0382137 -3.9725075 -3.9394627 -3.9634237 -4.0387659 -4.1148691 -4.1741424 -4.2120094 -4.222796 -4.2164774 -4.2080011][-4.2121873 -4.2046556 -4.1890492 -4.154676 -4.106328 -4.06802 -4.055181 -4.0784159 -4.13204 -4.1775875 -4.2050695 -4.2174087 -4.2167988 -4.2078013 -4.203948][-4.2183838 -4.2145586 -4.2092204 -4.1948476 -4.1718655 -4.1485043 -4.1414948 -4.1616187 -4.2014222 -4.2275681 -4.2347455 -4.2300849 -4.2235866 -4.2168021 -4.2193022][-4.2428265 -4.2462826 -4.2461905 -4.237884 -4.2234893 -4.20478 -4.1969204 -4.2173066 -4.2526612 -4.2729416 -4.2716703 -4.2594557 -4.2493153 -4.2438979 -4.2480955][-4.2702069 -4.2788191 -4.2778883 -4.2664113 -4.2511806 -4.2333579 -4.2256608 -4.243886 -4.27451 -4.2951293 -4.2957926 -4.2868323 -4.2772112 -4.2696023 -4.2684956][-4.286088 -4.294529 -4.2914605 -4.278399 -4.2640038 -4.2493877 -4.2430859 -4.2537351 -4.2754822 -4.2945614 -4.29819 -4.2944994 -4.2884336 -4.2812543 -4.2761335][-4.27671 -4.2827992 -4.281178 -4.2752156 -4.2695885 -4.2632227 -4.258956 -4.2614927 -4.2738895 -4.2881594 -4.2934265 -4.2930827 -4.2899451 -4.2864189 -4.28216]]...]
INFO - root - 2017-12-05 23:54:05.306890: step 55710, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.909 sec/batch; 69h:52m:25s remains)
INFO - root - 2017-12-05 23:54:13.777052: step 55720, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 63h:50m:32s remains)
INFO - root - 2017-12-05 23:54:22.223504: step 55730, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 65h:33m:31s remains)
INFO - root - 2017-12-05 23:54:30.568932: step 55740, loss = 2.08, batch loss = 2.02 (10.9 examples/sec; 0.734 sec/batch; 56h:27m:56s remains)
INFO - root - 2017-12-05 23:54:39.078003: step 55750, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 67h:03m:07s remains)
INFO - root - 2017-12-05 23:54:47.692557: step 55760, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 64h:33m:40s remains)
INFO - root - 2017-12-05 23:54:56.195352: step 55770, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 66h:55m:19s remains)
INFO - root - 2017-12-05 23:55:04.614244: step 55780, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 66h:10m:54s remains)
INFO - root - 2017-12-05 23:55:13.384591: step 55790, loss = 2.08, batch loss = 2.03 (9.3 examples/sec; 0.863 sec/batch; 66h:18m:18s remains)
INFO - root - 2017-12-05 23:55:21.973968: step 55800, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 66h:25m:29s remains)
2017-12-05 23:55:22.718523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1031852 -4.0890541 -4.0727825 -4.0678983 -4.0697465 -4.078639 -4.0983448 -4.1214833 -4.1377411 -4.1442108 -4.1428633 -4.1381631 -4.1226115 -4.0966749 -4.0851631][-4.0918579 -4.0775719 -4.0639334 -4.0568814 -4.0528793 -4.0540557 -4.0790482 -4.1158762 -4.1431804 -4.1542621 -4.1505475 -4.1506319 -4.1464186 -4.1253519 -4.1142311][-4.0782623 -4.0636849 -4.04751 -4.0366273 -4.0282774 -4.0268488 -4.0576448 -4.0992055 -4.129437 -4.14395 -4.1428361 -4.1471429 -4.1555858 -4.1457882 -4.1387072][-4.0785637 -4.057447 -4.0332465 -4.0271997 -4.0230589 -4.0203233 -4.04165 -4.0767159 -4.101779 -4.1223316 -4.1351547 -4.1522536 -4.1727791 -4.1750345 -4.175806][-4.08681 -4.057754 -4.0261183 -4.0206871 -4.0186443 -4.0080872 -4.0159731 -4.0459919 -4.0762038 -4.1076207 -4.1362729 -4.17221 -4.2066584 -4.2181134 -4.2243938][-4.0751987 -4.046298 -4.0188127 -4.0096755 -3.9966788 -3.9720197 -3.9645789 -3.9920003 -4.0356731 -4.0857573 -4.1317883 -4.1865454 -4.2315326 -4.2477708 -4.2551875][-4.0472846 -4.0273147 -4.0118065 -4.0074711 -3.988158 -3.9529984 -3.9325967 -3.9577971 -4.0078058 -4.0669951 -4.1209211 -4.1869235 -4.2374759 -4.2577085 -4.2636657][-4.0011411 -3.9936411 -3.9912434 -4.0030317 -3.9944651 -3.9670339 -3.9544916 -3.9793234 -4.0275431 -4.0776663 -4.1231523 -4.1837711 -4.2319384 -4.25421 -4.265698][-3.9625587 -3.9620333 -3.9684436 -4.0008583 -4.0141926 -4.0089927 -4.0139 -4.0353317 -4.0706882 -4.1054211 -4.135519 -4.1832762 -4.2263188 -4.2502627 -4.2648325][-3.9654219 -3.9642019 -3.9771624 -4.0249796 -4.0504131 -4.0558619 -4.066783 -4.0868826 -4.1113257 -4.1339722 -4.1526985 -4.1877561 -4.2256756 -4.2493563 -4.265132][-4.0110331 -4.0068541 -4.0197158 -4.064538 -4.0888309 -4.0956349 -4.1031971 -4.1211433 -4.1422338 -4.1616068 -4.17287 -4.1983132 -4.2282386 -4.248889 -4.2649579][-4.0529509 -4.0474367 -4.0550771 -4.0885458 -4.1108413 -4.1216221 -4.1296453 -4.1443839 -4.1663132 -4.1855526 -4.18987 -4.2042141 -4.2260017 -4.2433238 -4.2592173][-4.0831914 -4.0735388 -4.073184 -4.0939941 -4.114481 -4.1256695 -4.132442 -4.1429372 -4.1632738 -4.1809144 -4.1807742 -4.1883945 -4.2070088 -4.2254848 -4.2449746][-4.1186724 -4.1022167 -4.0889859 -4.093226 -4.1105809 -4.1219139 -4.1298118 -4.1395092 -4.1552892 -4.1665311 -4.1632824 -4.1691971 -4.1899347 -4.2137542 -4.2392907][-4.1489029 -4.1215358 -4.088202 -4.0726943 -4.0883508 -4.1052804 -4.1172309 -4.1281352 -4.1402326 -4.1489787 -4.1482472 -4.1579733 -4.1823177 -4.2120223 -4.2435255]]...]
INFO - root - 2017-12-05 23:55:31.258103: step 55810, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.845 sec/batch; 64h:58m:56s remains)
INFO - root - 2017-12-05 23:55:39.943115: step 55820, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 66h:12m:21s remains)
INFO - root - 2017-12-05 23:55:48.497300: step 55830, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 67h:06m:08s remains)
INFO - root - 2017-12-05 23:55:56.874467: step 55840, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:37m:44s remains)
INFO - root - 2017-12-05 23:56:05.325960: step 55850, loss = 2.07, batch loss = 2.01 (11.0 examples/sec; 0.726 sec/batch; 55h:48m:26s remains)
INFO - root - 2017-12-05 23:56:13.795499: step 55860, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 63h:23m:27s remains)
INFO - root - 2017-12-05 23:56:22.239533: step 55870, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 65h:10m:50s remains)
INFO - root - 2017-12-05 23:56:30.650295: step 55880, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 63h:51m:55s remains)
INFO - root - 2017-12-05 23:56:39.124752: step 55890, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 65h:50m:29s remains)
INFO - root - 2017-12-05 23:56:47.613929: step 55900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 65h:16m:29s remains)
2017-12-05 23:56:48.367716: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2666645 -4.2447338 -4.2221556 -4.2082782 -4.1956749 -4.1991348 -4.2150149 -4.2323904 -4.2544451 -4.2620344 -4.2558403 -4.2370496 -4.2116871 -4.1973748 -4.2093205][-4.2597046 -4.2346344 -4.2061486 -4.1891723 -4.1725516 -4.1715522 -4.1852522 -4.1985807 -4.2168202 -4.2199826 -4.2135859 -4.1944695 -4.1653724 -4.146739 -4.1593313][-4.2519045 -4.2230225 -4.1876645 -4.1641774 -4.1456671 -4.1423783 -4.1514359 -4.1617346 -4.17477 -4.1714568 -4.167923 -4.1543355 -4.1289124 -4.1089892 -4.1238952][-4.2431145 -4.2079868 -4.1642613 -4.1330891 -4.1137552 -4.110508 -4.1115179 -4.1180634 -4.1298952 -4.1275077 -4.134995 -4.138731 -4.1305 -4.1160398 -4.1305661][-4.2378421 -4.1978269 -4.1494989 -4.1130104 -4.0877585 -4.0788927 -4.0643539 -4.0630369 -4.0810304 -4.0898528 -4.1148219 -4.1399412 -4.1492343 -4.1425104 -4.1557159][-4.2258196 -4.183723 -4.1350012 -4.0919995 -4.052928 -4.0280323 -3.9859722 -3.9648237 -3.9889326 -4.0226994 -4.0722728 -4.12094 -4.1486859 -4.1551161 -4.1703868][-4.207725 -4.1633697 -4.1151514 -4.0627251 -4.0021529 -3.9523234 -3.8774228 -3.8230212 -3.8486719 -3.9269702 -4.0144954 -4.0893631 -4.1314754 -4.1469469 -4.1677566][-4.204895 -4.1618338 -4.1111279 -4.0527534 -3.9810555 -3.9153588 -3.8194156 -3.7343554 -3.7457049 -3.8577485 -3.9682698 -4.0524592 -4.0954089 -4.1095734 -4.13305][-4.2174835 -4.1801152 -4.1289916 -4.0751348 -4.0087247 -3.9482243 -3.8672867 -3.7946143 -3.8001046 -3.8984761 -3.9907749 -4.0528774 -4.0790048 -4.0805964 -4.1005044][-4.2265315 -4.1925521 -4.1447048 -4.1038885 -4.0575662 -4.0165176 -3.9673526 -3.9301198 -3.9431741 -4.0138264 -4.0740304 -4.1024041 -4.1013808 -4.092731 -4.1090446][-4.2267823 -4.1927996 -4.1512928 -4.1246343 -4.0970707 -4.0748043 -4.0547438 -4.0451527 -4.0646577 -4.1097569 -4.1394238 -4.146884 -4.1319356 -4.1180682 -4.1339946][-4.2383471 -4.2082334 -4.1747403 -4.1557112 -4.1392169 -4.1304541 -4.1264949 -4.1276693 -4.1487365 -4.1735826 -4.1822624 -4.1807971 -4.1649795 -4.1508546 -4.1634126][-4.2616558 -4.2364578 -4.20562 -4.1850481 -4.1719069 -4.1730952 -4.18256 -4.1897917 -4.2078133 -4.221437 -4.2223606 -4.2180419 -4.2070165 -4.1953664 -4.2017856][-4.2861443 -4.2642164 -4.2361422 -4.2168236 -4.206264 -4.2118587 -4.2256522 -4.2321196 -4.2428346 -4.2499328 -4.2519341 -4.2513561 -4.246244 -4.2408152 -4.2472272][-4.3070645 -4.2871947 -4.2651186 -4.2504816 -4.2417941 -4.2457013 -4.2554193 -4.2586961 -4.2602773 -4.2616673 -4.2632551 -4.2652574 -4.266253 -4.2678728 -4.2774291]]...]
INFO - root - 2017-12-05 23:56:56.995247: step 55910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:41m:29s remains)
INFO - root - 2017-12-05 23:57:05.622876: step 55920, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 65h:46m:09s remains)
INFO - root - 2017-12-05 23:57:14.250148: step 55930, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 64h:47m:25s remains)
INFO - root - 2017-12-05 23:57:22.616119: step 55940, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 63h:45m:36s remains)
INFO - root - 2017-12-05 23:57:31.082828: step 55950, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 64h:31m:20s remains)
INFO - root - 2017-12-05 23:57:39.600255: step 55960, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 63h:13m:58s remains)
INFO - root - 2017-12-05 23:57:47.964774: step 55970, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 64h:20m:59s remains)
INFO - root - 2017-12-05 23:57:56.604827: step 55980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 66h:18m:10s remains)
INFO - root - 2017-12-05 23:58:05.272381: step 55990, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 65h:31m:39s remains)
INFO - root - 2017-12-05 23:58:13.807184: step 56000, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 64h:43m:02s remains)
2017-12-05 23:58:14.600872: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3022437 -4.2856503 -4.2647433 -4.2492223 -4.2451024 -4.2311578 -4.2117028 -4.1959782 -4.1885266 -4.1829915 -4.18736 -4.2069011 -4.219101 -4.2147942 -4.1909981][-4.2917528 -4.2710676 -4.2454028 -4.2290454 -4.2309217 -4.2268667 -4.2165313 -4.2060809 -4.2018213 -4.195271 -4.1973429 -4.2200174 -4.2372727 -4.2360897 -4.2121043][-4.2824111 -4.2573042 -4.2268558 -4.2068491 -4.2081723 -4.2088981 -4.2060609 -4.2028947 -4.2027864 -4.1952 -4.1943188 -4.2214007 -4.2467871 -4.2495065 -4.2252626][-4.2802453 -4.2536135 -4.218421 -4.1907563 -4.1837511 -4.1822014 -4.18353 -4.1859279 -4.1877928 -4.1771846 -4.1755223 -4.2085123 -4.2409935 -4.2476015 -4.2271786][-4.2832088 -4.2576 -4.2188644 -4.1839075 -4.1651554 -4.15517 -4.1532593 -4.1562438 -4.1606522 -4.1512141 -4.1526413 -4.1882057 -4.2243156 -4.2340851 -4.2207346][-4.2870665 -4.26345 -4.2236061 -4.1832542 -4.1484947 -4.1251369 -4.1143851 -4.1143932 -4.1213703 -4.1192808 -4.1287861 -4.1658978 -4.2029538 -4.218328 -4.2161241][-4.2899632 -4.2699738 -4.2306232 -4.1816568 -4.1312408 -4.0949135 -4.0794106 -4.0816054 -4.0957222 -4.1076593 -4.125607 -4.161037 -4.1942897 -4.2091012 -4.2145314][-4.2963147 -4.2794542 -4.2387257 -4.1795959 -4.1158848 -4.0703878 -4.0565176 -4.0667682 -4.0933433 -4.1217046 -4.145618 -4.1733069 -4.1985097 -4.20767 -4.2173319][-4.3007922 -4.2852945 -4.2407074 -4.1727486 -4.0997715 -4.0485864 -4.0397778 -4.0615134 -4.1017585 -4.1421051 -4.1668968 -4.1878223 -4.2075825 -4.2129769 -4.221386][-4.3029327 -4.2858858 -4.2381058 -4.164782 -4.0872455 -4.0349026 -4.0339808 -4.066555 -4.1156178 -4.1613483 -4.18356 -4.1954741 -4.2063742 -4.2084751 -4.2142725][-4.3048072 -4.28826 -4.2433863 -4.1720109 -4.0953851 -4.0466948 -4.0499473 -4.08335 -4.1325092 -4.1780624 -4.1970024 -4.1993284 -4.2011418 -4.2025313 -4.2108827][-4.3075342 -4.2925339 -4.2521081 -4.1877003 -4.1196218 -4.0781026 -4.0825686 -4.1121845 -4.1559482 -4.1917529 -4.201992 -4.1963339 -4.1929741 -4.1946096 -4.2066927][-4.3130455 -4.3005829 -4.2681947 -4.2149897 -4.1602144 -4.1290307 -4.1340404 -4.1578579 -4.1905985 -4.2132468 -4.2127614 -4.2030396 -4.1986952 -4.199811 -4.2131495][-4.3211675 -4.3128357 -4.2907028 -4.2517872 -4.21279 -4.192152 -4.1967878 -4.2153835 -4.2387285 -4.2509789 -4.2444534 -4.2347775 -4.229805 -4.2301307 -4.2409782][-4.3295078 -4.3258448 -4.3137765 -4.2876987 -4.2644544 -4.2546163 -4.2597327 -4.2741537 -4.2900839 -4.2941751 -4.2846847 -4.2756114 -4.2728624 -4.2753882 -4.2839932]]...]
INFO - root - 2017-12-05 23:58:23.125402: step 56010, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:35m:11s remains)
INFO - root - 2017-12-05 23:58:31.591441: step 56020, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:30m:33s remains)
INFO - root - 2017-12-05 23:58:40.091356: step 56030, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 67h:03m:09s remains)
INFO - root - 2017-12-05 23:58:48.398768: step 56040, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 63h:20m:12s remains)
INFO - root - 2017-12-05 23:58:56.875054: step 56050, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 66h:31m:23s remains)
INFO - root - 2017-12-05 23:59:05.328697: step 56060, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.833 sec/batch; 63h:58m:15s remains)
INFO - root - 2017-12-05 23:59:13.901536: step 56070, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 64h:48m:22s remains)
INFO - root - 2017-12-05 23:59:22.286107: step 56080, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.867 sec/batch; 66h:33m:50s remains)
INFO - root - 2017-12-05 23:59:30.798575: step 56090, loss = 2.08, batch loss = 2.03 (9.8 examples/sec; 0.820 sec/batch; 62h:58m:34s remains)
INFO - root - 2017-12-05 23:59:39.424093: step 56100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 66h:40m:10s remains)
2017-12-05 23:59:40.136060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1550961 -4.1335974 -4.1435223 -4.1623888 -4.1765718 -4.1742 -4.1677132 -4.1707306 -4.1704407 -4.1640396 -4.1564417 -4.1574183 -4.1643686 -4.166852 -4.1621385][-4.1835971 -4.1798368 -4.1951528 -4.2159972 -4.22789 -4.2162719 -4.2010345 -4.2001948 -4.2043495 -4.2027569 -4.1968231 -4.1994371 -4.2093577 -4.2162528 -4.2140288][-4.2101774 -4.2164555 -4.2285337 -4.2435231 -4.2496552 -4.2357364 -4.2203975 -4.2211165 -4.2332244 -4.2405958 -4.2404037 -4.2470036 -4.2584748 -4.2663541 -4.2651243][-4.2427292 -4.2480412 -4.25129 -4.2553687 -4.2559733 -4.2457747 -4.2340808 -4.2356267 -4.2539067 -4.2718749 -4.2821817 -4.2965374 -4.3101439 -4.3157225 -4.3120894][-4.2771535 -4.2737031 -4.268249 -4.262218 -4.2564983 -4.2409983 -4.2221026 -4.2189207 -4.2402182 -4.2680831 -4.2893071 -4.312 -4.3282061 -4.3335872 -4.3253422][-4.2958183 -4.2809873 -4.2639523 -4.2462139 -4.2308145 -4.201436 -4.1601954 -4.13927 -4.1632314 -4.2050514 -4.240375 -4.2728953 -4.2937536 -4.3014526 -4.2919941][-4.2875891 -4.2624483 -4.2323232 -4.201014 -4.1704454 -4.1206532 -4.0481153 -3.9989948 -4.0229268 -4.0835328 -4.138341 -4.1899829 -4.2276525 -4.2463837 -4.243083][-4.2498446 -4.2201185 -4.1838341 -4.1456604 -4.1002765 -4.0303454 -3.927634 -3.8459854 -3.8686562 -3.9489303 -4.0270233 -4.1011415 -4.1646376 -4.2050948 -4.2207088][-4.2112722 -4.1869321 -4.1561804 -4.1201715 -4.068562 -3.9924712 -3.8801994 -3.7757316 -3.7904804 -3.878799 -3.9737387 -4.0657048 -4.1472826 -4.2023497 -4.2345943][-4.2143645 -4.1994834 -4.1790709 -4.1537127 -4.1121325 -4.051959 -3.9649377 -3.8759072 -3.8747897 -3.946291 -4.0318017 -4.1176186 -4.193161 -4.2410507 -4.2690244][-4.2525864 -4.2420845 -4.2299232 -4.2143784 -4.1876745 -4.147543 -4.0905018 -4.0331783 -4.023046 -4.0686345 -4.1324816 -4.1956449 -4.2513509 -4.2844024 -4.3008151][-4.28948 -4.2786622 -4.26891 -4.258131 -4.2417264 -4.2179637 -4.1828117 -4.1484265 -4.1343322 -4.1598878 -4.2071304 -4.2506914 -4.28924 -4.3124118 -4.3197718][-4.2971125 -4.2849722 -4.2762775 -4.2702193 -4.2615604 -4.2507877 -4.2317076 -4.2090364 -4.1940227 -4.2070866 -4.2428846 -4.2777419 -4.3064446 -4.3234196 -4.3243837][-4.2799106 -4.2649403 -4.2557392 -4.2509775 -4.2459874 -4.2407341 -4.2311029 -4.2182412 -4.2091913 -4.21984 -4.2524838 -4.2852397 -4.3072772 -4.3187947 -4.3173666][-4.2571731 -4.2411213 -4.2310057 -4.2269168 -4.2252836 -4.2249265 -4.2224689 -4.2203918 -4.2212973 -4.2338314 -4.26292 -4.2916789 -4.3070817 -4.3129706 -4.3096018]]...]
INFO - root - 2017-12-05 23:59:48.675260: step 56110, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 66h:43m:58s remains)
INFO - root - 2017-12-05 23:59:57.185859: step 56120, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 65h:06m:51s remains)
INFO - root - 2017-12-06 00:00:05.788393: step 56130, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 64h:23m:40s remains)
INFO - root - 2017-12-06 00:00:14.298350: step 56140, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 64h:59m:19s remains)
INFO - root - 2017-12-06 00:00:22.874634: step 56150, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.887 sec/batch; 68h:06m:19s remains)
INFO - root - 2017-12-06 00:00:31.289123: step 56160, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.838 sec/batch; 64h:20m:39s remains)
INFO - root - 2017-12-06 00:00:39.800273: step 56170, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 64h:24m:13s remains)
INFO - root - 2017-12-06 00:00:48.310828: step 56180, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 65h:33m:24s remains)
INFO - root - 2017-12-06 00:00:56.802737: step 56190, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 64h:53m:16s remains)
INFO - root - 2017-12-06 00:01:05.263198: step 56200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 66h:19m:05s remains)
2017-12-06 00:01:06.154645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2618117 -4.2651076 -4.2615938 -4.2526808 -4.242311 -4.2368155 -4.2373734 -4.2418246 -4.2430177 -4.2323041 -4.2217703 -4.2251029 -4.2367249 -4.2495685 -4.2593651][-4.2532997 -4.2570176 -4.2538323 -4.2457485 -4.2365665 -4.2295046 -4.2280879 -4.2314763 -4.2296295 -4.2137585 -4.1992459 -4.2032824 -4.2204089 -4.2396226 -4.2575788][-4.2477236 -4.2506914 -4.2493677 -4.24204 -4.2313342 -4.2214866 -4.2202353 -4.2226682 -4.217813 -4.2027235 -4.1872106 -4.1866941 -4.2017088 -4.2208242 -4.2448912][-4.2346435 -4.2378254 -4.2400856 -4.2363315 -4.2231779 -4.2122984 -4.209157 -4.2095485 -4.2076106 -4.2025881 -4.1937838 -4.1894674 -4.1993265 -4.214067 -4.239327][-4.2121768 -4.2133713 -4.215775 -4.2118044 -4.1925807 -4.1744928 -4.1638188 -4.1639638 -4.1754541 -4.1923919 -4.2030506 -4.2052813 -4.2114463 -4.2208982 -4.2393751][-4.1956968 -4.1880422 -4.1827159 -4.1728988 -4.1448526 -4.1095691 -4.0759745 -4.0688415 -4.1045008 -4.1521788 -4.1879425 -4.2012897 -4.2039804 -4.2099762 -4.22308][-4.1919603 -4.1734018 -4.1554689 -4.1329527 -4.0898156 -4.0264173 -3.95521 -3.9306059 -3.9908721 -4.0712209 -4.1343942 -4.1621728 -4.1640368 -4.1689873 -4.185813][-4.184968 -4.1615267 -4.1354723 -4.1042938 -4.0493879 -3.9651189 -3.8678722 -3.8304696 -3.9073133 -4.0103869 -4.0908346 -4.1292081 -4.1313314 -4.1338272 -4.1556778][-4.1896844 -4.1724877 -4.1519103 -4.1290307 -4.0849929 -4.01511 -3.940727 -3.9124963 -3.9685211 -4.0523739 -4.1173697 -4.1424642 -4.1334825 -4.1287341 -4.1467891][-4.2167811 -4.2077928 -4.1967072 -4.187108 -4.1609755 -4.1154528 -4.0737529 -4.0527234 -4.0784168 -4.1319628 -4.173326 -4.1807685 -4.1601939 -4.1483579 -4.1599507][-4.23897 -4.235496 -4.2322707 -4.2333694 -4.2240071 -4.199615 -4.17744 -4.1630015 -4.1710491 -4.199605 -4.22177 -4.2218294 -4.2048469 -4.1937571 -4.2012677][-4.24831 -4.2522764 -4.2570934 -4.2661982 -4.2681966 -4.2606821 -4.2518697 -4.2460594 -4.2473483 -4.2574873 -4.2648697 -4.2627497 -4.2523136 -4.2438793 -4.2483907][-4.2369556 -4.2472935 -4.259129 -4.2732854 -4.2820516 -4.2829962 -4.281251 -4.2801042 -4.2802696 -4.2827539 -4.2840347 -4.282351 -4.2783155 -4.2735739 -4.2744079][-4.194181 -4.2124391 -4.2326117 -4.2521567 -4.2652025 -4.2684932 -4.2674441 -4.2669368 -4.2688818 -4.2727952 -4.2789755 -4.2835255 -4.2855406 -4.2847023 -4.2825017][-4.1424379 -4.1667681 -4.1963015 -4.2244811 -4.24578 -4.2529111 -4.2508607 -4.2492361 -4.2508078 -4.2567692 -4.2677197 -4.2780008 -4.2844548 -4.2863231 -4.2839251]]...]
INFO - root - 2017-12-06 00:01:14.591709: step 56210, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 66h:03m:57s remains)
INFO - root - 2017-12-06 00:01:23.166089: step 56220, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 64h:20m:04s remains)
INFO - root - 2017-12-06 00:01:31.655109: step 56230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:37m:51s remains)
INFO - root - 2017-12-06 00:01:40.113748: step 56240, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 65h:24m:42s remains)
INFO - root - 2017-12-06 00:01:48.674460: step 56250, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 64h:41m:26s remains)
INFO - root - 2017-12-06 00:01:57.195988: step 56260, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 64h:50m:33s remains)
INFO - root - 2017-12-06 00:02:05.781630: step 56270, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.814 sec/batch; 62h:29m:17s remains)
INFO - root - 2017-12-06 00:02:14.387199: step 56280, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 67h:16m:07s remains)
INFO - root - 2017-12-06 00:02:22.785656: step 56290, loss = 2.05, batch loss = 2.00 (10.2 examples/sec; 0.781 sec/batch; 59h:53m:41s remains)
INFO - root - 2017-12-06 00:02:31.246587: step 56300, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 63h:50m:27s remains)
2017-12-06 00:02:32.022346: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2781982 -4.2749615 -4.2737927 -4.2733216 -4.2764978 -4.2823019 -4.2891722 -4.2979431 -4.3079872 -4.313539 -4.3129478 -4.3084326 -4.3057365 -4.3044362 -4.308507][-4.238585 -4.2291079 -4.2258573 -4.2278371 -4.2370067 -4.2465158 -4.2549906 -4.2650733 -4.2790914 -4.289413 -4.2911472 -4.284039 -4.2766457 -4.270124 -4.2722049][-4.2109871 -4.19084 -4.1770244 -4.178968 -4.1967 -4.2107525 -4.2182527 -4.2276664 -4.2472515 -4.26521 -4.270031 -4.26041 -4.2481208 -4.2330184 -4.2297077][-4.1977372 -4.1690555 -4.1436467 -4.14148 -4.161221 -4.1715245 -4.1707745 -4.1744189 -4.2008743 -4.2335262 -4.2501512 -4.2449975 -4.2310562 -4.209527 -4.2027235][-4.1843376 -4.1544814 -4.1241055 -4.1185875 -4.1310906 -4.1286774 -4.11152 -4.1082273 -4.1492953 -4.204587 -4.2368817 -4.2408128 -4.2311091 -4.2097931 -4.2038188][-4.1736908 -4.14483 -4.1121416 -4.10257 -4.1030636 -4.0787473 -4.0382252 -4.0265236 -4.0862732 -4.167098 -4.2187777 -4.2365947 -4.2358727 -4.2176304 -4.2135282][-4.173584 -4.1456127 -4.1089892 -4.0849962 -4.0643296 -4.0136604 -3.9470015 -3.9236929 -4.0063357 -4.1156907 -4.1884189 -4.2194448 -4.2315669 -4.2219586 -4.2209597][-4.1798229 -4.1518307 -4.11131 -4.0753007 -4.0369582 -3.9698448 -3.8758922 -3.8381708 -3.9431539 -4.0773787 -4.1603231 -4.1978517 -4.21735 -4.2122912 -4.2153845][-4.1787829 -4.1532168 -4.123445 -4.10065 -4.0707445 -4.0194468 -3.9394212 -3.9100122 -3.9995468 -4.1083674 -4.1674862 -4.1914062 -4.2031269 -4.1950188 -4.1990089][-4.1735339 -4.1527615 -4.1362319 -4.1294985 -4.1214509 -4.1045322 -4.0611768 -4.0479822 -4.1069207 -4.1723084 -4.1993647 -4.2005692 -4.1983194 -4.179337 -4.175868][-4.172708 -4.1519485 -4.1437659 -4.15071 -4.1628 -4.1698308 -4.1492519 -4.1419597 -4.1745682 -4.2110004 -4.221868 -4.2086654 -4.1938643 -4.1684165 -4.1660652][-4.1919131 -4.1641393 -4.1546793 -4.1679487 -4.1917338 -4.2091823 -4.2024322 -4.1974125 -4.2144456 -4.2370906 -4.2433109 -4.2270656 -4.2067432 -4.1814361 -4.1806922][-4.2302957 -4.2003059 -4.1854587 -4.1947131 -4.21798 -4.2331424 -4.2348232 -4.2347846 -4.2445168 -4.2607694 -4.2659144 -4.2539434 -4.2351518 -4.2160954 -4.2167087][-4.2681551 -4.2418814 -4.223352 -4.2229514 -4.2384171 -4.2496967 -4.2578416 -4.2629104 -4.2713175 -4.2842507 -4.2882805 -4.2807736 -4.2668047 -4.2536058 -4.2590337][-4.2969213 -4.2790484 -4.2626939 -4.2571492 -4.2647405 -4.2717223 -4.2791724 -4.286015 -4.2939339 -4.3047876 -4.3088346 -4.3047605 -4.29602 -4.2908525 -4.2977204]]...]
INFO - root - 2017-12-06 00:02:40.518330: step 56310, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 66h:17m:52s remains)
INFO - root - 2017-12-06 00:02:49.141877: step 56320, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.873 sec/batch; 66h:58m:47s remains)
INFO - root - 2017-12-06 00:02:57.634810: step 56330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 66h:51m:48s remains)
INFO - root - 2017-12-06 00:03:06.047491: step 56340, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.848 sec/batch; 65h:01m:07s remains)
INFO - root - 2017-12-06 00:03:14.627033: step 56350, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.838 sec/batch; 64h:15m:11s remains)
INFO - root - 2017-12-06 00:03:23.163922: step 56360, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 65h:55m:11s remains)
INFO - root - 2017-12-06 00:03:31.543043: step 56370, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 64h:50m:57s remains)
INFO - root - 2017-12-06 00:03:39.979120: step 56380, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.806 sec/batch; 61h:49m:47s remains)
INFO - root - 2017-12-06 00:03:48.430209: step 56390, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 66h:16m:45s remains)
INFO - root - 2017-12-06 00:03:56.857102: step 56400, loss = 2.09, batch loss = 2.04 (10.8 examples/sec; 0.738 sec/batch; 56h:35m:17s remains)
2017-12-06 00:03:57.700410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2492285 -4.2462292 -4.2438073 -4.2449832 -4.2418003 -4.23401 -4.2274017 -4.2207289 -4.2243471 -4.23445 -4.2400055 -4.2470493 -4.25851 -4.2698193 -4.2798548][-4.2357926 -4.2264681 -4.2203374 -4.2188983 -4.2131209 -4.204783 -4.1989689 -4.196806 -4.2085228 -4.2278047 -4.2412033 -4.2515135 -4.262289 -4.2696581 -4.2738996][-4.2298875 -4.2172737 -4.2072077 -4.1993804 -4.1829371 -4.1684666 -4.1609573 -4.163538 -4.1871071 -4.2166796 -4.2397065 -4.2578244 -4.2679033 -4.2713838 -4.2685742][-4.2299685 -4.2188578 -4.2061892 -4.1881695 -4.1559815 -4.12845 -4.1144381 -4.1171455 -4.1515031 -4.1930819 -4.2276082 -4.2542043 -4.2654848 -4.2679682 -4.2600865][-4.2248139 -4.2152224 -4.2016044 -4.1709795 -4.1180439 -4.0696464 -4.0386777 -4.0376029 -4.088984 -4.1528068 -4.2036843 -4.2411771 -4.2534494 -4.2536354 -4.2420063][-4.2218223 -4.2123089 -4.1960998 -4.152781 -4.0766988 -3.9997752 -3.9411216 -3.9269991 -3.9988716 -4.0934148 -4.1667852 -4.2172008 -4.2336588 -4.2334409 -4.2210822][-4.2164717 -4.2043948 -4.1824927 -4.1302209 -4.0406222 -3.937731 -3.8402121 -3.7981024 -3.8805878 -4.0106363 -4.1107388 -4.1769991 -4.2038865 -4.2091174 -4.1968575][-4.211072 -4.1968956 -4.1693497 -4.1143861 -4.0246429 -3.9122841 -3.7905562 -3.7192805 -3.7913117 -3.9354968 -4.05592 -4.1388125 -4.177494 -4.1882854 -4.177979][-4.2017341 -4.1908064 -4.16785 -4.1243091 -4.0511975 -3.956362 -3.8579469 -3.7959676 -3.8365374 -3.942559 -4.042202 -4.119391 -4.16377 -4.1826205 -4.179225][-4.175766 -4.1737633 -4.1662731 -4.1450224 -4.0993781 -4.0384245 -3.985503 -3.953068 -3.9750385 -4.0266757 -4.0786891 -4.1312714 -4.17118 -4.1923852 -4.1948166][-4.1452532 -4.1555653 -4.1610775 -4.1557903 -4.1360593 -4.1107574 -4.097116 -4.0894585 -4.1014705 -4.1189489 -4.1379395 -4.1649618 -4.189333 -4.1999326 -4.2015619][-4.1269522 -4.14381 -4.1531229 -4.1490164 -4.1399713 -4.140389 -4.152379 -4.1580644 -4.1654234 -4.173986 -4.1833568 -4.1943502 -4.2018404 -4.1978512 -4.1896191][-4.120522 -4.131413 -4.1327872 -4.1202331 -4.1103983 -4.1212292 -4.1409769 -4.1496921 -4.15337 -4.168714 -4.186286 -4.19555 -4.1998296 -4.1914353 -4.1731391][-4.1301594 -4.1311975 -4.1191087 -4.0935 -4.0733361 -4.0793705 -4.0931911 -4.0949354 -4.0956922 -4.1224613 -4.1560154 -4.1804476 -4.1963415 -4.19048 -4.1644287][-4.1407475 -4.1400208 -4.1232138 -4.0872307 -4.0478072 -4.0357094 -4.032938 -4.0247145 -4.0312543 -4.0792308 -4.1358032 -4.1778731 -4.2060266 -4.2070217 -4.1791053]]...]
INFO - root - 2017-12-06 00:04:06.209206: step 56410, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 64h:42m:15s remains)
INFO - root - 2017-12-06 00:04:14.667049: step 56420, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.865 sec/batch; 66h:18m:28s remains)
INFO - root - 2017-12-06 00:04:23.143024: step 56430, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 67h:07m:42s remains)
INFO - root - 2017-12-06 00:04:31.589453: step 56440, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 64h:41m:34s remains)
INFO - root - 2017-12-06 00:04:40.124415: step 56450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 66h:20m:03s remains)
INFO - root - 2017-12-06 00:04:48.559666: step 56460, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 63h:14m:10s remains)
INFO - root - 2017-12-06 00:04:57.015300: step 56470, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 64h:07m:21s remains)
INFO - root - 2017-12-06 00:05:05.493386: step 56480, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 65h:24m:42s remains)
INFO - root - 2017-12-06 00:05:13.925528: step 56490, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 67h:52m:34s remains)
INFO - root - 2017-12-06 00:05:22.505196: step 56500, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 64h:52m:47s remains)
2017-12-06 00:05:23.268294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1402678 -4.1422687 -4.1445556 -4.1439648 -4.1460276 -4.1651025 -4.1912193 -4.215816 -4.2269211 -4.2212753 -4.2006283 -4.1747251 -4.152566 -4.1390848 -4.1380963][-4.1475658 -4.1490974 -4.1512179 -4.1508102 -4.1483231 -4.1559038 -4.1761241 -4.2023306 -4.22514 -4.2397704 -4.2461033 -4.2416964 -4.2263136 -4.2039304 -4.1846504][-4.1667705 -4.1684747 -4.1717315 -4.1729159 -4.1637106 -4.1517448 -4.1526003 -4.1772552 -4.2152076 -4.248929 -4.2730918 -4.2801151 -4.2713695 -4.2508645 -4.2254853][-4.1941681 -4.196959 -4.1999626 -4.2019405 -4.1886725 -4.1583958 -4.1335368 -4.1457152 -4.1897869 -4.2384481 -4.2720675 -4.2823257 -4.2785764 -4.2661285 -4.2447214][-4.211328 -4.2133508 -4.2149425 -4.2215042 -4.2114711 -4.1706061 -4.1211672 -4.1121292 -4.1534934 -4.21006 -4.2497683 -4.2621155 -4.2628708 -4.2572279 -4.2427287][-4.1995969 -4.1971149 -4.2019858 -4.2205472 -4.2220936 -4.1783323 -4.1055055 -4.0678992 -4.0983458 -4.1579604 -4.2027793 -4.2184715 -4.2255144 -4.2295613 -4.2254229][-4.1639585 -4.1566916 -4.1687131 -4.2032728 -4.2210908 -4.1826878 -4.095645 -4.033462 -4.046349 -4.1028481 -4.1518135 -4.17361 -4.189641 -4.204463 -4.2132058][-4.1272154 -4.1187849 -4.1377726 -4.1857543 -4.2183571 -4.1911464 -4.1099963 -4.0341372 -4.0255508 -4.0669827 -4.1133356 -4.14458 -4.1735067 -4.1980867 -4.21714][-4.1122093 -4.1074777 -4.1301107 -4.1803341 -4.217627 -4.200881 -4.1354094 -4.0650907 -4.0414839 -4.0627031 -4.1009636 -4.1342607 -4.1658583 -4.1940794 -4.2192049][-4.1240225 -4.126873 -4.1458035 -4.1832304 -4.2118936 -4.19926 -4.1507816 -4.0979352 -4.075346 -4.085402 -4.1105838 -4.1335144 -4.1559258 -4.1824951 -4.2113118][-4.1578031 -4.1668425 -4.1780286 -4.19694 -4.2113409 -4.1972685 -4.1609125 -4.1259985 -4.1144724 -4.1222115 -4.1390085 -4.1509075 -4.1590729 -4.175138 -4.2019062][-4.1931849 -4.203547 -4.2078438 -4.2147727 -4.2200451 -4.2067842 -4.1795454 -4.1576147 -4.1560888 -4.1668563 -4.1796732 -4.1823564 -4.1747355 -4.1721106 -4.1907377][-4.2183456 -4.2300725 -4.2296891 -4.2294345 -4.23001 -4.2182574 -4.1965909 -4.1801176 -4.1838312 -4.198245 -4.2098355 -4.2053628 -4.1858335 -4.1677928 -4.1777663][-4.2250948 -4.240829 -4.2411356 -4.237587 -4.2341275 -4.2222261 -4.2027121 -4.1873188 -4.193469 -4.2108083 -4.2224693 -4.2136531 -4.1825204 -4.1504211 -4.1526103][-4.2179995 -4.2363148 -4.2387681 -4.235074 -4.2317758 -4.2207036 -4.2035918 -4.1896253 -4.1959376 -4.2123122 -4.2220759 -4.2106681 -4.1728 -4.1303716 -4.1229024]]...]
INFO - root - 2017-12-06 00:05:31.660240: step 56510, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 65h:08m:53s remains)
INFO - root - 2017-12-06 00:05:40.171491: step 56520, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 64h:55m:11s remains)
INFO - root - 2017-12-06 00:05:48.646032: step 56530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 65h:26m:40s remains)
INFO - root - 2017-12-06 00:05:57.091509: step 56540, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.878 sec/batch; 67h:19m:53s remains)
INFO - root - 2017-12-06 00:06:05.637284: step 56550, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.899 sec/batch; 68h:54m:27s remains)
INFO - root - 2017-12-06 00:06:14.122459: step 56560, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 66h:31m:00s remains)
INFO - root - 2017-12-06 00:06:22.711976: step 56570, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 66h:47m:27s remains)
INFO - root - 2017-12-06 00:06:31.146083: step 56580, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 66h:58m:02s remains)
INFO - root - 2017-12-06 00:06:39.739972: step 56590, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.856 sec/batch; 65h:34m:22s remains)
INFO - root - 2017-12-06 00:06:48.316689: step 56600, loss = 2.12, batch loss = 2.06 (9.3 examples/sec; 0.864 sec/batch; 66h:12m:40s remains)
2017-12-06 00:06:49.101772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.28972 -4.274754 -4.2618132 -4.2558222 -4.2554855 -4.2605491 -4.2640443 -4.2700887 -4.2800403 -4.283833 -4.2791958 -4.2738223 -4.2725825 -4.2732849 -4.2785015][-4.2767406 -4.2565365 -4.2374053 -4.2280025 -4.2272425 -4.2333379 -4.2384448 -4.2476406 -4.2633348 -4.2711148 -4.26531 -4.2569065 -4.2547565 -4.2553444 -4.2621946][-4.265161 -4.2423749 -4.2179832 -4.2022548 -4.1964679 -4.199605 -4.2017989 -4.2109289 -4.2316418 -4.2443314 -4.2402549 -4.2334228 -4.2345185 -4.2380505 -4.2481365][-4.2604942 -4.23815 -4.2093959 -4.1857128 -4.1696105 -4.1655879 -4.1619725 -4.1679158 -4.1926646 -4.2134318 -4.2167807 -4.2144146 -4.2219572 -4.2330041 -4.2466316][-4.2585316 -4.2338667 -4.1966538 -4.1613007 -4.1338763 -4.1215844 -4.1109862 -4.111938 -4.1386037 -4.1715608 -4.1861482 -4.1885881 -4.2043843 -4.2273126 -4.248487][-4.2561045 -4.2255054 -4.1760325 -4.1243281 -4.083693 -4.058742 -4.033741 -4.0196681 -4.0455108 -4.094058 -4.1235085 -4.1365228 -4.1644745 -4.2052784 -4.239315][-4.2464585 -4.2060738 -4.140729 -4.0698566 -4.013442 -3.9682615 -3.9144464 -3.8723049 -3.8932052 -3.9608784 -4.0052485 -4.0307441 -4.077014 -4.143012 -4.1994462][-4.2332277 -4.1864209 -4.1158476 -4.0392275 -3.9772563 -3.9200706 -3.8442941 -3.7828856 -3.7964277 -3.8754773 -3.9256 -3.95341 -4.0074368 -4.0872865 -4.1596823][-4.2343149 -4.19812 -4.1445284 -4.0860395 -4.0349822 -3.9889967 -3.9330161 -3.8946733 -3.9081626 -3.9699993 -4.0012951 -4.0167522 -4.0516109 -4.1105547 -4.1702156][-4.24761 -4.2265887 -4.1927032 -4.1568127 -4.1231852 -4.0904417 -4.0558724 -4.0438681 -4.0610538 -4.1032658 -4.1136179 -4.1115804 -4.1227441 -4.1543474 -4.1935482][-4.2599144 -4.2493868 -4.2261424 -4.2013068 -4.1794472 -4.1571007 -4.1354251 -4.1376195 -4.1553745 -4.1810713 -4.1822343 -4.1752577 -4.1765714 -4.192543 -4.2176156][-4.2739873 -4.2678494 -4.249609 -4.2280426 -4.2084179 -4.1907916 -4.17751 -4.1851892 -4.2006464 -4.2152815 -4.2144265 -4.2110996 -4.21301 -4.2243643 -4.2412686][-4.287786 -4.2803588 -4.262125 -4.24139 -4.2245088 -4.2103391 -4.2024913 -4.212575 -4.2273321 -4.238018 -4.2378306 -4.2358723 -4.2389755 -4.2483463 -4.26116][-4.2988615 -4.288949 -4.2717953 -4.2523518 -4.2344785 -4.2212143 -4.2171588 -4.2276053 -4.239655 -4.2474627 -4.2502546 -4.2502265 -4.2533536 -4.2616544 -4.27193][-4.3062854 -4.2954869 -4.2796359 -4.2614365 -4.2445846 -4.232873 -4.2306347 -4.2375703 -4.2440372 -4.2481008 -4.252687 -4.2570052 -4.2636619 -4.2722421 -4.2811537]]...]
INFO - root - 2017-12-06 00:06:57.613807: step 56610, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 66h:41m:28s remains)
INFO - root - 2017-12-06 00:07:06.101683: step 56620, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.879 sec/batch; 67h:19m:39s remains)
INFO - root - 2017-12-06 00:07:14.494386: step 56630, loss = 2.03, batch loss = 1.98 (9.6 examples/sec; 0.834 sec/batch; 63h:53m:58s remains)
INFO - root - 2017-12-06 00:07:22.920964: step 56640, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 63h:42m:19s remains)
INFO - root - 2017-12-06 00:07:31.531266: step 56650, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 67h:39m:19s remains)
INFO - root - 2017-12-06 00:07:39.920675: step 56660, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 63h:41m:35s remains)
INFO - root - 2017-12-06 00:07:48.605703: step 56670, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 66h:23m:37s remains)
INFO - root - 2017-12-06 00:07:57.085687: step 56680, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 64h:49m:31s remains)
INFO - root - 2017-12-06 00:08:05.535947: step 56690, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.830 sec/batch; 63h:36m:30s remains)
INFO - root - 2017-12-06 00:08:14.089199: step 56700, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 65h:02m:32s remains)
2017-12-06 00:08:14.872418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1073771 -4.0965447 -4.1057758 -4.1312695 -4.1467257 -4.1577635 -4.1607513 -4.1526551 -4.1424947 -4.1497068 -4.160758 -4.1640487 -4.1687317 -4.1761818 -4.1867604][-4.1118808 -4.1004319 -4.1006184 -4.1208186 -4.13915 -4.1501274 -4.1522288 -4.141602 -4.1253185 -4.1289396 -4.1343608 -4.1277595 -4.1228642 -4.1262984 -4.13962][-4.1565132 -4.1470027 -4.141995 -4.1505752 -4.1610355 -4.1658344 -4.1645303 -4.1519494 -4.1329246 -4.1326556 -4.1337247 -4.1217537 -4.1046786 -4.0986404 -4.1085753][-4.1980557 -4.1908469 -4.18549 -4.1846752 -4.18505 -4.18265 -4.1737084 -4.1597404 -4.1446342 -4.1477938 -4.1528149 -4.1458611 -4.1251588 -4.1141677 -4.1131954][-4.1995649 -4.1986814 -4.19505 -4.1876073 -4.1798744 -4.1688967 -4.1503987 -4.1310043 -4.1232128 -4.137794 -4.1581922 -4.1707859 -4.1602721 -4.1496449 -4.1443973][-4.1288266 -4.139359 -4.1404824 -4.1285567 -4.1137266 -4.0934825 -4.0620422 -4.0318995 -4.0296316 -4.0638275 -4.1101327 -4.1488323 -4.1580234 -4.1531239 -4.150106][-3.99785 -4.0208058 -4.0273223 -4.0115252 -3.9899387 -3.9618018 -3.9151134 -3.8663921 -3.8602848 -3.9175196 -3.9965584 -4.0640154 -4.0969748 -4.1052241 -4.1070342][-3.9029171 -3.9423714 -3.9562 -3.9450591 -3.9248846 -3.8972898 -3.8432827 -3.7774692 -3.7473037 -3.8014679 -3.88648 -3.9643753 -4.0101523 -4.0274706 -4.0387821][-3.9403055 -3.9857807 -4.0034285 -4.0015903 -3.9972224 -3.9825594 -3.941359 -3.8860335 -3.8387351 -3.851254 -3.9011283 -3.9596262 -4.0001764 -4.015871 -4.0258956][-4.0659847 -4.1013942 -4.1140947 -4.1164765 -4.1223664 -4.1194615 -4.0979733 -4.0620165 -4.0204253 -4.0082769 -4.0281615 -4.0622807 -4.0870991 -4.0923586 -4.0937271][-4.1788874 -4.2040596 -4.2128639 -4.2150388 -4.2208343 -4.2237945 -4.2157588 -4.1964016 -4.1669974 -4.1473632 -4.1522212 -4.1719847 -4.1845236 -4.1816297 -4.1748009][-4.245513 -4.2637835 -4.2698007 -4.2713828 -4.2745156 -4.2762833 -4.2751465 -4.2653856 -4.2445664 -4.2242179 -4.223134 -4.23446 -4.2385092 -4.2292705 -4.2165742][-4.2732759 -4.2855921 -4.2905445 -4.2921314 -4.2928967 -4.2923527 -4.2919321 -4.2860913 -4.2683935 -4.246491 -4.2411289 -4.2463727 -4.2421312 -4.2266183 -4.2072573][-4.2821136 -4.2910237 -4.2970643 -4.2995167 -4.298955 -4.2964649 -4.2948632 -4.2881341 -4.2678046 -4.2386565 -4.2233944 -4.2209425 -4.2132168 -4.1948376 -4.1724472][-4.2846584 -4.2919683 -4.2988782 -4.302319 -4.3019781 -4.3001037 -4.2990623 -4.2925096 -4.2708964 -4.2387276 -4.2167525 -4.207068 -4.1978869 -4.1810546 -4.1604109]]...]
INFO - root - 2017-12-06 00:08:23.443321: step 56710, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.842 sec/batch; 64h:31m:27s remains)
INFO - root - 2017-12-06 00:08:32.031582: step 56720, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 67h:10m:40s remains)
INFO - root - 2017-12-06 00:08:40.419938: step 56730, loss = 2.06, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 68h:01m:59s remains)
INFO - root - 2017-12-06 00:08:48.917757: step 56740, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 64h:08m:31s remains)
INFO - root - 2017-12-06 00:08:57.485383: step 56750, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.808 sec/batch; 61h:51m:53s remains)
INFO - root - 2017-12-06 00:09:05.873201: step 56760, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 64h:06m:14s remains)
INFO - root - 2017-12-06 00:09:14.343818: step 56770, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 64h:57m:32s remains)
INFO - root - 2017-12-06 00:09:22.932438: step 56780, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 65h:31m:59s remains)
INFO - root - 2017-12-06 00:09:31.373456: step 56790, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 66h:46m:01s remains)
INFO - root - 2017-12-06 00:09:39.990935: step 56800, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.877 sec/batch; 67h:11m:31s remains)
2017-12-06 00:09:40.842812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2438669 -4.2445912 -4.2451453 -4.2445288 -4.2370362 -4.2369738 -4.2406764 -4.2413797 -4.2392349 -4.2327623 -4.2167068 -4.2054105 -4.2105923 -4.2199888 -4.2276797][-4.2094245 -4.2108207 -4.2102737 -4.2070551 -4.1945338 -4.1956472 -4.2064304 -4.2131548 -4.2139363 -4.2073679 -4.185267 -4.1679277 -4.1730127 -4.190805 -4.204401][-4.1864824 -4.1830812 -4.1766925 -4.1654277 -4.146378 -4.1494102 -4.1695256 -4.1913037 -4.1983147 -4.1918306 -4.16401 -4.14475 -4.1512413 -4.1764379 -4.1933527][-4.1694326 -4.1582351 -4.1399956 -4.1181679 -4.0965233 -4.1045632 -4.1352468 -4.1692824 -4.1803036 -4.1747475 -4.1475906 -4.1226549 -4.1249456 -4.1519623 -4.1724696][-4.1543379 -4.1370478 -4.1112661 -4.0801926 -4.0544887 -4.0585876 -4.086741 -4.1256375 -4.1456857 -4.1492014 -4.1314216 -4.0983281 -4.0890493 -4.1100135 -4.13426][-4.1451969 -4.1277037 -4.1012969 -4.0589395 -4.0195637 -4.0098734 -4.0271115 -4.0705047 -4.1010041 -4.1158676 -4.1113515 -4.0741353 -4.0531974 -4.067555 -4.0927172][-4.13995 -4.1261573 -4.1031361 -4.0520434 -3.9963787 -3.9673619 -3.96616 -4.0025482 -4.0391388 -4.0622945 -4.0703831 -4.0416641 -4.0185909 -4.0280051 -4.0437307][-4.1455159 -4.1338143 -4.1096025 -4.056334 -3.9933109 -3.9524817 -3.9341617 -3.9556031 -3.9883902 -4.0126505 -4.0295782 -4.0153522 -3.9940121 -3.9943655 -3.993917][-4.1435556 -4.1333227 -4.115695 -4.0699482 -4.0158124 -3.9784341 -3.95607 -3.9643018 -3.9806559 -3.9941139 -4.0110564 -4.0106492 -3.99794 -3.9996092 -3.9957061][-4.1521182 -4.1358795 -4.1202965 -4.0841269 -4.0506644 -4.0296817 -4.0165715 -4.0167036 -4.0185065 -4.0186105 -4.0286713 -4.0399241 -4.0465269 -4.0595641 -4.0625272][-4.166852 -4.1483326 -4.1342783 -4.1092358 -4.0955396 -4.0931892 -4.0898814 -4.0838957 -4.079895 -4.0748487 -4.0777669 -4.0949111 -4.1135669 -4.1295137 -4.1286259][-4.1843791 -4.1696253 -4.1575952 -4.1409936 -4.1391363 -4.14829 -4.1539316 -4.150589 -4.1469574 -4.1409717 -4.1386251 -4.1530046 -4.171217 -4.1863327 -4.1849289][-4.2040095 -4.1944451 -4.1871238 -4.1799011 -4.1834321 -4.1955762 -4.2060142 -4.2078905 -4.2051449 -4.1980553 -4.1929245 -4.2003031 -4.2140031 -4.2273555 -4.2323875][-4.2238832 -4.2179747 -4.2138562 -4.2106977 -4.2161336 -4.2279506 -4.2390928 -4.2434125 -4.2425194 -4.2355509 -4.2290359 -4.2299843 -4.2410159 -4.2551155 -4.2635884][-4.2475381 -4.2442861 -4.2443991 -4.2469511 -4.2529354 -4.2616696 -4.2697825 -4.2702494 -4.2646289 -4.256474 -4.2488513 -4.2477441 -4.2587752 -4.2741637 -4.2844324]]...]
INFO - root - 2017-12-06 00:09:49.353186: step 56810, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 64h:13m:18s remains)
INFO - root - 2017-12-06 00:09:57.694110: step 56820, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.814 sec/batch; 62h:20m:24s remains)
INFO - root - 2017-12-06 00:10:06.173971: step 56830, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 64h:13m:16s remains)
INFO - root - 2017-12-06 00:10:14.455649: step 56840, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 65h:11m:50s remains)
INFO - root - 2017-12-06 00:10:22.977710: step 56850, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:29m:40s remains)
INFO - root - 2017-12-06 00:10:31.462459: step 56860, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 66h:20m:42s remains)
INFO - root - 2017-12-06 00:10:39.873190: step 56870, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 63h:12m:29s remains)
INFO - root - 2017-12-06 00:10:48.411418: step 56880, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 63h:48m:45s remains)
INFO - root - 2017-12-06 00:10:56.845049: step 56890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:28m:35s remains)
INFO - root - 2017-12-06 00:11:05.188912: step 56900, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.876 sec/batch; 67h:02m:16s remains)
2017-12-06 00:11:06.008898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3038039 -4.2959409 -4.270771 -4.2535343 -4.2542582 -4.2589831 -4.2538962 -4.2387638 -4.2260132 -4.2284527 -4.2412696 -4.2621045 -4.2866144 -4.3068814 -4.3205466][-4.296082 -4.2950869 -4.2760315 -4.2629123 -4.2624497 -4.2575426 -4.2331147 -4.1982055 -4.1736693 -4.1747069 -4.19455 -4.2244248 -4.260025 -4.2911983 -4.31323][-4.2789474 -4.2896824 -4.2791252 -4.2696371 -4.2667379 -4.2525506 -4.2074771 -4.1505818 -4.1100144 -4.108058 -4.1356974 -4.1777558 -4.2259064 -4.2699614 -4.3031611][-4.2430754 -4.2684689 -4.2702937 -4.2656937 -4.262764 -4.2426777 -4.1824293 -4.1043839 -4.0439744 -4.036202 -4.0729723 -4.1288018 -4.1924119 -4.2501354 -4.2929053][-4.1814852 -4.2246227 -4.2426491 -4.2469196 -4.2458482 -4.219873 -4.1474819 -4.0533752 -3.9788432 -3.9707706 -4.0250282 -4.098454 -4.1756072 -4.24175 -4.2883048][-4.1073227 -4.1693397 -4.2052355 -4.2220683 -4.2258968 -4.1954045 -4.1121144 -4.0008235 -3.9133837 -3.9138324 -3.9935279 -4.0863 -4.1724052 -4.2428069 -4.2894435][-4.0409017 -4.1182127 -4.1745348 -4.20607 -4.2142305 -4.1749883 -4.0765886 -3.9445953 -3.8487737 -3.8648968 -3.9676425 -4.0780115 -4.1755257 -4.2510219 -4.2950239][-4.0118794 -4.0916438 -4.1627011 -4.2088008 -4.2202015 -4.1747246 -4.069097 -3.9378467 -3.8516519 -3.8768582 -3.9825866 -4.098814 -4.20077 -4.2757549 -4.3117981][-4.0536036 -4.1197791 -4.1876593 -4.2334795 -4.2418151 -4.1988082 -4.1119618 -4.0109758 -3.9480827 -3.9690304 -4.0548162 -4.1524906 -4.2401657 -4.3024211 -4.3287148][-4.1270752 -4.170723 -4.2179737 -4.2472992 -4.2457018 -4.2097521 -4.1497831 -4.0830784 -4.0367742 -4.0456638 -4.1062593 -4.182498 -4.2559557 -4.3089862 -4.3324695][-4.1872005 -4.2103119 -4.2310085 -4.2382236 -4.2239838 -4.1991739 -4.1692591 -4.1280913 -4.0870471 -4.080833 -4.1231503 -4.1852684 -4.2475657 -4.2960911 -4.3220811][-4.2158685 -4.2196379 -4.2183571 -4.2091269 -4.1886911 -4.1764846 -4.1710877 -4.1492262 -4.1116242 -4.1004038 -4.1330733 -4.1847763 -4.2359524 -4.2800574 -4.3089828][-4.2268028 -4.2223349 -4.2100372 -4.1943617 -4.1763892 -4.1758695 -4.1821237 -4.1670623 -4.1326103 -4.1247797 -4.1530957 -4.1966891 -4.2390366 -4.2773824 -4.3040981][-4.2516184 -4.2474113 -4.2320342 -4.2154889 -4.2020726 -4.2056336 -4.212605 -4.2018681 -4.1770391 -4.1774707 -4.20196 -4.2346931 -4.2654 -4.29288 -4.3121142][-4.2849984 -4.2855358 -4.2761555 -4.2648845 -4.2569237 -4.2610865 -4.2669363 -4.2599258 -4.2441692 -4.2465281 -4.2630911 -4.2834592 -4.3015866 -4.3163891 -4.3258338]]...]
INFO - root - 2017-12-06 00:11:14.419051: step 56910, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 65h:51m:30s remains)
INFO - root - 2017-12-06 00:11:22.982906: step 56920, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 64h:16m:03s remains)
INFO - root - 2017-12-06 00:11:31.447228: step 56930, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 67h:22m:27s remains)
INFO - root - 2017-12-06 00:11:39.960563: step 56940, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 66h:05m:34s remains)
INFO - root - 2017-12-06 00:11:48.288016: step 56950, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:46m:35s remains)
INFO - root - 2017-12-06 00:11:56.780445: step 56960, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 64h:18m:35s remains)
INFO - root - 2017-12-06 00:12:05.269423: step 56970, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 66h:16m:20s remains)
INFO - root - 2017-12-06 00:12:13.726343: step 56980, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 66h:40m:21s remains)
INFO - root - 2017-12-06 00:12:22.212091: step 56990, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 66h:41m:54s remains)
INFO - root - 2017-12-06 00:12:30.652453: step 57000, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:26m:04s remains)
2017-12-06 00:12:31.486849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2613378 -4.28928 -4.2969337 -4.2889919 -4.2761059 -4.26692 -4.259994 -4.2611833 -4.2741685 -4.2906694 -4.3054948 -4.3160276 -4.3261003 -4.3320031 -4.333003][-4.2213144 -4.2657804 -4.2872992 -4.2854548 -4.265811 -4.2352262 -4.2056146 -4.1994171 -4.22175 -4.2510052 -4.2779293 -4.2984405 -4.3166208 -4.3271437 -4.3266268][-4.1931834 -4.2441339 -4.2685132 -4.2652893 -4.2330618 -4.1777248 -4.1250744 -4.1163058 -4.154366 -4.2040586 -4.2466784 -4.278244 -4.3028765 -4.3166347 -4.3168464][-4.1739407 -4.2252073 -4.2491512 -4.2408533 -4.193439 -4.111237 -4.034596 -4.0241323 -4.0821033 -4.1572962 -4.2172527 -4.2584066 -4.2891822 -4.3063831 -4.3074427][-4.1790619 -4.2247314 -4.2418385 -4.2232151 -4.1586852 -4.0493283 -3.9464931 -3.9350924 -4.0185857 -4.1179776 -4.1932306 -4.2427182 -4.279458 -4.3002667 -4.3023415][-4.1865273 -4.2259393 -4.2389369 -4.2131019 -4.1369271 -4.00445 -3.8733168 -3.8562727 -3.9642491 -4.0849247 -4.1737051 -4.230433 -4.2714977 -4.2966852 -4.3019629][-4.1816688 -4.2182851 -4.2322927 -4.2088356 -4.1282139 -3.9789805 -3.8207572 -3.7947524 -3.9237833 -4.0632515 -4.1612792 -4.2232113 -4.2668223 -4.2944026 -4.3025947][-4.1675262 -4.2054577 -4.2232809 -4.2065897 -4.1284318 -3.976428 -3.8065042 -3.7722511 -3.9078507 -4.0554461 -4.1557903 -4.21927 -4.2633824 -4.2907982 -4.3002577][-4.1608276 -4.1971407 -4.2191644 -4.209836 -4.1433034 -4.0105457 -3.8623812 -3.8326607 -3.9505079 -4.0824494 -4.1687217 -4.222724 -4.2620206 -4.2871327 -4.2960896][-4.1644716 -4.1948214 -4.2166815 -4.21627 -4.1708055 -4.0701814 -3.9606485 -3.9425778 -4.03181 -4.1291094 -4.1903181 -4.22867 -4.2574224 -4.2783089 -4.2866168][-4.16227 -4.1874342 -4.2079029 -4.214725 -4.1921206 -4.1252046 -4.0519738 -4.0437989 -4.1073422 -4.173358 -4.2101068 -4.2297473 -4.2474418 -4.2654428 -4.2766643][-4.1522708 -4.1741662 -4.1881647 -4.1993036 -4.1976571 -4.1649246 -4.1254148 -4.1208696 -4.1605439 -4.2021794 -4.2204766 -4.2264328 -4.2370477 -4.2543087 -4.2709017][-4.1539254 -4.1688819 -4.1708989 -4.1795373 -4.1923943 -4.1868529 -4.1725755 -4.1712127 -4.1958365 -4.2194819 -4.2256522 -4.2218032 -4.2260585 -4.244144 -4.2672405][-4.170012 -4.1765089 -4.1642327 -4.1648684 -4.1819582 -4.191112 -4.1928124 -4.1983476 -4.2155457 -4.225738 -4.2233233 -4.2150354 -4.2177186 -4.2370853 -4.2641268][-4.204865 -4.20807 -4.1849308 -4.1721959 -4.1827211 -4.19704 -4.2046337 -4.2112761 -4.2215257 -4.223289 -4.2182188 -4.2121968 -4.2165079 -4.237 -4.2646708]]...]
INFO - root - 2017-12-06 00:12:39.990388: step 57010, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.827 sec/batch; 63h:17m:52s remains)
INFO - root - 2017-12-06 00:12:48.471808: step 57020, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 65h:32m:53s remains)
INFO - root - 2017-12-06 00:12:56.973235: step 57030, loss = 2.09, batch loss = 2.03 (9.8 examples/sec; 0.818 sec/batch; 62h:35m:47s remains)
INFO - root - 2017-12-06 00:13:05.360183: step 57040, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 64h:57m:54s remains)
INFO - root - 2017-12-06 00:13:13.965660: step 57050, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 65h:59m:22s remains)
INFO - root - 2017-12-06 00:13:22.446870: step 57060, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 66h:19m:40s remains)
INFO - root - 2017-12-06 00:13:30.949594: step 57070, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 65h:56m:15s remains)
INFO - root - 2017-12-06 00:13:39.489357: step 57080, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.873 sec/batch; 66h:45m:54s remains)
INFO - root - 2017-12-06 00:13:48.146667: step 57090, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 65h:31m:45s remains)
INFO - root - 2017-12-06 00:13:56.661694: step 57100, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.897 sec/batch; 68h:36m:33s remains)
2017-12-06 00:13:57.445428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3163385 -4.3100109 -4.3004794 -4.2834034 -4.2613854 -4.24173 -4.2253475 -4.2058024 -4.1977406 -4.2127857 -4.2368336 -4.2509727 -4.2529445 -4.2552586 -4.2552652][-4.3164358 -4.3085728 -4.2965 -4.2773786 -4.2532897 -4.2279086 -4.2029181 -4.1703634 -4.1528344 -4.1727319 -4.2095518 -4.2341218 -4.2412858 -4.247818 -4.2514381][-4.3127122 -4.30419 -4.2932138 -4.2743707 -4.2472472 -4.2145405 -4.1784978 -4.1359839 -4.1156635 -4.1433244 -4.1884813 -4.2150888 -4.2202973 -4.2272339 -4.2312775][-4.3092213 -4.301897 -4.293725 -4.2742615 -4.2429175 -4.2025385 -4.1572986 -4.1095986 -4.0922174 -4.1277456 -4.1758852 -4.1983447 -4.1984954 -4.2049446 -4.2092886][-4.30842 -4.3026171 -4.2938423 -4.268477 -4.2309227 -4.181426 -4.12737 -4.07662 -4.0639682 -4.1078081 -4.1610661 -4.1873679 -4.1908731 -4.1972113 -4.2004356][-4.3098578 -4.3050079 -4.2923613 -4.2584333 -4.2106524 -4.151123 -4.0876336 -4.0341687 -4.0269957 -4.0830956 -4.1489496 -4.1892767 -4.1991639 -4.2046 -4.2072487][-4.3123364 -4.3084197 -4.2923174 -4.2504473 -4.1919322 -4.12648 -4.06573 -4.0208335 -4.0235729 -4.0882955 -4.1613493 -4.2082458 -4.2192268 -4.2221336 -4.2246814][-4.3134036 -4.3087988 -4.2907486 -4.2449903 -4.1836095 -4.1249037 -4.0827608 -4.0578814 -4.068984 -4.1293592 -4.1956544 -4.2388544 -4.2470069 -4.2462058 -4.2461967][-4.3111553 -4.303968 -4.2839789 -4.2371864 -4.1804447 -4.13593 -4.1163278 -4.1092696 -4.1259637 -4.1783729 -4.2333694 -4.2708 -4.276938 -4.2741332 -4.2702465][-4.30668 -4.2958713 -4.2740669 -4.2289662 -4.1795731 -4.149488 -4.1472678 -4.152658 -4.1739531 -4.2187791 -4.2605181 -4.2894363 -4.2956114 -4.2965336 -4.2918591][-4.3019881 -4.2878609 -4.26621 -4.2281289 -4.1884203 -4.1689973 -4.1785293 -4.192759 -4.2174582 -4.2541742 -4.2801051 -4.2962971 -4.3006568 -4.3050876 -4.303782][-4.2979989 -4.2818027 -4.2615829 -4.2337394 -4.2055736 -4.1949153 -4.2106352 -4.2305584 -4.2567635 -4.2850657 -4.2975388 -4.3001814 -4.2989306 -4.3037891 -4.304471][-4.2957273 -4.2796021 -4.2630949 -4.2450638 -4.2274513 -4.2234445 -4.2408247 -4.2595119 -4.2820048 -4.30195 -4.3049951 -4.2977643 -4.2923331 -4.2968059 -4.29962][-4.2972937 -4.2837462 -4.2728043 -4.2634354 -4.2545719 -4.2518959 -4.2632093 -4.2748861 -4.2910843 -4.3042536 -4.3025069 -4.2916422 -4.2846079 -4.2889104 -4.2928057][-4.3022933 -4.2923312 -4.2876029 -4.28537 -4.2799506 -4.2728338 -4.2750025 -4.2809868 -4.2922206 -4.3013215 -4.29624 -4.2831106 -4.2749672 -4.2769747 -4.27882]]...]
INFO - root - 2017-12-06 00:14:05.955146: step 57110, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.813 sec/batch; 62h:11m:37s remains)
INFO - root - 2017-12-06 00:14:14.528362: step 57120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 66h:29m:21s remains)
INFO - root - 2017-12-06 00:14:23.172305: step 57130, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.895 sec/batch; 68h:27m:51s remains)
INFO - root - 2017-12-06 00:14:31.604635: step 57140, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 65h:08m:25s remains)
INFO - root - 2017-12-06 00:14:40.163141: step 57150, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.872 sec/batch; 66h:39m:38s remains)
INFO - root - 2017-12-06 00:14:48.706648: step 57160, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 66h:24m:11s remains)
INFO - root - 2017-12-06 00:14:57.241789: step 57170, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 66h:49m:08s remains)
INFO - root - 2017-12-06 00:15:05.710454: step 57180, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.817 sec/batch; 62h:29m:10s remains)
INFO - root - 2017-12-06 00:15:14.312152: step 57190, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 65h:28m:53s remains)
INFO - root - 2017-12-06 00:15:22.819222: step 57200, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 64h:31m:40s remains)
2017-12-06 00:15:23.707360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1153092 -4.1584368 -4.1961541 -4.1960478 -4.179625 -4.1715751 -4.1792717 -4.1860452 -4.177134 -4.1676755 -4.176064 -4.1965885 -4.2137866 -4.2153368 -4.2085652][-4.1205511 -4.1698194 -4.2095318 -4.2097716 -4.1936626 -4.1874294 -4.19222 -4.1971855 -4.188 -4.1791568 -4.1846466 -4.1989269 -4.2102928 -4.2077785 -4.1986237][-4.1340456 -4.1856108 -4.2231884 -4.2253385 -4.2118697 -4.2048044 -4.2098765 -4.2153344 -4.2087169 -4.2048044 -4.2121344 -4.2215142 -4.22363 -4.2161212 -4.2065673][-4.1148396 -4.1653128 -4.202086 -4.2039943 -4.183445 -4.173502 -4.1874332 -4.2047887 -4.2096806 -4.2126803 -4.2191644 -4.2226143 -4.218627 -4.2076044 -4.1958895][-4.1045256 -4.1477852 -4.1688848 -4.1521463 -4.1088843 -4.0891423 -4.117053 -4.1529465 -4.1796622 -4.1989532 -4.2017531 -4.2015486 -4.2016244 -4.1959033 -4.1827774][-4.12419 -4.145823 -4.1362114 -4.08406 -4.0036349 -3.9629564 -3.9959404 -4.0505867 -4.1109681 -4.1627369 -4.1725321 -4.1752853 -4.1891842 -4.1965356 -4.1910911][-4.1630592 -4.1566129 -4.1175227 -4.035028 -3.9176881 -3.8467882 -3.8756583 -3.9422145 -4.0359859 -4.1225824 -4.1465259 -4.1593323 -4.180891 -4.1994658 -4.20886][-4.2008634 -4.178998 -4.1262503 -4.0447788 -3.9291189 -3.8431265 -3.852417 -3.9127612 -4.0169635 -4.1119661 -4.14199 -4.1629243 -4.1900997 -4.2136674 -4.2339797][-4.225081 -4.2017031 -4.1546693 -4.09802 -4.0198679 -3.9515877 -3.9484658 -3.9908631 -4.0752316 -4.1505508 -4.1747851 -4.1958909 -4.2193947 -4.2392735 -4.2608676][-4.250391 -4.2289 -4.1960468 -4.1652031 -4.1209455 -4.0762215 -4.069911 -4.09749 -4.1570024 -4.2119837 -4.2306032 -4.2475948 -4.2658443 -4.2814384 -4.2974038][-4.2750192 -4.2590837 -4.2397933 -4.2239022 -4.2027626 -4.17822 -4.1739759 -4.1892586 -4.2255697 -4.2607622 -4.2742167 -4.2877536 -4.3017149 -4.3128352 -4.3210435][-4.2976494 -4.2852073 -4.2742887 -4.2665777 -4.2589827 -4.2487073 -4.2478161 -4.257123 -4.2766442 -4.2948933 -4.3015242 -4.309144 -4.3168344 -4.3229837 -4.3246017][-4.3167677 -4.3048234 -4.2954946 -4.290123 -4.286768 -4.2860117 -4.2888689 -4.2957072 -4.3058782 -4.3133078 -4.3156462 -4.3177409 -4.3217411 -4.3250628 -4.3237491][-4.3284955 -4.3157516 -4.3039331 -4.2960038 -4.2922344 -4.295742 -4.3015614 -4.3071041 -4.3150616 -4.32096 -4.324142 -4.3249459 -4.3252072 -4.3256474 -4.3223491][-4.33298 -4.32206 -4.3098326 -4.2996645 -4.2950649 -4.2985916 -4.3054976 -4.3122954 -4.318563 -4.3233352 -4.3261657 -4.3258944 -4.3246922 -4.3244228 -4.3227406]]...]
INFO - root - 2017-12-06 00:15:32.326678: step 57210, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 66h:48m:10s remains)
INFO - root - 2017-12-06 00:15:40.798099: step 57220, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 64h:54m:39s remains)
INFO - root - 2017-12-06 00:15:49.222336: step 57230, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 65h:09m:12s remains)
INFO - root - 2017-12-06 00:15:57.570706: step 57240, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 63h:46m:44s remains)
INFO - root - 2017-12-06 00:16:06.059127: step 57250, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 65h:28m:54s remains)
INFO - root - 2017-12-06 00:16:14.553790: step 57260, loss = 2.06, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 63h:04m:18s remains)
INFO - root - 2017-12-06 00:16:23.038917: step 57270, loss = 2.05, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 63h:35m:16s remains)
INFO - root - 2017-12-06 00:16:31.574732: step 57280, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.873 sec/batch; 66h:42m:40s remains)
INFO - root - 2017-12-06 00:16:40.117095: step 57290, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 66h:22m:21s remains)
INFO - root - 2017-12-06 00:16:48.766530: step 57300, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 64h:55m:36s remains)
2017-12-06 00:16:49.556300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2266788 -4.2264891 -4.2283974 -4.22736 -4.2135506 -4.1998978 -4.2033548 -4.2090826 -4.2120576 -4.2161303 -4.2154179 -4.2025652 -4.1949573 -4.1991611 -4.2113614][-4.2015958 -4.2060061 -4.206316 -4.1984167 -4.1762347 -4.1549377 -4.1591163 -4.1646881 -4.1701751 -4.1802616 -4.1863165 -4.1717143 -4.1581988 -4.1584778 -4.1724267][-4.1682658 -4.1778512 -4.1789603 -4.1632366 -4.1321073 -4.1025867 -4.1095924 -4.1215396 -4.1386108 -4.1537957 -4.1605282 -4.14473 -4.128407 -4.1226048 -4.1329393][-4.1326818 -4.1372771 -4.1341176 -4.1179228 -4.0913496 -4.0624409 -4.069345 -4.0833974 -4.1036806 -4.1222711 -4.1270142 -4.1078391 -4.0864081 -4.0733409 -4.074841][-4.1029658 -4.1010561 -4.0927844 -4.0763211 -4.054595 -4.0172734 -3.9961662 -4.003593 -4.0353913 -4.0680451 -4.0792332 -4.066905 -4.0451326 -4.0240746 -4.0199547][-4.0942621 -4.0860844 -4.0717568 -4.04877 -4.01723 -3.9513364 -3.8592961 -3.8412237 -3.9152822 -3.9897547 -4.0276904 -4.0318961 -4.0213633 -4.0008764 -3.9976976][-4.1137848 -4.1062312 -4.0845356 -4.04201 -3.9886551 -3.8966336 -3.752584 -3.7143509 -3.8321905 -3.9476752 -4.0082927 -4.0234566 -4.0152483 -3.9916351 -3.9768653][-4.1393933 -4.133913 -4.1093888 -4.0608144 -4.0115762 -3.9535763 -3.8573294 -3.8229918 -3.8943596 -3.9834614 -4.0389962 -4.0532489 -4.030118 -3.9938669 -3.973763][-4.139235 -4.1407251 -4.1272025 -4.0915546 -4.0595603 -4.0386095 -4.0050817 -3.986938 -4.0002561 -4.0433044 -4.085237 -4.1014729 -4.0793753 -4.0507374 -4.0404458][-4.134686 -4.1404772 -4.1401172 -4.1196713 -4.0981903 -4.0913239 -4.0825567 -4.0705338 -4.0594234 -4.0772223 -4.1051722 -4.1203251 -4.1100116 -4.1045141 -4.1106014][-4.1282229 -4.1386871 -4.151351 -4.1456375 -4.1306424 -4.1198425 -4.1173644 -4.1136603 -4.0998297 -4.0971718 -4.1134324 -4.1210423 -4.1123128 -4.1152058 -4.12685][-4.1262341 -4.1423426 -4.164134 -4.1667223 -4.15885 -4.1486912 -4.1497283 -4.1518784 -4.141777 -4.1315665 -4.134819 -4.13793 -4.1254506 -4.1291504 -4.142312][-4.1552467 -4.1700077 -4.192059 -4.1996183 -4.1970763 -4.18963 -4.1928582 -4.1912866 -4.1828194 -4.1734 -4.1697364 -4.1681752 -4.1572113 -4.1617551 -4.17556][-4.200264 -4.210947 -4.2271776 -4.23503 -4.2366056 -4.2351294 -4.2337317 -4.2256203 -4.2197156 -4.2150245 -4.209991 -4.2044487 -4.1979375 -4.2024617 -4.2131143][-4.2478886 -4.2527137 -4.26109 -4.2700047 -4.2752032 -4.27703 -4.2725296 -4.2619667 -4.2582641 -4.2571597 -4.2558918 -4.2498646 -4.2452197 -4.2466664 -4.2498245]]...]
INFO - root - 2017-12-06 00:16:58.127635: step 57310, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.844 sec/batch; 64h:30m:51s remains)
INFO - root - 2017-12-06 00:17:06.686420: step 57320, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 66h:21m:03s remains)
INFO - root - 2017-12-06 00:17:15.214093: step 57330, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 64h:08m:00s remains)
INFO - root - 2017-12-06 00:17:23.662176: step 57340, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 67h:12m:20s remains)
INFO - root - 2017-12-06 00:17:32.238799: step 57350, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 64h:12m:13s remains)
INFO - root - 2017-12-06 00:17:40.983597: step 57360, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:21m:44s remains)
INFO - root - 2017-12-06 00:17:49.483720: step 57370, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 64h:46m:51s remains)
INFO - root - 2017-12-06 00:17:58.030274: step 57380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 64h:55m:59s remains)
INFO - root - 2017-12-06 00:18:06.547242: step 57390, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 66h:19m:45s remains)
INFO - root - 2017-12-06 00:18:15.142939: step 57400, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 67h:29m:39s remains)
2017-12-06 00:18:15.964030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2912922 -4.2726593 -4.2482448 -4.21112 -4.1483269 -4.066958 -4.0286937 -4.0577087 -4.1011672 -4.12956 -4.1406817 -4.146915 -4.168951 -4.2071519 -4.2289639][-4.288991 -4.2663908 -4.2379527 -4.1938977 -4.1240759 -4.0388794 -3.9999003 -4.0303731 -4.0746708 -4.1052747 -4.1183658 -4.1261015 -4.1531544 -4.19468 -4.2178564][-4.2893152 -4.2649584 -4.2338886 -4.18621 -4.113234 -4.0256138 -3.9822574 -4.0119944 -4.0619226 -4.0975027 -4.1122313 -4.119904 -4.1462336 -4.1832337 -4.20111][-4.2904706 -4.2662797 -4.2352357 -4.1882334 -4.1147118 -4.023531 -3.9717746 -3.9982042 -4.058042 -4.1025448 -4.1202493 -4.12403 -4.1457491 -4.1744061 -4.1870885][-4.2904925 -4.26702 -4.2357626 -4.189085 -4.11242 -4.0122457 -3.9458015 -3.9691794 -4.0465345 -4.1078739 -4.1308584 -4.1277761 -4.1413059 -4.1640353 -4.1767316][-4.2900562 -4.2676086 -4.2367387 -4.1893115 -4.1047525 -3.9847777 -3.8908589 -3.9089534 -4.0080867 -4.0919418 -4.1303997 -4.1296659 -4.1371961 -4.1564984 -4.1701589][-4.2903714 -4.2694845 -4.2394471 -4.1918087 -4.1011853 -3.9614816 -3.8343718 -3.8365073 -3.9503686 -4.052 -4.1074347 -4.1204381 -4.1312895 -4.1502113 -4.1655407][-4.2907171 -4.2721677 -4.2451067 -4.2017913 -4.1140308 -3.9757197 -3.8464649 -3.8360906 -3.9359665 -4.0321755 -4.0908933 -4.1126342 -4.1296959 -4.1526351 -4.1691074][-4.2903957 -4.2730985 -4.2490983 -4.2133131 -4.1371756 -4.0203376 -3.9271584 -3.9277182 -3.9971685 -4.0627031 -4.10409 -4.1235223 -4.1446214 -4.1709633 -4.186419][-4.2887115 -4.2705169 -4.24838 -4.2188807 -4.1553617 -4.0610895 -4.0019345 -4.019074 -4.0688858 -4.105824 -4.1254005 -4.1355519 -4.1578817 -4.1881123 -4.2022052][-4.2864776 -4.2655745 -4.2440147 -4.2164741 -4.1621728 -4.0837021 -4.0469189 -4.0735145 -4.1112795 -4.1323619 -4.1387391 -4.1443076 -4.1632519 -4.191628 -4.2019935][-4.285285 -4.2616129 -4.2381921 -4.2082858 -4.1569195 -4.0872169 -4.0635161 -4.0919566 -4.1219354 -4.1352711 -4.1380954 -4.1421452 -4.1537771 -4.1766777 -4.1875496][-4.2857618 -4.2619386 -4.23786 -4.2048612 -4.1503854 -4.0836997 -4.0651088 -4.0883479 -4.108551 -4.1168356 -4.1209888 -4.1272483 -4.1354384 -4.1537871 -4.16301][-4.2873473 -4.2671022 -4.245029 -4.2110171 -4.1527481 -4.0847549 -4.062047 -4.0762143 -4.0900564 -4.0951762 -4.1003327 -4.1073952 -4.1167731 -4.1304078 -4.1375279][-4.2884231 -4.2720957 -4.2533889 -4.2220674 -4.1669559 -4.1004963 -4.0706472 -4.0710535 -4.078845 -4.0819473 -4.0851703 -4.0918818 -4.1013904 -4.1118379 -4.1179914]]...]
INFO - root - 2017-12-06 00:18:24.504116: step 57410, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.837 sec/batch; 63h:56m:15s remains)
INFO - root - 2017-12-06 00:18:33.069656: step 57420, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 65h:35m:09s remains)
INFO - root - 2017-12-06 00:18:41.450326: step 57430, loss = 2.03, batch loss = 1.97 (10.2 examples/sec; 0.784 sec/batch; 59h:52m:36s remains)
INFO - root - 2017-12-06 00:18:49.914179: step 57440, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 65h:36m:36s remains)
INFO - root - 2017-12-06 00:18:58.608952: step 57450, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 66h:33m:29s remains)
INFO - root - 2017-12-06 00:19:07.286421: step 57460, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 65h:49m:58s remains)
INFO - root - 2017-12-06 00:19:15.910882: step 57470, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 66h:05m:42s remains)
INFO - root - 2017-12-06 00:19:24.447311: step 57480, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 65h:58m:06s remains)
INFO - root - 2017-12-06 00:19:33.013803: step 57490, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.820 sec/batch; 62h:38m:45s remains)
INFO - root - 2017-12-06 00:19:41.422905: step 57500, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 64h:54m:06s remains)
2017-12-06 00:19:42.203518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2607112 -4.2621808 -4.2583513 -4.2517762 -4.2507505 -4.2488632 -4.2450662 -4.2359252 -4.2309628 -4.2343707 -4.2299685 -4.2177348 -4.2149873 -4.2233605 -4.2277737][-4.2384996 -4.2437053 -4.244648 -4.2432346 -4.2469196 -4.2474346 -4.25062 -4.2508011 -4.2478013 -4.2457528 -4.2339382 -4.2146277 -4.2065806 -4.2112017 -4.212862][-4.2316365 -4.239831 -4.2436838 -4.2442508 -4.2486391 -4.2511964 -4.2576828 -4.26271 -4.2611022 -4.2538013 -4.23934 -4.223618 -4.2151523 -4.2112885 -4.20197][-4.239768 -4.243628 -4.24476 -4.2430725 -4.244091 -4.2466097 -4.2532873 -4.2612872 -4.2630091 -4.2597122 -4.2491841 -4.2395377 -4.2327251 -4.218646 -4.1942329][-4.2482862 -4.2493792 -4.246057 -4.2407627 -4.2371182 -4.2363267 -4.2411656 -4.2510633 -4.2597575 -4.265893 -4.2638469 -4.2584028 -4.2494717 -4.22726 -4.1927691][-4.2524233 -4.2562613 -4.2531333 -4.2441845 -4.2336721 -4.2273827 -4.2279439 -4.2362738 -4.2482753 -4.2621412 -4.2685561 -4.2656479 -4.2535052 -4.2303138 -4.1976748][-4.2534904 -4.2593541 -4.25913 -4.2489061 -4.2322736 -4.2187138 -4.20919 -4.2096367 -4.2207651 -4.241116 -4.2564278 -4.2585344 -4.246819 -4.2252541 -4.2006364][-4.2502952 -4.2542896 -4.2536869 -4.2438116 -4.2240939 -4.2022367 -4.1784487 -4.165462 -4.1730571 -4.1991367 -4.2243562 -4.2317572 -4.2185678 -4.1953611 -4.1784658][-4.2395978 -4.2385159 -4.2339377 -4.2215347 -4.1986947 -4.1654634 -4.1243052 -4.0939732 -4.0904722 -4.1172142 -4.1564112 -4.17595 -4.1643257 -4.1367245 -4.1212139][-4.2197218 -4.2189679 -4.2123981 -4.2014933 -4.1793485 -4.1381125 -4.0827293 -4.0353179 -4.0145411 -4.0304418 -4.0777249 -4.108912 -4.1010633 -4.0670176 -4.0436835][-4.2002831 -4.1980543 -4.1905518 -4.1809335 -4.1614923 -4.1208334 -4.0630574 -4.0140586 -3.9865232 -3.9913588 -4.0342908 -4.069171 -4.0684991 -4.0374522 -4.0117531][-4.2078629 -4.201633 -4.1913648 -4.1843195 -4.1710191 -4.1422052 -4.1014023 -4.0700026 -4.0484886 -4.0455356 -4.0747828 -4.1016693 -4.1062183 -4.0882244 -4.0693769][-4.2456903 -4.237803 -4.2243414 -4.2175193 -4.2116704 -4.1982927 -4.1773281 -4.1622152 -4.14847 -4.1418486 -4.1537638 -4.1664977 -4.1692204 -4.1616516 -4.1516185][-4.2850718 -4.2792726 -4.2672868 -4.2599468 -4.2568674 -4.2541704 -4.2452884 -4.2381592 -4.2306585 -4.2249889 -4.2270045 -4.2304955 -4.2316957 -4.2311172 -4.2280674][-4.3209887 -4.3160248 -4.3054161 -4.2975726 -4.2933512 -4.2933431 -4.2935672 -4.2907329 -4.2858319 -4.2847495 -4.2859693 -4.2861843 -4.2870283 -4.2880425 -4.2878561]]...]
INFO - root - 2017-12-06 00:19:50.771170: step 57510, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 66h:19m:41s remains)
INFO - root - 2017-12-06 00:19:59.356784: step 57520, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.852 sec/batch; 65h:06m:02s remains)
INFO - root - 2017-12-06 00:20:07.907577: step 57530, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 65h:33m:53s remains)
INFO - root - 2017-12-06 00:20:16.240026: step 57540, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 66h:50m:55s remains)
INFO - root - 2017-12-06 00:20:24.838990: step 57550, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 66h:29m:21s remains)
INFO - root - 2017-12-06 00:20:33.276787: step 57560, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.824 sec/batch; 62h:56m:14s remains)
INFO - root - 2017-12-06 00:20:41.852893: step 57570, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 65h:54m:46s remains)
INFO - root - 2017-12-06 00:20:50.466481: step 57580, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 66h:59m:37s remains)
INFO - root - 2017-12-06 00:20:59.035378: step 57590, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.901 sec/batch; 68h:49m:58s remains)
INFO - root - 2017-12-06 00:21:07.582592: step 57600, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:41m:27s remains)
2017-12-06 00:21:08.339954: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2183146 -4.2151203 -4.2170334 -4.2028055 -4.1788464 -4.167099 -4.1561327 -4.1390882 -4.1260333 -4.1365604 -4.1696239 -4.2066264 -4.2270269 -4.2229638 -4.1997423][-4.2404232 -4.2417693 -4.2375941 -4.2178874 -4.1879916 -4.1679487 -4.1513147 -4.1268597 -4.1105542 -4.1230197 -4.1590405 -4.1955566 -4.214798 -4.2079587 -4.1776972][-4.255157 -4.259069 -4.2500248 -4.223876 -4.1895256 -4.1606116 -4.134624 -4.1029272 -4.0857587 -4.1029105 -4.1418619 -4.1807971 -4.2015829 -4.1942716 -4.1606874][-4.2613621 -4.2632208 -4.2488127 -4.2174892 -4.1783395 -4.1424747 -4.1086521 -4.0699887 -4.0516958 -4.07337 -4.1166687 -4.1609516 -4.18905 -4.187398 -4.1557851][-4.257165 -4.2535152 -4.2331109 -4.1972513 -4.1580925 -4.121942 -4.0849657 -4.0447464 -4.0254097 -4.05392 -4.103507 -4.1526546 -4.1861548 -4.1904755 -4.1657233][-4.2469635 -4.23807 -4.2145467 -4.1796317 -4.1440296 -4.1113052 -4.0759783 -4.0344667 -4.0154066 -4.0524797 -4.1088119 -4.1607294 -4.1951394 -4.2022495 -4.1825624][-4.2390552 -4.23006 -4.2060943 -4.1726174 -4.139277 -4.1064835 -4.0702128 -4.0266137 -4.0120635 -4.0599585 -4.1230283 -4.1776276 -4.2121382 -4.2167997 -4.1982179][-4.2394586 -4.2313075 -4.2065382 -4.1729445 -4.1397696 -4.1044679 -4.0611959 -4.0105162 -4.001411 -4.0639453 -4.1353 -4.194335 -4.2284336 -4.2303553 -4.2127452][-4.2425303 -4.2339225 -4.2057238 -4.1711721 -4.1405997 -4.1024981 -4.048697 -3.9892311 -3.9867518 -4.060709 -4.1374493 -4.1987338 -4.2345114 -4.2387652 -4.2264142][-4.2477651 -4.2373767 -4.2048397 -4.1685734 -4.1384497 -4.09817 -4.0367155 -3.9751656 -3.9829142 -4.0615454 -4.1343093 -4.1946206 -4.2316284 -4.2417178 -4.2336493][-4.2534266 -4.2418814 -4.2048197 -4.1639824 -4.1306634 -4.088593 -4.0288377 -3.980617 -4.0053344 -4.0798764 -4.1407585 -4.19292 -4.2285132 -4.2427669 -4.2354589][-4.2562647 -4.2477684 -4.2114697 -4.1702175 -4.1365314 -4.0952353 -4.0430007 -4.0142636 -4.0481582 -4.1078534 -4.1502686 -4.1896563 -4.2236109 -4.2415185 -4.2343874][-4.2562051 -4.2545214 -4.2272959 -4.1921639 -4.15924 -4.1165977 -4.0690937 -4.0553961 -4.0894008 -4.1299152 -4.1523509 -4.181612 -4.2166429 -4.23703 -4.2321682][-4.2461095 -4.2517495 -4.2359037 -4.2078295 -4.1743112 -4.1316791 -4.0932851 -4.091908 -4.1195273 -4.1383529 -4.1415534 -4.1642361 -4.203258 -4.2286186 -4.2287588][-4.2260709 -4.2340965 -4.224905 -4.2014971 -4.1703978 -4.1370234 -4.11796 -4.1293364 -4.1477895 -4.1476274 -4.1369371 -4.1522341 -4.1922626 -4.2227764 -4.228725]]...]
INFO - root - 2017-12-06 00:21:16.814256: step 57610, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 65h:02m:50s remains)
INFO - root - 2017-12-06 00:21:25.470608: step 57620, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 65h:40m:30s remains)
INFO - root - 2017-12-06 00:21:33.958573: step 57630, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 65h:39m:44s remains)
INFO - root - 2017-12-06 00:21:42.466938: step 57640, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 65h:30m:20s remains)
INFO - root - 2017-12-06 00:21:50.917325: step 57650, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 67h:01m:57s remains)
INFO - root - 2017-12-06 00:21:59.587451: step 57660, loss = 2.04, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 67h:25m:06s remains)
INFO - root - 2017-12-06 00:22:08.106222: step 57670, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.821 sec/batch; 62h:42m:31s remains)
INFO - root - 2017-12-06 00:22:16.693262: step 57680, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 65h:10m:34s remains)
INFO - root - 2017-12-06 00:22:25.189016: step 57690, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 64h:48m:40s remains)
INFO - root - 2017-12-06 00:22:33.706514: step 57700, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 64h:47m:07s remains)
2017-12-06 00:22:34.516433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2673674 -4.257874 -4.2440553 -4.2280407 -4.201323 -4.1607637 -4.1121354 -4.0658026 -4.0364747 -4.0398736 -4.0613475 -4.0859809 -4.1067605 -4.1107173 -4.1058245][-4.2599626 -4.2468433 -4.2309022 -4.2115126 -4.1759672 -4.1257167 -4.0662231 -4.0061812 -3.9666104 -3.971308 -4.0020161 -4.0418105 -4.0760841 -4.0828228 -4.072155][-4.2536564 -4.2339516 -4.2150908 -4.1924667 -4.1515236 -4.0951781 -4.0270271 -3.9589784 -3.9189534 -3.9316039 -3.9764204 -4.0318594 -4.0715694 -4.0722318 -4.0522351][-4.2402105 -4.2154317 -4.1986747 -4.1787949 -4.142108 -4.0852675 -4.0127325 -3.9446039 -3.9164283 -3.9476519 -4.0065284 -4.062746 -4.0913453 -4.0836964 -4.0570235][-4.2196121 -4.1926832 -4.1820741 -4.1720324 -4.1457958 -4.0905633 -4.0144382 -3.947897 -3.933455 -3.9788706 -4.0433078 -4.0906692 -4.1055737 -4.0893621 -4.0553288][-4.1919017 -4.1660352 -4.1625366 -4.1629639 -4.1475716 -4.098237 -4.0222797 -3.9562688 -3.9493635 -4.004921 -4.0737367 -4.1108942 -4.1117697 -4.0838737 -4.0430417][-4.1701126 -4.1479483 -4.1499233 -4.1590586 -4.1485515 -4.1004004 -4.0211883 -3.9546158 -3.955035 -4.0267277 -4.10323 -4.1317797 -4.1188326 -4.0778065 -4.0361738][-4.1542993 -4.1431665 -4.1558723 -4.1702242 -4.1614914 -4.1119165 -4.0266585 -3.9586747 -3.9682598 -4.0546851 -4.1342511 -4.1547709 -4.1335826 -4.0866013 -4.0471625][-4.1528125 -4.1562314 -4.1807871 -4.1999941 -4.194211 -4.143774 -4.0563307 -3.9932339 -4.015183 -4.1044335 -4.1769795 -4.1903305 -4.1639667 -4.1128674 -4.0746546][-4.16449 -4.1783938 -4.211009 -4.234808 -4.232542 -4.1869817 -4.1089563 -4.0568228 -4.0841355 -4.1619816 -4.2195168 -4.2260633 -4.1966615 -4.1464896 -4.10863][-4.1889453 -4.2036433 -4.236176 -4.2611527 -4.2648745 -4.2320218 -4.1687908 -4.1277637 -4.1511931 -4.2109742 -4.2547164 -4.2583842 -4.2319961 -4.1875429 -4.1535287][-4.2248354 -4.2362571 -4.2605481 -4.2824116 -4.2904782 -4.269434 -4.2207994 -4.1876469 -4.2043161 -4.2492461 -4.2848277 -4.2896614 -4.2714434 -4.2356024 -4.2060337][-4.2608347 -4.2680707 -4.2828112 -4.3001165 -4.3095355 -4.2970524 -4.2618017 -4.23557 -4.2453804 -4.2797594 -4.308229 -4.3141165 -4.3039036 -4.2782326 -4.2540812][-4.2799211 -4.2884321 -4.2993031 -4.3138189 -4.3245797 -4.3186326 -4.2932582 -4.2699847 -4.27002 -4.2925949 -4.3141918 -4.3205371 -4.315722 -4.2994947 -4.2818956][-4.2776694 -4.289701 -4.3008513 -4.3153844 -4.3274431 -4.3268409 -4.3078904 -4.2838373 -4.272531 -4.2828393 -4.2981582 -4.3050332 -4.3023372 -4.2941637 -4.2846975]]...]
INFO - root - 2017-12-06 00:22:43.048942: step 57710, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.827 sec/batch; 63h:09m:27s remains)
INFO - root - 2017-12-06 00:22:51.459154: step 57720, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:17m:12s remains)
INFO - root - 2017-12-06 00:22:59.988252: step 57730, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.866 sec/batch; 66h:06m:05s remains)
INFO - root - 2017-12-06 00:23:08.418728: step 57740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 65h:28m:02s remains)
INFO - root - 2017-12-06 00:23:16.894341: step 57750, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.808 sec/batch; 61h:40m:25s remains)
INFO - root - 2017-12-06 00:23:25.428469: step 57760, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 64h:02m:16s remains)
INFO - root - 2017-12-06 00:23:33.923941: step 57770, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 65h:00m:56s remains)
INFO - root - 2017-12-06 00:23:42.547063: step 57780, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 65h:47m:00s remains)
INFO - root - 2017-12-06 00:23:51.100138: step 57790, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 63h:30m:47s remains)
INFO - root - 2017-12-06 00:23:59.669128: step 57800, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 65h:44m:18s remains)
2017-12-06 00:24:00.405242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1880116 -4.1716404 -4.1614408 -4.1551647 -4.1532068 -4.1553378 -4.1537123 -4.1559086 -4.1614819 -4.1634889 -4.1629763 -4.1623645 -4.159482 -4.1613855 -4.1637945][-4.1837125 -4.1679115 -4.1568332 -4.1564522 -4.1690087 -4.1824255 -4.1854749 -4.1862683 -4.1874037 -4.1813293 -4.1695142 -4.1613398 -4.156992 -4.157867 -4.1578536][-4.177134 -4.1716666 -4.1692915 -4.1803823 -4.2037554 -4.2191982 -4.2197704 -4.2136221 -4.2082028 -4.1939244 -4.1711349 -4.1559057 -4.1540051 -4.1592774 -4.1590271][-4.1755452 -4.1841164 -4.1923571 -4.2102938 -4.2324409 -4.2387738 -4.231503 -4.218071 -4.2057266 -4.184607 -4.154552 -4.1366715 -4.1370854 -4.1464214 -4.1454272][-4.1844678 -4.2043276 -4.2152462 -4.2268305 -4.2362804 -4.2289643 -4.2118316 -4.1936922 -4.1741371 -4.1460552 -4.1131182 -4.0966287 -4.0979338 -4.1092157 -4.1130605][-4.2057719 -4.2207565 -4.2208281 -4.2151575 -4.2075562 -4.1869364 -4.1623945 -4.1447239 -4.12465 -4.0960164 -4.0655513 -4.0548372 -4.0596857 -4.0728865 -4.082675][-4.2300611 -4.2302437 -4.2125359 -4.18953 -4.1657615 -4.13507 -4.1060071 -4.0931134 -4.0830746 -4.065403 -4.0421333 -4.0392795 -4.050065 -4.06729 -4.0793762][-4.2383585 -4.2243781 -4.1942039 -4.1624823 -4.1318369 -4.0999446 -4.0716367 -4.0618663 -4.0632582 -4.06289 -4.0509295 -4.0548062 -4.0714979 -4.0926862 -4.1055818][-4.2213755 -4.1979165 -4.1641164 -4.1361508 -4.1147661 -4.0926361 -4.073525 -4.064343 -4.0697207 -4.0786452 -4.0718727 -4.0779529 -4.0984831 -4.1219316 -4.1360717][-4.1886296 -4.1630659 -4.129849 -4.1109071 -4.1019068 -4.09239 -4.0873766 -4.0836883 -4.0903168 -4.1007714 -4.0953221 -4.0996232 -4.1183262 -4.1388211 -4.1520386][-4.1618347 -4.1369991 -4.1072907 -4.0965533 -4.0997028 -4.1024671 -4.1071796 -4.11217 -4.1233087 -4.1315036 -4.1259809 -4.1269732 -4.1350608 -4.1451516 -4.1511784][-4.14418 -4.123724 -4.1026893 -4.0998545 -4.1135263 -4.1238713 -4.1271963 -4.1330585 -4.1464505 -4.1570516 -4.1562042 -4.154345 -4.1489544 -4.1465659 -4.1427555][-4.1360512 -4.1210761 -4.1121626 -4.1196318 -4.1391063 -4.1498079 -4.1469231 -4.1486778 -4.16162 -4.1727762 -4.1754394 -4.1731024 -4.1609249 -4.1510348 -4.1420441][-4.138464 -4.1316366 -4.1370287 -4.1530366 -4.1714931 -4.1793814 -4.1717553 -4.170486 -4.1796212 -4.1824565 -4.1781197 -4.170784 -4.1573257 -4.1461496 -4.1403871][-4.1562395 -4.1591716 -4.1748047 -4.1937447 -4.2072124 -4.2122908 -4.203414 -4.1983466 -4.1987576 -4.18886 -4.1734128 -4.1586776 -4.1444669 -4.1375856 -4.1405144]]...]
INFO - root - 2017-12-06 00:24:08.923552: step 57810, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 65h:17m:35s remains)
INFO - root - 2017-12-06 00:24:17.456970: step 57820, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 64h:21m:57s remains)
INFO - root - 2017-12-06 00:24:25.799711: step 57830, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.842 sec/batch; 64h:16m:22s remains)
INFO - root - 2017-12-06 00:24:34.361120: step 57840, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 65h:34m:10s remains)
INFO - root - 2017-12-06 00:24:42.933313: step 57850, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 64h:16m:47s remains)
INFO - root - 2017-12-06 00:24:51.323406: step 57860, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.860 sec/batch; 65h:38m:30s remains)
INFO - root - 2017-12-06 00:24:59.975075: step 57870, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 65h:12m:24s remains)
INFO - root - 2017-12-06 00:25:08.593118: step 57880, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 65h:26m:07s remains)
INFO - root - 2017-12-06 00:25:17.139969: step 57890, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 65h:04m:35s remains)
INFO - root - 2017-12-06 00:25:25.642238: step 57900, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 65h:25m:34s remains)
2017-12-06 00:25:26.413809: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1754642 -4.1784258 -4.1797071 -4.1937118 -4.2098832 -4.2120605 -4.2027979 -4.1923976 -4.1844559 -4.1857166 -4.200973 -4.2214584 -4.2341871 -4.2352338 -4.2275562][-4.1564207 -4.1672039 -4.1693134 -4.178812 -4.1888022 -4.1838841 -4.1686478 -4.153285 -4.1470237 -4.1574616 -4.1807489 -4.2059779 -4.2227244 -4.2233238 -4.2113662][-4.1424847 -4.1600151 -4.1593132 -4.1566167 -4.1540084 -4.134397 -4.0990095 -4.0706015 -4.0716968 -4.1039128 -4.1446242 -4.179595 -4.2055793 -4.2086339 -4.1908917][-4.140069 -4.1613679 -4.1569376 -4.1413622 -4.1148648 -4.0637641 -3.9959831 -3.9510586 -3.9750767 -4.0482979 -4.1110511 -4.1577973 -4.188457 -4.18742 -4.1604581][-4.1490464 -4.1666813 -4.1565762 -4.1282954 -4.077692 -3.995775 -3.8949833 -3.8440104 -3.9034476 -4.0197263 -4.103766 -4.1520157 -4.1762061 -4.1656437 -4.1311145][-4.1530461 -4.1651707 -4.1527157 -4.1208954 -4.0607738 -3.9660368 -3.8534839 -3.8155122 -3.8983731 -4.0293164 -4.1203318 -4.1565561 -4.1652069 -4.1443157 -4.1013923][-4.1508861 -4.1610394 -4.1517396 -4.1250558 -4.0706596 -3.9827209 -3.8817933 -3.8616695 -3.9486043 -4.0715508 -4.1561794 -4.174047 -4.1596246 -4.1267872 -4.0781517][-4.1409311 -4.1551476 -4.1592188 -4.1435995 -4.1027679 -4.0329037 -3.9539676 -3.9436486 -4.0190206 -4.1210074 -4.187077 -4.1841288 -4.1526647 -4.116179 -4.0689487][-4.1153021 -4.1371117 -4.1523523 -4.1456709 -4.1152139 -4.0684285 -4.0282173 -4.0349665 -4.097044 -4.1718426 -4.211987 -4.18864 -4.1460791 -4.1102595 -4.0702238][-4.0886812 -4.1145468 -4.13729 -4.137146 -4.11713 -4.0946059 -4.0905304 -4.1118584 -4.1617627 -4.2086086 -4.2213426 -4.1871085 -4.1467295 -4.1177979 -4.0891747][-4.0924363 -4.1162653 -4.14342 -4.1464143 -4.1317482 -4.1237059 -4.1368937 -4.1638012 -4.2009072 -4.2262983 -4.2206693 -4.188632 -4.1586189 -4.1392717 -4.1225471][-4.1151228 -4.1347094 -4.1585059 -4.1593833 -4.149354 -4.1521158 -4.1708674 -4.1971345 -4.22221 -4.2319493 -4.2217827 -4.2017984 -4.1844168 -4.1745992 -4.1660671][-4.1635313 -4.177815 -4.1912313 -4.1870952 -4.1772323 -4.1869826 -4.2091246 -4.2296953 -4.2413955 -4.2406297 -4.2295165 -4.2203722 -4.215354 -4.2157726 -4.2134371][-4.2201014 -4.2278109 -4.2301183 -4.2227988 -4.2152538 -4.2281141 -4.2502308 -4.2653527 -4.2653065 -4.2591472 -4.2508235 -4.247333 -4.2461982 -4.2480268 -4.2469726][-4.2639146 -4.266449 -4.2658 -4.2592645 -4.2547569 -4.2676239 -4.28664 -4.2976418 -4.2923489 -4.2838593 -4.2774415 -4.2767034 -4.2757359 -4.2762914 -4.2763581]]...]
INFO - root - 2017-12-06 00:25:35.037999: step 57910, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.867 sec/batch; 66h:06m:30s remains)
INFO - root - 2017-12-06 00:25:43.645173: step 57920, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.881 sec/batch; 67h:13m:32s remains)
INFO - root - 2017-12-06 00:25:52.191640: step 57930, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 64h:56m:19s remains)
INFO - root - 2017-12-06 00:26:00.640991: step 57940, loss = 2.06, batch loss = 2.00 (8.9 examples/sec; 0.897 sec/batch; 68h:25m:37s remains)
INFO - root - 2017-12-06 00:26:09.246920: step 57950, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 64h:41m:05s remains)
INFO - root - 2017-12-06 00:26:17.812760: step 57960, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.886 sec/batch; 67h:36m:16s remains)
INFO - root - 2017-12-06 00:26:26.289050: step 57970, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 65h:05m:16s remains)
INFO - root - 2017-12-06 00:26:34.795808: step 57980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 65h:46m:58s remains)
INFO - root - 2017-12-06 00:26:43.426249: step 57990, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.860 sec/batch; 65h:36m:21s remains)
INFO - root - 2017-12-06 00:26:51.935317: step 58000, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 64h:50m:28s remains)
2017-12-06 00:26:52.717411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1857672 -4.1465006 -4.11219 -4.098259 -4.1215687 -4.1656914 -4.2032967 -4.22794 -4.2388721 -4.2407055 -4.2335358 -4.2169032 -4.1999841 -4.1771269 -4.145988][-4.179111 -4.1345897 -4.0961723 -4.0813613 -4.1153278 -4.1722736 -4.2133255 -4.2293277 -4.2316589 -4.2290277 -4.2224426 -4.2109628 -4.2014704 -4.1851668 -4.1577621][-4.1427493 -4.1118846 -4.0818639 -4.0705795 -4.105298 -4.1567688 -4.1900945 -4.1965008 -4.1937046 -4.1918521 -4.1930261 -4.197135 -4.1996808 -4.192174 -4.1778884][-4.1039405 -4.0949306 -4.0797534 -4.0743346 -4.0985951 -4.1271005 -4.1388922 -4.1366167 -4.1304164 -4.1347809 -4.15073 -4.1689005 -4.184444 -4.1927094 -4.1949172][-4.0785923 -4.0882649 -4.0886879 -4.0876889 -4.0967951 -4.0919552 -4.0737109 -4.0518966 -4.0374317 -4.0547881 -4.0907788 -4.1291971 -4.165627 -4.189971 -4.2017856][-4.0772243 -4.094893 -4.1042895 -4.100162 -4.0844679 -4.0412393 -3.9807894 -3.9283147 -3.9118509 -3.9570425 -4.0261216 -4.0949039 -4.1573896 -4.1940584 -4.2055984][-4.0878592 -4.1049376 -4.1122289 -4.0946555 -4.0515695 -3.9733083 -3.876703 -3.7975335 -3.7966447 -3.8828719 -3.9881988 -4.0824909 -4.159173 -4.1987581 -4.206255][-4.0952792 -4.1024828 -4.0982966 -4.0682783 -4.0121512 -3.9333165 -3.8549063 -3.8004739 -3.821182 -3.9071505 -4.0062776 -4.0982037 -4.1704817 -4.2068152 -4.2109427][-4.0925984 -4.0869021 -4.0692883 -4.0342884 -3.9795246 -3.9275994 -3.9068661 -3.9072084 -3.9342299 -3.9851997 -4.0531521 -4.1256361 -4.1852593 -4.2176523 -4.2207751][-4.0899353 -4.0668855 -4.0337987 -3.9932 -3.9468083 -3.9302206 -3.9518018 -3.976047 -3.9949825 -4.0158019 -4.0555305 -4.1143894 -4.1699886 -4.2035775 -4.2078629][-4.1057963 -4.0722322 -4.0302982 -3.9865515 -3.9492657 -3.9514742 -3.9774804 -3.9960153 -4.0059476 -4.0001554 -4.0133419 -4.0653458 -4.1275597 -4.1664882 -4.1703625][-4.1384521 -4.102334 -4.0646439 -4.0257525 -3.9912014 -3.9888008 -4.0005574 -4.00608 -4.0060816 -3.9786398 -3.9644876 -4.0103559 -4.0828028 -4.1255674 -4.1272874][-4.1585 -4.1211658 -4.0909433 -4.0598364 -4.0288515 -4.0193377 -4.0255742 -4.033196 -4.0285692 -3.9872849 -3.9573894 -3.9903955 -4.0499845 -4.0877075 -4.0994821][-4.1502695 -4.1088719 -4.08153 -4.0599642 -4.0324974 -4.0207295 -4.0334654 -4.0513282 -4.0455093 -4.0090127 -3.9846373 -4.0063305 -4.0428205 -4.0773134 -4.1081228][-4.1161571 -4.0615358 -4.0332708 -4.0217085 -4.0050774 -4.0039468 -4.0305037 -4.0576773 -4.0545707 -4.032732 -4.0294552 -4.0503578 -4.076251 -4.1109381 -4.1501427]]...]
INFO - root - 2017-12-06 00:27:01.189536: step 58010, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 65h:27m:59s remains)
INFO - root - 2017-12-06 00:27:09.755293: step 58020, loss = 2.09, batch loss = 2.04 (9.7 examples/sec; 0.822 sec/batch; 62h:42m:34s remains)
INFO - root - 2017-12-06 00:27:18.280233: step 58030, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 64h:31m:50s remains)
INFO - root - 2017-12-06 00:27:26.734673: step 58040, loss = 2.03, batch loss = 1.97 (9.1 examples/sec; 0.883 sec/batch; 67h:17m:04s remains)
INFO - root - 2017-12-06 00:27:35.152503: step 58050, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.888 sec/batch; 67h:42m:58s remains)
INFO - root - 2017-12-06 00:27:43.649046: step 58060, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 64h:47m:41s remains)
INFO - root - 2017-12-06 00:27:52.155615: step 58070, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 65h:59m:55s remains)
INFO - root - 2017-12-06 00:28:00.662260: step 58080, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 64h:24m:29s remains)
INFO - root - 2017-12-06 00:28:09.105659: step 58090, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.840 sec/batch; 64h:03m:45s remains)
INFO - root - 2017-12-06 00:28:17.742558: step 58100, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 65h:12m:12s remains)
2017-12-06 00:28:18.509615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2289195 -4.2686229 -4.280623 -4.278851 -4.2537918 -4.2009816 -4.1392064 -4.0930057 -4.1010308 -4.1486306 -4.1794095 -4.1786757 -4.1467724 -4.0944533 -4.0537043][-4.1757193 -4.237504 -4.2639394 -4.2648988 -4.2347221 -4.1780558 -4.1126938 -4.0646276 -4.068471 -4.1183348 -4.15913 -4.1620374 -4.1188669 -4.0495162 -3.9990613][-4.1233735 -4.205647 -4.2445369 -4.2477283 -4.2123137 -4.1585064 -4.1017661 -4.061718 -4.0611696 -4.1070561 -4.1566858 -4.1642566 -4.1130695 -4.0296206 -3.9700277][-4.08305 -4.16929 -4.2163963 -4.2198877 -4.1812682 -4.1303968 -4.0888696 -4.060462 -4.0587783 -4.1011639 -4.1560726 -4.1639547 -4.1040964 -4.0114489 -3.9474616][-4.0509892 -4.1250811 -4.1724215 -4.1768627 -4.1403093 -4.0985389 -4.074614 -4.0595369 -4.0633688 -4.1034741 -4.1581087 -4.1623874 -4.0964432 -4.0012479 -3.9449325][-4.0261326 -4.0849652 -4.13292 -4.1397448 -4.1068406 -4.0744538 -4.0627384 -4.0583968 -4.0763531 -4.1211276 -4.1718616 -4.1706161 -4.1019235 -4.012001 -3.9697897][-4.0119438 -4.052084 -4.0946569 -4.1010237 -4.07337 -4.0466561 -4.037941 -4.0388427 -4.0693154 -4.1202674 -4.1672096 -4.1618214 -4.0894279 -4.0099072 -3.987366][-3.9990833 -4.0243587 -4.0614777 -4.0683355 -4.0460467 -4.0204296 -4.0010805 -3.9926245 -4.02467 -4.0786848 -4.1270566 -4.1212854 -4.0519538 -3.9858313 -3.9835722][-3.9973328 -4.0099 -4.0366125 -4.0427933 -4.0289621 -4.0101089 -3.9832098 -3.9676249 -3.993767 -4.0427756 -4.0823822 -4.0752125 -4.016046 -3.9640424 -3.9744837][-4.037015 -4.0433373 -4.0586586 -4.0634317 -4.058815 -4.04482 -4.0161996 -3.9962032 -4.0136504 -4.0576692 -4.0886359 -4.0767074 -4.0278711 -3.9903369 -4.003252][-4.0866489 -4.09017 -4.0987082 -4.1070929 -4.1114407 -4.1050673 -4.0810981 -4.0631175 -4.0765214 -4.1084208 -4.127346 -4.114953 -4.0793929 -4.0563178 -4.0679264][-4.1349192 -4.134635 -4.1375909 -4.146441 -4.1540465 -4.1561451 -4.1446056 -4.1386871 -4.1570344 -4.1793232 -4.1871037 -4.1717582 -4.1446266 -4.1305976 -4.1389523][-4.1857982 -4.188808 -4.1872125 -4.1887522 -4.1908407 -4.1943359 -4.191607 -4.1944876 -4.2164726 -4.2380295 -4.2429252 -4.2292123 -4.2075596 -4.1968808 -4.2004232][-4.2316585 -4.2355943 -4.235393 -4.235218 -4.2349777 -4.23777 -4.2364531 -4.2407722 -4.2581611 -4.2768679 -4.2854242 -4.2782936 -4.2638083 -4.2541637 -4.2530103][-4.2759018 -4.278358 -4.2783422 -4.2787156 -4.2789993 -4.2817307 -4.2798986 -4.280777 -4.2893658 -4.3013768 -4.310822 -4.3100858 -4.3007097 -4.29111 -4.2880206]]...]
INFO - root - 2017-12-06 00:28:27.094852: step 58110, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 66h:07m:33s remains)
INFO - root - 2017-12-06 00:28:35.566730: step 58120, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:27m:05s remains)
INFO - root - 2017-12-06 00:28:44.040537: step 58130, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 65h:35m:36s remains)
INFO - root - 2017-12-06 00:28:52.421934: step 58140, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 64h:15m:37s remains)
INFO - root - 2017-12-06 00:29:01.144160: step 58150, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 64h:39m:33s remains)
INFO - root - 2017-12-06 00:29:09.601233: step 58160, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.808 sec/batch; 61h:36m:05s remains)
INFO - root - 2017-12-06 00:29:18.251729: step 58170, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 65h:28m:02s remains)
INFO - root - 2017-12-06 00:29:26.661681: step 58180, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 64h:46m:58s remains)
INFO - root - 2017-12-06 00:29:35.340287: step 58190, loss = 2.04, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 63h:55m:14s remains)
INFO - root - 2017-12-06 00:29:43.926783: step 58200, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 65h:46m:35s remains)
2017-12-06 00:29:44.709983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2974062 -4.2837782 -4.273212 -4.2643027 -4.2575502 -4.2547312 -4.2594395 -4.2695804 -4.274498 -4.27643 -4.2856455 -4.2956095 -4.2995305 -4.2938051 -4.2881603][-4.2867541 -4.2667861 -4.2534533 -4.2447395 -4.2396684 -4.2332339 -4.2313228 -4.24178 -4.2498131 -4.25434 -4.26746 -4.2828383 -4.2903676 -4.2820024 -4.2695436][-4.2801952 -4.2575569 -4.2418647 -4.231637 -4.2254276 -4.2122169 -4.2008119 -4.2067242 -4.2140546 -4.2191014 -4.2396927 -4.2643652 -4.2775922 -4.2715631 -4.2468328][-4.2752705 -4.2519994 -4.235302 -4.2215247 -4.2123675 -4.1968408 -4.1826887 -4.1834955 -4.1835737 -4.1833978 -4.20617 -4.2372308 -4.2563133 -4.2542815 -4.2184639][-4.2655392 -4.2415357 -4.2258191 -4.2103972 -4.2006989 -4.1891727 -4.1830597 -4.1841578 -4.1767693 -4.1684942 -4.1854787 -4.2170482 -4.2403259 -4.2413321 -4.2052364][-4.2502565 -4.2195654 -4.1995296 -4.1855726 -4.1787353 -4.172492 -4.1721411 -4.17447 -4.1701961 -4.1652985 -4.1791759 -4.2032404 -4.226181 -4.233088 -4.2105765][-4.23329 -4.1912041 -4.1580386 -4.1400003 -4.1375632 -4.1372719 -4.1345716 -4.1339765 -4.138957 -4.1466255 -4.1665397 -4.1883907 -4.2106457 -4.2239347 -4.2214251][-4.2279305 -4.1774068 -4.1249309 -4.090198 -4.0898671 -4.1022282 -4.1067572 -4.1067834 -4.1191764 -4.1341748 -4.1544876 -4.1761589 -4.200747 -4.2253637 -4.2425647][-4.2319517 -4.189352 -4.1263914 -4.0710025 -4.0573516 -4.0709863 -4.0790677 -4.0824 -4.1007133 -4.11909 -4.14 -4.1604972 -4.1881356 -4.2257361 -4.2571487][-4.2381725 -4.2152348 -4.1672306 -4.1098833 -4.0809488 -4.0757823 -4.0746346 -4.0754895 -4.0952888 -4.1173744 -4.140213 -4.1563082 -4.1794777 -4.2209411 -4.2619643][-4.2391272 -4.2379303 -4.218 -4.1793027 -4.1466479 -4.1219292 -4.1048031 -4.0985012 -4.1134472 -4.1337495 -4.1555047 -4.1668921 -4.1834717 -4.221982 -4.2673588][-4.2378669 -4.2528772 -4.2584553 -4.241539 -4.2161312 -4.1884718 -4.1648011 -4.1531615 -4.1568127 -4.167347 -4.1842465 -4.1925311 -4.20282 -4.2331204 -4.2724991][-4.2454777 -4.2648182 -4.2824 -4.2797337 -4.2693915 -4.2533216 -4.2327147 -4.2200637 -4.2149277 -4.2137876 -4.222085 -4.2259574 -4.2295103 -4.2511868 -4.2800732][-4.2624068 -4.2789426 -4.2952213 -4.2981863 -4.2997313 -4.2964606 -4.2861295 -4.2779274 -4.2726893 -4.2662578 -4.263658 -4.2602186 -4.2593369 -4.2740107 -4.2933807][-4.28869 -4.3002462 -4.3103023 -4.31279 -4.3171368 -4.3206072 -4.3194356 -4.31642 -4.3139849 -4.3100295 -4.303556 -4.2946897 -4.2912078 -4.2993569 -4.3098145]]...]
INFO - root - 2017-12-06 00:29:53.192614: step 58210, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.851 sec/batch; 64h:49m:42s remains)
INFO - root - 2017-12-06 00:30:01.743381: step 58220, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 67h:34m:38s remains)
INFO - root - 2017-12-06 00:30:10.303384: step 58230, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 66h:54m:40s remains)
INFO - root - 2017-12-06 00:30:18.709253: step 58240, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 65h:33m:42s remains)
INFO - root - 2017-12-06 00:30:27.184492: step 58250, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 66h:04m:50s remains)
INFO - root - 2017-12-06 00:30:35.692800: step 58260, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 64h:04m:57s remains)
INFO - root - 2017-12-06 00:30:44.119905: step 58270, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 62h:31m:01s remains)
INFO - root - 2017-12-06 00:30:52.745720: step 58280, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.852 sec/batch; 64h:55m:16s remains)
INFO - root - 2017-12-06 00:31:01.271864: step 58290, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 64h:26m:27s remains)
INFO - root - 2017-12-06 00:31:09.886485: step 58300, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.845 sec/batch; 64h:21m:23s remains)
2017-12-06 00:31:10.745958: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2074862 -4.2150187 -4.2209072 -4.2172956 -4.2050114 -4.1993794 -4.2219176 -4.2473679 -4.2524652 -4.2437153 -4.2330208 -4.229444 -4.2357965 -4.2507181 -4.2646704][-4.196754 -4.2001987 -4.2072511 -4.2060513 -4.1941223 -4.19019 -4.2146611 -4.2395968 -4.2456417 -4.2366738 -4.2279186 -4.2260013 -4.2331805 -4.249671 -4.2622418][-4.1870475 -4.188199 -4.1968074 -4.197979 -4.1855412 -4.1780882 -4.1967874 -4.2167444 -4.2243457 -4.2162423 -4.2117167 -4.212853 -4.2201967 -4.2384796 -4.2533517][-4.1797595 -4.1794782 -4.18842 -4.1884942 -4.1712217 -4.1566749 -4.1686211 -4.1864414 -4.1947308 -4.1861548 -4.1838822 -4.1910748 -4.2027178 -4.224556 -4.2439928][-4.1754484 -4.1746836 -4.176455 -4.1662927 -4.1374388 -4.1152515 -4.1198578 -4.13256 -4.1425433 -4.1392837 -4.1439719 -4.1620588 -4.18214 -4.2095675 -4.2346311][-4.1777568 -4.1703076 -4.1583447 -4.1356106 -4.0944147 -4.0610037 -4.0542583 -4.0576577 -4.0687909 -4.0785069 -4.0992913 -4.1328855 -4.1627946 -4.1985559 -4.2289872][-4.1756229 -4.154367 -4.1290131 -4.098413 -4.0515571 -4.01088 -3.9942079 -3.9897773 -4.0039072 -4.0318594 -4.0720744 -4.1181602 -4.1525359 -4.1929469 -4.2260509][-4.1726213 -4.1405196 -4.1057158 -4.0743675 -4.0263586 -3.9867208 -3.9810045 -3.9884791 -4.007741 -4.0428753 -4.0886679 -4.1298666 -4.1570096 -4.19619 -4.2285409][-4.1854606 -4.1497688 -4.1148019 -4.0860796 -4.046061 -4.0219131 -4.0356736 -4.0558443 -4.0724645 -4.096096 -4.1299696 -4.160707 -4.1836205 -4.2185092 -4.2466125][-4.2147651 -4.1818361 -4.1541376 -4.1329961 -4.1044989 -4.0941005 -4.11677 -4.1357937 -4.1426086 -4.1535134 -4.1741176 -4.1976233 -4.2186031 -4.24811 -4.2705793][-4.2498198 -4.2284908 -4.2102251 -4.1982117 -4.1812925 -4.1774955 -4.1922021 -4.1994948 -4.1956263 -4.1973634 -4.2086225 -4.2268395 -4.2471046 -4.2734151 -4.2923141][-4.275507 -4.263741 -4.2524948 -4.2441134 -4.2361689 -4.2360859 -4.2409096 -4.2377963 -4.2302623 -4.2330904 -4.2420564 -4.2558422 -4.2720919 -4.2913857 -4.3057022][-4.2967458 -4.2898221 -4.2838454 -4.2793927 -4.2769742 -4.2803593 -4.2830825 -4.2780676 -4.2723031 -4.2762036 -4.2838855 -4.2920732 -4.300601 -4.3112454 -4.3201218][-4.3147097 -4.3098683 -4.3078003 -4.3078847 -4.3104815 -4.3159652 -4.3195 -4.3161592 -4.3125839 -4.315412 -4.3186622 -4.3217387 -4.3250809 -4.3294425 -4.3335376][-4.3267493 -4.3239207 -4.3224683 -4.3233166 -4.3255835 -4.3289275 -4.3325195 -4.3320117 -4.3301597 -4.3305039 -4.3315325 -4.3322883 -4.3337879 -4.335947 -4.338326]]...]
INFO - root - 2017-12-06 00:31:19.387147: step 58310, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.886 sec/batch; 67h:29m:29s remains)
INFO - root - 2017-12-06 00:31:27.992345: step 58320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 63h:55m:48s remains)
INFO - root - 2017-12-06 00:31:36.571118: step 58330, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 66h:34m:17s remains)
INFO - root - 2017-12-06 00:31:44.973448: step 58340, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 65h:07m:13s remains)
INFO - root - 2017-12-06 00:31:53.472044: step 58350, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 63h:59m:26s remains)
INFO - root - 2017-12-06 00:32:02.144736: step 58360, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 65h:19m:52s remains)
INFO - root - 2017-12-06 00:32:10.782458: step 58370, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 65h:35m:50s remains)
INFO - root - 2017-12-06 00:32:19.247635: step 58380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 64h:53m:51s remains)
INFO - root - 2017-12-06 00:32:27.733653: step 58390, loss = 2.07, batch loss = 2.01 (10.5 examples/sec; 0.762 sec/batch; 58h:03m:25s remains)
INFO - root - 2017-12-06 00:32:36.201269: step 58400, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 65h:06m:37s remains)
2017-12-06 00:32:37.016784: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1597576 -4.1766458 -4.1937189 -4.2019596 -4.1938338 -4.177238 -4.1837211 -4.2093616 -4.2227902 -4.2245245 -4.2136426 -4.1956964 -4.1773887 -4.1615887 -4.1644545][-4.1342597 -4.1598706 -4.1801286 -4.1852937 -4.1771932 -4.1602545 -4.1595244 -4.1824317 -4.19563 -4.19628 -4.185246 -4.1680069 -4.1495972 -4.1316323 -4.1357155][-4.1146154 -4.1416087 -4.1629 -4.167315 -4.16088 -4.1442685 -4.1361427 -4.1528029 -4.1658387 -4.1672564 -4.1602983 -4.14662 -4.1297956 -4.1130843 -4.1177182][-4.1246653 -4.1456151 -4.16034 -4.159153 -4.1488066 -4.1352053 -4.1304541 -4.1465926 -4.1612067 -4.16554 -4.1625323 -4.1521177 -4.1397252 -4.1317244 -4.1414404][-4.1451654 -4.1569295 -4.1608529 -4.1444464 -4.1177144 -4.1037588 -4.111722 -4.136498 -4.1573353 -4.1667776 -4.16836 -4.1622238 -4.1566272 -4.1585035 -4.1725631][-4.1379523 -4.1460338 -4.1449065 -4.11819 -4.0698419 -4.0443416 -4.0628033 -4.1006646 -4.1278906 -4.1362739 -4.1363583 -4.1328874 -4.1347623 -4.1453218 -4.1652794][-4.1084695 -4.1268945 -4.13529 -4.1087117 -4.0502286 -4.013495 -4.0287547 -4.0694609 -4.0994344 -4.1081219 -4.1041136 -4.0960822 -4.0997162 -4.113853 -4.1400814][-4.0835605 -4.1187935 -4.1390657 -4.1223083 -4.0736995 -4.0398707 -4.0439625 -4.0667973 -4.0866528 -4.0967402 -4.0903816 -4.0738087 -4.0725746 -4.0895443 -4.1190352][-4.082521 -4.12264 -4.1416206 -4.1272779 -4.0901427 -4.0637212 -4.0610218 -4.0713058 -4.0864568 -4.1014404 -4.0961342 -4.0748162 -4.0694818 -4.090631 -4.1218762][-4.0883732 -4.123507 -4.136271 -4.12323 -4.0965662 -4.0780106 -4.0733776 -4.0789623 -4.0900145 -4.102078 -4.0963492 -4.0743985 -4.0693569 -4.0933943 -4.1257253][-4.1148539 -4.1390429 -4.1459026 -4.1377277 -4.1251645 -4.1179361 -4.1160588 -4.1183138 -4.1241245 -4.1288171 -4.1205173 -4.0990133 -4.0923786 -4.1138229 -4.1426029][-4.1525316 -4.1649709 -4.1665449 -4.1619506 -4.1547084 -4.1511703 -4.1502657 -4.15135 -4.15549 -4.15988 -4.1555486 -4.1399837 -4.133677 -4.149219 -4.1716118][-4.1925626 -4.1965122 -4.1960425 -4.1923184 -4.1866713 -4.1835628 -4.1826129 -4.1820235 -4.1832142 -4.1867723 -4.187736 -4.181036 -4.1793942 -4.1899977 -4.2068887][-4.2333632 -4.2356386 -4.2352 -4.2334394 -4.2301331 -4.2270641 -4.2242618 -4.2212582 -4.2190633 -4.2205296 -4.2237978 -4.2249084 -4.2263174 -4.2308784 -4.2391572][-4.2577295 -4.2592549 -4.2590923 -4.2585926 -4.2568779 -4.2545552 -4.2518439 -4.2489367 -4.2471118 -4.2483993 -4.2524652 -4.2559042 -4.2566385 -4.2553167 -4.2551637]]...]
INFO - root - 2017-12-06 00:32:45.532684: step 58410, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.822 sec/batch; 62h:35m:58s remains)
INFO - root - 2017-12-06 00:32:54.043815: step 58420, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 65h:11m:31s remains)
INFO - root - 2017-12-06 00:33:02.572536: step 58430, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 63h:58m:35s remains)
INFO - root - 2017-12-06 00:33:10.992860: step 58440, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.822 sec/batch; 62h:33m:44s remains)
INFO - root - 2017-12-06 00:33:19.488588: step 58450, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:25m:07s remains)
INFO - root - 2017-12-06 00:33:28.048895: step 58460, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 63h:22m:33s remains)
INFO - root - 2017-12-06 00:33:36.711227: step 58470, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 66h:33m:54s remains)
INFO - root - 2017-12-06 00:33:45.235800: step 58480, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 65h:35m:22s remains)
INFO - root - 2017-12-06 00:33:53.869166: step 58490, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 65h:15m:46s remains)
INFO - root - 2017-12-06 00:34:02.366713: step 58500, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 67h:06m:14s remains)
2017-12-06 00:34:03.165268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1726885 -4.1933327 -4.2171917 -4.2126613 -4.1786165 -4.1397429 -4.1084161 -4.0843778 -4.0703216 -4.1018596 -4.1739554 -4.2408829 -4.2895136 -4.3125734 -4.3177743][-4.1684608 -4.1952929 -4.2231822 -4.2221236 -4.1867414 -4.1335058 -4.0882831 -4.0649366 -4.0540471 -4.0879197 -4.1611629 -4.2296805 -4.2841973 -4.3119512 -4.3151927][-4.1671162 -4.1969628 -4.2220516 -4.2201328 -4.1784773 -4.1099067 -4.0539851 -4.0377088 -4.0385389 -4.078547 -4.153564 -4.224194 -4.2816591 -4.3118138 -4.3152571][-4.1767035 -4.1996779 -4.2177172 -4.2094989 -4.1554809 -4.0742459 -4.0139446 -4.0053854 -4.0230517 -4.0759997 -4.1556234 -4.2246776 -4.2807837 -4.3102484 -4.3161039][-4.197495 -4.2111073 -4.2182117 -4.1974463 -4.12625 -4.0305176 -3.9627895 -3.9560382 -3.9955063 -4.0706215 -4.1557984 -4.2245 -4.27836 -4.3086925 -4.3171492][-4.2282138 -4.2314854 -4.2249866 -4.1894407 -4.1010289 -3.9932508 -3.9136047 -3.9013596 -3.9631453 -4.0590291 -4.1500378 -4.2233214 -4.2782378 -4.3098493 -4.3192406][-4.2575326 -4.2525115 -4.2360258 -4.1891074 -4.0911045 -3.9790809 -3.8917179 -3.8745365 -3.9455934 -4.0464664 -4.1428232 -4.2256036 -4.2817936 -4.3122373 -4.3212209][-4.2803593 -4.2700319 -4.2483563 -4.1954927 -4.0947752 -3.9864349 -3.9044416 -3.8874032 -3.9448798 -4.037725 -4.13504 -4.2265248 -4.2853951 -4.314981 -4.3228579][-4.2946205 -4.2835922 -4.2623711 -4.2096872 -4.1130939 -4.0114288 -3.939081 -3.9243958 -3.9572673 -4.0351253 -4.1298962 -4.2234793 -4.2846832 -4.3143287 -4.3230138][-4.2987423 -4.2912331 -4.2752924 -4.226624 -4.1359615 -4.0442204 -3.9826438 -3.9656923 -3.977562 -4.0378718 -4.1258278 -4.217844 -4.2798638 -4.3097434 -4.3204565][-4.2954822 -4.2908378 -4.2803273 -4.23903 -4.1572089 -4.0773044 -4.0236597 -3.9992433 -3.9935143 -4.0381579 -4.1175604 -4.2086353 -4.2728419 -4.3036356 -4.316834][-4.2943912 -4.2902865 -4.2827597 -4.2497625 -4.179533 -4.1118431 -4.0612597 -4.0253639 -4.0043464 -4.0363941 -4.1080208 -4.1993661 -4.2673845 -4.3017259 -4.3181758][-4.296072 -4.2913308 -4.2864542 -4.2610397 -4.202868 -4.1454124 -4.0954103 -4.0459661 -4.0105481 -4.0324821 -4.0988069 -4.19017 -4.2631636 -4.3030372 -4.3228092][-4.2975063 -4.2915664 -4.2876525 -4.2683692 -4.2233853 -4.1764507 -4.1271954 -4.0651774 -4.0178838 -4.0313458 -4.0938854 -4.1827993 -4.2583466 -4.3041153 -4.3255596][-4.2978382 -4.2913747 -4.2879729 -4.2747293 -4.2422643 -4.2061982 -4.1607018 -4.0896435 -4.0325561 -4.0345416 -4.0885763 -4.1713867 -4.2469378 -4.2960682 -4.3198509]]...]
INFO - root - 2017-12-06 00:34:11.698529: step 58510, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 65h:25m:12s remains)
INFO - root - 2017-12-06 00:34:20.238888: step 58520, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.840 sec/batch; 63h:56m:34s remains)
INFO - root - 2017-12-06 00:34:28.776923: step 58530, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.843 sec/batch; 64h:10m:18s remains)
INFO - root - 2017-12-06 00:34:37.188117: step 58540, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.836 sec/batch; 63h:36m:06s remains)
INFO - root - 2017-12-06 00:34:45.725777: step 58550, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 65h:23m:32s remains)
INFO - root - 2017-12-06 00:34:54.359247: step 58560, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.825 sec/batch; 62h:47m:22s remains)
INFO - root - 2017-12-06 00:35:02.955101: step 58570, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 64h:04m:51s remains)
INFO - root - 2017-12-06 00:35:11.576423: step 58580, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 66h:10m:12s remains)
INFO - root - 2017-12-06 00:35:20.116238: step 58590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 65h:03m:29s remains)
INFO - root - 2017-12-06 00:35:28.507471: step 58600, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 65h:27m:04s remains)
2017-12-06 00:35:29.289062: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1827269 -4.2244692 -4.2536578 -4.2674718 -4.270299 -4.2698817 -4.2720408 -4.278862 -4.2871909 -4.2938461 -4.2970304 -4.29832 -4.2994041 -4.3000507 -4.2991624][-4.1787276 -4.2164559 -4.2405424 -4.2485905 -4.2470407 -4.2457795 -4.2508383 -4.2629819 -4.2764459 -4.2864733 -4.2918367 -4.2946529 -4.2968183 -4.2984405 -4.2979307][-4.1926179 -4.2195163 -4.230608 -4.2262459 -4.2160668 -4.2127981 -4.2212906 -4.2394533 -4.2596474 -4.2747831 -4.2838192 -4.2896748 -4.2939129 -4.296906 -4.29682][-4.2195606 -4.2312503 -4.2236581 -4.2015214 -4.1789417 -4.1716337 -4.1824088 -4.2054863 -4.2326488 -4.254396 -4.26904 -4.2805076 -4.2897153 -4.2958374 -4.2970581][-4.2438531 -4.23994 -4.2140722 -4.1742854 -4.1400661 -4.1280713 -4.138166 -4.1623425 -4.1931615 -4.2203794 -4.2410426 -4.2601647 -4.2778149 -4.2905831 -4.2951512][-4.2566509 -4.240243 -4.2011161 -4.1493907 -4.1075535 -4.0913258 -4.0984688 -4.1185932 -4.1470656 -4.17601 -4.2014117 -4.2279239 -4.2556844 -4.2780395 -4.2884469][-4.264596 -4.2426152 -4.1997552 -4.145607 -4.10173 -4.0832009 -4.0863137 -4.0975623 -4.1153054 -4.1385655 -4.1640658 -4.1943979 -4.230423 -4.2630367 -4.2812009][-4.2733831 -4.2535834 -4.2164383 -4.1692352 -4.1302533 -4.1119475 -4.1110587 -4.1123934 -4.1165028 -4.1299644 -4.1503348 -4.1787052 -4.2171206 -4.2563424 -4.2810178][-4.28685 -4.2739539 -4.2476292 -4.2130265 -4.1842847 -4.1703153 -4.1668062 -4.1595449 -4.1512542 -4.1534071 -4.1648812 -4.18647 -4.2211475 -4.2604527 -4.2879424][-4.3038425 -4.2984962 -4.2830505 -4.2613535 -4.2430024 -4.2342896 -4.2306929 -4.2198925 -4.2053766 -4.1997824 -4.2028065 -4.2153707 -4.2415924 -4.2750025 -4.2998815][-4.3152795 -4.3143291 -4.3059158 -4.2932191 -4.2836804 -4.2811217 -4.2808323 -4.2727394 -4.2593565 -4.2511792 -4.2484465 -4.2530742 -4.269465 -4.2936172 -4.311924][-4.3232117 -4.3238463 -4.3186445 -4.311213 -4.3070259 -4.308403 -4.3117318 -4.30851 -4.2994933 -4.2922626 -4.2877254 -4.2875094 -4.2958426 -4.3106894 -4.3218603][-4.3287258 -4.3299127 -4.3262916 -4.322217 -4.3211346 -4.3236446 -4.3278236 -4.3280563 -4.3233113 -4.3176937 -4.31335 -4.3117704 -4.3153338 -4.3230619 -4.3286033][-4.3260689 -4.3281312 -4.326159 -4.3242149 -4.3243208 -4.3265691 -4.3298621 -4.3312211 -4.3293037 -4.3257694 -4.3227167 -4.3213634 -4.3225808 -4.3259578 -4.3282657][-4.3181133 -4.3201938 -4.3190541 -4.3178825 -4.3176804 -4.3189197 -4.3207912 -4.3221116 -4.3218627 -4.3204746 -4.3190765 -4.3185592 -4.3194313 -4.3211679 -4.3223553]]...]
INFO - root - 2017-12-06 00:35:37.722333: step 58610, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 63h:48m:58s remains)
INFO - root - 2017-12-06 00:35:46.317160: step 58620, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 62h:21m:09s remains)
INFO - root - 2017-12-06 00:35:54.818562: step 58630, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 64h:24m:07s remains)
INFO - root - 2017-12-06 00:36:03.257246: step 58640, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 65h:12m:06s remains)
INFO - root - 2017-12-06 00:36:11.814123: step 58650, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.888 sec/batch; 67h:31m:54s remains)
INFO - root - 2017-12-06 00:36:20.484858: step 58660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 65h:19m:29s remains)
INFO - root - 2017-12-06 00:36:28.942120: step 58670, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 64h:37m:35s remains)
INFO - root - 2017-12-06 00:36:37.604701: step 58680, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.861 sec/batch; 65h:30m:10s remains)
INFO - root - 2017-12-06 00:36:46.080033: step 58690, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 64h:23m:23s remains)
INFO - root - 2017-12-06 00:36:54.590543: step 58700, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 64h:17m:58s remains)
2017-12-06 00:36:55.371296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2981076 -4.3002691 -4.302475 -4.3004422 -4.2898245 -4.2742653 -4.2610483 -4.2561789 -4.2647362 -4.2780366 -4.2923493 -4.3108044 -4.3308473 -4.3388324 -4.3355103][-4.2618055 -4.260529 -4.2604504 -4.2582197 -4.2491875 -4.2337775 -4.2218122 -4.2196417 -4.2361646 -4.2595043 -4.2839789 -4.3078771 -4.3282123 -4.3370876 -4.3347268][-4.2256722 -4.2204556 -4.2171793 -4.2125974 -4.201261 -4.1841221 -4.1735277 -4.1762414 -4.2004428 -4.2290683 -4.2613869 -4.2930169 -4.3171525 -4.3307033 -4.3327065][-4.1984935 -4.1957707 -4.1932573 -4.1862578 -4.1695819 -4.1414866 -4.1243849 -4.1291904 -4.1600447 -4.1923714 -4.2280498 -4.2666874 -4.2981734 -4.318171 -4.3277836][-4.1751323 -4.18215 -4.185214 -4.1752472 -4.1465755 -4.0974541 -4.0654111 -4.0782843 -4.1257939 -4.1698375 -4.2109265 -4.2521906 -4.2859526 -4.3076344 -4.3219891][-4.1470747 -4.1626873 -4.1669178 -4.1475215 -4.1038165 -4.0319362 -3.9810276 -4.0085793 -4.0820441 -4.1446486 -4.1929612 -4.2336125 -4.2705631 -4.2964392 -4.3170705][-4.1254115 -4.1406016 -4.1352348 -4.0997 -4.0354433 -3.9336584 -3.8531342 -3.8960421 -4.0042224 -4.0918593 -4.1530209 -4.1961865 -4.2410026 -4.2783942 -4.306953][-4.1353564 -4.134223 -4.1090603 -4.0590863 -3.9754107 -3.8441327 -3.7356622 -3.7922401 -3.9344156 -4.0420408 -4.1167355 -4.167408 -4.2200437 -4.2649088 -4.2973976][-4.1633615 -4.1417665 -4.1037912 -4.0562191 -3.9784751 -3.864748 -3.7764695 -3.8230858 -3.9474363 -4.0489941 -4.1224246 -4.1752281 -4.2291985 -4.2735009 -4.3020039][-4.1885734 -4.1567 -4.1188889 -4.0880828 -4.0389724 -3.9719424 -3.9278421 -3.9562583 -4.0342851 -4.1096497 -4.1670065 -4.2106209 -4.2554541 -4.292944 -4.315731][-4.1949878 -4.1622686 -4.1375742 -4.1290488 -4.1092887 -4.0820189 -4.0711665 -4.0891747 -4.1291 -4.1783934 -4.2220297 -4.255765 -4.2885246 -4.3159137 -4.3316293][-4.1857667 -4.1608992 -4.1602349 -4.1742892 -4.178009 -4.1763916 -4.1821914 -4.19619 -4.2139692 -4.241817 -4.2715554 -4.2942147 -4.3143086 -4.3312817 -4.3403659][-4.1804566 -4.1728239 -4.1904807 -4.2187505 -4.2383151 -4.2512994 -4.2621775 -4.2715635 -4.2773547 -4.2894359 -4.3039136 -4.3162766 -4.3277364 -4.3369589 -4.3422561][-4.1913214 -4.2063746 -4.2395091 -4.2726436 -4.2954988 -4.3100748 -4.3181152 -4.3190961 -4.3167105 -4.3177829 -4.3229451 -4.3300004 -4.3362541 -4.3395767 -4.3405604][-4.2105584 -4.2436852 -4.2810006 -4.3097181 -4.3255506 -4.331768 -4.3299112 -4.3232408 -4.3177447 -4.3174467 -4.3215652 -4.3286629 -4.3333735 -4.3353305 -4.3365459]]...]
INFO - root - 2017-12-06 00:37:03.784814: step 58710, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 63h:57m:19s remains)
INFO - root - 2017-12-06 00:37:12.342766: step 58720, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 66h:39m:29s remains)
INFO - root - 2017-12-06 00:37:20.913841: step 58730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 65h:04m:37s remains)
INFO - root - 2017-12-06 00:37:29.247003: step 58740, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 64h:36m:10s remains)
INFO - root - 2017-12-06 00:37:37.787378: step 58750, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.838 sec/batch; 63h:43m:55s remains)
INFO - root - 2017-12-06 00:37:46.446183: step 58760, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.892 sec/batch; 67h:49m:11s remains)
INFO - root - 2017-12-06 00:37:54.939469: step 58770, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.845 sec/batch; 64h:16m:27s remains)
INFO - root - 2017-12-06 00:38:03.504499: step 58780, loss = 2.02, batch loss = 1.96 (9.2 examples/sec; 0.872 sec/batch; 66h:19m:36s remains)
INFO - root - 2017-12-06 00:38:12.035004: step 58790, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 62h:18m:51s remains)
INFO - root - 2017-12-06 00:38:20.558874: step 58800, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 63h:54m:09s remains)
2017-12-06 00:38:21.346949: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2332907 -4.2340937 -4.2357225 -4.2394128 -4.2396812 -4.2360816 -4.2388659 -4.2416039 -4.2328033 -4.2208872 -4.21819 -4.2270031 -4.2295866 -4.2281742 -4.2262712][-4.2095137 -4.21024 -4.2116575 -4.2148929 -4.2153521 -4.2109613 -4.21445 -4.2182932 -4.2095542 -4.196744 -4.1984687 -4.2173486 -4.2243452 -4.228126 -4.2332559][-4.1977749 -4.1998839 -4.1985283 -4.1974063 -4.1959405 -4.1902027 -4.19227 -4.1939278 -4.1834636 -4.1711073 -4.1779056 -4.2017875 -4.214819 -4.2292409 -4.2422748][-4.1842589 -4.185843 -4.1825552 -4.1801686 -4.1791978 -4.1733918 -4.1744442 -4.1740685 -4.1660137 -4.155962 -4.1657863 -4.1870728 -4.2004271 -4.2237434 -4.2437][-4.1739526 -4.1715345 -4.1601439 -4.1544409 -4.15395 -4.1433153 -4.1363897 -4.13432 -4.1358542 -4.1314416 -4.1374793 -4.15191 -4.1707792 -4.2056088 -4.2345419][-4.1451325 -4.138176 -4.1227536 -4.1168151 -4.1104403 -4.0771132 -4.0448213 -4.0488267 -4.0808873 -4.0979071 -4.1032968 -4.1095371 -4.1352835 -4.1801677 -4.2184658][-4.0974197 -4.0884523 -4.0696726 -4.0615621 -4.0470495 -3.9822381 -3.9105079 -3.9315515 -4.0154166 -4.0665278 -4.0757551 -4.0754881 -4.1070533 -4.1602526 -4.2075586][-4.0542054 -4.0452876 -4.0256505 -4.0125051 -3.9844706 -3.8905132 -3.7803469 -3.8154683 -3.9406204 -4.0181284 -4.0321245 -4.0354891 -4.0788932 -4.1442165 -4.2002611][-4.0699878 -4.0689464 -4.0571976 -4.049696 -4.0332813 -3.9651942 -3.8762472 -3.8965626 -3.9948227 -4.0575933 -4.06605 -4.06743 -4.108376 -4.1710014 -4.2215629][-4.1078525 -4.1136289 -4.1109281 -4.1098852 -4.1093135 -4.0782456 -4.0230579 -4.0235033 -4.0755119 -4.1174083 -4.1280694 -4.1317611 -4.1625562 -4.2093382 -4.2444282][-4.1515164 -4.1590433 -4.1625414 -4.1663632 -4.1739893 -4.1665573 -4.1346307 -4.1220074 -4.1409135 -4.1665611 -4.1830454 -4.188715 -4.2039318 -4.2303724 -4.2519112][-4.1829619 -4.1909976 -4.1971278 -4.2047658 -4.2165389 -4.2202172 -4.2050028 -4.1908727 -4.1908164 -4.2026987 -4.2199521 -4.2271156 -4.2318735 -4.2431288 -4.2564569][-4.2122712 -4.2181835 -4.223866 -4.2321906 -4.2445006 -4.2520103 -4.2462583 -4.2352304 -4.2256742 -4.2256227 -4.2372575 -4.2448444 -4.2454748 -4.2497411 -4.2609472][-4.2425227 -4.2436066 -4.2466531 -4.2551417 -4.2641258 -4.26972 -4.2688012 -4.2621756 -4.250371 -4.2413416 -4.2471137 -4.2567482 -4.2601533 -4.2654762 -4.2771163][-4.2773709 -4.2756734 -4.27545 -4.27955 -4.2826972 -4.2846303 -4.2843204 -4.2807412 -4.2708249 -4.2581153 -4.2577782 -4.2680855 -4.2776704 -4.287859 -4.2996187]]...]
INFO - root - 2017-12-06 00:38:29.935083: step 58810, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 65h:25m:07s remains)
INFO - root - 2017-12-06 00:38:38.366497: step 58820, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:39m:28s remains)
INFO - root - 2017-12-06 00:38:46.834747: step 58830, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.852 sec/batch; 64h:46m:16s remains)
INFO - root - 2017-12-06 00:38:55.230903: step 58840, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.820 sec/batch; 62h:19m:39s remains)
INFO - root - 2017-12-06 00:39:03.775298: step 58850, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 64h:13m:28s remains)
INFO - root - 2017-12-06 00:39:12.330927: step 58860, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 66h:54m:54s remains)
INFO - root - 2017-12-06 00:39:20.885738: step 58870, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 66h:10m:39s remains)
INFO - root - 2017-12-06 00:39:29.395436: step 58880, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 64h:59m:31s remains)
INFO - root - 2017-12-06 00:39:37.900340: step 58890, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 65h:05m:24s remains)
INFO - root - 2017-12-06 00:39:46.447538: step 58900, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 64h:18m:08s remains)
2017-12-06 00:39:47.279749: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3237567 -4.3111105 -4.2979407 -4.2804937 -4.2618904 -4.2440491 -4.2242861 -4.2050147 -4.1945329 -4.1945572 -4.2063832 -4.2309327 -4.2602115 -4.2841015 -4.30164][-4.3123374 -4.2952609 -4.273982 -4.2452707 -4.2149248 -4.1839857 -4.1515832 -4.1223688 -4.1055813 -4.1051044 -4.120121 -4.1554565 -4.2031469 -4.24228 -4.2690139][-4.2979536 -4.2749624 -4.2463174 -4.2074871 -4.1680474 -4.123888 -4.0755625 -4.0310154 -4.0061469 -4.0071096 -4.0296617 -4.0781469 -4.139339 -4.1874795 -4.2242622][-4.2875414 -4.2595735 -4.2241759 -4.177052 -4.1286721 -4.068707 -3.9990511 -3.9436278 -3.922287 -3.9355669 -3.9701722 -4.031199 -4.0948362 -4.1438079 -4.1861911][-4.2769785 -4.2442136 -4.2034335 -4.1484604 -4.086391 -4.0059652 -3.9154148 -3.8641782 -3.8676338 -3.9047034 -3.957978 -4.0234723 -4.0806904 -4.1262617 -4.1711578][-4.2674041 -4.2297397 -4.1855783 -4.1254516 -4.0507712 -3.9545181 -3.8560741 -3.8227723 -3.8595228 -3.9255073 -3.9922013 -4.0516024 -4.0985475 -4.1364923 -4.1780558][-4.2633405 -4.2240734 -4.1806741 -4.1213565 -4.0409594 -3.9417634 -3.8520164 -3.8395877 -3.9006951 -3.9830651 -4.0533195 -4.10423 -4.1390538 -4.164454 -4.1990829][-4.2652884 -4.2282524 -4.1877441 -4.1308837 -4.0473204 -3.9513097 -3.8716619 -3.8685379 -3.9416406 -4.039196 -4.1153216 -4.1591468 -4.1820931 -4.1969895 -4.223423][-4.2669659 -4.2337718 -4.1971335 -4.1430688 -4.059885 -3.96713 -3.8866897 -3.8797736 -3.9584801 -4.0713444 -4.1579332 -4.195704 -4.2068944 -4.2138963 -4.2343092][-4.2674828 -4.236454 -4.2036629 -4.1542034 -4.0770931 -3.9860239 -3.9011517 -3.8900592 -3.9720025 -4.0888009 -4.1758461 -4.209806 -4.21612 -4.2168541 -4.2295303][-4.2698541 -4.240706 -4.2104068 -4.167006 -4.0992403 -4.0141172 -3.9326465 -3.9227002 -4.0031152 -4.1082416 -4.1866279 -4.2185497 -4.2197618 -4.2134776 -4.2201185][-4.2739415 -4.2453079 -4.2162404 -4.1804872 -4.1254754 -4.0550365 -3.9890084 -3.9858291 -4.0556865 -4.141223 -4.20668 -4.2309585 -4.2238531 -4.20895 -4.2111516][-4.2856894 -4.2597952 -4.2334857 -4.2033997 -4.1619854 -4.109726 -4.0656815 -4.071909 -4.1289015 -4.193882 -4.2405128 -4.252471 -4.2357326 -4.2139859 -4.2153831][-4.3018017 -4.2803612 -4.2582765 -4.2352476 -4.2087369 -4.17643 -4.151762 -4.161664 -4.2021585 -4.2441478 -4.2721715 -4.2730217 -4.2507491 -4.2290092 -4.2335663][-4.3177371 -4.3012953 -4.2850437 -4.2722282 -4.2597651 -4.2438283 -4.2330432 -4.24147 -4.2647023 -4.2857022 -4.2973447 -4.29209 -4.2713761 -4.2555408 -4.2640853]]...]
INFO - root - 2017-12-06 00:39:55.951464: step 58910, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 66h:46m:19s remains)
INFO - root - 2017-12-06 00:40:04.558623: step 58920, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.887 sec/batch; 67h:25m:39s remains)
INFO - root - 2017-12-06 00:40:12.839453: step 58930, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 66h:20m:09s remains)
INFO - root - 2017-12-06 00:40:21.318036: step 58940, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 65h:08m:32s remains)
INFO - root - 2017-12-06 00:40:29.870982: step 58950, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 66h:23m:40s remains)
INFO - root - 2017-12-06 00:40:38.373839: step 58960, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.849 sec/batch; 64h:32m:20s remains)
INFO - root - 2017-12-06 00:40:46.865804: step 58970, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 65h:42m:02s remains)
INFO - root - 2017-12-06 00:40:55.501542: step 58980, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 65h:00m:49s remains)
INFO - root - 2017-12-06 00:41:04.076192: step 58990, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 66h:06m:59s remains)
INFO - root - 2017-12-06 00:41:12.724258: step 59000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 65h:42m:37s remains)
2017-12-06 00:41:13.539470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1180019 -4.1300535 -4.1194239 -4.1040945 -4.1012945 -4.1038942 -4.1045918 -4.097044 -4.1001067 -4.1196933 -4.1367431 -4.144002 -4.1299191 -4.1084471 -4.1077919][-4.1385274 -4.1382275 -4.1276903 -4.1175389 -4.1161971 -4.1201754 -4.12373 -4.1078324 -4.1015744 -4.1146173 -4.1340647 -4.1544247 -4.1502109 -4.1307206 -4.1294093][-4.1589341 -4.1548481 -4.1436529 -4.1373286 -4.1370764 -4.1451821 -4.15343 -4.1419859 -4.1259289 -4.1174283 -4.1231194 -4.139586 -4.1409078 -4.1341152 -4.1437922][-4.1659064 -4.1620917 -4.1508193 -4.1481695 -4.1499658 -4.160183 -4.1778641 -4.1812458 -4.1639714 -4.1325665 -4.1125088 -4.1150761 -4.1185513 -4.1272211 -4.1411157][-4.149847 -4.1447654 -4.1356859 -4.1372728 -4.1389217 -4.1428905 -4.1678982 -4.190886 -4.1850748 -4.1495547 -4.115706 -4.1060047 -4.1102858 -4.1208782 -4.1273332][-4.1204529 -4.1188073 -4.1131759 -4.1107917 -4.0974064 -4.0808105 -4.1082668 -4.1563587 -4.1741476 -4.1554236 -4.1280274 -4.1115308 -4.1050158 -4.1090856 -4.1102676][-4.0987282 -4.1050639 -4.0975542 -4.0771542 -4.0274277 -3.9735491 -4.0113134 -4.0981264 -4.1462297 -4.152595 -4.1392908 -4.1181526 -4.1007104 -4.0953555 -4.1022735][-4.0996609 -4.111177 -4.0972323 -4.0578747 -3.9718409 -3.8816919 -3.9388032 -4.0576177 -4.1181493 -4.1369524 -4.1305351 -4.1099343 -4.0917115 -4.0890265 -4.1046562][-4.1319308 -4.1420078 -4.1282382 -4.0937352 -4.019968 -3.9472308 -3.9921727 -4.071877 -4.0984449 -4.103384 -4.1010275 -4.0901966 -4.0779533 -4.075151 -4.0910187][-4.1769819 -4.1816711 -4.1718588 -4.1545911 -4.1149716 -4.0751238 -4.0907574 -4.1032538 -4.0836821 -4.0729332 -4.0821328 -4.0886497 -4.0818696 -4.0644436 -4.063117][-4.2041316 -4.195447 -4.1861262 -4.183229 -4.1690106 -4.1515 -4.150547 -4.1292973 -4.0929303 -4.0807633 -4.0986528 -4.1138487 -4.0963573 -4.0529833 -4.0294714][-4.1963758 -4.1714668 -4.1657495 -4.1796837 -4.1829357 -4.1786265 -4.1786933 -4.158741 -4.1305585 -4.1211066 -4.1323094 -4.13736 -4.1075573 -4.0615106 -4.0402503][-4.1754041 -4.1351686 -4.1283889 -4.1531997 -4.1665359 -4.1723409 -4.1829596 -4.1738582 -4.1515317 -4.1393976 -4.14441 -4.1444807 -4.1146536 -4.0882678 -4.0815086][-4.1610832 -4.1191969 -4.1030521 -4.1205835 -4.1344833 -4.1481347 -4.1651831 -4.1577148 -4.1319017 -4.1165905 -4.1227512 -4.1271429 -4.1121583 -4.1043439 -4.0993218][-4.1624007 -4.1342845 -4.1121616 -4.1123409 -4.1205273 -4.135098 -4.1424451 -4.1282153 -4.1047773 -4.09678 -4.11273 -4.1218562 -4.1149545 -4.1037884 -4.0872602]]...]
INFO - root - 2017-12-06 00:41:21.969318: step 59010, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:39m:11s remains)
INFO - root - 2017-12-06 00:41:30.475776: step 59020, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 63h:54m:46s remains)
INFO - root - 2017-12-06 00:41:38.840197: step 59030, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 64h:41m:54s remains)
INFO - root - 2017-12-06 00:41:47.170476: step 59040, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 65h:59m:01s remains)
INFO - root - 2017-12-06 00:41:55.748231: step 59050, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 63h:08m:59s remains)
INFO - root - 2017-12-06 00:42:04.344405: step 59060, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.843 sec/batch; 64h:00m:04s remains)
INFO - root - 2017-12-06 00:42:12.855713: step 59070, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 62h:52m:00s remains)
INFO - root - 2017-12-06 00:42:21.432952: step 59080, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.848 sec/batch; 64h:25m:18s remains)
INFO - root - 2017-12-06 00:42:30.022672: step 59090, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 65h:21m:15s remains)
INFO - root - 2017-12-06 00:42:38.494586: step 59100, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.845 sec/batch; 64h:12m:00s remains)
2017-12-06 00:42:39.339140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3345962 -4.3317757 -4.33037 -4.3268766 -4.3213968 -4.3170371 -4.3173728 -4.3219666 -4.3279672 -4.3349195 -4.3417411 -4.3467512 -4.3492937 -4.3508854 -4.349822][-4.3290167 -4.3225894 -4.3174925 -4.308558 -4.295897 -4.2843261 -4.2796836 -4.2841768 -4.29598 -4.3130288 -4.3309832 -4.344347 -4.3500466 -4.3515072 -4.3493867][-4.3170495 -4.303134 -4.2920117 -4.2769508 -4.2543297 -4.2297859 -4.2131076 -4.2145162 -4.2352853 -4.2681437 -4.3037071 -4.3315759 -4.34681 -4.3518734 -4.3499227][-4.2971268 -4.2796702 -4.2653627 -4.2424221 -4.2039981 -4.1544466 -4.1127253 -4.1044221 -4.1366305 -4.1923242 -4.2524052 -4.3007445 -4.3309059 -4.3451147 -4.347755][-4.2695103 -4.2559977 -4.240026 -4.2059646 -4.1434793 -4.0553675 -3.9711273 -3.945492 -4.0007482 -4.0919719 -4.1835961 -4.25437 -4.3011632 -4.3286409 -4.3399472][-4.2407422 -4.2366896 -4.2228394 -4.1788216 -4.0926719 -3.9625995 -3.8220236 -3.7765393 -3.865824 -3.9986811 -4.1208987 -4.2106638 -4.2707562 -4.3083773 -4.3274245][-4.218924 -4.2279487 -4.2192893 -4.1742563 -4.0827122 -3.9408987 -3.7819483 -3.7285135 -3.8272979 -3.967272 -4.0975533 -4.1927414 -4.2571917 -4.2963924 -4.3164191][-4.2089624 -4.2312927 -4.2352052 -4.2035222 -4.1279573 -4.0173092 -3.8996208 -3.8568592 -3.9194913 -4.0209866 -4.1263952 -4.2079892 -4.2627573 -4.2944088 -4.3084154][-4.2116976 -4.2416434 -4.2573328 -4.2417264 -4.1899786 -4.118959 -4.0486631 -4.02105 -4.0543795 -4.1171494 -4.1874733 -4.2448339 -4.2819567 -4.3001208 -4.3044672][-4.2425709 -4.2675018 -4.2839904 -4.2799015 -4.2495561 -4.2082505 -4.1678925 -4.1482725 -4.1637259 -4.1998119 -4.2421308 -4.2769723 -4.2984972 -4.3063421 -4.302392][-4.2736492 -4.2926426 -4.3071437 -4.3107963 -4.2978516 -4.2773442 -4.255928 -4.2427564 -4.2465138 -4.2605848 -4.2794337 -4.2966213 -4.3055768 -4.3057675 -4.2981229][-4.288691 -4.3087196 -4.3258352 -4.3354387 -4.3324795 -4.3231964 -4.3104773 -4.2983685 -4.2911615 -4.2878962 -4.2896104 -4.2943134 -4.2964373 -4.2940283 -4.289][-4.2836919 -4.3075318 -4.3274679 -4.3380995 -4.3404012 -4.3365111 -4.3273463 -4.3141375 -4.3005204 -4.2889094 -4.2821832 -4.2799869 -4.2783103 -4.2759466 -4.275773][-4.2699246 -4.2956018 -4.3149719 -4.3244419 -4.3271003 -4.3240943 -4.3168817 -4.3058619 -4.2931976 -4.2804856 -4.2697892 -4.2613664 -4.2560992 -4.256556 -4.263473][-4.2681708 -4.288619 -4.3015347 -4.3045783 -4.302249 -4.2980146 -4.2927809 -4.2871752 -4.2801137 -4.2700138 -4.2578845 -4.2468429 -4.2440305 -4.2519283 -4.2662225]]...]
INFO - root - 2017-12-06 00:42:47.993750: step 59110, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 64h:17m:59s remains)
INFO - root - 2017-12-06 00:42:56.506044: step 59120, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.839 sec/batch; 63h:44m:59s remains)
INFO - root - 2017-12-06 00:43:05.061204: step 59130, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 65h:05m:32s remains)
INFO - root - 2017-12-06 00:43:13.328509: step 59140, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 63h:49m:04s remains)
INFO - root - 2017-12-06 00:43:21.775844: step 59150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 65h:41m:02s remains)
INFO - root - 2017-12-06 00:43:30.304563: step 59160, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.856 sec/batch; 64h:57m:54s remains)
INFO - root - 2017-12-06 00:43:38.888691: step 59170, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 65h:42m:14s remains)
INFO - root - 2017-12-06 00:43:47.337322: step 59180, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 64h:45m:51s remains)
INFO - root - 2017-12-06 00:43:55.929194: step 59190, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 64h:07m:24s remains)
INFO - root - 2017-12-06 00:44:04.396006: step 59200, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 63h:45m:16s remains)
2017-12-06 00:44:05.197886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2431283 -4.2721791 -4.2828088 -4.277916 -4.2643042 -4.2533565 -4.2502341 -4.2560439 -4.2651949 -4.2750797 -4.2851462 -4.2992148 -4.3096142 -4.3103395 -4.3011765][-4.2548394 -4.2803593 -4.2827892 -4.2661066 -4.2399745 -4.2197833 -4.2136879 -4.2209072 -4.2363906 -4.2547913 -4.270555 -4.2899737 -4.3046408 -4.3070874 -4.299675][-4.2536397 -4.2726383 -4.2617507 -4.2312965 -4.193696 -4.1677532 -4.1646419 -4.1804452 -4.2084546 -4.2401361 -4.2660284 -4.2904115 -4.3051257 -4.3050332 -4.2944708][-4.2462564 -4.25576 -4.2318988 -4.1862655 -4.1338286 -4.1022115 -4.1066236 -4.1375828 -4.182735 -4.2267652 -4.2616811 -4.288156 -4.2980251 -4.2907791 -4.275208][-4.2489614 -4.246634 -4.2110109 -4.1504073 -4.0797586 -4.036118 -4.046442 -4.0953345 -4.1579227 -4.2120328 -4.2522926 -4.2785735 -4.2792764 -4.2590594 -4.2376113][-4.2603145 -4.2507772 -4.2038269 -4.1233606 -4.0242357 -3.9575315 -3.9715495 -4.044167 -4.1310368 -4.1991253 -4.2455044 -4.2709684 -4.2616134 -4.2281461 -4.1963263][-4.2715406 -4.2619605 -4.210887 -4.1141558 -3.9844751 -3.8862414 -3.8948457 -3.9875472 -4.0956316 -4.1776824 -4.2315493 -4.2582912 -4.2455564 -4.206665 -4.1703539][-4.2712283 -4.26826 -4.2208734 -4.122488 -3.9877398 -3.8818483 -3.8874557 -3.9853518 -4.0990787 -4.1832294 -4.2366385 -4.2584906 -4.2412148 -4.1997089 -4.1636147][-4.2513981 -4.2574182 -4.2200451 -4.1356454 -4.0235281 -3.9376836 -3.94515 -4.0316238 -4.1353683 -4.213913 -4.2622232 -4.2801456 -4.2640905 -4.2275772 -4.1934361][-4.232935 -4.2430096 -4.2171168 -4.152534 -4.0695214 -4.0077128 -4.0163283 -4.0864935 -4.1719732 -4.2393365 -4.284503 -4.3035431 -4.2957749 -4.2701077 -4.243938][-4.2337637 -4.2435732 -4.2242103 -4.1780386 -4.1202183 -4.0773315 -4.085041 -4.1375656 -4.2028046 -4.2557125 -4.2966304 -4.3181849 -4.3178582 -4.2992105 -4.2780275][-4.2498846 -4.2601209 -4.2451768 -4.2117982 -4.1713085 -4.1420703 -4.1489382 -4.1878171 -4.2346191 -4.2741542 -4.3071313 -4.3271427 -4.3299289 -4.3153238 -4.2950549][-4.2719092 -4.2826962 -4.270577 -4.243679 -4.2095494 -4.185946 -4.1907797 -4.2200484 -4.2561445 -4.2861624 -4.3113847 -4.3305459 -4.3364816 -4.3264337 -4.3065319][-4.2887444 -4.2964182 -4.2845964 -4.2600245 -4.2273679 -4.2045622 -4.2069983 -4.2291136 -4.2587109 -4.28382 -4.3045235 -4.321002 -4.3300991 -4.3265872 -4.3118539][-4.2918839 -4.2938614 -4.2793741 -4.2566385 -4.2284884 -4.2086916 -4.2110763 -4.2307916 -4.2559361 -4.2767859 -4.2929378 -4.3057976 -4.3153715 -4.31526 -4.3067169]]...]
INFO - root - 2017-12-06 00:44:13.722373: step 59210, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 63h:31m:08s remains)
INFO - root - 2017-12-06 00:44:22.350386: step 59220, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 65h:34m:53s remains)
INFO - root - 2017-12-06 00:44:30.949326: step 59230, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 63h:48m:29s remains)
INFO - root - 2017-12-06 00:44:39.446108: step 59240, loss = 2.05, batch loss = 1.99 (8.9 examples/sec; 0.896 sec/batch; 68h:01m:52s remains)
INFO - root - 2017-12-06 00:44:47.849407: step 59250, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.840 sec/batch; 63h:46m:32s remains)
INFO - root - 2017-12-06 00:44:56.263183: step 59260, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 65h:25m:15s remains)
INFO - root - 2017-12-06 00:45:04.727860: step 59270, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 63h:35m:42s remains)
INFO - root - 2017-12-06 00:45:13.202447: step 59280, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 65h:03m:17s remains)
INFO - root - 2017-12-06 00:45:21.735488: step 59290, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 63h:48m:06s remains)
INFO - root - 2017-12-06 00:45:30.276408: step 59300, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.832 sec/batch; 63h:08m:16s remains)
2017-12-06 00:45:31.029005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3008485 -4.2975969 -4.2847247 -4.2714343 -4.262507 -4.2556677 -4.2534313 -4.26041 -4.2754035 -4.2937303 -4.3085418 -4.3147821 -4.3146906 -4.3120813 -4.3049641][-4.3058 -4.2999291 -4.2815337 -4.2625313 -4.2481589 -4.2359238 -4.2325182 -4.2402773 -4.2581234 -4.2826276 -4.3034573 -4.3156543 -4.3200121 -4.3203068 -4.3150005][-4.3086429 -4.2978597 -4.2749987 -4.2492304 -4.2260227 -4.2064037 -4.2003551 -4.2099 -4.232574 -4.2633014 -4.2866616 -4.3031507 -4.3137932 -4.319561 -4.3197613][-4.3030992 -4.2883835 -4.260931 -4.22762 -4.20047 -4.1771946 -4.1623611 -4.1673303 -4.1917505 -4.2256384 -4.2518892 -4.2729006 -4.2933187 -4.3084927 -4.315186][-4.2910881 -4.2729373 -4.23798 -4.1970544 -4.1630135 -4.13237 -4.0979795 -4.0953369 -4.1259 -4.1647782 -4.1975441 -4.2245827 -4.2590084 -4.2874165 -4.3015647][-4.2766366 -4.2531486 -4.2084956 -4.15544 -4.1052809 -4.0496111 -3.9821558 -3.9741406 -4.02484 -4.0823264 -4.1313376 -4.1706395 -4.2210197 -4.2638669 -4.2866244][-4.2654514 -4.2372012 -4.1835003 -4.1181297 -4.0465031 -3.9545791 -3.8375764 -3.812603 -3.8974509 -3.9929621 -4.0728664 -4.1355524 -4.2031345 -4.2553539 -4.2815166][-4.2640762 -4.234993 -4.178175 -4.1042161 -4.0206017 -3.912 -3.7719145 -3.7282779 -3.8278792 -3.9472883 -4.0512509 -4.13474 -4.2133512 -4.2636437 -4.2868385][-4.2708068 -4.2434077 -4.1915917 -4.1218657 -4.0468946 -3.9599495 -3.8616941 -3.8313675 -3.9011149 -3.9951787 -4.0928378 -4.1770592 -4.2484341 -4.2870975 -4.302772][-4.2869773 -4.2616639 -4.2167912 -4.1577621 -4.101675 -4.0459361 -3.9924734 -3.9862137 -4.0339723 -4.0955811 -4.1703978 -4.2400575 -4.294476 -4.3171868 -4.3210177][-4.3089743 -4.288465 -4.251863 -4.2083521 -4.1708007 -4.13844 -4.1138926 -4.1195583 -4.1553316 -4.195899 -4.2466564 -4.2981715 -4.3322744 -4.3400145 -4.335331][-4.3234377 -4.3132148 -4.2878547 -4.2621856 -4.2413244 -4.2241235 -4.2141552 -4.2182751 -4.2413654 -4.2684531 -4.3009009 -4.3341584 -4.3516026 -4.3500047 -4.3414][-4.3245249 -4.3232975 -4.3095884 -4.2959766 -4.2862062 -4.2779641 -4.2753525 -4.2812829 -4.2968173 -4.3132877 -4.330802 -4.3457365 -4.3498511 -4.3436613 -4.3342423][-4.3190546 -4.3241706 -4.3202305 -4.3153906 -4.31055 -4.307518 -4.3084245 -4.3148704 -4.3242249 -4.3298721 -4.3334827 -4.3358283 -4.3331227 -4.3278542 -4.3211508][-4.3130269 -4.3202996 -4.3222156 -4.3233433 -4.3235488 -4.3223958 -4.3229094 -4.3258395 -4.3276358 -4.3256397 -4.3215342 -4.3194237 -4.31706 -4.3153143 -4.3112378]]...]
INFO - root - 2017-12-06 00:45:39.530603: step 59310, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 65h:31m:52s remains)
INFO - root - 2017-12-06 00:45:48.200975: step 59320, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 65h:38m:30s remains)
INFO - root - 2017-12-06 00:45:56.747353: step 59330, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.809 sec/batch; 61h:25m:27s remains)
INFO - root - 2017-12-06 00:46:04.817431: step 59340, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 62h:59m:24s remains)
INFO - root - 2017-12-06 00:46:13.265515: step 59350, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 64h:17m:50s remains)
INFO - root - 2017-12-06 00:46:21.779244: step 59360, loss = 2.10, batch loss = 2.04 (9.5 examples/sec; 0.845 sec/batch; 64h:08m:10s remains)
INFO - root - 2017-12-06 00:46:30.264461: step 59370, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 66h:45m:06s remains)
INFO - root - 2017-12-06 00:46:38.732588: step 59380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 65h:00m:13s remains)
INFO - root - 2017-12-06 00:46:47.310913: step 59390, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 64h:28m:03s remains)
INFO - root - 2017-12-06 00:46:55.849498: step 59400, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 66h:00m:36s remains)
2017-12-06 00:46:56.656294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2553406 -4.250803 -4.2615719 -4.2782788 -4.284153 -4.2718134 -4.2575841 -4.2443409 -4.2320323 -4.2277436 -4.2269135 -4.2338309 -4.2376909 -4.2351613 -4.2334046][-4.2388344 -4.2348366 -4.2478552 -4.2639103 -4.2708778 -4.2592955 -4.2418094 -4.2218161 -4.201921 -4.1904917 -4.1912394 -4.2078853 -4.2151265 -4.2049031 -4.1952624][-4.2251716 -4.2203879 -4.2341905 -4.2489433 -4.2599092 -4.2520332 -4.2342343 -4.2061729 -4.1731935 -4.150795 -4.1588774 -4.1920705 -4.2048 -4.1883073 -4.1722374][-4.2134075 -4.207788 -4.2258949 -4.2455344 -4.2610912 -4.2538891 -4.2318177 -4.1902151 -4.1382794 -4.1096973 -4.1386781 -4.1894631 -4.2032065 -4.1832743 -4.1630087][-4.2058477 -4.2019587 -4.2248549 -4.249135 -4.2654185 -4.2559562 -4.2156081 -4.1460433 -4.0751209 -4.0639248 -4.1328497 -4.1967897 -4.204906 -4.1830726 -4.1618752][-4.2097569 -4.2073374 -4.229063 -4.249609 -4.26164 -4.24432 -4.177608 -4.0721087 -3.9939117 -4.0313487 -4.1428328 -4.2073107 -4.2069407 -4.1811428 -4.1615572][-4.2179089 -4.2133274 -4.2280073 -4.2411795 -4.2506437 -4.22429 -4.1292706 -3.9865766 -3.9210584 -4.0239172 -4.1589522 -4.2134504 -4.2057343 -4.1741047 -4.1552691][-4.2175345 -4.2080331 -4.2153354 -4.2239623 -4.2317271 -4.1932383 -4.0732012 -3.9106684 -3.8804574 -4.029995 -4.1626315 -4.2032061 -4.1896076 -4.153995 -4.1410756][-4.2116466 -4.1971664 -4.1984973 -4.2060189 -4.2117028 -4.1635084 -4.0386586 -3.9049973 -3.9317045 -4.074585 -4.1693687 -4.186233 -4.1650572 -4.1318421 -4.1337838][-4.2035775 -4.1832681 -4.1786084 -4.187263 -4.1914387 -4.1352949 -4.0219178 -3.9478509 -4.0219626 -4.128756 -4.1769619 -4.171555 -4.1452088 -4.1188436 -4.1353469][-4.1950607 -4.1669645 -4.1583161 -4.1667657 -4.1666207 -4.1065803 -4.0107188 -3.99094 -4.0850916 -4.161088 -4.180542 -4.16493 -4.1356783 -4.1177888 -4.143559][-4.2037063 -4.1735444 -4.1639037 -4.1712294 -4.1650991 -4.1063852 -4.0335221 -4.0484304 -4.1358166 -4.1898851 -4.19898 -4.1849251 -4.158247 -4.1460361 -4.1728039][-4.23426 -4.2105808 -4.2046242 -4.2105408 -4.2002769 -4.1513391 -4.1061425 -4.1302767 -4.1933136 -4.2279329 -4.2320709 -4.2234745 -4.2055116 -4.1996555 -4.2234845][-4.2682257 -4.252203 -4.249423 -4.2537246 -4.2440453 -4.2143421 -4.1929727 -4.2123151 -4.2486248 -4.2686534 -4.2719603 -4.2697611 -4.2596421 -4.2551575 -4.2723789][-4.296443 -4.2886786 -4.2876754 -4.2895589 -4.2831659 -4.2699509 -4.2624383 -4.2741671 -4.2927718 -4.3035965 -4.3063693 -4.3058448 -4.3005872 -4.2971234 -4.307817]]...]
INFO - root - 2017-12-06 00:47:05.196879: step 59410, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 63h:04m:00s remains)
INFO - root - 2017-12-06 00:47:13.801773: step 59420, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 65h:46m:06s remains)
INFO - root - 2017-12-06 00:47:22.233846: step 59430, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.821 sec/batch; 62h:18m:29s remains)
INFO - root - 2017-12-06 00:47:30.651042: step 59440, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.810 sec/batch; 61h:26m:39s remains)
INFO - root - 2017-12-06 00:47:39.244483: step 59450, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 63h:58m:17s remains)
INFO - root - 2017-12-06 00:47:47.709401: step 59460, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.889 sec/batch; 67h:26m:02s remains)
INFO - root - 2017-12-06 00:47:56.200882: step 59470, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 64h:40m:46s remains)
INFO - root - 2017-12-06 00:48:04.614020: step 59480, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.866 sec/batch; 65h:42m:38s remains)
INFO - root - 2017-12-06 00:48:13.130484: step 59490, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.870 sec/batch; 65h:57m:44s remains)
INFO - root - 2017-12-06 00:48:21.761308: step 59500, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 64h:06m:55s remains)
2017-12-06 00:48:22.510206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.331419 -4.3166833 -4.2954731 -4.2737145 -4.2577591 -4.2529221 -4.2515883 -4.2431717 -4.2421193 -4.2635231 -4.2892952 -4.2936144 -4.2809491 -4.2570682 -4.2444935][-4.3216472 -4.3002753 -4.2728095 -4.2460442 -4.2274337 -4.2207956 -4.2203021 -4.2174 -4.2214336 -4.24918 -4.2790217 -4.281745 -4.2680106 -4.2455373 -4.2308283][-4.3129711 -4.2853804 -4.2507615 -4.2175021 -4.1938143 -4.1824865 -4.1807418 -4.1803994 -4.1914644 -4.2234116 -4.2596469 -4.2668395 -4.2564855 -4.2391009 -4.2277532][-4.3076468 -4.2744803 -4.2331676 -4.1950507 -4.165678 -4.1464138 -4.1410842 -4.1417723 -4.1575508 -4.189786 -4.2299628 -4.2442522 -4.2404628 -4.2336359 -4.23177][-4.3077178 -4.2726526 -4.2310433 -4.1951404 -4.1656685 -4.1423693 -4.1369119 -4.1405416 -4.1556711 -4.18077 -4.2170086 -4.2349844 -4.2328 -4.2324238 -4.24069][-4.3115888 -4.2776456 -4.2404709 -4.21335 -4.1900229 -4.1676297 -4.1598024 -4.1637897 -4.1797318 -4.1975627 -4.2240396 -4.2397656 -4.2329507 -4.230185 -4.2387996][-4.3145237 -4.2823639 -4.2482181 -4.2276688 -4.2095852 -4.1913829 -4.1817007 -4.1830935 -4.2004137 -4.21561 -4.2348475 -4.2468295 -4.237432 -4.2322445 -4.2363343][-4.3151913 -4.282711 -4.2473707 -4.2248931 -4.2061038 -4.1888475 -4.1754179 -4.171957 -4.1875625 -4.2070475 -4.2280378 -4.2387652 -4.2341604 -4.2342243 -4.2393465][-4.3150835 -4.2823796 -4.2464843 -4.2220106 -4.2023497 -4.1862106 -4.1698623 -4.1584396 -4.1699195 -4.193244 -4.216 -4.2257543 -4.2283058 -4.2379465 -4.2467933][-4.31124 -4.2782288 -4.2425685 -4.2170544 -4.1979165 -4.1841636 -4.1684079 -4.1564021 -4.1685677 -4.1936822 -4.2165442 -4.2254376 -4.2313089 -4.2432914 -4.2542415][-4.3067327 -4.2725496 -4.2358441 -4.209414 -4.1920891 -4.1814017 -4.1688685 -4.1588907 -4.1676221 -4.1907196 -4.2128291 -4.2210445 -4.2269225 -4.2393265 -4.2524819][-4.3050966 -4.2727532 -4.2380567 -4.2126055 -4.199738 -4.1937518 -4.1837215 -4.174561 -4.1781669 -4.194315 -4.2115235 -4.218524 -4.2274284 -4.2408948 -4.2535572][-4.3076453 -4.2779684 -4.2462745 -4.2218189 -4.2100205 -4.2045655 -4.1950488 -4.18808 -4.1906543 -4.2013531 -4.2134161 -4.2211194 -4.2308722 -4.2441969 -4.258019][-4.313314 -4.2868629 -4.2586031 -4.2345796 -4.2196341 -4.211081 -4.2029819 -4.2020292 -4.2095079 -4.2182589 -4.2265272 -4.232872 -4.2437873 -4.2578087 -4.2694483][-4.321919 -4.2998009 -4.275064 -4.2521768 -4.2335696 -4.2211752 -4.215827 -4.2205019 -4.2326918 -4.2439122 -4.25104 -4.2568092 -4.2670078 -4.2776465 -4.2838469]]...]
INFO - root - 2017-12-06 00:48:30.999377: step 59510, loss = 2.10, batch loss = 2.04 (9.8 examples/sec; 0.820 sec/batch; 62h:11m:13s remains)
INFO - root - 2017-12-06 00:48:39.591780: step 59520, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 65h:01m:06s remains)
INFO - root - 2017-12-06 00:48:48.123382: step 59530, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 66h:07m:46s remains)
INFO - root - 2017-12-06 00:48:56.521645: step 59540, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 63h:23m:23s remains)
INFO - root - 2017-12-06 00:49:05.163477: step 59550, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.885 sec/batch; 67h:04m:59s remains)
INFO - root - 2017-12-06 00:49:13.561643: step 59560, loss = 2.08, batch loss = 2.03 (9.6 examples/sec; 0.831 sec/batch; 63h:00m:47s remains)
INFO - root - 2017-12-06 00:49:22.043682: step 59570, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 65h:13m:06s remains)
INFO - root - 2017-12-06 00:49:30.498540: step 59580, loss = 2.03, batch loss = 1.98 (9.8 examples/sec; 0.816 sec/batch; 61h:53m:10s remains)
INFO - root - 2017-12-06 00:49:38.958818: step 59590, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.882 sec/batch; 66h:54m:00s remains)
INFO - root - 2017-12-06 00:49:47.508278: step 59600, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 65h:17m:46s remains)
2017-12-06 00:49:48.278769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3043985 -4.308639 -4.3097758 -4.3102856 -4.3120441 -4.3125796 -4.312572 -4.314115 -4.3127775 -4.3054876 -4.29739 -4.2954893 -4.2987719 -4.3074322 -4.320663][-4.2787075 -4.2767267 -4.2751069 -4.2770653 -4.28413 -4.287571 -4.2865124 -4.2913451 -4.2927 -4.283937 -4.2721791 -4.2680459 -4.2739644 -4.2878113 -4.3082066][-4.2455182 -4.2349367 -4.2342205 -4.2413678 -4.2548771 -4.2615719 -4.2556 -4.2593904 -4.2635226 -4.2543664 -4.2416067 -4.2393451 -4.2531376 -4.2762008 -4.3027234][-4.2021127 -4.1924829 -4.1987596 -4.2146964 -4.233789 -4.23938 -4.225903 -4.2245722 -4.2304406 -4.2227983 -4.2098784 -4.2143779 -4.2399344 -4.2728343 -4.3016281][-4.1525207 -4.1464748 -4.161746 -4.1865525 -4.2055283 -4.2037539 -4.1803932 -4.1763554 -4.1866541 -4.1847267 -4.175869 -4.1908307 -4.2315254 -4.2755127 -4.3055959][-4.1063328 -4.0953751 -4.1114717 -4.1333876 -4.1465917 -4.1295977 -4.0918989 -4.0927095 -4.1215091 -4.1403532 -4.1487217 -4.176846 -4.2264581 -4.2749052 -4.3064504][-4.084549 -4.0617561 -4.0627565 -4.0684328 -4.0631938 -4.0151148 -3.9451418 -3.9557762 -4.0250297 -4.078444 -4.1101913 -4.1493392 -4.20419 -4.2590137 -4.2972755][-4.1145911 -4.081562 -4.0623193 -4.0441747 -4.0179715 -3.9402473 -3.8303251 -3.846633 -3.9547486 -4.0359392 -4.0834093 -4.1253104 -4.1772394 -4.2381554 -4.2846704][-4.179482 -4.1495714 -4.1273203 -4.1007509 -4.0706868 -4.0072565 -3.9166923 -3.9278553 -4.0185876 -4.0846634 -4.1221495 -4.1493964 -4.1839147 -4.2374015 -4.2834454][-4.22233 -4.2055211 -4.1929755 -4.1737413 -4.1551633 -4.1246448 -4.0780373 -4.0834265 -4.1317639 -4.1638846 -4.1792622 -4.1897268 -4.2106051 -4.2521605 -4.291285][-4.2426209 -4.2337265 -4.2240252 -4.2107644 -4.2051835 -4.1990352 -4.1793771 -4.1802468 -4.199789 -4.2096186 -4.2126579 -4.216279 -4.233448 -4.2700706 -4.3022132][-4.255291 -4.2468472 -4.2333517 -4.2227073 -4.2244816 -4.2302051 -4.2224531 -4.2245073 -4.2356119 -4.2398252 -4.2380762 -4.2395606 -4.2562761 -4.2895145 -4.3148861][-4.2675343 -4.2578406 -4.2470541 -4.2387013 -4.2400694 -4.2449584 -4.2445602 -4.250392 -4.2611532 -4.2662244 -4.2653952 -4.2670455 -4.281672 -4.3076315 -4.3262143][-4.2886429 -4.2791953 -4.2719407 -4.2677293 -4.2677331 -4.2675815 -4.26632 -4.2717652 -4.2816243 -4.2884812 -4.2895379 -4.2920933 -4.3043008 -4.3219986 -4.3341746][-4.3069096 -4.30169 -4.2980504 -4.2963777 -4.2966495 -4.29526 -4.2926083 -4.2953548 -4.3020234 -4.3058858 -4.3077226 -4.3122029 -4.3211827 -4.332449 -4.3395996]]...]
INFO - root - 2017-12-06 00:49:56.864292: step 59610, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 65h:19m:03s remains)
INFO - root - 2017-12-06 00:50:05.448536: step 59620, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.827 sec/batch; 62h:39m:36s remains)
INFO - root - 2017-12-06 00:50:13.910747: step 59630, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 63h:49m:04s remains)
INFO - root - 2017-12-06 00:50:22.428582: step 59640, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.889 sec/batch; 67h:22m:23s remains)
INFO - root - 2017-12-06 00:50:30.917821: step 59650, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 63h:57m:09s remains)
INFO - root - 2017-12-06 00:50:39.392752: step 59660, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 64h:13m:35s remains)
INFO - root - 2017-12-06 00:50:47.773563: step 59670, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 64h:35m:34s remains)
INFO - root - 2017-12-06 00:50:56.318054: step 59680, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 64h:15m:26s remains)
INFO - root - 2017-12-06 00:51:04.882612: step 59690, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 65h:28m:49s remains)
INFO - root - 2017-12-06 00:51:13.314485: step 59700, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.826 sec/batch; 62h:33m:58s remains)
2017-12-06 00:51:14.144657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.17907 -4.2220683 -4.2602592 -4.2796588 -4.2731404 -4.2451639 -4.1964917 -4.1444678 -4.1196127 -4.122251 -4.1413407 -4.1461768 -4.1261225 -4.1056628 -4.1178608][-4.2068925 -4.2461162 -4.2734103 -4.2816615 -4.2657533 -4.230073 -4.17639 -4.1299129 -4.1188512 -4.1342311 -4.1575046 -4.1768341 -4.1732121 -4.1496673 -4.1374512][-4.2353454 -4.2657061 -4.28128 -4.2783017 -4.2526169 -4.2091088 -4.1547275 -4.1192193 -4.1216106 -4.1467371 -4.1733408 -4.2009587 -4.2126513 -4.197824 -4.1797724][-4.2577162 -4.279984 -4.2844348 -4.2686529 -4.2379107 -4.1905565 -4.1348124 -4.1030846 -4.1099081 -4.1422687 -4.1771622 -4.2170482 -4.2453742 -4.2449851 -4.2297406][-4.2684 -4.2820654 -4.2750092 -4.2506185 -4.217875 -4.1671185 -4.1028643 -4.0619707 -4.0736032 -4.123045 -4.1756687 -4.225359 -4.2629461 -4.272429 -4.2591438][-4.2543 -4.2594509 -4.2447729 -4.2179036 -4.1852388 -4.1313577 -4.0518885 -3.9885671 -4.0020533 -4.078958 -4.1588936 -4.2197618 -4.2597747 -4.2704768 -4.2532716][-4.2299428 -4.2262282 -4.2054863 -4.1749954 -4.1386909 -4.0808582 -3.9833865 -3.8857915 -3.8964703 -4.0104079 -4.1302657 -4.2119446 -4.2551885 -4.2593102 -4.2372208][-4.2183838 -4.2022057 -4.1728764 -4.1338282 -4.0912147 -4.032547 -3.9314089 -3.8148947 -3.8216944 -3.9627757 -4.1066322 -4.2008567 -4.2433872 -4.2479386 -4.2297864][-4.2155252 -4.189887 -4.1512909 -4.1039577 -4.053637 -3.9995534 -3.9292011 -3.8506455 -3.8574457 -3.9759123 -4.1077447 -4.1984477 -4.2361903 -4.2424893 -4.2294741][-4.2136593 -4.1913052 -4.151607 -4.1024942 -4.0543256 -4.0205288 -3.9902859 -3.9534681 -3.9549108 -4.0327458 -4.1384854 -4.2134933 -4.2396016 -4.2406659 -4.2265263][-4.2162457 -4.2028179 -4.1675444 -4.121181 -4.0866566 -4.0788293 -4.0756593 -4.0622187 -4.0617228 -4.107439 -4.181396 -4.2367253 -4.256166 -4.2515292 -4.2372384][-4.2279835 -4.2130375 -4.1761684 -4.1369638 -4.1215439 -4.1363587 -4.149394 -4.1475039 -4.1436567 -4.1650581 -4.2107062 -4.2527342 -4.2715964 -4.2677307 -4.2575645][-4.242393 -4.2214327 -4.1812396 -4.1487718 -4.1468506 -4.1734915 -4.1978731 -4.2035756 -4.1959934 -4.2006488 -4.2276926 -4.2626271 -4.283783 -4.2836013 -4.2757621][-4.2536373 -4.229 -4.1906514 -4.1627693 -4.166625 -4.1956005 -4.2215028 -4.2269192 -4.2157068 -4.2144561 -4.2353234 -4.2660742 -4.2882843 -4.2918663 -4.2844019][-4.2631779 -4.2417078 -4.2128615 -4.1934652 -4.1976452 -4.2194219 -4.2315416 -4.2240515 -4.2087445 -4.2081833 -4.2285762 -4.2580247 -4.281496 -4.2888818 -4.28059]]...]
INFO - root - 2017-12-06 00:51:22.702670: step 59710, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:52m:01s remains)
INFO - root - 2017-12-06 00:51:31.160703: step 59720, loss = 2.08, batch loss = 2.03 (9.5 examples/sec; 0.843 sec/batch; 63h:51m:55s remains)
INFO - root - 2017-12-06 00:51:39.571862: step 59730, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 62h:05m:49s remains)
INFO - root - 2017-12-06 00:51:48.011915: step 59740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.859 sec/batch; 65h:02m:57s remains)
INFO - root - 2017-12-06 00:51:56.593915: step 59750, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.841 sec/batch; 63h:44m:06s remains)
INFO - root - 2017-12-06 00:52:05.161110: step 59760, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 64h:47m:31s remains)
INFO - root - 2017-12-06 00:52:13.607842: step 59770, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 63h:56m:44s remains)
INFO - root - 2017-12-06 00:52:22.094164: step 59780, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 65h:18m:38s remains)
INFO - root - 2017-12-06 00:52:30.630894: step 59790, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 64h:23m:02s remains)
INFO - root - 2017-12-06 00:52:39.119648: step 59800, loss = 2.08, batch loss = 2.03 (8.8 examples/sec; 0.910 sec/batch; 68h:53m:58s remains)
2017-12-06 00:52:39.933387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3090577 -4.3119936 -4.3135366 -4.313982 -4.3157578 -4.318666 -4.3219 -4.3246932 -4.3226972 -4.3196287 -4.3184242 -4.3188643 -4.3163977 -4.3163047 -4.3189445][-4.290535 -4.2900429 -4.2884207 -4.28883 -4.2916312 -4.294879 -4.2994547 -4.3030009 -4.2981057 -4.2905064 -4.2856164 -4.2848687 -4.2819343 -4.2847075 -4.2943087][-4.2667451 -4.2574124 -4.2477894 -4.24681 -4.2526112 -4.2583513 -4.2620826 -4.259995 -4.2490997 -4.2396712 -4.2330923 -4.2317333 -4.2299542 -4.2366476 -4.2561636][-4.2420349 -4.2228312 -4.2034583 -4.1956778 -4.1996408 -4.2038074 -4.2010293 -4.1860819 -4.1703 -4.1722717 -4.1738009 -4.1753831 -4.1747627 -4.1835718 -4.2123742][-4.2212715 -4.19 -4.1585121 -4.1394176 -4.1376028 -4.1360893 -4.1209331 -4.085371 -4.0682917 -4.0986238 -4.1264286 -4.1392169 -4.1417141 -4.1529512 -4.1867876][-4.1974177 -4.1525073 -4.1058455 -4.0702128 -4.052381 -4.0336504 -3.9930787 -3.9243052 -3.9075859 -3.9858809 -4.0613537 -4.1010017 -4.1190457 -4.1343541 -4.1681943][-4.1905346 -4.1376753 -4.0795388 -4.0280151 -3.988353 -3.9421995 -3.8649969 -3.7570918 -3.7431259 -3.8721228 -3.9986262 -4.0709505 -4.1005554 -4.1131163 -4.1460638][-4.1945033 -4.1385484 -4.0756369 -4.0146103 -3.9592221 -3.9028668 -3.8249557 -3.7234409 -3.7240105 -3.8676014 -4.0040174 -4.079988 -4.1026268 -4.1063218 -4.1362476][-4.2127047 -4.1642609 -4.1061769 -4.0468225 -3.993202 -3.9499066 -3.9065537 -3.8527112 -3.8578763 -3.9569468 -4.0526919 -4.1034775 -4.1091318 -4.1059923 -4.1311073][-4.2285628 -4.192358 -4.1458912 -4.0976219 -4.0539589 -4.0281029 -4.014658 -3.9916968 -3.987525 -4.0340695 -4.0835009 -4.1051407 -4.0948739 -4.0907574 -4.1216369][-4.2402363 -4.2116165 -4.1787391 -4.1442862 -4.1119003 -4.0968366 -4.0994806 -4.091486 -4.0817089 -4.0972676 -4.1137028 -4.1131873 -4.0966783 -4.0994329 -4.139142][-4.2565594 -4.2353144 -4.2135921 -4.1936445 -4.1749539 -4.1664467 -4.1719618 -4.173696 -4.1673994 -4.1685457 -4.1687708 -4.1607671 -4.1499095 -4.1600003 -4.1978889][-4.2767463 -4.2652764 -4.2554674 -4.247808 -4.23973 -4.2368789 -4.2434754 -4.24662 -4.2396827 -4.2369261 -4.2363844 -4.2318511 -4.2293253 -4.2398329 -4.2649655][-4.2968297 -4.2925062 -4.2895169 -4.2870054 -4.2833719 -4.2837267 -4.2910614 -4.2941351 -4.29039 -4.2894354 -4.2912397 -4.2906146 -4.2919712 -4.2980323 -4.3105888][-4.3134131 -4.3124633 -4.3115187 -4.3094816 -4.3077445 -4.309041 -4.31299 -4.3144383 -4.3135343 -4.3149185 -4.3187532 -4.3218203 -4.3242621 -4.3278356 -4.331748]]...]
INFO - root - 2017-12-06 00:52:48.360779: step 59810, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 64h:30m:04s remains)
INFO - root - 2017-12-06 00:52:56.864763: step 59820, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 64h:39m:02s remains)
INFO - root - 2017-12-06 00:53:05.468296: step 59830, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 66h:06m:13s remains)
INFO - root - 2017-12-06 00:53:13.836499: step 59840, loss = 2.02, batch loss = 1.96 (9.4 examples/sec; 0.853 sec/batch; 64h:34m:50s remains)
INFO - root - 2017-12-06 00:53:22.341661: step 59850, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.883 sec/batch; 66h:54m:32s remains)
INFO - root - 2017-12-06 00:53:30.835805: step 59860, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.841 sec/batch; 63h:41m:08s remains)
INFO - root - 2017-12-06 00:53:39.424498: step 59870, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.873 sec/batch; 66h:05m:59s remains)
INFO - root - 2017-12-06 00:53:47.970156: step 59880, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:24m:24s remains)
INFO - root - 2017-12-06 00:53:56.377746: step 59890, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 64h:17m:10s remains)
INFO - root - 2017-12-06 00:54:04.896559: step 59900, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.864 sec/batch; 65h:25m:20s remains)
2017-12-06 00:54:05.632826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2849927 -4.2695851 -4.2675548 -4.278275 -4.2908292 -4.3017392 -4.3135881 -4.3254514 -4.328557 -4.3233552 -4.3101683 -4.2949739 -4.287364 -4.287 -4.280283][-4.2653713 -4.2454491 -4.2385979 -4.2452359 -4.2519813 -4.2604675 -4.2767944 -4.296073 -4.3061547 -4.3065205 -4.2952533 -4.2800636 -4.2766323 -4.2839413 -4.2814884][-4.2682037 -4.2439446 -4.2270465 -4.220593 -4.2143583 -4.2151389 -4.2338271 -4.2630768 -4.2839608 -4.2917347 -4.2823558 -4.2669687 -4.2669883 -4.2795506 -4.2796326][-4.292058 -4.2637968 -4.2347388 -4.2124791 -4.1915317 -4.17931 -4.1919093 -4.2290621 -4.262712 -4.281858 -4.2793136 -4.268044 -4.2695289 -4.2820091 -4.282074][-4.3160796 -4.286921 -4.2475491 -4.211165 -4.1746111 -4.1453576 -4.1470394 -4.18684 -4.2304807 -4.2620921 -4.2708511 -4.2688217 -4.2733665 -4.2847219 -4.2883248][-4.324996 -4.2955117 -4.2474065 -4.1961484 -4.1433058 -4.09505 -4.0815735 -4.1193247 -4.1714911 -4.2185745 -4.2463641 -4.2608089 -4.2757506 -4.2913594 -4.2998266][-4.3197608 -4.2899041 -4.2338214 -4.1669741 -4.0968547 -4.0292597 -3.9963543 -4.0288115 -4.0922847 -4.1581631 -4.2101517 -4.2463589 -4.2757683 -4.2974539 -4.310607][-4.3022847 -4.2706842 -4.2105856 -4.1332068 -4.0473742 -3.95394 -3.8944964 -3.9202461 -4.0000873 -4.0905795 -4.1660595 -4.2219124 -4.2627225 -4.288568 -4.305541][-4.2832355 -4.2505913 -4.19666 -4.1236691 -4.0329742 -3.9191384 -3.8296332 -3.8387647 -3.9279583 -4.0396051 -4.1339197 -4.2025828 -4.2483878 -4.276824 -4.2969446][-4.2668586 -4.2373233 -4.2006855 -4.1484442 -4.0732355 -3.9701345 -3.8779454 -3.8643284 -3.9352369 -4.0412226 -4.1359711 -4.20474 -4.246666 -4.2717543 -4.2897711][-4.2531033 -4.2253661 -4.207541 -4.1862307 -4.142303 -4.074173 -4.0107088 -3.9929867 -4.033989 -4.1088915 -4.1802182 -4.233674 -4.2635555 -4.2802615 -4.2905235][-4.245769 -4.2161422 -4.2067857 -4.2088795 -4.197998 -4.1699128 -4.1439395 -4.1376367 -4.1616492 -4.2049608 -4.2458525 -4.2747083 -4.2860603 -4.2911444 -4.2931104][-4.25339 -4.2219119 -4.2108412 -4.22069 -4.2313714 -4.2318811 -4.2322636 -4.2357802 -4.2515965 -4.2736168 -4.2904582 -4.2986207 -4.2946815 -4.2905436 -4.2883782][-4.274816 -4.244472 -4.2275286 -4.2328906 -4.2481561 -4.261107 -4.2751379 -4.28604 -4.2983885 -4.3070793 -4.3072762 -4.3005977 -4.2842813 -4.27053 -4.2629943][-4.2942777 -4.2683206 -4.24844 -4.24851 -4.2616191 -4.2753983 -4.291286 -4.3021145 -4.3114095 -4.3144593 -4.3079081 -4.2937117 -4.2698579 -4.2486959 -4.2378807]]...]
INFO - root - 2017-12-06 00:54:14.110358: step 59910, loss = 2.03, batch loss = 1.98 (10.3 examples/sec; 0.776 sec/batch; 58h:45m:22s remains)
INFO - root - 2017-12-06 00:54:22.529166: step 59920, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 63h:38m:12s remains)
INFO - root - 2017-12-06 00:54:31.059642: step 59930, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 64h:37m:49s remains)
INFO - root - 2017-12-06 00:54:39.511562: step 59940, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 65h:43m:47s remains)
INFO - root - 2017-12-06 00:54:47.906538: step 59950, loss = 2.03, batch loss = 1.98 (9.7 examples/sec; 0.823 sec/batch; 62h:17m:57s remains)
INFO - root - 2017-12-06 00:54:56.258057: step 59960, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 63h:20m:17s remains)
INFO - root - 2017-12-06 00:55:04.677402: step 59970, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 65h:43m:30s remains)
INFO - root - 2017-12-06 00:55:13.275719: step 59980, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 65h:48m:22s remains)
INFO - root - 2017-12-06 00:55:21.712634: step 59990, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.847 sec/batch; 64h:07m:22s remains)
INFO - root - 2017-12-06 00:55:30.202246: step 60000, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 65h:23m:50s remains)
2017-12-06 00:55:31.049331: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2966304 -4.2986512 -4.2993579 -4.2944846 -4.2906675 -4.290628 -4.292891 -4.2983441 -4.305829 -4.3101082 -4.3109612 -4.309361 -4.3094959 -4.310132 -4.308105][-4.2672634 -4.26987 -4.2680793 -4.25879 -4.2510886 -4.2494841 -4.2530785 -4.2626705 -4.2763062 -4.283689 -4.2853189 -4.2861395 -4.2908974 -4.2938228 -4.2903132][-4.2197232 -4.2217388 -4.2154536 -4.1996393 -4.1850238 -4.1793346 -4.1827025 -4.198688 -4.223609 -4.2402058 -4.2461786 -4.2504339 -4.2615714 -4.267324 -4.2606091][-4.1788893 -4.1745405 -4.1580749 -4.132297 -4.1123028 -4.1050467 -4.109622 -4.1297088 -4.1656222 -4.1961265 -4.2101636 -4.2159309 -4.2314434 -4.2415276 -4.2364483][-4.1446342 -4.1309342 -4.1018763 -4.065825 -4.0444093 -4.0415759 -4.0460467 -4.0654278 -4.1078906 -4.1511459 -4.174778 -4.1815205 -4.1987143 -4.2116718 -4.2140269][-4.1120543 -4.0871959 -4.0459857 -4.0036874 -3.9892 -3.9965346 -4.0017815 -4.0166926 -4.0572362 -4.1063943 -4.135911 -4.1462488 -4.1615181 -4.1767373 -4.1871881][-4.0953345 -4.0598454 -4.0133972 -3.9727845 -3.9672217 -3.9824262 -3.9889691 -3.9967973 -4.0325022 -4.0848112 -4.117908 -4.1283908 -4.1413751 -4.1529417 -4.164844][-4.0896087 -4.05617 -4.0258322 -4.0000358 -3.9948411 -4.00229 -4.002646 -4.000761 -4.0288682 -4.0827231 -4.124423 -4.1390872 -4.1493297 -4.1546845 -4.1619167][-4.0839677 -4.0703831 -4.0665412 -4.0548315 -4.044035 -4.0375957 -4.0295029 -4.0191841 -4.0362434 -4.0871177 -4.1337557 -4.1512403 -4.1584764 -4.1604395 -4.16656][-4.087738 -4.0981483 -4.11255 -4.1111712 -4.0985746 -4.0838056 -4.0686183 -4.0531063 -4.0599837 -4.0981379 -4.1389093 -4.1567683 -4.1619668 -4.1632109 -4.1716084][-4.0980363 -4.1244535 -4.1504631 -4.1579661 -4.1471791 -4.1280284 -4.1086831 -4.095005 -4.0991898 -4.1251311 -4.1520171 -4.1658096 -4.1716294 -4.1752081 -4.1846771][-4.1091218 -4.1462994 -4.179183 -4.1914153 -4.1805749 -4.1579337 -4.1380162 -4.1286378 -4.1349549 -4.1541061 -4.1713123 -4.1838098 -4.1930361 -4.1994553 -4.2083192][-4.1241918 -4.1648221 -4.1967807 -4.2031589 -4.1876249 -4.1680512 -4.1555381 -4.1540713 -4.1638689 -4.1791129 -4.1899657 -4.2034731 -4.2174249 -4.2251163 -4.2320747][-4.1449766 -4.18105 -4.207509 -4.2095337 -4.1943994 -4.1827922 -4.1790948 -4.1827168 -4.1908174 -4.1992478 -4.2036896 -4.2175579 -4.2325826 -4.2386346 -4.2427564][-4.18379 -4.2125244 -4.2302556 -4.230473 -4.2192664 -4.2123189 -4.2106986 -4.2145319 -4.2213941 -4.2262626 -4.2290158 -4.2422047 -4.2542167 -4.25757 -4.2582788]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-lrhigh/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-06 00:55:40.364971: step 60010, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 63h:22m:39s remains)
INFO - root - 2017-12-06 00:55:48.840961: step 60020, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 64h:55m:28s remains)
INFO - root - 2017-12-06 00:55:57.341955: step 60030, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.828 sec/batch; 62h:41m:07s remains)
INFO - root - 2017-12-06 00:56:05.719397: step 60040, loss = 2.06, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 62h:49m:23s remains)
INFO - root - 2017-12-06 00:56:14.296722: step 60050, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 66h:20m:24s remains)
INFO - root - 2017-12-06 00:56:22.877504: step 60060, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 65h:09m:23s remains)
INFO - root - 2017-12-06 00:56:31.402909: step 60070, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.857 sec/batch; 64h:52m:14s remains)
INFO - root - 2017-12-06 00:56:39.993983: step 60080, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 66h:39m:49s remains)
INFO - root - 2017-12-06 00:56:48.537444: step 60090, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 64h:58m:37s remains)
INFO - root - 2017-12-06 00:56:57.156137: step 60100, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 65h:31m:59s remains)
2017-12-06 00:56:57.978960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1724439 -4.19476 -4.1833277 -4.1522036 -4.1260967 -4.1281672 -4.163444 -4.2026668 -4.2342515 -4.23968 -4.2386546 -4.2432 -4.2475133 -4.2569947 -4.2725244][-4.147275 -4.175714 -4.1668935 -4.1304269 -4.0915451 -4.0898757 -4.1366382 -4.1857867 -4.2251453 -4.2299547 -4.2250586 -4.2240505 -4.232461 -4.2457876 -4.262219][-4.1479225 -4.1750994 -4.1694212 -4.1336565 -4.0863833 -4.0778012 -4.124752 -4.1761765 -4.2169056 -4.2219639 -4.2145362 -4.2106457 -4.2242908 -4.2429481 -4.2592506][-4.1576352 -4.1745992 -4.1700583 -4.1371889 -4.0862751 -4.0666089 -4.1111012 -4.1723018 -4.218801 -4.2272816 -4.2190681 -4.2145324 -4.2315817 -4.2500696 -4.2640705][-4.1620255 -4.171082 -4.1663756 -4.13581 -4.0830855 -4.0567379 -4.1030703 -4.17537 -4.224402 -4.23098 -4.2179742 -4.2105389 -4.2291446 -4.2487354 -4.26503][-4.1560631 -4.1690407 -4.16837 -4.1356053 -4.0747113 -4.0333705 -4.0743546 -4.1534729 -4.2067719 -4.2130222 -4.1968732 -4.1920838 -4.2151461 -4.2406669 -4.2639728][-4.1474185 -4.1697216 -4.1734304 -4.1314025 -4.0493226 -3.9792726 -4.0068536 -4.0957503 -4.1612744 -4.1760941 -4.1632557 -4.1639905 -4.1923547 -4.226542 -4.2578359][-4.1268535 -4.1494846 -4.1462173 -4.0888243 -3.9781809 -3.8746552 -3.8969762 -4.00796 -4.0953145 -4.1241713 -4.1207843 -4.1292753 -4.1628728 -4.2023349 -4.2392716][-4.1052971 -4.12317 -4.1146827 -4.05224 -3.9358284 -3.8230433 -3.8536372 -3.9774969 -4.0733485 -4.1102915 -4.1185346 -4.1311913 -4.1606703 -4.1954193 -4.2314272][-4.1220007 -4.1420488 -4.14029 -4.0936031 -4.0009875 -3.9132016 -3.9444458 -4.0433064 -4.1183567 -4.1473188 -4.1526394 -4.1628971 -4.1830831 -4.2069349 -4.2371554][-4.1679215 -4.182106 -4.1780157 -4.1472788 -4.0817752 -4.017282 -4.0376725 -4.1026025 -4.1570568 -4.1799421 -4.18186 -4.1923103 -4.2063026 -4.2238178 -4.24928][-4.2184472 -4.2223244 -4.2128286 -4.1913743 -4.1467981 -4.1024742 -4.1139407 -4.1551218 -4.1957607 -4.2160673 -4.2184277 -4.2264738 -4.2346516 -4.2483339 -4.2697783][-4.2587514 -4.2547836 -4.2440171 -4.228775 -4.2017078 -4.1771278 -4.1848035 -4.2130775 -4.2420182 -4.2536955 -4.255043 -4.2585444 -4.260478 -4.2693491 -4.2881274][-4.2836576 -4.2793784 -4.27303 -4.2647696 -4.2516336 -4.2406058 -4.2459469 -4.2645087 -4.2828722 -4.2869515 -4.2855039 -4.2844934 -4.2851233 -4.2929525 -4.3073621][-4.3063407 -4.3030057 -4.3011904 -4.2991729 -4.2943435 -4.2897315 -4.2929449 -4.3041096 -4.313839 -4.3143382 -4.3125567 -4.3104997 -4.310595 -4.3158031 -4.3252711]]...]
INFO - root - 2017-12-06 00:57:06.534077: step 60110, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 66h:31m:33s remains)
INFO - root - 2017-12-06 00:57:15.074878: step 60120, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 64h:27m:01s remains)
INFO - root - 2017-12-06 00:57:23.629713: step 60130, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.856 sec/batch; 64h:46m:52s remains)
INFO - root - 2017-12-06 00:57:32.086814: step 60140, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 63h:06m:30s remains)
INFO - root - 2017-12-06 00:57:40.761221: step 60150, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.846 sec/batch; 64h:02m:22s remains)
INFO - root - 2017-12-06 00:57:49.253250: step 60160, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 65h:45m:58s remains)
INFO - root - 2017-12-06 00:57:57.752454: step 60170, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 64h:11m:30s remains)
INFO - root - 2017-12-06 00:58:06.424319: step 60180, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 65h:14m:26s remains)
INFO - root - 2017-12-06 00:58:14.966216: step 60190, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 65h:31m:27s remains)
INFO - root - 2017-12-06 00:58:23.493391: step 60200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:21m:56s remains)
2017-12-06 00:58:24.397419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2392755 -4.2467823 -4.2482657 -4.2429252 -4.236402 -4.2236176 -4.2133069 -4.2070446 -4.2239323 -4.2532763 -4.27193 -4.2868357 -4.3032389 -4.3200841 -4.3328586][-4.2167239 -4.22616 -4.2230768 -4.2148495 -4.208427 -4.1898794 -4.1693692 -4.1636939 -4.191905 -4.2298675 -4.2549825 -4.2768192 -4.2988992 -4.3187675 -4.3328619][-4.1988072 -4.2124352 -4.2041507 -4.1912332 -4.182508 -4.1547356 -4.122735 -4.1211224 -4.1684203 -4.2163157 -4.2445855 -4.2755747 -4.3032508 -4.3227711 -4.3346543][-4.1780777 -4.1966724 -4.1847153 -4.1687241 -4.154552 -4.1125765 -4.0592146 -4.0554152 -4.1243987 -4.1892142 -4.2279081 -4.2712288 -4.3064632 -4.3267632 -4.3371067][-4.1635485 -4.1881685 -4.175292 -4.155859 -4.1298838 -4.0636754 -3.9797058 -3.9738865 -4.067596 -4.1544528 -4.2061853 -4.2615504 -4.3037944 -4.3260746 -4.3358665][-4.1586604 -4.1934052 -4.1818886 -4.1567922 -4.1123958 -4.0064683 -3.8720257 -3.8672488 -4.0053291 -4.1246572 -4.1922073 -4.2569847 -4.305656 -4.3288069 -4.3369937][-4.1752148 -4.2211409 -4.2112813 -4.1791315 -4.1184053 -3.979522 -3.789979 -3.7750449 -3.9517629 -4.1004791 -4.1846089 -4.256846 -4.3088212 -4.3314981 -4.3383112][-4.2064247 -4.2609873 -4.2547717 -4.2245479 -4.1645875 -4.0298362 -3.8458302 -3.8113229 -3.9580131 -4.0971489 -4.1803608 -4.2523513 -4.3065252 -4.3298197 -4.3369288][-4.2264934 -4.2834787 -4.2869725 -4.2660022 -4.2164035 -4.1131535 -3.9785259 -3.9355426 -4.0136027 -4.1093512 -4.1797342 -4.249095 -4.3038268 -4.3279357 -4.3367348][-4.2444324 -4.2952242 -4.3054576 -4.2964873 -4.261539 -4.1897535 -4.096076 -4.0496397 -4.0739088 -4.1261787 -4.181469 -4.247982 -4.3034487 -4.3288536 -4.3385611][-4.2588062 -4.3005652 -4.3102207 -4.3088083 -4.2908511 -4.244225 -4.1825347 -4.1418738 -4.1369376 -4.1625566 -4.20464 -4.2633495 -4.3132091 -4.335217 -4.3414521][-4.2675357 -4.2979226 -4.305048 -4.3084488 -4.302505 -4.2762389 -4.2347836 -4.1993842 -4.1879892 -4.2065759 -4.2443438 -4.2916703 -4.3301806 -4.3458662 -4.3457651][-4.2766876 -4.294035 -4.294508 -4.2989488 -4.3008204 -4.2870388 -4.2604327 -4.2300081 -4.2207 -4.2398567 -4.274323 -4.3132877 -4.3421121 -4.3513803 -4.3480492][-4.2908649 -4.2954888 -4.2868977 -4.2860866 -4.2896914 -4.2832074 -4.2675805 -4.2507405 -4.2504873 -4.2721009 -4.3022633 -4.3318319 -4.3507872 -4.3545475 -4.3497438][-4.3149219 -4.3104315 -4.2931652 -4.2846818 -4.2834282 -4.2777734 -4.2697945 -4.2669687 -4.2759118 -4.2960544 -4.3206925 -4.3424611 -4.3541718 -4.3549833 -4.349649]]...]
INFO - root - 2017-12-06 00:58:32.993222: step 60210, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 65h:55m:18s remains)
INFO - root - 2017-12-06 00:58:41.631461: step 60220, loss = 2.10, batch loss = 2.04 (9.2 examples/sec; 0.865 sec/batch; 65h:25m:34s remains)
INFO - root - 2017-12-06 00:58:50.204943: step 60230, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 65h:12m:23s remains)
INFO - root - 2017-12-06 00:58:58.600636: step 60240, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 65h:36m:20s remains)
INFO - root - 2017-12-06 00:59:07.228149: step 60250, loss = 2.06, batch loss = 2.00 (8.8 examples/sec; 0.907 sec/batch; 68h:35m:35s remains)
INFO - root - 2017-12-06 00:59:15.818310: step 60260, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 64h:37m:41s remains)
INFO - root - 2017-12-06 00:59:24.394528: step 60270, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 65h:48m:30s remains)
INFO - root - 2017-12-06 00:59:32.964452: step 60280, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.874 sec/batch; 66h:06m:23s remains)
INFO - root - 2017-12-06 00:59:41.556301: step 60290, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 66h:08m:08s remains)
INFO - root - 2017-12-06 00:59:50.116461: step 60300, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.844 sec/batch; 63h:47m:01s remains)
2017-12-06 00:59:50.902035: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1734552 -4.1832104 -4.1827555 -4.1820908 -4.1875224 -4.2024474 -4.2177615 -4.228003 -4.2272282 -4.2215419 -4.2169385 -4.2218504 -4.2345281 -4.2527618 -4.28454][-4.1994591 -4.1966429 -4.1801534 -4.1650772 -4.1645641 -4.1839786 -4.20332 -4.2192121 -4.2226038 -4.2234168 -4.2174592 -4.2178755 -4.23085 -4.2492852 -4.2819843][-4.2161994 -4.2012272 -4.1696038 -4.1435242 -4.1423521 -4.1624856 -4.1780062 -4.1944652 -4.2011395 -4.2097149 -4.2079029 -4.2071557 -4.2209072 -4.241652 -4.2777109][-4.2198143 -4.1950264 -4.1526794 -4.1217055 -4.1198244 -4.1321397 -4.1378889 -4.1531048 -4.1687069 -4.1858053 -4.1949987 -4.2011523 -4.2176571 -4.2403407 -4.2798858][-4.2162256 -4.1836042 -4.1296811 -4.0946064 -4.08743 -4.0849409 -4.0721545 -4.0905061 -4.1241288 -4.1502714 -4.1732 -4.1939645 -4.218915 -4.2432551 -4.2841372][-4.1998415 -4.1610651 -4.0945492 -4.04985 -4.0318151 -4.0064216 -3.961102 -3.9848008 -4.05019 -4.0944166 -4.1332521 -4.1731553 -4.2129664 -4.2433281 -4.2841206][-4.1783171 -4.1350818 -4.0628047 -4.0089078 -3.9719689 -3.9086175 -3.8169031 -3.846118 -3.9517527 -4.0176706 -4.0694032 -4.1237946 -4.179275 -4.2253718 -4.2725811][-4.1765285 -4.12819 -4.0543308 -3.9922831 -3.9370456 -3.8491426 -3.7405825 -3.7805057 -3.8997881 -3.9709909 -4.0216169 -4.0789304 -4.1442962 -4.2060776 -4.2605581][-4.2030206 -4.1544614 -4.0827961 -4.0198073 -3.9623377 -3.8913593 -3.8263407 -3.8586631 -3.9342949 -3.9807169 -4.0190029 -4.0726342 -4.1397285 -4.2061257 -4.2588925][-4.2498307 -4.2102904 -4.1524286 -4.1000452 -4.0552125 -4.0134888 -3.9881628 -4.0050206 -4.0360036 -4.0586233 -4.0796671 -4.1206551 -4.1796374 -4.2370629 -4.2763662][-4.292922 -4.2668324 -4.231637 -4.201179 -4.1729283 -4.1510935 -4.1423626 -4.1447306 -4.1504712 -4.1609993 -4.1728578 -4.1987348 -4.2377667 -4.27695 -4.3011765][-4.3187852 -4.3058724 -4.2892532 -4.2769303 -4.2640829 -4.2547069 -4.2510233 -4.2459407 -4.2422414 -4.2490649 -4.2556019 -4.2671285 -4.285481 -4.3061123 -4.3190255][-4.3329329 -4.3273578 -4.3219323 -4.3181396 -4.3142142 -4.3107991 -4.3102145 -4.3063011 -4.3012428 -4.304903 -4.3089 -4.3119321 -4.3163719 -4.3245087 -4.329824][-4.3402734 -4.3368225 -4.3368635 -4.3384981 -4.3392982 -4.3381486 -4.3378243 -4.3371658 -4.3344035 -4.3345857 -4.3348656 -4.3346272 -4.3340778 -4.3369088 -4.3382511][-4.3416319 -4.3387952 -4.3401141 -4.341466 -4.3420868 -4.341217 -4.3410845 -4.3419867 -4.34181 -4.34094 -4.3397655 -4.3392458 -4.338305 -4.33925 -4.3394752]]...]
INFO - root - 2017-12-06 00:59:59.421680: step 60310, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 64h:00m:48s remains)
INFO - root - 2017-12-06 01:00:08.119406: step 60320, loss = 2.10, batch loss = 2.05 (9.0 examples/sec; 0.887 sec/batch; 67h:05m:16s remains)
INFO - root - 2017-12-06 01:00:16.682567: step 60330, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 65h:58m:16s remains)
INFO - root - 2017-12-06 01:00:25.181094: step 60340, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 65h:49m:59s remains)
INFO - root - 2017-12-06 01:00:33.660668: step 60350, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 65h:36m:36s remains)
INFO - root - 2017-12-06 01:00:42.237521: step 60360, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 65h:29m:42s remains)
INFO - root - 2017-12-06 01:00:50.832621: step 60370, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.849 sec/batch; 64h:09m:55s remains)
INFO - root - 2017-12-06 01:00:59.339092: step 60380, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 67h:08m:48s remains)
INFO - root - 2017-12-06 01:01:07.891780: step 60390, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 62h:57m:26s remains)
INFO - root - 2017-12-06 01:01:16.545110: step 60400, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 65h:30m:39s remains)
2017-12-06 01:01:17.380294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1953826 -4.2043405 -4.2145867 -4.2297192 -4.2483563 -4.259356 -4.2532835 -4.2408142 -4.2348776 -4.2385697 -4.24501 -4.2483711 -4.2577477 -4.2747707 -4.2810941][-4.211843 -4.2201328 -4.2250361 -4.2349887 -4.2478366 -4.25246 -4.2414637 -4.2283607 -4.2236257 -4.228447 -4.2346735 -4.2374229 -4.2483706 -4.2691026 -4.28][-4.2484355 -4.2529297 -4.2498655 -4.2516813 -4.2568512 -4.2549629 -4.24121 -4.230669 -4.2292671 -4.2377353 -4.248117 -4.2538767 -4.2640128 -4.2803974 -4.2896032][-4.2486644 -4.2495875 -4.2445664 -4.2421179 -4.2425413 -4.234601 -4.2180343 -4.2110815 -4.2133322 -4.2259836 -4.2440491 -4.2582169 -4.268549 -4.2784362 -4.2815347][-4.1977897 -4.2004976 -4.1990609 -4.19349 -4.1825209 -4.1588068 -4.1325154 -4.1321445 -4.1464925 -4.1719084 -4.2064676 -4.2310705 -4.2414074 -4.2442689 -4.2366943][-4.1258378 -4.1328382 -4.1366768 -4.1227226 -4.0896435 -4.0356145 -3.9871554 -3.98679 -4.0150948 -4.0636778 -4.1227384 -4.1607375 -4.1752195 -4.171875 -4.1498003][-4.0933547 -4.0993776 -4.1005173 -4.0756736 -4.0225482 -3.9447579 -3.8795474 -3.8760707 -3.9060724 -3.9594593 -4.02583 -4.0676551 -4.0857549 -4.0769043 -4.0415015][-4.1168256 -4.1205249 -4.1180453 -4.0957284 -4.0545259 -3.9959381 -3.9465959 -3.9406626 -3.9539642 -3.9811215 -4.015008 -4.034544 -4.0448251 -4.0352936 -4.0044155][-4.1413884 -4.1439428 -4.1460071 -4.1412115 -4.1325788 -4.1153011 -4.0970235 -4.0931654 -4.0940666 -4.09723 -4.1008267 -4.0970087 -4.0961714 -4.0906816 -4.0753727][-4.1343102 -4.1364841 -4.1457458 -4.1568336 -4.1699443 -4.1823735 -4.1879873 -4.1930814 -4.1960177 -4.19591 -4.1918387 -4.1812091 -4.1772075 -4.1768346 -4.1737256][-4.127409 -4.1251454 -4.1335335 -4.1470013 -4.1651053 -4.1883678 -4.2079492 -4.2207956 -4.2299976 -4.2349634 -4.2331009 -4.22216 -4.2172818 -4.2223878 -4.2286963][-4.1485457 -4.1383653 -4.1356483 -4.1388192 -4.1506739 -4.1712232 -4.1911373 -4.2069559 -4.2198749 -4.2310185 -4.2336125 -4.2240806 -4.2199225 -4.2278132 -4.24116][-4.1730771 -4.1578436 -4.1434402 -4.1327529 -4.1341391 -4.144897 -4.1593428 -4.176569 -4.1926785 -4.2078934 -4.2147484 -4.2065763 -4.2029634 -4.2122955 -4.2282181][-4.192646 -4.1772861 -4.1554575 -4.1358404 -4.1298995 -4.1312437 -4.136323 -4.1486831 -4.1657305 -4.1848073 -4.1962409 -4.1916456 -4.1884365 -4.1956658 -4.2100229][-4.20514 -4.1927981 -4.1735082 -4.1563106 -4.1464624 -4.139863 -4.1331911 -4.1347647 -4.1478977 -4.1703634 -4.1882243 -4.1912842 -4.1919408 -4.1975946 -4.2099848]]...]
INFO - root - 2017-12-06 01:01:25.911432: step 60410, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 64h:05m:59s remains)
INFO - root - 2017-12-06 01:01:34.498968: step 60420, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 65h:39m:35s remains)
INFO - root - 2017-12-06 01:01:43.138520: step 60430, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 64h:47m:25s remains)
INFO - root - 2017-12-06 01:01:51.568316: step 60440, loss = 2.11, batch loss = 2.05 (9.7 examples/sec; 0.827 sec/batch; 62h:28m:36s remains)
INFO - root - 2017-12-06 01:02:00.063734: step 60450, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 64h:17m:07s remains)
INFO - root - 2017-12-06 01:02:08.650264: step 60460, loss = 2.02, batch loss = 1.96 (9.6 examples/sec; 0.836 sec/batch; 63h:12m:23s remains)
INFO - root - 2017-12-06 01:02:17.157107: step 60470, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 64h:44m:28s remains)
INFO - root - 2017-12-06 01:02:25.783312: step 60480, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 66h:43m:19s remains)
INFO - root - 2017-12-06 01:02:34.341100: step 60490, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 65h:13m:27s remains)
INFO - root - 2017-12-06 01:02:42.904748: step 60500, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 65h:15m:58s remains)
2017-12-06 01:02:43.686174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2338581 -4.2089124 -4.1809039 -4.1448393 -4.1065617 -4.07688 -4.055222 -4.06825 -4.1199403 -4.1798458 -4.2188797 -4.23985 -4.2640529 -4.2843304 -4.293632][-4.2470217 -4.2263627 -4.1951675 -4.1507039 -4.0997372 -4.0562954 -4.030127 -4.0394025 -4.0878177 -4.1502724 -4.1969342 -4.2239695 -4.2559085 -4.2803593 -4.2930717][-4.2508221 -4.2312808 -4.1925445 -4.140542 -4.0800757 -4.0286884 -4.006032 -4.0214438 -4.0687823 -4.1288195 -4.1782327 -4.2086906 -4.2423396 -4.2699151 -4.2872171][-4.2485795 -4.2274132 -4.1824155 -4.1218257 -4.0558548 -3.9978402 -3.9768264 -4.0002069 -4.0529423 -4.1120706 -4.1614757 -4.1948776 -4.2311268 -4.2614107 -4.2794843][-4.2405353 -4.2233386 -4.1770964 -4.1093454 -4.0370312 -3.9663334 -3.9428577 -3.9780543 -4.0464883 -4.1103435 -4.1615477 -4.1945562 -4.2310672 -4.2622042 -4.2762361][-4.2327924 -4.2246432 -4.1830273 -4.1107221 -4.0234675 -3.9345088 -3.9031425 -3.9553175 -4.0394597 -4.1133981 -4.1717653 -4.2060037 -4.2386575 -4.2649274 -4.2745824][-4.2303052 -4.2311983 -4.1965246 -4.1220107 -4.0151877 -3.899087 -3.8503881 -3.9129581 -4.0145822 -4.1042886 -4.1733103 -4.2142758 -4.2413111 -4.2572961 -4.2620988][-4.2327003 -4.2376947 -4.2122574 -4.1421061 -4.0240941 -3.8892543 -3.8256142 -3.8924127 -4.0088935 -4.1080685 -4.1775522 -4.2169909 -4.238512 -4.2460089 -4.2433867][-4.2258015 -4.2295465 -4.2163162 -4.1649532 -4.0645919 -3.9370081 -3.8690045 -3.9294252 -4.0413814 -4.1300068 -4.1850715 -4.2167506 -4.2340274 -4.2366371 -4.2275219][-4.2029781 -4.2053795 -4.2051558 -4.1764088 -4.1052785 -4.005125 -3.9422634 -3.9853535 -4.0796189 -4.1517143 -4.1895127 -4.2100554 -4.2209959 -4.2222314 -4.2154121][-4.1808934 -4.175375 -4.180707 -4.1690512 -4.1266742 -4.0591226 -4.0086808 -4.0298042 -4.0973015 -4.1479297 -4.1694412 -4.175303 -4.1828928 -4.19214 -4.1981916][-4.1685615 -4.1506085 -4.1554136 -4.161818 -4.149364 -4.1104269 -4.0681858 -4.0591159 -4.0921197 -4.1164546 -4.1216335 -4.1182828 -4.1309142 -4.1578169 -4.1810741][-4.1691117 -4.1421752 -4.1421657 -4.1653686 -4.1765928 -4.1611023 -4.125042 -4.0901308 -4.0925412 -4.098145 -4.0934043 -4.0860062 -4.1065297 -4.1517754 -4.1857686][-4.1768618 -4.151051 -4.1472354 -4.1793828 -4.205358 -4.2077184 -4.1792216 -4.1318197 -4.1105442 -4.1030331 -4.0979071 -4.0950913 -4.1211071 -4.1706123 -4.2050776][-4.1850767 -4.163579 -4.1654305 -4.2043252 -4.2379184 -4.2468009 -4.2257047 -4.1751146 -4.1381888 -4.1207743 -4.11784 -4.1211915 -4.1470628 -4.1894269 -4.2214479]]...]
INFO - root - 2017-12-06 01:02:52.457059: step 60510, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 66h:12m:23s remains)
INFO - root - 2017-12-06 01:03:01.024001: step 60520, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.860 sec/batch; 64h:56m:50s remains)
INFO - root - 2017-12-06 01:03:09.571226: step 60530, loss = 2.08, batch loss = 2.02 (8.9 examples/sec; 0.898 sec/batch; 67h:50m:36s remains)
INFO - root - 2017-12-06 01:03:17.989016: step 60540, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 66h:06m:33s remains)
INFO - root - 2017-12-06 01:03:26.577743: step 60550, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 63h:10m:43s remains)
INFO - root - 2017-12-06 01:03:35.158069: step 60560, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 65h:18m:07s remains)
INFO - root - 2017-12-06 01:03:43.583596: step 60570, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.842 sec/batch; 63h:37m:45s remains)
INFO - root - 2017-12-06 01:03:52.042619: step 60580, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 65h:38m:53s remains)
INFO - root - 2017-12-06 01:04:00.586225: step 60590, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 62h:55m:53s remains)
INFO - root - 2017-12-06 01:04:09.224121: step 60600, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 65h:04m:19s remains)
2017-12-06 01:04:09.998593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2478642 -4.26279 -4.2619267 -4.2447114 -4.2182736 -4.2050261 -4.2070785 -4.2146049 -4.2216716 -4.2267666 -4.2293739 -4.2316618 -4.2381506 -4.255579 -4.2752905][-4.2551069 -4.2810006 -4.2890353 -4.2745419 -4.2403269 -4.2092981 -4.2012696 -4.2117419 -4.2212143 -4.2245135 -4.2278938 -4.2299986 -4.2385931 -4.2630682 -4.2881927][-4.2721829 -4.302959 -4.3130374 -4.299973 -4.2604208 -4.2173047 -4.2006745 -4.216783 -4.2331963 -4.2361174 -4.23512 -4.2333989 -4.2398658 -4.2680411 -4.2966232][-4.2785492 -4.3079028 -4.3170652 -4.3031573 -4.2589803 -4.2068844 -4.1832466 -4.2057676 -4.2318683 -4.2361455 -4.2292056 -4.2277513 -4.2363853 -4.266726 -4.2953444][-4.2662444 -4.2928972 -4.3041697 -4.2926807 -4.2468724 -4.1878572 -4.1574821 -4.1867213 -4.2216077 -4.228147 -4.21757 -4.2181554 -4.2298775 -4.2597427 -4.2867441][-4.2342863 -4.2597828 -4.2755442 -4.2669106 -4.2214589 -4.1573515 -4.1201019 -4.155674 -4.2041392 -4.2184396 -4.2109332 -4.214623 -4.2261171 -4.2527585 -4.2774076][-4.1951876 -4.2206955 -4.2439947 -4.2393265 -4.19397 -4.1256771 -4.0833011 -4.1225414 -4.18102 -4.2063546 -4.2076917 -4.2155895 -4.2284536 -4.2530346 -4.2744884][-4.1635857 -4.1896639 -4.221067 -4.2232742 -4.181222 -4.1109514 -4.0650587 -4.1014223 -4.1581016 -4.1881509 -4.1969523 -4.2126284 -4.2311883 -4.256186 -4.2768559][-4.1370745 -4.1617079 -4.1986613 -4.2099843 -4.1766429 -4.111773 -4.0668283 -4.0908661 -4.136631 -4.1645107 -4.1789885 -4.2046976 -4.2311964 -4.2564831 -4.2772765][-4.1173973 -4.1358433 -4.1729021 -4.1915402 -4.1688266 -4.1148582 -4.0734363 -4.0834451 -4.1154666 -4.1408634 -4.1607332 -4.1988258 -4.2342257 -4.2583709 -4.2781048][-4.1109834 -4.1222486 -4.1555719 -4.1776161 -4.1670995 -4.1254454 -4.08707 -4.0849156 -4.1033864 -4.121398 -4.1451941 -4.1921277 -4.233674 -4.2589703 -4.2770672][-4.1118579 -4.1142535 -4.1410804 -4.1671171 -4.1698465 -4.1417484 -4.1111293 -4.1009221 -4.1063433 -4.1128793 -4.1375275 -4.1891956 -4.2343373 -4.2581005 -4.2715344][-4.12273 -4.1177869 -4.1353521 -4.1615891 -4.1719208 -4.1529059 -4.12893 -4.1162763 -4.110806 -4.1103878 -4.1391082 -4.1948113 -4.2403054 -4.2589817 -4.2633071][-4.1538496 -4.143486 -4.15161 -4.1713295 -4.1797357 -4.1650167 -4.1441016 -4.1302166 -4.1162515 -4.1102509 -4.1441689 -4.1995831 -4.2406197 -4.2528458 -4.2499776][-4.1947951 -4.1842937 -4.1903853 -4.2019463 -4.20687 -4.1954103 -4.1772108 -4.1570282 -4.1349969 -4.1232309 -4.1518555 -4.1972008 -4.2301397 -4.2389736 -4.2345991]]...]
INFO - root - 2017-12-06 01:04:18.613692: step 60610, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 66h:42m:30s remains)
INFO - root - 2017-12-06 01:04:27.168781: step 60620, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.880 sec/batch; 66h:28m:59s remains)
INFO - root - 2017-12-06 01:04:35.727321: step 60630, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.888 sec/batch; 67h:02m:27s remains)
INFO - root - 2017-12-06 01:04:44.175576: step 60640, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 63h:53m:23s remains)
INFO - root - 2017-12-06 01:04:52.645253: step 60650, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.848 sec/batch; 64h:01m:22s remains)
INFO - root - 2017-12-06 01:05:01.285936: step 60660, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 64h:12m:51s remains)
INFO - root - 2017-12-06 01:05:09.945229: step 60670, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.855 sec/batch; 64h:34m:22s remains)
INFO - root - 2017-12-06 01:05:18.398889: step 60680, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 62h:19m:03s remains)
INFO - root - 2017-12-06 01:05:26.780549: step 60690, loss = 2.06, batch loss = 2.00 (10.1 examples/sec; 0.790 sec/batch; 59h:37m:02s remains)
INFO - root - 2017-12-06 01:05:35.338183: step 60700, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.864 sec/batch; 65h:12m:00s remains)
2017-12-06 01:05:36.120027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2928605 -4.2977 -4.2980142 -4.2986412 -4.3000383 -4.3016443 -4.3025589 -4.3022251 -4.3015556 -4.3015141 -4.3024306 -4.3035932 -4.3038225 -4.3029342 -4.3009505][-4.2847142 -4.2922945 -4.2936516 -4.2956409 -4.2988691 -4.3015671 -4.3023734 -4.3009987 -4.299468 -4.29993 -4.3026047 -4.3050714 -4.305366 -4.3036723 -4.3003736][-4.2560267 -4.2673178 -4.2716527 -4.2755485 -4.2805567 -4.2839522 -4.2847042 -4.282342 -4.2807155 -4.2827821 -4.2878113 -4.291399 -4.2909985 -4.287466 -4.2823648][-4.2162228 -4.2311878 -4.2392373 -4.2450085 -4.2502308 -4.2526922 -4.2527876 -4.2495608 -4.2479439 -4.2509284 -4.2572241 -4.26085 -4.2583632 -4.2515492 -4.2445707][-4.17796 -4.1964145 -4.2069983 -4.2119122 -4.2158413 -4.2180791 -4.21831 -4.2147584 -4.2122273 -4.2135463 -4.2170024 -4.2182393 -4.2141275 -4.2069111 -4.2023706][-4.1599617 -4.181633 -4.1927295 -4.1936865 -4.1928091 -4.1933632 -4.1949692 -4.1932507 -4.1910853 -4.1899757 -4.1887121 -4.1871448 -4.1857967 -4.1853085 -4.189105][-4.1558576 -4.1779795 -4.1851578 -4.18005 -4.1745963 -4.1755037 -4.1816545 -4.1865435 -4.1903877 -4.1907263 -4.187675 -4.1863461 -4.1916733 -4.2005892 -4.2123547][-4.1683311 -4.1852293 -4.1867237 -4.1773796 -4.1715274 -4.1753249 -4.1885452 -4.2043695 -4.2180219 -4.22372 -4.2227349 -4.2243066 -4.2332878 -4.244421 -4.2560372][-4.1887803 -4.2002416 -4.1994805 -4.18968 -4.1844406 -4.190917 -4.2098927 -4.2346621 -4.2561731 -4.2677565 -4.2699938 -4.2725391 -4.28016 -4.2877827 -4.2950754][-4.2023735 -4.2156515 -4.2192063 -4.2109327 -4.2036629 -4.2075734 -4.226387 -4.2541451 -4.2812738 -4.2981315 -4.301774 -4.3026314 -4.3068709 -4.3107276 -4.3146195][-4.2106233 -4.2302928 -4.2391195 -4.2307072 -4.2174015 -4.21676 -4.2324438 -4.2588587 -4.2878661 -4.3069654 -4.3120351 -4.3122091 -4.3138094 -4.3148856 -4.3167319][-4.2179565 -4.2411213 -4.2508698 -4.2413735 -4.2263627 -4.2254562 -4.2403569 -4.2643929 -4.2899303 -4.306603 -4.3108134 -4.3100348 -4.3092737 -4.3087649 -4.3099589][-4.2316074 -4.251811 -4.2574329 -4.2479787 -4.2358723 -4.2397485 -4.2574563 -4.2782712 -4.297123 -4.3082957 -4.3097854 -4.3075237 -4.3044486 -4.3025432 -4.3038487][-4.2479329 -4.2589707 -4.2597876 -4.2530055 -4.2477288 -4.25745 -4.2765183 -4.2936311 -4.3063445 -4.3121395 -4.3113828 -4.3077946 -4.3027897 -4.2996926 -4.30054][-4.2543726 -4.2594652 -4.2608929 -4.2611556 -4.2636294 -4.2761955 -4.2930865 -4.3047485 -4.3116903 -4.3136916 -4.31184 -4.3078237 -4.3022523 -4.2985816 -4.2983904]]...]
INFO - root - 2017-12-06 01:05:44.646713: step 60710, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.859 sec/batch; 64h:50m:21s remains)
INFO - root - 2017-12-06 01:05:53.280269: step 60720, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 66h:13m:32s remains)
INFO - root - 2017-12-06 01:06:01.790281: step 60730, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 64h:05m:14s remains)
INFO - root - 2017-12-06 01:06:10.221491: step 60740, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.856 sec/batch; 64h:35m:34s remains)
INFO - root - 2017-12-06 01:06:18.651826: step 60750, loss = 2.03, batch loss = 1.97 (9.2 examples/sec; 0.867 sec/batch; 65h:26m:05s remains)
INFO - root - 2017-12-06 01:06:27.191110: step 60760, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.857 sec/batch; 64h:40m:53s remains)
INFO - root - 2017-12-06 01:06:35.865659: step 60770, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 64h:40m:26s remains)
INFO - root - 2017-12-06 01:06:44.350844: step 60780, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.878 sec/batch; 66h:15m:55s remains)
INFO - root - 2017-12-06 01:06:52.723080: step 60790, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 63h:37m:02s remains)
INFO - root - 2017-12-06 01:07:01.265215: step 60800, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 62h:41m:35s remains)
2017-12-06 01:07:02.147366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3238945 -4.3177347 -4.3137026 -4.312777 -4.3166203 -4.3227792 -4.3289409 -4.3354268 -4.3373566 -4.3343873 -4.3321133 -4.3314762 -4.3319197 -4.3331442 -4.3344846][-4.3101335 -4.2987714 -4.2907395 -4.2888584 -4.293222 -4.3014922 -4.3134561 -4.3284659 -4.3356643 -4.3321419 -4.3283663 -4.3242421 -4.3221378 -4.3224893 -4.3240261][-4.2815933 -4.2619562 -4.2441506 -4.2367859 -4.2403059 -4.2508488 -4.2703938 -4.2945213 -4.3080525 -4.3070121 -4.303586 -4.2962217 -4.2923975 -4.29272 -4.2976604][-4.238627 -4.2055941 -4.174181 -4.1580849 -4.1619244 -4.1787229 -4.2087646 -4.2432427 -4.2635741 -4.2661 -4.2627845 -4.25449 -4.250989 -4.2525663 -4.262248][-4.1827855 -4.132441 -4.0858631 -4.0656257 -4.07467 -4.0995965 -4.1414862 -4.1894932 -4.2181439 -4.2294369 -4.2310681 -4.2246547 -4.2223716 -4.223999 -4.2358732][-4.138566 -4.0724292 -4.0137515 -3.988276 -3.9937453 -4.0219674 -4.0702047 -4.120379 -4.15828 -4.186974 -4.1983747 -4.1986976 -4.2026744 -4.2089639 -4.22477][-4.1271749 -4.0514016 -3.9796317 -3.9349766 -3.9186664 -3.9337888 -3.9671793 -4.0008907 -4.0494509 -4.1079693 -4.141593 -4.162344 -4.1817822 -4.1983547 -4.2189436][-4.1475253 -4.0729084 -3.9996719 -3.9412169 -3.9008434 -3.8784027 -3.8605781 -3.8476019 -3.9034815 -3.9994392 -4.0622678 -4.1113262 -4.1541114 -4.1865978 -4.2173953][-4.1863561 -4.1254983 -4.0675044 -4.0119843 -3.9590344 -3.9006934 -3.8259156 -3.760751 -3.8096685 -3.9249194 -4.0095749 -4.0823369 -4.1436467 -4.1888967 -4.2297716][-4.216157 -4.1699266 -4.129343 -4.0905938 -4.0502124 -3.9952879 -3.9162304 -3.8484802 -3.8861837 -3.975142 -4.0441933 -4.1105523 -4.1695395 -4.2148962 -4.2553658][-4.2275958 -4.1897359 -4.1649451 -4.1492553 -4.1354318 -4.1065373 -4.0512929 -4.0039029 -4.0277972 -4.07947 -4.1223836 -4.1671195 -4.2103934 -4.2472606 -4.2785368][-4.2302084 -4.1949935 -4.1797171 -4.1847591 -4.1931872 -4.1860976 -4.1552706 -4.1261792 -4.1375661 -4.1617942 -4.188972 -4.2188721 -4.2473922 -4.2735357 -4.2955537][-4.2373624 -4.2044997 -4.1937332 -4.2072077 -4.2247338 -4.2299819 -4.2179952 -4.2019105 -4.204483 -4.2156329 -4.2343988 -4.2546911 -4.2728281 -4.2896247 -4.3058529][-4.2556911 -4.2291684 -4.2219729 -4.2346988 -4.2516646 -4.2649493 -4.2659183 -4.25712 -4.2552233 -4.2619877 -4.2743626 -4.28527 -4.2945108 -4.3030891 -4.3128171][-4.2752233 -4.2575979 -4.2549081 -4.2655072 -4.2780838 -4.29236 -4.2995367 -4.2956238 -4.293108 -4.2977023 -4.3065557 -4.314261 -4.3181353 -4.3192654 -4.3219414]]...]
INFO - root - 2017-12-06 01:07:10.669995: step 60810, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 65h:23m:54s remains)
INFO - root - 2017-12-06 01:07:19.229881: step 60820, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 65h:49m:56s remains)
INFO - root - 2017-12-06 01:07:27.834491: step 60830, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:51m:53s remains)
INFO - root - 2017-12-06 01:07:36.217493: step 60840, loss = 2.08, batch loss = 2.02 (10.1 examples/sec; 0.791 sec/batch; 59h:40m:19s remains)
INFO - root - 2017-12-06 01:07:44.774415: step 60850, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.847 sec/batch; 63h:55m:58s remains)
INFO - root - 2017-12-06 01:07:53.323972: step 60860, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 63h:31m:31s remains)
INFO - root - 2017-12-06 01:08:01.871115: step 60870, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.839 sec/batch; 63h:18m:13s remains)
INFO - root - 2017-12-06 01:08:10.442492: step 60880, loss = 2.09, batch loss = 2.03 (9.1 examples/sec; 0.876 sec/batch; 66h:05m:55s remains)
INFO - root - 2017-12-06 01:08:18.914023: step 60890, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 65h:54m:57s remains)
INFO - root - 2017-12-06 01:08:27.412075: step 60900, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 65h:24m:48s remains)
2017-12-06 01:08:28.200445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1360722 -4.1608648 -4.2001257 -4.2458243 -4.2867117 -4.3101611 -4.3080587 -4.2938714 -4.2761073 -4.2538195 -4.2289438 -4.2135916 -4.2067924 -4.1994042 -4.1877851][-4.1565671 -4.1776719 -4.2064013 -4.2447567 -4.2795105 -4.2970371 -4.2890782 -4.2737284 -4.2604122 -4.2472386 -4.2309637 -4.2149706 -4.203527 -4.1965556 -4.1921287][-4.1736526 -4.1918612 -4.2106919 -4.2356744 -4.2572622 -4.2608485 -4.2383661 -4.2151771 -4.211421 -4.216476 -4.2168536 -4.20761 -4.1919723 -4.1779704 -4.1748252][-4.1933336 -4.2116556 -4.2214284 -4.22857 -4.227138 -4.2046676 -4.1529136 -4.1197038 -4.1341453 -4.1709995 -4.1981277 -4.1975369 -4.1739964 -4.1485276 -4.1406031][-4.2200136 -4.2382565 -4.2315917 -4.2175555 -4.1906276 -4.1296635 -4.0324435 -3.9856861 -4.0335054 -4.1184559 -4.1803265 -4.1971545 -4.1751018 -4.1420245 -4.1274152][-4.2464805 -4.258605 -4.235611 -4.2008457 -4.1426148 -4.0337534 -3.8772717 -3.811096 -3.9048357 -4.0512123 -4.15094 -4.1925821 -4.1864667 -4.1578183 -4.1403365][-4.2665048 -4.269402 -4.2348175 -4.1803045 -4.0839233 -3.9192333 -3.700485 -3.617027 -3.770206 -3.9771621 -4.1116238 -4.1699214 -4.1832948 -4.1664352 -4.1541548][-4.2737422 -4.2678671 -4.2275419 -4.1629467 -4.0382042 -3.8351831 -3.5853913 -3.499892 -3.6944232 -3.9361019 -4.0841222 -4.144577 -4.1643314 -4.1547923 -4.14985][-4.2738924 -4.263494 -4.2250824 -4.161015 -4.0405722 -3.8578889 -3.6514602 -3.5891809 -3.7591014 -3.9684334 -4.0942945 -4.1434193 -4.162004 -4.1518984 -4.1513667][-4.2774563 -4.2655911 -4.2306428 -4.1759152 -4.0844641 -3.9609988 -3.8321869 -3.800025 -3.9141767 -4.0525212 -4.1334987 -4.1633539 -4.1773195 -4.169138 -4.1700592][-4.2833118 -4.2718763 -4.242291 -4.2028956 -4.1459036 -4.0793638 -4.0136909 -3.9972312 -4.0623431 -4.1399012 -4.1808076 -4.1905322 -4.1954155 -4.1870275 -4.1863284][-4.2821088 -4.2728815 -4.2502546 -4.2253571 -4.1950707 -4.1671944 -4.1415505 -4.1350908 -4.1650424 -4.201046 -4.2157693 -4.2099657 -4.2033806 -4.1929846 -4.1894422][-4.2733412 -4.2688451 -4.2551303 -4.2424273 -4.2291265 -4.22383 -4.2220173 -4.2231307 -4.232059 -4.2420011 -4.2434993 -4.2334094 -4.2200265 -4.2056608 -4.1962748][-4.2735643 -4.2749496 -4.2684517 -4.2607894 -4.2522864 -4.2511525 -4.25483 -4.2605653 -4.263639 -4.2674026 -4.2689395 -4.2631097 -4.25163 -4.2387104 -4.2262354][-4.2951651 -4.2971506 -4.2901769 -4.2809 -4.2709837 -4.2670784 -4.2695465 -4.27716 -4.2825937 -4.2879138 -4.2920828 -4.2926517 -4.2884803 -4.2819595 -4.2706556]]...]
INFO - root - 2017-12-06 01:08:36.774296: step 60910, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 64h:19m:31s remains)
INFO - root - 2017-12-06 01:08:45.252969: step 60920, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 64h:10m:46s remains)
INFO - root - 2017-12-06 01:08:53.838272: step 60930, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 64h:18m:55s remains)
INFO - root - 2017-12-06 01:09:02.289888: step 60940, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:35m:49s remains)
INFO - root - 2017-12-06 01:09:10.794242: step 60950, loss = 2.10, batch loss = 2.04 (9.6 examples/sec; 0.835 sec/batch; 62h:57m:38s remains)
INFO - root - 2017-12-06 01:09:19.323957: step 60960, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.820 sec/batch; 61h:52m:51s remains)
INFO - root - 2017-12-06 01:09:27.863861: step 60970, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 65h:10m:32s remains)
INFO - root - 2017-12-06 01:09:36.427643: step 60980, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.851 sec/batch; 64h:11m:02s remains)
INFO - root - 2017-12-06 01:09:45.060369: step 60990, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.839 sec/batch; 63h:18m:32s remains)
INFO - root - 2017-12-06 01:09:53.641649: step 61000, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.891 sec/batch; 67h:09m:31s remains)
2017-12-06 01:09:54.520846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2539244 -4.2477531 -4.2374368 -4.2263584 -4.2189431 -4.2165971 -4.2193141 -4.2295322 -4.2454782 -4.2592 -4.2688279 -4.27403 -4.2778106 -4.2812095 -4.2809205][-4.2462311 -4.2384634 -4.2286019 -4.2156787 -4.2028565 -4.1936812 -4.193203 -4.2070866 -4.2306905 -4.2523937 -4.268528 -4.2793202 -4.2858891 -4.290184 -4.2902932][-4.2288814 -4.2174716 -4.2069631 -4.1933446 -4.1772366 -4.1596327 -4.1531844 -4.1673055 -4.197279 -4.228673 -4.2551007 -4.27655 -4.2883053 -4.2941203 -4.2945409][-4.2023587 -4.1844325 -4.1718221 -4.1584697 -4.1440058 -4.1218023 -4.1085019 -4.1202984 -4.1544156 -4.1930828 -4.2310953 -4.2662468 -4.286325 -4.2943993 -4.2944074][-4.1733928 -4.1460323 -4.128263 -4.1152945 -4.10357 -4.0771484 -4.0560884 -4.0645747 -4.0985613 -4.1377249 -4.1818409 -4.2311945 -4.2639308 -4.281209 -4.2864008][-4.1521692 -4.1098557 -4.0796437 -4.0632448 -4.0502892 -4.0208321 -3.9922075 -3.996124 -4.028832 -4.0631113 -4.1065187 -4.1664324 -4.2155809 -4.2478385 -4.2649393][-4.1418242 -4.0823307 -4.0339279 -4.0066466 -3.9846282 -3.9475307 -3.9078867 -3.9053562 -3.9368982 -3.9678104 -4.009881 -4.0784941 -4.1492629 -4.2019377 -4.2364335][-4.1558251 -4.0886545 -4.029635 -3.9896467 -3.9485326 -3.8937573 -3.8375652 -3.8215525 -3.8491008 -3.8806262 -3.9222035 -3.9966683 -4.0860729 -4.1580086 -4.2106276][-4.19963 -4.1420813 -4.0877528 -4.0416431 -3.9837108 -3.9152882 -3.8527389 -3.8243783 -3.8408778 -3.8720343 -3.918124 -3.9924226 -4.0806284 -4.15221 -4.20767][-4.2489896 -4.2107458 -4.1709085 -4.1300378 -4.071342 -4.0083337 -3.9598567 -3.9335871 -3.9382212 -3.9627643 -4.0067945 -4.06767 -4.1337328 -4.1889729 -4.2315331][-4.2890978 -4.27072 -4.2483277 -4.2203393 -4.1745486 -4.1258016 -4.0939374 -4.0734091 -4.0713053 -4.0878172 -4.1224813 -4.1635261 -4.2028723 -4.2366762 -4.2625961][-4.3094692 -4.3030357 -4.2942572 -4.2816167 -4.2548509 -4.224031 -4.2056069 -4.191906 -4.1873865 -4.196764 -4.2204137 -4.244225 -4.2626295 -4.2779922 -4.2900848][-4.3149743 -4.3143396 -4.31284 -4.3106179 -4.2999864 -4.284512 -4.2748976 -4.2666817 -4.2610664 -4.2638426 -4.2768345 -4.2885046 -4.2960062 -4.3016348 -4.3055081][-4.3121147 -4.3143706 -4.3163781 -4.3182206 -4.3152318 -4.3082924 -4.3042541 -4.3011327 -4.2978935 -4.2971272 -4.3019934 -4.3057671 -4.307302 -4.3088293 -4.3087859][-4.307394 -4.3096662 -4.3121419 -4.314117 -4.3133869 -4.3103848 -4.3087077 -4.3086643 -4.3085055 -4.3083 -4.3100348 -4.3108549 -4.3102536 -4.3094969 -4.3079386]]...]
INFO - root - 2017-12-06 01:10:03.007382: step 61010, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.828 sec/batch; 62h:25m:55s remains)
INFO - root - 2017-12-06 01:10:11.571673: step 61020, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.834 sec/batch; 62h:54m:14s remains)
INFO - root - 2017-12-06 01:10:20.272910: step 61030, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 64h:16m:23s remains)
INFO - root - 2017-12-06 01:10:28.679867: step 61040, loss = 2.06, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 66h:18m:07s remains)
INFO - root - 2017-12-06 01:10:37.118748: step 61050, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 64h:56m:59s remains)
INFO - root - 2017-12-06 01:10:45.683653: step 61060, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.851 sec/batch; 64h:09m:03s remains)
INFO - root - 2017-12-06 01:10:54.241443: step 61070, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:48m:43s remains)
INFO - root - 2017-12-06 01:11:02.751647: step 61080, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:34m:23s remains)
INFO - root - 2017-12-06 01:11:11.309774: step 61090, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 63h:51m:07s remains)
INFO - root - 2017-12-06 01:11:19.876035: step 61100, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 64h:18m:41s remains)
2017-12-06 01:11:20.657174: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3453727 -4.3439083 -4.3392577 -4.333868 -4.3302155 -4.3297248 -4.3313541 -4.3335147 -4.3345447 -4.3337574 -4.3313756 -4.3283391 -4.3243942 -4.3177967 -4.3071108][-4.3437843 -4.3383431 -4.330584 -4.323523 -4.3195224 -4.3200126 -4.3242083 -4.3298826 -4.3340664 -4.3356 -4.3345828 -4.3317585 -4.3273306 -4.3204966 -4.3096814][-4.3269024 -4.3227897 -4.3154116 -4.3065929 -4.3004746 -4.3003631 -4.306169 -4.3165784 -4.327117 -4.3342886 -4.3368835 -4.33551 -4.3316779 -4.3265262 -4.3182759][-4.30848 -4.30633 -4.2959046 -4.2815723 -4.26951 -4.2662668 -4.27387 -4.2908621 -4.3098631 -4.3249717 -4.3336821 -4.3355479 -4.3335633 -4.3306689 -4.3257442][-4.2859564 -4.282701 -4.2636628 -4.2386403 -4.2170949 -4.2112112 -4.2227683 -4.246459 -4.2745185 -4.2994761 -4.3169823 -4.3250427 -4.327198 -4.3281693 -4.3276138][-4.2506104 -4.2423816 -4.2111473 -4.1723666 -4.1411347 -4.1324029 -4.1448059 -4.172358 -4.2084684 -4.2448058 -4.2749271 -4.2931437 -4.3038511 -4.3107762 -4.3141079][-4.1972227 -4.1860633 -4.1472611 -4.0974407 -4.0573106 -4.0419984 -4.0480905 -4.0717392 -4.1111512 -4.1578817 -4.2015104 -4.233243 -4.2560782 -4.2689247 -4.2721596][-4.1387272 -4.13474 -4.1014342 -4.0501251 -4.0046153 -3.9805207 -3.9737189 -3.9846478 -4.0170851 -4.06572 -4.1151814 -4.1549544 -4.1843133 -4.1974378 -4.1946454][-4.1062722 -4.1164966 -4.0980577 -4.0536904 -4.0085831 -3.9743698 -3.9494545 -3.9434505 -3.9643993 -4.0080609 -4.0556712 -4.0950155 -4.121182 -4.1244073 -4.1073194][-4.1139121 -4.139205 -4.1379786 -4.1060591 -4.0647593 -4.0255127 -3.9920936 -3.9775934 -3.9918945 -4.0295405 -4.069046 -4.1003861 -4.1168981 -4.1064367 -4.0749087][-4.1725144 -4.2016497 -4.2090712 -4.1901011 -4.1586027 -4.1248579 -4.097331 -4.0880628 -4.0988607 -4.1265726 -4.1551867 -4.1777558 -4.1840687 -4.1642189 -4.1283073][-4.24944 -4.2719836 -4.2792945 -4.2682991 -4.2469921 -4.2235389 -4.206737 -4.2031054 -4.2115917 -4.2311845 -4.2519751 -4.2691975 -4.2728271 -4.2534113 -4.2211275][-4.3088436 -4.3209453 -4.3244882 -4.3171649 -4.3032317 -4.2875471 -4.2760663 -4.2723327 -4.2769675 -4.2904029 -4.3083005 -4.3252387 -4.3317132 -4.3189454 -4.2961946][-4.3410411 -4.3417583 -4.3393283 -4.3302445 -4.3158255 -4.298686 -4.2821116 -4.2690649 -4.2657571 -4.2775922 -4.3021932 -4.33012 -4.3466582 -4.3459625 -4.3354855][-4.3461165 -4.3382936 -4.3285127 -4.311594 -4.28478 -4.2518616 -4.2174659 -4.1869745 -4.1721525 -4.1834493 -4.2218218 -4.2719822 -4.3092842 -4.3273783 -4.3276148]]...]
INFO - root - 2017-12-06 01:11:29.115344: step 61110, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 62h:03m:35s remains)
INFO - root - 2017-12-06 01:11:37.657412: step 61120, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 63h:47m:48s remains)
INFO - root - 2017-12-06 01:11:46.165505: step 61130, loss = 2.05, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 64h:54m:50s remains)
INFO - root - 2017-12-06 01:11:54.551887: step 61140, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 62h:16m:10s remains)
INFO - root - 2017-12-06 01:12:03.118004: step 61150, loss = 2.08, batch loss = 2.02 (8.8 examples/sec; 0.907 sec/batch; 68h:21m:36s remains)
INFO - root - 2017-12-06 01:12:11.626146: step 61160, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 65h:25m:50s remains)
INFO - root - 2017-12-06 01:12:20.228390: step 61170, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.836 sec/batch; 63h:01m:19s remains)
INFO - root - 2017-12-06 01:12:28.822180: step 61180, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.878 sec/batch; 66h:11m:30s remains)
INFO - root - 2017-12-06 01:12:37.368571: step 61190, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.864 sec/batch; 65h:08m:42s remains)
INFO - root - 2017-12-06 01:12:45.906809: step 61200, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 64h:22m:42s remains)
2017-12-06 01:12:46.669006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1666718 -4.2139759 -4.2248211 -4.206748 -4.1492643 -4.0807796 -4.039186 -4.0595956 -4.1028852 -4.177422 -4.25243 -4.3041272 -4.3189316 -4.3168144 -4.3228927][-4.16129 -4.2090168 -4.21975 -4.1997104 -4.13853 -4.0602875 -4.0094023 -4.0311027 -4.0831146 -4.164412 -4.2513604 -4.312314 -4.3247318 -4.3179164 -4.3240571][-4.1514678 -4.1893854 -4.1984105 -4.1787815 -4.1148548 -4.0212026 -3.9549913 -3.9859912 -4.0529914 -4.1430526 -4.2433147 -4.3130431 -4.3253665 -4.3172817 -4.32384][-4.141757 -4.167665 -4.1776047 -4.1585045 -4.0894227 -3.9792249 -3.9018817 -3.9427962 -4.0211048 -4.1203537 -4.2302747 -4.3070517 -4.3237758 -4.3171759 -4.3240714][-4.14085 -4.1505108 -4.15736 -4.1424327 -4.0740514 -3.9664257 -3.8952904 -3.943872 -4.0199251 -4.1151686 -4.2239151 -4.2991991 -4.3186831 -4.3155646 -4.32312][-4.1404853 -4.1414442 -4.1452584 -4.1366439 -4.0741372 -3.9699421 -3.9098339 -3.9699793 -4.0444207 -4.1297226 -4.2293496 -4.2964444 -4.3158159 -4.313828 -4.3220277][-4.1474586 -4.1462111 -4.1519008 -4.1436658 -4.0748096 -3.9596636 -3.8987708 -3.9689655 -4.0472627 -4.1335235 -4.2341008 -4.2995811 -4.315794 -4.3126016 -4.3218164][-4.1390014 -4.138814 -4.1508803 -4.1377144 -4.0545335 -3.9221191 -3.8555629 -3.925261 -4.0068741 -4.1069369 -4.2210693 -4.2954831 -4.3130703 -4.3126564 -4.3231955][-4.1352005 -4.1283541 -4.1410313 -4.129529 -4.0431376 -3.9042504 -3.8352234 -3.8954046 -3.9708242 -4.0810575 -4.2064075 -4.29036 -4.31261 -4.3130503 -4.3243237][-4.1321235 -4.1204891 -4.137013 -4.1349778 -4.0568991 -3.9243913 -3.8655381 -3.9222007 -3.9932039 -4.0972548 -4.2130604 -4.2932482 -4.3134804 -4.3120527 -4.323957][-4.1402097 -4.1326547 -4.1549859 -4.1542497 -4.0813079 -3.9549522 -3.9110134 -3.97496 -4.045846 -4.1372371 -4.2311454 -4.2979674 -4.313179 -4.3105965 -4.322938][-4.1496539 -4.1492138 -4.1692562 -4.1603684 -4.0839682 -3.9571455 -3.9294097 -4.0080471 -4.0828166 -4.165112 -4.2447515 -4.3021936 -4.3132415 -4.3102169 -4.3223805][-4.1648474 -4.1705346 -4.1858692 -4.1630058 -4.0716333 -3.9345405 -3.916271 -4.0064411 -4.088304 -4.1713252 -4.2468657 -4.3039861 -4.3135757 -4.3106689 -4.3226786][-4.1753669 -4.1841593 -4.1954927 -4.1662583 -4.0699711 -3.9313908 -3.9160852 -4.0033402 -4.0845947 -4.1711588 -4.2472649 -4.3038578 -4.3145475 -4.3129697 -4.3240824][-4.17615 -4.1883721 -4.1967978 -4.1698232 -4.0851078 -3.9635537 -3.9538159 -4.0308027 -4.1032128 -4.1844482 -4.2548542 -4.3052683 -4.3164892 -4.3160553 -4.325079]]...]
INFO - root - 2017-12-06 01:12:55.210610: step 61210, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.875 sec/batch; 65h:58m:02s remains)
INFO - root - 2017-12-06 01:13:03.730631: step 61220, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 64h:24m:55s remains)
INFO - root - 2017-12-06 01:13:12.270581: step 61230, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.866 sec/batch; 65h:16m:33s remains)
INFO - root - 2017-12-06 01:13:20.679522: step 61240, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.812 sec/batch; 61h:10m:09s remains)
INFO - root - 2017-12-06 01:13:29.144935: step 61250, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.839 sec/batch; 63h:12m:00s remains)
INFO - root - 2017-12-06 01:13:37.692634: step 61260, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 63h:58m:27s remains)
INFO - root - 2017-12-06 01:13:46.137843: step 61270, loss = 2.04, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 64h:34m:42s remains)
INFO - root - 2017-12-06 01:13:54.616958: step 61280, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:48m:36s remains)
INFO - root - 2017-12-06 01:14:03.076182: step 61290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.858 sec/batch; 64h:39m:43s remains)
INFO - root - 2017-12-06 01:14:11.647189: step 61300, loss = 2.07, batch loss = 2.01 (8.7 examples/sec; 0.918 sec/batch; 69h:09m:08s remains)
2017-12-06 01:14:12.443698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1426034 -4.1504879 -4.1671419 -4.1774712 -4.1836319 -4.1830025 -4.1810808 -4.1811347 -4.1774006 -4.1699228 -4.16793 -4.1823139 -4.2085781 -4.2394481 -4.2678704][-4.1267524 -4.133091 -4.1530833 -4.1665239 -4.1733894 -4.1701431 -4.1627007 -4.1570992 -4.1479049 -4.1371746 -4.1336555 -4.1499352 -4.1799564 -4.2156267 -4.2505426][-4.1314821 -4.135725 -4.1551023 -4.16782 -4.1724987 -4.163795 -4.1496563 -4.1387038 -4.1279583 -4.1187592 -4.1182003 -4.1362367 -4.1659875 -4.2030635 -4.2414846][-4.154716 -4.1567469 -4.1709428 -4.1783257 -4.1763215 -4.1596789 -4.1404672 -4.1281939 -4.1214318 -4.1186662 -4.1229219 -4.1392293 -4.1625948 -4.1952381 -4.2340541][-4.1667781 -4.1637859 -4.1730146 -4.1726003 -4.156754 -4.1297789 -4.1090631 -4.1020117 -4.1083875 -4.1199794 -4.1329174 -4.1483665 -4.1647992 -4.1894784 -4.22356][-4.1677017 -4.1612806 -4.1641946 -4.1537724 -4.1203818 -4.0802388 -4.0584126 -4.0584888 -4.084383 -4.1179934 -4.1440716 -4.1633854 -4.1743727 -4.1880608 -4.2132926][-4.1629481 -4.153533 -4.1508484 -4.1343217 -4.0939689 -4.0475988 -4.0217581 -4.02122 -4.0560527 -4.1038074 -4.1390591 -4.1599669 -4.1664066 -4.1732006 -4.1933832][-4.1448565 -4.1344314 -4.1297073 -4.1119809 -4.0733538 -4.0229554 -3.990994 -3.9852982 -4.0193167 -4.0740194 -4.1156921 -4.1365213 -4.1411333 -4.1477394 -4.1705675][-4.1309915 -4.1230578 -4.1240067 -4.114356 -4.0857596 -4.0407047 -4.0086975 -4.0039911 -4.0359287 -4.0854778 -4.1209669 -4.134975 -4.1363759 -4.1431208 -4.1677594][-4.1565804 -4.15903 -4.1713395 -4.176034 -4.1632676 -4.1304865 -4.1027579 -4.0955091 -4.117507 -4.15234 -4.173595 -4.1779227 -4.174192 -4.1767707 -4.1953926][-4.2128386 -4.2219181 -4.23847 -4.2486997 -4.2418585 -4.2151008 -4.1893873 -4.1818376 -4.1966763 -4.2174954 -4.22536 -4.2241063 -4.2192812 -4.2199216 -4.2325764][-4.2615247 -4.2710705 -4.284142 -4.2906747 -4.2822533 -4.2582827 -4.2363906 -4.2316437 -4.247314 -4.2653065 -4.2681751 -4.2645712 -4.260417 -4.2614484 -4.2703118][-4.289649 -4.2971797 -4.3065338 -4.310462 -4.3032532 -4.2841759 -4.2652917 -4.2607956 -4.276598 -4.2969589 -4.3036909 -4.3016539 -4.2992291 -4.3003197 -4.3050346][-4.2910976 -4.2982616 -4.3074837 -4.3129377 -4.30952 -4.298594 -4.2854457 -4.2817512 -4.2955041 -4.3139119 -4.3212738 -4.3206911 -4.3198776 -4.3202305 -4.3214741][-4.2845736 -4.2893667 -4.2979918 -4.3045053 -4.3046718 -4.3000088 -4.2926989 -4.2905412 -4.3009453 -4.314321 -4.3191571 -4.3196793 -4.3212004 -4.3217077 -4.3209457]]...]
INFO - root - 2017-12-06 01:14:20.994371: step 61310, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.864 sec/batch; 65h:03m:39s remains)
INFO - root - 2017-12-06 01:14:29.504465: step 61320, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.828 sec/batch; 62h:23m:07s remains)
INFO - root - 2017-12-06 01:14:38.061010: step 61330, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.868 sec/batch; 65h:23m:37s remains)
INFO - root - 2017-12-06 01:14:46.388882: step 61340, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.861 sec/batch; 64h:49m:16s remains)
INFO - root - 2017-12-06 01:14:55.020709: step 61350, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 64h:30m:19s remains)
INFO - root - 2017-12-06 01:15:03.599655: step 61360, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.839 sec/batch; 63h:10m:50s remains)
INFO - root - 2017-12-06 01:15:12.123735: step 61370, loss = 2.05, batch loss = 1.99 (10.0 examples/sec; 0.803 sec/batch; 60h:28m:26s remains)
INFO - root - 2017-12-06 01:15:20.532237: step 61380, loss = 2.06, batch loss = 2.00 (9.9 examples/sec; 0.812 sec/batch; 61h:07m:57s remains)
INFO - root - 2017-12-06 01:15:29.034180: step 61390, loss = 2.11, batch loss = 2.05 (9.4 examples/sec; 0.853 sec/batch; 64h:12m:25s remains)
INFO - root - 2017-12-06 01:15:37.649698: step 61400, loss = 2.09, batch loss = 2.03 (9.0 examples/sec; 0.885 sec/batch; 66h:39m:51s remains)
2017-12-06 01:15:38.451966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.273675 -4.2644091 -4.2584195 -4.2624922 -4.2670503 -4.2688217 -4.2731519 -4.2783647 -4.279676 -4.2779932 -4.2766657 -4.2768378 -4.2764497 -4.2761159 -4.2780228][-4.2689371 -4.259583 -4.25253 -4.2572784 -4.2612681 -4.2645025 -4.2737546 -4.283329 -4.287559 -4.2867208 -4.2843251 -4.2832727 -4.2823467 -4.2807894 -4.2816844][-4.2376509 -4.2284961 -4.218802 -4.2211246 -4.2254567 -4.2346807 -4.2536941 -4.2730904 -4.2836008 -4.2853961 -4.2830505 -4.2811189 -4.2804465 -4.279686 -4.2817731][-4.1869397 -4.1807466 -4.1698594 -4.1684179 -4.1704521 -4.1832733 -4.2134261 -4.2427669 -4.2592764 -4.2645569 -4.2629447 -4.2606025 -4.2599831 -4.2605433 -4.2650733][-4.1385574 -4.1378312 -4.1285329 -4.1205697 -4.1128531 -4.1194711 -4.1520982 -4.1867127 -4.2091727 -4.2192636 -4.2197084 -4.2168436 -4.2148242 -4.2163129 -4.2238955][-4.1149807 -4.1160121 -4.1054378 -4.0853262 -4.0589032 -4.0474772 -4.0694189 -4.1025281 -4.1338739 -4.1531086 -4.159411 -4.1580029 -4.1542368 -4.1553221 -4.1631951][-4.1291914 -4.1226821 -4.1047955 -4.07108 -4.0250216 -3.9891465 -3.9831362 -4.0013289 -4.0393968 -4.0717344 -4.0886884 -4.0956559 -4.0947385 -4.093998 -4.0967903][-4.1793151 -4.1630244 -4.1377068 -4.0987616 -4.0458059 -3.9945168 -3.9587693 -3.9497252 -3.980906 -4.0183926 -4.0421238 -4.0563574 -4.057579 -4.0519085 -4.041326][-4.23537 -4.2171464 -4.1901822 -4.1538529 -4.1076431 -4.0608611 -4.0196629 -3.9971647 -4.0110912 -4.03572 -4.052177 -4.0618839 -4.0594649 -4.0478797 -4.0235353][-4.2739706 -4.2619467 -4.2421932 -4.2161655 -4.183588 -4.1509356 -4.1191626 -4.0957661 -4.0942097 -4.1032648 -4.1093044 -4.1105523 -4.103456 -4.0892596 -4.0610342][-4.2791009 -4.275938 -4.2680407 -4.2560058 -4.2376966 -4.218174 -4.1969571 -4.176199 -4.1673045 -4.1683421 -4.1713481 -4.1691456 -4.1601076 -4.1470704 -4.1216598][-4.2627487 -4.2643032 -4.2630167 -4.2594624 -4.2507205 -4.24125 -4.2300715 -4.214756 -4.2044392 -4.2031255 -4.207912 -4.207839 -4.2032466 -4.1953206 -4.17667][-4.2446456 -4.2472634 -4.2469306 -4.2458386 -4.2416067 -4.2387276 -4.2356405 -4.2266483 -4.2178879 -4.2174635 -4.2250586 -4.2276673 -4.2272539 -4.2244668 -4.2144585][-4.2336221 -4.2376037 -4.2377505 -4.2375 -4.2349763 -4.2346177 -4.2361655 -4.2329254 -4.227118 -4.2286487 -4.2376976 -4.2414908 -4.2418928 -4.2406731 -4.2363381][-4.243609 -4.2494378 -4.25164 -4.2535677 -4.2528057 -4.2543659 -4.2592983 -4.2601504 -4.256732 -4.2583585 -4.2655554 -4.2675266 -4.2663617 -4.2641339 -4.261085]]...]
INFO - root - 2017-12-06 01:15:46.973726: step 61410, loss = 2.09, batch loss = 2.03 (9.7 examples/sec; 0.827 sec/batch; 62h:16m:24s remains)
INFO - root - 2017-12-06 01:15:55.560497: step 61420, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 66h:10m:53s remains)
INFO - root - 2017-12-06 01:16:04.175885: step 61430, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 64h:41m:46s remains)
INFO - root - 2017-12-06 01:16:12.668776: step 61440, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.849 sec/batch; 63h:57m:31s remains)
INFO - root - 2017-12-06 01:16:21.147694: step 61450, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 64h:33m:36s remains)
INFO - root - 2017-12-06 01:16:29.656300: step 61460, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.842 sec/batch; 63h:23m:39s remains)
INFO - root - 2017-12-06 01:16:38.207866: step 61470, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.876 sec/batch; 65h:57m:54s remains)
INFO - root - 2017-12-06 01:16:46.647911: step 61480, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 65h:39m:17s remains)
INFO - root - 2017-12-06 01:16:55.225033: step 61490, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.834 sec/batch; 62h:49m:14s remains)
INFO - root - 2017-12-06 01:17:03.696456: step 61500, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.813 sec/batch; 61h:13m:26s remains)
2017-12-06 01:17:04.457679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3022194 -4.2944303 -4.2940536 -4.2979283 -4.302669 -4.307322 -4.3105421 -4.3137751 -4.3175049 -4.3204665 -4.3205113 -4.3166852 -4.3137712 -4.3141975 -4.3175378][-4.274837 -4.2662659 -4.266654 -4.2714386 -4.2776818 -4.2842607 -4.2906942 -4.296648 -4.3035707 -4.3110027 -4.3117566 -4.3034434 -4.2965193 -4.2973156 -4.3046794][-4.2518816 -4.2424293 -4.2412958 -4.2454891 -4.2512126 -4.2545891 -4.257791 -4.2657995 -4.2808352 -4.2971482 -4.2998409 -4.2906728 -4.2829652 -4.2846169 -4.2929244][-4.2367678 -4.2206235 -4.209404 -4.2073107 -4.2101183 -4.206718 -4.1980739 -4.2071776 -4.2353172 -4.2643714 -4.2697964 -4.2620978 -4.2572212 -4.2607012 -4.2664227][-4.2237649 -4.1918015 -4.1615334 -4.1503911 -4.1495833 -4.1356215 -4.1113467 -4.1201949 -4.1653223 -4.20945 -4.2183876 -4.213006 -4.2132797 -4.2149959 -4.2167368][-4.2095962 -4.161602 -4.11536 -4.0954638 -4.0869923 -4.0581274 -4.0120993 -4.0117025 -4.0760908 -4.1355271 -4.1478534 -4.1432395 -4.15312 -4.155365 -4.1579161][-4.1971126 -4.1342006 -4.0712705 -4.0365577 -4.0093169 -3.9526005 -3.8714693 -3.856236 -3.9507062 -4.0411224 -4.0700994 -4.070786 -4.0877171 -4.0956235 -4.1055007][-4.1934257 -4.1222644 -4.0465527 -3.9944804 -3.9416976 -3.8495986 -3.71565 -3.6826761 -3.8286695 -3.9637551 -4.0229669 -4.0349541 -4.047205 -4.0570354 -4.0719247][-4.2011619 -4.1342082 -4.0613132 -4.0122361 -3.9641414 -3.8710833 -3.7280874 -3.6924632 -3.8487034 -3.9855137 -4.044312 -4.0538664 -4.0563469 -4.0622211 -4.0761294][-4.20549 -4.1443691 -4.0811596 -4.0421295 -4.0133281 -3.9593973 -3.8793235 -3.8768108 -3.9842536 -4.0704613 -4.1025753 -4.1010509 -4.0899272 -4.0869951 -4.09746][-4.2209148 -4.1723256 -4.121613 -4.0882015 -4.0688071 -4.049799 -4.0234709 -4.0345912 -4.0977087 -4.1404376 -4.1542807 -4.147378 -4.1307392 -4.122436 -4.1322374][-4.2396979 -4.2009606 -4.1610875 -4.1358685 -4.1252661 -4.1231704 -4.121058 -4.1321526 -4.1656461 -4.1887035 -4.1975188 -4.1896906 -4.1766505 -4.1743073 -4.189292][-4.2605648 -4.2291884 -4.2010918 -4.1894484 -4.1903419 -4.1945081 -4.1993971 -4.2065868 -4.2226877 -4.2359514 -4.2407713 -4.2306361 -4.2233682 -4.2296448 -4.246686][-4.2807364 -4.25914 -4.2430019 -4.2406683 -4.246335 -4.2531295 -4.2593894 -4.2645969 -4.271152 -4.2748036 -4.2752666 -4.2677679 -4.2681789 -4.2791262 -4.2896161][-4.3008742 -4.2881112 -4.281044 -4.2805967 -4.2839937 -4.2891507 -4.2956805 -4.3002458 -4.3010316 -4.2976747 -4.2951045 -4.293571 -4.2979393 -4.3069859 -4.31136]]...]
INFO - root - 2017-12-06 01:17:13.071396: step 61510, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 66h:33m:08s remains)
INFO - root - 2017-12-06 01:17:21.630140: step 61520, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.869 sec/batch; 65h:23m:06s remains)
INFO - root - 2017-12-06 01:17:30.163939: step 61530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:15m:39s remains)
INFO - root - 2017-12-06 01:17:38.596872: step 61540, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 63h:36m:51s remains)
INFO - root - 2017-12-06 01:17:47.081859: step 61550, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.823 sec/batch; 61h:56m:24s remains)
INFO - root - 2017-12-06 01:17:55.597455: step 61560, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.854 sec/batch; 64h:15m:01s remains)
INFO - root - 2017-12-06 01:18:04.098130: step 61570, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.884 sec/batch; 66h:32m:28s remains)
INFO - root - 2017-12-06 01:18:12.638998: step 61580, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 62h:54m:55s remains)
INFO - root - 2017-12-06 01:18:21.083993: step 61590, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.889 sec/batch; 66h:52m:34s remains)
INFO - root - 2017-12-06 01:18:29.692580: step 61600, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.831 sec/batch; 62h:31m:57s remains)
2017-12-06 01:18:30.453776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3085089 -4.2954254 -4.284317 -4.28351 -4.2904043 -4.3026853 -4.3147388 -4.3012528 -4.2481837 -4.155726 -4.0680256 -4.0443015 -4.0978956 -4.184042 -4.2573166][-4.3315978 -4.3248062 -4.3145609 -4.3083725 -4.3073273 -4.3074412 -4.3061309 -4.2811823 -4.2289906 -4.1521859 -4.0879221 -4.0807104 -4.1380644 -4.21652 -4.2835636][-4.34141 -4.3351259 -4.3252535 -4.3156533 -4.3060756 -4.2943597 -4.2801142 -4.244482 -4.1958737 -4.1462312 -4.1133747 -4.121851 -4.1793709 -4.2507515 -4.3099971][-4.3378353 -4.3316989 -4.3198061 -4.3029571 -4.2831154 -4.258646 -4.2260928 -4.1763439 -4.1369739 -4.125102 -4.1301613 -4.1553731 -4.2094755 -4.2721334 -4.3226843][-4.3216138 -4.3171659 -4.3026514 -4.2771807 -4.2411857 -4.1919842 -4.1267905 -4.0545673 -4.0251775 -4.0596929 -4.1146932 -4.1688566 -4.2302308 -4.2877288 -4.3304248][-4.3030663 -4.301281 -4.2848139 -4.2481751 -4.1890025 -4.1025662 -3.9849813 -3.8698683 -3.8557563 -3.9527459 -4.065423 -4.1558857 -4.2335725 -4.2934771 -4.3333678][-4.2842159 -4.2875042 -4.272449 -4.228261 -4.1459789 -4.0143056 -3.8304644 -3.6674104 -3.6879797 -3.8541231 -4.0150676 -4.1351237 -4.226872 -4.2913189 -4.3289762][-4.2704859 -4.283247 -4.2746964 -4.23398 -4.1447778 -3.9895549 -3.7827961 -3.6317859 -3.6930025 -3.8739743 -4.0335364 -4.151638 -4.2399259 -4.2984447 -4.3296952][-4.2565517 -4.2800288 -4.2854691 -4.2614694 -4.1877723 -4.0526233 -3.8961189 -3.8127885 -3.8731124 -3.9992385 -4.1132188 -4.2038674 -4.2740169 -4.3179169 -4.3375258][-4.2362442 -4.2742491 -4.2939625 -4.2857904 -4.2336192 -4.1348124 -4.0402761 -4.0077772 -4.0531173 -4.128922 -4.2049761 -4.2676897 -4.3135552 -4.3379288 -4.3462672][-4.224597 -4.2746911 -4.2994924 -4.2932138 -4.2535439 -4.1861911 -4.1368713 -4.1324029 -4.1682963 -4.2187414 -4.2689075 -4.3081083 -4.3343406 -4.3467312 -4.3487282][-4.2393975 -4.2902446 -4.3092322 -4.298089 -4.2634025 -4.2155457 -4.1899405 -4.1993213 -4.2340884 -4.2727208 -4.3047123 -4.3262939 -4.3396826 -4.3471475 -4.3460178][-4.2771425 -4.3170686 -4.3252149 -4.3069334 -4.2707314 -4.2310252 -4.2180276 -4.2369623 -4.2730932 -4.3041067 -4.323669 -4.3338161 -4.3403182 -4.345665 -4.3423648][-4.3111305 -4.336298 -4.33424 -4.3088665 -4.2669463 -4.2298665 -4.2237892 -4.2508626 -4.2900014 -4.3167472 -4.3297095 -4.3340669 -4.3360844 -4.3394794 -4.3363552][-4.3264608 -4.3413224 -4.3350496 -4.3038526 -4.2561126 -4.2191491 -4.2174692 -4.2510519 -4.2940841 -4.3199754 -4.3295441 -4.3297296 -4.3286071 -4.3310966 -4.3296843]]...]
INFO - root - 2017-12-06 01:18:38.998586: step 61610, loss = 2.03, batch loss = 1.98 (9.1 examples/sec; 0.884 sec/batch; 66h:30m:59s remains)
INFO - root - 2017-12-06 01:18:47.580662: step 61620, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 64h:21m:11s remains)
INFO - root - 2017-12-06 01:18:56.079568: step 61630, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.844 sec/batch; 63h:31m:21s remains)
INFO - root - 2017-12-06 01:19:04.663184: step 61640, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 64h:25m:52s remains)
INFO - root - 2017-12-06 01:19:13.218984: step 61650, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.868 sec/batch; 65h:20m:21s remains)
INFO - root - 2017-12-06 01:19:21.807932: step 61660, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.847 sec/batch; 63h:43m:54s remains)
INFO - root - 2017-12-06 01:19:30.250767: step 61670, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 65h:17m:33s remains)
INFO - root - 2017-12-06 01:19:38.886297: step 61680, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.862 sec/batch; 64h:50m:23s remains)
INFO - root - 2017-12-06 01:19:47.504927: step 61690, loss = 2.07, batch loss = 2.01 (8.9 examples/sec; 0.894 sec/batch; 67h:17m:11s remains)
INFO - root - 2017-12-06 01:19:56.044841: step 61700, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.865 sec/batch; 65h:03m:19s remains)
2017-12-06 01:19:56.836357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1965876 -4.2303386 -4.2587128 -4.2709465 -4.2737908 -4.2726283 -4.27213 -4.2671485 -4.2653284 -4.263833 -4.2646365 -4.2602091 -4.2417631 -4.2042165 -4.1689749][-4.1771827 -4.21104 -4.2413197 -4.2539921 -4.2591739 -4.2591543 -4.258256 -4.2546048 -4.2636747 -4.2775984 -4.2830381 -4.2797012 -4.2611957 -4.2204852 -4.1816654][-4.1736736 -4.199254 -4.2245188 -4.2343006 -4.2383051 -4.2372489 -4.2325687 -4.2271895 -4.2463484 -4.2796388 -4.2960067 -4.2971759 -4.2794209 -4.2417431 -4.2089849][-4.1858826 -4.202033 -4.2185946 -4.2208095 -4.2177563 -4.2101851 -4.1960077 -4.1864834 -4.216907 -4.2681346 -4.2989664 -4.3078713 -4.2920132 -4.2596221 -4.2358103][-4.1934891 -4.2065334 -4.2167978 -4.2093554 -4.1934118 -4.1729388 -4.1479535 -4.1341529 -4.1738977 -4.2415056 -4.2866521 -4.3044853 -4.2946773 -4.2725635 -4.2577167][-4.1934772 -4.2087884 -4.2169805 -4.202219 -4.1707711 -4.1319609 -4.0906773 -4.0691547 -4.1187496 -4.2022934 -4.2588754 -4.2831545 -4.2776589 -4.266561 -4.2616549][-4.1992507 -4.2173252 -4.22436 -4.2024965 -4.1611333 -4.1103816 -4.0566597 -4.0278 -4.0801191 -4.1683922 -4.228785 -4.2536917 -4.2504845 -4.2461262 -4.2487907][-4.215898 -4.2345982 -4.2396183 -4.2140803 -4.1665545 -4.1068335 -4.0455694 -4.016499 -4.0667276 -4.1514854 -4.2050476 -4.2255769 -4.228457 -4.2310762 -4.2397265][-4.2366014 -4.2532706 -4.2580462 -4.2361193 -4.192174 -4.1332226 -4.0742526 -4.0505733 -4.0910821 -4.160368 -4.2024603 -4.2204366 -4.230423 -4.2362933 -4.2435756][-4.2560759 -4.2730289 -4.2797737 -4.2671041 -4.2347674 -4.1872296 -4.1394577 -4.1208186 -4.146492 -4.1919622 -4.22045 -4.2391071 -4.256165 -4.267518 -4.2714038][-4.2583485 -4.276711 -4.2859483 -4.28346 -4.2644315 -4.2311621 -4.1966543 -4.1815557 -4.1927338 -4.2162795 -4.2312403 -4.2480264 -4.2659049 -4.2780027 -4.2782292][-4.22316 -4.2425818 -4.2579947 -4.2688642 -4.2655859 -4.248539 -4.2271533 -4.2153387 -4.2150769 -4.2200332 -4.2252893 -4.2418313 -4.2559075 -4.2610455 -4.2577305][-4.1786423 -4.1980181 -4.2205286 -4.2413082 -4.2487841 -4.2445273 -4.2352848 -4.2258687 -4.2182412 -4.2103133 -4.2085409 -4.2258534 -4.23758 -4.2389846 -4.2361226][-4.1483011 -4.1635861 -4.1850691 -4.2058172 -4.2157779 -4.2189994 -4.217288 -4.2118592 -4.2026911 -4.1914659 -4.1887293 -4.20344 -4.2123642 -4.2123241 -4.2122903][-4.1502962 -4.1608028 -4.1727176 -4.1867518 -4.195199 -4.2009048 -4.2021179 -4.2006292 -4.1947641 -4.1849556 -4.1816568 -4.1911268 -4.1954646 -4.194562 -4.1962285]]...]
INFO - root - 2017-12-06 01:20:05.440520: step 61710, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 64h:02m:27s remains)
INFO - root - 2017-12-06 01:20:13.979282: step 61720, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.822 sec/batch; 61h:47m:42s remains)
INFO - root - 2017-12-06 01:20:22.500373: step 61730, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 64h:35m:15s remains)
INFO - root - 2017-12-06 01:20:30.947074: step 61740, loss = 2.05, batch loss = 1.99 (9.9 examples/sec; 0.809 sec/batch; 60h:50m:22s remains)
INFO - root - 2017-12-06 01:20:39.620150: step 61750, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 65h:17m:30s remains)
INFO - root - 2017-12-06 01:20:48.127733: step 61760, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 64h:12m:05s remains)
INFO - root - 2017-12-06 01:20:56.632138: step 61770, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.876 sec/batch; 65h:52m:25s remains)
INFO - root - 2017-12-06 01:21:04.890361: step 61780, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.877 sec/batch; 65h:57m:00s remains)
INFO - root - 2017-12-06 01:21:13.435433: step 61790, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 64h:02m:43s remains)
INFO - root - 2017-12-06 01:21:21.880861: step 61800, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.821 sec/batch; 61h:45m:07s remains)
2017-12-06 01:21:22.667082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2420912 -4.2413068 -4.2431664 -4.2478647 -4.2525334 -4.2551 -4.2568622 -4.25893 -4.2606549 -4.2618685 -4.2634726 -4.2637382 -4.2619462 -4.2594404 -4.2573514][-4.2524981 -4.253006 -4.2569795 -4.2632461 -4.2693505 -4.2739549 -4.2783895 -4.2828093 -4.285327 -4.2866163 -4.2874074 -4.2864666 -4.2835145 -4.2800279 -4.2770433][-4.2718649 -4.2734795 -4.2780366 -4.2837029 -4.2888184 -4.2925262 -4.2962418 -4.2991056 -4.3004827 -4.3015561 -4.3022771 -4.3023038 -4.3011184 -4.2987232 -4.2955828][-4.2810516 -4.2834263 -4.2875357 -4.2916813 -4.2941656 -4.2941761 -4.2940207 -4.2929964 -4.292398 -4.2933693 -4.2961917 -4.3007078 -4.30388 -4.3043728 -4.3022246][-4.2718525 -4.2750359 -4.2797585 -4.2829113 -4.2817416 -4.276485 -4.2702241 -4.2626085 -4.2563 -4.2546782 -4.2607422 -4.2733817 -4.2857938 -4.2938743 -4.29675][-4.2528882 -4.2559609 -4.2606864 -4.261272 -4.2544966 -4.2417097 -4.2258296 -4.2067566 -4.1892848 -4.18254 -4.1927619 -4.2161717 -4.2427511 -4.2637348 -4.2773247][-4.2305222 -4.2309351 -4.2346826 -4.2324028 -4.2207751 -4.2014823 -4.1782274 -4.1491694 -4.1206765 -4.1062155 -4.1161709 -4.1463027 -4.1841545 -4.21709 -4.2423229][-4.2099676 -4.2065725 -4.2099028 -4.2076488 -4.1955509 -4.1757846 -4.1514812 -4.1207652 -4.0899234 -4.0707474 -4.0736928 -4.099205 -4.1372075 -4.1743279 -4.2061267][-4.2077541 -4.2030563 -4.2072144 -4.2063904 -4.1956654 -4.1766853 -4.1544642 -4.1288524 -4.1053505 -4.091228 -4.0916548 -4.1092257 -4.1371179 -4.1661057 -4.1934805][-4.2283483 -4.2241683 -4.2282705 -4.2272286 -4.215775 -4.1959023 -4.1747389 -4.1542554 -4.1411061 -4.1387014 -4.1467366 -4.1636305 -4.1826515 -4.1987047 -4.2140722][-4.2471228 -4.2433586 -4.2459068 -4.2445459 -4.2350678 -4.21893 -4.202703 -4.1895709 -4.1857076 -4.1934276 -4.2074418 -4.2240024 -4.2356944 -4.2410765 -4.2452073][-4.2604618 -4.2569561 -4.2570128 -4.2544332 -4.2478232 -4.238472 -4.2295704 -4.2239108 -4.2260609 -4.237289 -4.2508683 -4.2638621 -4.2700872 -4.2702904 -4.2684436][-4.2704511 -4.2684507 -4.2676148 -4.2647023 -4.2599449 -4.2549872 -4.2508478 -4.2484937 -4.2502704 -4.2579875 -4.2665858 -4.2746935 -4.2780914 -4.2772288 -4.2738571][-4.2757792 -4.2754169 -4.2755337 -4.273212 -4.2690687 -4.2651062 -4.2624345 -4.2600837 -4.2583113 -4.2589 -4.2598348 -4.2608447 -4.2596889 -4.2568164 -4.2516413][-4.2828121 -4.2818327 -4.2814684 -4.2795997 -4.2768912 -4.2744193 -4.2724662 -4.2694511 -4.2644815 -4.2588305 -4.2519603 -4.2447567 -4.2370186 -4.2301636 -4.2222362]]...]
INFO - root - 2017-12-06 01:21:31.177235: step 61810, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 63h:38m:32s remains)
INFO - root - 2017-12-06 01:21:39.737104: step 61820, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 65h:43m:09s remains)
INFO - root - 2017-12-06 01:21:48.280787: step 61830, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.870 sec/batch; 65h:26m:43s remains)
INFO - root - 2017-12-06 01:21:56.788394: step 61840, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.841 sec/batch; 63h:12m:07s remains)
INFO - root - 2017-12-06 01:22:05.484277: step 61850, loss = 2.03, batch loss = 1.97 (9.3 examples/sec; 0.858 sec/batch; 64h:32m:13s remains)
INFO - root - 2017-12-06 01:22:14.017665: step 61860, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.831 sec/batch; 62h:26m:30s remains)
INFO - root - 2017-12-06 01:22:22.501458: step 61870, loss = 2.07, batch loss = 2.01 (9.9 examples/sec; 0.810 sec/batch; 60h:51m:41s remains)
INFO - root - 2017-12-06 01:22:31.054972: step 61880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 65h:03m:45s remains)
INFO - root - 2017-12-06 01:22:39.562366: step 61890, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 65h:09m:17s remains)
INFO - root - 2017-12-06 01:22:48.173638: step 61900, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 65h:25m:20s remains)
2017-12-06 01:22:48.925255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2783484 -4.2919049 -4.2872658 -4.2710342 -4.2396574 -4.1900697 -4.137753 -4.115273 -4.1350923 -4.1755524 -4.2244935 -4.2770309 -4.31368 -4.3299041 -4.3332052][-4.2798314 -4.2942629 -4.2893705 -4.2681742 -4.2277808 -4.1715546 -4.1178946 -4.1007695 -4.1279283 -4.1739993 -4.2258916 -4.278337 -4.3160377 -4.3329005 -4.3355618][-4.2715592 -4.2840986 -4.2789178 -4.2567186 -4.2110066 -4.1461267 -4.0888839 -4.0755491 -4.1100221 -4.1642179 -4.2220821 -4.2761517 -4.3156023 -4.33311 -4.3360438][-4.2677188 -4.2794275 -4.2726603 -4.2496691 -4.2000632 -4.1263695 -4.0617423 -4.04683 -4.0848513 -4.1472206 -4.2127528 -4.2712193 -4.3129711 -4.3318443 -4.3353419][-4.2645273 -4.2758131 -4.2673135 -4.2415571 -4.1883183 -4.1091371 -4.0383677 -4.016396 -4.053133 -4.1230726 -4.1992488 -4.2649264 -4.3102455 -4.3306928 -4.3347206][-4.2618365 -4.26847 -4.2572646 -4.2313261 -4.179162 -4.1005192 -4.0282621 -4.0012417 -4.0380111 -4.1154985 -4.2007637 -4.2722311 -4.31681 -4.3343787 -4.3361492][-4.2538347 -4.2564411 -4.2464848 -4.2251267 -4.1807 -4.1075249 -4.037118 -4.0089331 -4.0472217 -4.127058 -4.2155919 -4.2888446 -4.3288341 -4.341063 -4.3390384][-4.2386332 -4.2399964 -4.2355661 -4.2216177 -4.1867542 -4.1199241 -4.05023 -4.0228081 -4.0630817 -4.142695 -4.228127 -4.2995048 -4.3363895 -4.3456087 -4.3408232][-4.2071881 -4.2088828 -4.2136006 -4.212647 -4.1906118 -4.1363707 -4.0724983 -4.0463724 -4.0859194 -4.1611195 -4.2382689 -4.3048143 -4.3391094 -4.3467126 -4.3402581][-4.1676483 -4.1698308 -4.1803527 -4.1925497 -4.1851563 -4.14487 -4.0897121 -4.0666213 -4.1036682 -4.171845 -4.2419448 -4.3052139 -4.3378344 -4.3447328 -4.3377962][-4.1677976 -4.1681743 -4.1730275 -4.1845608 -4.1816397 -4.1467404 -4.0978842 -4.0791125 -4.1136675 -4.1781034 -4.24516 -4.304625 -4.3345661 -4.3416867 -4.3352413][-4.2007108 -4.1978846 -4.1910725 -4.1905837 -4.1806097 -4.1430755 -4.0935521 -4.0754089 -4.1090107 -4.1762176 -4.24586 -4.30336 -4.332231 -4.3394365 -4.3338418][-4.2310724 -4.2261238 -4.2086263 -4.1995783 -4.1854339 -4.1473856 -4.0939703 -4.0737705 -4.10903 -4.1790991 -4.2499828 -4.3058009 -4.3330913 -4.3395467 -4.3344803][-4.2471 -4.2424378 -4.2223949 -4.2128034 -4.2036076 -4.169796 -4.1178436 -4.0957375 -4.12921 -4.1952882 -4.2616639 -4.3125753 -4.3361125 -4.3409348 -4.3363509][-4.2551908 -4.2528715 -4.23693 -4.2299204 -4.2231469 -4.1920829 -4.1416168 -4.1159248 -4.1443396 -4.2028604 -4.2640395 -4.3131347 -4.3368058 -4.3411326 -4.3374128]]...]
INFO - root - 2017-12-06 01:22:57.402918: step 61910, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 63h:05m:13s remains)
INFO - root - 2017-12-06 01:23:05.994745: step 61920, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 63h:58m:32s remains)
INFO - root - 2017-12-06 01:23:14.499002: step 61930, loss = 2.06, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 61h:13m:40s remains)
INFO - root - 2017-12-06 01:23:22.959991: step 61940, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 65h:21m:43s remains)
INFO - root - 2017-12-06 01:23:31.533843: step 61950, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.857 sec/batch; 64h:26m:30s remains)
INFO - root - 2017-12-06 01:23:40.028103: step 61960, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 65h:50m:19s remains)
INFO - root - 2017-12-06 01:23:48.662884: step 61970, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 64h:53m:31s remains)
INFO - root - 2017-12-06 01:23:57.189214: step 61980, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 63h:38m:53s remains)
INFO - root - 2017-12-06 01:24:05.598366: step 61990, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:46m:06s remains)
INFO - root - 2017-12-06 01:24:14.057719: step 62000, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.822 sec/batch; 61h:46m:56s remains)
2017-12-06 01:24:14.864774: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3167443 -4.3098493 -4.2921486 -4.2654018 -4.2333264 -4.1879292 -4.136467 -4.1141205 -4.1172333 -4.1346474 -4.1532869 -4.1512284 -4.1318078 -4.1266389 -4.1525297][-4.3172059 -4.3104849 -4.2934804 -4.2669787 -4.23828 -4.1936951 -4.1372242 -4.1101751 -4.1178331 -4.1408639 -4.1533513 -4.1414118 -4.1175766 -4.1140575 -4.1506381][-4.317102 -4.3103795 -4.2955575 -4.2698541 -4.2432632 -4.1996093 -4.1411247 -4.1144648 -4.1268778 -4.1528463 -4.1566319 -4.1293478 -4.0985241 -4.1003675 -4.149806][-4.3165412 -4.3096919 -4.2969346 -4.2728319 -4.2455077 -4.1996984 -4.138103 -4.1099286 -4.1212049 -4.1449766 -4.1433625 -4.108489 -4.0755205 -4.0856123 -4.1478515][-4.3158455 -4.3081074 -4.2955389 -4.2714858 -4.2406788 -4.1902065 -4.1230054 -4.0882721 -4.0925064 -4.1095467 -4.1037011 -4.070961 -4.0422854 -4.0678363 -4.1393743][-4.3151832 -4.3059883 -4.2925611 -4.2686906 -4.236095 -4.180769 -4.1103868 -4.0727668 -4.0710564 -4.0780821 -4.0633082 -4.0325694 -4.0159969 -4.0551763 -4.1314511][-4.314877 -4.3046441 -4.2906337 -4.2692537 -4.2399335 -4.1848187 -4.1170931 -4.080039 -4.07646 -4.0765066 -4.0606322 -4.0324807 -4.0198631 -4.0585971 -4.1263628][-4.3149004 -4.3050003 -4.2923818 -4.2757878 -4.2517362 -4.1996875 -4.1372452 -4.1020579 -4.0951533 -4.0911603 -4.0823207 -4.0626955 -4.0496197 -4.0738 -4.1232309][-4.3152089 -4.3064365 -4.2957821 -4.2836018 -4.2653046 -4.2181654 -4.1627846 -4.1308784 -4.122725 -4.11826 -4.112916 -4.0998554 -4.0848651 -4.091599 -4.1238933][-4.3155937 -4.3078356 -4.2986608 -4.2889977 -4.2755179 -4.2323465 -4.1786838 -4.1481876 -4.1428618 -4.1407838 -4.138608 -4.1335254 -4.1221037 -4.1177711 -4.1381645][-4.3160472 -4.3091712 -4.3006682 -4.2910485 -4.2777319 -4.234663 -4.1749945 -4.1419635 -4.1416097 -4.1511521 -4.1564445 -4.15674 -4.1488581 -4.1409307 -4.1568093][-4.3159456 -4.3099623 -4.3011708 -4.2882872 -4.2696953 -4.2207727 -4.1531377 -4.1156282 -4.1159482 -4.1373048 -4.1539297 -4.1592064 -4.1527166 -4.1434507 -4.1589956][-4.3154469 -4.3094234 -4.2994294 -4.2837915 -4.25923 -4.2027631 -4.1311207 -4.0925217 -4.0956678 -4.123919 -4.1464033 -4.1520777 -4.1466708 -4.136241 -4.1505532][-4.3149958 -4.3080816 -4.2960157 -4.27881 -4.250946 -4.1911664 -4.1200466 -4.0819039 -4.08502 -4.1119742 -4.1322513 -4.134614 -4.130064 -4.1197381 -4.13337][-4.31502 -4.3067322 -4.2915497 -4.2717528 -4.2403593 -4.1803322 -4.1141548 -4.0767088 -4.0775723 -4.102036 -4.1171494 -4.1096373 -4.104104 -4.0961776 -4.1128078]]...]
INFO - root - 2017-12-06 01:24:23.274564: step 62010, loss = 2.06, batch loss = 2.00 (10.4 examples/sec; 0.769 sec/batch; 57h:46m:27s remains)
INFO - root - 2017-12-06 01:24:31.797022: step 62020, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 61h:50m:12s remains)
INFO - root - 2017-12-06 01:24:40.314019: step 62030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.856 sec/batch; 64h:19m:56s remains)
INFO - root - 2017-12-06 01:24:48.765523: step 62040, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 65h:08m:22s remains)
INFO - root - 2017-12-06 01:24:57.360286: step 62050, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 64h:27m:16s remains)
INFO - root - 2017-12-06 01:25:05.912472: step 62060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 63h:56m:20s remains)
INFO - root - 2017-12-06 01:25:14.377804: step 62070, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 63h:26m:55s remains)
INFO - root - 2017-12-06 01:25:22.880400: step 62080, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 64h:04m:00s remains)
INFO - root - 2017-12-06 01:25:31.440586: step 62090, loss = 2.10, batch loss = 2.05 (9.4 examples/sec; 0.849 sec/batch; 63h:47m:09s remains)
INFO - root - 2017-12-06 01:25:39.954615: step 62100, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 64h:59m:59s remains)
2017-12-06 01:25:40.737657: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3656297 -4.3649244 -4.3664355 -4.3691907 -4.3725662 -4.3756971 -4.3779359 -4.3790731 -4.3792028 -4.3796663 -4.3800778 -4.3800483 -4.3792758 -4.377625 -4.3761115][-4.3468809 -4.3473434 -4.3497844 -4.35507 -4.3626046 -4.3690672 -4.3727708 -4.3744864 -4.3742986 -4.3749247 -4.37582 -4.3756065 -4.3730211 -4.3678155 -4.3620982][-4.3190861 -4.3209348 -4.3220882 -4.3267503 -4.3378649 -4.3496642 -4.3556957 -4.3589139 -4.3609457 -4.3645539 -4.3670135 -4.3668413 -4.361249 -4.3507838 -4.3387852][-4.2718668 -4.2746162 -4.2737317 -4.2737284 -4.2834635 -4.2983913 -4.3049831 -4.3097105 -4.3189545 -4.3310723 -4.3388696 -4.341011 -4.3340387 -4.3191085 -4.3029523][-4.2095156 -4.2060452 -4.201663 -4.1984897 -4.2022386 -4.2127991 -4.2168717 -4.2237387 -4.2442708 -4.2707467 -4.2878218 -4.2956052 -4.290236 -4.2733917 -4.2563772][-4.1336141 -4.1193089 -4.1093297 -4.1026778 -4.0934777 -4.0862474 -4.0773077 -4.0866833 -4.1272383 -4.1772027 -4.2104816 -4.229517 -4.2312469 -4.21719 -4.2025118][-4.0765619 -4.0564446 -4.0353451 -4.0161438 -3.982286 -3.9393353 -3.8988883 -3.9023678 -3.9701917 -4.0534897 -4.1124167 -4.1493134 -4.1664805 -4.1618576 -4.1543045][-4.0675044 -4.0443096 -4.0192165 -3.9927735 -3.9425941 -3.8675289 -3.7834661 -3.7530551 -3.8271484 -3.933255 -4.0152688 -4.0705628 -4.1079173 -4.1183362 -4.1213961][-4.0918217 -4.0716648 -4.0527196 -4.0342855 -3.9967408 -3.9362473 -3.8579473 -3.8106029 -3.8501523 -3.9296937 -3.9989991 -4.0476747 -4.08659 -4.1018863 -4.1105113][-4.1175909 -4.0962968 -4.0825405 -4.0730906 -4.0568933 -4.0274596 -3.979037 -3.9424515 -3.959446 -4.0109615 -4.0609231 -4.0957603 -4.1227689 -4.1311555 -4.1346116][-4.1524153 -4.1324611 -4.1176395 -4.1101885 -4.1062608 -4.0985284 -4.0750775 -4.0563669 -4.0682812 -4.1043005 -4.1390367 -4.1627378 -4.1805916 -4.1850333 -4.1841564][-4.1771169 -4.1589832 -4.1413851 -4.1300778 -4.1304021 -4.1339507 -4.1292467 -4.1287074 -4.1490555 -4.1850729 -4.2154937 -4.2323837 -4.2432256 -4.2458544 -4.2424731][-4.186255 -4.1692328 -4.1534977 -4.1417351 -4.1408567 -4.1461692 -4.1524849 -4.16468 -4.1951089 -4.2373009 -4.2708483 -4.2898335 -4.298059 -4.2995234 -4.293798][-4.1740022 -4.16162 -4.1505294 -4.1453562 -4.1480007 -4.1540089 -4.162674 -4.1795487 -4.2139821 -4.2601838 -4.29878 -4.3229389 -4.3325968 -4.3355188 -4.3300457][-4.16016 -4.1551528 -4.1533155 -4.1576338 -4.1675763 -4.1782374 -4.1863604 -4.2000284 -4.230824 -4.2747021 -4.3117132 -4.3362961 -4.3486514 -4.3523655 -4.3475256]]...]
INFO - root - 2017-12-06 01:25:49.301452: step 62110, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 63h:50m:26s remains)
INFO - root - 2017-12-06 01:25:57.771224: step 62120, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:59m:33s remains)
INFO - root - 2017-12-06 01:26:06.219470: step 62130, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.845 sec/batch; 63h:27m:55s remains)
INFO - root - 2017-12-06 01:26:14.748691: step 62140, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 65h:38m:51s remains)
INFO - root - 2017-12-06 01:26:23.419689: step 62150, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 65h:19m:19s remains)
INFO - root - 2017-12-06 01:26:32.083992: step 62160, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.867 sec/batch; 65h:08m:20s remains)
INFO - root - 2017-12-06 01:26:40.626026: step 62170, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.875 sec/batch; 65h:41m:13s remains)
INFO - root - 2017-12-06 01:26:49.244438: step 62180, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 66h:11m:34s remains)
INFO - root - 2017-12-06 01:26:57.858384: step 62190, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.834 sec/batch; 62h:36m:34s remains)
INFO - root - 2017-12-06 01:27:06.428021: step 62200, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 62h:42m:30s remains)
2017-12-06 01:27:07.256135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1837015 -4.1888018 -4.2008352 -4.2188568 -4.2252145 -4.2266765 -4.2286797 -4.2285171 -4.2269444 -4.2330213 -4.2445731 -4.2502127 -4.2432909 -4.2330027 -4.2291403][-4.1798487 -4.1750088 -4.1847343 -4.2050147 -4.2227483 -4.2350574 -4.2449617 -4.2519016 -4.2530246 -4.2540827 -4.2605534 -4.2645516 -4.2576733 -4.24857 -4.2464948][-4.1726122 -4.1562591 -4.1589355 -4.1779642 -4.2027178 -4.2244248 -4.2438517 -4.2609315 -4.2689619 -4.2682805 -4.2707028 -4.2693758 -4.257112 -4.2459307 -4.2442379][-4.1680293 -4.1466107 -4.1399736 -4.1505675 -4.1715446 -4.1943388 -4.2179193 -4.2401638 -4.2568564 -4.2642665 -4.2697015 -4.2668672 -4.2510509 -4.236856 -4.233531][-4.1682539 -4.1471472 -4.13426 -4.1337452 -4.1431561 -4.1560388 -4.1746569 -4.1965508 -4.2219343 -4.2429008 -4.2573295 -4.2567949 -4.240819 -4.2279892 -4.2278743][-4.1653347 -4.1455488 -4.1319232 -4.1260195 -4.1244259 -4.122726 -4.1256995 -4.1409993 -4.1745114 -4.2099385 -4.2363043 -4.239799 -4.2242308 -4.2129483 -4.2154045][-4.1719527 -4.1545377 -4.1410136 -4.1318135 -4.1196208 -4.0986867 -4.0770483 -4.07757 -4.115972 -4.1647816 -4.2010717 -4.2086267 -4.1929526 -4.1802044 -4.182344][-4.1885028 -4.1767859 -4.1645765 -4.1524296 -4.134254 -4.1027741 -4.0637603 -4.0494857 -4.0815392 -4.12922 -4.1642818 -4.1736355 -4.1625557 -4.1499553 -4.1474304][-4.1930323 -4.1905251 -4.1825418 -4.1717005 -4.1579046 -4.1340618 -4.0995359 -4.0807705 -4.0960646 -4.1267109 -4.1508861 -4.1560435 -4.1442704 -4.1308947 -4.1223516][-4.1833916 -4.1876907 -4.1833906 -4.1745186 -4.16875 -4.1607423 -4.1413093 -4.1266875 -4.1306944 -4.1463747 -4.1602244 -4.1586943 -4.1438446 -4.1288338 -4.1183872][-4.1711764 -4.1736655 -4.166863 -4.1590748 -4.1628885 -4.1730409 -4.172101 -4.1668534 -4.1663251 -4.173923 -4.1801052 -4.1742244 -4.1586576 -4.1452336 -4.1374083][-4.1696658 -4.16609 -4.1548638 -4.1457672 -4.1551795 -4.1775112 -4.1912622 -4.1963053 -4.1958494 -4.1974864 -4.1974111 -4.1874561 -4.1732593 -4.16599 -4.1651821][-4.1819735 -4.1738634 -4.1597419 -4.146193 -4.1522646 -4.1752429 -4.1959534 -4.207994 -4.208488 -4.2049713 -4.1980772 -4.1855397 -4.1752167 -4.1765089 -4.1860662][-4.204844 -4.1949086 -4.17981 -4.1625962 -4.1612825 -4.1755209 -4.1907611 -4.1997914 -4.2011123 -4.1984525 -4.1903033 -4.1767654 -4.1684518 -4.1755939 -4.1933804][-4.2346139 -4.2278314 -4.2134175 -4.1933093 -4.1827459 -4.1823273 -4.1839581 -4.1832223 -4.182817 -4.1842341 -4.1821027 -4.1726885 -4.1676965 -4.1767769 -4.1966233]]...]
INFO - root - 2017-12-06 01:27:15.781485: step 62210, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 63h:26m:19s remains)
INFO - root - 2017-12-06 01:27:24.198050: step 62220, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.838 sec/batch; 62h:56m:10s remains)
INFO - root - 2017-12-06 01:27:32.679092: step 62230, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 65h:45m:59s remains)
INFO - root - 2017-12-06 01:27:41.154344: step 62240, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.870 sec/batch; 65h:17m:00s remains)
INFO - root - 2017-12-06 01:27:49.647786: step 62250, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 63h:17m:53s remains)
INFO - root - 2017-12-06 01:27:58.231415: step 62260, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 63h:23m:52s remains)
INFO - root - 2017-12-06 01:28:06.882146: step 62270, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 64h:48m:59s remains)
INFO - root - 2017-12-06 01:28:15.367190: step 62280, loss = 2.09, batch loss = 2.03 (9.6 examples/sec; 0.833 sec/batch; 62h:33m:03s remains)
INFO - root - 2017-12-06 01:28:23.864844: step 62290, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:42m:27s remains)
INFO - root - 2017-12-06 01:28:32.404169: step 62300, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 63h:40m:56s remains)
2017-12-06 01:28:33.158469: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3166833 -4.3071604 -4.3000226 -4.2996254 -4.304461 -4.3118138 -4.3167105 -4.3165374 -4.3131847 -4.3121762 -4.312541 -4.3122268 -4.3134747 -4.3174157 -4.3204088][-4.2901492 -4.2737994 -4.26248 -4.2585874 -4.2631969 -4.2715282 -4.2777071 -4.27896 -4.2763567 -4.27523 -4.2746153 -4.2739735 -4.2755394 -4.281703 -4.2866621][-4.2610149 -4.2408385 -4.2283711 -4.2234569 -4.2273207 -4.2345037 -4.2383518 -4.2407103 -4.24546 -4.2509537 -4.2509632 -4.2507653 -4.2541366 -4.2613587 -4.2646971][-4.2399807 -4.2209616 -4.2123847 -4.2096553 -4.2123842 -4.2162027 -4.2147641 -4.2178316 -4.2318368 -4.2472191 -4.249227 -4.2502904 -4.2527075 -4.2585163 -4.2618375][-4.2258749 -4.2052579 -4.1979079 -4.1964092 -4.198173 -4.1971836 -4.1901555 -4.1898718 -4.2105494 -4.2359343 -4.2418184 -4.2438827 -4.2461109 -4.2539711 -4.2604904][-4.2141194 -4.1873636 -4.1739464 -4.1683741 -4.1665163 -4.1576018 -4.1420245 -4.1357884 -4.1623449 -4.198791 -4.2111773 -4.2194791 -4.2290421 -4.24272 -4.2518578][-4.2081051 -4.1787086 -4.1566958 -4.14319 -4.1344256 -4.1183958 -4.0867038 -4.0682545 -4.096282 -4.142446 -4.1657877 -4.1860213 -4.20983 -4.2289834 -4.2366352][-4.2086177 -4.1827221 -4.1570978 -4.1406646 -4.1252322 -4.0997319 -4.0481768 -4.0125861 -4.0353475 -4.0812445 -4.1062536 -4.139668 -4.177783 -4.2009015 -4.2084718][-4.2077274 -4.1840053 -4.1566982 -4.1435919 -4.1289358 -4.0999269 -4.0394578 -3.9923589 -4.0027366 -4.033813 -4.0507936 -4.089479 -4.1339092 -4.1587119 -4.1699409][-4.1989241 -4.171967 -4.1432047 -4.1354241 -4.1288147 -4.1035614 -4.0493679 -4.0032706 -4.0106869 -4.0256934 -4.0322409 -4.0691085 -4.1125636 -4.1359615 -4.1475096][-4.188786 -4.1518321 -4.1213331 -4.1198287 -4.1235094 -4.107059 -4.0690589 -4.0382838 -4.0534935 -4.0590868 -4.0599022 -4.0950427 -4.1323032 -4.1498704 -4.1571693][-4.188942 -4.1408343 -4.109355 -4.1123824 -4.1237664 -4.116622 -4.0947609 -4.0817857 -4.1038814 -4.1093588 -4.1128531 -4.1426487 -4.1705484 -4.1820254 -4.1839871][-4.2022824 -4.1511807 -4.1203418 -4.1202526 -4.1325111 -4.1338568 -4.1266127 -4.1241832 -4.1454992 -4.1538596 -4.1609292 -4.1804447 -4.1995678 -4.2103291 -4.216435][-4.2229724 -4.1758265 -4.1480122 -4.1450295 -4.1512718 -4.1569095 -4.1602964 -4.1658664 -4.1850314 -4.1928291 -4.1974611 -4.2076268 -4.2194028 -4.231647 -4.2448277][-4.2510023 -4.211689 -4.1881642 -4.1839547 -4.1855145 -4.1895628 -4.1942945 -4.2046514 -4.2226772 -4.2266917 -4.2264643 -4.2295947 -4.2369909 -4.248683 -4.2636886]]...]
INFO - root - 2017-12-06 01:28:41.613661: step 62310, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 64h:37m:35s remains)
INFO - root - 2017-12-06 01:28:50.171820: step 62320, loss = 2.10, batch loss = 2.04 (9.7 examples/sec; 0.826 sec/batch; 61h:58m:26s remains)
INFO - root - 2017-12-06 01:28:58.635163: step 62330, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.771 sec/batch; 57h:53m:07s remains)
INFO - root - 2017-12-06 01:29:07.074885: step 62340, loss = 2.07, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 66h:00m:02s remains)
INFO - root - 2017-12-06 01:29:15.713513: step 62350, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.862 sec/batch; 64h:41m:16s remains)
INFO - root - 2017-12-06 01:29:24.299053: step 62360, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.884 sec/batch; 66h:20m:35s remains)
INFO - root - 2017-12-06 01:29:32.858105: step 62370, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.871 sec/batch; 65h:19m:46s remains)
INFO - root - 2017-12-06 01:29:41.416031: step 62380, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:46m:25s remains)
INFO - root - 2017-12-06 01:29:50.039891: step 62390, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 64h:55m:22s remains)
INFO - root - 2017-12-06 01:29:58.551935: step 62400, loss = 2.06, batch loss = 2.00 (9.8 examples/sec; 0.816 sec/batch; 61h:11m:27s remains)
2017-12-06 01:29:59.370372: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1798706 -4.191462 -4.1933575 -4.1893897 -4.1891437 -4.18621 -4.1791859 -4.1677327 -4.1583943 -4.1602206 -4.1637769 -4.16167 -4.165061 -4.1774874 -4.1840386][-4.1582828 -4.173511 -4.1782684 -4.1733642 -4.1713057 -4.1714125 -4.1746416 -4.1764455 -4.1693287 -4.1666884 -4.168838 -4.1676364 -4.1669073 -4.1744161 -4.1813116][-4.1649942 -4.1797771 -4.1810169 -4.1676607 -4.1537533 -4.1511521 -4.162374 -4.1740808 -4.172339 -4.171174 -4.1771398 -4.1808934 -4.1778107 -4.1790953 -4.1828022][-4.1851516 -4.1907988 -4.1845403 -4.1620555 -4.1291776 -4.1171165 -4.1301904 -4.1476502 -4.153996 -4.1651015 -4.1836362 -4.1973753 -4.1970253 -4.1924157 -4.1902061][-4.1943474 -4.189168 -4.1688814 -4.1328812 -4.0830569 -4.0565095 -4.0683928 -4.09258 -4.1120777 -4.1414142 -4.1818037 -4.2132759 -4.2245536 -4.220036 -4.2088113][-4.1802955 -4.1673021 -4.1315422 -4.0816083 -4.019392 -3.9783468 -3.9821143 -4.0115814 -4.0502653 -4.1026011 -4.1663036 -4.2175045 -4.2418585 -4.2423315 -4.2294121][-4.14651 -4.1306219 -4.087636 -4.0301766 -3.9631906 -3.9042666 -3.8873847 -3.9171839 -3.9771836 -4.047431 -4.1257625 -4.1935053 -4.2314334 -4.2441673 -4.2391362][-4.10746 -4.0880442 -4.0489058 -4.0001097 -3.9395158 -3.8691678 -3.8319786 -3.8602805 -3.9360137 -4.012609 -4.0887947 -4.1571932 -4.2036772 -4.228292 -4.2341366][-4.0845227 -4.06737 -4.0366583 -4.0015788 -3.9515882 -3.8843424 -3.8427219 -3.8645611 -3.9388537 -4.012321 -4.0744247 -4.1307831 -4.1777625 -4.2107635 -4.2233119][-4.0838914 -4.0752625 -4.0597315 -4.0352397 -3.9950764 -3.9407125 -3.9038646 -3.9135923 -3.9708862 -4.0337105 -4.0788431 -4.1171989 -4.1557312 -4.1853538 -4.1938481][-4.0879493 -4.0895352 -4.0891294 -4.0760078 -4.041522 -3.9989076 -3.9661827 -3.9633017 -3.9997814 -4.0484495 -4.0848827 -4.1116867 -4.1385121 -4.1596231 -4.1624527][-4.0940285 -4.1079535 -4.1200356 -4.1160154 -4.0901966 -4.057126 -4.0275011 -4.0096712 -4.0227365 -4.057188 -4.0940132 -4.1197548 -4.1384931 -4.1532311 -4.15472][-4.1198845 -4.1376829 -4.1526723 -4.1537189 -4.1389689 -4.1146927 -4.0874677 -4.0589094 -4.0475039 -4.0643053 -4.099987 -4.1280279 -4.1452641 -4.160748 -4.1678424][-4.1771336 -4.1886539 -4.1992064 -4.2008548 -4.1900015 -4.1708875 -4.146524 -4.1140432 -4.0886459 -4.0923371 -4.1212878 -4.14663 -4.1622038 -4.1790848 -4.1942492][-4.2483974 -4.2524714 -4.2556477 -4.2580628 -4.2511263 -4.2359004 -4.2168903 -4.1887722 -4.1612716 -4.1554089 -4.1722369 -4.1889834 -4.2002749 -4.2163143 -4.235394]]...]
INFO - root - 2017-12-06 01:30:07.889136: step 62410, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 61h:45m:17s remains)
INFO - root - 2017-12-06 01:30:16.413672: step 62420, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 63h:07m:16s remains)
INFO - root - 2017-12-06 01:30:25.004311: step 62430, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:29m:08s remains)
INFO - root - 2017-12-06 01:30:33.368987: step 62440, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 63h:33m:18s remains)
INFO - root - 2017-12-06 01:30:42.037130: step 62450, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.828 sec/batch; 62h:04m:40s remains)
INFO - root - 2017-12-06 01:30:50.554169: step 62460, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:30m:15s remains)
INFO - root - 2017-12-06 01:30:59.103043: step 62470, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.869 sec/batch; 65h:10m:59s remains)
INFO - root - 2017-12-06 01:31:07.557191: step 62480, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.833 sec/batch; 62h:29m:45s remains)
INFO - root - 2017-12-06 01:31:16.161885: step 62490, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.841 sec/batch; 63h:03m:29s remains)
INFO - root - 2017-12-06 01:31:24.778329: step 62500, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 65h:00m:49s remains)
2017-12-06 01:31:25.574327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.152513 -4.1713781 -4.2017679 -4.2334814 -4.253376 -4.2557688 -4.2381716 -4.2106943 -4.1889391 -4.1860709 -4.1855946 -4.1798811 -4.1832566 -4.1995549 -4.2195897][-4.0780149 -4.0953903 -4.1312957 -4.1675849 -4.1892643 -4.1942029 -4.1736665 -4.1438313 -4.1235609 -4.1227913 -4.1242156 -4.1204982 -4.1325016 -4.1580348 -4.184937][-3.9967499 -4.0094376 -4.0470114 -4.0864539 -4.1113315 -4.1172557 -4.0976992 -4.0695124 -4.0531063 -4.0517159 -4.0564575 -4.0625005 -4.0882115 -4.1228633 -4.157917][-3.9459984 -3.9567771 -3.9964948 -4.0432873 -4.0715613 -4.071939 -4.0505157 -4.0248113 -4.0150166 -4.0148182 -4.0193014 -4.032908 -4.0698524 -4.1129608 -4.1529531][-3.9452281 -3.9598448 -3.9955034 -4.0396767 -4.0640831 -4.0605006 -4.0376005 -4.017076 -4.0160933 -4.0230012 -4.0275068 -4.0434222 -4.0850639 -4.1290574 -4.1658297][-3.9503715 -3.9668674 -3.9951017 -4.0318594 -4.055079 -4.0469408 -4.0157714 -3.9902663 -4.0028543 -4.0276351 -4.040175 -4.0572767 -4.0998187 -4.1391034 -4.1698565][-3.9332469 -3.9490268 -3.9778862 -4.0136905 -4.0375209 -4.0129232 -3.9521363 -3.9024267 -3.9236486 -3.9760413 -4.0152993 -4.0504408 -4.1015677 -4.1456537 -4.1756678][-3.8979154 -3.9219861 -3.9663618 -4.0074244 -4.0167575 -3.9577391 -3.847791 -3.7586298 -3.7888725 -3.8760538 -3.9519854 -4.0076742 -4.0717764 -4.129333 -4.1685576][-3.8643672 -3.8920062 -3.9454045 -3.9915204 -4.0017891 -3.9331353 -3.8041441 -3.6990488 -3.7278309 -3.822233 -3.9053228 -3.9604921 -4.0209827 -4.0818567 -4.1282306][-3.8642187 -3.8851185 -3.9363823 -3.9869268 -4.0113974 -3.9714954 -3.8837209 -3.8130474 -3.8243191 -3.8837698 -3.9373894 -3.9659164 -3.9992104 -4.0425682 -4.0859842][-3.907795 -3.9203632 -3.9644823 -4.0159116 -4.0548248 -4.0451069 -3.9958832 -3.9534864 -3.954824 -3.9863081 -4.0151157 -4.0193195 -4.022562 -4.0428762 -4.0758548][-3.9874153 -3.9941692 -4.03149 -4.0758286 -4.119225 -4.1273222 -4.09579 -4.0677247 -4.0680418 -4.0865521 -4.10221 -4.095386 -4.0878391 -4.0955443 -4.1199026][-4.084136 -4.0846415 -4.1155143 -4.1540976 -4.193656 -4.2041678 -4.1802626 -4.159349 -4.1608777 -4.1736984 -4.1838555 -4.1777391 -4.1731339 -4.1790581 -4.1970644][-4.1791687 -4.1771207 -4.1987839 -4.2269821 -4.2560663 -4.2628183 -4.2471457 -4.2335911 -4.2338367 -4.2399068 -4.2457304 -4.2460566 -4.2478356 -4.2514524 -4.2605033][-4.2563558 -4.2585292 -4.274817 -4.2918034 -4.3047252 -4.3059506 -4.2972226 -4.2892709 -4.2861094 -4.2864423 -4.28911 -4.2926722 -4.295424 -4.2963796 -4.299787]]...]
INFO - root - 2017-12-06 01:31:34.160551: step 62510, loss = 2.04, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 63h:28m:07s remains)
INFO - root - 2017-12-06 01:31:42.786393: step 62520, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 63h:56m:44s remains)
INFO - root - 2017-12-06 01:31:51.311268: step 62530, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 61h:42m:30s remains)
INFO - root - 2017-12-06 01:31:59.872286: step 62540, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.874 sec/batch; 65h:32m:53s remains)
INFO - root - 2017-12-06 01:32:08.215239: step 62550, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.865 sec/batch; 64h:51m:41s remains)
INFO - root - 2017-12-06 01:32:16.756428: step 62560, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.872 sec/batch; 65h:21m:28s remains)
INFO - root - 2017-12-06 01:32:25.355106: step 62570, loss = 2.08, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 63h:44m:03s remains)
INFO - root - 2017-12-06 01:32:33.875511: step 62580, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:28m:03s remains)
INFO - root - 2017-12-06 01:32:42.459306: step 62590, loss = 2.10, batch loss = 2.04 (9.3 examples/sec; 0.859 sec/batch; 64h:23m:11s remains)
INFO - root - 2017-12-06 01:32:51.114825: step 62600, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.837 sec/batch; 62h:43m:37s remains)
2017-12-06 01:32:51.872295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1020083 -4.1008334 -4.1033053 -4.0983129 -4.0864382 -4.0750289 -4.0680113 -4.0797291 -4.1135607 -4.148592 -4.1635013 -4.1764917 -4.1876812 -4.1822605 -4.1531582][-4.1175809 -4.1015425 -4.0937061 -4.0857096 -4.0749149 -4.05387 -4.03687 -4.048254 -4.0841703 -4.1208258 -4.1392303 -4.161551 -4.1852374 -4.1954684 -4.1759295][-4.1534162 -4.1289124 -4.1069937 -4.084168 -4.0569186 -4.0188274 -3.9900758 -4.0029812 -4.0458255 -4.09014 -4.1133924 -4.1439948 -4.1831379 -4.2089195 -4.203125][-4.2092385 -4.1778059 -4.1398516 -4.0981493 -4.043653 -3.9813514 -3.9450624 -3.9696007 -4.0305481 -4.0826964 -4.1074228 -4.1369977 -4.1785331 -4.2168727 -4.2291403][-4.2544961 -4.2213058 -4.1700416 -4.104898 -4.03061 -3.9592433 -3.9296317 -3.9655387 -4.0329633 -4.089716 -4.1182342 -4.1462193 -4.1837149 -4.2245989 -4.251904][-4.2767725 -4.245708 -4.1860194 -4.1076627 -4.027916 -3.9582865 -3.9389849 -3.9811568 -4.0435677 -4.1029043 -4.1334214 -4.1634574 -4.2015858 -4.2400918 -4.268939][-4.2834067 -4.2532167 -4.1932359 -4.110672 -4.0338244 -3.9681139 -3.9473982 -3.9856815 -4.0443473 -4.1088271 -4.1503983 -4.1890874 -4.231832 -4.2661929 -4.2896972][-4.2807589 -4.2547607 -4.202312 -4.1239223 -4.0520649 -3.9848886 -3.9483495 -3.9765041 -4.0370574 -4.1120362 -4.1724176 -4.2186127 -4.2617073 -4.2914996 -4.3100986][-4.2738323 -4.2515206 -4.2102938 -4.1414762 -4.0742974 -4.0064878 -3.9553447 -3.9808218 -4.04828 -4.1280956 -4.1954908 -4.2433629 -4.2845445 -4.3114958 -4.3268895][-4.2593508 -4.2408743 -4.2112 -4.1534204 -4.0909104 -4.0255322 -3.9783695 -4.0097728 -4.0804892 -4.1555805 -4.2161913 -4.2567554 -4.2938232 -4.3191481 -4.3367496][-4.2434525 -4.2282143 -4.2082577 -4.165463 -4.11254 -4.0581274 -4.0275545 -4.0616503 -4.1268945 -4.1921329 -4.2366333 -4.2642994 -4.2933655 -4.3174582 -4.3375859][-4.2285852 -4.2167053 -4.2023721 -4.1741252 -4.136116 -4.0973444 -4.0833721 -4.113843 -4.1667809 -4.2182627 -4.2497396 -4.2672324 -4.2903013 -4.3130803 -4.3346643][-4.2188492 -4.2109237 -4.2004685 -4.184864 -4.1610775 -4.1381469 -4.1317339 -4.1526723 -4.1948791 -4.2366948 -4.2590113 -4.2699847 -4.2886138 -4.3106813 -4.3338246][-4.2181158 -4.2109017 -4.204319 -4.1975389 -4.1860785 -4.1737108 -4.1730938 -4.184289 -4.2123547 -4.2438145 -4.2624011 -4.2717733 -4.2895374 -4.31152 -4.3345404][-4.2202735 -4.2103362 -4.20737 -4.2073855 -4.2043004 -4.2037916 -4.20624 -4.2095432 -4.2218957 -4.2428131 -4.2598653 -4.2726965 -4.2914782 -4.3122163 -4.3335261]]...]
INFO - root - 2017-12-06 01:33:00.541746: step 62610, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.862 sec/batch; 64h:36m:40s remains)
INFO - root - 2017-12-06 01:33:09.163726: step 62620, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.840 sec/batch; 62h:57m:45s remains)
INFO - root - 2017-12-06 01:33:17.782140: step 62630, loss = 2.01, batch loss = 1.96 (9.5 examples/sec; 0.844 sec/batch; 63h:15m:26s remains)
INFO - root - 2017-12-06 01:33:26.133971: step 62640, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.850 sec/batch; 63h:42m:59s remains)
INFO - root - 2017-12-06 01:33:34.629850: step 62650, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 65h:14m:34s remains)
INFO - root - 2017-12-06 01:33:43.189388: step 62660, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.865 sec/batch; 64h:49m:23s remains)
INFO - root - 2017-12-06 01:33:51.738119: step 62670, loss = 2.03, batch loss = 1.98 (9.5 examples/sec; 0.846 sec/batch; 63h:23m:58s remains)
INFO - root - 2017-12-06 01:34:00.339542: step 62680, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.850 sec/batch; 63h:42m:49s remains)
INFO - root - 2017-12-06 01:34:08.880548: step 62690, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 65h:05m:13s remains)
INFO - root - 2017-12-06 01:34:17.417773: step 62700, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.854 sec/batch; 63h:59m:00s remains)
2017-12-06 01:34:18.212142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1292796 -4.1197019 -4.1217961 -4.1586552 -4.1950111 -4.2035365 -4.206943 -4.2174945 -4.2045131 -4.1859932 -4.1806054 -4.1683187 -4.163722 -4.158412 -4.1396184][-4.1238337 -4.1131663 -4.1180778 -4.1525683 -4.18803 -4.1955657 -4.1966872 -4.2099771 -4.2098322 -4.2067013 -4.2029071 -4.1866207 -4.1802788 -4.1698833 -4.1432786][-4.1366792 -4.1279206 -4.1337714 -4.1605482 -4.1874642 -4.1932673 -4.193162 -4.2088771 -4.221056 -4.2310143 -4.2277813 -4.2056818 -4.1938934 -4.1772213 -4.1410027][-4.1741881 -4.1754231 -4.1762142 -4.1842427 -4.1905093 -4.1863184 -4.1841087 -4.2002468 -4.2202792 -4.2373137 -4.2337933 -4.2075133 -4.1882949 -4.163301 -4.1169038][-4.1937261 -4.2004876 -4.1929879 -4.1716657 -4.1443186 -4.1240773 -4.12219 -4.143775 -4.17391 -4.2013884 -4.2047219 -4.1874537 -4.1722679 -4.1443067 -4.1020927][-4.1807079 -4.1840439 -4.1546831 -4.0955291 -4.0271339 -3.9885175 -3.9963098 -4.0283294 -4.0716214 -4.1174493 -4.1396389 -4.14745 -4.1479812 -4.1322727 -4.1104431][-4.1538939 -4.1441808 -4.0917292 -3.9924221 -3.8833423 -3.8410296 -3.8711114 -3.9182343 -3.9744775 -4.0443769 -4.0917721 -4.1257148 -4.1476941 -4.1531386 -4.1464987][-4.13762 -4.1229367 -4.0682764 -3.9630733 -3.8507662 -3.8181844 -3.8643732 -3.9164085 -3.974745 -4.0514131 -4.1079288 -4.1479306 -4.1762795 -4.1916566 -4.1894665][-4.1464939 -4.137857 -4.1018014 -4.0267158 -3.942574 -3.9199939 -3.9610975 -4.0066285 -4.0520735 -4.1092529 -4.1565061 -4.1895137 -4.2119651 -4.2262115 -4.2236][-4.1626997 -4.1606336 -4.1439924 -4.1034255 -4.0557122 -4.0440159 -4.072505 -4.1076517 -4.1398039 -4.1742191 -4.2082272 -4.2325869 -4.2479792 -4.2524452 -4.2415295][-4.1819525 -4.18679 -4.1827154 -4.1656952 -4.1422491 -4.1367655 -4.1572723 -4.18385 -4.2067032 -4.2283263 -4.2490916 -4.260006 -4.2643247 -4.2587624 -4.243834][-4.1885304 -4.1937256 -4.1929488 -4.1873255 -4.1765366 -4.1759491 -4.1962261 -4.2191715 -4.2339511 -4.2457933 -4.2540522 -4.2549553 -4.2521067 -4.2432518 -4.2309484][-4.1753297 -4.1767092 -4.1739655 -4.1710505 -4.167738 -4.1715822 -4.18707 -4.2024822 -4.208559 -4.2118397 -4.214263 -4.2143111 -4.2131929 -4.210423 -4.2075124][-4.1677542 -4.163651 -4.1566405 -4.1527996 -4.152194 -4.1545715 -4.1611052 -4.1677933 -4.1706748 -4.1731758 -4.1763353 -4.1798558 -4.1840529 -4.1879034 -4.1918941][-4.1758871 -4.168335 -4.1599565 -4.1548777 -4.1533837 -4.1534677 -4.1555538 -4.1588488 -4.1622024 -4.1662717 -4.1718082 -4.1789694 -4.1863437 -4.192697 -4.1978054]]...]
INFO - root - 2017-12-06 01:34:26.760901: step 62710, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.874 sec/batch; 65h:31m:40s remains)
INFO - root - 2017-12-06 01:34:35.343238: step 62720, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 64h:39m:18s remains)
INFO - root - 2017-12-06 01:34:43.968103: step 62730, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 64h:51m:20s remains)
INFO - root - 2017-12-06 01:34:52.484367: step 62740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:51m:48s remains)
INFO - root - 2017-12-06 01:35:00.998235: step 62750, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.846 sec/batch; 63h:23m:57s remains)
INFO - root - 2017-12-06 01:35:09.422583: step 62760, loss = 2.04, batch loss = 1.98 (9.9 examples/sec; 0.807 sec/batch; 60h:28m:41s remains)
INFO - root - 2017-12-06 01:35:17.846579: step 62770, loss = 2.09, batch loss = 2.03 (9.9 examples/sec; 0.809 sec/batch; 60h:36m:41s remains)
INFO - root - 2017-12-06 01:35:26.407463: step 62780, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:48m:58s remains)
INFO - root - 2017-12-06 01:35:35.004399: step 62790, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 63h:43m:56s remains)
INFO - root - 2017-12-06 01:35:43.490341: step 62800, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.871 sec/batch; 65h:13m:48s remains)
2017-12-06 01:35:44.274509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1498966 -4.1701059 -4.187243 -4.196444 -4.2017484 -4.19762 -4.1968327 -4.2008715 -4.2136197 -4.22968 -4.2367392 -4.2163634 -4.1705289 -4.1235929 -4.1144347][-4.0976357 -4.1190114 -4.1390347 -4.1549516 -4.1663923 -4.1641626 -4.1621857 -4.1670704 -4.1844244 -4.2055497 -4.2140732 -4.1922307 -4.1422782 -4.0901251 -4.0782285][-4.0430632 -4.0682483 -4.093565 -4.1177621 -4.13762 -4.1375527 -4.1272044 -4.126277 -4.1438975 -4.1701632 -4.1870389 -4.1716571 -4.1262889 -4.0720644 -4.0547915][-4.0137873 -4.0301509 -4.0538883 -4.0782156 -4.0994158 -4.0933814 -4.0710835 -4.0705457 -4.0909262 -4.1289692 -4.1609974 -4.1553764 -4.1184707 -4.0644803 -4.0452671][-4.0390863 -4.0369463 -4.0487127 -4.0541687 -4.0544791 -4.0275922 -3.9895177 -4.0005021 -4.0407543 -4.0993924 -4.1503348 -4.1580553 -4.1298437 -4.0778542 -4.0572362][-4.0873508 -4.0715394 -4.0710807 -4.0622854 -4.040174 -3.9918139 -3.9325621 -3.9384055 -3.9941072 -4.0669293 -4.1319752 -4.1529856 -4.1407371 -4.1013174 -4.0820236][-4.1143179 -4.0915289 -4.0840554 -4.0686746 -4.0387492 -3.9921696 -3.9338658 -3.9290566 -3.9792621 -4.0483351 -4.1144037 -4.145215 -4.1488433 -4.1259775 -4.1107039][-4.1118984 -4.0920253 -4.0789709 -4.0577927 -4.0286851 -3.9964645 -3.9601924 -3.9614208 -4.0070319 -4.0692191 -4.1187558 -4.1375532 -4.1402168 -4.1244359 -4.1131811][-4.116509 -4.1076784 -4.09435 -4.0694447 -4.0387893 -4.0134697 -3.9971082 -4.0132041 -4.0583348 -4.1128535 -4.1420012 -4.1367645 -4.124732 -4.1070948 -4.1006846][-4.1345253 -4.1353884 -4.1259389 -4.1003604 -4.0719638 -4.0471263 -4.0421729 -4.0699534 -4.1121988 -4.153502 -4.1643367 -4.14033 -4.1134477 -4.092669 -4.0882859][-4.1402774 -4.1481586 -4.1414347 -4.1182017 -4.0920739 -4.0703635 -4.0740547 -4.1061268 -4.1431313 -4.1697297 -4.1693277 -4.1417079 -4.1068673 -4.0763254 -4.0664458][-4.1381316 -4.1504965 -4.1453962 -4.1264052 -4.1038408 -4.0892639 -4.0969911 -4.1232843 -4.1527867 -4.171011 -4.1661897 -4.1397285 -4.10049 -4.0635214 -4.0513635][-4.1610589 -4.1753683 -4.1751914 -4.161027 -4.1444597 -4.1395493 -4.1490583 -4.1687465 -4.1885324 -4.2005415 -4.1903348 -4.1593876 -4.1196322 -4.0864253 -4.0810642][-4.2022247 -4.2167897 -4.2216191 -4.2105279 -4.1976476 -4.2023172 -4.2171521 -4.2312436 -4.24194 -4.2478895 -4.2362728 -4.2039623 -4.1674004 -4.1407156 -4.1407928][-4.2473078 -4.2596664 -4.2659488 -4.2566743 -4.2447844 -4.2506423 -4.2634549 -4.2710428 -4.276403 -4.2800994 -4.2738509 -4.247705 -4.2154393 -4.1924305 -4.1956263]]...]
INFO - root - 2017-12-06 01:35:52.807471: step 62810, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.855 sec/batch; 64h:04m:16s remains)
INFO - root - 2017-12-06 01:36:01.376561: step 62820, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 65h:20m:04s remains)
INFO - root - 2017-12-06 01:36:10.046767: step 62830, loss = 2.06, batch loss = 2.01 (9.2 examples/sec; 0.868 sec/batch; 65h:02m:30s remains)
INFO - root - 2017-12-06 01:36:18.439980: step 62840, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.885 sec/batch; 66h:16m:10s remains)
INFO - root - 2017-12-06 01:36:26.983714: step 62850, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.830 sec/batch; 62h:10m:43s remains)
INFO - root - 2017-12-06 01:36:35.614307: step 62860, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 64h:08m:18s remains)
INFO - root - 2017-12-06 01:36:44.020965: step 62870, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.836 sec/batch; 62h:38m:27s remains)
INFO - root - 2017-12-06 01:36:52.392908: step 62880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 64h:39m:56s remains)
INFO - root - 2017-12-06 01:37:00.967689: step 62890, loss = 2.04, batch loss = 1.99 (9.2 examples/sec; 0.867 sec/batch; 64h:54m:26s remains)
INFO - root - 2017-12-06 01:37:09.539205: step 62900, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.862 sec/batch; 64h:31m:06s remains)
2017-12-06 01:37:10.320069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1516113 -4.0996718 -4.0800595 -4.1024647 -4.1506453 -4.1982594 -4.2262936 -4.2225103 -4.2096214 -4.2047725 -4.2112789 -4.2176657 -4.2125797 -4.2063289 -4.2060757][-4.1370463 -4.0882974 -4.0777025 -4.1166329 -4.1733165 -4.2126994 -4.2239637 -4.2060051 -4.1866665 -4.18678 -4.2062931 -4.2217464 -4.2159352 -4.2038689 -4.20085][-4.1433392 -4.1078868 -4.1057835 -4.1478562 -4.1977739 -4.219471 -4.2042556 -4.163589 -4.1374464 -4.1513057 -4.1945648 -4.2289219 -4.231739 -4.2232943 -4.2212687][-4.1581697 -4.1357017 -4.1412725 -4.17813 -4.2136321 -4.2194943 -4.1787148 -4.1084933 -4.0719781 -4.0997095 -4.1681118 -4.2262769 -4.247714 -4.2536716 -4.2582216][-4.1888785 -4.1766357 -4.1860681 -4.2132988 -4.2333117 -4.220612 -4.1517482 -4.0479631 -3.9940619 -4.0302291 -4.1214309 -4.205431 -4.2476077 -4.2679138 -4.2817802][-4.2201161 -4.2198195 -4.2317238 -4.249887 -4.2523661 -4.21871 -4.12655 -3.9996841 -3.9285665 -3.9708447 -4.0792065 -4.1805315 -4.2390122 -4.2681127 -4.2848487][-4.2449141 -4.2549272 -4.2659407 -4.2761869 -4.2634463 -4.2072635 -4.0999975 -3.9616475 -3.8814263 -3.9247332 -4.0411148 -4.1533294 -4.2228518 -4.2553892 -4.2714472][-4.263514 -4.274848 -4.2789645 -4.2765245 -4.251677 -4.1841125 -4.07486 -3.9403329 -3.8617041 -3.9051008 -4.026176 -4.1428442 -4.2174654 -4.2491765 -4.2603078][-4.2647605 -4.2718363 -4.26815 -4.2580047 -4.2332444 -4.173398 -4.0776939 -3.9612668 -3.8973665 -3.9415362 -4.0544906 -4.1639948 -4.23323 -4.2580743 -4.2622881][-4.2606993 -4.2642021 -4.2549167 -4.2426171 -4.2268929 -4.1849566 -4.1100721 -4.0189896 -3.9761593 -4.020071 -4.1100025 -4.2011137 -4.2593775 -4.2754269 -4.2705474][-4.2562733 -4.2574973 -4.2439008 -4.2312603 -4.2225428 -4.1967349 -4.144433 -4.0858107 -4.0666242 -4.1060438 -4.1733255 -4.2420712 -4.2902961 -4.29996 -4.2875853][-4.2516332 -4.2523112 -4.2397847 -4.2295737 -4.2252684 -4.208714 -4.1769166 -4.1479034 -4.1495986 -4.185699 -4.2357974 -4.2830691 -4.3172417 -4.3162732 -4.2970428][-4.252142 -4.2510071 -4.2391462 -4.229085 -4.2250652 -4.2172031 -4.2038751 -4.1986265 -4.2126546 -4.2457786 -4.2807689 -4.3094382 -4.3253889 -4.3139315 -4.2928109][-4.2602491 -4.2572002 -4.2450671 -4.2328482 -4.2278795 -4.2260737 -4.2268643 -4.2371778 -4.2570319 -4.2837586 -4.30624 -4.3207426 -4.3200593 -4.3000965 -4.2805252][-4.2685256 -4.2646422 -4.2534146 -4.2390227 -4.2299809 -4.2301173 -4.2378154 -4.256135 -4.2790313 -4.3006377 -4.3133721 -4.3146195 -4.3009224 -4.2761841 -4.2594957]]...]
INFO - root - 2017-12-06 01:37:18.750450: step 62910, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 63h:31m:51s remains)
INFO - root - 2017-12-06 01:37:27.218894: step 62920, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.836 sec/batch; 62h:36m:56s remains)
INFO - root - 2017-12-06 01:37:35.731190: step 62930, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 65h:24m:40s remains)
INFO - root - 2017-12-06 01:37:44.173739: step 62940, loss = 2.03, batch loss = 1.98 (9.2 examples/sec; 0.867 sec/batch; 64h:55m:46s remains)
INFO - root - 2017-12-06 01:37:52.719447: step 62950, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.861 sec/batch; 64h:25m:56s remains)
INFO - root - 2017-12-06 01:38:01.157223: step 62960, loss = 2.03, batch loss = 1.97 (9.7 examples/sec; 0.823 sec/batch; 61h:37m:03s remains)
INFO - root - 2017-12-06 01:38:09.582505: step 62970, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.847 sec/batch; 63h:23m:33s remains)
INFO - root - 2017-12-06 01:38:18.004656: step 62980, loss = 2.04, batch loss = 1.98 (10.0 examples/sec; 0.798 sec/batch; 59h:46m:14s remains)
INFO - root - 2017-12-06 01:38:26.635142: step 62990, loss = 2.12, batch loss = 2.06 (9.3 examples/sec; 0.858 sec/batch; 64h:13m:37s remains)
INFO - root - 2017-12-06 01:38:35.196491: step 63000, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.815 sec/batch; 61h:02m:05s remains)
2017-12-06 01:38:36.017322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0438671 -4.0809107 -4.1123781 -4.11421 -4.1028566 -4.10031 -4.112289 -4.1233468 -4.1165118 -4.108645 -4.1202188 -4.1277952 -4.1256075 -4.1364903 -4.142117][-4.0557313 -4.0997257 -4.1319275 -4.12804 -4.1149659 -4.1070032 -4.1163697 -4.1288671 -4.1278262 -4.1208758 -4.1342211 -4.1443138 -4.1411762 -4.14988 -4.1579][-4.088943 -4.1247964 -4.1457148 -4.1344543 -4.118443 -4.1039705 -4.1082416 -4.1221561 -4.1333122 -4.1380415 -4.1537147 -4.1655045 -4.1558843 -4.1516657 -4.151052][-4.0982685 -4.1205487 -4.1346827 -4.1230092 -4.110096 -4.0964766 -4.0938954 -4.1032 -4.122015 -4.138392 -4.1592646 -4.1708651 -4.1534357 -4.1342878 -4.1209359][-4.086628 -4.0992532 -4.1126223 -4.1085339 -4.103539 -4.0847597 -4.0590672 -4.0506129 -4.0798516 -4.1155162 -4.1432481 -4.1451769 -4.1183834 -4.092175 -4.0750375][-4.0706897 -4.0769086 -4.0912266 -4.0888796 -4.0825033 -4.046917 -3.9866922 -3.9540453 -3.9979045 -4.0630312 -4.1035361 -4.1028247 -4.07749 -4.0594921 -4.0528593][-4.0631466 -4.05776 -4.0668278 -4.0581861 -4.0332546 -3.9642785 -3.8587866 -3.8031778 -3.8803983 -3.9868858 -4.0442681 -4.0531726 -4.0473247 -4.0558205 -4.0654163][-4.0957561 -4.0699096 -4.0591178 -4.0371985 -3.9957325 -3.90339 -3.7796485 -3.7266898 -3.827004 -3.9498034 -4.0112495 -4.034162 -4.0549865 -4.0907912 -4.1198783][-4.1486163 -4.1168342 -4.0960941 -4.0728927 -4.0315809 -3.9473929 -3.8542161 -3.8303733 -3.9117477 -4.0018616 -4.0421505 -4.0604944 -4.093998 -4.1483345 -4.1915078][-4.1647692 -4.1420794 -4.1263375 -4.1062055 -4.0730128 -4.009923 -3.9521351 -3.9480419 -4.0045452 -4.0556393 -4.0669856 -4.0737219 -4.1111197 -4.1740737 -4.2213855][-4.14412 -4.1395874 -4.1345973 -4.1172853 -4.0916333 -4.0513787 -4.0257425 -4.0377655 -4.0769958 -4.0970387 -4.0860291 -4.08512 -4.1168242 -4.1673846 -4.2003093][-4.1186838 -4.1315145 -4.1375623 -4.1240697 -4.1066966 -4.0883408 -4.0876284 -4.1097956 -4.1364079 -4.1425848 -4.1293159 -4.1234112 -4.1402936 -4.1677537 -4.1781769][-4.1164713 -4.1437941 -4.1597981 -4.14791 -4.1371622 -4.1341729 -4.1473556 -4.1664395 -4.1789784 -4.1772585 -4.1682277 -4.1629996 -4.1692624 -4.1801476 -4.1768246][-4.1319203 -4.1638513 -4.1839046 -4.1763844 -4.1716137 -4.175601 -4.1880136 -4.1991343 -4.203547 -4.2013574 -4.1987743 -4.196238 -4.198163 -4.2006469 -4.1933212][-4.1617918 -4.1863956 -4.2026711 -4.1992774 -4.1992011 -4.2049141 -4.2130761 -4.2187128 -4.2208204 -4.2194695 -4.2186575 -4.21804 -4.2194114 -4.2202835 -4.2141604]]...]
INFO - root - 2017-12-06 01:38:44.620510: step 63010, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.840 sec/batch; 62h:51m:18s remains)
INFO - root - 2017-12-06 01:38:53.101990: step 63020, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.837 sec/batch; 62h:37m:00s remains)
INFO - root - 2017-12-06 01:39:01.478069: step 63030, loss = 2.06, batch loss = 2.00 (10.3 examples/sec; 0.774 sec/batch; 57h:55m:15s remains)
INFO - root - 2017-12-06 01:39:10.023772: step 63040, loss = 2.09, batch loss = 2.03 (9.4 examples/sec; 0.854 sec/batch; 63h:56m:00s remains)
INFO - root - 2017-12-06 01:39:18.532379: step 63050, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.860 sec/batch; 64h:22m:36s remains)
INFO - root - 2017-12-06 01:39:27.047389: step 63060, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.852 sec/batch; 63h:44m:47s remains)
INFO - root - 2017-12-06 01:39:35.687856: step 63070, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.822 sec/batch; 61h:29m:54s remains)
INFO - root - 2017-12-06 01:39:44.159814: step 63080, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 64h:45m:06s remains)
INFO - root - 2017-12-06 01:39:52.586686: step 63090, loss = 2.05, batch loss = 1.99 (10.7 examples/sec; 0.751 sec/batch; 56h:10m:59s remains)
INFO - root - 2017-12-06 01:40:01.155198: step 63100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.862 sec/batch; 64h:31m:23s remains)
2017-12-06 01:40:01.925275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1191187 -4.1093373 -4.1092739 -4.1097956 -4.12472 -4.1496978 -4.1569705 -4.1335864 -4.1135287 -4.1325507 -4.152895 -4.1850405 -4.2260876 -4.2694025 -4.2977066][-4.0725861 -4.0748897 -4.0905614 -4.099123 -4.1095252 -4.126236 -4.1369753 -4.1142225 -4.0959358 -4.1138644 -4.1366892 -4.1664062 -4.2096334 -4.2585511 -4.2935467][-4.066186 -4.0824356 -4.107677 -4.120997 -4.1229682 -4.1230536 -4.124198 -4.0997472 -4.0858521 -4.104269 -4.1326685 -4.1632004 -4.20421 -4.2552423 -4.2952857][-4.1119375 -4.1362362 -4.1596417 -4.1653881 -4.1585026 -4.1391354 -4.1220388 -4.0974774 -4.0910716 -4.1063619 -4.1357546 -4.1687093 -4.2071691 -4.2574043 -4.2988772][-4.1439333 -4.1692929 -4.18916 -4.1855145 -4.1636519 -4.1179233 -4.0771952 -4.0580959 -4.0757494 -4.10204 -4.13629 -4.1710615 -4.2103686 -4.2595544 -4.3006248][-4.1523976 -4.1712818 -4.1776872 -4.1531119 -4.1084213 -4.0299773 -3.9597998 -3.9550493 -4.0225425 -4.08589 -4.1387038 -4.1815553 -4.2192049 -4.2610645 -4.2983351][-4.1519761 -4.1588759 -4.1509027 -4.1062794 -4.0308337 -3.9122334 -3.7956564 -3.79707 -3.9240503 -4.0411406 -4.1159124 -4.1697083 -4.2127004 -4.2524533 -4.2878594][-4.1465158 -4.1482964 -4.1381583 -4.0984383 -4.0290689 -3.9125197 -3.7879946 -3.7814441 -3.9072983 -4.0354147 -4.1142025 -4.1690059 -4.2147961 -4.251677 -4.2819247][-4.1601515 -4.1617861 -4.1610031 -4.1467934 -4.1118937 -4.03691 -3.9473643 -3.9301755 -4.0064411 -4.0939374 -4.1493344 -4.1883583 -4.2300978 -4.2661934 -4.2909718][-4.1946621 -4.1950736 -4.2004447 -4.2025328 -4.19114 -4.1479058 -4.089633 -4.0661469 -4.1000466 -4.1496563 -4.1846323 -4.2115817 -4.248703 -4.2825055 -4.3026352][-4.2112274 -4.2135944 -4.228663 -4.2403846 -4.2375455 -4.2153831 -4.180099 -4.1540427 -4.1625462 -4.1900954 -4.2152638 -4.237226 -4.2686715 -4.2978444 -4.3133445][-4.1998382 -4.208724 -4.2306266 -4.2457795 -4.249248 -4.2422504 -4.2239623 -4.2014375 -4.1980171 -4.2135358 -4.2346539 -4.2553892 -4.2805953 -4.3044152 -4.3161297][-4.179388 -4.1880822 -4.2057991 -4.2206326 -4.23236 -4.2412529 -4.2389235 -4.2235866 -4.2187357 -4.2279282 -4.2435489 -4.2609825 -4.2802334 -4.2989316 -4.3098927][-4.1756363 -4.179471 -4.188673 -4.2000108 -4.2129297 -4.227077 -4.2330618 -4.2263575 -4.2248411 -4.2314076 -4.2441249 -4.259387 -4.2757654 -4.2926779 -4.3048353][-4.198266 -4.1978688 -4.1987634 -4.2034364 -4.2106524 -4.2220731 -4.230679 -4.230659 -4.2346044 -4.24285 -4.2547112 -4.2698298 -4.2857084 -4.300549 -4.3100667]]...]
INFO - root - 2017-12-06 01:40:10.517858: step 63110, loss = 2.08, batch loss = 2.02 (9.7 examples/sec; 0.827 sec/batch; 61h:52m:40s remains)
INFO - root - 2017-12-06 01:40:19.011364: step 63120, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.864 sec/batch; 64h:39m:53s remains)
INFO - root - 2017-12-06 01:40:27.375522: step 63130, loss = 2.05, batch loss = 2.00 (11.3 examples/sec; 0.710 sec/batch; 53h:07m:28s remains)
INFO - root - 2017-12-06 01:40:35.976763: step 63140, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.877 sec/batch; 65h:35m:57s remains)
INFO - root - 2017-12-06 01:40:44.462524: step 63150, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.847 sec/batch; 63h:20m:15s remains)
INFO - root - 2017-12-06 01:40:52.901913: step 63160, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.846 sec/batch; 63h:16m:22s remains)
INFO - root - 2017-12-06 01:41:01.382601: step 63170, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:35m:19s remains)
INFO - root - 2017-12-06 01:41:09.872160: step 63180, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 64h:58m:41s remains)
INFO - root - 2017-12-06 01:41:18.389288: step 63190, loss = 2.04, batch loss = 1.98 (9.7 examples/sec; 0.828 sec/batch; 61h:56m:56s remains)
INFO - root - 2017-12-06 01:41:26.874744: step 63200, loss = 2.06, batch loss = 2.01 (10.0 examples/sec; 0.800 sec/batch; 59h:48m:26s remains)
2017-12-06 01:41:27.615356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2723842 -4.2684636 -4.2667584 -4.2620611 -4.2544861 -4.2519464 -4.2673912 -4.2814989 -4.2897782 -4.294816 -4.2922592 -4.2884 -4.2838717 -4.2688255 -4.22706][-4.2645063 -4.262568 -4.257194 -4.2424841 -4.227108 -4.2196264 -4.2358565 -4.2488194 -4.2595162 -4.270659 -4.2729354 -4.2706261 -4.2654872 -4.2458119 -4.1916823][-4.2567344 -4.2511139 -4.2368741 -4.2092 -4.1809721 -4.1664338 -4.181138 -4.1905603 -4.2009997 -4.2213478 -4.2341089 -4.2391248 -4.2389464 -4.2217727 -4.1666975][-4.2474403 -4.231864 -4.2060823 -4.1687346 -4.1291642 -4.1071362 -4.1119828 -4.1119061 -4.1151853 -4.1456709 -4.1745634 -4.1914291 -4.1999784 -4.1922398 -4.1540093][-4.239542 -4.2164555 -4.1834011 -4.1412292 -4.1003556 -4.0766411 -4.0691509 -4.0512257 -4.0372071 -4.071125 -4.1214342 -4.1554871 -4.1781211 -4.18533 -4.1690588][-4.2306442 -4.2008519 -4.1605148 -4.1108932 -4.0689044 -4.0466442 -4.0257959 -3.9804063 -3.9370983 -3.9792242 -4.0650291 -4.1253705 -4.1712217 -4.1965418 -4.2009993][-4.2218809 -4.182158 -4.1242304 -4.0593896 -4.0107703 -3.9852264 -3.9458375 -3.8691616 -3.7971015 -3.8490362 -3.9802709 -4.07881 -4.1565313 -4.2051425 -4.229135][-4.2239728 -4.1856461 -4.1241641 -4.0505095 -3.9894743 -3.9478116 -3.8929157 -3.7976973 -3.7177968 -3.7803881 -3.9388769 -4.05943 -4.1504936 -4.2119679 -4.24517][-4.24093 -4.214304 -4.1697845 -4.1132603 -4.0555687 -3.9986987 -3.9346788 -3.8478007 -3.7901132 -3.8552382 -3.9922814 -4.0908828 -4.165875 -4.2214713 -4.2505317][-4.2630672 -4.2440081 -4.2175646 -4.1844044 -4.1443563 -4.0934896 -4.0386209 -3.9746721 -3.9423568 -3.9925666 -4.0850377 -4.14589 -4.1960111 -4.2380037 -4.2552228][-4.28262 -4.2662134 -4.2483363 -4.2306142 -4.2053938 -4.1682744 -4.1278019 -4.0864191 -4.0695643 -4.1011763 -4.15877 -4.1984358 -4.2388377 -4.2726521 -4.2807531][-4.2942991 -4.2831554 -4.2704797 -4.2598019 -4.247745 -4.225966 -4.2009764 -4.1719031 -4.159595 -4.1773911 -4.2121477 -4.2399006 -4.2754574 -4.3041873 -4.3079524][-4.3038726 -4.2964554 -4.2897468 -4.2830954 -4.2779012 -4.26793 -4.2544684 -4.2350984 -4.2240372 -4.2323861 -4.2506962 -4.2663093 -4.2923779 -4.3143578 -4.3159785][-4.3117661 -4.306067 -4.3035088 -4.3005018 -4.2982154 -4.2941771 -4.2879133 -4.2772722 -4.2702761 -4.2743435 -4.2836981 -4.290226 -4.3045645 -4.3179636 -4.3194828][-4.31565 -4.3101096 -4.3078828 -4.3069644 -4.3053422 -4.304141 -4.30363 -4.3007493 -4.2990336 -4.3038049 -4.3087735 -4.3111334 -4.3186345 -4.3261805 -4.3280663]]...]
INFO - root - 2017-12-06 01:41:36.102234: step 63210, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.847 sec/batch; 63h:23m:41s remains)
INFO - root - 2017-12-06 01:41:44.583549: step 63220, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.835 sec/batch; 62h:27m:17s remains)
INFO - root - 2017-12-06 01:41:53.163474: step 63230, loss = 2.06, batch loss = 2.00 (10.5 examples/sec; 0.758 sec/batch; 56h:43m:15s remains)
INFO - root - 2017-12-06 01:42:01.624142: step 63240, loss = 2.05, batch loss = 2.00 (9.1 examples/sec; 0.881 sec/batch; 65h:52m:32s remains)
INFO - root - 2017-12-06 01:42:10.231200: step 63250, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.857 sec/batch; 64h:04m:49s remains)
INFO - root - 2017-12-06 01:42:18.797984: step 63260, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.856 sec/batch; 64h:01m:45s remains)
INFO - root - 2017-12-06 01:42:27.343203: step 63270, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.851 sec/batch; 63h:37m:08s remains)
INFO - root - 2017-12-06 01:42:35.846924: step 63280, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.853 sec/batch; 63h:45m:49s remains)
INFO - root - 2017-12-06 01:42:44.430280: step 63290, loss = 2.05, batch loss = 1.99 (10.1 examples/sec; 0.794 sec/batch; 59h:24m:45s remains)
INFO - root - 2017-12-06 01:42:53.027856: step 63300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.864 sec/batch; 64h:34m:34s remains)
2017-12-06 01:42:53.828050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2444124 -4.2415004 -4.2345557 -4.2288585 -4.2225127 -4.2213292 -4.2335734 -4.2379847 -4.2275472 -4.2234826 -4.2355394 -4.2562938 -4.2732506 -4.2833629 -4.2956543][-4.2401247 -4.2455888 -4.2430429 -4.2359467 -4.2201509 -4.2092957 -4.2155533 -4.217783 -4.215827 -4.22124 -4.2376227 -4.2619781 -4.2788477 -4.2869649 -4.2984104][-4.2346735 -4.2448735 -4.2465248 -4.2363787 -4.2114878 -4.19077 -4.1861229 -4.1908894 -4.198164 -4.2105746 -4.2346039 -4.2635074 -4.2822881 -4.2897911 -4.3000617][-4.2333384 -4.2470136 -4.2497182 -4.2316656 -4.1955585 -4.163228 -4.1484122 -4.1567011 -4.172019 -4.1913085 -4.22714 -4.265039 -4.2877035 -4.2959423 -4.3045063][-4.2307415 -4.2428484 -4.245595 -4.2215142 -4.1725769 -4.1228456 -4.0947037 -4.0994453 -4.1200476 -4.1539879 -4.2077966 -4.2599158 -4.2898006 -4.3008389 -4.3089771][-4.2177348 -4.2306695 -4.2337704 -4.2044463 -4.1422434 -4.0716858 -4.0193172 -4.0025878 -4.0211148 -4.0757451 -4.1574559 -4.2318368 -4.2748985 -4.2936153 -4.3047967][-4.18389 -4.1990018 -4.2049375 -4.1766348 -4.1111832 -4.027184 -3.9488544 -3.8988366 -3.9053884 -3.9832294 -4.0952086 -4.1917882 -4.2490211 -4.2740941 -4.2903352][-4.1496687 -4.1792583 -4.2000742 -4.1833425 -4.1272907 -4.0468426 -3.9607596 -3.8955922 -3.8899651 -3.96641 -4.0778608 -4.1732254 -4.2307596 -4.2549086 -4.2729135][-4.1486359 -4.1912432 -4.2252307 -4.2227798 -4.1847191 -4.1211238 -4.0466523 -3.9875288 -3.976145 -4.0242605 -4.1056318 -4.1839962 -4.2340293 -4.2552443 -4.2705426][-4.1566849 -4.1967897 -4.2344456 -4.2447438 -4.2272482 -4.184588 -4.1279044 -4.0832849 -4.0674505 -4.0847907 -4.1355176 -4.1996918 -4.2434936 -4.2640862 -4.2768512][-4.1712804 -4.2046685 -4.2403531 -4.2543674 -4.2462192 -4.2176795 -4.1783395 -4.1488237 -4.1344361 -4.1381216 -4.1726003 -4.2230463 -4.2574849 -4.2732944 -4.2838464][-4.2070656 -4.2254076 -4.2499046 -4.2595582 -4.2542868 -4.2368331 -4.213273 -4.1963792 -4.1851964 -4.1827908 -4.2061682 -4.2440953 -4.2692733 -4.2816024 -4.291841][-4.2388425 -4.2460036 -4.2590871 -4.2649961 -4.2625809 -4.2546973 -4.2446356 -4.2374978 -4.2309804 -4.2267628 -4.2397194 -4.2634144 -4.280468 -4.2889671 -4.2977862][-4.2684474 -4.2675595 -4.2714081 -4.2738566 -4.2738791 -4.2720065 -4.2696209 -4.267365 -4.2641349 -4.2617731 -4.2686257 -4.283474 -4.2950339 -4.2981181 -4.3028541][-4.2865224 -4.28294 -4.2837205 -4.2846208 -4.2847886 -4.2839022 -4.2834744 -4.2831159 -4.2824135 -4.280735 -4.2848492 -4.2959027 -4.3052382 -4.3068414 -4.308382]]...]
INFO - root - 2017-12-06 01:43:02.176517: step 63310, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.846 sec/batch; 63h:13m:57s remains)
INFO - root - 2017-12-06 01:43:10.647389: step 63320, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 62h:48m:52s remains)
INFO - root - 2017-12-06 01:43:19.135457: step 63330, loss = 2.09, batch loss = 2.03 (10.8 examples/sec; 0.744 sec/batch; 55h:38m:05s remains)
INFO - root - 2017-12-06 01:43:27.709168: step 63340, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 65h:46m:05s remains)
INFO - root - 2017-12-06 01:43:36.184897: step 63350, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.832 sec/batch; 62h:10m:43s remains)
INFO - root - 2017-12-06 01:43:44.681716: step 63360, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.831 sec/batch; 62h:09m:25s remains)
INFO - root - 2017-12-06 01:43:53.210404: step 63370, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.857 sec/batch; 64h:03m:59s remains)
INFO - root - 2017-12-06 01:44:01.780148: step 63380, loss = 2.07, batch loss = 2.02 (9.4 examples/sec; 0.847 sec/batch; 63h:21m:08s remains)
INFO - root - 2017-12-06 01:44:10.313924: step 63390, loss = 2.07, batch loss = 2.01 (9.0 examples/sec; 0.885 sec/batch; 66h:10m:12s remains)
INFO - root - 2017-12-06 01:44:18.641274: step 63400, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.841 sec/batch; 62h:50m:16s remains)
2017-12-06 01:44:19.382995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1597939 -4.20135 -4.2250447 -4.2360678 -4.2350512 -4.2232895 -4.21507 -4.2246094 -4.2517128 -4.2743611 -4.2798018 -4.2729678 -4.2603631 -4.2465882 -4.2360616][-4.1612549 -4.2045417 -4.2264729 -4.2265573 -4.2095118 -4.181901 -4.162847 -4.1772819 -4.2113028 -4.2375932 -4.2426496 -4.2341342 -4.218586 -4.2088614 -4.2085543][-4.1321683 -4.1846738 -4.2137108 -4.2102804 -4.1815567 -4.1409092 -4.1138515 -4.12872 -4.1699815 -4.2030578 -4.2130713 -4.2069426 -4.1930752 -4.1902871 -4.1993928][-4.086309 -4.14918 -4.1904244 -4.1947269 -4.1635036 -4.1097512 -4.0633864 -4.0649753 -4.1183081 -4.1721525 -4.2008386 -4.2102132 -4.2090878 -4.2173409 -4.2306905][-4.0294914 -4.1066413 -4.1680112 -4.1920404 -4.1637082 -4.0959854 -4.0141416 -3.9799829 -4.0365987 -4.1202369 -4.1800323 -4.2128277 -4.2252507 -4.2454162 -4.2656946][-3.979202 -4.0707235 -4.1538854 -4.1964917 -4.1738749 -4.0875959 -3.9549499 -3.8600359 -3.9001904 -4.0206137 -4.1297259 -4.1950307 -4.22833 -4.2596455 -4.2863617][-3.9594147 -4.0498114 -4.1445036 -4.1999235 -4.1838446 -4.0884724 -3.9130292 -3.7487214 -3.7372415 -3.8780968 -4.0423012 -4.146379 -4.20779 -4.2559319 -4.2899909][-3.975318 -4.0450058 -4.1353445 -4.1942759 -4.1923084 -4.1120253 -3.9369173 -3.7425642 -3.6594441 -3.7698293 -3.9645298 -4.0980706 -4.1856441 -4.2478833 -4.2885132][-4.0324111 -4.0779405 -4.1478205 -4.2031274 -4.219439 -4.174346 -4.0396452 -3.8564873 -3.7358968 -3.7804818 -3.9492908 -4.0833883 -4.1830597 -4.2501469 -4.2902856][-4.098927 -4.1422377 -4.1940742 -4.2386394 -4.2576203 -4.2313604 -4.1363068 -3.9892108 -3.8732443 -3.8777463 -3.9954784 -4.1054316 -4.1992011 -4.2646275 -4.298131][-4.1776609 -4.2266307 -4.25979 -4.2814431 -4.2872877 -4.2661304 -4.2013111 -4.0931578 -4.0053854 -3.996902 -4.0687923 -4.1501656 -4.224493 -4.280304 -4.3048911][-4.2400193 -4.2840915 -4.3008289 -4.305038 -4.3009572 -4.2863345 -4.2455363 -4.1699853 -4.1058412 -4.09601 -4.1392136 -4.1977935 -4.2542539 -4.2946491 -4.3059421][-4.2643118 -4.2995243 -4.3076844 -4.3024035 -4.2957811 -4.2887406 -4.2705774 -4.2275877 -4.1888189 -4.1820779 -4.2051153 -4.24272 -4.2842288 -4.3102841 -4.3100662][-4.2787628 -4.3078818 -4.3123507 -4.30228 -4.2941728 -4.2904639 -4.2879481 -4.2722 -4.2531142 -4.2486038 -4.2571249 -4.2774916 -4.305728 -4.3235598 -4.3191609][-4.2928271 -4.3167896 -4.3178568 -4.3034883 -4.2936988 -4.2908249 -4.2920241 -4.2890706 -4.2834949 -4.2824526 -4.2861366 -4.2977724 -4.3167219 -4.3282533 -4.3245454]]...]
INFO - root - 2017-12-06 01:44:28.058209: step 63410, loss = 2.04, batch loss = 1.99 (9.4 examples/sec; 0.853 sec/batch; 63h:44m:19s remains)
INFO - root - 2017-12-06 01:44:36.529730: step 63420, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.861 sec/batch; 64h:20m:51s remains)
INFO - root - 2017-12-06 01:44:45.036736: step 63430, loss = 2.07, batch loss = 2.01 (10.2 examples/sec; 0.781 sec/batch; 58h:21m:00s remains)
INFO - root - 2017-12-06 01:44:53.603408: step 63440, loss = 2.07, batch loss = 2.02 (9.5 examples/sec; 0.846 sec/batch; 63h:15m:32s remains)
INFO - root - 2017-12-06 01:45:02.049273: step 63450, loss = 2.09, batch loss = 2.04 (9.5 examples/sec; 0.842 sec/batch; 62h:54m:33s remains)
INFO - root - 2017-12-06 01:45:10.656021: step 63460, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 63h:28m:27s remains)
INFO - root - 2017-12-06 01:45:19.042598: step 63470, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.825 sec/batch; 61h:38m:16s remains)
INFO - root - 2017-12-06 01:45:27.563163: step 63480, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.831 sec/batch; 62h:04m:11s remains)
INFO - root - 2017-12-06 01:45:36.070486: step 63490, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 65h:09m:00s remains)
INFO - root - 2017-12-06 01:45:44.563763: step 63500, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 63h:04m:12s remains)
2017-12-06 01:45:45.335574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.30553 -4.276052 -4.2396255 -4.2017956 -4.1718688 -4.15779 -4.1846867 -4.2070718 -4.2049055 -4.1863666 -4.1475992 -4.1087589 -4.0844779 -4.0794325 -4.0949006][-4.2999988 -4.2719355 -4.2365985 -4.197113 -4.168097 -4.1606579 -4.1960945 -4.221189 -4.2170606 -4.2011786 -4.1712403 -4.1403751 -4.119041 -4.1107659 -4.119163][-4.2926059 -4.26362 -4.2239842 -4.1745806 -4.13965 -4.1284075 -4.1647029 -4.1959591 -4.198823 -4.1941471 -4.1842117 -4.1681461 -4.1561346 -4.1535606 -4.1571436][-4.2879953 -4.2561021 -4.2098989 -4.1469464 -4.0994558 -4.0749125 -4.1024961 -4.1420336 -4.1599703 -4.1684461 -4.1792493 -4.1826453 -4.1843872 -4.1857543 -4.1789446][-4.2881927 -4.2550988 -4.2045569 -4.1347394 -4.0771642 -4.0393047 -4.05282 -4.0935588 -4.124382 -4.143609 -4.1704149 -4.1894274 -4.2019477 -4.2004142 -4.185256][-4.2933621 -4.2617335 -4.211936 -4.1452227 -4.0898294 -4.0463343 -4.0467 -4.0843649 -4.1196518 -4.1419258 -4.1708236 -4.1902757 -4.1974754 -4.1891627 -4.1655865][-4.2975049 -4.2697186 -4.2264671 -4.1682429 -4.117136 -4.0797257 -4.0753036 -4.1055851 -4.131454 -4.1475921 -4.1710458 -4.1826115 -4.174716 -4.15716 -4.1324291][-4.2920575 -4.2649469 -4.2258282 -4.1738524 -4.128047 -4.0985541 -4.0950346 -4.1149759 -4.1268253 -4.1361027 -4.1526585 -4.1542687 -4.1329575 -4.1103044 -4.0973949][-4.2776055 -4.2468863 -4.208878 -4.1638741 -4.129405 -4.1104436 -4.1117287 -4.123785 -4.1223764 -4.1220684 -4.1309962 -4.1201415 -4.082531 -4.0561485 -4.0572324][-4.2648745 -4.2305069 -4.1912575 -4.1498041 -4.1283154 -4.119904 -4.1269031 -4.140872 -4.1303391 -4.1188087 -4.1173987 -4.0938377 -4.0429134 -4.0114241 -4.0201716][-4.2639017 -4.2311792 -4.1931062 -4.1550455 -4.139791 -4.1339517 -4.1422992 -4.1605139 -4.1512971 -4.1325364 -4.1202612 -4.0840931 -4.0227833 -3.9831882 -3.9964566][-4.2678676 -4.2372265 -4.202497 -4.1700177 -4.1596975 -4.1559105 -4.1655989 -4.1822882 -4.1770282 -4.1556578 -4.1372643 -4.0964346 -4.0364923 -3.9965549 -4.00889][-4.2689381 -4.2389073 -4.2077479 -4.1794872 -4.1762443 -4.1755962 -4.1833954 -4.1957617 -4.19362 -4.1752172 -4.1563993 -4.1209631 -4.0686369 -4.0351171 -4.0444274][-4.272994 -4.2433496 -4.2110124 -4.18226 -4.1804743 -4.1797624 -4.1872625 -4.197135 -4.1959858 -4.1834879 -4.1712584 -4.14963 -4.1136146 -4.0870647 -4.0916471][-4.2793417 -4.2515931 -4.2189546 -4.1846366 -4.1734166 -4.1651735 -4.171052 -4.1841373 -4.1837559 -4.1735706 -4.1682234 -4.1614141 -4.137917 -4.11729 -4.1166425]]...]
INFO - root - 2017-12-06 01:45:53.808168: step 63510, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 63h:48m:02s remains)
INFO - root - 2017-12-06 01:46:02.229005: step 63520, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.837 sec/batch; 62h:34m:20s remains)
INFO - root - 2017-12-06 01:46:10.566418: step 63530, loss = 2.08, batch loss = 2.02 (10.4 examples/sec; 0.766 sec/batch; 57h:15m:36s remains)
INFO - root - 2017-12-06 01:46:19.144358: step 63540, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 65h:46m:49s remains)
INFO - root - 2017-12-06 01:46:27.670138: step 63550, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 64h:48m:43s remains)
INFO - root - 2017-12-06 01:46:36.198122: step 63560, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.850 sec/batch; 63h:31m:26s remains)
INFO - root - 2017-12-06 01:46:44.751859: step 63570, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.849 sec/batch; 63h:24m:30s remains)
INFO - root - 2017-12-06 01:46:53.308693: step 63580, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 63h:13m:10s remains)
INFO - root - 2017-12-06 01:47:01.763962: step 63590, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.861 sec/batch; 64h:16m:47s remains)
INFO - root - 2017-12-06 01:47:10.306232: step 63600, loss = 2.03, batch loss = 1.97 (9.4 examples/sec; 0.849 sec/batch; 63h:24m:58s remains)
2017-12-06 01:47:11.061633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1493163 -4.1117511 -4.0783072 -4.0501604 -4.0479569 -4.0466433 -4.025105 -4.010674 -4.0261383 -4.0581427 -4.0882764 -4.0987468 -4.0845456 -4.0607634 -4.0538063][-4.1827993 -4.1618838 -4.1465464 -4.1435452 -4.1536245 -4.150701 -4.1235971 -4.1015062 -4.1078334 -4.12802 -4.1462941 -4.14981 -4.1332173 -4.1149926 -4.1148887][-4.2010684 -4.1919575 -4.1933937 -4.2086811 -4.2234721 -4.221314 -4.1969581 -4.1762605 -4.1735654 -4.1809707 -4.1927557 -4.1991463 -4.1889386 -4.1775603 -4.1821747][-4.2212763 -4.2228537 -4.2318325 -4.2506108 -4.2592263 -4.2565904 -4.2354198 -4.2151766 -4.20564 -4.2050033 -4.2151217 -4.2266879 -4.22381 -4.2207637 -4.2309079][-4.2227697 -4.2340226 -4.2435622 -4.251967 -4.2468081 -4.23282 -4.2083349 -4.1910958 -4.1892772 -4.1995869 -4.2170215 -4.2352405 -4.2430944 -4.2504282 -4.26583][-4.2100763 -4.2280526 -4.229413 -4.2138505 -4.1805239 -4.1460028 -4.1268158 -4.1313806 -4.15195 -4.1831932 -4.2153683 -4.243722 -4.2608895 -4.275444 -4.2905607][-4.1935644 -4.2169447 -4.2085962 -4.1703882 -4.1158924 -4.0736775 -4.0676866 -4.0940251 -4.1344352 -4.1768904 -4.2147679 -4.2471933 -4.2702703 -4.2855582 -4.2936273][-4.1861439 -4.2130008 -4.2027669 -4.1602941 -4.1040435 -4.0603461 -4.0569229 -4.0870104 -4.1310711 -4.1734347 -4.2086716 -4.2377472 -4.261167 -4.2769661 -4.2786417][-4.1606703 -4.1909728 -4.194447 -4.1671872 -4.1185346 -4.0688605 -4.0544758 -4.0748982 -4.113534 -4.1502151 -4.182425 -4.2058496 -4.2276378 -4.2438941 -4.2404656][-4.1130271 -4.1423721 -4.1580787 -4.148006 -4.1065793 -4.054462 -4.0345325 -4.0499163 -4.083571 -4.1195574 -4.1553597 -4.1791024 -4.1976457 -4.2107372 -4.2050014][-4.0826106 -4.1003857 -4.1091995 -4.1045871 -4.0736709 -4.0295734 -4.0192142 -4.03904 -4.070374 -4.1091352 -4.1520238 -4.1791382 -4.1934667 -4.2023897 -4.1960378][-4.1038146 -4.1084919 -4.1063137 -4.1050286 -4.0887432 -4.059288 -4.0601292 -4.0851479 -4.1128793 -4.1445112 -4.18161 -4.2067623 -4.2196746 -4.2285771 -4.2270656][-4.171073 -4.1658473 -4.1601653 -4.1593242 -4.1515636 -4.1383619 -4.1468349 -4.1706762 -4.1924472 -4.2135782 -4.2390542 -4.2575569 -4.2665415 -4.273736 -4.2758722][-4.2453289 -4.2359872 -4.2283964 -4.2277184 -4.2239661 -4.2210379 -4.2304826 -4.2477932 -4.262454 -4.2738442 -4.2879419 -4.2973905 -4.3000331 -4.3033404 -4.3043265][-4.3037419 -4.2938132 -4.2863 -4.2836447 -4.2813659 -4.2799053 -4.2853975 -4.2977781 -4.3089962 -4.3174882 -4.3250976 -4.32694 -4.3253407 -4.3249774 -4.3238921]]...]
INFO - root - 2017-12-06 01:47:19.448407: step 63610, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:39m:37s remains)
INFO - root - 2017-12-06 01:47:28.073630: step 63620, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.865 sec/batch; 64h:34m:20s remains)
INFO - root - 2017-12-06 01:47:36.530656: step 63630, loss = 2.08, batch loss = 2.02 (10.5 examples/sec; 0.762 sec/batch; 56h:56m:22s remains)
INFO - root - 2017-12-06 01:47:45.039163: step 63640, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.842 sec/batch; 62h:51m:07s remains)
INFO - root - 2017-12-06 01:47:53.432722: step 63650, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 63h:54m:13s remains)
INFO - root - 2017-12-06 01:48:02.026261: step 63660, loss = 2.04, batch loss = 1.98 (9.2 examples/sec; 0.872 sec/batch; 65h:09m:22s remains)
INFO - root - 2017-12-06 01:48:10.557320: step 63670, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.848 sec/batch; 63h:21m:08s remains)
INFO - root - 2017-12-06 01:48:19.088497: step 63680, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 64h:49m:58s remains)
INFO - root - 2017-12-06 01:48:27.579249: step 63690, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.858 sec/batch; 64h:05m:22s remains)
INFO - root - 2017-12-06 01:48:36.054918: step 63700, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 62h:20m:16s remains)
2017-12-06 01:48:36.885989: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3274412 -4.3218708 -4.3225474 -4.3242788 -4.3211813 -4.318634 -4.3229566 -4.3289008 -4.3323355 -4.3374619 -4.3440928 -4.3471475 -4.3466215 -4.3444448 -4.3408875][-4.3056259 -4.2968764 -4.2966685 -4.2964587 -4.2901607 -4.28348 -4.288619 -4.299067 -4.3067589 -4.3184953 -4.3322282 -4.3412247 -4.3434086 -4.3419652 -4.3377252][-4.2780457 -4.2691851 -4.2677383 -4.2634406 -4.256247 -4.2487874 -4.2547503 -4.2683625 -4.2804008 -4.2984643 -4.3184781 -4.3315744 -4.3365822 -4.3374224 -4.3325148][-4.2490954 -4.2407651 -4.2345438 -4.2234249 -4.2156076 -4.2114854 -4.2189116 -4.235158 -4.2503562 -4.2751875 -4.3029952 -4.3189716 -4.3252091 -4.3258133 -4.3219547][-4.2203436 -4.2100229 -4.1945429 -4.171762 -4.1593051 -4.1575904 -4.164423 -4.1815071 -4.1999121 -4.2367492 -4.2782807 -4.29923 -4.3077621 -4.3081684 -4.3050022][-4.200232 -4.1866488 -4.1599469 -4.1220326 -4.0981269 -4.0929332 -4.0964828 -4.1113777 -4.1346297 -4.1851282 -4.2423596 -4.271666 -4.2851191 -4.2902255 -4.2900138][-4.1959376 -4.1842489 -4.1542988 -4.1080461 -4.0706525 -4.0527363 -4.043643 -4.0483642 -4.0724373 -4.128727 -4.1957107 -4.2359209 -4.2583165 -4.2727637 -4.2793126][-4.2098565 -4.2046113 -4.1856346 -4.152575 -4.1175709 -4.0877285 -4.0595927 -4.0448618 -4.0560889 -4.0987778 -4.1538529 -4.1975055 -4.2300134 -4.2528434 -4.2630873][-4.2176952 -4.2192774 -4.209374 -4.1918249 -4.1666451 -4.1320968 -4.092103 -4.0632925 -4.0605831 -4.0881166 -4.1278348 -4.16737 -4.202064 -4.2254133 -4.2341065][-4.2126031 -4.221036 -4.2185268 -4.208818 -4.1911674 -4.1575851 -4.1129956 -4.0795288 -4.0780272 -4.1032796 -4.1364913 -4.1674261 -4.1930289 -4.2078691 -4.208436][-4.199935 -4.2135673 -4.2159815 -4.2109575 -4.2004514 -4.1726718 -4.1322269 -4.1043878 -4.1086149 -4.1355219 -4.1649995 -4.1875491 -4.2030334 -4.204607 -4.1963682][-4.1933 -4.2099047 -4.2180343 -4.2184587 -4.2176204 -4.2050772 -4.1779814 -4.1589193 -4.1627812 -4.1815143 -4.2003341 -4.2113347 -4.21373 -4.2035971 -4.1896768][-4.1980419 -4.2137566 -4.2249956 -4.2312841 -4.2389793 -4.2428989 -4.2325459 -4.2233806 -4.2255387 -4.2356672 -4.2424612 -4.2397532 -4.2281466 -4.2071724 -4.1905584][-4.2051816 -4.2162743 -4.2310419 -4.2448459 -4.2586918 -4.2665238 -4.2620573 -4.25799 -4.2591429 -4.2653608 -4.2626772 -4.2502518 -4.2338285 -4.213984 -4.2051563][-4.204092 -4.2090173 -4.224009 -4.2429533 -4.2616949 -4.268775 -4.2609973 -4.2537909 -4.2518306 -4.2552423 -4.248055 -4.2389216 -4.2342424 -4.230279 -4.2363467]]...]
INFO - root - 2017-12-06 01:48:45.470303: step 63710, loss = 2.05, batch loss = 1.99 (9.8 examples/sec; 0.814 sec/batch; 60h:46m:17s remains)
INFO - root - 2017-12-06 01:48:53.976117: step 63720, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 64h:02m:55s remains)
INFO - root - 2017-12-06 01:49:02.409717: step 63730, loss = 2.07, batch loss = 2.01 (10.7 examples/sec; 0.751 sec/batch; 56h:02m:29s remains)
INFO - root - 2017-12-06 01:49:10.890202: step 63740, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.861 sec/batch; 64h:16m:10s remains)
INFO - root - 2017-12-06 01:49:19.349051: step 63750, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 62h:21m:16s remains)
INFO - root - 2017-12-06 01:49:27.860089: step 63760, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 63h:48m:43s remains)
INFO - root - 2017-12-06 01:49:36.424706: step 63770, loss = 2.08, batch loss = 2.02 (9.5 examples/sec; 0.844 sec/batch; 63h:00m:26s remains)
INFO - root - 2017-12-06 01:49:44.862688: step 63780, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.843 sec/batch; 62h:56m:51s remains)
INFO - root - 2017-12-06 01:49:53.376300: step 63790, loss = 2.09, batch loss = 2.03 (9.5 examples/sec; 0.844 sec/batch; 63h:00m:10s remains)
INFO - root - 2017-12-06 01:50:02.010797: step 63800, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.855 sec/batch; 63h:48m:24s remains)
2017-12-06 01:50:02.802611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0979109 -4.1089835 -4.1133595 -4.1194305 -4.1383 -4.1680946 -4.1833396 -4.186152 -4.1919603 -4.1959195 -4.1949587 -4.193038 -4.1910243 -4.1884561 -4.187067][-4.0945892 -4.09949 -4.0964832 -4.0985003 -4.12089 -4.156517 -4.1765156 -4.1861567 -4.199564 -4.2092495 -4.209116 -4.2068586 -4.2031074 -4.1959386 -4.1883712][-4.1311321 -4.1259389 -4.1116347 -4.1088781 -4.1345849 -4.1679592 -4.188416 -4.2067327 -4.2310262 -4.2490363 -4.2511277 -4.2493014 -4.2448845 -4.2322369 -4.2145567][-4.1605229 -4.1444936 -4.11837 -4.1080227 -4.1310596 -4.1566319 -4.1705174 -4.1922774 -4.2309241 -4.2611842 -4.2668943 -4.268858 -4.2656374 -4.2496438 -4.2225714][-4.1645479 -4.1373024 -4.0997806 -4.0778522 -4.0943561 -4.1105533 -4.1066279 -4.1141658 -4.166954 -4.2143903 -4.2309232 -4.246305 -4.2525826 -4.2417645 -4.2086396][-4.1645713 -4.1256 -4.0737228 -4.0317221 -4.0308166 -4.031971 -4.0015121 -3.9821265 -4.0577912 -4.1309628 -4.1659665 -4.2037892 -4.2229013 -4.2171445 -4.1810231][-4.1732116 -4.1323795 -4.0749989 -4.0161462 -3.9892509 -3.9600325 -3.8874419 -3.821219 -3.9193363 -4.0289969 -4.084342 -4.1431632 -4.1798162 -4.1837468 -4.15531][-4.1923528 -4.162497 -4.119071 -4.0675807 -4.0340719 -3.9924769 -3.8981483 -3.7937918 -3.8721545 -3.98492 -4.03948 -4.0923848 -4.1410031 -4.1630406 -4.1574717][-4.2158489 -4.1976805 -4.1724162 -4.1415653 -4.1194773 -4.093142 -4.0283728 -3.9464564 -3.9792788 -4.0485387 -4.07948 -4.0987563 -4.1373577 -4.1659627 -4.1697521][-4.2474093 -4.2364984 -4.2219167 -4.2033749 -4.1912475 -4.1758542 -4.1394391 -4.093327 -4.1006274 -4.137424 -4.1536517 -4.1538653 -4.1778607 -4.2027464 -4.2067094][-4.2711625 -4.2668362 -4.2609515 -4.2497792 -4.2402687 -4.231514 -4.2140384 -4.1905575 -4.1904616 -4.2115321 -4.2242746 -4.2232146 -4.2379355 -4.2560992 -4.2563205][-4.2744679 -4.2728596 -4.2724686 -4.2683625 -4.2612004 -4.25602 -4.2494893 -4.2392159 -4.2420435 -4.26212 -4.2784519 -4.2811556 -4.2872252 -4.2949748 -4.2907939][-4.2656612 -4.2624326 -4.2641516 -4.2655716 -4.2634535 -4.2626162 -4.2622571 -4.2599597 -4.2657604 -4.2829618 -4.2998753 -4.3036771 -4.3020158 -4.30207 -4.2975349][-4.2615824 -4.2562809 -4.256484 -4.2592006 -4.2610164 -4.2643919 -4.2678533 -4.2712722 -4.2771831 -4.2872286 -4.2984176 -4.2996721 -4.2951212 -4.2935219 -4.2926054][-4.2723 -4.2664928 -4.2629027 -4.2630558 -4.2659478 -4.2699995 -4.2722883 -4.2745156 -4.2773452 -4.2787242 -4.2810125 -4.279346 -4.2760782 -4.2772903 -4.2793088]]...]
INFO - root - 2017-12-06 01:50:11.242292: step 63810, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.850 sec/batch; 63h:24m:55s remains)
INFO - root - 2017-12-06 01:50:19.782170: step 63820, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.849 sec/batch; 63h:20m:02s remains)
INFO - root - 2017-12-06 01:50:28.104678: step 63830, loss = 2.05, batch loss = 1.99 (10.5 examples/sec; 0.763 sec/batch; 56h:55m:50s remains)
INFO - root - 2017-12-06 01:50:36.608414: step 63840, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 64h:48m:56s remains)
INFO - root - 2017-12-06 01:50:45.126891: step 63850, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 63h:48m:31s remains)
INFO - root - 2017-12-06 01:50:53.579876: step 63860, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 64h:51m:49s remains)
INFO - root - 2017-12-06 01:51:02.101000: step 63870, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 63h:47m:47s remains)
INFO - root - 2017-12-06 01:51:10.599581: step 63880, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.874 sec/batch; 65h:12m:15s remains)
INFO - root - 2017-12-06 01:51:19.129188: step 63890, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.853 sec/batch; 63h:38m:37s remains)
INFO - root - 2017-12-06 01:51:27.658015: step 63900, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 63h:26m:13s remains)
2017-12-06 01:51:28.462601: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2995868 -4.2954369 -4.2800226 -4.2509027 -4.2252021 -4.1948266 -4.1609855 -4.1558352 -4.184196 -4.1918564 -4.194694 -4.2068911 -4.2015991 -4.1895614 -4.1850233][-4.2985716 -4.2940316 -4.2770658 -4.24475 -4.2157073 -4.1823907 -4.1485434 -4.1471634 -4.1778021 -4.1845655 -4.1885548 -4.2009425 -4.194478 -4.1837239 -4.1828842][-4.2980967 -4.2930832 -4.2748151 -4.2395234 -4.2073841 -4.1713357 -4.1372766 -4.139565 -4.173872 -4.1815019 -4.18261 -4.1921544 -4.1863832 -4.1806107 -4.18314][-4.2978559 -4.2932558 -4.2751546 -4.238976 -4.205719 -4.1669946 -4.1279521 -4.1265073 -4.1623373 -4.1741943 -4.1724048 -4.176868 -4.1742806 -4.175395 -4.182817][-4.2984934 -4.2948356 -4.2777524 -4.2421494 -4.2088118 -4.1657257 -4.11383 -4.0997443 -4.136157 -4.1568837 -4.1556773 -4.1568365 -4.1582909 -4.16771 -4.182384][-4.2991023 -4.2963119 -4.2792735 -4.2428174 -4.2058482 -4.1490507 -4.07062 -4.03754 -4.0799007 -4.1196923 -4.13056 -4.1336842 -4.1422162 -4.1613708 -4.1844316][-4.2991762 -4.2958488 -4.2761345 -4.234024 -4.1859818 -4.1060629 -3.9904823 -3.9348283 -3.989996 -4.0605636 -4.0986185 -4.1186161 -4.1403065 -4.1681004 -4.1952877][-4.2980571 -4.2916036 -4.26742 -4.2215562 -4.1675239 -4.0778255 -3.949214 -3.889447 -3.9540954 -4.044208 -4.1014347 -4.1349292 -4.1633697 -4.1901283 -4.2106919][-4.2962322 -4.2870688 -4.2626638 -4.2217255 -4.1768594 -4.1026564 -3.9949284 -3.9528444 -4.0123563 -4.0886092 -4.13782 -4.1683254 -4.1917934 -4.2108979 -4.2214308][-4.2938094 -4.284255 -4.2625222 -4.2287316 -4.1947622 -4.1396308 -4.0571208 -4.0334835 -4.082448 -4.1349869 -4.1688795 -4.1971035 -4.2167048 -4.2293973 -4.2324848][-4.290204 -4.2811069 -4.2610765 -4.230998 -4.201395 -4.154932 -4.0919523 -4.0829782 -4.1263938 -4.1682196 -4.1972737 -4.2227278 -4.239893 -4.2492542 -4.247582][-4.286314 -4.27857 -4.26084 -4.232933 -4.205307 -4.16518 -4.1208076 -4.1228962 -4.1613231 -4.19607 -4.2238259 -4.2468238 -4.2607617 -4.265543 -4.2579112][-4.2807794 -4.2718406 -4.2575836 -4.2340803 -4.2086539 -4.1784148 -4.1535044 -4.162394 -4.1906672 -4.2162614 -4.2399344 -4.2577357 -4.2656822 -4.2659926 -4.2544293][-4.274087 -4.2594504 -4.2438283 -4.2230296 -4.2018871 -4.183291 -4.1742253 -4.1873436 -4.2059903 -4.2231011 -4.2396564 -4.2499428 -4.2527118 -4.2511015 -4.2399387][-4.2690239 -4.2458296 -4.2240448 -4.2035623 -4.1871905 -4.179059 -4.1826696 -4.1991792 -4.2123318 -4.223197 -4.2339754 -4.2383914 -4.2370505 -4.2345557 -4.2234492]]...]
INFO - root - 2017-12-06 01:51:36.993403: step 63910, loss = 2.05, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 64h:49m:05s remains)
INFO - root - 2017-12-06 01:51:45.484436: step 63920, loss = 2.05, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 61h:38m:07s remains)
INFO - root - 2017-12-06 01:51:53.933403: step 63930, loss = 2.06, batch loss = 2.00 (10.2 examples/sec; 0.786 sec/batch; 58h:37m:56s remains)
INFO - root - 2017-12-06 01:52:02.403536: step 63940, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.825 sec/batch; 61h:32m:29s remains)
INFO - root - 2017-12-06 01:52:10.864578: step 63950, loss = 2.03, batch loss = 1.97 (9.6 examples/sec; 0.837 sec/batch; 62h:24m:21s remains)
INFO - root - 2017-12-06 01:52:19.479340: step 63960, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.890 sec/batch; 66h:25m:12s remains)
INFO - root - 2017-12-06 01:52:27.871515: step 63970, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.879 sec/batch; 65h:35m:29s remains)
INFO - root - 2017-12-06 01:52:36.451154: step 63980, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 65h:02m:00s remains)
INFO - root - 2017-12-06 01:52:45.001459: step 63990, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.868 sec/batch; 64h:42m:22s remains)
INFO - root - 2017-12-06 01:52:53.486328: step 64000, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.875 sec/batch; 65h:14m:34s remains)
2017-12-06 01:52:54.346939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3150721 -4.3140593 -4.3135605 -4.3095083 -4.30535 -4.30826 -4.3165097 -4.3278956 -4.33518 -4.3374138 -4.3351126 -4.3323646 -4.3288784 -4.3260255 -4.3238115][-4.33653 -4.3335452 -4.3300781 -4.322803 -4.3161154 -4.3152542 -4.3196187 -4.329174 -4.3367996 -4.340981 -4.3433781 -4.3472204 -4.3492227 -4.3487659 -4.3467193][-4.3464384 -4.3430772 -4.3384666 -4.3304524 -4.3229971 -4.3182683 -4.316968 -4.3195562 -4.3220887 -4.32483 -4.3319178 -4.3448834 -4.3554082 -4.3594403 -4.3578992][-4.3473277 -4.3435054 -4.3391929 -4.331717 -4.3230476 -4.3137703 -4.3041792 -4.2951932 -4.2894268 -4.291954 -4.3073387 -4.3319316 -4.3525376 -4.362174 -4.362042][-4.345777 -4.3401957 -4.3332596 -4.3225622 -4.3106313 -4.2956104 -4.2763667 -4.2523389 -4.2354755 -4.2379975 -4.2619586 -4.2991438 -4.33175 -4.3504295 -4.3570218][-4.3417587 -4.3336658 -4.3218722 -4.3047624 -4.2840147 -4.2564569 -4.220499 -4.1743722 -4.1421952 -4.1474371 -4.1876879 -4.2404008 -4.2890215 -4.3216329 -4.3395209][-4.3361592 -4.3254147 -4.3081479 -4.2809348 -4.2432356 -4.1933813 -4.12912 -4.0562463 -4.0120206 -4.03274 -4.1019564 -4.1753678 -4.2411752 -4.2855911 -4.3102775][-4.3323369 -4.3210483 -4.2997994 -4.2619166 -4.2041049 -4.1256342 -4.028801 -3.9317281 -3.8826303 -3.9275217 -4.0309391 -4.1273708 -4.20507 -4.2482257 -4.2672105][-4.3320494 -4.3208709 -4.2967463 -4.2521062 -4.1833043 -4.0873165 -3.971137 -3.8636253 -3.8184204 -3.8776743 -3.9962938 -4.1054735 -4.1831083 -4.2142382 -4.2169485][-4.3340721 -4.3236308 -4.2999787 -4.2565546 -4.1897926 -4.0940371 -3.9808369 -3.8870759 -3.854917 -3.9114971 -4.0159373 -4.1129379 -4.1740203 -4.1844406 -4.1622019][-4.3360109 -4.3273611 -4.3071694 -4.2696552 -4.2124491 -4.1296024 -4.0369053 -3.9723575 -3.9595194 -4.0068707 -4.0823579 -4.1510434 -4.186254 -4.1723723 -4.1240859][-4.3375316 -4.3308744 -4.3160191 -4.2875166 -4.2463913 -4.1874604 -4.1256495 -4.0892777 -4.0892453 -4.1237431 -4.170486 -4.209465 -4.2208967 -4.1919518 -4.1333985][-4.3384452 -4.3337774 -4.3257976 -4.3095784 -4.2863164 -4.2539148 -4.2213612 -4.2045016 -4.2090154 -4.2301993 -4.2558346 -4.2728086 -4.2662244 -4.2320681 -4.1795297][-4.3383713 -4.3354845 -4.3331733 -4.327177 -4.3175454 -4.3048077 -4.2933812 -4.2893648 -4.2961035 -4.3094053 -4.3211446 -4.3239284 -4.3090615 -4.2782578 -4.2377186][-4.3381681 -4.3368883 -4.3379364 -4.3381281 -4.3360949 -4.3332219 -4.3318744 -4.3332543 -4.3388629 -4.3459368 -4.3499532 -4.3479095 -4.3357744 -4.3161359 -4.2931447]]...]
INFO - root - 2017-12-06 01:53:02.869746: step 64010, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 64h:20m:18s remains)
INFO - root - 2017-12-06 01:53:11.433260: step 64020, loss = 2.06, batch loss = 2.01 (9.5 examples/sec; 0.846 sec/batch; 63h:05m:22s remains)
INFO - root - 2017-12-06 01:53:19.973712: step 64030, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 64h:36m:20s remains)
INFO - root - 2017-12-06 01:53:28.479944: step 64040, loss = 2.06, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 63h:14m:39s remains)
INFO - root - 2017-12-06 01:53:37.000110: step 64050, loss = 2.07, batch loss = 2.02 (9.2 examples/sec; 0.870 sec/batch; 64h:53m:45s remains)
INFO - root - 2017-12-06 01:53:45.551679: step 64060, loss = 2.05, batch loss = 1.99 (9.7 examples/sec; 0.823 sec/batch; 61h:22m:56s remains)
INFO - root - 2017-12-06 01:53:54.158456: step 64070, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.844 sec/batch; 62h:55m:04s remains)
INFO - root - 2017-12-06 01:54:02.568342: step 64080, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 62h:38m:52s remains)
INFO - root - 2017-12-06 01:54:11.144923: step 64090, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.869 sec/batch; 64h:48m:20s remains)
INFO - root - 2017-12-06 01:54:19.614212: step 64100, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.859 sec/batch; 64h:04m:31s remains)
2017-12-06 01:54:20.411282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2418351 -4.2433281 -4.2384629 -4.2417154 -4.2549872 -4.2670555 -4.2633657 -4.2512131 -4.2444305 -4.2407804 -4.2375407 -4.2437525 -4.2640882 -4.2771955 -4.2808652][-4.2370734 -4.241828 -4.2357965 -4.2376866 -4.2484841 -4.2575555 -4.2493191 -4.2319827 -4.218277 -4.2091427 -4.2041368 -4.2149115 -4.2426233 -4.2625427 -4.2710409][-4.2269731 -4.2359328 -4.2313776 -4.2342396 -4.2423911 -4.2467327 -4.2333941 -4.2067723 -4.1794133 -4.1650767 -4.1627793 -4.182765 -4.2203283 -4.2501731 -4.263164][-4.212142 -4.2233658 -4.2226505 -4.2286682 -4.2318726 -4.2282882 -4.2096248 -4.1723566 -4.1342216 -4.1187453 -4.1295514 -4.16855 -4.2165937 -4.251255 -4.2646751][-4.2102327 -4.224369 -4.2250395 -4.2269998 -4.2169809 -4.2011032 -4.1709943 -4.1178446 -4.0745935 -4.0706296 -4.1066723 -4.1679082 -4.2251463 -4.2598391 -4.2716618][-4.2185755 -4.2369509 -4.238 -4.2329106 -4.2080483 -4.174509 -4.12292 -4.0457578 -4.0032983 -4.0246868 -4.0940146 -4.1727338 -4.2339282 -4.2683926 -4.2780433][-4.2256503 -4.2441235 -4.2429094 -4.2296987 -4.1932192 -4.1412678 -4.0657587 -3.970598 -3.9400907 -4.0011473 -4.1039305 -4.1917081 -4.2501545 -4.2800722 -4.2842326][-4.2213783 -4.2361875 -4.2313113 -4.2130165 -4.1735358 -4.1145053 -4.0288634 -3.9339893 -3.9277763 -4.0212607 -4.1347623 -4.2155595 -4.2622528 -4.28441 -4.2834935][-4.2114816 -4.2218361 -4.21383 -4.1917062 -4.1539273 -4.0940127 -4.0153537 -3.9429829 -3.9626286 -4.0644917 -4.1658382 -4.2302527 -4.26282 -4.2790523 -4.2742472][-4.2132134 -4.2178845 -4.2020845 -4.172513 -4.1339674 -4.075511 -4.0136395 -3.9786496 -4.0238194 -4.1178675 -4.196341 -4.2361417 -4.2553144 -4.2665429 -4.25935][-4.2274132 -4.2264953 -4.2010365 -4.1647725 -4.1202602 -4.0615258 -4.0175643 -4.0192137 -4.0789032 -4.1630936 -4.2210078 -4.2426057 -4.2490411 -4.2499442 -4.2367272][-4.2393351 -4.2365756 -4.2058563 -4.1666136 -4.121366 -4.0692563 -4.0430074 -4.0631542 -4.1240478 -4.1947365 -4.236125 -4.2466173 -4.2382054 -4.2252417 -4.2055111][-4.253983 -4.2533884 -4.2234874 -4.1839347 -4.1450386 -4.10632 -4.0932455 -4.1185045 -4.1697211 -4.22226 -4.2474885 -4.2459555 -4.2258477 -4.2044086 -4.1799994][-4.2684331 -4.2692714 -4.2416382 -4.2018018 -4.1654954 -4.13948 -4.1404858 -4.1701374 -4.2097387 -4.2429008 -4.2519221 -4.2380042 -4.2107487 -4.18504 -4.1561155][-4.2728052 -4.2747583 -4.25046 -4.2126803 -4.1749153 -4.1567817 -4.1724496 -4.2073522 -4.2376151 -4.253159 -4.2475557 -4.2249408 -4.1947074 -4.1664076 -4.1412206]]...]
INFO - root - 2017-12-06 01:54:28.907223: step 64110, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.863 sec/batch; 64h:19m:45s remains)
INFO - root - 2017-12-06 01:54:37.315728: step 64120, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.873 sec/batch; 65h:06m:27s remains)
INFO - root - 2017-12-06 01:54:45.700835: step 64130, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:18m:03s remains)
INFO - root - 2017-12-06 01:54:54.331717: step 64140, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.865 sec/batch; 64h:30m:32s remains)
INFO - root - 2017-12-06 01:55:02.658521: step 64150, loss = 2.07, batch loss = 2.01 (9.7 examples/sec; 0.826 sec/batch; 61h:32m:19s remains)
INFO - root - 2017-12-06 01:55:11.114887: step 64160, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 64h:11m:06s remains)
INFO - root - 2017-12-06 01:55:19.609305: step 64170, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.852 sec/batch; 63h:31m:59s remains)
INFO - root - 2017-12-06 01:55:28.203403: step 64180, loss = 2.07, batch loss = 2.02 (9.6 examples/sec; 0.836 sec/batch; 62h:19m:17s remains)
INFO - root - 2017-12-06 01:55:36.550291: step 64190, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.826 sec/batch; 61h:35m:00s remains)
INFO - root - 2017-12-06 01:55:45.090961: step 64200, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:19m:50s remains)
2017-12-06 01:55:45.855728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2668519 -4.2386436 -4.1969194 -4.1449418 -4.0962172 -4.0656476 -4.0596442 -4.0942507 -4.1567369 -4.2093277 -4.2303262 -4.223906 -4.2056637 -4.1873565 -4.17277][-4.2608862 -4.2265205 -4.1775017 -4.1204605 -4.0683413 -4.0337009 -4.02663 -4.0655642 -4.1297812 -4.1821632 -4.2002931 -4.1886468 -4.1682644 -4.1521273 -4.1345711][-4.2635536 -4.2298193 -4.1844025 -4.134007 -4.0840468 -4.0478005 -4.0405774 -4.0789108 -4.1350746 -4.17796 -4.1855874 -4.1673188 -4.1440892 -4.1264043 -4.097836][-4.2697096 -4.2398739 -4.2051535 -4.1676631 -4.1239123 -4.0863314 -4.0724077 -4.0927234 -4.12304 -4.149826 -4.1538053 -4.1361976 -4.112349 -4.0971437 -4.0714569][-4.2749448 -4.245327 -4.2148557 -4.1824908 -4.1415644 -4.0953989 -4.0628209 -4.0502439 -4.0536952 -4.0750856 -4.0895605 -4.0906787 -4.081562 -4.0832424 -4.0719624][-4.2701807 -4.2355552 -4.1986146 -4.1613488 -4.1138258 -4.0547948 -3.9992194 -3.9524982 -3.9367452 -3.9624026 -4.0043831 -4.047081 -4.0697474 -4.0923805 -4.095006][-4.2636271 -4.2250123 -4.18117 -4.1355667 -4.081985 -4.0149841 -3.9442468 -3.8767464 -3.8533473 -3.8866425 -3.9542711 -4.0337706 -4.0835829 -4.1209755 -4.1335654][-4.2619071 -4.2193184 -4.165133 -4.1082373 -4.0515356 -3.9857171 -3.9158151 -3.8515139 -3.8384912 -3.8820868 -3.9531145 -4.0395575 -4.0997047 -4.1480064 -4.1634068][-4.2664509 -4.222033 -4.1593456 -4.0906749 -4.0336609 -3.9790735 -3.9247777 -3.8821 -3.88031 -3.922339 -3.9819353 -4.0561604 -4.1037068 -4.1415877 -4.1482615][-4.2721491 -4.2306838 -4.1714177 -4.1080856 -4.0622525 -4.0269308 -3.9907389 -3.9639373 -3.96096 -3.9866338 -4.0253921 -4.077126 -4.1055384 -4.1225615 -4.1147981][-4.2781677 -4.2444415 -4.1979227 -4.1493187 -4.1134119 -4.0914855 -4.0675592 -4.0473986 -4.0386438 -4.0464272 -4.0669646 -4.09959 -4.1139374 -4.1160192 -4.1054745][-4.28794 -4.262784 -4.2310729 -4.1982446 -4.1721973 -4.1569724 -4.139636 -4.1251459 -4.1203308 -4.1225209 -4.1312356 -4.1469707 -4.1500235 -4.1460953 -4.1388655][-4.2995243 -4.2801919 -4.2594695 -4.2383556 -4.2222147 -4.2144251 -4.2035589 -4.1974263 -4.2004786 -4.2037387 -4.2068229 -4.21262 -4.21015 -4.2032509 -4.1978407][-4.3117085 -4.2955928 -4.2815342 -4.2683492 -4.2593136 -4.2584896 -4.2543774 -4.2524891 -4.2573404 -4.2617221 -4.2631741 -4.2636657 -4.2604203 -4.25407 -4.2542987][-4.3225603 -4.3084598 -4.2956705 -4.2848115 -4.2782288 -4.2778177 -4.2753272 -4.274786 -4.278769 -4.2835436 -4.2842913 -4.2825961 -4.2810273 -4.2793765 -4.2845912]]...]
INFO - root - 2017-12-06 01:55:54.385959: step 64210, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.817 sec/batch; 60h:52m:52s remains)
INFO - root - 2017-12-06 01:56:02.931704: step 64220, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.856 sec/batch; 63h:46m:38s remains)
INFO - root - 2017-12-06 01:56:11.261507: step 64230, loss = 2.07, batch loss = 2.02 (9.9 examples/sec; 0.812 sec/batch; 60h:31m:18s remains)
INFO - root - 2017-12-06 01:56:19.872592: step 64240, loss = 2.07, batch loss = 2.01 (9.6 examples/sec; 0.830 sec/batch; 61h:50m:51s remains)
INFO - root - 2017-12-06 01:56:28.308724: step 64250, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.866 sec/batch; 64h:32m:18s remains)
INFO - root - 2017-12-06 01:56:36.855125: step 64260, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.865 sec/batch; 64h:26m:53s remains)
INFO - root - 2017-12-06 01:56:45.392674: step 64270, loss = 2.02, batch loss = 1.96 (9.5 examples/sec; 0.838 sec/batch; 62h:26m:17s remains)
INFO - root - 2017-12-06 01:56:53.958058: step 64280, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.860 sec/batch; 64h:04m:07s remains)
INFO - root - 2017-12-06 01:57:02.647248: step 64290, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.867 sec/batch; 64h:35m:27s remains)
INFO - root - 2017-12-06 01:57:11.126262: step 64300, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 63h:58m:50s remains)
2017-12-06 01:57:11.945787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2422271 -4.2513385 -4.2560449 -4.260181 -4.2656188 -4.2710414 -4.2677226 -4.2584515 -4.2516446 -4.2477937 -4.2427611 -4.2289758 -4.2138138 -4.2147374 -4.2221851][-4.2385564 -4.2455878 -4.2492151 -4.2530093 -4.2600732 -4.2675261 -4.2607412 -4.2449775 -4.2332997 -4.2262197 -4.2194853 -4.2032089 -4.1856012 -4.1867104 -4.1933837][-4.2699914 -4.27047 -4.2677484 -4.2655597 -4.2669983 -4.2685008 -4.2567124 -4.2389417 -4.229527 -4.2274613 -4.2259889 -4.2143731 -4.1973667 -4.1962872 -4.2009029][-4.3107491 -4.2997332 -4.2824488 -4.2664719 -4.25526 -4.2466826 -4.2309375 -4.2169933 -4.2186403 -4.2301688 -4.2419238 -4.2423282 -4.2332573 -4.2355251 -4.2410502][-4.3164773 -4.2916479 -4.2576084 -4.2272453 -4.2028275 -4.1806931 -4.1584239 -4.1485004 -4.1631961 -4.1931391 -4.2233429 -4.2410994 -4.2474036 -4.2601943 -4.2740908][-4.2845826 -4.2539749 -4.2132206 -4.1728883 -4.13206 -4.0904608 -4.0523872 -4.0395694 -4.0685892 -4.1200633 -4.1708779 -4.2072186 -4.2310829 -4.2593303 -4.2852187][-4.2365861 -4.2074342 -4.1650276 -4.1118941 -4.0504766 -3.9828935 -3.922013 -3.9023752 -3.943759 -4.0142274 -4.0805554 -4.1307096 -4.1735744 -4.223208 -4.2654448][-4.191462 -4.1699529 -4.1305265 -4.0671515 -3.9898777 -3.9111438 -3.8466232 -3.8328977 -3.8769128 -3.94588 -4.0076752 -4.05975 -4.1137738 -4.1785779 -4.2334375][-4.1720219 -4.167634 -4.1417766 -4.0864863 -4.0227013 -3.9671879 -3.9303613 -3.925899 -3.948849 -3.9864609 -4.0208139 -4.0561342 -4.1030822 -4.1642885 -4.2187719][-4.1879654 -4.1955085 -4.18216 -4.1443939 -4.1054082 -4.0774865 -4.0606704 -4.056181 -4.0580845 -4.0676026 -4.081759 -4.1035223 -4.1390271 -4.1882 -4.2352896][-4.2293549 -4.2425561 -4.236733 -4.2147884 -4.19348 -4.1801267 -4.1735978 -4.1724882 -4.1709991 -4.1723676 -4.1801353 -4.1942711 -4.21748 -4.2498937 -4.2831779][-4.291851 -4.3021321 -4.2988153 -4.2869534 -4.2759943 -4.2700748 -4.2703032 -4.275115 -4.2779026 -4.2791114 -4.2836413 -4.2917423 -4.3035946 -4.3191853 -4.3363671][-4.3435621 -4.3484683 -4.3450751 -4.3374529 -4.3321271 -4.3298388 -4.3315063 -4.3356037 -4.3387227 -4.3404841 -4.3444552 -4.3498535 -4.356029 -4.3622484 -4.3685422][-4.3692994 -4.3719964 -4.3708086 -4.3669128 -4.3648176 -4.3638515 -4.364181 -4.3651018 -4.3658876 -4.366631 -4.3690743 -4.3718443 -4.3746839 -4.3763986 -4.3772922][-4.3760395 -4.3777204 -4.3781962 -4.3774281 -4.3775845 -4.3780437 -4.3778892 -4.3768849 -4.375567 -4.3744278 -4.374835 -4.3760405 -4.3774409 -4.3775992 -4.3766065]]...]
INFO - root - 2017-12-06 01:57:20.546629: step 64310, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 63h:58m:42s remains)
INFO - root - 2017-12-06 01:57:29.091751: step 64320, loss = 2.08, batch loss = 2.02 (9.6 examples/sec; 0.833 sec/batch; 62h:05m:05s remains)
INFO - root - 2017-12-06 01:57:37.562252: step 64330, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 64h:16m:41s remains)
INFO - root - 2017-12-06 01:57:46.135487: step 64340, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.869 sec/batch; 64h:45m:48s remains)
INFO - root - 2017-12-06 01:57:54.624150: step 64350, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 65h:27m:46s remains)
INFO - root - 2017-12-06 01:58:03.179918: step 64360, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 64h:52m:34s remains)
INFO - root - 2017-12-06 01:58:11.673200: step 64370, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.829 sec/batch; 61h:45m:05s remains)
INFO - root - 2017-12-06 01:58:20.200719: step 64380, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.851 sec/batch; 63h:22m:06s remains)
INFO - root - 2017-12-06 01:58:28.760039: step 64390, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 64h:42m:08s remains)
INFO - root - 2017-12-06 01:58:37.405199: step 64400, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 63h:23m:10s remains)
2017-12-06 01:58:38.199942: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2765303 -4.2778134 -4.2825928 -4.2836919 -4.28075 -4.2784381 -4.2780666 -4.280622 -4.2838659 -4.2856932 -4.2853317 -4.2843351 -4.2830229 -4.2802181 -4.2795672][-4.2326679 -4.2322359 -4.2403965 -4.2474275 -4.2495904 -4.2530513 -4.2581654 -4.2657895 -4.2734904 -4.2778692 -4.2772961 -4.2740517 -4.2697263 -4.2622423 -4.2573261][-4.204608 -4.2022696 -4.2112308 -4.2225528 -4.2272992 -4.2323656 -4.2392993 -4.2490759 -4.2600985 -4.2676206 -4.2696509 -4.2684379 -4.2637281 -4.2557521 -4.2495322][-4.2004209 -4.1942406 -4.1976252 -4.20543 -4.2061191 -4.2073073 -4.2113862 -4.2206669 -4.2354484 -4.2477274 -4.2551007 -4.2590609 -4.2587185 -4.2538252 -4.2489738][-4.2024903 -4.187139 -4.1788397 -4.1758304 -4.16805 -4.1628861 -4.1610956 -4.1668797 -4.1840563 -4.2015133 -4.2145658 -4.2269878 -4.2343011 -4.2353768 -4.2358303][-4.1968155 -4.1703238 -4.1469693 -4.126452 -4.1026554 -4.0848565 -4.0699549 -4.0688338 -4.0901809 -4.1154757 -4.1381054 -4.1645951 -4.1829362 -4.1921992 -4.201333][-4.1502204 -4.1089716 -4.0676169 -4.0258007 -3.9828939 -3.9485 -3.9163234 -3.9039297 -3.9311495 -3.9694505 -4.0067387 -4.0510006 -4.0845428 -4.1066813 -4.1308479][-4.0713854 -4.0159588 -3.9615936 -3.9058597 -3.8446076 -3.7915077 -3.7392437 -3.7124262 -3.7471788 -3.8047471 -3.8627696 -3.9275987 -3.9816592 -4.018302 -4.0584455][-4.00573 -3.9524159 -3.9087667 -3.8648543 -3.8108506 -3.763948 -3.7175686 -3.6909342 -3.7249243 -3.7865326 -3.8480127 -3.911741 -3.965332 -3.99801 -4.0339012][-3.9958115 -3.9661164 -3.9528625 -3.9432323 -3.9194055 -3.8965161 -3.8727946 -3.8568206 -3.879724 -3.9242907 -3.9652662 -4.0019908 -4.0290489 -4.0388832 -4.0537481][-4.012763 -4.00205 -4.0110908 -4.0273685 -4.0269661 -4.0241642 -4.0191016 -4.0120115 -4.0246015 -4.0501628 -4.0698204 -4.0825467 -4.0896559 -4.0844154 -4.0828338][-4.064404 -4.0608335 -4.0771971 -4.1018839 -4.11095 -4.1169324 -4.1199923 -4.1184154 -4.1254463 -4.1392484 -4.1480064 -4.1495523 -4.148849 -4.1387696 -4.1297507][-4.1576939 -4.1576385 -4.1729612 -4.1941772 -4.2038484 -4.2116623 -4.2164369 -4.2169719 -4.2197046 -4.2246013 -4.2270217 -4.2262607 -4.2258716 -4.2194409 -4.2125831][-4.2454944 -4.2483072 -4.261004 -4.2759242 -4.2820687 -4.2864227 -4.2887449 -4.2886686 -4.2888594 -4.2897363 -4.2898602 -4.2900157 -4.2917027 -4.2894626 -4.2866535][-4.3028173 -4.3051338 -4.3128033 -4.3201747 -4.3221955 -4.3230739 -4.3235016 -4.3230596 -4.3226838 -4.3227735 -4.3237715 -4.3258667 -4.3293114 -4.329998 -4.3295455]]...]
INFO - root - 2017-12-06 01:58:46.750797: step 64410, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.876 sec/batch; 65h:14m:52s remains)
INFO - root - 2017-12-06 01:58:55.329940: step 64420, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.859 sec/batch; 63h:58m:16s remains)
INFO - root - 2017-12-06 01:59:03.791851: step 64430, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 64h:57m:50s remains)
INFO - root - 2017-12-06 01:59:12.316244: step 64440, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.858 sec/batch; 63h:53m:15s remains)
INFO - root - 2017-12-06 01:59:20.828881: step 64450, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.853 sec/batch; 63h:30m:29s remains)
INFO - root - 2017-12-06 01:59:29.444161: step 64460, loss = 2.06, batch loss = 2.01 (9.3 examples/sec; 0.856 sec/batch; 63h:43m:48s remains)
INFO - root - 2017-12-06 01:59:37.881087: step 64470, loss = 2.09, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 63h:25m:34s remains)
INFO - root - 2017-12-06 01:59:46.496379: step 64480, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.886 sec/batch; 65h:55m:44s remains)
INFO - root - 2017-12-06 01:59:55.036538: step 64490, loss = 2.05, batch loss = 2.00 (9.5 examples/sec; 0.844 sec/batch; 62h:49m:43s remains)
INFO - root - 2017-12-06 02:00:03.632861: step 64500, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.848 sec/batch; 63h:09m:20s remains)
2017-12-06 02:00:04.439551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2568617 -4.25645 -4.2529907 -4.2391233 -4.2184391 -4.1940365 -4.1837258 -4.1914015 -4.2091203 -4.2305264 -4.2463388 -4.2560434 -4.2485089 -4.2338052 -4.2212062][-4.237762 -4.2424827 -4.2423668 -4.2316432 -4.2119594 -4.1831079 -4.1639009 -4.160881 -4.1700478 -4.1888571 -4.2087336 -4.2286968 -4.2339616 -4.2289405 -4.2272625][-4.231482 -4.2446876 -4.250123 -4.2471018 -4.2335072 -4.2070866 -4.1835966 -4.1726642 -4.1725321 -4.1809483 -4.1942372 -4.2126007 -4.2225213 -4.2208047 -4.22473][-4.2451715 -4.2627635 -4.2711143 -4.274766 -4.2692966 -4.2486343 -4.2234559 -4.2103376 -4.2088728 -4.2112107 -4.2208338 -4.2370172 -4.2480521 -4.24383 -4.2427392][-4.2522 -4.2664189 -4.272635 -4.2825508 -4.2841997 -4.2661147 -4.2364321 -4.2259049 -4.2329879 -4.2394347 -4.2503934 -4.2636981 -4.2761931 -4.2726216 -4.2703466][-4.2406869 -4.24158 -4.2388444 -4.2465744 -4.2440324 -4.2152352 -4.1741843 -4.1778178 -4.2139688 -4.2392216 -4.25546 -4.2696252 -4.2865825 -4.2885022 -4.2904325][-4.2060027 -4.1911707 -4.1760445 -4.1721449 -4.149219 -4.0893216 -4.0271249 -4.0555797 -4.1342731 -4.1843667 -4.2127719 -4.2363262 -4.2651591 -4.2789025 -4.2865829][-4.1783242 -4.1515684 -4.1241722 -4.1032686 -4.056016 -3.9590716 -3.8716135 -3.9248285 -4.0376539 -4.1028419 -4.1389647 -4.1719246 -4.2141595 -4.2444782 -4.2618618][-4.2012811 -4.1757851 -4.1507411 -4.1273613 -4.0826597 -3.9914067 -3.9083893 -3.95675 -4.0514636 -4.0977826 -4.1215725 -4.1507955 -4.1957307 -4.2325325 -4.2535048][-4.246912 -4.2345648 -4.225565 -4.2151928 -4.1934185 -4.1357436 -4.0791922 -4.1067867 -4.1570029 -4.1754971 -4.1823506 -4.1990881 -4.2332573 -4.2638974 -4.2799549][-4.27902 -4.2746673 -4.2751131 -4.274931 -4.2699957 -4.242836 -4.2144423 -4.2312379 -4.2548981 -4.2630496 -4.262794 -4.270093 -4.2906704 -4.3089476 -4.3173509][-4.2841144 -4.2802768 -4.2856345 -4.291543 -4.296154 -4.2871361 -4.2779469 -4.2917919 -4.3046823 -4.3131776 -4.3149762 -4.3187337 -4.3300467 -4.3393855 -4.3432217][-4.2612395 -4.2543888 -4.2613096 -4.2679291 -4.2729268 -4.2691488 -4.2713747 -4.2904181 -4.305728 -4.3218012 -4.3281116 -4.3305879 -4.3368654 -4.3421988 -4.34527][-4.2400036 -4.2295823 -4.2333779 -4.2350316 -4.234479 -4.2310061 -4.2387671 -4.2633271 -4.2846031 -4.306613 -4.3175917 -4.3212194 -4.3266473 -4.33218 -4.3366995][-4.2469196 -4.2394843 -4.2407889 -4.2376881 -4.2331476 -4.2312379 -4.2417655 -4.2642579 -4.2819166 -4.2999454 -4.3102303 -4.3146086 -4.3197637 -4.3250995 -4.3301768]]...]
INFO - root - 2017-12-06 02:00:12.962354: step 64510, loss = 2.03, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 63h:33m:29s remains)
INFO - root - 2017-12-06 02:00:21.404521: step 64520, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.880 sec/batch; 65h:29m:55s remains)
INFO - root - 2017-12-06 02:00:29.835509: step 64530, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.852 sec/batch; 63h:23m:05s remains)
INFO - root - 2017-12-06 02:00:38.389788: step 64540, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:14m:45s remains)
INFO - root - 2017-12-06 02:00:47.072277: step 64550, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.892 sec/batch; 66h:24m:12s remains)
INFO - root - 2017-12-06 02:00:55.698686: step 64560, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.835 sec/batch; 62h:08m:02s remains)
INFO - root - 2017-12-06 02:01:03.871506: step 64570, loss = 2.09, batch loss = 2.03 (12.0 examples/sec; 0.664 sec/batch; 49h:24m:55s remains)
INFO - root - 2017-12-06 02:01:12.364651: step 64580, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.831 sec/batch; 61h:51m:59s remains)
INFO - root - 2017-12-06 02:01:20.958300: step 64590, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 63h:38m:48s remains)
INFO - root - 2017-12-06 02:01:29.418336: step 64600, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.839 sec/batch; 62h:25m:23s remains)
2017-12-06 02:01:30.193559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2219381 -4.1783233 -4.1456451 -4.1233945 -4.1021762 -4.0788965 -4.0415192 -4.0265517 -4.076169 -4.1319451 -4.1560178 -4.1703529 -4.181025 -4.1877184 -4.1863742][-4.2295227 -4.1913719 -4.1644912 -4.1480117 -4.1274376 -4.0977368 -4.05666 -4.0425429 -4.0870919 -4.1348262 -4.1509185 -4.165503 -4.1767564 -4.1792965 -4.1685939][-4.2395363 -4.2037253 -4.1726184 -4.1568193 -4.1359811 -4.1009526 -4.059216 -4.0521059 -4.0967278 -4.1354847 -4.1450644 -4.1622124 -4.175395 -4.1755447 -4.1555943][-4.2465963 -4.2082019 -4.1709628 -4.1543422 -4.13213 -4.0880861 -4.0379868 -4.0295644 -4.0797153 -4.1187668 -4.1295109 -4.1464953 -4.1569223 -4.1551046 -4.1288509][-4.2487407 -4.2043228 -4.1588421 -4.1312504 -4.0992842 -4.0405364 -3.9716167 -3.9652488 -4.0338073 -4.0843935 -4.1001277 -4.1163797 -4.124578 -4.1183023 -4.0831323][-4.2457981 -4.1933975 -4.1372666 -4.09368 -4.0422888 -3.9526274 -3.8404009 -3.8357697 -3.9412332 -4.0144114 -4.0435605 -4.0679135 -4.0775223 -4.0676537 -4.0295844][-4.2397013 -4.1812472 -4.115541 -4.0513883 -3.9645128 -3.8183331 -3.6363268 -3.6432164 -3.8284526 -3.9532669 -4.0056319 -4.0357022 -4.0452604 -4.0355535 -3.9984238][-4.236711 -4.1713324 -4.0963216 -4.0224876 -3.9233303 -3.7664902 -3.5800266 -3.6191781 -3.8469377 -3.9848022 -4.0301194 -4.0520291 -4.0587864 -4.0456729 -4.0048871][-4.2356939 -4.17179 -4.1016932 -4.0429354 -3.9742479 -3.8875074 -3.7932317 -3.8296967 -3.9849555 -4.0779595 -4.0912237 -4.0890203 -4.0879507 -4.0750127 -4.0441437][-4.2445884 -4.1923776 -4.1382604 -4.1003714 -4.0589304 -4.0184884 -3.9754641 -3.9953413 -4.0928483 -4.150816 -4.1408195 -4.1188078 -4.1118178 -4.107676 -4.0965457][-4.254034 -4.2116661 -4.17158 -4.145647 -4.1176128 -4.0983844 -4.0694346 -4.0685711 -4.133657 -4.173687 -4.1522908 -4.1206856 -4.11452 -4.1259079 -4.1339965][-4.2548418 -4.2169156 -4.1783981 -4.1526494 -4.1297808 -4.1157618 -4.0810318 -4.0596466 -4.111062 -4.1476841 -4.1291189 -4.1108375 -4.1186213 -4.1423712 -4.1634183][-4.2536182 -4.2148509 -4.1715808 -4.141324 -4.1160984 -4.1002073 -4.0617533 -4.0398746 -4.098443 -4.1444993 -4.1388011 -4.1358361 -4.1513815 -4.1720963 -4.189476][-4.2570353 -4.21556 -4.1680975 -4.1418123 -4.1257396 -4.1117735 -4.0753727 -4.0587459 -4.1217995 -4.1770973 -4.1812077 -4.1876025 -4.2015476 -4.2107782 -4.21955][-4.2634931 -4.226758 -4.1892266 -4.1722379 -4.1629438 -4.1540432 -4.1300855 -4.1221194 -4.1777344 -4.2294126 -4.2411742 -4.2503886 -4.2584949 -4.2587266 -4.2563777]]...]
INFO - root - 2017-12-06 02:01:38.687214: step 64610, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 63h:37m:41s remains)
INFO - root - 2017-12-06 02:01:47.354612: step 64620, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.843 sec/batch; 62h:43m:52s remains)
INFO - root - 2017-12-06 02:01:55.676617: step 64630, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.849 sec/batch; 63h:10m:17s remains)
INFO - root - 2017-12-06 02:02:04.215310: step 64640, loss = 2.05, batch loss = 1.99 (8.5 examples/sec; 0.942 sec/batch; 70h:03m:24s remains)
INFO - root - 2017-12-06 02:02:12.808698: step 64650, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.845 sec/batch; 62h:52m:04s remains)
INFO - root - 2017-12-06 02:02:21.403341: step 64660, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.880 sec/batch; 65h:29m:04s remains)
INFO - root - 2017-12-06 02:02:29.989129: step 64670, loss = 2.04, batch loss = 1.98 (9.6 examples/sec; 0.837 sec/batch; 62h:17m:35s remains)
INFO - root - 2017-12-06 02:02:38.526220: step 64680, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 64h:54m:18s remains)
INFO - root - 2017-12-06 02:02:47.053731: step 64690, loss = 2.07, batch loss = 2.02 (9.3 examples/sec; 0.860 sec/batch; 63h:58m:44s remains)
INFO - root - 2017-12-06 02:02:55.501059: step 64700, loss = 2.04, batch loss = 1.98 (9.8 examples/sec; 0.814 sec/batch; 60h:32m:11s remains)
2017-12-06 02:02:56.340311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2175956 -4.1985688 -4.1999383 -4.217257 -4.2333865 -4.2283516 -4.2123661 -4.1925573 -4.1700096 -4.1434393 -4.1402578 -4.162 -4.1892562 -4.1882539 -4.1712008][-4.1835041 -4.1538458 -4.1472383 -4.161624 -4.1850209 -4.1996522 -4.2072144 -4.20236 -4.183311 -4.1582055 -4.1604986 -4.1896477 -4.2195749 -4.2167511 -4.1968837][-4.1644254 -4.1292953 -4.1147938 -4.1161342 -4.1338534 -4.1645832 -4.1965137 -4.2103186 -4.2036257 -4.189795 -4.1991549 -4.2252545 -4.243485 -4.2328577 -4.2119422][-4.1583691 -4.1313934 -4.1138654 -4.1003265 -4.1041036 -4.1351795 -4.1757 -4.2067614 -4.2178578 -4.21697 -4.2286396 -4.2490692 -4.2524457 -4.2351632 -4.2146177][-4.1530566 -4.137331 -4.1270723 -4.1108475 -4.10045 -4.1117678 -4.1459923 -4.1888914 -4.2175875 -4.2257271 -4.2330327 -4.2428145 -4.2343082 -4.2160254 -4.2014704][-4.1639242 -4.1558809 -4.1471891 -4.1272697 -4.102407 -4.0846334 -4.0924797 -4.1309853 -4.1734695 -4.1993241 -4.2094026 -4.2109051 -4.1965265 -4.18398 -4.1859446][-4.1489387 -4.149755 -4.1390963 -4.1072283 -4.0615396 -4.0076613 -3.970139 -3.9871426 -4.0539541 -4.118742 -4.1454439 -4.1431203 -4.1251221 -4.1273909 -4.1535025][-4.0957131 -4.10585 -4.0923347 -4.05078 -3.9918988 -3.9029551 -3.8046184 -3.783515 -3.8892832 -4.0005336 -4.0466008 -4.0424342 -4.0322886 -4.0565243 -4.108273][-4.0668492 -4.08293 -4.0684209 -4.0270705 -3.9718256 -3.8783774 -3.7648785 -3.725354 -3.8369112 -3.9613783 -4.0178771 -4.0203123 -4.0202284 -4.0506887 -4.1072927][-4.0990515 -4.1154013 -4.1034064 -4.0734997 -4.0362358 -3.9762175 -3.9078715 -3.8839746 -3.9526455 -4.0402312 -4.0877438 -4.0964789 -4.099206 -4.1165791 -4.1558447][-4.1366434 -4.1497874 -4.14852 -4.1357908 -4.11849 -4.0925984 -4.0622587 -4.0513496 -4.08179 -4.1280227 -4.1598063 -4.1674185 -4.1648397 -4.1675191 -4.1840878][-4.1690106 -4.1856833 -4.190506 -4.1886463 -4.1828117 -4.176074 -4.1665363 -4.1592946 -4.1672459 -4.18448 -4.1998973 -4.1986833 -4.187129 -4.1805215 -4.1871109][-4.1950068 -4.2153053 -4.2242641 -4.228672 -4.2269821 -4.2276111 -4.2283058 -4.2224188 -4.2174826 -4.2169027 -4.2142606 -4.1989107 -4.1775765 -4.1669817 -4.1742744][-4.217617 -4.2391086 -4.2480769 -4.252821 -4.2519803 -4.2515917 -4.2511516 -4.2452221 -4.2313304 -4.2171173 -4.1998887 -4.1760912 -4.1533108 -4.1465268 -4.1544218][-4.2238631 -4.2387338 -4.2420988 -4.2433743 -4.2394757 -4.2359462 -4.2327833 -4.2277031 -4.2136974 -4.1954789 -4.16913 -4.1390824 -4.1185641 -4.1187625 -4.12823]]...]
INFO - root - 2017-12-06 02:03:04.741716: step 64710, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.815 sec/batch; 60h:38m:24s remains)
INFO - root - 2017-12-06 02:03:13.264102: step 64720, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.855 sec/batch; 63h:35m:58s remains)
INFO - root - 2017-12-06 02:03:21.785096: step 64730, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.874 sec/batch; 65h:00m:46s remains)
INFO - root - 2017-12-06 02:03:30.231096: step 64740, loss = 2.07, batch loss = 2.01 (9.4 examples/sec; 0.855 sec/batch; 63h:34m:23s remains)
INFO - root - 2017-12-06 02:03:38.776960: step 64750, loss = 2.04, batch loss = 1.98 (9.4 examples/sec; 0.854 sec/batch; 63h:29m:04s remains)
INFO - root - 2017-12-06 02:03:47.459253: step 64760, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.882 sec/batch; 65h:36m:08s remains)
INFO - root - 2017-12-06 02:03:55.952894: step 64770, loss = 2.08, batch loss = 2.02 (9.3 examples/sec; 0.859 sec/batch; 63h:52m:04s remains)
INFO - root - 2017-12-06 02:04:04.507051: step 64780, loss = 2.07, batch loss = 2.01 (9.8 examples/sec; 0.819 sec/batch; 60h:56m:17s remains)
INFO - root - 2017-12-06 02:04:12.941459: step 64790, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.851 sec/batch; 63h:16m:21s remains)
INFO - root - 2017-12-06 02:04:21.497441: step 64800, loss = 2.04, batch loss = 1.98 (9.0 examples/sec; 0.886 sec/batch; 65h:53m:02s remains)
2017-12-06 02:04:22.334340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2599158 -4.2635422 -4.2702303 -4.2756948 -4.2774725 -4.2719641 -4.2621393 -4.2559552 -4.24807 -4.2286797 -4.1987472 -4.1635904 -4.135603 -4.1519923 -4.2124271][-4.2459755 -4.2432852 -4.244945 -4.2512145 -4.2546821 -4.2514729 -4.246551 -4.2461972 -4.2466979 -4.2342162 -4.2074041 -4.1752038 -4.1479959 -4.1579828 -4.2111096][-4.2269197 -4.2243791 -4.2183328 -4.2185049 -4.2208767 -4.2180924 -4.2148485 -4.2194648 -4.2270083 -4.2203059 -4.1972461 -4.1707778 -4.1496854 -4.1605644 -4.2091408][-4.21051 -4.2151103 -4.2074 -4.2002997 -4.195734 -4.1877751 -4.182035 -4.1875453 -4.2013607 -4.1965542 -4.1741118 -4.1515646 -4.1378889 -4.1547141 -4.2026482][-4.1955481 -4.2057781 -4.1983151 -4.1868815 -4.1719193 -4.1528115 -4.1373019 -4.1429367 -4.1635408 -4.167109 -4.1476083 -4.1268449 -4.1201468 -4.1426816 -4.1918941][-4.1733456 -4.1858125 -4.1815662 -4.1701207 -4.1483259 -4.1170683 -4.0859461 -4.0845728 -4.1108427 -4.1300869 -4.1225429 -4.1083689 -4.1094985 -4.1378403 -4.189611][-4.1463475 -4.1660061 -4.1686406 -4.16153 -4.1410418 -4.1046247 -4.0619264 -4.0481453 -4.0705276 -4.0957093 -4.0975189 -4.0892763 -4.0977082 -4.1342235 -4.1919651][-4.1423745 -4.1689954 -4.1799946 -4.1788898 -4.1632123 -4.1281395 -4.0846262 -4.0619712 -4.0715566 -4.090673 -4.0934362 -4.083859 -4.0925379 -4.1327729 -4.1938066][-4.1738973 -4.2058125 -4.2237763 -4.2258534 -4.2105093 -4.1809235 -4.1432428 -4.1154332 -4.1144247 -4.1258793 -4.1237564 -4.1121659 -4.1169915 -4.1532307 -4.2090149][-4.2195234 -4.2543612 -4.2752571 -4.2778735 -4.2641816 -4.2429261 -4.2120328 -4.1842813 -4.1768856 -4.18024 -4.1721096 -4.1614718 -4.1653471 -4.1944189 -4.2405581][-4.2616444 -4.2943454 -4.3116827 -4.3147473 -4.3045664 -4.2913227 -4.2679377 -4.2448268 -4.2358041 -4.231811 -4.2179933 -4.2099423 -4.2150922 -4.2381639 -4.2758985][-4.2898288 -4.3182716 -4.3298531 -4.3305588 -4.3224587 -4.312984 -4.2941647 -4.2750297 -4.2664971 -4.2587333 -4.2461009 -4.2431479 -4.2533784 -4.2755661 -4.3080177][-4.2985458 -4.3230729 -4.3292046 -4.3278632 -4.3226342 -4.3150587 -4.2993422 -4.2861643 -4.2784638 -4.2687621 -4.26054 -4.2618403 -4.2738256 -4.2943363 -4.32351][-4.2920251 -4.3115263 -4.3165007 -4.3152623 -4.3141026 -4.3088474 -4.2967973 -4.2881966 -4.28302 -4.2762976 -4.2726216 -4.274755 -4.2821174 -4.2966962 -4.3219805][-4.2886076 -4.3007784 -4.3031993 -4.301127 -4.3010445 -4.3001142 -4.2940288 -4.2880635 -4.2840719 -4.2797308 -4.2784085 -4.279376 -4.2824879 -4.2923913 -4.3127251]]...]
INFO - root - 2017-12-06 02:04:30.899075: step 64810, loss = 2.10, batch loss = 2.04 (9.4 examples/sec; 0.852 sec/batch; 63h:23m:11s remains)
INFO - root - 2017-12-06 02:04:39.574074: step 64820, loss = 2.05, batch loss = 1.99 (9.1 examples/sec; 0.883 sec/batch; 65h:39m:28s remains)
INFO - root - 2017-12-06 02:04:47.975832: step 64830, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 65h:51m:09s remains)
INFO - root - 2017-12-06 02:04:56.592679: step 64840, loss = 2.06, batch loss = 2.00 (9.5 examples/sec; 0.845 sec/batch; 62h:49m:27s remains)
INFO - root - 2017-12-06 02:05:05.025361: step 64850, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.859 sec/batch; 63h:54m:00s remains)
INFO - root - 2017-12-06 02:05:13.434023: step 64860, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 64h:39m:48s remains)
INFO - root - 2017-12-06 02:05:21.995635: step 64870, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.878 sec/batch; 65h:16m:26s remains)
INFO - root - 2017-12-06 02:05:30.633259: step 64880, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.858 sec/batch; 63h:45m:53s remains)
INFO - root - 2017-12-06 02:05:39.090096: step 64890, loss = 2.07, batch loss = 2.01 (10.6 examples/sec; 0.757 sec/batch; 56h:14m:41s remains)
INFO - root - 2017-12-06 02:05:47.556800: step 64900, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.871 sec/batch; 64h:45m:53s remains)
2017-12-06 02:05:48.302607: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2522445 -4.2327175 -4.2309022 -4.2395077 -4.2541242 -4.2786932 -4.3022661 -4.3229375 -4.3350444 -4.3308239 -4.3063812 -4.2664895 -4.2354856 -4.229569 -4.2363811][-4.2580066 -4.2394052 -4.2335849 -4.2361574 -4.2474794 -4.2724428 -4.2995286 -4.32362 -4.3359747 -4.329504 -4.3084712 -4.2741385 -4.2459226 -4.2374129 -4.2360849][-4.2542324 -4.2386827 -4.2265978 -4.2206211 -4.2285562 -4.2557678 -4.2860413 -4.306848 -4.3159356 -4.3113079 -4.2956452 -4.2713656 -4.2551122 -4.2533131 -4.2501421][-4.2374449 -4.2247524 -4.20273 -4.1815991 -4.1850243 -4.2148304 -4.2508755 -4.2677231 -4.2782092 -4.2833471 -4.2804112 -4.2701054 -4.2668524 -4.2726846 -4.2713795][-4.2115397 -4.1892552 -4.1528592 -4.1127 -4.1075344 -4.1385717 -4.1826167 -4.204309 -4.2250347 -4.2442417 -4.2594719 -4.2651834 -4.2719164 -4.2836547 -4.2876172][-4.1775208 -4.1327572 -4.07361 -4.0089521 -3.9913249 -4.0303574 -4.089972 -4.1288896 -4.1632748 -4.1973138 -4.2289486 -4.2436781 -4.2579651 -4.2809525 -4.2971282][-4.1464357 -4.0805044 -3.9941573 -3.8911266 -3.8523526 -3.9087167 -3.9933536 -4.0516348 -4.0945082 -4.1393147 -4.1859422 -4.2079606 -4.2319169 -4.2705688 -4.3001671][-4.1365304 -4.06454 -3.9704654 -3.8531432 -3.8007698 -3.8604887 -3.9548867 -4.019588 -4.061285 -4.1069245 -4.1610279 -4.1924491 -4.2241964 -4.2695079 -4.3027568][-4.1639252 -4.1024833 -4.0294995 -3.9464657 -3.9030302 -3.9371161 -4.0067611 -4.0592618 -4.0910931 -4.1256227 -4.1744437 -4.2119036 -4.2468128 -4.2822704 -4.3047943][-4.2136421 -4.1673365 -4.1207805 -4.0785303 -4.0559511 -4.071414 -4.110815 -4.1448092 -4.166779 -4.1890192 -4.224565 -4.2559338 -4.2795339 -4.2921004 -4.290997][-4.2617936 -4.2315941 -4.2065682 -4.1918769 -4.1842027 -4.192194 -4.2165909 -4.2397709 -4.2546697 -4.2662683 -4.2837567 -4.2958403 -4.2968698 -4.2839942 -4.2620749][-4.3013239 -4.286159 -4.2748694 -4.273922 -4.2735157 -4.2759204 -4.2876344 -4.3015175 -4.3130584 -4.3167109 -4.3160343 -4.3091693 -4.2880878 -4.256669 -4.2214241][-4.3279805 -4.3206739 -4.3155389 -4.3204794 -4.3238034 -4.3221154 -4.3242431 -4.330617 -4.3367119 -4.3310566 -4.3180161 -4.2989717 -4.2619286 -4.2222257 -4.1834288][-4.3342013 -4.3312125 -4.3280396 -4.3348732 -4.3401866 -4.3396506 -4.3382812 -4.33876 -4.3365111 -4.3209853 -4.2989 -4.2749386 -4.2338667 -4.1942973 -4.1573343][-4.3258581 -4.3302474 -4.330852 -4.3369985 -4.3424144 -4.3432293 -4.3404651 -4.3366404 -4.3265462 -4.3021045 -4.2745781 -4.2538486 -4.2167864 -4.1780796 -4.1454015]]...]
INFO - root - 2017-12-06 02:05:56.804941: step 64910, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.860 sec/batch; 63h:54m:40s remains)
INFO - root - 2017-12-06 02:06:05.416331: step 64920, loss = 2.08, batch loss = 2.02 (9.2 examples/sec; 0.871 sec/batch; 64h:45m:38s remains)
INFO - root - 2017-12-06 02:06:13.811845: step 64930, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.863 sec/batch; 64h:09m:10s remains)
INFO - root - 2017-12-06 02:06:22.466232: step 64940, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.872 sec/batch; 64h:48m:49s remains)
INFO - root - 2017-12-06 02:06:31.055794: step 64950, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.870 sec/batch; 64h:38m:35s remains)
INFO - root - 2017-12-06 02:06:39.503392: step 64960, loss = 2.08, batch loss = 2.02 (9.0 examples/sec; 0.885 sec/batch; 65h:44m:47s remains)
INFO - root - 2017-12-06 02:06:48.085434: step 64970, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.867 sec/batch; 64h:27m:49s remains)
INFO - root - 2017-12-06 02:06:56.625720: step 64980, loss = 2.03, batch loss = 1.97 (9.5 examples/sec; 0.842 sec/batch; 62h:33m:30s remains)
INFO - root - 2017-12-06 02:07:05.166929: step 64990, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 64h:32m:03s remains)
INFO - root - 2017-12-06 02:07:13.642057: step 65000, loss = 2.07, batch loss = 2.01 (9.2 examples/sec; 0.865 sec/batch; 64h:18m:29s remains)
2017-12-06 02:07:14.448724: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1111183 -4.0987835 -4.0837502 -4.0852823 -4.0981178 -4.1027637 -4.0991764 -4.1038461 -4.1063933 -4.0979118 -4.105422 -4.1096239 -4.0945086 -4.07535 -4.0679431][-4.1166739 -4.09591 -4.0705466 -4.068028 -4.0879455 -4.1002431 -4.1066561 -4.116559 -4.12032 -4.1179123 -4.1298389 -4.1356263 -4.1202478 -4.1006689 -4.0922446][-4.1609578 -4.1460228 -4.1210279 -4.1160097 -4.13562 -4.1505713 -4.1644773 -4.1760654 -4.1773605 -4.1739221 -4.1767139 -4.1741757 -4.1573639 -4.1419554 -4.1404424][-4.2057638 -4.2069926 -4.1976819 -4.1956763 -4.2092147 -4.2194834 -4.2301559 -4.2402158 -4.2422223 -4.2340264 -4.2233777 -4.209218 -4.1868782 -4.1677155 -4.1634388][-4.2267447 -4.242733 -4.2502518 -4.2507195 -4.2555833 -4.2552681 -4.2547665 -4.2574859 -4.2606444 -4.2572427 -4.2460513 -4.2291555 -4.2061577 -4.1829476 -4.1702185][-4.2231827 -4.2445803 -4.2573628 -4.2550044 -4.2487621 -4.2368946 -4.2247014 -4.2178636 -4.2205167 -4.2245359 -4.2272339 -4.2251406 -4.2133446 -4.1966867 -4.1798582][-4.20342 -4.2200971 -4.2253785 -4.2095761 -4.1819239 -4.1518769 -4.1291451 -4.1193595 -4.1256886 -4.14727 -4.1766353 -4.2001257 -4.206387 -4.2017956 -4.1896596][-4.1734047 -4.17982 -4.1688695 -4.1375489 -4.0905976 -4.0403538 -4.0014572 -3.9831133 -3.9921117 -4.0352616 -4.1020694 -4.1619644 -4.1928325 -4.2032046 -4.2009077][-4.1403408 -4.1361136 -4.1119175 -4.076746 -4.0281963 -3.9720488 -3.927485 -3.9046123 -3.9050696 -3.9516842 -4.0362282 -4.1207929 -4.1738873 -4.1961818 -4.200758][-4.122478 -4.1123986 -4.0892997 -4.0729585 -4.0426741 -3.9990091 -3.9694443 -3.9531896 -3.9363163 -3.9539735 -4.0196671 -4.0975456 -4.1513519 -4.1767945 -4.1843681][-4.1205559 -4.1147642 -4.1085467 -4.1125813 -4.0987873 -4.0697346 -4.0621362 -4.0624194 -4.0401707 -4.025353 -4.0483003 -4.094666 -4.1333685 -4.1517696 -4.1523147][-4.157186 -4.1601825 -4.1663022 -4.1753974 -4.1657505 -4.1442752 -4.1486096 -4.1613712 -4.1460581 -4.1186614 -4.1102166 -4.1264162 -4.1433282 -4.1503205 -4.143445][-4.202364 -4.2121987 -4.2200994 -4.2233334 -4.2155266 -4.2053161 -4.212873 -4.2251496 -4.2186894 -4.19504 -4.1749544 -4.1753073 -4.1803417 -4.1813631 -4.1721997][-4.2406764 -4.2491765 -4.2525969 -4.249795 -4.2448492 -4.2425714 -4.2468691 -4.2505803 -4.2454848 -4.2304058 -4.2138853 -4.2085156 -4.2105165 -4.2160707 -4.2163773][-4.2655153 -4.2703347 -4.2711778 -4.2662044 -4.2653084 -4.26718 -4.2673435 -4.2668123 -4.2629771 -4.2541819 -4.2409468 -4.2321162 -4.2334042 -4.2403321 -4.2454171]]...]
INFO - root - 2017-12-06 02:07:23.086456: step 65010, loss = 2.05, batch loss = 1.99 (9.5 examples/sec; 0.840 sec/batch; 62h:23m:10s remains)
INFO - root - 2017-12-06 02:07:31.564968: step 65020, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.855 sec/batch; 63h:33m:11s remains)
INFO - root - 2017-12-06 02:07:40.015484: step 65030, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 63h:59m:44s remains)
INFO - root - 2017-12-06 02:07:48.499982: step 65040, loss = 2.07, batch loss = 2.01 (9.3 examples/sec; 0.863 sec/batch; 64h:07m:30s remains)
INFO - root - 2017-12-06 02:07:57.162026: step 65050, loss = 2.03, batch loss = 1.98 (9.3 examples/sec; 0.858 sec/batch; 63h:43m:10s remains)
INFO - root - 2017-12-06 02:08:05.631632: step 65060, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.869 sec/batch; 64h:34m:40s remains)
INFO - root - 2017-12-06 02:08:14.102732: step 65070, loss = 2.06, batch loss = 2.00 (9.1 examples/sec; 0.883 sec/batch; 65h:37m:39s remains)
INFO - root - 2017-12-06 02:08:22.632958: step 65080, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.868 sec/batch; 64h:29m:09s remains)
INFO - root - 2017-12-06 02:08:31.104206: step 65090, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.850 sec/batch; 63h:06m:22s remains)
INFO - root - 2017-12-06 02:08:39.661661: step 65100, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.866 sec/batch; 64h:19m:05s remains)
2017-12-06 02:08:40.431428: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3250556 -4.3187766 -4.3114939 -4.301281 -4.2809658 -4.2535319 -4.2268062 -4.2205362 -4.2201567 -4.2253556 -4.2400503 -4.2538242 -4.2569327 -4.2586765 -4.2489042][-4.3221784 -4.3125739 -4.2986403 -4.2781734 -4.2426429 -4.2003455 -4.1611376 -4.1499438 -4.148622 -4.1525583 -4.1719303 -4.1905723 -4.1953039 -4.1969681 -4.1865425][-4.3179173 -4.3001084 -4.2777557 -4.2468958 -4.200881 -4.1457396 -4.0897412 -4.0716734 -4.0720706 -4.0786428 -4.1027246 -4.1284733 -4.1307197 -4.1264114 -4.1191254][-4.3123903 -4.2852192 -4.2536325 -4.2142863 -4.1607428 -4.095345 -4.02169 -3.9931614 -3.9987035 -4.0146346 -4.0375805 -4.0655308 -4.069736 -4.0660062 -4.0625453][-4.3053188 -4.268806 -4.2239347 -4.1753154 -4.1203418 -4.046257 -3.9592526 -3.9230995 -3.9433346 -3.9646907 -3.9786112 -4.0107474 -4.0272231 -4.0263748 -4.0212035][-4.2970333 -4.2551932 -4.2044077 -4.1540437 -4.10211 -4.0211492 -3.9249511 -3.8848333 -3.9135377 -3.9316428 -3.9427047 -3.9845457 -4.0115371 -4.0078549 -4.0023861][-4.2896528 -4.2495322 -4.1972189 -4.1439743 -4.0877275 -3.999692 -3.8976614 -3.8530712 -3.8811026 -3.8974733 -3.9158859 -3.9672334 -4.001359 -4.0017548 -3.999347][-4.2817259 -4.2415705 -4.1867852 -4.1288433 -4.0595493 -3.9558806 -3.8417063 -3.7957273 -3.83471 -3.8697126 -3.9124846 -3.9739733 -4.0079546 -4.0177608 -4.0206962][-4.2752204 -4.2317967 -4.177947 -4.1163387 -4.0413976 -3.9284177 -3.8066568 -3.7630706 -3.8253903 -3.8966107 -3.9631035 -4.0214858 -4.0525875 -4.069581 -4.07292][-4.2727275 -4.2276096 -4.173985 -4.1143274 -4.0433598 -3.9426792 -3.840692 -3.8127828 -3.8881423 -3.9777255 -4.0484958 -4.0953445 -4.1150608 -4.1290803 -4.1301532][-4.2742238 -4.2309122 -4.1787415 -4.123353 -4.06634 -3.9945748 -3.9268494 -3.9132318 -3.9817936 -4.06745 -4.1266975 -4.1594515 -4.1718116 -4.1771917 -4.1679406][-4.2836 -4.2475734 -4.2017331 -4.1558847 -4.1171064 -4.0773234 -4.03608 -4.0279627 -4.0822186 -4.1518135 -4.198019 -4.2172985 -4.22759 -4.2282934 -4.209558][-4.298717 -4.2735777 -4.2395964 -4.2091007 -4.1885018 -4.1697741 -4.1497087 -4.1478295 -4.1853442 -4.2326579 -4.2643161 -4.2768154 -4.2830887 -4.2827644 -4.2653074][-4.3126664 -4.2976108 -4.2754784 -4.2571616 -4.2472816 -4.2407885 -4.2342396 -4.2359304 -4.2599316 -4.28676 -4.3055625 -4.313149 -4.3131943 -4.3128672 -4.3035975][-4.3220558 -4.3126321 -4.2992744 -4.2880516 -4.28175 -4.280066 -4.2807016 -4.2859516 -4.300108 -4.3129044 -4.3217325 -4.3249016 -4.3235807 -4.3245597 -4.3222256]]...]
INFO - root - 2017-12-06 02:08:48.931083: step 65110, loss = 2.06, batch loss = 2.00 (9.2 examples/sec; 0.872 sec/batch; 64h:44m:37s remains)
INFO - root - 2017-12-06 02:08:57.448869: step 65120, loss = 2.08, batch loss = 2.03 (9.2 examples/sec; 0.872 sec/batch; 64h:48m:08s remains)
INFO - root - 2017-12-06 02:09:06.003374: step 65130, loss = 2.08, batch loss = 2.03 (9.1 examples/sec; 0.879 sec/batch; 65h:16m:37s remains)
INFO - root - 2017-12-06 02:09:14.547200: step 65140, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.863 sec/batch; 64h:07m:05s remains)
INFO - root - 2017-12-06 02:09:23.026124: step 65150, loss = 2.05, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 63h:23m:29s remains)
INFO - root - 2017-12-06 02:09:31.464435: step 65160, loss = 2.05, batch loss = 1.99 (9.6 examples/sec; 0.834 sec/batch; 61h:57m:34s remains)
INFO - root - 2017-12-06 02:09:40.064644: step 65170, loss = 2.05, batch loss = 1.99 (9.4 examples/sec; 0.855 sec/batch; 63h:27m:47s remains)
INFO - root - 2017-12-06 02:09:48.579451: step 65180, loss = 2.07, batch loss = 2.01 (9.1 examples/sec; 0.879 sec/batch; 65h:15m:27s remains)
INFO - root - 2017-12-06 02:09:57.104413: step 65190, loss = 2.09, batch loss = 2.03 (9.3 examples/sec; 0.857 sec/batch; 63h:36m:23s remains)
INFO - root - 2017-12-06 02:10:05.677875: step 65200, loss = 2.04, batch loss = 1.98 (9.3 examples/sec; 0.862 sec/batch; 63h:58m:01s remains)
2017-12-06 02:10:06.541881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1920762 -4.2052941 -4.2099357 -4.225543 -4.2371635 -4.2418203 -4.2362361 -4.2296548 -4.22594 -4.22733 -4.2318335 -4.237247 -4.2386894 -4.2426724 -4.2484636][-4.1966281 -4.2084527 -4.2112365 -4.2223248 -4.2315845 -4.2343473 -4.2277536 -4.2238288 -4.2240515 -4.2272129 -4.2302408 -4.2301111 -4.2269998 -4.2290564 -4.2351818][-4.2033362 -4.209693 -4.2087812 -4.2156658 -4.2238941 -4.2289906 -4.2270103 -4.2265854 -4.2262883 -4.2274795 -4.2253647 -4.2167115 -4.2051539 -4.1988816 -4.202033][-4.210906 -4.2083092 -4.19726 -4.1933794 -4.1969986 -4.2048216 -4.2125621 -4.2247882 -4.23122 -4.2309589 -4.2223196 -4.2041802 -4.1855659 -4.171227 -4.1701407][-4.2106366 -4.1960273 -4.1718554 -4.152782 -4.1437483 -4.1475544 -4.1663666 -4.2001352 -4.2227292 -4.2294116 -4.2177587 -4.1904058 -4.1654792 -4.1490545 -4.1470556][-4.1926246 -4.16609 -4.1326113 -4.0921769 -4.0536218 -4.0294714 -4.0536222 -4.121315 -4.1736822 -4.2023454 -4.202682 -4.180759 -4.1560688 -4.1397853 -4.1357808][-4.148983 -4.1155663 -4.0830412 -4.0310483 -3.9584842 -3.8880594 -3.900301 -4.0004191 -4.0862312 -4.1388545 -4.1567645 -4.1547117 -4.1457591 -4.1371 -4.1306539][-4.1221986 -4.087409 -4.0593543 -4.0182333 -3.9519839 -3.8683121 -3.8616638 -3.9504488 -4.0342374 -4.0911751 -4.1193781 -4.1371551 -4.1481476 -4.151341 -4.1467886][-4.1259704 -4.0970283 -4.07953 -4.0644617 -4.0338626 -3.9839525 -3.9774528 -4.0276752 -4.0805974 -4.12362 -4.1471367 -4.1613507 -4.1711426 -4.175724 -4.1754842][-4.1622367 -4.1420403 -4.1351614 -4.1374912 -4.1298323 -4.1070457 -4.1054306 -4.1300573 -4.1581988 -4.1830497 -4.1963882 -4.2001896 -4.1988382 -4.1988068 -4.1995482][-4.1929092 -4.1811252 -4.1853747 -4.1979847 -4.2021542 -4.1938066 -4.1921611 -4.2000031 -4.2102919 -4.2222915 -4.2286968 -4.2261229 -4.218986 -4.214323 -4.212708][-4.2011242 -4.1954575 -4.2086062 -4.2297831 -4.2424922 -4.2440076 -4.2432041 -4.24344 -4.244967 -4.2474389 -4.2460055 -4.23786 -4.2281857 -4.2221093 -4.2187238][-4.2013445 -4.2001028 -4.2162991 -4.2414122 -4.2610884 -4.268837 -4.2698493 -4.2656994 -4.2600102 -4.2545881 -4.248518 -4.2395658 -4.2315626 -4.2280064 -4.2243876][-4.2190051 -4.2199473 -4.2354531 -4.25742 -4.27806 -4.2873654 -4.28762 -4.2823849 -4.272418 -4.2619348 -4.2537942 -4.2476735 -4.2442102 -4.2437463 -4.24143][-4.2365732 -4.2383404 -4.2509117 -4.2677212 -4.2839179 -4.2917552 -4.2915912 -4.2882571 -4.279685 -4.2692637 -4.2621913 -4.258286 -4.2589993 -4.2624149 -4.263586]]...]
INFO - root - 2017-12-06 02:10:15.059618: step 65210, loss = 2.06, batch loss = 2.00 (10.8 examples/sec; 0.742 sec/batch; 55h:07m:05s remains)
INFO - root - 2017-12-06 02:10:23.700226: step 65220, loss = 2.06, batch loss = 2.00 (9.0 examples/sec; 0.886 sec/batch; 65h:47m:43s remains)
INFO - root - 2017-12-06 02:10:32.016715: step 65230, loss = 2.04, batch loss = 1.98 (9.1 examples/sec; 0.877 sec/batch; 65h:08m:25s remains)
INFO - root - 2017-12-06 02:10:40.698340: step 65240, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.848 sec/batch; 62h:56m:50s remains)
INFO - root - 2017-12-06 02:10:49.147551: step 65250, loss = 2.07, batch loss = 2.01 (9.5 examples/sec; 0.842 sec/batch; 62h:28m:15s remains)
INFO - root - 2017-12-06 02:10:57.649853: step 65260, loss = 2.06, batch loss = 2.00 (9.4 examples/sec; 0.854 sec/batch; 63h:25m:08s remains)
INFO - root - 2017-12-06 02:11:06.265075: step 65270, loss = 2.08, batch loss = 2.02 (9.4 examples/sec; 0.854 sec/batch; 63h:21m:21s remains)
INFO - root - 2017-12-06 02:11:14.871574: step 65280, loss = 2.09, batch loss = 2.04 (9.3 examples/sec; 0.857 sec/batch; 63h:36m:36s remains)
INFO - root - 2017-12-06 02:11:23.274395: step 65290, loss = 2.06, batch loss = 2.00 (9.7 examples/sec; 0.823 sec/batch; 61h:06m:35s remains)
INFO - root - 2017-12-06 02:11:31.843595: step 65300, loss = 2.05, batch loss = 1.99 (9.3 examples/sec; 0.861 sec/batch; 63h:54m:26s remains)
2017-12-06 02:11:32.610215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2240257 -4.2231894 -4.2375479 -4.252552 -4.2545557 -4.2368236 -4.1885424 -4.1247106 -4.0822773 -4.1066713 -4.1757989 -4.2488737 -4.2824664 -4.2845421 -4.2737489][-4.2127748 -4.2157464 -4.22995 -4.2407775 -4.2398591 -4.220799 -4.1664572 -4.0955644 -4.0520449 -4.0846529 -4.1653876 -4.2464743 -4.2847571 -4.2942419 -4.2906666][-4.2077074 -4.2132249 -4.2209077 -4.2228189 -4.2154775 -4.1911917 -4.1304927 -4.0534167 -4.0109749 -4.0523863 -4.1427031 -4.230288 -4.2738409 -4.2932005 -4.3002973][-4.2122173 -4.2169337 -4.212811 -4.2023907 -4.18538 -4.1484356 -4.0759983 -3.9905281 -3.9477298 -4.0013871 -4.1039295 -4.1993556 -4.2538261 -4.2864752 -4.303967][-4.2201176 -4.2245531 -4.2101631 -4.1868634 -4.1554127 -4.1012878 -4.0127535 -3.9093442 -3.8626575 -3.9376137 -4.0591455 -4.1669731 -4.2339411 -4.277029 -4.3033814][-4.2217746 -4.22828 -4.2114177 -4.1792579 -4.1352711 -4.0701509 -3.9713836 -3.859323 -3.8164034 -3.910109 -4.0409865 -4.1525226 -4.2241845 -4.2713532 -4.3013349][-4.2238441 -4.2327824 -4.2179832 -4.1818833 -4.1296587 -4.0624809 -3.9755793 -3.8873088 -3.8655629 -3.9533181 -4.0673137 -4.1640663 -4.2302146 -4.2745767 -4.30212][-4.2233663 -4.2384319 -4.2288747 -4.1918221 -4.1321263 -4.0656271 -4.0015111 -3.9549515 -3.9585397 -4.0294971 -4.115355 -4.1910481 -4.2463207 -4.2810354 -4.2996244][-4.2162948 -4.2382135 -4.2362189 -4.2006426 -4.1355572 -4.0713177 -4.0280352 -4.017312 -4.0412135 -4.1017346 -4.1655273 -4.2207179 -4.2616572 -4.2849731 -4.2949824][-4.2129736 -4.23975 -4.2457504 -4.215414 -4.1505194 -4.0899429 -4.0596042 -4.0687184 -4.1030293 -4.1546955 -4.2033224 -4.2437334 -4.2733517 -4.2881794 -4.2924471][-4.2166953 -4.245039 -4.2563953 -4.2308159 -4.1716776 -4.1185889 -4.0943265 -4.1084008 -4.1431041 -4.1883759 -4.2296238 -4.2626781 -4.2851105 -4.2923341 -4.2901039][-4.2199354 -4.2472706 -4.2613473 -4.2428832 -4.1936665 -4.1477275 -4.1245203 -4.1343331 -4.1631551 -4.2056279 -4.2456684 -4.2758555 -4.2913651 -4.2884822 -4.2773204][-4.2101364 -4.2328744 -4.2499852 -4.2425747 -4.20537 -4.1655073 -4.1429548 -4.1453252 -4.1688995 -4.2118282 -4.2531857 -4.2795005 -4.2859697 -4.2709589 -4.2482729][-4.1881809 -4.2029171 -4.2219362 -4.2262921 -4.2033424 -4.1713181 -4.1517429 -4.1544976 -4.1781569 -4.2187834 -4.2560964 -4.275423 -4.2717309 -4.2463655 -4.214859][-4.160933 -4.1713185 -4.1914263 -4.2067661 -4.1995249 -4.1784344 -4.1656752 -4.1709194 -4.1920094 -4.2218609 -4.2487516 -4.2625141 -4.2535248 -4.2248182 -4.1901116]]...]
INFO - root - 2017-12-06 02:11:41.120963: step 65310, loss = 2.09, batch loss = 2.03 (9.2 examples/sec; 0.865 sec/batch; 64h:13m:07s remains)
INFO - root - 2017-12-06 02:11:49.835819: step 65320, loss = 2.05, batch loss = 1.99 (9.0 examples/sec; 0.888 sec/batch; 65h:52m:54s remains)
INFO - root - 2017-12-06 02:11:58.229854: step 65330, loss = 2.08, batch loss = 2.02 (9.9 examples/sec; 0.806 sec/batch; 59h:47m:38s remains)
INFO - root - 2017-12-06 02:12:06.730408: step 65340, loss = 2.06, batch loss = 2.00 (9.3 examples/sec; 0.861 sec/batch; 63h:52m:21s remains)
INFO - root - 2017-12-06 02:12:15.328474: step 65350, loss = 2.05, batch loss = 1.99 (9.2 examples/sec; 0.873 sec/batch; 64h:45m:34s remains)
INFO - root - 2017-12-06 02:12:23.791388: step 65360, loss = 2.06, batch loss = 2.00 (9.6 examples/sec; 0.833 sec/batch; 61h:50m:44s remains)
INFO - root - 2017-12-06 02:12:32.385448: step 65370, loss = 2.08, batch loss = 2.02 (9.1 examples/sec; 0.879 sec/batch; 65h:13m:38s remains)
